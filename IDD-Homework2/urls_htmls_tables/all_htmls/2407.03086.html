<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.03086] Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation</title><meta property="og:description" content="While federated learning leverages distributed client resources, it faces challenges due to heterogeneous client capabilities. This necessitates allocating models suited to clientsâ€™ resources and careful parameter aggrâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.03086">

<!--Generated on Mon Aug  5 18:19:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Effective Heterogeneous Federated Learning <span id="id2.id1" class="ltx_text ltx_font_italic">via</span> Efficient Hypernetwork-based Weight Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yujin Shin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yujin_shin@yonsei.ac.kr">yujin_shin@yonsei.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">Yonsei University</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kichang Lee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kichang.lee@yonsei.ac.kr">kichang.lee@yonsei.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Yonsei University</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sungmin Lee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:i.am.sungmin.lee@yonsei.ac.kr">i.am.sungmin.lee@yonsei.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Yonsei University</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">You Rim Choi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yrchoi@snu.ac.kr">yrchoi@snu.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Seoul National University</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hyung-Sin Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:hyungkim@snu.ac.kr">hyungkim@snu.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">Seoul National University</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">JeongGil Ko
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jeonggil.ko@yonsei.ac.kr">jeonggil.ko@yonsei.ac.kr</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Yonsei University, POSTECH</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id1.1" class="ltx_p">While federated learning leverages distributed client resources, it faces challenges due to heterogeneous client capabilities. This necessitates allocating models suited to clientsâ€™ resources and careful parameter aggregation to accommodate this heterogeneity. We propose <span id="id1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, a novel federated learning framework for supporting client heterogeneity by combining a multi-exit network architecture with hypernetwork-based model weight generation. This approach aligns the feature spaces of heterogeneous model layers and resolves per-layer information disparity during weight aggregation.
To practically realize <span id="id1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>, we also propose a low-rank factorization approach to minimize computation and memory overhead associated with hypernetworks.
Our evaluations on a real-world heterogeneous device testbed indicate that <span id="id1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> enhances accuracy by 5.12% over FedAvg, reduces the hypernetwork memory requirements by 98.22%, and accelerates its operations by 1.86<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> compared to a naive hypernetwork approach. These results demonstrate <span id="id1.1.4" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s effectiveness in leveraging and engaging heterogeneous clients for federated learning.</p>
</div>
<div class="ltx_acknowledgements">*Both authors contributed equally to this research.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The proliferation of deep neural networks fueled by extensive datasets has significantly enhanced mobile and IoT applications, ranging from remote physiological signal monitoringÂ <cite class="ltx_cite ltx_citemacro_citep">(Park etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2020</a>; Shao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>, human action recognitionÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>; Jiang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>, and audio recognitionÂ <cite class="ltx_cite ltx_citemacro_citep">(Coucke etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2018</a>; Yang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2023</a>)</cite>. Smart devices such as smartphones, smartwatches, and wearable sensors capture high-fidelity data, enabling robust machine learning model developmentÂ <cite class="ltx_cite ltx_citemacro_citep">(Stisen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2015</a>; Pham etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite>. These advancements support diverse real-world applicationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Park etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>; Yun etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2024</a>; Huynh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. However, client data often contains sensitive information, and the high communication costs of transmitting raw data to a central server pose challenges for creating comprehensive large-scale datasets suitable for centralized deep model training.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, federated learning has emerged as a promising approach to building effective machine learning models without explicitly sharing private data. Instead, it shares locally trained model parameters, indirectly transferring local knowledge in a distributed and collaborative mannerÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>. Naturally, federated learning has catalyzed advancements in on-device trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Gim and Ko, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2022</a>; Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2022</a>)</cite>, enabling a diverse array of real-world applications, including healthcare, smart cities, and autonomous drivingÂ <cite class="ltx_cite ltx_citemacro_citep">(Silva etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>; Ouyang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite extensive research in federated learning, most prior works assume a homogeneous model architecture to be shared across all clientsÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021b</a>)</cite>. This assumption overlooks the diverse range of devices participating in federated learning (i.e., device heterogeneity), especially in practical cross-platform settings. Devices can vary significantly in processing power, memory capacity, and available energyÂ <cite class="ltx_cite ltx_citemacro_citep">(Park and Ko, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>. This issue of computational resource heterogeneity is even more pronounced in mobile and embedded applications, where clients use devices with vastly different computational capabilities, from smartphones to embedded IoT platformsÂ <cite class="ltx_cite ltx_citemacro_citep">(Cho etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Without considering client-side computational resource heterogeneity, model design is constrained to the capabilities of the least powerful client, leading to suboptimal performance. Furthermore, clients with weaker computing capabilities may struggle to meet training deadlines, raising fairness concerns when excluded due to timeoutsÂ <cite class="ltx_cite ltx_citemacro_citep">(Shin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2022</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020a</a>)</cite> or can delay the overall federated learning process as the server waits for clients lagging behindÂ <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Park and Ko, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>. Therefore, addressing device heterogeneity in federated learning is crucial to balance effective model building and achieve efficient training timesÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020b</a>; Diao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. To maximize the distributed computing resources, a federated learning framework must adapt its model capacity to allow all clients to participate in the federated learning process with a suitable modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Alam etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>; Ilhan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Previous solutions targeting device heterogeneity in federated learning have leveraged techniques such as knowledge distillationÂ <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; He etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, local model pruningÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib36" title="" class="ltx_ref">2020b</a>)</cite>, and adaptive model scalingÂ <cite class="ltx_cite ltx_citemacro_citep">(Ilhan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Diao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. In knowledge distillation, the server is required to maintain a well-balanced public dataset, which is not always easy to secure. Local model pruning relaxes such constraints but requires full model training at clients for model compression. Finally, while adaptive model splitting can be a viable approach, we later show that the heterogeneity of clients leads to information imbalance when model parameters are gathered at the serverÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2024</a>; Yao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This work builds on adaptive model scaling, where neural networks can be split into varying depths and clients can utilize differently sized subnetworks of the original model based on their computational capabilities. To achieve this, we adopt a <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">multi-exit model architecture</span>, integrating intermediate exits to split a larger model depth-wise. This ensures that heterogeneous clients can align their feature spaces across layers, facilitating consistent context sharing during parameter aggregation (Sec.Â <a href="#S4.SS2" title="4.2. Multi-Exit Neural Network Architecture â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">With heterogeneous platforms utilizing multi-exit architectures, all clients execute the model up to the first exit, while only a subset of powerful clients proceed to the final. This results in varying sample availability (i.e., information disparity) across different exits at the server, particularly affecting deeper layers with fewer samples for aggregation. Our approach mitigates this challenge using <span id="S1.p7.1.1" class="ltx_text ltx_font_italic">hypernetworks</span> to infer and generate weights for layers lacking sufficient samples, enhancing model aggregation effectiveness. However, despite hypernetworks operating on the server, their resource requirements are extremely high; resulting in large memory usage and long computation times. To address this, we propose a <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">Low-Rank Factorization (LRF)-based compression scheme for neural network parameters</span>, aimed at optimizing hypernetwork operations for federated learning (Sec.Â <a href="#S4.SS3" title="4.3. Efficient Hypernetwork Design for Multi-exit Architectures â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We integrate these techniques as a heterogeneous federated learning framework, <span id="S1.p8.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>, and present extensive evaluations via GPU-based emulations and a real-world testbed of heterogeneous embedded platforms. <span id="S1.p8.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> outperforms all state-of-the-art baselines and our testbed results show that it improves the model accuracy by <span id="S1.p8.1.4" class="ltx_text ltx_font_bold">5.13%</span> compared to FedAvg. Furthermore, with its LRF-based optimization, <span id="S1.p8.1.5" class="ltx_text ltx_font_italic">HypeMeFed</span> reduces the hypernetworkâ€™s memory requirement by <span id="S1.p8.1.6" class="ltx_text ltx_font_bold">98.22%</span> and achieves <span id="S1.p8.1.1" class="ltx_text ltx_font_bold">1.86<math id="S1.p8.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p8.1.1.m1.1a"><mo id="S1.p8.1.1.m1.1.1" xref="S1.p8.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.1.m1.1b"><times id="S1.p8.1.1.m1.1.1.cmml" xref="S1.p8.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.1.m1.1c">\times</annotation></semantics></math></span> faster hypernetwork operations compared to a naive approach. Overall, our results suggest that <span id="S1.p8.1.7" class="ltx_text ltx_font_italic">HypeMeFed</span> is an effective solution to enable federated learning for heterogeneous clients.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The contributions of this work are three-fold as follows:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We identify practical challenges in federated learning for heterogeneous clients and propose <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>. <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> integrates a multi-exit neural network architecture with hypernetwork-based model weight generation to offer a practical solution for supporting federated learning on systems consisting of heterogeneous clients.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">While hypernetworks offer an environment to generate model weights for portions of the network that lack client-sourced parameters, their naive use results in significant computational and memory overhead. We propose an LRF-based approach to compress and optimize its operations to a practically applicable scale, making hypernetworks suitable for federated learning scenarios.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We perform extensive evaluations using both GPU-based emulations and embedded platform implementations to show the effectiveness of <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>. Our results suggest that <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> is a practical solution for enabling federated learning over clients with heterogeneous capabilities.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work and Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We begin by positioning our work within the current literature and discuss the foundational concepts and methodologies relevant to our study.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Device Heterogeneity in Federated Learning.</span> While federated learning research has mostly focused on homogeneous models with little consideration of client-side computing capabilitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Diao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, supporting federated learning for heterogeneous clients is crucial. It allows for tailored models that adapt to diverse computational capabilities, data distributions, and privacy requirements of various devicesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2022</a>; Bonawitz etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020a</a>)</cite>. Unlike homogeneous approaches, heterogeneous federated learning leverages each clientâ€™s unique capabilities, ensuring that even resource-constrained devices contribute effectively to enhance model robustness, maximize the potential of collective data, and lead to better generalization across tasks and environmentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">One early approach to achieve device heterogeneity in federated learning is federated distillation (FD)Â <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Seo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>, which leverages knowledge distillationÂ <cite class="ltx_cite ltx_citemacro_citep">(Hinton etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2015</a>)</cite>. Instead of sharing model weights, FD aggregates class scores from a large public dataset computed locally at each client. FD accommodates diverse client models but relies heavily on the quality and availability of public datasets and requires this data for all clients, posing challenges for resource-limited platforms.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">An alternative approach involves pruning local models based on the Lottery Ticket hypothesisÂ <cite class="ltx_cite ltx_citemacro_citep">(Frankle and Carbin, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>. In LotteryFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020b</a>)</cite> and HermesÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021a</a>)</cite>, each client learns a subnetwork of the global model (i.e., lottery ticket network) through iterative pruning, rewinding, and training, exchanging only sparse pruned parameters. However, this iterative process is practically challenging for resource-limited clients, as they must initially train the heavy full model to determine which weights to prune. This requires a large number of rounds for model convergence and can cause training delays.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">A different approach reduces model capacity by scaling width, depth, or both, aligning with individual client conditions based on available resources. In width-based scaling (e.g., HeteroFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Diao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, FedRolexÂ <cite class="ltx_cite ltx_citemacro_citep">(Alam etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>), clients subsample different channel parts per layer for server aggregation. HeteroFL applies static subsampling across global rounds, while FedRolex employs a rolling method to distribute channel parameters evenly among clients by rotating the sampling frame.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In contrast, depth-based scaling methods like DepthFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Kim etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, REEFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024a</a>)</cite>, and the hybrid method ScaleFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Ilhan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> adjust models in depth using a multi-exit architecture. ScaleFL further improves shallower models with self-distillation from deeper models.
Despite their superior performance compared to width-based methods, they suffer from information disparity between shallower and deeper layers. With clients participating in aggregation using varying model depths, shallower layers receive more samples, amplifying disparity in information, representation capacity, and contribution to the global model. This information disparity not only harms the generalizability of the global model but also undermines self-distillation, as students (shallower models) learn with more data than teachers (deeper models), which is atypical.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">This motivates <span id="S2.p7.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> to propose an alternative method to mitigate information disparity. Additionally, existing depth-scaling methods lack justification for multi-exit architectures and comprehensive analysis of information disparities. This work provides an in-depth analysis in SectionÂ <a href="#S3.SS1" title="3.1. Challenges and Potential Solutions â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and designs a federated framework based on our findings.</p>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.1" class="ltx_p"><span id="S2.p8.1.1" class="ltx_text ltx_font_bold">Multi-Exit Network Architecture.</span> As the name implies, multi-exit architectures, inject multiple (early) exit points within a neural network to enhance inference latency by enabling adaptive inference and eliminating unnecessary operationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Laskaridis etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024a</a>)</cite>. Teerapittayanon et al. introduced BranchNet, a pioneering multi-exit deep neural network featuring multiple classifiers integrated within the baseline model to improve computational latencyÂ <cite class="ltx_cite ltx_citemacro_citep">(Teerapittayanon etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2016</a>)</cite>. Given that mobile and embedded devices often experience lagging inference due to resource limitations, multi-exit architectures have been often applied in various real-world sensing applications within mobile and embedded computing environmentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Kouris etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>; Leontiadis etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>; Kim etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.p9" class="ltx_para ltx_noindent">
<p id="S2.p9.1" class="ltx_p"><span id="S2.p9.1.1" class="ltx_text ltx_font_bold">Hypernetworks.</span> Hypernetworks are neural networks designed to generate weights for another neural networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Ha etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>, offering flexibility in applications like computer visionÂ <cite class="ltx_cite ltx_citemacro_citep">(Jia etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2016</a>; Klocek etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> and 3D scene representationÂ <cite class="ltx_cite ltx_citemacro_citep">(Littwin and Wolf, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Sitzmann etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2020</a>)</cite>. In federated learning, hypernetworks are used to enhance model initialization and personalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Shamsian etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021</a>)</cite> and to determine aggregation ratios for model personalizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">Our work proposes a novel approach by designing and leveraging efficient hypernetworks specifically to address challenges posed by heterogeneous federated learning scenarios. Unlike previous works that exploit hypernetworks for general model initialization or focus on model personalization, our approach aims to dynamically generate missing model parameters to address the information disparity problem in heterogeneous federated learning. Furthermore, we introduce a model compression-based hypernetwork approach to minimize the computational and memory overhead associated with hypernetwork operations.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Challenges and Preliminary Study</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The recently evolving paradigm of distributed training via federated learning offers a novel approach to developing effective models by utilizing data collected on end devices such as IoT and mobile platforms. Federated learning method enables the system to fully exploit data from many clients while maintaining a privacy-aware environment by ensuring that raw data remains on the clientâ€™s devices.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To fully leverage this distributed data, it is crucial to maximize the number of clients participating in the federated learning process. However, studies highlight that opportunities for participation are often limited due to various factors. A major obstacle is the heterogeneity of computing resources, which can limit both model performance and client participation. It is unrealistic to assume that all clients have similar computing resources and can train the same model, as clients with limited resources (e.g., CPU, RAM, GPU) may be unable to join the training process, despite possessing valuable data. Conversely, if the server employs a lightweight model to accommodate as many clients as possible, the performance of clients with ample resources can be constrained by the limited model capacity. Additionally, less capable clients may require extensive training time, potentially exceeding the serverâ€™s deadline and preventing their participation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Challenges and Potential Solutions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To accommodate the diverse range of computing resources in clients with heterogeneous capabilities, a server can configure and allocate models with different levels of capacity (i.e., number of parameters) suitable for each client. A model with a relatively small number of parameters generally requires less memory, computation, and latency for both inference and trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Horvath etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. However, doing so in federated learning presents a challenge: the server must aggregate knowledge (i.e., parameters) from locally trained models with non-identical architectures. One feasible approach is to exploit subnetwork architectures of the full neural network by splitting the original network in a depth-wise manner (i.e., layer-level split). While promising, given that heterogeneous models maintain some level of architectural commonness, we identify two notable issues with the naive approach of simply dividing the network in a depth-wise manner.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">[Issue 1] Misaligned Feature Space:</span>
The first issue with depth-wise network splits is the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">misaligned feature space</span>. Layers closer to the input focus on local features, while layers closer to the output capture global features for tasks like classification. When devices with varying capabilities hold different â€œsplitsâ€ of the network, the feature spaces for the same layer can differ due to their relative positions in subnetworks. For example, a layer might act as an intermediate layer in the full model but as a final layer in a subnetwork. This misalignment can significantly harm model performance in federated learning, as also reported by Luo et al.Â <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">[Solution 1] Multi-exit Architecture:</span>
To better align the representation space in each subnetwork, we leverage a <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_italic">multi-exit architecture</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Teerapittayanon etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2016</a>; Laskaridis etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>. This approach adds intermediate exit points and enables each depth-wise split subnetwork to independently learn global features while also exploiting them as local features for subsequent subnetworks. With a multi-exit architecture, we can improve feature alignment in models with different layer depths. This ensures that each layer contributes to both local and global feature learning, enhancing the overall performance and consistency of the federated learning process. We will discuss more about the multi-exit architecture and its implementation in SectionÂ <a href="#S4" title="4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="300" height="176" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Parameter aggregation in heterogeneous federated learning showing information disparity.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F1.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="368" height="139" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Hypernetwork-based weight generation.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F2.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">[Issue 2] Per-layer information disparity:</span>
FigureÂ <a href="#S3.F1" title="Figure 1 â€£ 3.1. Challenges and Potential Solutions â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> visualizes the operations of a naive parameter aggregation scheme at the federated learning server for clients operating heterogeneous models. Unlike the earlier layers (closer to the input, depicted in pink) shared across all subnetworks, the number of gathered parameters for the deeper layers can be limited (green and blue). This occurs because all clients execute the earlier (shallow) layers of the model, but only clients with sufficient resources execute the deeper layers. Consequently, when aggregated at the server, the quantities of gathered weights differ for each network layer, depending on how subnetworks are distributed in the network. We define this <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">per-layer information disparity</span> as the second issue in realizing depth-wise network splits in federated learning. This issue, where specific layers in the neural network receive less information to aggregate, hinders proper model convergence and generalization.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.2" class="ltx_p"><span id="S3.SS1.p5.2.1" class="ltx_text ltx_font_bold">[Solution 2] Hypernetwork-based network parameter generation:</span> To address per-layer information disparity, we explore leveraging a <span id="S3.SS1.p5.2.2" class="ltx_text ltx_font_italic">hypernetwork for network parameter generation</span>. Hypernetworks use a set of network parameters to generate the weights for a different network. FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3.1. Challenges and Potential Solutions â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how a hypernetwork architecture can be used for federated learning network weight generation. Specifically, the hypernetwork takes the trained parameters from the shallow layers and generates the necessary parameters for deeper layers to facilitate effective aggregation. By doing so, we can generate weights for the <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="n^{th}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><msup id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">n</mi><mrow id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml"><mi id="S3.SS1.p5.1.m1.1.1.3.2" xref="S3.SS1.p5.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.1.1.3.1" xref="S3.SS1.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p5.1.m1.1.1.3.3" xref="S3.SS1.p5.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">ğ‘›</ci><apply id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3"><times id="S3.SS1.p5.1.m1.1.1.3.1.cmml" xref="S3.SS1.p5.1.m1.1.1.3.1"></times><ci id="S3.SS1.p5.1.m1.1.1.3.2.cmml" xref="S3.SS1.p5.1.m1.1.1.3.2">ğ‘¡</ci><ci id="S3.SS1.p5.1.m1.1.1.3.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">n^{th}</annotation></semantics></math> layer using the <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="n-1^{th}" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">n</mi><mo id="S3.SS1.p5.2.m2.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.cmml">âˆ’</mo><msup id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml"><mn id="S3.SS1.p5.2.m2.1.1.3.2" xref="S3.SS1.p5.2.m2.1.1.3.2.cmml">1</mn><mrow id="S3.SS1.p5.2.m2.1.1.3.3" xref="S3.SS1.p5.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.p5.2.m2.1.1.3.3.2" xref="S3.SS1.p5.2.m2.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.2.m2.1.1.3.3.1" xref="S3.SS1.p5.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p5.2.m2.1.1.3.3.3" xref="S3.SS1.p5.2.m2.1.1.3.3.3.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><minus id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1"></minus><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">ğ‘›</ci><apply id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.3.1.cmml" xref="S3.SS1.p5.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p5.2.m2.1.1.3.2.cmml" xref="S3.SS1.p5.2.m2.1.1.3.2">1</cn><apply id="S3.SS1.p5.2.m2.1.1.3.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3.3"><times id="S3.SS1.p5.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p5.2.m2.1.1.3.3.1"></times><ci id="S3.SS1.p5.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p5.2.m2.1.1.3.3.2">ğ‘¡</ci><ci id="S3.SS1.p5.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3.3.3">â„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">n-1^{th}</annotation></semantics></math> layer weights. This resolves the information disparity issue occurring at the deep layers of a federated learning model. We emphasize that the hypernetwork is trained on the server using model parameters obtained from clients.
We will detail the design and optimization of hypernetworks in SectionÂ <a href="#S4" title="4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Preliminary Study</h3>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F3.sf1.2" class="ltx_block">
<img src="/html/2407.03086/assets/x3.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="143" height="80" alt="Refer to caption"><span id="S3.F3.sf1.2.1" class="ltx_ERROR undefined">\Description</span>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F3.sf1.4.2" class="ltx_text" style="font-size:80%;">Accuracy performance comparisons for different federated learning configurations and the impact of our potential solutions</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F3.sf2.2" class="ltx_block">
<img src="/html/2407.03086/assets/x4.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="143" height="71" alt="Refer to caption"><span id="S3.F3.sf2.2.1" class="ltx_ERROR undefined">\Description</span>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F3.sf2.4.2" class="ltx_text" style="font-size:80%;">CKA values measured at the first layer of a one-layer split (S), two-layer split (M) and full model (L), with (O) and without (X) the multi-exit architecture</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F3.sf3.2" class="ltx_block">
<img src="/html/2407.03086/assets/x5.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="143" height="68" alt="Refer to caption"><span id="S3.F3.sf3.2.1" class="ltx_ERROR undefined">\Description</span>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F3.sf3.4.2" class="ltx_text" style="font-size:80%;">Density distribution of trained (ground truth), hypernetwork-generated, and randomly initialized weight values</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Accuracy, CKA, and weight value distribution plots for different federated learning configurations and potential solutions proposed in this work.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F3.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2407.03086/assets/x6.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Visualized samples of trained, generated, and randomly initialized parameters.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We now present a preliminary study designed to examine the potential of exploiting our proposed solutions for heterogeneous federated learning scenarios.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For this study, we utilize the FashionMNIST datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2017</a>)</cite>, independently and identically distributed (i.e., IID) across 50 clients, with 10 randomly selected clients participating in each federated learning round. We adopt an IID setup to isolate the impact of our proposed solution from data distribution effects. Our experiments focus on three subnetwork architectures of varying depths derived from a full model consisting of three convolutional layers. Specifically, 17, 17, and 16 clients among the 50 operate 1, 2, and 3-layered models, respectively. We compare against three baselines: (i) all 50 clients using a lightweight single-layer model (â€œSmallâ€ in Fig.Â <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (a)), (ii) only capable clients using the full three-layer model (â€œLargeâ€), and (iii) a non-practical but ideal scenario where all clients use the full three-layer model (â€œLarge (All)â€).</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (a) shows the accuracy results for these configurations. As expected, the â€œLarge (All)â€ scenario, where all 50 clients use the deepest model, achieves the highest performance. However, in the more practical â€œLargeâ€ scenario where only a subset (16/50) of clients can use deeper architectures and are the only ones participating, the accuracy remains low and fails to converge, performing worse than the scenario where all clients use the single-layer model (â€œSmallâ€). These results highlight the importance of optimizing model architectures to accommodate client diversity and maximize client engagement in federated learning processes.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (a) we also plot four additional cases where our proposed solutions in SectionÂ <a href="#S3.SS1" title="3.1. Challenges and Potential Solutions â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> were applied in different combinations. As we allocate clients with their largest-adoptable model, we text cases where neither the multi-exit approach nor the hypernetwork is used (â€œME X/HN Xâ€), cases where one of the two is used (â€œME O/HN Xâ€ and â€œME X/HN Oâ€), and a final case there both approaches are integrated into the federated learning process (â€œME O/HN Oâ€). We use these plots to examine the feasibility of applying our solutions for federated learning scenarios. The results in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (a) suggest that in the absence of the multi-exit architecture, convergence issues arise due to misaligned feature spaces, while the system faces issues due to per-layer information disparity when without the hypernetwork-based weight generations. As a result, the case with both multi-exit and hypernetwork (â€œME O/HN Oâ€) shows the highest performance, close to that of an ideal case where all clients exploit the full model (â€œLarge (All)â€). These results emphasize the need for both the multi-exit and hypernetwork approaches.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (b) analyzes feature representation similarity across clients using the Centered Kernel Alignment (CKA) metricÂ <cite class="ltx_cite ltx_citemacro_citep">(Kornblith etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. CKA measures similarity between feature spaces of neural networks, where values closer to 1 indicate higher similarity. Effective federated learning models typically show closely aligned feature spacesÂ <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>. We compute CKA using the first layerâ€™s feature space as a baseline for three configurations: single-layer split (â€œSâ€), two-layer split (â€œMâ€), and the full model (â€œLâ€). Additionally, we examine these configurations with and without multi-exit layers, denoted by â€œOâ€ and â€œXâ€ respectively. For example, â€œS-L (X)â€ compares the single-layer and three-layer models without multi-exits.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Without multi-exit layers, the one-layer (S) and two-layer (M) models differ significantly from the full model (L), with CKA values of 0.6761 and 0.6764, respectively. However, incorporating multi-exit architecture improves feature alignment noticeably, yielding CKA values of 0.7812 and 0.9407 for these configurations. Comparisons like â€œM-L (X-O)â€ and â€œM-L (O-X)â€ in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (b) show considerable differences in feature alignment depending on the presence of multi-exit layers, with CKA values of 0.7165 and 0.8343, respectively. These findings highlight the effectiveness of multi-exit architectures in depth-wise split networks for federated learning, enhancing feature space alignment and potentially boosting overall model performance.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">We evaluate the effectiveness of using a hypernetwork by comparing its generated model weights with the actual weights of a three-layer model (ground truth). The full model was trained on 50 clients, with 40 used for hypernetwork training. We then generated the last layer parameters for the remaining 10 models based on their second-layer parameters. For comparison, we also include randomly generated weights, plotting their distributions in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>Â (c). The results demonstrate that hypernetwork-based weight generation accurately estimates network weights comparable to the ground truth.
Furthermore, by visualizing the network weights as a matrix and comparing their differences with the ground truth in FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.2. Preliminary Study â€£ 3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can see that the difference in network weight estimation is minimal and that the hypernetwork is capable of intelligently inferring network weights.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">Overall, our preliminary results suggest the potential effectiveness of leveraging the multi-exit architecture and hypernetworks for heterogeneous federated learning. Using these results as empirical evidence, we discuss the details of our proposed <span id="S3.SS2.p8.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> design in the following section.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>System Design</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Overview of <span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>
</h3>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x7.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="134" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Overview of <span id="S4.F5.3.1" class="ltx_text ltx_font_italic">HypeMeFed</span>. <span id="S4.F5.4.2" class="ltx_text ltx_font_italic">HypeMeFed</span> leverages heterogeneous models generated from a multi-exit network architecture with hypernetworks to resolve the per-layer information disparity issue. Best viewed in color.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F5.5" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our work proposes <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, a federated learning framework for networked systems consisting of clients with heterogeneous computational capabilities. Specifically, <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> aims to optimize federated learning across heterogeneous clients and address the challenges detailed in SectionÂ <a href="#S3" title="3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> by integrating a multi-exit architecture and employing a hypernetwork-based neural network parameter generation scheme. This ensures effective client participation in the federated learning process regardless of the clientsâ€™ varying computational capabilities. FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4.1. Overview of HypeMeFed â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides an overview of <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s design, highlighting its operational process in a single federated learning round:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.2" class="ltx_p">The server distributes global model parameters to clients based on their computing resources. The <span id="S4.I1.i1.p1.2.1" class="ltx_text ltx_font_italic">full</span> global model is configured with multiple exit layers organized in a depth-wise manner ( <svg id="S4.I1.i1.p1.1.pic1" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i1.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>). Clients with higher computational resources receive the full model, while the ones that are limited receive a smaller subset of the model split at earlier exit layers ( <svg id="S4.I1.i1.p1.2.pic2" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i1.p1.2.pic2.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>). (c.f., Sec.Â <a href="#S4.SS2" title="4.2. Multi-Exit Neural Network Architecture â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Clients train their models with local data ( <svg id="S4.I1.i2.p1.1.pic1" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i2.p1.1.pic1.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>). <span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> retains the standard local training procedure unchanged. The key difference lies in clients using heterogeneous models and training them with a joint loss function that incorporates predictions from all available exits.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">After local training, each client sends back its trained model parameters to the server ( <svg id="S4.I1.i3.p1.1.pic1" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i3.p1.1.pic1.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>). Clients transmit only the parameters relevant to their assigned model configuration, ensuring efficient parameter aggregation.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.2" class="ltx_p">The server trains its hypernetworks by leveraging client data that contain sufficient layer information ( <svg id="S4.I1.i4.p1.1.pic1" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i4.p1.1.pic1.1.1.1.1.1" class="ltx_text">5</span></foreignObject></g></g></svg>). The hypernetwork is then used to predict and generate network weights (<math id="S4.I1.i4.p1.2.m1.1" class="ltx_Math" alttext="\hat{\theta}" display="inline"><semantics id="S4.I1.i4.p1.2.m1.1a"><mover accent="true" id="S4.I1.i4.p1.2.m1.1.1" xref="S4.I1.i4.p1.2.m1.1.1.cmml"><mi id="S4.I1.i4.p1.2.m1.1.1.2" xref="S4.I1.i4.p1.2.m1.1.1.2.cmml">Î¸</mi><mo id="S4.I1.i4.p1.2.m1.1.1.1" xref="S4.I1.i4.p1.2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.2.m1.1b"><apply id="S4.I1.i4.p1.2.m1.1.1.cmml" xref="S4.I1.i4.p1.2.m1.1.1"><ci id="S4.I1.i4.p1.2.m1.1.1.1.cmml" xref="S4.I1.i4.p1.2.m1.1.1.1">^</ci><ci id="S4.I1.i4.p1.2.m1.1.1.2.cmml" xref="S4.I1.i4.p1.2.m1.1.1.2">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.2.m1.1c">\hat{\theta}</annotation></semantics></math> in Fig.Â <a href="#S4.F5" title="Figure 5 â€£ 4.1. Overview of HypeMeFed â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) for layers suffering from the information disparity issue, particularly the deeper layers or exits of the model. (c.f., SecÂ <a href="#S4.SS3" title="4.3. Efficient Hypernetwork Design for Multi-exit Architectures â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> and Sec.Â <a href="#S4.SS4" title="4.4. Hypernetwork Implementations â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.2" class="ltx_p">Finally, the server aggregates the parameters to generate an updated global model based on the client-sent data and the hypernetwork-generated parameters ( <svg id="S4.I1.i5.p1.1.pic1" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i5.p1.1.pic1.1.1.1.1.1" class="ltx_text">6</span></foreignObject></g></g></svg>,  <svg id="S4.I1.i5.p1.2.pic2" class="ltx_picture" height="14.95" overflow="visible" version="1.1" width="14.95"><g transform="translate(0,14.95) matrix(1 0 0 -1 0 0) translate(7.47,0) translate(0,7.47)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 7.2 0 C 7.2 3.97 3.97 7.2 0 7.2 C -3.97 7.2 -7.2 3.97 -7.2 0 C -7.2 -3.97 -3.97 -7.2 0 -7.2 C 3.97 -7.2 7.2 -3.97 7.2 0 Z M 0 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.I1.i5.p1.2.pic2.1.1.1.1.1" class="ltx_text">7</span></foreignObject></g></g></svg>).</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">This structured approach ensures that <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> can utilize both powerful and weaker devices, enhancing overall federated learning performance and inclusivity. The following sections detail <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s core components: the multi-exit network architecture and weight generation hypernetworks.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Multi-Exit Neural Network Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The global model in <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> incorporates multiple intermediate classification layers known as â€œexit layers,â€ distributed in a depth-wise manner within the neural network. Each exit layer, split in a depth-wise manner, functions as a subnetwork of the full model, enabling effective aggregation of client models and supporting heterogeneous federated learning across devices with varying computing capabilities.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The placement and number of exit layers in <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> are key design choices. In this work, we use three exit layers positioned at one-third, two-thirds, and the end of the federated learning model to exploit a range of features that the model embedsÂ <cite class="ltx_cite ltx_citemacro_citep">(Park etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>. This setup allows the categorization of clients into three groups based on their computational capabilities: the first group, with limited resources, uses a subnetwork up to the first exit layer; the second group, with moderate resources, uses up to the second exit; and the third group, with ample resources, uses the full network with all three exits. During local training, clients compute outputs for all exit layers included in their allocated model. For instance, a client with sufficient resources generates predictions from all three exit layers and a resource limited client will compute only up to the first. Each layerâ€™s output contributes to the overall loss, summing the losses from all exitsâ€”a standard practice in multi-exit neural networksÂ <cite class="ltx_cite ltx_citemacro_citep">(Laskaridis etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Teerapittayanon etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2016</a>; Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2024a</a>)</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">This multi-exit architecture in <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> offers two key advantages. First, it computes losses at both early and final layers, mitigating the vanishing gradient problem and improving training dynamicsÂ <cite class="ltx_cite ltx_citemacro_citep">(Teerapittayanon etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2016</a>)</cite>. Second, it addresses feature space misalignment from depth-wise model splitting (SectionÂ <a href="#S3" title="3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), ensuring that early layers capture both local and global features, promoting better feature alignment and convergence in federated learning.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> facilitates seamless aggregation of trained model parameters despite heterogeneous network architectures by training and sharing all exit layer parameters. Specifically, <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> can easily be integrated with various federated learning schemes like FedAvgÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>, FedProxÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020a</a>)</cite>, and SCAFFOLDÂ <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite> without modifying the local training and aggregation process. In this work, <span id="S4.SS2.p4.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> adopts the FedAvg approach, a straightforward method for averaging model parameters during aggregation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Efficient Hypernetwork Design for Multi-exit Architectures</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2407.03086/assets/x8.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Details on hypernetwork operations.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Despite its benefits, as discussed in SectionÂ <a href="#S3" title="3. Challenges and Preliminary Study â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, depth-wise model splitting suffers from the information disparity issue at deeper layers. To mitigate this, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> employs a hypernetwork to generate missing weight information. FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.3. Efficient Hypernetwork Design for Multi-exit Architectures â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the workflow of this hypernetwork.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The rationale behind using hypernetworks for parameter generation in deeper layers stems from the neural networksâ€™ nature as directed acyclic graphs, where computations in later layers depend on results from earlier computations. We hypothesize a nonlinear relationship between parameters of preceding and subsequent layers, which our approach captures through a neural network to predict parameters for deeper exits based on information from earlier exits.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">To integrate hypernetworks in federated learning, a straightforward approach involves vectorizing parameters from preceding layers and feeding them into a multi-layer perceptron (MLP) model. The central server, which aggregates parameters from clients with different computational capabilities, possesses information on both preceding and subsequent layer parameters. This allows the server to utilize parameter information from clients computing up to the second and third exits to train the hypernetwork and understand relationships between adjacent exit parameters. However, neural networks typically entail numerous parameters, demanding substantial memory and time for hypernetwork training. Despite <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s hypernetworks operating on a resource-rich server, a naÃ¯ve hypernetwork design can be computationally intensive and memory-consuming, posing practical challenges in real-world deployments. To tackle this, we propose a more efficient hypernetwork architecture that balances effective parameter generation with the computational constraints of federated learning environments.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> introduces an efficient hypernetwork architecture using a low-rank factorization (LRF) strategy. Instead of vectorizing and reconstructing all model parameters, which is computationally intensive, LRF compresses essential neural network parameters, reducing the computational load and memory requirements. By leveraging the inherent sparsity in neural networks, similar to pruning techniques, this approach is both feasible and efficientÂ <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Frankle and Carbin, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Evci etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.4" class="ltx_p">As FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.3. Efficient Hypernetwork Design for Multi-exit Architectures â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates, <span id="S4.SS3.p5.4.4" class="ltx_text ltx_font_italic">HypeMeFed</span> first reshapes the nearest preceding parameters into a matrix form. For instance, a convolution layer holding parameters with the shape <span id="S4.SS3.p5.3.3" class="ltx_text ltx_font_typewriter">IC<math id="S4.SS3.p5.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p5.1.1.m1.1a"><mo id="S4.SS3.p5.1.1.m1.1.1" xref="S4.SS3.p5.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.1.m1.1b"><times id="S4.SS3.p5.1.1.m1.1.1.cmml" xref="S4.SS3.p5.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.1.m1.1c">\times</annotation></semantics></math>OC<math id="S4.SS3.p5.2.2.m2.1" class="ltx_Math" alttext="\times\\
" display="inline"><semantics id="S4.SS3.p5.2.2.m2.1a"><mo id="S4.SS3.p5.2.2.m2.1.1" xref="S4.SS3.p5.2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.2.m2.1b"><times id="S4.SS3.p5.2.2.m2.1.1.cmml" xref="S4.SS3.p5.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.2.m2.1c">\times\\
</annotation></semantics></math>KS<math id="S4.SS3.p5.3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p5.3.3.m3.1a"><mo id="S4.SS3.p5.3.3.m3.1.1" xref="S4.SS3.p5.3.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.3.3.m3.1b"><times id="S4.SS3.p5.3.3.m3.1.1.cmml" xref="S4.SS3.p5.3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.3.3.m3.1c">\times</annotation></semantics></math>KS</span>, where <span id="S4.SS3.p5.4.5" class="ltx_text ltx_font_typewriter">IC</span> denotes the number of input channels, <span id="S4.SS3.p5.4.6" class="ltx_text ltx_font_typewriter">OC</span> denotes the number of output channels, and <span id="S4.SS3.p5.4.7" class="ltx_text ltx_font_typewriter">KS</span> denotes the kernel size, is reshaped into a matrix <math id="S4.SS3.p5.4.m1.2" class="ltx_Math" alttext="\mathbf{A}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}" display="inline"><semantics id="S4.SS3.p5.4.m1.2a"><mrow id="S4.SS3.p5.4.m1.2.3" xref="S4.SS3.p5.4.m1.2.3.cmml"><mi id="S4.SS3.p5.4.m1.2.3.2" xref="S4.SS3.p5.4.m1.2.3.2.cmml">ğ€</mi><mo id="S4.SS3.p5.4.m1.2.3.1" xref="S4.SS3.p5.4.m1.2.3.1.cmml">âˆˆ</mo><msup id="S4.SS3.p5.4.m1.2.3.3" xref="S4.SS3.p5.4.m1.2.3.3.cmml"><mi id="S4.SS3.p5.4.m1.2.3.3.2" xref="S4.SS3.p5.4.m1.2.3.3.2.cmml">â„</mi><mrow id="S4.SS3.p5.4.m1.2.2.2" xref="S4.SS3.p5.4.m1.2.2.2.cmml"><mrow id="S4.SS3.p5.4.m1.1.1.1.1.1" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p5.4.m1.1.1.1.1.1.2" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p5.4.m1.1.1.1.1.1.1" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p5.4.m1.1.1.1.1.1.1.2" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.2a.cmml">IC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p5.4.m1.1.1.1.1.1.1.1" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p5.4.m1.1.1.1.1.1.1.3" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS3.p5.4.m1.1.1.1.1.1.3" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS3.p5.4.m1.2.2.2.3" xref="S4.SS3.p5.4.m1.2.2.2.3.cmml">Ã—</mo><mrow id="S4.SS3.p5.4.m1.2.2.2.2.1" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.SS3.p5.4.m1.2.2.2.2.1.2" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS3.p5.4.m1.2.2.2.2.1.1" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p5.4.m1.2.2.2.2.1.1.2" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.2a.cmml">OC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p5.4.m1.2.2.2.2.1.1.1" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p5.4.m1.2.2.2.2.1.1.3" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.3a.cmml">KS</mtext></mrow><mo stretchy="false" id="S4.SS3.p5.4.m1.2.2.2.2.1.3" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.4.m1.2b"><apply id="S4.SS3.p5.4.m1.2.3.cmml" xref="S4.SS3.p5.4.m1.2.3"><in id="S4.SS3.p5.4.m1.2.3.1.cmml" xref="S4.SS3.p5.4.m1.2.3.1"></in><ci id="S4.SS3.p5.4.m1.2.3.2.cmml" xref="S4.SS3.p5.4.m1.2.3.2">ğ€</ci><apply id="S4.SS3.p5.4.m1.2.3.3.cmml" xref="S4.SS3.p5.4.m1.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.p5.4.m1.2.3.3.1.cmml" xref="S4.SS3.p5.4.m1.2.3.3">superscript</csymbol><ci id="S4.SS3.p5.4.m1.2.3.3.2.cmml" xref="S4.SS3.p5.4.m1.2.3.3.2">â„</ci><apply id="S4.SS3.p5.4.m1.2.2.2.cmml" xref="S4.SS3.p5.4.m1.2.2.2"><times id="S4.SS3.p5.4.m1.2.2.2.3.cmml" xref="S4.SS3.p5.4.m1.2.2.2.3"></times><apply id="S4.SS3.p5.4.m1.1.1.1.1.1.1.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1"><times id="S4.SS3.p5.4.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p5.4.m1.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p5.4.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.2">IC</mtext></ci><ci id="S4.SS3.p5.4.m1.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p5.4.m1.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p5.4.m1.1.1.1.1.1.1.3">KS</mtext></ci></apply><apply id="S4.SS3.p5.4.m1.2.2.2.2.1.1.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1"><times id="S4.SS3.p5.4.m1.2.2.2.2.1.1.1.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.1"></times><ci id="S4.SS3.p5.4.m1.2.2.2.2.1.1.2a.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p5.4.m1.2.2.2.2.1.1.2.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.2">OC</mtext></ci><ci id="S4.SS3.p5.4.m1.2.2.2.2.1.1.3a.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p5.4.m1.2.2.2.2.1.1.3.cmml" xref="S4.SS3.p5.4.m1.2.2.2.2.1.1.3">KS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.4.m1.2c">\mathbf{A}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.11" class="ltx_p">To initiate the compression of this high-dimensional matrix, <span id="S4.SS3.p6.11.1" class="ltx_text ltx_font_italic">HypeMeFed</span> decomposes the matrix-shaped parameters via Singular Value Decomposition (SVD), resulting in three matrices (<math id="S4.SS3.p6.1.m1.1" class="ltx_Math" alttext="\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}" display="inline"><semantics id="S4.SS3.p6.1.m1.1a"><mrow id="S4.SS3.p6.1.m1.1.1" xref="S4.SS3.p6.1.m1.1.1.cmml"><mi id="S4.SS3.p6.1.m1.1.1.2" xref="S4.SS3.p6.1.m1.1.1.2.cmml">ğ€</mi><mo id="S4.SS3.p6.1.m1.1.1.1" xref="S4.SS3.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS3.p6.1.m1.1.1.3" xref="S4.SS3.p6.1.m1.1.1.3.cmml"><mi id="S4.SS3.p6.1.m1.1.1.3.2" xref="S4.SS3.p6.1.m1.1.1.3.2.cmml">ğ”</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p6.1.m1.1.1.3.1" xref="S4.SS3.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS3.p6.1.m1.1.1.3.3" xref="S4.SS3.p6.1.m1.1.1.3.3.cmml">ğšº</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p6.1.m1.1.1.3.1a" xref="S4.SS3.p6.1.m1.1.1.3.1.cmml">â€‹</mo><msup id="S4.SS3.p6.1.m1.1.1.3.4" xref="S4.SS3.p6.1.m1.1.1.3.4.cmml"><mi id="S4.SS3.p6.1.m1.1.1.3.4.2" xref="S4.SS3.p6.1.m1.1.1.3.4.2.cmml">ğ•</mi><mo id="S4.SS3.p6.1.m1.1.1.3.4.3" xref="S4.SS3.p6.1.m1.1.1.3.4.3.cmml">âŠ¤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.1.m1.1b"><apply id="S4.SS3.p6.1.m1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1"><eq id="S4.SS3.p6.1.m1.1.1.1.cmml" xref="S4.SS3.p6.1.m1.1.1.1"></eq><ci id="S4.SS3.p6.1.m1.1.1.2.cmml" xref="S4.SS3.p6.1.m1.1.1.2">ğ€</ci><apply id="S4.SS3.p6.1.m1.1.1.3.cmml" xref="S4.SS3.p6.1.m1.1.1.3"><times id="S4.SS3.p6.1.m1.1.1.3.1.cmml" xref="S4.SS3.p6.1.m1.1.1.3.1"></times><ci id="S4.SS3.p6.1.m1.1.1.3.2.cmml" xref="S4.SS3.p6.1.m1.1.1.3.2">ğ”</ci><ci id="S4.SS3.p6.1.m1.1.1.3.3.cmml" xref="S4.SS3.p6.1.m1.1.1.3.3">ğšº</ci><apply id="S4.SS3.p6.1.m1.1.1.3.4.cmml" xref="S4.SS3.p6.1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S4.SS3.p6.1.m1.1.1.3.4.1.cmml" xref="S4.SS3.p6.1.m1.1.1.3.4">superscript</csymbol><ci id="S4.SS3.p6.1.m1.1.1.3.4.2.cmml" xref="S4.SS3.p6.1.m1.1.1.3.4.2">ğ•</ci><csymbol cd="latexml" id="S4.SS3.p6.1.m1.1.1.3.4.3.cmml" xref="S4.SS3.p6.1.m1.1.1.3.4.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.1.m1.1c">\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}</annotation></semantics></math>), namely the left-singular vectors (<math id="S4.SS3.p6.2.m2.2" class="ltx_Math" alttext="\mathbf{U}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{IC}\times\texttt{KS})}" display="inline"><semantics id="S4.SS3.p6.2.m2.2a"><mrow id="S4.SS3.p6.2.m2.2.3" xref="S4.SS3.p6.2.m2.2.3.cmml"><mi id="S4.SS3.p6.2.m2.2.3.2" xref="S4.SS3.p6.2.m2.2.3.2.cmml">ğ”</mi><mo id="S4.SS3.p6.2.m2.2.3.1" xref="S4.SS3.p6.2.m2.2.3.1.cmml">âˆˆ</mo><msup id="S4.SS3.p6.2.m2.2.3.3" xref="S4.SS3.p6.2.m2.2.3.3.cmml"><mi id="S4.SS3.p6.2.m2.2.3.3.2" xref="S4.SS3.p6.2.m2.2.3.3.2.cmml">â„</mi><mrow id="S4.SS3.p6.2.m2.2.2.2" xref="S4.SS3.p6.2.m2.2.2.2.cmml"><mrow id="S4.SS3.p6.2.m2.1.1.1.1.1" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.2.m2.1.1.1.1.1.2" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p6.2.m2.1.1.1.1.1.1" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.2.m2.1.1.1.1.1.1.2" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.2a.cmml">IC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.2.m2.1.1.1.1.1.1.1" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.2.m2.1.1.1.1.1.1.3" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS3.p6.2.m2.1.1.1.1.1.3" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS3.p6.2.m2.2.2.2.3" xref="S4.SS3.p6.2.m2.2.2.2.3.cmml">Ã—</mo><mrow id="S4.SS3.p6.2.m2.2.2.2.2.1" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.2.m2.2.2.2.2.1.2" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS3.p6.2.m2.2.2.2.2.1.1" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.2.m2.2.2.2.2.1.1.2" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.2a.cmml">IC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.2.m2.2.2.2.2.1.1.1" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.2.m2.2.2.2.2.1.1.3" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.3a.cmml">KS</mtext></mrow><mo stretchy="false" id="S4.SS3.p6.2.m2.2.2.2.2.1.3" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.2.m2.2b"><apply id="S4.SS3.p6.2.m2.2.3.cmml" xref="S4.SS3.p6.2.m2.2.3"><in id="S4.SS3.p6.2.m2.2.3.1.cmml" xref="S4.SS3.p6.2.m2.2.3.1"></in><ci id="S4.SS3.p6.2.m2.2.3.2.cmml" xref="S4.SS3.p6.2.m2.2.3.2">ğ”</ci><apply id="S4.SS3.p6.2.m2.2.3.3.cmml" xref="S4.SS3.p6.2.m2.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.p6.2.m2.2.3.3.1.cmml" xref="S4.SS3.p6.2.m2.2.3.3">superscript</csymbol><ci id="S4.SS3.p6.2.m2.2.3.3.2.cmml" xref="S4.SS3.p6.2.m2.2.3.3.2">â„</ci><apply id="S4.SS3.p6.2.m2.2.2.2.cmml" xref="S4.SS3.p6.2.m2.2.2.2"><times id="S4.SS3.p6.2.m2.2.2.2.3.cmml" xref="S4.SS3.p6.2.m2.2.2.2.3"></times><apply id="S4.SS3.p6.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1"><times id="S4.SS3.p6.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p6.2.m2.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.2">IC</mtext></ci><ci id="S4.SS3.p6.2.m2.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p6.2.m2.1.1.1.1.1.1.3">KS</mtext></ci></apply><apply id="S4.SS3.p6.2.m2.2.2.2.2.1.1.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1"><times id="S4.SS3.p6.2.m2.2.2.2.2.1.1.1.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.1"></times><ci id="S4.SS3.p6.2.m2.2.2.2.2.1.1.2a.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.2.m2.2.2.2.2.1.1.2.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.2">IC</mtext></ci><ci id="S4.SS3.p6.2.m2.2.2.2.2.1.1.3a.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.2.m2.2.2.2.2.1.1.3.cmml" xref="S4.SS3.p6.2.m2.2.2.2.2.1.1.3">KS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.2.m2.2c">\mathbf{U}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{IC}\times\texttt{KS})}</annotation></semantics></math>), right-singular vectors (<math id="S4.SS3.p6.3.m3.2" class="ltx_Math" alttext="\mathbf{V}\in\mathbb{R}^{(\texttt{OC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}" display="inline"><semantics id="S4.SS3.p6.3.m3.2a"><mrow id="S4.SS3.p6.3.m3.2.3" xref="S4.SS3.p6.3.m3.2.3.cmml"><mi id="S4.SS3.p6.3.m3.2.3.2" xref="S4.SS3.p6.3.m3.2.3.2.cmml">ğ•</mi><mo id="S4.SS3.p6.3.m3.2.3.1" xref="S4.SS3.p6.3.m3.2.3.1.cmml">âˆˆ</mo><msup id="S4.SS3.p6.3.m3.2.3.3" xref="S4.SS3.p6.3.m3.2.3.3.cmml"><mi id="S4.SS3.p6.3.m3.2.3.3.2" xref="S4.SS3.p6.3.m3.2.3.3.2.cmml">â„</mi><mrow id="S4.SS3.p6.3.m3.2.2.2" xref="S4.SS3.p6.3.m3.2.2.2.cmml"><mrow id="S4.SS3.p6.3.m3.1.1.1.1.1" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.3.m3.1.1.1.1.1.2" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p6.3.m3.1.1.1.1.1.1" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.3.m3.1.1.1.1.1.1.2" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.2a.cmml">OC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.3.m3.1.1.1.1.1.1.1" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.3.m3.1.1.1.1.1.1.3" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS3.p6.3.m3.1.1.1.1.1.3" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS3.p6.3.m3.2.2.2.3" xref="S4.SS3.p6.3.m3.2.2.2.3.cmml">Ã—</mo><mrow id="S4.SS3.p6.3.m3.2.2.2.2.1" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.3.m3.2.2.2.2.1.2" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS3.p6.3.m3.2.2.2.2.1.1" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.3.m3.2.2.2.2.1.1.2" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.2a.cmml">OC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.3.m3.2.2.2.2.1.1.1" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.3.m3.2.2.2.2.1.1.3" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.3a.cmml">KS</mtext></mrow><mo stretchy="false" id="S4.SS3.p6.3.m3.2.2.2.2.1.3" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.3.m3.2b"><apply id="S4.SS3.p6.3.m3.2.3.cmml" xref="S4.SS3.p6.3.m3.2.3"><in id="S4.SS3.p6.3.m3.2.3.1.cmml" xref="S4.SS3.p6.3.m3.2.3.1"></in><ci id="S4.SS3.p6.3.m3.2.3.2.cmml" xref="S4.SS3.p6.3.m3.2.3.2">ğ•</ci><apply id="S4.SS3.p6.3.m3.2.3.3.cmml" xref="S4.SS3.p6.3.m3.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.p6.3.m3.2.3.3.1.cmml" xref="S4.SS3.p6.3.m3.2.3.3">superscript</csymbol><ci id="S4.SS3.p6.3.m3.2.3.3.2.cmml" xref="S4.SS3.p6.3.m3.2.3.3.2">â„</ci><apply id="S4.SS3.p6.3.m3.2.2.2.cmml" xref="S4.SS3.p6.3.m3.2.2.2"><times id="S4.SS3.p6.3.m3.2.2.2.3.cmml" xref="S4.SS3.p6.3.m3.2.2.2.3"></times><apply id="S4.SS3.p6.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1"><times id="S4.SS3.p6.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p6.3.m3.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.2">OC</mtext></ci><ci id="S4.SS3.p6.3.m3.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.3.m3.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p6.3.m3.1.1.1.1.1.1.3">KS</mtext></ci></apply><apply id="S4.SS3.p6.3.m3.2.2.2.2.1.1.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1"><times id="S4.SS3.p6.3.m3.2.2.2.2.1.1.1.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.1"></times><ci id="S4.SS3.p6.3.m3.2.2.2.2.1.1.2a.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.3.m3.2.2.2.2.1.1.2.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.2">OC</mtext></ci><ci id="S4.SS3.p6.3.m3.2.2.2.2.1.1.3a.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.3.m3.2.2.2.2.1.1.3.cmml" xref="S4.SS3.p6.3.m3.2.2.2.2.1.1.3">KS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.3.m3.2c">\mathbf{V}\in\mathbb{R}^{(\texttt{OC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}</annotation></semantics></math>), and the singular values (<math id="S4.SS3.p6.4.m4.2" class="ltx_Math" alttext="\mathbf{\Sigma}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}" display="inline"><semantics id="S4.SS3.p6.4.m4.2a"><mrow id="S4.SS3.p6.4.m4.2.3" xref="S4.SS3.p6.4.m4.2.3.cmml"><mi id="S4.SS3.p6.4.m4.2.3.2" xref="S4.SS3.p6.4.m4.2.3.2.cmml">ğšº</mi><mo id="S4.SS3.p6.4.m4.2.3.1" xref="S4.SS3.p6.4.m4.2.3.1.cmml">âˆˆ</mo><msup id="S4.SS3.p6.4.m4.2.3.3" xref="S4.SS3.p6.4.m4.2.3.3.cmml"><mi id="S4.SS3.p6.4.m4.2.3.3.2" xref="S4.SS3.p6.4.m4.2.3.3.2.cmml">â„</mi><mrow id="S4.SS3.p6.4.m4.2.2.2" xref="S4.SS3.p6.4.m4.2.2.2.cmml"><mrow id="S4.SS3.p6.4.m4.1.1.1.1.1" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.4.m4.1.1.1.1.1.2" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p6.4.m4.1.1.1.1.1.1" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.4.m4.1.1.1.1.1.1.2" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.2a.cmml">IC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.4.m4.1.1.1.1.1.1.1" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.4.m4.1.1.1.1.1.1.3" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS3.p6.4.m4.1.1.1.1.1.3" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS3.p6.4.m4.2.2.2.3" xref="S4.SS3.p6.4.m4.2.2.2.3.cmml">Ã—</mo><mrow id="S4.SS3.p6.4.m4.2.2.2.2.1" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.4.m4.2.2.2.2.1.2" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS3.p6.4.m4.2.2.2.2.1.1" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.4.m4.2.2.2.2.1.1.2" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.2a.cmml">OC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.4.m4.2.2.2.2.1.1.1" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.4.m4.2.2.2.2.1.1.3" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.3a.cmml">KS</mtext></mrow><mo stretchy="false" id="S4.SS3.p6.4.m4.2.2.2.2.1.3" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.4.m4.2b"><apply id="S4.SS3.p6.4.m4.2.3.cmml" xref="S4.SS3.p6.4.m4.2.3"><in id="S4.SS3.p6.4.m4.2.3.1.cmml" xref="S4.SS3.p6.4.m4.2.3.1"></in><ci id="S4.SS3.p6.4.m4.2.3.2.cmml" xref="S4.SS3.p6.4.m4.2.3.2">ğšº</ci><apply id="S4.SS3.p6.4.m4.2.3.3.cmml" xref="S4.SS3.p6.4.m4.2.3.3"><csymbol cd="ambiguous" id="S4.SS3.p6.4.m4.2.3.3.1.cmml" xref="S4.SS3.p6.4.m4.2.3.3">superscript</csymbol><ci id="S4.SS3.p6.4.m4.2.3.3.2.cmml" xref="S4.SS3.p6.4.m4.2.3.3.2">â„</ci><apply id="S4.SS3.p6.4.m4.2.2.2.cmml" xref="S4.SS3.p6.4.m4.2.2.2"><times id="S4.SS3.p6.4.m4.2.2.2.3.cmml" xref="S4.SS3.p6.4.m4.2.2.2.3"></times><apply id="S4.SS3.p6.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1"><times id="S4.SS3.p6.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p6.4.m4.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.2">IC</mtext></ci><ci id="S4.SS3.p6.4.m4.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p6.4.m4.1.1.1.1.1.1.3">KS</mtext></ci></apply><apply id="S4.SS3.p6.4.m4.2.2.2.2.1.1.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1"><times id="S4.SS3.p6.4.m4.2.2.2.2.1.1.1.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.1"></times><ci id="S4.SS3.p6.4.m4.2.2.2.2.1.1.2a.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.4.m4.2.2.2.2.1.1.2.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.2">OC</mtext></ci><ci id="S4.SS3.p6.4.m4.2.2.2.2.1.1.3a.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.4.m4.2.2.2.2.1.1.3.cmml" xref="S4.SS3.p6.4.m4.2.2.2.2.1.1.3">KS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.4.m4.2c">\mathbf{\Sigma}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times(\texttt{OC}\times\texttt{KS})}</annotation></semantics></math>). Note that the singular values <math id="S4.SS3.p6.5.m5.1" class="ltx_Math" alttext="\mathbf{\Sigma}" display="inline"><semantics id="S4.SS3.p6.5.m5.1a"><mi id="S4.SS3.p6.5.m5.1.1" xref="S4.SS3.p6.5.m5.1.1.cmml">ğšº</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.5.m5.1b"><ci id="S4.SS3.p6.5.m5.1.1.cmml" xref="S4.SS3.p6.5.m5.1.1">ğšº</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.5.m5.1c">\mathbf{\Sigma}</annotation></semantics></math> indicate the importance of each corresponding <math id="S4.SS3.p6.6.m6.1" class="ltx_Math" alttext="\mathbf{U}" display="inline"><semantics id="S4.SS3.p6.6.m6.1a"><mi id="S4.SS3.p6.6.m6.1.1" xref="S4.SS3.p6.6.m6.1.1.cmml">ğ”</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.6.m6.1b"><ci id="S4.SS3.p6.6.m6.1.1.cmml" xref="S4.SS3.p6.6.m6.1.1">ğ”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.6.m6.1c">\mathbf{U}</annotation></semantics></math> and <math id="S4.SS3.p6.7.m7.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S4.SS3.p6.7.m7.1a"><mi id="S4.SS3.p6.7.m7.1.1" xref="S4.SS3.p6.7.m7.1.1.cmml">ğ•</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.7.m7.1b"><ci id="S4.SS3.p6.7.m7.1.1.cmml" xref="S4.SS3.p6.7.m7.1.1">ğ•</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.7.m7.1c">\mathbf{V}</annotation></semantics></math> element. Thus, allowing us to selectively discard less significant parameters, and retaining the most critical components for generating the subsequent layer weights. For example, by discarding the low-rank columns and rows and retaining only the top <math id="S4.SS3.p6.8.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p6.8.m8.1a"><mi id="S4.SS3.p6.8.m8.1.1" xref="S4.SS3.p6.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.8.m8.1b"><ci id="S4.SS3.p6.8.m8.1.1.cmml" xref="S4.SS3.p6.8.m8.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.8.m8.1c">k</annotation></semantics></math> singular values, we obtain reduced singular vectors that consider the scaling factor of the singular values: <math id="S4.SS3.p6.9.m9.1" class="ltx_Math" alttext="\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times k}" display="inline"><semantics id="S4.SS3.p6.9.m9.1a"><mrow id="S4.SS3.p6.9.m9.1.2" xref="S4.SS3.p6.9.m9.1.2.cmml"><mrow id="S4.SS3.p6.9.m9.1.2.2" xref="S4.SS3.p6.9.m9.1.2.2.cmml"><mover accent="true" id="S4.SS3.p6.9.m9.1.2.2.2" xref="S4.SS3.p6.9.m9.1.2.2.2.cmml"><mi id="S4.SS3.p6.9.m9.1.2.2.2.2" xref="S4.SS3.p6.9.m9.1.2.2.2.2.cmml">ğ”</mi><mo id="S4.SS3.p6.9.m9.1.2.2.2.1" xref="S4.SS3.p6.9.m9.1.2.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.SS3.p6.9.m9.1.2.2.1" xref="S4.SS3.p6.9.m9.1.2.2.1.cmml">â€‹</mo><msup id="S4.SS3.p6.9.m9.1.2.2.3" xref="S4.SS3.p6.9.m9.1.2.2.3.cmml"><mover accent="true" id="S4.SS3.p6.9.m9.1.2.2.3.2" xref="S4.SS3.p6.9.m9.1.2.2.3.2.cmml"><mi id="S4.SS3.p6.9.m9.1.2.2.3.2.2" xref="S4.SS3.p6.9.m9.1.2.2.3.2.2.cmml">ğšº</mi><mo id="S4.SS3.p6.9.m9.1.2.2.3.2.1" xref="S4.SS3.p6.9.m9.1.2.2.3.2.1.cmml">~</mo></mover><mfrac id="S4.SS3.p6.9.m9.1.2.2.3.3" xref="S4.SS3.p6.9.m9.1.2.2.3.3.cmml"><mn id="S4.SS3.p6.9.m9.1.2.2.3.3.2" xref="S4.SS3.p6.9.m9.1.2.2.3.3.2.cmml">ğŸ</mn><mn id="S4.SS3.p6.9.m9.1.2.2.3.3.3" xref="S4.SS3.p6.9.m9.1.2.2.3.3.3.cmml">ğŸ</mn></mfrac></msup></mrow><mo id="S4.SS3.p6.9.m9.1.2.1" xref="S4.SS3.p6.9.m9.1.2.1.cmml">âˆˆ</mo><msup id="S4.SS3.p6.9.m9.1.2.3" xref="S4.SS3.p6.9.m9.1.2.3.cmml"><mi id="S4.SS3.p6.9.m9.1.2.3.2" xref="S4.SS3.p6.9.m9.1.2.3.2.cmml">â„</mi><mrow id="S4.SS3.p6.9.m9.1.1.1" xref="S4.SS3.p6.9.m9.1.1.1.cmml"><mrow id="S4.SS3.p6.9.m9.1.1.1.1.1" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.9.m9.1.1.1.1.1.2" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p6.9.m9.1.1.1.1.1.1" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.9.m9.1.1.1.1.1.1.2" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.2a.cmml">IC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.9.m9.1.1.1.1.1.1.1" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.9.m9.1.1.1.1.1.1.3" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo rspace="0.055em" stretchy="false" id="S4.SS3.p6.9.m9.1.1.1.1.1.3" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S4.SS3.p6.9.m9.1.1.1.2" xref="S4.SS3.p6.9.m9.1.1.1.2.cmml">Ã—</mo><mi id="S4.SS3.p6.9.m9.1.1.1.3" xref="S4.SS3.p6.9.m9.1.1.1.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.9.m9.1b"><apply id="S4.SS3.p6.9.m9.1.2.cmml" xref="S4.SS3.p6.9.m9.1.2"><in id="S4.SS3.p6.9.m9.1.2.1.cmml" xref="S4.SS3.p6.9.m9.1.2.1"></in><apply id="S4.SS3.p6.9.m9.1.2.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2"><times id="S4.SS3.p6.9.m9.1.2.2.1.cmml" xref="S4.SS3.p6.9.m9.1.2.2.1"></times><apply id="S4.SS3.p6.9.m9.1.2.2.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2.2"><ci id="S4.SS3.p6.9.m9.1.2.2.2.1.cmml" xref="S4.SS3.p6.9.m9.1.2.2.2.1">~</ci><ci id="S4.SS3.p6.9.m9.1.2.2.2.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2.2.2">ğ”</ci></apply><apply id="S4.SS3.p6.9.m9.1.2.2.3.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3"><csymbol cd="ambiguous" id="S4.SS3.p6.9.m9.1.2.2.3.1.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3">superscript</csymbol><apply id="S4.SS3.p6.9.m9.1.2.2.3.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.2"><ci id="S4.SS3.p6.9.m9.1.2.2.3.2.1.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.2.1">~</ci><ci id="S4.SS3.p6.9.m9.1.2.2.3.2.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.2.2">ğšº</ci></apply><apply id="S4.SS3.p6.9.m9.1.2.2.3.3.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.3"><divide id="S4.SS3.p6.9.m9.1.2.2.3.3.1.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.3"></divide><cn type="integer" id="S4.SS3.p6.9.m9.1.2.2.3.3.2.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.3.2">1</cn><cn type="integer" id="S4.SS3.p6.9.m9.1.2.2.3.3.3.cmml" xref="S4.SS3.p6.9.m9.1.2.2.3.3.3">2</cn></apply></apply></apply><apply id="S4.SS3.p6.9.m9.1.2.3.cmml" xref="S4.SS3.p6.9.m9.1.2.3"><csymbol cd="ambiguous" id="S4.SS3.p6.9.m9.1.2.3.1.cmml" xref="S4.SS3.p6.9.m9.1.2.3">superscript</csymbol><ci id="S4.SS3.p6.9.m9.1.2.3.2.cmml" xref="S4.SS3.p6.9.m9.1.2.3.2">â„</ci><apply id="S4.SS3.p6.9.m9.1.1.1.cmml" xref="S4.SS3.p6.9.m9.1.1.1"><times id="S4.SS3.p6.9.m9.1.1.1.2.cmml" xref="S4.SS3.p6.9.m9.1.1.1.2"></times><apply id="S4.SS3.p6.9.m9.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1"><times id="S4.SS3.p6.9.m9.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p6.9.m9.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.9.m9.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.2">IC</mtext></ci><ci id="S4.SS3.p6.9.m9.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.9.m9.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p6.9.m9.1.1.1.1.1.1.3">KS</mtext></ci></apply><ci id="S4.SS3.p6.9.m9.1.1.1.3.cmml" xref="S4.SS3.p6.9.m9.1.1.1.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.9.m9.1c">\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}\in\mathbb{R}^{(\texttt{IC}\times\texttt{KS})\times k}</annotation></semantics></math> and <math id="S4.SS3.p6.10.m10.1" class="ltx_Math" alttext="\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}\in\mathbb{R}^{k\times(\texttt{OC}\times\texttt{KS})}" display="inline"><semantics id="S4.SS3.p6.10.m10.1a"><mrow id="S4.SS3.p6.10.m10.1.2" xref="S4.SS3.p6.10.m10.1.2.cmml"><mrow id="S4.SS3.p6.10.m10.1.2.2" xref="S4.SS3.p6.10.m10.1.2.2.cmml"><msup id="S4.SS3.p6.10.m10.1.2.2.2" xref="S4.SS3.p6.10.m10.1.2.2.2.cmml"><mover accent="true" id="S4.SS3.p6.10.m10.1.2.2.2.2" xref="S4.SS3.p6.10.m10.1.2.2.2.2.cmml"><mi id="S4.SS3.p6.10.m10.1.2.2.2.2.2" xref="S4.SS3.p6.10.m10.1.2.2.2.2.2.cmml">ğšº</mi><mo id="S4.SS3.p6.10.m10.1.2.2.2.2.1" xref="S4.SS3.p6.10.m10.1.2.2.2.2.1.cmml">~</mo></mover><mfrac id="S4.SS3.p6.10.m10.1.2.2.2.3" xref="S4.SS3.p6.10.m10.1.2.2.2.3.cmml"><mn id="S4.SS3.p6.10.m10.1.2.2.2.3.2" xref="S4.SS3.p6.10.m10.1.2.2.2.3.2.cmml">ğŸ</mn><mn id="S4.SS3.p6.10.m10.1.2.2.2.3.3" xref="S4.SS3.p6.10.m10.1.2.2.2.3.3.cmml">ğŸ</mn></mfrac></msup><mo lspace="0em" rspace="0em" id="S4.SS3.p6.10.m10.1.2.2.1" xref="S4.SS3.p6.10.m10.1.2.2.1.cmml">â€‹</mo><mover accent="true" id="S4.SS3.p6.10.m10.1.2.2.3" xref="S4.SS3.p6.10.m10.1.2.2.3.cmml"><mi id="S4.SS3.p6.10.m10.1.2.2.3.2" xref="S4.SS3.p6.10.m10.1.2.2.3.2.cmml">ğ•</mi><mo id="S4.SS3.p6.10.m10.1.2.2.3.1" xref="S4.SS3.p6.10.m10.1.2.2.3.1.cmml">~</mo></mover></mrow><mo id="S4.SS3.p6.10.m10.1.2.1" xref="S4.SS3.p6.10.m10.1.2.1.cmml">âˆˆ</mo><msup id="S4.SS3.p6.10.m10.1.2.3" xref="S4.SS3.p6.10.m10.1.2.3.cmml"><mi id="S4.SS3.p6.10.m10.1.2.3.2" xref="S4.SS3.p6.10.m10.1.2.3.2.cmml">â„</mi><mrow id="S4.SS3.p6.10.m10.1.1.1" xref="S4.SS3.p6.10.m10.1.1.1.cmml"><mi id="S4.SS3.p6.10.m10.1.1.1.3" xref="S4.SS3.p6.10.m10.1.1.1.3.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.10.m10.1.1.1.2" xref="S4.SS3.p6.10.m10.1.1.1.2.cmml">Ã—</mo><mrow id="S4.SS3.p6.10.m10.1.1.1.1.1" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p6.10.m10.1.1.1.1.1.2" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p6.10.m10.1.1.1.1.1.1" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.10.m10.1.1.1.1.1.1.2" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.2a.cmml">OC</mtext><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p6.10.m10.1.1.1.1.1.1.1" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.1.cmml">Ã—</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS3.p6.10.m10.1.1.1.1.1.1.3" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.3a.cmml">KS</mtext></mrow><mo stretchy="false" id="S4.SS3.p6.10.m10.1.1.1.1.1.3" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.10.m10.1b"><apply id="S4.SS3.p6.10.m10.1.2.cmml" xref="S4.SS3.p6.10.m10.1.2"><in id="S4.SS3.p6.10.m10.1.2.1.cmml" xref="S4.SS3.p6.10.m10.1.2.1"></in><apply id="S4.SS3.p6.10.m10.1.2.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2"><times id="S4.SS3.p6.10.m10.1.2.2.1.cmml" xref="S4.SS3.p6.10.m10.1.2.2.1"></times><apply id="S4.SS3.p6.10.m10.1.2.2.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p6.10.m10.1.2.2.2.1.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2">superscript</csymbol><apply id="S4.SS3.p6.10.m10.1.2.2.2.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.2"><ci id="S4.SS3.p6.10.m10.1.2.2.2.2.1.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.2.1">~</ci><ci id="S4.SS3.p6.10.m10.1.2.2.2.2.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.2.2">ğšº</ci></apply><apply id="S4.SS3.p6.10.m10.1.2.2.2.3.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.3"><divide id="S4.SS3.p6.10.m10.1.2.2.2.3.1.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.3"></divide><cn type="integer" id="S4.SS3.p6.10.m10.1.2.2.2.3.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.3.2">1</cn><cn type="integer" id="S4.SS3.p6.10.m10.1.2.2.2.3.3.cmml" xref="S4.SS3.p6.10.m10.1.2.2.2.3.3">2</cn></apply></apply><apply id="S4.SS3.p6.10.m10.1.2.2.3.cmml" xref="S4.SS3.p6.10.m10.1.2.2.3"><ci id="S4.SS3.p6.10.m10.1.2.2.3.1.cmml" xref="S4.SS3.p6.10.m10.1.2.2.3.1">~</ci><ci id="S4.SS3.p6.10.m10.1.2.2.3.2.cmml" xref="S4.SS3.p6.10.m10.1.2.2.3.2">ğ•</ci></apply></apply><apply id="S4.SS3.p6.10.m10.1.2.3.cmml" xref="S4.SS3.p6.10.m10.1.2.3"><csymbol cd="ambiguous" id="S4.SS3.p6.10.m10.1.2.3.1.cmml" xref="S4.SS3.p6.10.m10.1.2.3">superscript</csymbol><ci id="S4.SS3.p6.10.m10.1.2.3.2.cmml" xref="S4.SS3.p6.10.m10.1.2.3.2">â„</ci><apply id="S4.SS3.p6.10.m10.1.1.1.cmml" xref="S4.SS3.p6.10.m10.1.1.1"><times id="S4.SS3.p6.10.m10.1.1.1.2.cmml" xref="S4.SS3.p6.10.m10.1.1.1.2"></times><ci id="S4.SS3.p6.10.m10.1.1.1.3.cmml" xref="S4.SS3.p6.10.m10.1.1.1.3">ğ‘˜</ci><apply id="S4.SS3.p6.10.m10.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1"><times id="S4.SS3.p6.10.m10.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.1"></times><ci id="S4.SS3.p6.10.m10.1.1.1.1.1.1.2a.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.10.m10.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.2">OC</mtext></ci><ci id="S4.SS3.p6.10.m10.1.1.1.1.1.1.3a.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS3.p6.10.m10.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p6.10.m10.1.1.1.1.1.1.3">KS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.10.m10.1c">\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}\in\mathbb{R}^{k\times(\texttt{OC}\times\texttt{KS})}</annotation></semantics></math>.
The hypernetwork is then trained to predict these compressed singular vectors, approximating the original matrix as <math id="S4.SS3.p6.11.m11.1" class="ltx_Math" alttext="\mathbf{A}\approx\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}" display="inline"><semantics id="S4.SS3.p6.11.m11.1a"><mrow id="S4.SS3.p6.11.m11.1.1" xref="S4.SS3.p6.11.m11.1.1.cmml"><mi id="S4.SS3.p6.11.m11.1.1.2" xref="S4.SS3.p6.11.m11.1.1.2.cmml">ğ€</mi><mo id="S4.SS3.p6.11.m11.1.1.1" xref="S4.SS3.p6.11.m11.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS3.p6.11.m11.1.1.3" xref="S4.SS3.p6.11.m11.1.1.3.cmml"><mover accent="true" id="S4.SS3.p6.11.m11.1.1.3.2" xref="S4.SS3.p6.11.m11.1.1.3.2.cmml"><mi id="S4.SS3.p6.11.m11.1.1.3.2.2" xref="S4.SS3.p6.11.m11.1.1.3.2.2.cmml">ğ”</mi><mo id="S4.SS3.p6.11.m11.1.1.3.2.1" xref="S4.SS3.p6.11.m11.1.1.3.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.SS3.p6.11.m11.1.1.3.1" xref="S4.SS3.p6.11.m11.1.1.3.1.cmml">â€‹</mo><msup id="S4.SS3.p6.11.m11.1.1.3.3" xref="S4.SS3.p6.11.m11.1.1.3.3.cmml"><mover accent="true" id="S4.SS3.p6.11.m11.1.1.3.3.2" xref="S4.SS3.p6.11.m11.1.1.3.3.2.cmml"><mi id="S4.SS3.p6.11.m11.1.1.3.3.2.2" xref="S4.SS3.p6.11.m11.1.1.3.3.2.2.cmml">ğšº</mi><mo id="S4.SS3.p6.11.m11.1.1.3.3.2.1" xref="S4.SS3.p6.11.m11.1.1.3.3.2.1.cmml">~</mo></mover><mfrac id="S4.SS3.p6.11.m11.1.1.3.3.3" xref="S4.SS3.p6.11.m11.1.1.3.3.3.cmml"><mn id="S4.SS3.p6.11.m11.1.1.3.3.3.2" xref="S4.SS3.p6.11.m11.1.1.3.3.3.2.cmml">ğŸ</mn><mn id="S4.SS3.p6.11.m11.1.1.3.3.3.3" xref="S4.SS3.p6.11.m11.1.1.3.3.3.3.cmml">ğŸ</mn></mfrac></msup><mo lspace="0em" rspace="0em" id="S4.SS3.p6.11.m11.1.1.3.1a" xref="S4.SS3.p6.11.m11.1.1.3.1.cmml">â€‹</mo><msup id="S4.SS3.p6.11.m11.1.1.3.4" xref="S4.SS3.p6.11.m11.1.1.3.4.cmml"><mover accent="true" id="S4.SS3.p6.11.m11.1.1.3.4.2" xref="S4.SS3.p6.11.m11.1.1.3.4.2.cmml"><mi id="S4.SS3.p6.11.m11.1.1.3.4.2.2" xref="S4.SS3.p6.11.m11.1.1.3.4.2.2.cmml">ğšº</mi><mo id="S4.SS3.p6.11.m11.1.1.3.4.2.1" xref="S4.SS3.p6.11.m11.1.1.3.4.2.1.cmml">~</mo></mover><mfrac id="S4.SS3.p6.11.m11.1.1.3.4.3" xref="S4.SS3.p6.11.m11.1.1.3.4.3.cmml"><mn id="S4.SS3.p6.11.m11.1.1.3.4.3.2" xref="S4.SS3.p6.11.m11.1.1.3.4.3.2.cmml">ğŸ</mn><mn id="S4.SS3.p6.11.m11.1.1.3.4.3.3" xref="S4.SS3.p6.11.m11.1.1.3.4.3.3.cmml">ğŸ</mn></mfrac></msup><mo lspace="0em" rspace="0em" id="S4.SS3.p6.11.m11.1.1.3.1b" xref="S4.SS3.p6.11.m11.1.1.3.1.cmml">â€‹</mo><mover accent="true" id="S4.SS3.p6.11.m11.1.1.3.5" xref="S4.SS3.p6.11.m11.1.1.3.5.cmml"><mi id="S4.SS3.p6.11.m11.1.1.3.5.2" xref="S4.SS3.p6.11.m11.1.1.3.5.2.cmml">ğ•</mi><mo id="S4.SS3.p6.11.m11.1.1.3.5.1" xref="S4.SS3.p6.11.m11.1.1.3.5.1.cmml">~</mo></mover></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p6.11.m11.1b"><apply id="S4.SS3.p6.11.m11.1.1.cmml" xref="S4.SS3.p6.11.m11.1.1"><approx id="S4.SS3.p6.11.m11.1.1.1.cmml" xref="S4.SS3.p6.11.m11.1.1.1"></approx><ci id="S4.SS3.p6.11.m11.1.1.2.cmml" xref="S4.SS3.p6.11.m11.1.1.2">ğ€</ci><apply id="S4.SS3.p6.11.m11.1.1.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3"><times id="S4.SS3.p6.11.m11.1.1.3.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.1"></times><apply id="S4.SS3.p6.11.m11.1.1.3.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.2"><ci id="S4.SS3.p6.11.m11.1.1.3.2.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.2.1">~</ci><ci id="S4.SS3.p6.11.m11.1.1.3.2.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.2.2">ğ”</ci></apply><apply id="S4.SS3.p6.11.m11.1.1.3.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS3.p6.11.m11.1.1.3.3.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3">superscript</csymbol><apply id="S4.SS3.p6.11.m11.1.1.3.3.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.2"><ci id="S4.SS3.p6.11.m11.1.1.3.3.2.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.2.1">~</ci><ci id="S4.SS3.p6.11.m11.1.1.3.3.2.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.2.2">ğšº</ci></apply><apply id="S4.SS3.p6.11.m11.1.1.3.3.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.3"><divide id="S4.SS3.p6.11.m11.1.1.3.3.3.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.3"></divide><cn type="integer" id="S4.SS3.p6.11.m11.1.1.3.3.3.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.3.2">1</cn><cn type="integer" id="S4.SS3.p6.11.m11.1.1.3.3.3.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3.3.3.3">2</cn></apply></apply><apply id="S4.SS3.p6.11.m11.1.1.3.4.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4"><csymbol cd="ambiguous" id="S4.SS3.p6.11.m11.1.1.3.4.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4">superscript</csymbol><apply id="S4.SS3.p6.11.m11.1.1.3.4.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.2"><ci id="S4.SS3.p6.11.m11.1.1.3.4.2.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.2.1">~</ci><ci id="S4.SS3.p6.11.m11.1.1.3.4.2.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.2.2">ğšº</ci></apply><apply id="S4.SS3.p6.11.m11.1.1.3.4.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.3"><divide id="S4.SS3.p6.11.m11.1.1.3.4.3.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.3"></divide><cn type="integer" id="S4.SS3.p6.11.m11.1.1.3.4.3.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.3.2">1</cn><cn type="integer" id="S4.SS3.p6.11.m11.1.1.3.4.3.3.cmml" xref="S4.SS3.p6.11.m11.1.1.3.4.3.3">2</cn></apply></apply><apply id="S4.SS3.p6.11.m11.1.1.3.5.cmml" xref="S4.SS3.p6.11.m11.1.1.3.5"><ci id="S4.SS3.p6.11.m11.1.1.3.5.1.cmml" xref="S4.SS3.p6.11.m11.1.1.3.5.1">~</ci><ci id="S4.SS3.p6.11.m11.1.1.3.5.2.cmml" xref="S4.SS3.p6.11.m11.1.1.3.5.2">ğ•</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p6.11.m11.1c">\mathbf{A}\approx\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">Upon obtaining the two compressed singular value matrices, model parameters are generated efficiently through the straightforward multiplication of these vectors. This method ensures both efficiency and accuracy in parameter generation for deeper layers, addressing the per-layer information disparity problem in federated learning. By leveraging the hypernetworkâ€™s ability to capture nonlinear layer relationships, <span id="S4.SS3.p7.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> enables effective participation from clients with varying computational capacities, promoting robust model convergence and enhancing overall performance.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Hypernetwork Implementations</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">Given the early-exit architecture of <span id="S4.SS4.p1.4.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, which splits the model into three subnetworks, we employ two hypernetworks: one to predict the second exit parameters from the first exit parameters (<math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="H_{1\rightarrow 2}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">H</mi><mrow id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml"><mn id="S4.SS4.p1.1.m1.1.1.3.2" xref="S4.SS4.p1.1.m1.1.1.3.2.cmml">1</mn><mo stretchy="false" id="S4.SS4.p1.1.m1.1.1.3.1" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">â†’</mo><mn id="S4.SS4.p1.1.m1.1.1.3.3" xref="S4.SS4.p1.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">ğ»</ci><apply id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3"><ci id="S4.SS4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS4.p1.1.m1.1.1.3.1">â†’</ci><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.2.cmml" xref="S4.SS4.p1.1.m1.1.1.3.2">1</cn><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">H_{1\rightarrow 2}</annotation></semantics></math>) and another to predict the third exit parameters from the second (<math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="H_{2\rightarrow 3}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">H</mi><mrow id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml"><mn id="S4.SS4.p1.2.m2.1.1.3.2" xref="S4.SS4.p1.2.m2.1.1.3.2.cmml">2</mn><mo stretchy="false" id="S4.SS4.p1.2.m2.1.1.3.1" xref="S4.SS4.p1.2.m2.1.1.3.1.cmml">â†’</mo><mn id="S4.SS4.p1.2.m2.1.1.3.3" xref="S4.SS4.p1.2.m2.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">ğ»</ci><apply id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3"><ci id="S4.SS4.p1.2.m2.1.1.3.1.cmml" xref="S4.SS4.p1.2.m2.1.1.3.1">â†’</ci><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.2.cmml" xref="S4.SS4.p1.2.m2.1.1.3.2">2</cn><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">H_{2\rightarrow 3}</annotation></semantics></math>). These hypernetworks are trained at the server using parameter samples collected from clients each round. Clients executing the model up to the second exit provide samples for <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="H_{1\rightarrow 2}" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><msub id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">H</mi><mrow id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml"><mn id="S4.SS4.p1.3.m3.1.1.3.2" xref="S4.SS4.p1.3.m3.1.1.3.2.cmml">1</mn><mo stretchy="false" id="S4.SS4.p1.3.m3.1.1.3.1" xref="S4.SS4.p1.3.m3.1.1.3.1.cmml">â†’</mo><mn id="S4.SS4.p1.3.m3.1.1.3.3" xref="S4.SS4.p1.3.m3.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">ğ»</ci><apply id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3"><ci id="S4.SS4.p1.3.m3.1.1.3.1.cmml" xref="S4.SS4.p1.3.m3.1.1.3.1">â†’</ci><cn type="integer" id="S4.SS4.p1.3.m3.1.1.3.2.cmml" xref="S4.SS4.p1.3.m3.1.1.3.2">1</cn><cn type="integer" id="S4.SS4.p1.3.m3.1.1.3.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">H_{1\rightarrow 2}</annotation></semantics></math>, while those executing the full model provide samples for <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="H_{2\rightarrow 3}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msub id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">H</mi><mrow id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml"><mn id="S4.SS4.p1.4.m4.1.1.3.2" xref="S4.SS4.p1.4.m4.1.1.3.2.cmml">2</mn><mo stretchy="false" id="S4.SS4.p1.4.m4.1.1.3.1" xref="S4.SS4.p1.4.m4.1.1.3.1.cmml">â†’</mo><mn id="S4.SS4.p1.4.m4.1.1.3.3" xref="S4.SS4.p1.4.m4.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">ğ»</ci><apply id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3"><ci id="S4.SS4.p1.4.m4.1.1.3.1.cmml" xref="S4.SS4.p1.4.m4.1.1.3.1">â†’</ci><cn type="integer" id="S4.SS4.p1.4.m4.1.1.3.2.cmml" xref="S4.SS4.p1.4.m4.1.1.3.2">2</cn><cn type="integer" id="S4.SS4.p1.4.m4.1.1.3.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">H_{2\rightarrow 3}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Hypernetworks from the previous federated learning round serve as initial values for the next, and new samples are used for further training. We discard samples once they are used to ensure that the hypernetwork generates up-to-date weights while reducing training costs. This approach maintains the effectiveness and efficiency of hypernetworks in adapting to dynamically changing parameters in federated learning.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> selectively generates only the core parameters of the feature extractor, specifically the convolutional layers, while excluding the batch normalization (BN) and classification layers. This approach is based on two key empirical observations: (i) BN and classification layers do not significantly enhance global model performance, and (ii) since these layers are closely related to local data, globally generating them can dilute the unique knowledge of each client, compromising model personalization. We demonstrate this in SectionÂ <a href="#S5.SS3" title="5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, where we evaluate personalization accuracy after a few epochs of local fine-tuning of the global model with client data.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Once client model parameter data is collected at each federated learning round and the hypernetworks are trained using the operations described above, <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> uses the fine-tuned hypernetworks to predict subsequent layer parameters based on the compressed representations. This approach ensures that clients with varying computational capacities can fairly participate in the federated learning process by generating the necessary weight information for effective model convergence and improved performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Experiment Setup</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We perform an extensive set of experiments to validate the performance of <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> using three datasets with various comparison baselines, and present details below.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Dataset and Model.</span> In this work, we use three different datasets for our evaluations and a VGG-based baseline CNN model architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and Zisserman, <a href="#bib.bib61" title="" class="ltx_ref">2014</a>)</cite>. We discuss the details of the dataset and model used as follows.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">SVHN datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Netzer etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2011</a>)</cite></span> includes 99,289 labeled images of 10 digits (1-10) extracted from real-world house numbers, offering a diverse representation of physical world images and is well-suited for mobile image classification tasks. The model comprises four convolutional blocks, each with two layers featuring channels (32, 64, 128, 256), followed by max-pooling at the end of each block.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mo id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><ci id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">STL10 datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Coates etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2011</a>)</cite></span> comprises 10 classes with diverse real-world objects, containing 1.3K samples per class. Images in STL10 are high-resolution at 96x96 pixels, distinguishing it from datasets like CIFAR, making STL10 suitable for real-world evaluations of <span id="S5.SS1.p4.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> in federated learning. The model architecture mirrors that used for SVHN, but with channel configurations of 64, 128, 256, and 512.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><math id="S5.SS1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.p5.1.m1.1a"><mo id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><ci id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">UniMiB SHAR datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Micucci etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite></span>
comprises 4.2K samples from 30 subjects, capturing data from smartphone accelerometers for eight fall types. UniMiB SHAR represents a widely adopted real-world sensing application. We adopted the same model configuration as the STL10 dataset.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para ltx_noindent">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">Baselines.</span> For comparison, we utilize five different federated learning schemes: <span id="S5.SS1.p6.1.2" class="ltx_text ltx_font_bold">(1) FedAvgÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>:</span> We employ FedAvg as a baseline federated learning approach, which traditionally does not address device heterogeneity. We compare two configurations: in the first, all clients utilize the smallest subnetwork of the multi-exit architecture (FedAvg-S); in the second, all clients utilize the largest model (FedAvg-L). Note that the FedAvg-L configuration is neither realistic nor fair in heterogeneous federated learning, but we test as the upper limit performance;
<span id="S5.SS1.p6.1.3" class="ltx_text ltx_font_bold">(2) HeteroFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Diao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite></span>: HeteroFL supports clients with heterogeneous computing resources by leveraging width-wise submodel scaling (compared to our depth-wise splits), where a scaling factor determines the size of the submodel and amount of shared parameters;
<span id="S5.SS1.p6.1.4" class="ltx_text ltx_font_bold">(3) ScaleFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Ilhan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite></span>: ScaleFL splits the model both width- and depth-wise to reduce the number of model parameters with a preset split ratio, supporting federated learning on devices with heterogeneous computing resources. Additionally, ScaleFL employs self-distillation to optimize each multi-exit;
<span id="S5.SS1.p6.1.5" class="ltx_text ltx_font_bold">(4) LotteryFLÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020b</a>)</cite></span>: Based on the lottery ticket hypothesisÂ <cite class="ltx_cite ltx_citemacro_citep">(Frankle and Carbin, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>, LotteryFL achieves personalized model improvements by adaptively pruning model parameters during the client-side local training process to reduce local model capacity;
<span id="S5.SS1.p6.1.6" class="ltx_text ltx_font_bold">(5) FedRolexÂ <cite class="ltx_cite ltx_citemacro_citep">(Alam etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite></span>: FedRolex reduces client-side computation by utilizing a subset of the full model, selected using a moving cyclic window. FedRolex supports model variations by adjusting the window size according to client requirements; wider windows enable the use of models with more parameters.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.3" class="ltx_p">Each of the baselines underwent 300 federated learning rounds with a non-overlapping dataset distributed following a non-IID pattern based on the Dirichlet distributionÂ <cite class="ltx_cite ltx_citemacro_citep">(Hsu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, where <math id="S5.SS1.p7.1.m1.2" class="ltx_Math" alttext="\alpha\in(0,\infty)" display="inline"><semantics id="S5.SS1.p7.1.m1.2a"><mrow id="S5.SS1.p7.1.m1.2.3" xref="S5.SS1.p7.1.m1.2.3.cmml"><mi id="S5.SS1.p7.1.m1.2.3.2" xref="S5.SS1.p7.1.m1.2.3.2.cmml">Î±</mi><mo id="S5.SS1.p7.1.m1.2.3.1" xref="S5.SS1.p7.1.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S5.SS1.p7.1.m1.2.3.3.2" xref="S5.SS1.p7.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S5.SS1.p7.1.m1.2.3.3.2.1" xref="S5.SS1.p7.1.m1.2.3.3.1.cmml">(</mo><mn id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">0</mn><mo id="S5.SS1.p7.1.m1.2.3.3.2.2" xref="S5.SS1.p7.1.m1.2.3.3.1.cmml">,</mo><mi mathvariant="normal" id="S5.SS1.p7.1.m1.2.2" xref="S5.SS1.p7.1.m1.2.2.cmml">âˆ</mi><mo stretchy="false" id="S5.SS1.p7.1.m1.2.3.3.2.3" xref="S5.SS1.p7.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.2b"><apply id="S5.SS1.p7.1.m1.2.3.cmml" xref="S5.SS1.p7.1.m1.2.3"><in id="S5.SS1.p7.1.m1.2.3.1.cmml" xref="S5.SS1.p7.1.m1.2.3.1"></in><ci id="S5.SS1.p7.1.m1.2.3.2.cmml" xref="S5.SS1.p7.1.m1.2.3.2">ğ›¼</ci><interval closure="open" id="S5.SS1.p7.1.m1.2.3.3.1.cmml" xref="S5.SS1.p7.1.m1.2.3.3.2"><cn type="integer" id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">0</cn><infinity id="S5.SS1.p7.1.m1.2.2.cmml" xref="S5.SS1.p7.1.m1.2.2"></infinity></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.2c">\alpha\in(0,\infty)</annotation></semantics></math> is used to control the degree of data disparity (high <math id="S5.SS1.p7.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS1.p7.2.m2.1a"><mi id="S5.SS1.p7.2.m2.1.1" xref="S5.SS1.p7.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.2.m2.1b"><ci id="S5.SS1.p7.2.m2.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.2.m2.1c">\alpha</annotation></semantics></math> leads to uniformly distributed dataset in terms of the amount of data samples and label distribution). In our experiments, we applied <math id="S5.SS1.p7.3.m3.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S5.SS1.p7.3.m3.1a"><mrow id="S5.SS1.p7.3.m3.1.1" xref="S5.SS1.p7.3.m3.1.1.cmml"><mi id="S5.SS1.p7.3.m3.1.1.2" xref="S5.SS1.p7.3.m3.1.1.2.cmml">Î±</mi><mo id="S5.SS1.p7.3.m3.1.1.1" xref="S5.SS1.p7.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p7.3.m3.1.1.3" xref="S5.SS1.p7.3.m3.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.3.m3.1b"><apply id="S5.SS1.p7.3.m3.1.1.cmml" xref="S5.SS1.p7.3.m3.1.1"><eq id="S5.SS1.p7.3.m3.1.1.1.cmml" xref="S5.SS1.p7.3.m3.1.1.1"></eq><ci id="S5.SS1.p7.3.m3.1.1.2.cmml" xref="S5.SS1.p7.3.m3.1.1.2">ğ›¼</ci><cn type="float" id="S5.SS1.p7.3.m3.1.1.3.cmml" xref="S5.SS1.p7.3.m3.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.3.m3.1c">\alpha=0.5</annotation></semantics></math> as default unless explicitly specified. Local training was performed for five epochs per round with learning rates of 0.005, 0.0005, and 0.0005, and batch sizes of 16, 32, and 128 for the UniMiB, STL10, and SVHN datasets, respectively, using the Adam OptimizerÂ <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib26" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p"><span id="S5.SS1.p8.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> integrates three exit layers into the federated learning model. To accommodate heterogeneous clients, we categorize 50 clients into three groups: 17 using a small model with one exit layer, another 17 using a medium-capacity model with two exit layers, and the remaining 16 using the full model with all three exit layers. For UniMiB, we configure 30 clients (10 per group) due to its limited dataset size. To assure heterogeneity in the training process, 20% of the clients were uniformly selected from the three groups to participate in each federated learning round. Except for FedAvg, which does not support device heterogeneity, all other baselines maintained model capacities balanced across client groups. For LotteryFL, we relaxed resource constraints initially to fully train the model for its anticipated performance.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para">
<p id="S5.SS1.p9.3" class="ltx_p">For the hypernetwork, we employed MLP models consisting of two linear layers with ReLU activation for each layer. Training occurred over 25 epochs at the server for each round, utilizing a single-batch approach with a learning rate set to 0.0005 and the Adam Optimizer. <span id="S5.SS1.p9.3.1" class="ltx_text ltx_font_italic">HypeMeFed</span> utilizes SVD for compressing neural network parameters, yielding two singular vectors (<math id="S5.SS1.p9.1.m1.1" class="ltx_Math" alttext="\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}" display="inline"><semantics id="S5.SS1.p9.1.m1.1a"><mrow id="S5.SS1.p9.1.m1.1.1" xref="S5.SS1.p9.1.m1.1.1.cmml"><mover accent="true" id="S5.SS1.p9.1.m1.1.1.2" xref="S5.SS1.p9.1.m1.1.1.2.cmml"><mi id="S5.SS1.p9.1.m1.1.1.2.2" xref="S5.SS1.p9.1.m1.1.1.2.2.cmml">ğ”</mi><mo id="S5.SS1.p9.1.m1.1.1.2.1" xref="S5.SS1.p9.1.m1.1.1.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S5.SS1.p9.1.m1.1.1.1" xref="S5.SS1.p9.1.m1.1.1.1.cmml">â€‹</mo><msup id="S5.SS1.p9.1.m1.1.1.3" xref="S5.SS1.p9.1.m1.1.1.3.cmml"><mover accent="true" id="S5.SS1.p9.1.m1.1.1.3.2" xref="S5.SS1.p9.1.m1.1.1.3.2.cmml"><mi id="S5.SS1.p9.1.m1.1.1.3.2.2" xref="S5.SS1.p9.1.m1.1.1.3.2.2.cmml">ğšº</mi><mo id="S5.SS1.p9.1.m1.1.1.3.2.1" xref="S5.SS1.p9.1.m1.1.1.3.2.1.cmml">~</mo></mover><mfrac id="S5.SS1.p9.1.m1.1.1.3.3" xref="S5.SS1.p9.1.m1.1.1.3.3.cmml"><mn id="S5.SS1.p9.1.m1.1.1.3.3.2" xref="S5.SS1.p9.1.m1.1.1.3.3.2.cmml">ğŸ</mn><mn id="S5.SS1.p9.1.m1.1.1.3.3.3" xref="S5.SS1.p9.1.m1.1.1.3.3.3.cmml">ğŸ</mn></mfrac></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.1.m1.1b"><apply id="S5.SS1.p9.1.m1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1"><times id="S5.SS1.p9.1.m1.1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1.1"></times><apply id="S5.SS1.p9.1.m1.1.1.2.cmml" xref="S5.SS1.p9.1.m1.1.1.2"><ci id="S5.SS1.p9.1.m1.1.1.2.1.cmml" xref="S5.SS1.p9.1.m1.1.1.2.1">~</ci><ci id="S5.SS1.p9.1.m1.1.1.2.2.cmml" xref="S5.SS1.p9.1.m1.1.1.2.2">ğ”</ci></apply><apply id="S5.SS1.p9.1.m1.1.1.3.cmml" xref="S5.SS1.p9.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p9.1.m1.1.1.3.1.cmml" xref="S5.SS1.p9.1.m1.1.1.3">superscript</csymbol><apply id="S5.SS1.p9.1.m1.1.1.3.2.cmml" xref="S5.SS1.p9.1.m1.1.1.3.2"><ci id="S5.SS1.p9.1.m1.1.1.3.2.1.cmml" xref="S5.SS1.p9.1.m1.1.1.3.2.1">~</ci><ci id="S5.SS1.p9.1.m1.1.1.3.2.2.cmml" xref="S5.SS1.p9.1.m1.1.1.3.2.2">ğšº</ci></apply><apply id="S5.SS1.p9.1.m1.1.1.3.3.cmml" xref="S5.SS1.p9.1.m1.1.1.3.3"><divide id="S5.SS1.p9.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p9.1.m1.1.1.3.3"></divide><cn type="integer" id="S5.SS1.p9.1.m1.1.1.3.3.2.cmml" xref="S5.SS1.p9.1.m1.1.1.3.3.2">1</cn><cn type="integer" id="S5.SS1.p9.1.m1.1.1.3.3.3.cmml" xref="S5.SS1.p9.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.1.m1.1c">\mathbf{\tilde{U}\tilde{\Sigma}^{\frac{1}{2}}}</annotation></semantics></math> and <math id="S5.SS1.p9.2.m2.1" class="ltx_Math" alttext="\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}" display="inline"><semantics id="S5.SS1.p9.2.m2.1a"><mrow id="S5.SS1.p9.2.m2.1.1" xref="S5.SS1.p9.2.m2.1.1.cmml"><msup id="S5.SS1.p9.2.m2.1.1.2" xref="S5.SS1.p9.2.m2.1.1.2.cmml"><mover accent="true" id="S5.SS1.p9.2.m2.1.1.2.2" xref="S5.SS1.p9.2.m2.1.1.2.2.cmml"><mi id="S5.SS1.p9.2.m2.1.1.2.2.2" xref="S5.SS1.p9.2.m2.1.1.2.2.2.cmml">ğšº</mi><mo id="S5.SS1.p9.2.m2.1.1.2.2.1" xref="S5.SS1.p9.2.m2.1.1.2.2.1.cmml">~</mo></mover><mfrac id="S5.SS1.p9.2.m2.1.1.2.3" xref="S5.SS1.p9.2.m2.1.1.2.3.cmml"><mn id="S5.SS1.p9.2.m2.1.1.2.3.2" xref="S5.SS1.p9.2.m2.1.1.2.3.2.cmml">ğŸ</mn><mn id="S5.SS1.p9.2.m2.1.1.2.3.3" xref="S5.SS1.p9.2.m2.1.1.2.3.3.cmml">ğŸ</mn></mfrac></msup><mo lspace="0em" rspace="0em" id="S5.SS1.p9.2.m2.1.1.1" xref="S5.SS1.p9.2.m2.1.1.1.cmml">â€‹</mo><mover accent="true" id="S5.SS1.p9.2.m2.1.1.3" xref="S5.SS1.p9.2.m2.1.1.3.cmml"><mi id="S5.SS1.p9.2.m2.1.1.3.2" xref="S5.SS1.p9.2.m2.1.1.3.2.cmml">ğ•</mi><mo id="S5.SS1.p9.2.m2.1.1.3.1" xref="S5.SS1.p9.2.m2.1.1.3.1.cmml">~</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.2.m2.1b"><apply id="S5.SS1.p9.2.m2.1.1.cmml" xref="S5.SS1.p9.2.m2.1.1"><times id="S5.SS1.p9.2.m2.1.1.1.cmml" xref="S5.SS1.p9.2.m2.1.1.1"></times><apply id="S5.SS1.p9.2.m2.1.1.2.cmml" xref="S5.SS1.p9.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p9.2.m2.1.1.2.1.cmml" xref="S5.SS1.p9.2.m2.1.1.2">superscript</csymbol><apply id="S5.SS1.p9.2.m2.1.1.2.2.cmml" xref="S5.SS1.p9.2.m2.1.1.2.2"><ci id="S5.SS1.p9.2.m2.1.1.2.2.1.cmml" xref="S5.SS1.p9.2.m2.1.1.2.2.1">~</ci><ci id="S5.SS1.p9.2.m2.1.1.2.2.2.cmml" xref="S5.SS1.p9.2.m2.1.1.2.2.2">ğšº</ci></apply><apply id="S5.SS1.p9.2.m2.1.1.2.3.cmml" xref="S5.SS1.p9.2.m2.1.1.2.3"><divide id="S5.SS1.p9.2.m2.1.1.2.3.1.cmml" xref="S5.SS1.p9.2.m2.1.1.2.3"></divide><cn type="integer" id="S5.SS1.p9.2.m2.1.1.2.3.2.cmml" xref="S5.SS1.p9.2.m2.1.1.2.3.2">1</cn><cn type="integer" id="S5.SS1.p9.2.m2.1.1.2.3.3.cmml" xref="S5.SS1.p9.2.m2.1.1.2.3.3">2</cn></apply></apply><apply id="S5.SS1.p9.2.m2.1.1.3.cmml" xref="S5.SS1.p9.2.m2.1.1.3"><ci id="S5.SS1.p9.2.m2.1.1.3.1.cmml" xref="S5.SS1.p9.2.m2.1.1.3.1">~</ci><ci id="S5.SS1.p9.2.m2.1.1.3.2.cmml" xref="S5.SS1.p9.2.m2.1.1.3.2">ğ•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.2.m2.1c">\mathbf{\tilde{\Sigma}^{\frac{1}{2}}\tilde{V}}</annotation></semantics></math> in FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.3. Efficient Hypernetwork Design for Multi-exit Architectures â€£ 4. System Design â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), and employs a MLP model for each singular vector. Specifically, we select the top <math id="S5.SS1.p9.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p9.3.m3.1a"><mi id="S5.SS1.p9.3.m3.1.1" xref="S5.SS1.p9.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.3.m3.1b"><ci id="S5.SS1.p9.3.m3.1.1.cmml" xref="S5.SS1.p9.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.3.m3.1c">k</annotation></semantics></math>=100 singular values to compress model weights unless otherwise specified.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Overall Accuracy Performance</h3>

<figure id="S5.F7" class="ltx_figure"><img src="/html/2407.03086/assets/x9.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Global model accuracy for <span id="S5.F7.4.1" class="ltx_text ltx_font_italic">HypeMeFed</span> and baselines with varying datasets (<math id="S5.F7.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.F7.2.m1.1b"><mi id="S5.F7.2.m1.1.1" xref="S5.F7.2.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.F7.2.m1.1c"><ci id="S5.F7.2.m1.1.1.cmml" xref="S5.F7.2.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.2.m1.1d">\alpha</annotation></semantics></math>=0.5; 300 rounds).</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">FigureÂ <a href="#S5.F7" title="Figure 7 â€£ 5.2. Overall Accuracy Performance â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> plots the global model accuracy following server-side aggregation for <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> and baselines. For <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>, employing a multi-exit architecture, each model generates multiple predictions per sample, and we evaluate using the final exit prediction. As anticipated, FedAvg-L achieves the highest accuracy across datasets by universally employing the full model, representing an ideal but unrealistic scenario. In contrast, FedAvg-S, sharing a small model to accommodate clients with tight resource limitations, exhibits a consistent accuracy decline. <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> outperforms FedAvg-S, effectively narrowing the accuracy gap towards FedAvg-L.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">From FigureÂ <a href="#S5.F7" title="Figure 7 â€£ 5.2. Overall Accuracy Performance â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, it is evident that baseline methods exhibit inferior performance relative to FedAvg-S, struggling to balance accuracy and support for heterogeneous platforms. Specifically, HeteroFL reduces convolutional layer channels to meet client resource constraints (width-wise scaling), resulting in overly narrow models and poor overall performance. FedRolex, employing a cyclic window approach, requires extensive rounds to cover all model parameters, leading to suboptimal accuracy within a 300-round training period. In its original evaluationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Alam etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, FedRolex required over 1,000 federated rounds for effective convergence, presenting challenges in practical deployments. Additionally, while LotteryFL focuses on improving personalized model accuracy, its global model accuracy falls short.
ScaleFL on one hand, combines depth-wise model splits with width-wise scaling. While the original implementationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ilhan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> also integrates a self-knowledge distillation (KD) operation (c.f., SecÂ <a href="#S2" title="2. Related work and Background â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we noticed that this disturbs the model convergence in some cases (c.f., SecÂ <a href="#S5.SS3.SSS1" title="5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.1</span></a>), showing even lower accuracy; thus, we report results without self-KD. Our findings from ScaleFL suggest that amalgamating hypernetworks to depth-wise model splits offer a more effective approach for supporting heterogeneous federated learning.
We note that this paradigm of heterogenous federated learning schemes performing lower than FedAvg-S is also seen in previous workÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x10.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="438" height="133" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Accuracy with varying non-IID levels (<math id="S5.F8.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.F8.2.m1.1b"><mi id="S5.F8.2.m1.1.1" xref="S5.F8.2.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.F8.2.m1.1c"><ci id="S5.F8.2.m1.1.1.cmml" xref="S5.F8.2.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F8.2.m1.1d">\alpha</annotation></semantics></math>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F8.3" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.2" class="ltx_p">Next, while <span id="S5.SS2.p3.2.1" class="ltx_text ltx_font_italic">HypeMeFed</span> targets to address device heterogeneity, we examine its robustness against data heterogeneity by analyzing the accuracy with varying non-IID degrees in the data (<math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\alpha</annotation></semantics></math>). FigureÂ <a href="#S5.F8" title="Figure 8 â€£ 5.2. Overall Accuracy Performance â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> depicts the accuracy for four different <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mi id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><ci id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">\alpha</annotation></semantics></math> values: 0.1 (strong non-IID), 0.5, 1.0, and 10,000 (IID). The results suggest that as data becomes uniformly distributed, accuracy tends to improve. However, most baselines still fail to outperform FedAvg-S, while <span id="S5.SS2.p3.2.2" class="ltx_text ltx_font_italic">HypeMeFed</span> consistently achieves better or at least competitive model accuracy.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> targets to uniformly select participating clients with varying resource capabilities. Nevertheless, while we omit results due to the lack of space, we note that we have empirically confirmed that a non-uniform mix of heterogeneous clients does not affect <span id="S5.SS2.p4.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s performance.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Impact of Hypernetworks</h3>

<figure id="S5.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x11.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="392" height="140" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Model accuracy trends with increasing federated learning rounds for <span id="S5.F9.2.1" class="ltx_text ltx_font_italic">HypeMeFed</span> and ScaleFL.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F9.3" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Impact of Weight Generation</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">Given the overall performance improvements that <span id="S5.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> brings, we now examine the impact of the hypernetwork on <span id="S5.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>â€™s performance. For this, we first examine the global model accuracy trends (at the server) for increasing federated learning rounds for <span id="S5.SS3.SSS1.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span>, ScaleFL, and ScaleFL without self-knowledge distillation in FigureÂ <a href="#S5.F9" title="Figure 9 â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
Note that ScaleFL also leverages a multi-exit architecture similar to that of <span id="S5.SS3.SSS1.p1.1.4" class="ltx_text ltx_font_italic">HypeMeFed</span>, but without explicit weight estimations for the deeper layers. From our evaluations using the STL10 and UniMiB datasets with different <math id="S5.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS3.SSS1.p1.1.m1.1a"><mi id="S5.SS3.SSS1.p1.1.m1.1.1" xref="S5.SS3.SSS1.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.1.m1.1b"><ci id="S5.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS1.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.1.m1.1c">\alpha</annotation></semantics></math>, we can notice that ScaleFL fails to properly converge and show accuracy drops during the federated learning operations as the rounds progress (see red highlights in Fig.Â <a href="#S5.F9" title="Figure 9 â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). By removing the self-KD process, we can see a continuously increasing trend with convergence, but <span id="S5.SS3.SSS1.p1.1.5" class="ltx_text ltx_font_italic">HypeMeFed</span> eventually converges to a higher accuracy.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<p id="S5.T1.10" class="ltx_p"><span id="S5.T1.10.10" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.T1.10.10.10.10" class="ltx_inline-block ltx_transformed_outer" style="width:487.1pt;height:132.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T1.10.10.10.10.10" class="ltx_p"><span id="S5.T1.10.10.10.10.10.10" class="ltx_text">
<span id="S5.T1.10.10.10.10.10.10.10" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S5.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S5.T1.1.1.1.1.1.1.1.1.2.1" class="ltx_text">CNN - UniMiB</span></span>
<span id="S5.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_colspan ltx_colspan_5">Number of Singular Values (<math id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.1.1.1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.1.1.1.1.m1.1c">k</annotation></semantics></math>)</span></span>
<span id="S5.T1.10.10.10.10.10.10.10.11.1" class="ltx_tr">
<span id="S5.T1.10.10.10.10.10.10.10.11.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">All</span>
<span id="S5.T1.10.10.10.10.10.10.10.11.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25</span>
<span id="S5.T1.10.10.10.10.10.10.10.11.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50</span>
<span id="S5.T1.10.10.10.10.10.10.10.11.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</span>
<span id="S5.T1.10.10.10.10.10.10.10.11.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">200</span></span>
<span id="S5.T1.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S5.T1.2.2.2.2.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Accuracy<math id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1.1" xref="S5.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1b"><ci id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.2.2.2.2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S5.T1.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.44 (0.00%)</span>
<span id="S5.T1.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.55 (-1.89%)</span>
<span id="S5.T1.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.76 (-1.68%)</span>
<span id="S5.T1.2.2.2.2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.86 (-1.58%)</span>
<span id="S5.T1.2.2.2.2.2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.2.2.2.2.2.2.2.2.6.1" class="ltx_text ltx_font_bold">38.89 (-1.55%)</span></span></span>
<span id="S5.T1.3.3.3.3.3.3.3.3" class="ltx_tr">
<span id="S5.T1.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">HN size (MB)<math id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1.1" xref="S5.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1b"><ci id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.3.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="S5.T1.3.3.3.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">455.34 (0.00%)</span>
<span id="S5.T1.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.3.3.3.3.3.3.3.3.3.1" class="ltx_text ltx_font_bold">5.60 (-98.77%)</span></span>
<span id="S5.T1.3.3.3.3.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.68 (-98.75%)</span>
<span id="S5.T1.3.3.3.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.83 (-98.72%)</span>
<span id="S5.T1.3.3.3.3.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.14 (-98.65%)</span></span>
<span id="S5.T1.4.4.4.4.4.4.4.4" class="ltx_tr">
<span id="S5.T1.4.4.4.4.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"># of parameters (M)<math id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1.1" xref="S5.T1.4.4.4.4.4.4.4.4.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1b"><ci id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.4.4.4.4.4.4.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.4.4.4.4.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="S5.T1.4.4.4.4.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">227.37 (0.00%)</span>
<span id="S5.T1.4.4.4.4.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.4.4.4.4.4.4.4.3.1" class="ltx_text ltx_font_bold">1.40 (-99.39%)</span></span>
<span id="S5.T1.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.42 (-99.38%)</span>
<span id="S5.T1.4.4.4.4.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.46 (-99.36%)</span>
<span id="S5.T1.4.4.4.4.4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.54 (-99.32%)</span></span>
<span id="S5.T1.10.10.10.10.10.10.10.10" class="ltx_tr">
<span id="S5.T1.5.5.5.5.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.5.5.5.5.5.5.5.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.5.5.5.5.5.5.5.5.1.1.2" class="ltx_tr">
<span id="S5.T1.5.5.5.5.5.5.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Avg training time</span></span>
<span id="S5.T1.5.5.5.5.5.5.5.5.1.1.1" class="ltx_tr">
<span id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">per epoch(ms)<math id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1.1" xref="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1b"><ci id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1.1.cmml" xref="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.5.5.5.5.5.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></span>
</span></span>
<span id="S5.T1.6.6.6.6.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.6.6.6.6.6.6.6.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.6.6.6.6.6.6.6.6.2.1.2" class="ltx_tr">
<span id="S5.T1.6.6.6.6.6.6.6.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">226.56Â±0.10</span></span>
<span id="S5.T1.6.6.6.6.6.6.6.6.2.1.1" class="ltx_tr">
<span id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(1.00<math id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1a"><mo id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1.1" xref="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1b"><times id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1.1.cmml" xref="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.6.6.6.6.6.6.2.1.1.1.m1.1c">\times</annotation></semantics></math>)</span></span>
</span></span>
<span id="S5.T1.7.7.7.7.7.7.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.7.7.7.7.7.7.7.7.3.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.7.7.7.7.7.7.7.7.3.1.2" class="ltx_tr">
<span id="S5.T1.7.7.7.7.7.7.7.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.7.7.7.7.7.7.7.7.3.1.2.1.1" class="ltx_text ltx_font_bold">91.14Â±0.54</span></span></span>
<span id="S5.T1.7.7.7.7.7.7.7.7.3.1.1" class="ltx_tr">
<span id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.1" class="ltx_text ltx_font_bold">(2.49</span><math id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1a"><mo id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1.1" xref="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1b"><times id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1.1.cmml" xref="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.m1.1c">\times</annotation></semantics></math><span id="S5.T1.7.7.7.7.7.7.7.7.3.1.1.1.2" class="ltx_text ltx_font_bold">)</span></span></span>
</span></span>
<span id="S5.T1.8.8.8.8.8.8.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.8.8.8.8.8.8.8.8.4.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.8.8.8.8.8.8.8.8.4.1.2" class="ltx_tr">
<span id="S5.T1.8.8.8.8.8.8.8.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">91.54Â±0.76</span></span>
<span id="S5.T1.8.8.8.8.8.8.8.8.4.1.1" class="ltx_tr">
<span id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(2.48<math id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1a"><mo id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1.1" xref="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1b"><times id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1.1.cmml" xref="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.8.8.8.8.8.8.4.1.1.1.m1.1c">\times</annotation></semantics></math>)</span></span>
</span></span>
<span id="S5.T1.9.9.9.9.9.9.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.9.9.9.9.9.9.9.9.5.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.9.9.9.9.9.9.9.9.5.1.2" class="ltx_tr">
<span id="S5.T1.9.9.9.9.9.9.9.9.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">92.24Â±0.71</span></span>
<span id="S5.T1.9.9.9.9.9.9.9.9.5.1.1" class="ltx_tr">
<span id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(2.46<math id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1a"><mo id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1.1" xref="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1b"><times id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1.1.cmml" xref="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.9.9.9.9.9.9.5.1.1.1.m1.1c">\times</annotation></semantics></math>)</span></span>
</span></span>
<span id="S5.T1.10.10.10.10.10.10.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.10.10.10.10.10.10.10.10.6.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T1.10.10.10.10.10.10.10.10.6.1.2" class="ltx_tr">
<span id="S5.T1.10.10.10.10.10.10.10.10.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">92.34Â±0.58</span></span>
<span id="S5.T1.10.10.10.10.10.10.10.10.6.1.1" class="ltx_tr">
<span id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(2.48<math id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1a"><mo id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1.1" xref="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1b"><times id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1.1.cmml" xref="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.10.10.10.10.10.10.6.1.1.1.m1.1c">\times</annotation></semantics></math>)</span></span>
</span></span></span>
</span>
</span></span></span>
</span></span></span>


<span id="S5.T1.10.11" class="ltx_ERROR undefined">\Description</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Accuracy, hypernetwork size and hypernetwork training time for different <math id="S5.T1.12.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T1.12.m1.1b"><mi id="S5.T1.12.m1.1.1" xref="S5.T1.12.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T1.12.m1.1c"><ci id="S5.T1.12.m1.1.1.cmml" xref="S5.T1.12.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.m1.1d">k</annotation></semantics></math>.</figcaption>
</figure>
<figure id="S5.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x12.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="438" height="119" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Impact of hypernetwork-based Batch Norm (BN) and Fully Connected (FC) layer weight generation.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F10.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S5.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.03086/assets/x13.png" id="S5.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="276" height="33" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf1.4.2.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S5.F11.sf1.2.1" class="ltx_text" style="font-size:80%;">Weights matrix from original model (ground truth) and weights generated by hypernetwork for a single client with varying <math id="S5.F11.sf1.2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.F11.sf1.2.1.m1.1b"><mi id="S5.F11.sf1.2.1.m1.1.1" xref="S5.F11.sf1.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F11.sf1.2.1.m1.1c"><ci id="S5.F11.sf1.2.1.m1.1.1.cmml" xref="S5.F11.sf1.2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F11.sf1.2.1.m1.1d">k</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.03086/assets/x14.png" id="S5.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="276" height="62" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F11.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S5.F11.sf2.3.2" class="ltx_text" style="font-size:80%;">Difference between generated and original weights measured in MAE</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Visualization plots of weights (and differences) from original model and hypernetwork.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S5.F11.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S5.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x15.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="322" height="134" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>MAE between ground truth and generated weights with varying number of singular values <math id="S5.F12.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.F12.2.m1.1b"><mi id="S5.F12.2.m1.1.1" xref="S5.F12.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F12.2.m1.1c"><ci id="S5.F12.2.m1.1.1.cmml" xref="S5.F12.2.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F12.2.m1.1d">k</annotation></semantics></math>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F12.3" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Weight Generation for Batch Norm and Fully Connected Layers</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">In <span id="S5.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, the hypernetwork focuses on generating weights exclusively for convolutional layers between two exit points. Batch normalization (BN) and fully connected (FC) layers utilize aggregated client information without weight generation to minimize computational and memory overhead. To assess the impact of this design choice on model accuracy, we present results for different hypernetwork-based weight generation scenarios in FigureÂ <a href="#S5.F10" title="Figure 10 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">Overall, as shown in the left of FigureÂ <a href="#S5.F10" title="Figure 10 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, with increasing training rounds, the global model accuracy does not exhibit a significant difference between scenarios where the hypernetwork generates BN and FC layer weights and the default scenario where it only generates convolutional layer weights. Additionally, the right plots of FigureÂ <a href="#S5.F10" title="Figure 10 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> show that personalized accuracy (i.e., accuracy of local model at each client) is highest when weights are generated only for the convolutional layers.</p>
</div>
<div id="S5.SS3.SSS2.p3" class="ltx_para">
<p id="S5.SS3.SSS2.p3.1" class="ltx_p">We hypothesize that this counter-intuitive observation arises from the increased number of denominators in the aggregation process due to the generated weights, thereby reducing the impact of model personalization-related information (e.g., locally trained BN and FC layers). In contrast, the feature extractor (convolutional layers) focuses on extracting universally applicable features across client modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Lubana etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2022</a>; Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2021</a>; Hu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, maximizing the benefits of weight generation.</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Impact of Rank on SVD Operations</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.4" class="ltx_p">In <span id="S5.SS3.SSS3.p1.4.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, instead of generating weights for the entire model, we employ a low-rank factorization approach based on Singular Value Decomposition (SVD). This method selects the top <math id="S5.SS3.SSS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p1.1.m1.1a"><mi id="S5.SS3.SSS3.p1.1.m1.1.1" xref="S5.SS3.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p1.1.m1.1b"><ci id="S5.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p1.1.m1.1c">k</annotation></semantics></math> ranked parameters for weight generation using the hypernetwork. A larger <math id="S5.SS3.SSS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p1.2.m2.1a"><mi id="S5.SS3.SSS3.p1.2.m2.1.1" xref="S5.SS3.SSS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p1.2.m2.1b"><ci id="S5.SS3.SSS3.p1.2.m2.1.1.cmml" xref="S5.SS3.SSS3.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p1.2.m2.1c">k</annotation></semantics></math> enhances model accuracy but increases computational and memory overhead, while a smaller <math id="S5.SS3.SSS3.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p1.3.m3.1a"><mi id="S5.SS3.SSS3.p1.3.m3.1.1" xref="S5.SS3.SSS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p1.3.m3.1b"><ci id="S5.SS3.SSS3.p1.3.m3.1.1.cmml" xref="S5.SS3.SSS3.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p1.3.m3.1c">k</annotation></semantics></math> reduces overhead at the cost of potential performance decrease. Balancing between computational efficiency and model accuracy is crucial in practical federated learning settings with heterogeneous client resources. By tuning <math id="S5.SS3.SSS3.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p1.4.m4.1a"><mi id="S5.SS3.SSS3.p1.4.m4.1.1" xref="S5.SS3.SSS3.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p1.4.m4.1b"><ci id="S5.SS3.SSS3.p1.4.m4.1.1.cmml" xref="S5.SS3.SSS3.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p1.4.m4.1c">k</annotation></semantics></math>, <span id="S5.SS3.SSS3.p1.4.2" class="ltx_text ltx_font_italic">HypeMeFed</span> can accommodate varying device constraints while maintaining high performance and efficient resource utilization.</p>
</div>
<div id="S5.SS3.SSS3.p2" class="ltx_para">
<p id="S5.SS3.SSS3.p2.3" class="ltx_p">To examine the impact of <math id="S5.SS3.SSS3.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p2.1.m1.1a"><mi id="S5.SS3.SSS3.p2.1.m1.1.1" xref="S5.SS3.SSS3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p2.1.m1.1b"><ci id="S5.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS3.SSS3.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p2.1.m1.1c">k</annotation></semantics></math> we take a deeper look into this tradeoff using TableÂ <a href="#S5.T1" title="Table 1 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As the results in the table show for the CNN baseline model and the UbiMiB dataset, the model compression process does show a performance degradation in accuracy performance. Nevertheless, we can notice that this loss is minimal even for a very small <math id="S5.SS3.SSS3.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p2.2.m2.1a"><mi id="S5.SS3.SSS3.p2.2.m2.1.1" xref="S5.SS3.SSS3.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p2.2.m2.1b"><ci id="S5.SS3.SSS3.p2.2.m2.1.1.cmml" xref="S5.SS3.SSS3.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p2.2.m2.1c">k</annotation></semantics></math> (e.g., 1.89% loss for <math id="S5.SS3.SSS3.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p2.3.m3.1a"><mi id="S5.SS3.SSS3.p2.3.m3.1.1" xref="S5.SS3.SSS3.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p2.3.m3.1b"><ci id="S5.SS3.SSS3.p2.3.m3.1.1.cmml" xref="S5.SS3.SSS3.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p2.3.m3.1c">k</annotation></semantics></math>=25). At the same time, the reduction in parameters and the size of the hypernetwork reduces by nearly two orders of magnitude. Naturally, this reduces the computation latency to a minimal level as well.</p>
</div>
<div id="S5.SS3.SSS3.p3" class="ltx_para">
<p id="S5.SS3.SSS3.p3.3" class="ltx_p">On a different dimension, FigureÂ <a href="#S5.F11" title="Figure 11 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> visualizes the convolutional layer parameters of the CNN model trained with the UniMiB dataset from a client (i.e., â€œoriginalâ€) and compares them with random weights and hypernetwork-generated weights with different <math id="S5.SS3.SSS3.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p3.1.m1.1a"><mi id="S5.SS3.SSS3.p3.1.m1.1.1" xref="S5.SS3.SSS3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p3.1.m1.1b"><ci id="S5.SS3.SSS3.p3.1.m1.1.1.cmml" xref="S5.SS3.SSS3.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p3.1.m1.1c">k</annotation></semantics></math> values. FigureÂ <a href="#S5.F11" title="Figure 11 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>Â (a) shows the actual weight values, while FigureÂ <a href="#S5.F11" title="Figure 11 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>Â (b) displays the differences against the original weights. As illustrated, a hypernetwork with a higher <math id="S5.SS3.SSS3.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p3.2.m2.1a"><mi id="S5.SS3.SSS3.p3.2.m2.1.1" xref="S5.SS3.SSS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p3.2.m2.1b"><ci id="S5.SS3.SSS3.p3.2.m2.1.1.cmml" xref="S5.SS3.SSS3.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p3.2.m2.1c">k</annotation></semantics></math> exhibits fewer differences from the original weights, whereas even a small <math id="S5.SS3.SSS3.p3.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p3.3.m3.1a"><mi id="S5.SS3.SSS3.p3.3.m3.1.1" xref="S5.SS3.SSS3.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p3.3.m3.1b"><ci id="S5.SS3.SSS3.p3.3.m3.1.1.cmml" xref="S5.SS3.SSS3.p3.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p3.3.m3.1c">k</annotation></semantics></math> demonstrates noticeable improvements over random weights.</p>
</div>
<div id="S5.SS3.SSS3.p4" class="ltx_para">
<p id="S5.SS3.SSS3.p4.3" class="ltx_p">To validate this observation, FigureÂ <a href="#S5.F12" title="Figure 12 â€£ 5.3.1. Impact of Weight Generation â€£ 5.3. Impact of Hypernetworks â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the Mean Absolute Error (MAE) of the weights for different <math id="S5.SS3.SSS3.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p4.1.m1.1a"><mi id="S5.SS3.SSS3.p4.1.m1.1.1" xref="S5.SS3.SSS3.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p4.1.m1.1b"><ci id="S5.SS3.SSS3.p4.1.m1.1.1.cmml" xref="S5.SS3.SSS3.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p4.1.m1.1c">k</annotation></semantics></math>. Quantitatively, the results confirm that as <math id="S5.SS3.SSS3.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p4.2.m2.1a"><mi id="S5.SS3.SSS3.p4.2.m2.1.1" xref="S5.SS3.SSS3.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p4.2.m2.1b"><ci id="S5.SS3.SSS3.p4.2.m2.1.1.cmml" xref="S5.SS3.SSS3.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p4.2.m2.1c">k</annotation></semantics></math> increases, the hypernetwork-generated weights closely approximate the original, with the scenario where the hypernetwork generates all weights showing the lowest MAE. This indicates that leveraging a hypernetwork with an appropriate <math id="S5.SS3.SSS3.p4.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.SSS3.p4.3.m3.1a"><mi id="S5.SS3.SSS3.p4.3.m3.1.1" xref="S5.SS3.SSS3.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS3.p4.3.m3.1b"><ci id="S5.SS3.SSS3.p4.3.m3.1.1.cmml" xref="S5.SS3.SSS3.p4.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS3.p4.3.m3.1c">k</annotation></semantics></math> effectively balances computational efficiency and model accuracy, thereby supporting robust federated learning across heterogeneous clients.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Applicability to Large Models</h3>

<figure id="S5.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x16.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="345" height="132" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Performance of <span id="S5.F13.2.1" class="ltx_text ltx_font_italic">HypeMeFed</span> with ResNet18.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F13.3" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Experiments conducted so far utilize a relatively small-sized model with a limited number of convolutional layers. The next question we answer is whether <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> can scale to larger and deeper models. For this, we examine the performance with (i) a more complex model with extensive parameters and (ii) models with deeper layers. Specifically, to test a model with extensive parameters, we conducted experiments with ResNet18Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite>â€”a relatively large model with 11.5M parameters, featuring a residual connection architecture. We used the CIFAR100 datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2009</a>)</cite>, containing 50,000 image samples across 100 categories. Three intermediate multi-exit layers were added to the baseline ResNet18 model, splitting it depth-wise into three segments.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.3" class="ltx_p">FigureÂ <a href="#S5.F13" title="Figure 13 â€£ 5.4. Applicability to Large Models â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> plots the global model accuracy trend with increasing federated learning rounds. The results suggest that <span id="S5.SS4.p2.3.1" class="ltx_text ltx_font_italic">HypeMeFed</span> is applicable to larger models, as it successfully converges and shows improved model accuracy compared to the baseline where all users utilize the smallest model (FedAvg-S in Fig.Â <a href="#S5.F13" title="Figure 13 â€£ 5.4. Applicability to Large Models â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>). In fact, <span id="S5.SS4.p2.3.2" class="ltx_text ltx_font_italic">HypeMeFed</span> achieves performance comparable to the upper bound assuming all clients can utilize the largest model (i.e., FedAvg-L). Here, training the full-rank hypernetwork for ResNet18 requires more than 44.74Â GB (<math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mo id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">\sim</annotation></semantics></math>5.59B parameters) of memory, which is infeasible and computationally extensive. However, with <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">k</annotation></semantics></math>=100 for the hypernetwork optimizations, only 113.28Â MB (<math id="S5.SS4.p2.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS4.p2.3.m3.1a"><mo id="S5.SS4.p2.3.m3.1.1" xref="S5.SS4.p2.3.m3.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.3.m3.1b"><csymbol cd="latexml" id="S5.SS4.p2.3.m3.1.1.cmml" xref="S5.SS4.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.3.m3.1c">\sim</annotation></semantics></math>14M parameters) was needed, reducing the memory usage by 99.87%. These results demonstrate that <span id="S5.SS4.p2.3.3" class="ltx_text ltx_font_italic">HypeMeFed</span> can effectively scale to larger models while maintaining high accuracy and significantly reducing memory overhead.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Note that, <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> adopts an auto-regressive approach to generate subsequent layer weights, utilizing either preceding layer weights or previously generated weights. Thus, investigating the influence of model depth, particularly the number of exit layers, becomes pivotal. To explore this, we employed a basic CNN model with 10 exit layers on the FashionMNIST dataset, accommodating varying levels of client device capabilities. We use this dataset given the number of classes and subnetworks that need to be generated for deep networks. Training involved five clients per model capacity (i.e., number of exits in the model), encompassing a total of 50 clients participating in the training phase.</p>
</div>
<figure id="S5.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.03086/assets/x17.png" id="S5.F14.sf1.g1" class="ltx_graphics ltx_img_landscape" width="360" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S5.F14.sf1.3.2" class="ltx_text" style="font-size:80%;">Normalized MAE between generated weights and original weights in different layers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.03086/assets/x18.png" id="S5.F14.sf2.g1" class="ltx_graphics ltx_img_landscape" width="360" height="95" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F14.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S5.F14.sf2.3.2" class="ltx_text" style="font-size:80%;">MAE between generated weights and original weights in the final layer with respect to the initial layer.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Weight generation errors in deep model.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><span id="S5.F14.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.3" class="ltx_p">FigureÂ <a href="#S5.F14" title="Figure 14 â€£ 5.4. Applicability to Large Models â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>Â (a) plots the normalized Mean Absolute Error (MAE) between the original and generated weights with respect to layer depth. Note that we present normalized MAE given that the magnitude of the weight norm differs across layers.
As the results show, the normalized MAE tends to increase with layer depth despite generating all weights (<math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mi id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><ci id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">k</annotation></semantics></math>=Full). This trend likely arises from the challenge of accurately capturing global features in deeper layers, as discussed in previous studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>; Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2024b</a>)</cite>. Despite this observation, <span id="S5.SS4.p4.3.1" class="ltx_text ltx_font_italic">HypeMeFed</span> achieves a higher global accuracy of 89.47% (+3.71%) compared to the FedAvg-S (85.76%), where all clients use the lightest (1-exit) model. FigureÂ <a href="#S5.F14" title="Figure 14 â€£ 5.4. Applicability to Large Models â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>Â (b) plots the MAE between original and generated weights at the final convolution layer. Given an auto-regressive generation approach, where we predict <math id="S5.SS4.p4.2.m2.1" class="ltx_Math" alttext="n^{th}" display="inline"><semantics id="S5.SS4.p4.2.m2.1a"><msup id="S5.SS4.p4.2.m2.1.1" xref="S5.SS4.p4.2.m2.1.1.cmml"><mi id="S5.SS4.p4.2.m2.1.1.2" xref="S5.SS4.p4.2.m2.1.1.2.cmml">n</mi><mrow id="S5.SS4.p4.2.m2.1.1.3" xref="S5.SS4.p4.2.m2.1.1.3.cmml"><mi id="S5.SS4.p4.2.m2.1.1.3.2" xref="S5.SS4.p4.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.2.m2.1.1.3.1" xref="S5.SS4.p4.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS4.p4.2.m2.1.1.3.3" xref="S5.SS4.p4.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.2.m2.1b"><apply id="S5.SS4.p4.2.m2.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p4.2.m2.1.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1">superscript</csymbol><ci id="S5.SS4.p4.2.m2.1.1.2.cmml" xref="S5.SS4.p4.2.m2.1.1.2">ğ‘›</ci><apply id="S5.SS4.p4.2.m2.1.1.3.cmml" xref="S5.SS4.p4.2.m2.1.1.3"><times id="S5.SS4.p4.2.m2.1.1.3.1.cmml" xref="S5.SS4.p4.2.m2.1.1.3.1"></times><ci id="S5.SS4.p4.2.m2.1.1.3.2.cmml" xref="S5.SS4.p4.2.m2.1.1.3.2">ğ‘¡</ci><ci id="S5.SS4.p4.2.m2.1.1.3.3.cmml" xref="S5.SS4.p4.2.m2.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.2.m2.1c">n^{th}</annotation></semantics></math> layer weights using <math id="S5.SS4.p4.3.m3.1" class="ltx_Math" alttext="(n-1)^{th}" display="inline"><semantics id="S5.SS4.p4.3.m3.1a"><msup id="S5.SS4.p4.3.m3.1.1" xref="S5.SS4.p4.3.m3.1.1.cmml"><mrow id="S5.SS4.p4.3.m3.1.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p4.3.m3.1.1.1.1.2" xref="S5.SS4.p4.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p4.3.m3.1.1.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.1.1.cmml"><mi id="S5.SS4.p4.3.m3.1.1.1.1.1.2" xref="S5.SS4.p4.3.m3.1.1.1.1.1.2.cmml">n</mi><mo id="S5.SS4.p4.3.m3.1.1.1.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S5.SS4.p4.3.m3.1.1.1.1.1.3" xref="S5.SS4.p4.3.m3.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S5.SS4.p4.3.m3.1.1.1.1.3" xref="S5.SS4.p4.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S5.SS4.p4.3.m3.1.1.3" xref="S5.SS4.p4.3.m3.1.1.3.cmml"><mi id="S5.SS4.p4.3.m3.1.1.3.2" xref="S5.SS4.p4.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p4.3.m3.1.1.3.1" xref="S5.SS4.p4.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS4.p4.3.m3.1.1.3.3" xref="S5.SS4.p4.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.3.m3.1b"><apply id="S5.SS4.p4.3.m3.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.p4.3.m3.1.1.2.cmml" xref="S5.SS4.p4.3.m3.1.1">superscript</csymbol><apply id="S5.SS4.p4.3.m3.1.1.1.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1"><minus id="S5.SS4.p4.3.m3.1.1.1.1.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.1.1"></minus><ci id="S5.SS4.p4.3.m3.1.1.1.1.1.2.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.1.2">ğ‘›</ci><cn type="integer" id="S5.SS4.p4.3.m3.1.1.1.1.1.3.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S5.SS4.p4.3.m3.1.1.3.cmml" xref="S5.SS4.p4.3.m3.1.1.3"><times id="S5.SS4.p4.3.m3.1.1.3.1.cmml" xref="S5.SS4.p4.3.m3.1.1.3.1"></times><ci id="S5.SS4.p4.3.m3.1.1.3.2.cmml" xref="S5.SS4.p4.3.m3.1.1.3.2">ğ‘¡</ci><ci id="S5.SS4.p4.3.m3.1.1.3.3.cmml" xref="S5.SS4.p4.3.m3.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.3.m3.1c">(n-1)^{th}</annotation></semantics></math> layer weights, we assess the accumulated error by comparing MAE across different initial layers used to generate the final layer. Notice that the MAE remains consistent across initiating layers, indicating the robustness of the proposed hypernetwork.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Evaluation on Real-World Testbed</h3>

<figure id="S5.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.03086/assets/x19.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="103" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Per round latency and global model accuracy measured from heterogeneous client testbed.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F15.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">To demonstrate the practicality of <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span> in real-world scenarios, we evaluated <span id="S5.SS5.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> on a testbed of heterogeneous embedded platforms: 4 Raspberry Pi 4, 4 Nvidia Jetson Nano, and 4 NVIDIA Jetson TX2, totaling 12 devices with varying computational resourcesâ€”weak, moderate, and ample, respectively. Additionally, we used a server with an Nvidia RTX 3090 GPU, an Intel i9-K@3.6GHz CPU, and 64GB RAM. We test the UniMiB dataset with a VGG model, where each client holds a non-overlapping dataset distributed following the Dirichlet distribution (<math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">\alpha</annotation></semantics></math>=0.5). FedAvg-S and FedAvg-L universally used single- and three-layered models, respectively, while clients in <span id="S5.SS5.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> used models suitable for each deviceâ€™s resources. Unless otherwise specified, all configurations matched those mentioned in SectionÂ <a href="#S5" title="5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.2" class="ltx_p">FigureÂ <a href="#S5.F15" title="Figure 15 â€£ 5.5. Evaluation on Real-World Testbed â€£ 5. Evaluation â€£ Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> plots the latency per federated learning round for different client platforms along with the server-side latency (left) and the global model accuracy for FedAvg-L, FedAvg-S, and <span id="S5.SS5.p2.2.1" class="ltx_text ltx_font_italic">HypeMeFed</span> (right). With all clients using the full model, FedAvg-L achieves high accuracy, but the Raspberry Pi shows prolonged latency in processing this model, potentially delaying the overall federated learning process. In FedAvg-S, latency is kept low, but using a single-layer model causes a 20.41% accuracy drop. By applying <span id="S5.SS5.p2.2.2" class="ltx_text ltx_font_italic">HypeMeFed</span> and adaptively configuring models for each device, we balance training latency and accuracy, outperforming FedAvg-S by 5.13%. Note a naive hypernetwork induces 3.9Â sec of server computation time and 5.6Â GB of memory overhead, while our optimizations (<math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><mi id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><ci id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">k</annotation></semantics></math>=100) reduce this to 2.1Â sec (1.86<math id="S5.SS5.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS5.p2.2.m2.1a"><mo id="S5.SS5.p2.2.m2.1.1" xref="S5.SS5.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.2.m2.1b"><times id="S5.SS5.p2.2.m2.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.2.m2.1c">\times</annotation></semantics></math> speedup) and 100.8Â MB (98.22% reduction), agreeing with our previous results.
Overall, our testbed validations confirm that <span id="S5.SS5.p2.2.3" class="ltx_text ltx_font_italic">HypeMeFed</span> can be a practical and efficient framework for supporting federated learning on heterogeneous clients.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We now compile a set of notable discussion points for future research that we have identified through our research.</p>
</div>
<div id="S6.p2" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Optimizing hypernetwork architectures and performance.</span> In this work, we demonstrated the feasibility of exploiting hypernetworks in federated learning through <span id="S6.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span>. While <span id="S6.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> effectively addresses the complexity issues by leveraging SVD-based low-rank factorization, research on hypernetworks is still in its early stages. We identify two potential research directions. First, <span id="S6.I1.i1.p1.1.4" class="ltx_text ltx_font_italic">HypeMeFed</span> utilizes the same hypernetwork architecture across all layers. Exploring optimal configurations for each layer may further enhance hypernetwork performance. Second, despite reduced complexity, training hypernetworks introduces server-side latency, delaying the federated learning process. Developing efficient hypernetwork training algorithms or suitable alternatives could mitigate this issue and improve overall system performance.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Diversifying model architectures.</span> In this work, we focused our evaluations on CNN-based architectures. However, our experiences show that <span id="S6.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> can also support other architectures, such as MLPs, RNNs, and LSTMs, by effectively generating weights using its hypernetwork. As we expand our research to include more complex neural network architectures, such as Transformers, we anticipate that the complexity of the hypernetwork will present significant challenges that needs to be addressed.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">HypeMeFed<span id="S6.I1.i3.p1.1.1.1" class="ltx_text ltx_font_upright"> as an integrated system.</span></span> <span id="S6.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> can be used as an orthogonal scheme with other federated learning frameworks. Specifically, <span id="S6.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> does not employ pruning or quantization to the model, allowing for straightforward integration with various schemes. However, this work primarily focuses on exploring the feasibility of <span id="S6.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">HypeMeFed</span>, and we did not investigate the potential benefits of such integration as a comprehensive solution. Moving forward, we believe that enhancing <span id="S6.I1.i3.p1.1.5" class="ltx_text ltx_font_italic">HypeMeFed</span> could involve leveraging our proposed techniques to address both systemic and statistical heterogeneity effectively.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work presents <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">HypeMeFed</span>, a novel framework designed to address the challenges of device heterogeneity in federated learning. By combining multi-exit network architectures with hypernetwork-based model weight generation, <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">HypeMeFed</span> effectively resolves the feature space misalignment between heterogeneous models and the per-layer information disparity issue during weight aggregation. Furthermore, the proposed low rank factorization-based hypernetwork optimization minimizes computational and memory overhead, making hypernetworks feasible (and applicable) for real-world use. Our evaluations with GPU emulations and a real-world heterogeneous embedded platform testbed confirm that <span id="S7.p1.1.3" class="ltx_text ltx_font_italic">HypeMeFed</span> significantly improves model accuracy and can effectively engage heterogeneous clients in federated learning, proving its practicality in real applications.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank Jaeho Jin for setting up the testbed and Sinyoung Oh for designing the figures.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang. 2022.

</span>
<span class="ltx_bibblock">Fedrolex: Model-heterogeneous federated learning with rolling sub-model extraction.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 35 (2022), 29677â€“29690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub KoneÄná»³, Stefano Mazzocchi, Brendan McMahan, etÂ al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">Proceedings of machine learning and systems</em> 1 (2019), 374â€“388.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyunsung Cho, Akhil Mathur, and Fahim Kawsar. 2022.

</span>
<span class="ltx_bibblock">Flame: Federated learning across multi-device environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 6, 3 (2022), 1â€“29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coates etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Adam Coates, Andrew Ng, and Honglak Lee. 2011.

</span>
<span class="ltx_bibblock">An analysis of single-layer networks in unsupervised feature learning. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the fourteenth international conference on artificial intelligence and statistics</em>. JMLR Workshop and Conference Proceedings, 215â€“223.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coucke etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Alice Coucke, Alaa Saade, Adrien Ball, ThÃ©odore Bluche, Alexandre Caulier, David Leroy, ClÃ©ment Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, etÂ al<span id="bib.bib6.3.1" class="ltx_text">.</span> 2018.

</span>
<span class="ltx_bibblock">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.10190</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding, and Vahid Tarokh. 2020.

</span>
<span class="ltx_bibblock">Heterofl: Computation and communication efficient federated learning for heterogeneous clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01264</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evci etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin. 2022.

</span>
<span class="ltx_bibblock">Gradient flow in sparse neural networks and how lottery tickets win. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, Vol.Â 36. 6577â€“6586.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2018)</span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin. 2018.

</span>
<span class="ltx_bibblock">The lottery ticket hypothesis: Finding sparse, trainable neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.03635</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gim and Ko (2022)</span>
<span class="ltx_bibblock">
In Gim and JeongGil Ko. 2022.

</span>
<span class="ltx_bibblock">Memory-efficient dnn training on mobile devices. In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services</em>. 464â€“476.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
David Ha, Andrew Dai, and QuocÂ V Le. 2016.

</span>
<span class="ltx_bibblock">Hypernetworks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.09106</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Federated learning of large cnns at the edge.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 14068â€“14080.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 770â€“778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yihui He, Xiangyu Zhang, and Jian Sun. 2017.

</span>
<span class="ltx_bibblock">Channel pruning for accelerating very deep neural networks. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>. 1389â€“1397.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane. 2021.

</span>
<span class="ltx_bibblock">Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 12876â€“12889.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Tzu-MingÂ Harry Hsu, Hang Qi, and Matthew Brown. 2019.

</span>
<span class="ltx_bibblock">Measuring the effects of non-identical data distribution for federated visual classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06335</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Erdong Hu, Yuxin Tang, Anastasios Kyrillidis, and Chris Jermaine. 2023.

</span>
<span class="ltx_bibblock">Federated learning over images: vertical decompositions and pre-trained backbones are difficult to beat. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 19385â€“19396.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huynh etÂ al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sinh Huynh, RajeshÂ Krishna Balan, JeongGil Ko, and Youngki Lee. 2019.

</span>
<span class="ltx_bibblock">VitaMon: measuring heart rate variability using smartphone front camera. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference on Embedded Networked Sensor Systems</em>. 1â€“14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilhan etÂ al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Fatih Ilhan, Gong Su, and Ling Liu. 2023.

</span>
<span class="ltx_bibblock">Scalefl: Resource-adaptive federated learning with heterogeneous clients. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 24532â€“24541.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Xu Jia, Bert DeÂ Brabandere, Tinne Tuytelaars, and LucÂ V Gool. 2016.

</span>
<span class="ltx_bibblock">Dynamic filter networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 29 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wenjun Jiang, Chenglin Miao, Fenglong Ma, Shuochao Yao, Yaqing Wang, Ye Yuan, Hongfei Xue, Chen Song, Xin Ma, Dimitrios Koutsonikolas, etÂ al<span id="bib.bib22.3.1" class="ltx_text">.</span> 2018.

</span>
<span class="ltx_bibblock">Towards environment independent device free human activity recognition. In <em id="bib.bib22.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th annual international conference on mobile computing and networking</em>. 289â€“304.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy etÂ al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
SaiÂ Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and AnandaÂ Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 5132â€“5143.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Minjae Kim, Sangyoon Yu, Suhyun Kim, and Soo-Mook Moon. 2023.

</span>
<span class="ltx_bibblock">DepthFL: Depthwise federated learning for heterogeneous clients. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Youngwoo Kim, Donghyeon Han, Changhyeon Kim, and Hoi-Jun Yoo. 2020.

</span>
<span class="ltx_bibblock">A 0.22â€“0.89 mW low-power and highly-secure always-on face recognition processor with adversarial attack prevention.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems II: Express Briefs</em> 67, 5 (2020), 846â€“850.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
DiederikÂ P Kingma and Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klocek etÂ al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sylwester Klocek, Åukasz Maziarka, Maciej WoÅ‚czyk, Jacek Tabor, Jakub Nowak, and Marek Åšmieja. 2019.

</span>
<span class="ltx_bibblock">Hypernetwork functional image representation. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Neural Networks</em>. Springer, 496â€“510.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornblith etÂ al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. 2019.

</span>
<span class="ltx_bibblock">Similarity of neural network representations revisited. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 3519â€“3529.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kouris etÂ al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Alexandros Kouris, StylianosÂ I Venieris, Stefanos Laskaridis, and NicholasÂ D Lane. 2022.

</span>
<span class="ltx_bibblock">Adaptable mobile vision systems through multi-exit neural networks. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services</em>. 575â€“576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, etÂ al<span id="bib.bib30.3.1" class="ltx_text">.</span> 2009.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">(2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskaridis etÂ al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Stefanos Laskaridis, Alexandros Kouris, and NicholasÂ D Lane. 2021.

</span>
<span class="ltx_bibblock">Adaptive inference through early-exit networks: Design, challenges and directions. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning</em>. 1â€“6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Kichang Lee, Songkuk Kim, and JeongGil Ko. 2024b.

</span>
<span class="ltx_bibblock">Improving Local Training in Federated Learning via Temperature Scaling.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.09986v2</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Royson Lee, Javier Fernandez-Marques, ShellÂ Xu Hu, Da Li, Stefanos Laskaridis, Åukasz Dudziak, Timothy Hospedales, Ferenc HuszÃ¡r, and NicholasÂ D Lane. 2024a.

</span>
<span class="ltx_bibblock">Recurrent Early Exits for Federated Learning with Heterogeneous Clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.14791</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leontiadis etÂ al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ilias Leontiadis, Stefanos Laskaridis, StylianosÂ I Venieris, and NicholasÂ D Lane. 2021.

</span>
<span class="ltx_bibblock">Itâ€™s always personal: Using early exits for efficient on-device CNN personalisation. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications</em>. 15â€“21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, and Yiran Chen. 2021a.

</span>
<span class="ltx_bibblock">Hermes: an efficient federated learning framework for heterogeneous mobile clients. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th Annual International Conference on Mobile Computing and Networking</em>. 420â€“437.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. 2020b.

</span>
<span class="ltx_bibblock">Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.03371</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. 2021b.

</span>
<span class="ltx_bibblock">Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems</em>. 42â€“55.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang (2019)</span>
<span class="ltx_bibblock">
Daliang Li and Junpu Wang. 2019.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2020a.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine learning and systems</em> 2 (2020), 429â€“450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. 2022.

</span>
<span class="ltx_bibblock">On-device training under 256kb memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 35 (2022), 22941â€“22954.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, SebastianÂ U Stich, and Martin Jaggi. 2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 33 (2020), 2351â€“2363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Littwin and Wolf (2019)</span>
<span class="ltx_bibblock">
Gidi Littwin and Lior Wolf. 2019.

</span>
<span class="ltx_bibblock">Deep meta functionals for shape representation. In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 1824â€“1833.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lubana etÂ al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
EkdeepÂ Singh Lubana, ChiÂ Ian Tang, Fahim Kawsar, RobertÂ P Dick, and Akhil Mathur. 2022.

</span>
<span class="ltx_bibblock">Orchestra: Unsupervised federated learning via globally consistent clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.11506</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. 2021.

</span>
<span class="ltx_bibblock">No fear of heterogeneity: Classifier calibration for federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 5972â€“5984.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022.

</span>
<span class="ltx_bibblock">Layer-wised model aggregation for personalized federated learning. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10092â€“10101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera y Arcas. 2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>. PMLR, 1273â€“1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micucci etÂ al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Daniela Micucci, Marco Mobilio, and Paolo Napoletano. 2017.

</span>
<span class="ltx_bibblock">Unimib shar: A dataset for human activity recognition using acceleration data from smartphones.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 7, 10 (2017), 1101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Netzer etÂ al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, AndrewÂ Y Ng, etÂ al<span id="bib.bib48.3.1" class="ltx_text">.</span> 2011.

</span>
<span class="ltx_bibblock">Reading digits in natural images with unsupervised feature learning. In <em id="bib.bib48.4.1" class="ltx_emph ltx_font_italic">NIPS workshop on deep learning and unsupervised feature learning</em>, Vol.Â 2011. Granada, 4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaomin Ouyang, Zhiyuan Xie, Heming Fu, Sitong Cheng, Li Pan, Neiwen Ling, Guoliang Xing, Jiayu Zhou, and Jianwei Huang. 2023.

</span>
<span class="ltx_bibblock">Harmony: Heterogeneous multi-modal federated learning through disentangled model training. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services</em>. 530â€“543.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
HyeonJung Park, Youngki Lee, and JeongGil Ko. 2021.

</span>
<span class="ltx_bibblock">Enabling real-time sign language translation on mobile platforms with on-board depth cameras.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 5, 2 (2021), 1â€“30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jaeyeon Park, Hyeon Cho, RajeshÂ Krishna Balan, and JeongGil Ko. 2020.

</span>
<span class="ltx_bibblock">Heartquake: Accurate low-cost non-invasive ecg monitoring using bed-mounted geophones.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 4, 3 (2020), 1â€“28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park and Ko (2024)</span>
<span class="ltx_bibblock">
JaeYeon Park and JeongGil Ko. 2024.

</span>
<span class="ltx_bibblock">FedHM: Practical federated learning for heterogeneous model deployments.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ICT Express</em> 10, 2 (2024), 387â€“392.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
JaeYeon Park, Kichang Lee, Sungmin Lee, Mi Zhang, and JeongGil Ko. 2023.

</span>
<span class="ltx_bibblock">Attfl: A personalized federated learning framework for time-series mobile and embedded sensor data processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 3 (2023), 1â€“31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Nhat Pham, Hong Jia, Minh Tran, Tuan Dinh, Nam Bui, Young Kwon, Dong Ma, Phuc Nguyen, Cecilia Mascolo, and Tam Vu. 2022.

</span>
<span class="ltx_bibblock">PROS: an efficient pattern-driven compressive sensing framework for low-power biopotential-based wearables with on-chip intelligence. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th Annual International Conference on Mobile Computing And Networking</em>. 661â€“675.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo etÂ al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyowoon Seo, Jihong Park, Seungeun Oh, Mehdi Bennis, and Seong-Lyun Kim. 2022.

</span>
<span class="ltx_bibblock">16 federated knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Machine Learning and Wireless Communications</em> (2022), 457.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shamsian etÂ al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. 2021.

</span>
<span class="ltx_bibblock">Personalized federated learning using hypernetworks. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 9489â€“9502.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Qijia Shao, Jiting Liu, Emily Bejerano, HoÂ Man Colman, Jingping Nie, Xiaofan Jiang, and Xia Zhou. 2024.

</span>
<span class="ltx_bibblock">Joey: Supporting Kangaroo Mother Care with Computational Fabrics. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services</em>. 237â€“251.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Leming Shen, Qiang Yang, Kaiyan Cui, Yuanqing Zheng, Xiao-Yong Wei, Jianwei Liu, and Jinsong Han. 2024.

</span>
<span class="ltx_bibblock">FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated Clients. In <em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services</em>. 398â€“411.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin etÂ al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jaemin Shin, Yuanchun Li, Yunxin Liu, and Sung-Ju Lee. 2022.

</span>
<span class="ltx_bibblock">Fedbalancer: Data and pace control for efficient federated learning on heterogeneous clients. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services</em>. 436â€“449.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva etÂ al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Santiago Silva, Andre Altmann, Boris Gutman, and Marco Lorenzi. 2020.

</span>
<span class="ltx_bibblock">Fed-biomed: A general open-source frontend framework for federated learning in healthcare. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">Domain Adaptation and Representation Transfer, and Distributed and Collaborative Learning: Second MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4â€“8, 2020, Proceedings 2</em>. Springer, 201â€“210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2014)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2014.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sitzmann etÂ al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. 2020.

</span>
<span class="ltx_bibblock">Implicit neural representations with periodic activation functions.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 33 (2020), 7462â€“7473.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stisen etÂ al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Allan Stisen, Henrik Blunck, Sourav Bhattacharya, ThorÂ Siiger Prentow, MikkelÂ Baun KjÃ¦rgaard, Anind Dey, Tobias Sonne, and MadsÂ MÃ¸ller Jensen. 2015.

</span>
<span class="ltx_bibblock">Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. In <em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th ACM conference on embedded networked sensor systems</em>. 127â€“140.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teerapittayanon etÂ al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016.

</span>
<span class="ltx_bibblock">Branchynet: Fast inference via early exiting from deep neural networks. In <em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">2016 23rd international conference on pattern recognition (ICPR)</em>. IEEE, 2464â€“2469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. 2022.

</span>
<span class="ltx_bibblock">Melon: Breaking the memory wall for resource-efficient on-device machine learning. In <em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services</em>. 450â€“463.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017.

</span>
<span class="ltx_bibblock">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1708.07747</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Huatao Xu, Pengfei Zhou, Rui Tan, Mo Li, and Guobin Shen. 2021.

</span>
<span class="ltx_bibblock">Limu-bert: Unleashing the potential of unlabeled data for imu sensing applications. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems</em>. 220â€“233.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Lilin Xu, Chaojie Gu, Rui Tan, Shibo He, and Jiming Chen. 2023.

</span>
<span class="ltx_bibblock">MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Baichen Yang, Qingyong Hu, Wentao Xie, Xinchen Wang, Wei Luo, and Qian Zhang. 2023.

</span>
<span class="ltx_bibblock">PDAssess: A Privacy-preserving Free-speech based Parkinsonâ€™s Disease Daily Assessment System. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st ACM Conference on Embedded Networked Sensor Systems</em>. 251â€“264.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Dezhong Yao, Wanning Pan, MichaelÂ J Oâ€™Neill, Yutong Dai, Yao Wan, Hai Jin, and Lichao Sun. 2021.

</span>
<span class="ltx_bibblock">Fedhm: Efficient federated learning for heterogeneous models via low-rank factorization.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.14655</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yun etÂ al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jonghyuk Yun, Kyoosik Lee, Kichang Lee, Bangjie Sun, Jaeho Jeon, Jeonggil Ko, Inseok Hwang, and Jun Han. 2024.

</span>
<span class="ltx_bibblock">PowDew: Detecting Counterfeit Powdered Food Products using a Commodity Smartphone. In <em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services</em>. 210â€“222.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tuo Zhang, Lei Gao, Chaoyang He, Mi Zhang, Bhaskar Krishnamachari, and AÂ Salman Avestimehr. 2022.

</span>
<span class="ltx_bibblock">Federated learning for the internet of things: Applications, challenges, and opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em> 5, 1 (2022), 24â€“29.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.03085" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.03086" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.03086">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.03086" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.03087" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:19:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
