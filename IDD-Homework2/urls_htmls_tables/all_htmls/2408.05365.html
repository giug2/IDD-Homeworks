<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs</title>
<!--Generated on Thu Sep 19 01:22:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Hallucinations,  creativity,  LLMs,  knowledge graph,  fine-tuning
" lang="en" name="keywords"/>
<base href="/html/2408.05365v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S1" title="In FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2" title="In FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.SS1" title="In II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">LLMs for the Financial Domain</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.SS2" title="In II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">LLMs for style-transfer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.SS3" title="In II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Hallucination and Creativity in LLMs</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3" title="In FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods and Data</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS1" title="In III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Notation and Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS2" title="In III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Data Preparation and FT-process</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4" title="In FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments and Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.SS1" title="In IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Validation and Test Prompts</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.SS2" title="In IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Quantitative Analysis of Two-stage FT model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.SS3" title="In IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Qualitative Analysis: Hallucination, creativity monitoring</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S5" title="In FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusions and Discussion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sohini Roychowdhury,
Marko Krema,
Brian Moore,
Xingjian Lai,
Dike Effedua,
Bharat Jethwani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Corporate Data and Analytics Office (CDAO), Accenture LLP, USA, Email: sohini.roychowdhury@accenture.com
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Financial report generation using general purpose large language models (LLMs) pose two major challenges namely, the lack of compound sentences and hallucinations. Advanced prompt engineering and retrieval augmented generation (RAG) techniques are limited in scope for curing these writing style discrepancies. In this work we propose a novel two-stage fine-tuning (FT) process wherein public domain financial reports are processed into prompt-completions and augmented using simple LLM prompts to then enable sectional financial report generation using minimal instructions and tabular data inputs. The proposed fine-tuning process exploits the self-learning capability of LLMs by allowing hallucinations in the first stage and showing the corrections in the second stage. Our proposed fine-tuning framework results doubles the number of correct questions answers and reduces hallucinations by over 50%. Additionally, the two-stage FT model has lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity and knowledge density with lower uncertainty and cross entropy. Thus, the proposed framework can be generalized to domain specific fine-tuning tasks at minimized tuning costs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Hallucinations, creativity, LLMs, knowledge graph, fine-tuning

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have powered several question answering chat-bots and automation processes as major use-cases in the recent past. While most research advancements have been around general purpose LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib1" title="">1</a>]</cite> such as ChatGPTs, LLama, Gemini, Claude etc., domain specific products have typically benefited largely from retrieval augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib2" title="">2</a>]</cite> and limited training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib3" title="">3</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib4" title="">4</a>]</cite>. The financial domain specifically is characterized specifically with significant numerical data, data transformations, abbreviations and definitions. With the LLM advancements, the major financial tasks that can now be fulfilled include: automated financial statement analysis, personalized narrative generation for financial reports, automated tagging and labelling of financial data and reports, financial forecasting and prediction, risk management and compliance and audit processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib5" title="">5</a>]</cite>. The major notable contributions for LLMs in the domain of finance include the BloombergGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib6" title="">6</a>]</cite> that is capable of sentiment analysis, named entity recognition, and question answering when applied to financial text; and FinancialGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib4" title="">4</a>]</cite> that incorporates various financial data formats, including news, filings, social media, and company announcements into the training phase to enable creation of financial products and services and supports informed investment and risk management strategies.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While training a financial-domain specific LLM poses challenges with regards to training data quality and hardware resources needed for large scale training epochs, most of these well-established financial use-cases rely on web-search approaches combined with knowledge graphs (KGs) to ensure <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">recency</span> in the data quality and standards of responses/outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib5" title="">5</a>]</cite>. The generative AI-based approach of retrieving updated financial information and serving the analysis in domain-representative jargon, also known as the agentic-RAG, comprises of two steps. First, a web-search is orchestrated for the user-query; second from the retrieved web-link texts, paragraphs that contain relevant information are identified using the KGs. Typically, KGs are a structured representation of data or textual information and they consist of nodes (entities) and edges (relationships) that connect them. For example, a node might represent an entity such as an organization, place, or person, while an edge could represent a relationship like <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">resulted in</span>, <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">risen/fallen</span> etc. An example of KG representation from financial data is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>. The most appropriate paragraphs/KGs from the web links are then used to serve the information in a personalized and structured manner to create accurate and reliable AI systems that are capable of fact-checking, improved understanding, enhanced domain-specific reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="152" id="S1.F1.g1" src="extracted/5864498/images/KG.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of KG generation from financial text. The most pertinent KG to the user-query is retrieved for financial reporting purposes.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Although agentic-RAG processes have benefited some financial use-cases, these approaches do not scale to financial report generation tasks, wherein, the intention is to transfer the domain-specific writing style. As an experiment, we used as tabular data and English language instructions describing the persona of a financial analyst and specific instructions to GPT4o model to generate multiple paragraphs of financial reporting text. The sample output is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S1.F2" title="Figure 2 ‣ I Introduction ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>, where unwanted text is struck out by our financial domain experts. Advanced prompt engineering and instructional guardrails <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib2" title="">2</a>]</cite> for this use-case led to the following observations.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">General wordiness of the output and the use of unwanted words such as {successively, landscape of growth} did not change even after providing detailed instructions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Controlling for LLM parameters like temperature, top_p and max tokens did not improve the overall <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">quality</span> of generated text.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Increased instructions to generate multiple sentences from tabular data reduced the overall performance of text generation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Compound sentences that are atypical for the financial domain, such as contrasting sentiments regarding the same entity being presented in a single compound sentence, cannot be generated reliably using the agentic-RAG approaches.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="114" id="S1.F2.g1" src="extracted/5864498/images/F_p1.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of simple RAG-based approach using GPT4o to generate Financial Reporting text using input data as instructions and tabular sources.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">One other major drawback of LLM-based solutions/products is the occurrence of <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">fake responses</span> or <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">hallucinations</span> for prompt responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib2" title="">2</a>]</cite>. <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">Hallucinations</span> are largely caused by the uncertainty in the later layers for predicting the <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">next token/word</span> in a response sequence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib8" title="">8</a>]</cite>. Existing works in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib8" title="">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib9" title="">9</a>]</cite> have shown that hallucinations are un-repeatable occurrences caused by uncertainty in token generation and the same model parameters are noticed for <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">creative</span> responses as well. The major difference between <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">creativity</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.7">hallucinations</span> is that creative responses are true facts that may not be present in the context provided to the LLM, but they may be learned or extrapolated accurately by the LLM. Hallucinations on the other hand are also contextual bifurcations from the knowledge available to the LLM, but they are factually inaccurate. In this work, we extend the hypotheses from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib8" title="">8</a>]</cite>, to explore the self-correction capabilities of LLM. Our hypothesis is that just as early learners/children learn a new skill/creativity by exploration and making mistakes, if we allow LLMs to learn new domain-specific jargon while making mistakes/hallucinations followed by showing the LLM the mistakes it made so far, then the creativity of LLMs can be enhanced while controlling for future hallucinations. We present a multi-step fine-tuning approach for LLMs with the goal of enhanced creativity and compound sentence generation for the financial domain while exploring the self-correction and controlled hallucination responses from general-purpose LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper makes two key contributions. First, we propose a two-stage LLM fine-tuning process that minimizes hallucinations and incomplete responses, while promoting creative and compound sentences that align with the Financial reporting writing styles. We demonstrate the step-wise enhancements in the knowledge density per generated paragraph across the fine-tuning stages, while ensuring minimal fine-tuning cost of under $18 for fine tuning GPT3.5 model. Second, we propose multiple novel metrics that can assess the performance of fine-tuned LLMs that are based on KG-based approaches. These metrics enable tracking the required creativity standards per generated sentence while flagging hallucinations using “spacy”-based libraries. We test the fine-tuned model by applying a basic prompt that includes minimal instructions and tabular data shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S1.F3" title="Figure 3 ‣ I Introduction ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">3</span></a> as input and the corresponding output with creative and compound sentences is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S1.F4" title="Figure 4 ‣ I Introduction ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S1.F3.g1" src="extracted/5864498/images/p1.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Basic prompt with instructions and tabular data as input.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="S1.F4.g1" src="extracted/5864498/images/GR.png" width="658"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of proposed hallucination controlled two-stage fine-tuning for Financial Report Generation. The two major considerations in style transfer are the formation of compound sentences and creative logical sentences that are not hallucinations.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Domain-specific generative AI led automation tasks such as a financial chatbot or financial news writer, have seen specific improvements in overall knowledge retrieval tasks by using search engine and search engine along with KG capabilities as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.F5" title="Figure 5 ‣ II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">5</span></a> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib10" title="">10</a>]</cite>. The major reason for improved answering capabilities with web searches and KG isolation of text is that the LLMs follow a unique knowledge distribution, with a head, body/torso and tail <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib11" title="">11</a>]</cite>. Knowledge from the LLM head (commonly occurring and rarely changing facts) are easily retrievable with minimal hallucinations. Contrastingly, knowledge from the LLM tail (rapidly changing facts, such as share prices etc.) can lead to stale data in the responses or inaccuracies/hallucinations or “I dont know” responses from LLMs. Thus, it is imperative to teach the LLM to work with latest data and reasonably modify the text generation style to ensure lowered hallucinations and unknown responses as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib11" title="">11</a>]</cite>. A summary of latest works on the financial domain, style-transfer and detection of hallucination and creativity are presented below.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="75" id="S2.F5.g1" src="extracted/5864498/images/h2t.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Impact of LLM augmentative approaches using LLM only, Web search<math alttext="+" class="ltx_Math" display="inline" id="S2.F5.4.m1.1"><semantics id="S2.F5.4.m1.1b"><mo id="S2.F5.4.m1.1.1" xref="S2.F5.4.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S2.F5.4.m1.1c"><plus id="S2.F5.4.m1.1.1.cmml" xref="S2.F5.4.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S2.F5.4.m1.1d">+</annotation><annotation encoding="application/x-llamapun" id="S2.F5.4.m1.1e">+</annotation></semantics></math>LLM and Web search<math alttext="+" class="ltx_Math" display="inline" id="S2.F5.5.m2.1"><semantics id="S2.F5.5.m2.1b"><mo id="S2.F5.5.m2.1.1" xref="S2.F5.5.m2.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S2.F5.5.m2.1c"><plus id="S2.F5.5.m2.1.1.cmml" xref="S2.F5.5.m2.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S2.F5.5.m2.1d">+</annotation><annotation encoding="application/x-llamapun" id="S2.F5.5.m2.1e">+</annotation></semantics></math>KG<math alttext="+" class="ltx_Math" display="inline" id="S2.F5.6.m3.1"><semantics id="S2.F5.6.m3.1b"><mo id="S2.F5.6.m3.1.1" xref="S2.F5.6.m3.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S2.F5.6.m3.1c"><plus id="S2.F5.6.m3.1.1.cmml" xref="S2.F5.6.m3.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S2.F5.6.m3.1d">+</annotation><annotation encoding="application/x-llamapun" id="S2.F5.6.m3.1e">+</annotation></semantics></math>LLM to improve quality of responses based on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib10" title="">10</a>]</cite>.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">LLMs for the Financial Domain</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">So far, LLMs have provided advanced capabilities for insights, trends, and assessments in the financial domain. Notable models like FINBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib12" title="">12</a>]</cite>, introduced in 2022, demonstrate the adaptation of LLMs to financial domains. Innovations continue with Bloomberg GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib6" title="">6</a>]</cite> with 50 billion parameters trained on extensive financial domain data, making it one of the largest and most powerful financial-specific LLMs to date. Looking forward, developments in multi-modal LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib13" title="">13</a>]</cite> and specialized agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib14" title="">14</a>]</cite> provide added capability in financial sentiment analysis and market predictions, underscoring the significant role of LLMs in shaping the future of financial analytics. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.T1" title="TABLE I ‣ II-A LLMs for the Financial Domain ‣ II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the recent research on LLMs in the financial field, detailing their contributions.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Existing algorithms developed for Financial insights, trends and assessments.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.1" style="width:657.3pt;height:274.4pt;vertical-align:-6.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.2pt,33.5pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1">Paper Title</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.2.1">Year</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.3.1">Core Capabilities</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.1">FINBERT: A Large Language Model for</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.2">2022</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.3">- It is a LLM that adapts to the financial domain being trained using Google’s BERT algorithm.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T1.1.1.3.2.1">Extracting Information from Financial Text<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.3.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.3.2.3">- It is trained on a large corpus of unlabeled financial texts including corporate filings,</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.3">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T1.1.1.4.3.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.4.3.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.4.3.3">analyst reports, and earning conference call scripts.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.5.4.1">What do LLMs know about Financial Markets?</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.5.4.2">2022</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.5.4.3">- The LLM is prompted to produce Chain-of-Thought summaries to produce labels for financial sentiment.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T1.1.1.6.5.1">A Case Study on Reddit Market Sentiment Analysis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.6.5.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.6.5.3">- It is tested with GPT 3/ PALM to understand market sentiment.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.7.6.1">Data-centric FinGPT: Democratizing Internet-scale</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.7.6.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.7.6.3">- This open-source framework has collected and curated financial data from 30+ diverse online sources.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T1.1.1.8.7.1">Data for Financial Large Language Models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.8.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.8.7.3">- The use-cases includes advisory, sentiment analysis, low-code development.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.9.8.1">Bloomberg GPT: A Large Language Model for Finance<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib6" title="">6</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.9.8.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.9.8.3">-This 50-billion parameter LLM is trained on a wide range of financial data.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.10.9">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T1.1.1.10.9.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.10.9.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.10.9.3">-It is trained on 363 billion token datasets constructed based on Bloomberg’s data sources</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.11.10">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T1.1.1.11.10.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.11.10.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.11.10.3">-Training data is augmented with 345 billion tokens from general purpose datasets.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.12.11.1">Modal-adaptive Knowledge-enhanced Graph-based Financial</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.12.11.2">2024</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.12.11.3">-Video features, audio features, and text features (multi-modal information) are used to predict</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T1.1.1.13.12.1">Prediction from Monetary Policy Conference Calls with LLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib13" title="">13</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.13.12.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.13.12.3">price movement and volatility by understanding the monetary policy conference calls.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.14.13">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T1.1.1.14.13.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.14.13.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.14.13.3">BEiT-3 and Hidden-unit Bert (HuBERT) used to extract video</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.15.14">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T1.1.1.15.14.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.15.14.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.15.14.3">and audio features and ChatGLM2 as for processing text features.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.16.15.1">Designing Heterogeneous LLM Agents for</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.16.15.2">2024</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T1.1.1.16.15.3">-This instantiates specialized agents using prior domain knowledge of errors</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T1.1.1.17.16.1">Financial Sentiment Analysis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib14" title="">14</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.1.17.16.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T1.1.1.17.16.3">made in financial sentiment analysis (FSA).</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.18.17">
<td class="ltx_td ltx_border_b ltx_border_l ltx_border_r" id="S2.T1.1.1.18.17.1"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.1.18.17.2"></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S2.T1.1.1.18.17.3">-Agent discussions helped improve accuracies for FSA without needing to fine-tune the LLM model.</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Financial report data is typically characterized by a mix of structured (tables, charts) and unstructured (narrative text) information. Here, techniques like Named Entity Recognition (NER) are used to extract key financial metrics from these texts. These reports are often lengthy and complex, with hidden data labels that require careful analysis to assess model performance. Additionally, financial reports contain important sentiment and tone information, crucial for financial analysis and transparency. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.T2" title="TABLE II ‣ II-A LLMs for the Financial Domain ‣ II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">II</span></a> provides a summary of recent studies highlighting these specific characteristics of the finance-domain reports and data.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Characteristics of financial report data.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.1" style="width:701.1pt;height:203.9pt;vertical-align:-7.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.5pt,1.0pt) scale(0.99,0.99) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1.1">Paper Title</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.2.1">Year</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.3.1">Domain-specific findings</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.2.2.1">Comprehensive Review of Text-mining</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.2.2.2">2020</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.2.2.3">-Financial reports contain a mixture of the following:</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T2.1.1.3.3.1">applications in Finance<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.3.3.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.3.3.3">structured data (tables and charts) and unstructured narratives</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.4.4">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T2.1.1.4.4.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.4.4.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.4.4.3">-NER using standard libraries (nltk, spacy) can be used to extract</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.5.5">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T2.1.1.5.5.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.5.5.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.5.5.3">entities (e.g., company names, financial metrics) from unstructured texts.</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.6.6">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T2.1.1.6.6.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.6.6.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.6.6.3">-Context-dependent (domain specific) language can be often found</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.7.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T2.1.1.7.7.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.7.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.7.7.3">in financial reports like annual reports.</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.8.8.1">GPT-InvestAR: Enhancing Stock Investment Strategies</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.8.8.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.8.8.3">-To assess the LLM performance, domain-specific data labeling is required</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T2.1.1.9.9.1">through Annual Report Analysis with LLMs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T2.1.1.9.9.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.1.9.9.3">to report accuracy. E.g.: “<span class="ltx_text ltx_font_italic" id="S2.T2.1.1.9.9.3.1">percentage return</span> of each stock between <span class="ltx_text ltx_font_italic" id="S2.T2.1.1.9.9.3.2">filing dates</span>”.</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.10.10.1">NLP Sentiment Analysis and Accounting</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.10.10.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.10.10.3">-Financial reports contain important sentiment/tone and sentence formation</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S2.T2.1.1.11.11.1">Transparency: A New Era of Financial Record Keeping<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T2.1.1.11.11.2"></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S2.T2.1.1.11.11.3">information that needs to be maintained across paragraphs.</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">LLMs for style-transfer</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Research for writing style-transfer has seen significant development in recent years as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S2.T3" title="TABLE III ‣ II-B LLMs for style-transfer ‣ II Related Work ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">III</span></a>. Initial efforts focused on creating datasets and methodologies for formality style transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib20" title="">20</a>]</cite>, with advancements leading to more sophisticated techniques for maintaining semantic integrity while altering style. By 2023, models like ChatGPT demonstrated improved capabilities in evaluating and editing text for style transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib21" title="">21</a>]</cite>. Recent research explored multidimensional evaluations and integrated new approaches for enhancing style adaptation. However, there is a need for methods that enable effective style-transfer in the financial domain to address the content understanding (tables to answers) challenge and to generate semantically-acceptable text with minimal prompting/instructions to conserve input token size constrains for LLMs. This work is aimed to address this need to develop financial text generation methods with minimal data and fine-tuning costs.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Examples of Style Transfer works.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T3.1" style="width:687.1pt;height:388.7pt;vertical-align:-6.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.9pt,53.9pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.1.1">Paper Title</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.2.1">Year</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.3.1">Core Capabilities</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.2.1.1">Dear Sir or Madam, May I Introduce the GYAFC Dataset:</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.2.1.2">2018</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.2.1.3">-Parallel supervision: source-target sentence pairs are labeled.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.3.2.1">Corpus, Benchmarks and Metrics for Formality Style Transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib20" title="">20</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.3.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.3.2.3">-Early work on style transfer with large corpus of training data.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.4.3.1">Disentangled Representation Learning for Non-Parallel</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.4.3.2">2019</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.4.3.3">-Non-parallel supervision with style labels.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.5.4.1">Text Style Transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.5.4.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.5.4.3">-Latent representations of style and content need to be learned separately.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.6.5.1">ChatGPT vs Human-authored Text: Insights into Controllable</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.6.5.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.6.5.3">-Text style transfer can be summarized as the task that involves transforming an input text</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.7.6.1">Text Summarization and Sentence Style Transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.7.6.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.7.6.3">to a target style while maintaining the style-independent semantics.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.8.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.8.7.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.8.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.8.7.3">-To assess ChatGPT’s summarization performance, the following metrics are used: Flesch Reading Ease,</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.9.8">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.9.8.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.9.8.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.9.8.3">Coleman-Liau Index (CLI), Dale-Chall Readability Score (DCR), Rouge Score; to assess formal style,</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.10.9">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.10.9.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.10.9.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.10.9.3">the following metrics are used: Formality Indicator, MTLD Lexical Diversity metric.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.11.10">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.11.10.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.11.10.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.11.10.3">-Evaluation shows stylistic variations produced by humans are considerably larger than</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.12.11">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.12.11.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.12.11.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.12.11.3">those demonstrated by ChatGPT.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.13.12.1">Prompt-based Editing for Text Style Transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.13.12.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.13.12.3">-Prompt-based editing approach to text style transfer.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.14.13">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.14.13.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.14.13.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.14.13.3">-Prompt a LLM for style classification and use classification probability to compute a style score.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.15.14">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.15.14.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.15.14.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.15.14.3">Perform discrete search with word-level editing to maximize score function for style-transfer tasks.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.16.15.1">Multidimensional Evaluation for Text Style Transfer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib21" title="">21</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.16.15.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.16.15.3">-Leveraged ChatGPT to evaluate the text transfer capabilities</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.17.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.17.16.1">using ChatGPT</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.17.16.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.17.16.3">of other text style transfer models in the domain of: style strength, content preservation, and fluency</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.18.17">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.18.17.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.18.17.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.18.17.3">-ChatGPT achieves competitive correlations with human judgements</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.19.18.1">CAT-LLM: Prompting Large Language Models with Text Style Definitions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib25" title="">25</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.19.18.2">2023</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.19.18.3">The model incorporates a text style definition module to comprehensively</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.20.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.20.19.1">for Chinese Article-style Transfer</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.20.19.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.20.19.3">analyze text features in target articles from both words and sentences levels to learn target style.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.21.20">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.21.20.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.21.20.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.21.20.3">-Evaluated with 5 styles of Chinese articles.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.22.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.22.21.1">Whose LLM is it Anyway? Linguistic Comparison and LLM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.22.21.2">2024</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.22.21.3">-Linguistic styles between three popular models are analyzed</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.23.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S2.T3.1.1.23.22.1">Attribution for GPT-3.5, GPT 4 and Bard<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.23.22.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.23.22.3">in terms of: vocabulary, Part-Of-Speech distribution, dependency distribution, sentiment.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.24.23">
<td class="ltx_td ltx_border_l ltx_border_r" id="S2.T3.1.1.24.23.1"></td>
<td class="ltx_td ltx_border_r" id="S2.T3.1.1.24.23.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S2.T3.1.1.24.23.3">-The results point to significant linguistic variations.</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.25.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S2.T3.1.1.25.24.1">Unsupervised Text Style Transfer via LLMs</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T3.1.1.25.24.2">2024</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T3.1.1.25.24.3">-Attention masking and LLM models are effectively combined to support unsupervised</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.26.25">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S2.T3.1.1.26.25.1">and Attention Masking with Multi-way Interactions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T3.1.1.26.25.2"></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S2.T3.1.1.26.25.3">text style-transfer in this paper.</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Hallucination and Creativity in LLMs</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">While fine-tuning LLMs for niche domain-writing styles such as finance, sales, medical reporting etc. are necessary, most LLM fine-tuning techniques do not focus on hallucinations introduced by un-prepared data sources. A recent work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib8" title="">8</a>]</cite> presents the mathematical framework to define LLM hallucinations using probability and information theoretic approaches. This work demonstrates that LLM hallucinations are characterized by low probabilities of sequential tokens. Also, it illustrates that hallucinations arise from self-supervised learning since the training process typically relies on metrics such as ROUGE, TER, BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib28" title="">28</a>]</cite> etc. that focus on ensuring that the response stay similar to the context, even if a well formed answer exists. Also, hallucinations are <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">implausible</span> based on the context and can be considered as inference-level anomalies that cannot be replicated owing to low token probabilities. This work also demonstrates that minimizing hallucinations can minimize creativity of an LLM outcome. In this work, we expand on this observation and utilize novel metrics to detect the likelihood of sequential token generation across training epochs. We rely on the self-learning capability of the LLMs by introducing a second stage of fine-tuning on previously hallucinated text, which in-turn boosts creativity and compound sentence generation capabilities.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Another recent work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib9" title="">9</a>]</cite> demonstrates the two phases of LLM hallucinations, wherein, the first divergent-phase hallucination induces creativity that can be controlled by advanced prompt engineering, and fine-tuning to promote creativity. The second convergent-phase hallucination involves standard RAG scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib2" title="">2</a>]</cite> that require intention recognition and hallucination detection through RAG-based control mechanisms. This work expands on the divergent phase to carefully pre-process the data followed by hallucination control flags to detect creativity.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods and Data</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.3">LLMs typically perform two distinct tasks of natural language understanding (NLU), wherein the user data and query is converted to machine translation entities or tokens followed by natural language generation (NLG), wherein a sequence of words/tokens are generated based on the probabilities of the prior generated words. In this work, we focus on the NLG aspect of an LLM and the ability to transfer domain specific jargon, such as compound sentence generation and creative language generation. To evaluate the LLM responses, we analyze the user-query (<math alttext="Q" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_Q</annotation></semantics></math>), contextual data (<math alttext="D" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_D</annotation></semantics></math>) and the LLM responses (<math alttext="R" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_R</annotation></semantics></math>) together.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">From prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib8" title="">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib9" title="">9</a>]</cite>, we know that hallucinations and creativity in generated tokens have similar model-level level characteristics. Therefore, by ensuring a low value of top_p and temperature, both hallucinations and creativity can be considerably reduced <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib29" title="">29</a>]</cite>. However, in this work, our goal is to minimize hallucinations while promoting creative sentence generation. As an example, consider the following data context, queries and responses <math alttext="R_{1},R_{2}" class="ltx_Math" display="inline" id="S3.p2.1.m1.2"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.2.2" xref="S3.p2.1.m1.2.2.3.cmml"><msub id="S3.p2.1.m1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.1.1.2" xref="S3.p2.1.m1.1.1.1.1.2.cmml">R</mi><mn id="S3.p2.1.m1.1.1.1.1.3" xref="S3.p2.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.p2.1.m1.2.2.2.3" xref="S3.p2.1.m1.2.2.3.cmml">,</mo><msub id="S3.p2.1.m1.2.2.2.2" xref="S3.p2.1.m1.2.2.2.2.cmml"><mi id="S3.p2.1.m1.2.2.2.2.2" xref="S3.p2.1.m1.2.2.2.2.2.cmml">R</mi><mn id="S3.p2.1.m1.2.2.2.2.3" xref="S3.p2.1.m1.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.2.3.cmml" xref="S3.p2.1.m1.2.2.2"><apply id="S3.p2.1.m1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.1.1.2">𝑅</ci><cn id="S3.p2.1.m1.1.1.1.1.3.cmml" type="integer" xref="S3.p2.1.m1.1.1.1.1.3">1</cn></apply><apply id="S3.p2.1.m1.2.2.2.2.cmml" xref="S3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.2.2.2.2.1.cmml" xref="S3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.p2.1.m1.2.2.2.2.2">𝑅</ci><cn id="S3.p2.1.m1.2.2.2.2.3.cmml" type="integer" xref="S3.p2.1.m1.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">R_{1},R_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.2d">italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><math alttext="D" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_D</annotation></semantics></math>: “The company ACL had targeted 30% profits but it finished Q2 at 28.8% profits.”</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><math alttext="Q" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_Q</annotation></semantics></math>: “How was ACL’s performance in Q2?”.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><math alttext="R_{1}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><msub id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">R</mi><mn id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">𝑅</ci><cn id="S3.I1.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">R_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>: “ACL met its target of 30% profit in the Q2 quarter.”</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><math alttext="R_{2}" class="ltx_Math" display="inline" id="S3.I1.i4.p1.1.m1.1"><semantics id="S3.I1.i4.p1.1.m1.1a"><msub id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mi id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">R</mi><mn id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">𝑅</ci><cn id="S3.I1.i4.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I1.i4.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">R_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.1.m1.1d">italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>: “ACL missed the planned target of 30% by 1.2% by the close of Q2.”</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.p2.5">In this situation <math alttext="R_{1}" class="ltx_Math" display="inline" id="S3.p2.2.m1.1"><semantics id="S3.p2.2.m1.1a"><msub id="S3.p2.2.m1.1.1" xref="S3.p2.2.m1.1.1.cmml"><mi id="S3.p2.2.m1.1.1.2" xref="S3.p2.2.m1.1.1.2.cmml">R</mi><mn id="S3.p2.2.m1.1.1.3" xref="S3.p2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m1.1b"><apply id="S3.p2.2.m1.1.1.cmml" xref="S3.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m1.1.1.1.cmml" xref="S3.p2.2.m1.1.1">subscript</csymbol><ci id="S3.p2.2.m1.1.1.2.cmml" xref="S3.p2.2.m1.1.1.2">𝑅</ci><cn id="S3.p2.2.m1.1.1.3.cmml" type="integer" xref="S3.p2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m1.1c">R_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m1.1d">italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic" id="S3.p2.5.1">hallucination</span> while <math alttext="R_{2}" class="ltx_Math" display="inline" id="S3.p2.3.m2.1"><semantics id="S3.p2.3.m2.1a"><msub id="S3.p2.3.m2.1.1" xref="S3.p2.3.m2.1.1.cmml"><mi id="S3.p2.3.m2.1.1.2" xref="S3.p2.3.m2.1.1.2.cmml">R</mi><mn id="S3.p2.3.m2.1.1.3" xref="S3.p2.3.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m2.1b"><apply id="S3.p2.3.m2.1.1.cmml" xref="S3.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m2.1.1.1.cmml" xref="S3.p2.3.m2.1.1">subscript</csymbol><ci id="S3.p2.3.m2.1.1.2.cmml" xref="S3.p2.3.m2.1.1.2">𝑅</ci><cn id="S3.p2.3.m2.1.1.3.cmml" type="integer" xref="S3.p2.3.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m2.1c">R_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m2.1d">italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is a <span class="ltx_text ltx_font_italic" id="S3.p2.5.2">creative</span> response. We assess the <span class="ltx_text ltx_font_italic" id="S3.p2.5.3">quality</span> of generated sentences after domain-specific fine tuning (FT) to isolate the log-probabilities at sentence level for creative versus hallucinated sentences. Additionally, the entities (nouns, locations, currencies etc.) in the generated text (<math alttext="e_{k}" class="ltx_Math" display="inline" id="S3.p2.4.m3.1"><semantics id="S3.p2.4.m3.1a"><msub id="S3.p2.4.m3.1.1" xref="S3.p2.4.m3.1.1.cmml"><mi id="S3.p2.4.m3.1.1.2" xref="S3.p2.4.m3.1.1.2.cmml">e</mi><mi id="S3.p2.4.m3.1.1.3" xref="S3.p2.4.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.4.m3.1b"><apply id="S3.p2.4.m3.1.1.cmml" xref="S3.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m3.1.1.1.cmml" xref="S3.p2.4.m3.1.1">subscript</csymbol><ci id="S3.p2.4.m3.1.1.2.cmml" xref="S3.p2.4.m3.1.1.2">𝑒</ci><ci id="S3.p2.4.m3.1.1.3.cmml" xref="S3.p2.4.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m3.1c">e_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m3.1d">italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) and their relationships (<math alttext="\rho_{k}" class="ltx_Math" display="inline" id="S3.p2.5.m4.1"><semantics id="S3.p2.5.m4.1a"><msub id="S3.p2.5.m4.1.1" xref="S3.p2.5.m4.1.1.cmml"><mi id="S3.p2.5.m4.1.1.2" xref="S3.p2.5.m4.1.1.2.cmml">ρ</mi><mi id="S3.p2.5.m4.1.1.3" xref="S3.p2.5.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.5.m4.1b"><apply id="S3.p2.5.m4.1.1.cmml" xref="S3.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m4.1.1.1.cmml" xref="S3.p2.5.m4.1.1">subscript</csymbol><ci id="S3.p2.5.m4.1.1.2.cmml" xref="S3.p2.5.m4.1.1.2">𝜌</ci><ci id="S3.p2.5.m4.1.1.3.cmml" xref="S3.p2.5.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m4.1c">\rho_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m4.1d">italic_ρ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) can be extracted using standard libraries such as “nltk” and “spacy” to asses the formation of compound sentences in terms of the density of entities and relationships per sentence. The metrics used to evaluate the <span class="ltx_text ltx_font_italic" id="S3.p2.5.4">quality</span> of fine-tuned text are shown in section <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS1" title="III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. For our experiments, we perform two-stage FT on OpenAI GPT3.5 model using the RLHF technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib30" title="">30</a>]</cite>. As training data, we collect public domain financial reports that are labelled for financial entities and pre-processed into the “prompt-completion” format as shown in section <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS2" title="III-B Data Preparation and FT-process ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Notation and Metrics</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.7">The goal of this work is to generate domain-friendly natural language text from a minimal prompt that contains basic instructions and financial data in a tabular format. For a sequence of words/tokens, the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_i</annotation></semantics></math>’th generated token <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑥</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the log probability associated with the token is <math alttext="p_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">p</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑝</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. It is noteworthy that each sequential token is generated as a function (<math alttext="F" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_F</annotation></semantics></math>) of the prior tokens and the token with highest probability across the top contenders for the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_i</annotation></semantics></math>’th position (<math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑥</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>), represented by equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.E1" title="In III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">1</span></a>). Also, the log-probability of top 5 contenders for each sequential position is collected as <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">𝑃</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the output of OpenAI’s GPT3.5 to quantitatively detect creativity and hallucinations.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x_{i}=F(x_{i-1},x_{i-2}...,\arg\max(P_{i})),\forall P_{i}=\{p_{i,1},p_{i,2}..p%
_{i,5}\}." class="ltx_math_unparsed" display="block" id="S3.E1.m1.7"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7b"><msub id="S3.E1.m1.7.8"><mi id="S3.E1.m1.7.8.2">x</mi><mi id="S3.E1.m1.7.8.3">i</mi></msub><mo id="S3.E1.m1.7.9">=</mo><mi id="S3.E1.m1.7.10">F</mi><mrow id="S3.E1.m1.7.11"><mo id="S3.E1.m1.7.11.1" stretchy="false">(</mo><msub id="S3.E1.m1.7.11.2"><mi id="S3.E1.m1.7.11.2.2">x</mi><mrow id="S3.E1.m1.7.11.2.3"><mi id="S3.E1.m1.7.11.2.3.2">i</mi><mo id="S3.E1.m1.7.11.2.3.1">−</mo><mn id="S3.E1.m1.7.11.2.3.3">1</mn></mrow></msub><mo id="S3.E1.m1.7.11.3">,</mo><msub id="S3.E1.m1.7.11.4"><mi id="S3.E1.m1.7.11.4.2">x</mi><mrow id="S3.E1.m1.7.11.4.3"><mi id="S3.E1.m1.7.11.4.3.2">i</mi><mo id="S3.E1.m1.7.11.4.3.1">−</mo><mn id="S3.E1.m1.7.11.4.3.3">2</mn></mrow></msub><mi id="S3.E1.m1.7.11.5" mathvariant="normal">…</mi><mo id="S3.E1.m1.7.11.6">,</mo><mi id="S3.E1.m1.7.11.7">arg</mi><mi id="S3.E1.m1.7.7">max</mi><mrow id="S3.E1.m1.7.11.8"><mo id="S3.E1.m1.7.11.8.1" stretchy="false">(</mo><msub id="S3.E1.m1.7.11.8.2"><mi id="S3.E1.m1.7.11.8.2.2">P</mi><mi id="S3.E1.m1.7.11.8.2.3">i</mi></msub><mo id="S3.E1.m1.7.11.8.3" stretchy="false">)</mo></mrow><mo id="S3.E1.m1.7.11.9" stretchy="false">)</mo></mrow><mo id="S3.E1.m1.7.12">,</mo><mo id="S3.E1.m1.7.13" rspace="0.167em">∀</mo><msub id="S3.E1.m1.7.14"><mi id="S3.E1.m1.7.14.2">P</mi><mi id="S3.E1.m1.7.14.3">i</mi></msub><mo id="S3.E1.m1.7.15">=</mo><mrow id="S3.E1.m1.7.16"><mo id="S3.E1.m1.7.16.1" stretchy="false">{</mo><msub id="S3.E1.m1.7.16.2"><mi id="S3.E1.m1.7.16.2.2">p</mi><mrow id="S3.E1.m1.2.2.2.4"><mi id="S3.E1.m1.1.1.1.1">i</mi><mo id="S3.E1.m1.2.2.2.4.1">,</mo><mn id="S3.E1.m1.2.2.2.2">1</mn></mrow></msub><mo id="S3.E1.m1.7.16.3">,</mo><msub id="S3.E1.m1.7.16.4"><mi id="S3.E1.m1.7.16.4.2">p</mi><mrow id="S3.E1.m1.4.4.2.4"><mi id="S3.E1.m1.3.3.1.1">i</mi><mo id="S3.E1.m1.4.4.2.4.1">,</mo><mn id="S3.E1.m1.4.4.2.2">2</mn></mrow></msub><mo id="S3.E1.m1.7.16.5" lspace="0em" rspace="0.0835em">.</mo><mo id="S3.E1.m1.7.16.6" lspace="0.0835em" rspace="0.167em">.</mo><msub id="S3.E1.m1.7.16.7"><mi id="S3.E1.m1.7.16.7.2">p</mi><mrow id="S3.E1.m1.6.6.2.4"><mi id="S3.E1.m1.5.5.1.1">i</mi><mo id="S3.E1.m1.6.6.2.4.1">,</mo><mn id="S3.E1.m1.6.6.2.2">5</mn></mrow></msub><mo id="S3.E1.m1.7.16.8" stretchy="false">}</mo></mrow><mo id="S3.E1.m1.7.17" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.7c">x_{i}=F(x_{i-1},x_{i-2}...,\arg\max(P_{i})),\forall P_{i}=\{p_{i,1},p_{i,2}..p%
_{i,5}\}.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.7d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_F ( italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i - 2 end_POSTSUBSCRIPT … , roman_arg roman_max ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) , ∀ italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_p start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT . . italic_p start_POSTSUBSCRIPT italic_i , 5 end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.8">As an example, each response word in the second column in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.T4" title="TABLE IV ‣ III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">IV</span></a> is selected across 5 top contenders, as the word/token one with the highest probability (or lowest log-probability).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Further, we assess the quality of fine-tuned generated text using the sequential log-probabilities per token and the following metrics.</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.2">Perplexity (<math alttext="Per" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.1"><semantics id="S3.I2.i1.p1.1.m1.1a"><mrow id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.1.1.2" xref="S3.I2.i1.p1.1.m1.1.1.2.cmml">P</mi><mo id="S3.I2.i1.p1.1.m1.1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.I2.i1.p1.1.m1.1.1.3" xref="S3.I2.i1.p1.1.m1.1.1.3.cmml">e</mi><mo id="S3.I2.i1.p1.1.m1.1.1.1a" xref="S3.I2.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.I2.i1.p1.1.m1.1.1.4" xref="S3.I2.i1.p1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><apply id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1"><times id="S3.I2.i1.p1.1.m1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1"></times><ci id="S3.I2.i1.p1.1.m1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.1.1.2">𝑃</ci><ci id="S3.I2.i1.p1.1.m1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.1.1.3">𝑒</ci><ci id="S3.I2.i1.p1.1.m1.1.1.4.cmml" xref="S3.I2.i1.p1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>): A lower value signifies that each sequential token is generated with high confidence following the FT process in equation (<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.E2" title="In III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">2</span></a>). <math alttext="t" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" xref="S3.I2.i1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">italic_t</annotation></semantics></math> represents the number of tokens per sentence.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">BLUE (Bilingual Evaluation Understudy) score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib28" title="">28</a>]</cite>: A high value represents high similarity between the generated text to the reference context (<math alttext="D" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><mi id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><ci id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">italic_D</annotation></semantics></math>), thereby representing accuracy of the generated text.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">TER (Translation Edit Rate) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib28" title="">28</a>]</cite>: A lower value indicates fewer edits required to transform the generated text to the reference context, thereby representing higher quality of generated text.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib28" title="">28</a>]</cite>: A high value represents high similarity between generated text and reference context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">chrF++ (character level F score) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#bib.bib28" title="">28</a>]</cite>: A high value operates at a character level to denote accuracy of generation in terms of similarity with reference context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i6.p1">
<p class="ltx_p" id="S3.I2.i6.p1.3">Averaged sequential log-loss per sentence (ASLS): A high value represents highly discernible tokens in a sentence, given <math alttext="t" class="ltx_Math" display="inline" id="S3.I2.i6.p1.1.m1.1"><semantics id="S3.I2.i6.p1.1.m1.1a"><mi id="S3.I2.i6.p1.1.m1.1.1" xref="S3.I2.i6.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i6.p1.1.m1.1b"><ci id="S3.I2.i6.p1.1.m1.1.1.cmml" xref="S3.I2.i6.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i6.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i6.p1.1.m1.1d">italic_t</annotation></semantics></math> tokens per sentence. High ASLS is indicative of highly non-uniform probability distribution per token <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.I2.i6.p1.2.m2.1"><semantics id="S3.I2.i6.p1.2.m2.1a"><msub id="S3.I2.i6.p1.2.m2.1.1" xref="S3.I2.i6.p1.2.m2.1.1.cmml"><mi id="S3.I2.i6.p1.2.m2.1.1.2" xref="S3.I2.i6.p1.2.m2.1.1.2.cmml">P</mi><mi id="S3.I2.i6.p1.2.m2.1.1.3" xref="S3.I2.i6.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i6.p1.2.m2.1b"><apply id="S3.I2.i6.p1.2.m2.1.1.cmml" xref="S3.I2.i6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i6.p1.2.m2.1.1.1.cmml" xref="S3.I2.i6.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I2.i6.p1.2.m2.1.1.2.cmml" xref="S3.I2.i6.p1.2.m2.1.1.2">𝑃</ci><ci id="S3.I2.i6.p1.2.m2.1.1.3.cmml" xref="S3.I2.i6.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i6.p1.2.m2.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i6.p1.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and is shown in equation (3). This novel sentence-level metric evaluates how confident the LLM is in generating subsequent tokens. For instance, if top 5 log-probabilities per sequential tokens are equally likely, that would result in a low ASLS, which indicates un-learned or hallucinated or creative text generation. ASLS is an extension from cross entropy loss (<math alttext="CE" class="ltx_Math" display="inline" id="S3.I2.i6.p1.3.m3.1"><semantics id="S3.I2.i6.p1.3.m3.1a"><mrow id="S3.I2.i6.p1.3.m3.1.1" xref="S3.I2.i6.p1.3.m3.1.1.cmml"><mi id="S3.I2.i6.p1.3.m3.1.1.2" xref="S3.I2.i6.p1.3.m3.1.1.2.cmml">C</mi><mo id="S3.I2.i6.p1.3.m3.1.1.1" xref="S3.I2.i6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S3.I2.i6.p1.3.m3.1.1.3" xref="S3.I2.i6.p1.3.m3.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i6.p1.3.m3.1b"><apply id="S3.I2.i6.p1.3.m3.1.1.cmml" xref="S3.I2.i6.p1.3.m3.1.1"><times id="S3.I2.i6.p1.3.m3.1.1.1.cmml" xref="S3.I2.i6.p1.3.m3.1.1.1"></times><ci id="S3.I2.i6.p1.3.m3.1.1.2.cmml" xref="S3.I2.i6.p1.3.m3.1.1.2">𝐶</ci><ci id="S3.I2.i6.p1.3.m3.1.1.3.cmml" xref="S3.I2.i6.p1.3.m3.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i6.p1.3.m3.1c">CE</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i6.p1.3.m3.1d">italic_C italic_E</annotation></semantics></math> in equation (4)), where a lower value indicates higher confidence of token selection.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i7.p1">
<p class="ltx_p" id="S3.I2.i7.p1.3">Knowledge density per sentence (KDPS): A high value indicates dense information in terms of entities (<math alttext="e_{k}" class="ltx_Math" display="inline" id="S3.I2.i7.p1.1.m1.1"><semantics id="S3.I2.i7.p1.1.m1.1a"><msub id="S3.I2.i7.p1.1.m1.1.1" xref="S3.I2.i7.p1.1.m1.1.1.cmml"><mi id="S3.I2.i7.p1.1.m1.1.1.2" xref="S3.I2.i7.p1.1.m1.1.1.2.cmml">e</mi><mi id="S3.I2.i7.p1.1.m1.1.1.3" xref="S3.I2.i7.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i7.p1.1.m1.1b"><apply id="S3.I2.i7.p1.1.m1.1.1.cmml" xref="S3.I2.i7.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i7.p1.1.m1.1.1.1.cmml" xref="S3.I2.i7.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i7.p1.1.m1.1.1.2.cmml" xref="S3.I2.i7.p1.1.m1.1.1.2">𝑒</ci><ci id="S3.I2.i7.p1.1.m1.1.1.3.cmml" xref="S3.I2.i7.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i7.p1.1.m1.1c">e_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i7.p1.1.m1.1d">italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) and their relationships (<math alttext="\rho_{k}" class="ltx_Math" display="inline" id="S3.I2.i7.p1.2.m2.1"><semantics id="S3.I2.i7.p1.2.m2.1a"><msub id="S3.I2.i7.p1.2.m2.1.1" xref="S3.I2.i7.p1.2.m2.1.1.cmml"><mi id="S3.I2.i7.p1.2.m2.1.1.2" xref="S3.I2.i7.p1.2.m2.1.1.2.cmml">ρ</mi><mi id="S3.I2.i7.p1.2.m2.1.1.3" xref="S3.I2.i7.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i7.p1.2.m2.1b"><apply id="S3.I2.i7.p1.2.m2.1.1.cmml" xref="S3.I2.i7.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I2.i7.p1.2.m2.1.1.1.cmml" xref="S3.I2.i7.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I2.i7.p1.2.m2.1.1.2.cmml" xref="S3.I2.i7.p1.2.m2.1.1.2">𝜌</ci><ci id="S3.I2.i7.p1.2.m2.1.1.3.cmml" xref="S3.I2.i7.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i7.p1.2.m2.1c">\rho_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i7.p1.2.m2.1d">italic_ρ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>) per sentence, given <math alttext="s" class="ltx_Math" display="inline" id="S3.I2.i7.p1.3.m3.1"><semantics id="S3.I2.i7.p1.3.m3.1a"><mi id="S3.I2.i7.p1.3.m3.1.1" xref="S3.I2.i7.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i7.p1.3.m3.1b"><ci id="S3.I2.i7.p1.3.m3.1.1.cmml" xref="S3.I2.i7.p1.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i7.p1.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i7.p1.3.m3.1d">italic_s</annotation></semantics></math> sentences in a paragraph. This is a domain-specific metric and is representative of compound sentences at paragraph level in equation (5).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Per=\exp(-\frac{1}{t}\sum_{i}^{t}(x_{i}|x_{i-1...}))," class="ltx_Math" display="inline" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">P</mi><mo id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml">e</mi><mo id="S3.E2.m1.2.2.1.1.3.1a" xref="S3.E2.m1.2.2.1.1.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.3.4" xref="S3.E2.m1.2.2.1.1.3.4.cmml">r</mi></mrow><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">exp</mi><mo id="S3.E2.m1.2.2.1.1.1.1a" xref="S3.E2.m1.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mfrac id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3.cmml">t</mi></mfrac></mstyle><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><munderover id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></munderover></mstyle><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">1</mn><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3" mathvariant="normal" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml">…</mi></mrow></mrow></msub></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">𝑃</ci><ci id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3">𝑒</ci><ci id="S3.E2.m1.2.2.1.1.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.4">𝑟</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><exp id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></exp><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3"><divide id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3"></divide><cn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑥</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.2">𝑖</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.1"></times><cn id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.2">1</cn><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.3.3.3">…</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle Per=\exp(-\frac{1}{t}\sum_{i}^{t}(x_{i}|x_{i-1...})),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">italic_P italic_e italic_r = roman_exp ( - divide start_ARG 1 end_ARG start_ARG italic_t end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_i - 1 … end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle ASLS=-\frac{1}{t}\sum_{i}^{t}\sum_{j}^{5}p_{i,j}," class="ltx_Math" display="inline" id="S3.E3.m1.3"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml"><mi id="S3.E3.m1.3.3.1.1.2.2" xref="S3.E3.m1.3.3.1.1.2.2.cmml">A</mi><mo id="S3.E3.m1.3.3.1.1.2.1" xref="S3.E3.m1.3.3.1.1.2.1.cmml">⁢</mo><mi id="S3.E3.m1.3.3.1.1.2.3" xref="S3.E3.m1.3.3.1.1.2.3.cmml">S</mi><mo id="S3.E3.m1.3.3.1.1.2.1a" xref="S3.E3.m1.3.3.1.1.2.1.cmml">⁢</mo><mi id="S3.E3.m1.3.3.1.1.2.4" xref="S3.E3.m1.3.3.1.1.2.4.cmml">L</mi><mo id="S3.E3.m1.3.3.1.1.2.1b" xref="S3.E3.m1.3.3.1.1.2.1.cmml">⁢</mo><mi id="S3.E3.m1.3.3.1.1.2.5" xref="S3.E3.m1.3.3.1.1.2.5.cmml">S</mi></mrow><mo id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml"><mo id="S3.E3.m1.3.3.1.1.3a" xref="S3.E3.m1.3.3.1.1.3.cmml">−</mo><mrow id="S3.E3.m1.3.3.1.1.3.2" xref="S3.E3.m1.3.3.1.1.3.2.cmml"><mstyle displaystyle="true" id="S3.E3.m1.3.3.1.1.3.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.cmml"><mfrac id="S3.E3.m1.3.3.1.1.3.2.2a" xref="S3.E3.m1.3.3.1.1.3.2.2.cmml"><mn id="S3.E3.m1.3.3.1.1.3.2.2.2" xref="S3.E3.m1.3.3.1.1.3.2.2.2.cmml">1</mn><mi id="S3.E3.m1.3.3.1.1.3.2.2.3" xref="S3.E3.m1.3.3.1.1.3.2.2.3.cmml">t</mi></mfrac></mstyle><mo id="S3.E3.m1.3.3.1.1.3.2.1" xref="S3.E3.m1.3.3.1.1.3.2.1.cmml">⁢</mo><mrow id="S3.E3.m1.3.3.1.1.3.2.3" xref="S3.E3.m1.3.3.1.1.3.2.3.cmml"><mstyle displaystyle="true" id="S3.E3.m1.3.3.1.1.3.2.3.1" xref="S3.E3.m1.3.3.1.1.3.2.3.1.cmml"><munderover id="S3.E3.m1.3.3.1.1.3.2.3.1a" xref="S3.E3.m1.3.3.1.1.3.2.3.1.cmml"><mo id="S3.E3.m1.3.3.1.1.3.2.3.1.2.2" movablelimits="false" xref="S3.E3.m1.3.3.1.1.3.2.3.1.2.2.cmml">∑</mo><mi id="S3.E3.m1.3.3.1.1.3.2.3.1.2.3" xref="S3.E3.m1.3.3.1.1.3.2.3.1.2.3.cmml">i</mi><mi id="S3.E3.m1.3.3.1.1.3.2.3.1.3" xref="S3.E3.m1.3.3.1.1.3.2.3.1.3.cmml">t</mi></munderover></mstyle><mrow id="S3.E3.m1.3.3.1.1.3.2.3.2" xref="S3.E3.m1.3.3.1.1.3.2.3.2.cmml"><mstyle displaystyle="true" id="S3.E3.m1.3.3.1.1.3.2.3.2.1" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.cmml"><munderover id="S3.E3.m1.3.3.1.1.3.2.3.2.1a" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.cmml"><mo id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.2" movablelimits="false" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.2.cmml">∑</mo><mi id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.3" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.3.cmml">j</mi><mn id="S3.E3.m1.3.3.1.1.3.2.3.2.1.3" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.3.cmml">5</mn></munderover></mstyle><msub id="S3.E3.m1.3.3.1.1.3.2.3.2.2" xref="S3.E3.m1.3.3.1.1.3.2.3.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.3.2.3.2.2.2" xref="S3.E3.m1.3.3.1.1.3.2.3.2.2.2.cmml">p</mi><mrow id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.2.2.2.4.1" xref="S3.E3.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">j</mi></mrow></msub></mrow></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"></eq><apply id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"><times id="S3.E3.m1.3.3.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1"></times><ci id="S3.E3.m1.3.3.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2">𝐴</ci><ci id="S3.E3.m1.3.3.1.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.2.3">𝑆</ci><ci id="S3.E3.m1.3.3.1.1.2.4.cmml" xref="S3.E3.m1.3.3.1.1.2.4">𝐿</ci><ci id="S3.E3.m1.3.3.1.1.2.5.cmml" xref="S3.E3.m1.3.3.1.1.2.5">𝑆</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3"><minus id="S3.E3.m1.3.3.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3"></minus><apply id="S3.E3.m1.3.3.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2"><times id="S3.E3.m1.3.3.1.1.3.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.1"></times><apply id="S3.E3.m1.3.3.1.1.3.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2"><divide id="S3.E3.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2"></divide><cn id="S3.E3.m1.3.3.1.1.3.2.2.2.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.3.2.2.2">1</cn><ci id="S3.E3.m1.3.3.1.1.3.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.2.3">𝑡</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3"><apply id="S3.E3.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.3.2.3.1.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1">subscript</csymbol><sum id="S3.E3.m1.3.3.1.1.3.2.3.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1.2.2"></sum><ci id="S3.E3.m1.3.3.1.1.3.2.3.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1.2.3">𝑖</ci></apply><ci id="S3.E3.m1.3.3.1.1.3.2.3.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.1.3">𝑡</ci></apply><apply id="S3.E3.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2"><apply id="S3.E3.m1.3.3.1.1.3.2.3.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.2.1.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1">superscript</csymbol><apply id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1">subscript</csymbol><sum id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.2"></sum><ci id="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.3.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.2.3">𝑗</ci></apply><cn id="S3.E3.m1.3.3.1.1.3.2.3.2.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.3.2.3.2.1.3">5</cn></apply><apply id="S3.E3.m1.3.3.1.1.3.2.3.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.3.2.3.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.2">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.3.2.3.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.3.2.3.2.2.2">𝑝</ci><list id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.4"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑖</ci><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">𝑗</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\displaystyle ASLS=-\frac{1}{t}\sum_{i}^{t}\sum_{j}^{5}p_{i,j},</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.3d">italic_A italic_S italic_L italic_S = - divide start_ARG 1 end_ARG start_ARG italic_t end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle CE=\sum_{i}^{t}-{\max(p_{i,j})_{j=1}^{5}}," class="ltx_math_unparsed" display="inline" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3b"><mi id="S3.E4.m1.3.4">C</mi><mi id="S3.E4.m1.3.5">E</mi><mo id="S3.E4.m1.3.6">=</mo><mstyle displaystyle="true" id="S3.E4.m1.3.7"><munderover id="S3.E4.m1.3.7a"><mo id="S3.E4.m1.3.7.2.2" movablelimits="false">∑</mo><mi id="S3.E4.m1.3.7.2.3">i</mi><mi id="S3.E4.m1.3.7.3">t</mi></munderover></mstyle><mo id="S3.E4.m1.3.8">−</mo><mi id="S3.E4.m1.3.3">max</mi><msubsup id="S3.E4.m1.3.9"><mrow id="S3.E4.m1.3.9.2.2"><mo id="S3.E4.m1.3.9.2.2.1" stretchy="false">(</mo><msub id="S3.E4.m1.3.9.2.2.2"><mi id="S3.E4.m1.3.9.2.2.2.2">p</mi><mrow id="S3.E4.m1.2.2.2.4"><mi id="S3.E4.m1.1.1.1.1">i</mi><mo id="S3.E4.m1.2.2.2.4.1">,</mo><mi id="S3.E4.m1.2.2.2.2">j</mi></mrow></msub><mo id="S3.E4.m1.3.9.2.2.3" stretchy="false">)</mo></mrow><mrow id="S3.E4.m1.3.9.2.3"><mi id="S3.E4.m1.3.9.2.3.2">j</mi><mo id="S3.E4.m1.3.9.2.3.1">=</mo><mn id="S3.E4.m1.3.9.2.3.3">1</mn></mrow><mn id="S3.E4.m1.3.9.3">5</mn></msubsup><mo id="S3.E4.m1.3.10">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E4.m1.3c">\displaystyle CE=\sum_{i}^{t}-{\max(p_{i,j})_{j=1}^{5}},</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_C italic_E = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - roman_max ( italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle KDPS=\frac{1}{s}\sum_{k=1}^{s}(e_{k}+\rho_{k})." class="ltx_Math" display="inline" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml">K</mi><mo id="S3.E5.m1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3.cmml">D</mi><mo id="S3.E5.m1.1.1.1.1.3.1a" xref="S3.E5.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.1.1.3.4" xref="S3.E5.m1.1.1.1.1.3.4.cmml">P</mi><mo id="S3.E5.m1.1.1.1.1.3.1b" xref="S3.E5.m1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.1.1.3.5" xref="S3.E5.m1.1.1.1.1.3.5.cmml">S</mi></mrow><mo id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.3.cmml"><mfrac id="S3.E5.m1.1.1.1.1.1.3a" xref="S3.E5.m1.1.1.1.1.1.3.cmml"><mn id="S3.E5.m1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E5.m1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.3.3.cmml">s</mi></mfrac></mstyle><mo id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml"><munderover id="S3.E5.m1.1.1.1.1.1.1.2a" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E5.m1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S3.E5.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E5.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E5.m1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.3.cmml">s</mi></munderover></mstyle><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">ρ</mi><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" lspace="0em" xref="S3.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><times id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.1"></times><ci id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2">𝐾</ci><ci id="S3.E5.m1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3">𝐷</ci><ci id="S3.E5.m1.1.1.1.1.3.4.cmml" xref="S3.E5.m1.1.1.1.1.3.4">𝑃</ci><ci id="S3.E5.m1.1.1.1.1.3.5.cmml" xref="S3.E5.m1.1.1.1.1.3.5">𝑆</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3"><divide id="S3.E5.m1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.3"></divide><cn id="S3.E5.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E5.m1.1.1.1.1.1.3.2">1</cn><ci id="S3.E5.m1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3.3">𝑠</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E5.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.E5.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.2">𝑘</ci><cn id="S3.E5.m1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E5.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.3">𝑠</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1"><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.2">𝑒</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2">𝜌</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle KDPS=\frac{1}{s}\sum_{k=1}^{s}(e_{k}+\rho_{k}).</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_K italic_D italic_P italic_S = divide start_ARG 1 end_ARG start_ARG italic_s end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_ρ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The impact of the well-known language evaluation metrics perplexity, BLUE, TER, ROUGE and chrF++ on LLM generated responses varying in certainty is illustrated by the examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.T4" title="TABLE IV ‣ III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">IV</span></a>. Here, we observe that for relatively close-ended user queries, metrics (<math alttext="Per" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">P</mi><mo id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">e</mi><mo id="S3.SS1.p4.1.m1.1.1.1a" xref="S3.SS1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS1.p4.1.m1.1.1.4" xref="S3.SS1.p4.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑃</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝑒</ci><ci id="S3.SS1.p4.1.m1.1.1.4.cmml" xref="S3.SS1.p4.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>, TER) have considerably low whereas (BLUE, ROUGE and chrF++) have higher values. Our goal with two-stage FT process is to ensure similar qualities in generated text from simple prompts and tabular contextual data for paragraph level financial report generation.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Examples of text generation Scores on well known user queries.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.3" style="width:671.1pt;height:100pt;vertical-align:-6.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-180.7pt,25.2pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.3.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.3.3.4.1.1.1">User-Query</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.3.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.3.3.4.1.2.1">Response</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.3.3.4.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.3.3.4.1.3.1">Metrics</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.1.2">Was Constantinople officially renamed Istanbul in 1930?</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.1.1.1.3">Yes, Constantinople was officially renamed Istanbul in 1930 by the Turkish government.</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.1">
<math alttext="Per" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.1.m1.1a"><mrow id="S3.T4.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.cmml"><mi id="S3.T4.1.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.1.m1.1.1.2.cmml">P</mi><mo id="S3.T4.1.1.1.1.m1.1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.1.1.1.1.m1.1.1.3" xref="S3.T4.1.1.1.1.m1.1.1.3.cmml">e</mi><mo id="S3.T4.1.1.1.1.m1.1.1.1a" xref="S3.T4.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.1.1.1.1.m1.1.1.4" xref="S3.T4.1.1.1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1"><times id="S3.T4.1.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1.1"></times><ci id="S3.T4.1.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.1.m1.1.1.2">𝑃</ci><ci id="S3.T4.1.1.1.1.m1.1.1.3.cmml" xref="S3.T4.1.1.1.1.m1.1.1.3">𝑒</ci><ci id="S3.T4.1.1.1.1.m1.1.1.4.cmml" xref="S3.T4.1.1.1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>=1.03, BLEU=27.82, ROUGE=0.63, chrF++=78.16, TER=100</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.2.2.2.2">Did the Ottoman Empire capture Constantinople in 1453?</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.2.2.2.3">Yes, the Ottoman Empire captured Constantinople in 1453, marking the end of the</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.2.2.2.1">
<math alttext="Per" class="ltx_Math" display="inline" id="S3.T4.2.2.2.1.m1.1"><semantics id="S3.T4.2.2.2.1.m1.1a"><mrow id="S3.T4.2.2.2.1.m1.1.1" xref="S3.T4.2.2.2.1.m1.1.1.cmml"><mi id="S3.T4.2.2.2.1.m1.1.1.2" xref="S3.T4.2.2.2.1.m1.1.1.2.cmml">P</mi><mo id="S3.T4.2.2.2.1.m1.1.1.1" xref="S3.T4.2.2.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.2.2.2.1.m1.1.1.3" xref="S3.T4.2.2.2.1.m1.1.1.3.cmml">e</mi><mo id="S3.T4.2.2.2.1.m1.1.1.1a" xref="S3.T4.2.2.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.2.2.2.1.m1.1.1.4" xref="S3.T4.2.2.2.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.1.m1.1b"><apply id="S3.T4.2.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.2.1.m1.1.1"><times id="S3.T4.2.2.2.1.m1.1.1.1.cmml" xref="S3.T4.2.2.2.1.m1.1.1.1"></times><ci id="S3.T4.2.2.2.1.m1.1.1.2.cmml" xref="S3.T4.2.2.2.1.m1.1.1.2">𝑃</ci><ci id="S3.T4.2.2.2.1.m1.1.1.3.cmml" xref="S3.T4.2.2.2.1.m1.1.1.3">𝑒</ci><ci id="S3.T4.2.2.2.1.m1.1.1.4.cmml" xref="S3.T4.2.2.2.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.2.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>=1.16, BLEU=4.3, ROUGE=0.24, chrF++=44.93, TER=550.0</td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.5.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S3.T4.3.3.5.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.3.3.5.2.2">Byzantine Empire and the beginning of Ottoman rule in the region.</td>
<td class="ltx_td ltx_border_r" id="S3.T4.3.3.5.2.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.3.3.6.3.1">How does Istanbul’s unique geographical position, between</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.3.3.6.3.2">Istanbul’s unique geographical position as a city that straddles Europe and Asia</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T4.3.3.6.3.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S3.T4.3.3.7.4.1">Europe and Asia, affect its role in international trade and diplomacy?</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.3.3.7.4.2">has historically made it a key hub for international trade and diplomacy. ….</td>
<td class="ltx_td ltx_border_r" id="S3.T4.3.3.7.4.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.3">
<td class="ltx_td ltx_border_l ltx_border_r" id="S3.T4.3.3.3.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.3.3.3.3">Overall, Istanbul’s unique geographical position as a city continues to shape its role</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.3.3.3.1">
<math alttext="Per" class="ltx_Math" display="inline" id="S3.T4.3.3.3.1.m1.1"><semantics id="S3.T4.3.3.3.1.m1.1a"><mrow id="S3.T4.3.3.3.1.m1.1.1" xref="S3.T4.3.3.3.1.m1.1.1.cmml"><mi id="S3.T4.3.3.3.1.m1.1.1.2" xref="S3.T4.3.3.3.1.m1.1.1.2.cmml">P</mi><mo id="S3.T4.3.3.3.1.m1.1.1.1" xref="S3.T4.3.3.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.3.3.3.1.m1.1.1.3" xref="S3.T4.3.3.3.1.m1.1.1.3.cmml">e</mi><mo id="S3.T4.3.3.3.1.m1.1.1.1a" xref="S3.T4.3.3.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.T4.3.3.3.1.m1.1.1.4" xref="S3.T4.3.3.3.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.1.m1.1b"><apply id="S3.T4.3.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.3.1.m1.1.1"><times id="S3.T4.3.3.3.1.m1.1.1.1.cmml" xref="S3.T4.3.3.3.1.m1.1.1.1"></times><ci id="S3.T4.3.3.3.1.m1.1.1.2.cmml" xref="S3.T4.3.3.3.1.m1.1.1.2">𝑃</ci><ci id="S3.T4.3.3.3.1.m1.1.1.3.cmml" xref="S3.T4.3.3.3.1.m1.1.1.3">𝑒</ci><ci id="S3.T4.3.3.3.1.m1.1.1.4.cmml" xref="S3.T4.3.3.3.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.3.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>=1.2, BLEU=5.01, ROUGE=0.16, chrF++=33.48, TER=952.63</td>
</tr>
<tr class="ltx_tr" id="S3.T4.3.3.8.5">
<td class="ltx_td ltx_border_b ltx_border_l ltx_border_r" id="S3.T4.3.3.8.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S3.T4.3.3.8.5.2">in international trade and diplomacy today.</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S3.T4.3.3.8.5.3"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Data Preparation and FT-process</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">For our two-stage FT process, we generate prompt-response pairs from public-domain financial reports. The first step here is to generate sample data tables that can serve as inputs. For this, we generate a simple prompt that takes 150 financial report text as input and outputs the data in tabular format. This tabular data is further augmented by small variations to the numbers and schema through minimal LLM prompt alterations and manually verified for accuracy. This resulted in 1000 samples of tabular data and their adjoining reports. Next, we begin the two-stage FT process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For the first FT step, we generate a variety of sections from the financial reports such as {introduction, analysis, conclusion, discussion} sections. Each section requires specific verbiage, prompt-completions, specific instructions and examples. The prompt aspects that remain unchanged across all the prompt-completion samples are style-transfer attributes like tone, assertiveness, and persona. Thus, with minimal changes to a GPT4o LLM prompt, we obtain several prompts and their completions (expected outputs) that correspond to a variety of financial report sections. This process of reverse engineering the data required for style-transfer requires minimal manual supervision and basic prompt engineering for data augmentation purposes as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.F6" title="Figure 6 ‣ III-B Data Preparation and FT-process ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S3.F6.g1" src="extracted/5864498/images/prep.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Data Preparation process for the two-stage FT process.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The second error-correction step of the FT process involves manual detection of hallucinations, incomplete sentences and poor quality sentences resulting from feeding the above pre-processed data to GPT3.5 for 100 epochs while monitoring for [<math alttext="Per,TER,BLUE" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.3"><semantics id="S3.SS2.p3.1.m1.3a"><mrow id="S3.SS2.p3.1.m1.3.3.3" xref="S3.SS2.p3.1.m1.3.3.4.cmml"><mrow id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">P</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml">e</mi><mo id="S3.SS2.p3.1.m1.1.1.1.1.1a" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.1.1.1.1.4" xref="S3.SS2.p3.1.m1.1.1.1.1.4.cmml">r</mi></mrow><mo id="S3.SS2.p3.1.m1.3.3.3.4" xref="S3.SS2.p3.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.cmml">T</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.3.cmml">E</mi><mo id="S3.SS2.p3.1.m1.2.2.2.2.1a" xref="S3.SS2.p3.1.m1.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.4" xref="S3.SS2.p3.1.m1.2.2.2.2.4.cmml">R</mi></mrow><mo id="S3.SS2.p3.1.m1.3.3.3.5" xref="S3.SS2.p3.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS2.p3.1.m1.3.3.3.3" xref="S3.SS2.p3.1.m1.3.3.3.3.cmml"><mi id="S3.SS2.p3.1.m1.3.3.3.3.2" xref="S3.SS2.p3.1.m1.3.3.3.3.2.cmml">B</mi><mo id="S3.SS2.p3.1.m1.3.3.3.3.1" xref="S3.SS2.p3.1.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.3.3.3.3.3" xref="S3.SS2.p3.1.m1.3.3.3.3.3.cmml">L</mi><mo id="S3.SS2.p3.1.m1.3.3.3.3.1a" xref="S3.SS2.p3.1.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.3.3.3.3.4" xref="S3.SS2.p3.1.m1.3.3.3.3.4.cmml">U</mi><mo id="S3.SS2.p3.1.m1.3.3.3.3.1b" xref="S3.SS2.p3.1.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.m1.3.3.3.3.5" xref="S3.SS2.p3.1.m1.3.3.3.3.5.cmml">E</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.3b"><list id="S3.SS2.p3.1.m1.3.3.4.cmml" xref="S3.SS2.p3.1.m1.3.3.3"><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"></times><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">𝑃</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3">𝑒</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.4">𝑟</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2"><times id="S3.SS2.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2">𝑇</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.3">𝐸</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.4.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.4">𝑅</ci></apply><apply id="S3.SS2.p3.1.m1.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3"><times id="S3.SS2.p3.1.m1.3.3.3.3.1.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3.1"></times><ci id="S3.SS2.p3.1.m1.3.3.3.3.2.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3.2">𝐵</ci><ci id="S3.SS2.p3.1.m1.3.3.3.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3.3">𝐿</ci><ci id="S3.SS2.p3.1.m1.3.3.3.3.4.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3.4">𝑈</ci><ci id="S3.SS2.p3.1.m1.3.3.3.3.5.cmml" xref="S3.SS2.p3.1.m1.3.3.3.3.5">𝐸</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.3c">Per,TER,BLUE</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.3d">italic_P italic_e italic_r , italic_T italic_E italic_R , italic_B italic_L italic_U italic_E</annotation></semantics></math>] metrics. Samples of <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">poor quality sentences</span> and hallucinations are manually corrected, and from these corrections, we generate 800 samples of poor sentences and equivalent good sentences that can then be fed to GPT3.5 for the second-FT step. Additionally, for end-to-end validation, two 10-page detailed financial reports are manually annotated for verbiage and quality of sentences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments and Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We perform three sets of experiments to assess the <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">quality</span> of generated text after the proposed two-stage FT process. First, we analyze the average <math alttext="Per" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">P</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">e</mi><mo id="S4.p1.1.m1.1.1.1a" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.4" xref="S4.p1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">𝑒</ci><ci id="S4.p1.1.m1.1.1.4.cmml" xref="S4.p1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math> per generated section in comparison to the manually annotated reports using out-of-the-box (vanlillaGPT3.5) model, single stage FT model (that is fine-tuned for style), and two-stage FT model (that is fine-tuned for hallucinations). Second, we qualitatively evaluate the generated financial sections to explain the metrics for each model at a sentence level per generated paragraph. Third, we track the NLG metrics from section <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS1" title="III-A Notation and Metrics ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> to analyze the quality of generated financial report sections across all the generated sample paragraphs and sections.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Validation and Test Prompts</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">The <math alttext="Per" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">P</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">e</mi><mo id="S4.SS1.p1.1.m1.1.1.1a" xref="S4.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.1.m1.1.1.4" xref="S4.SS1.p1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝑒</ci><ci id="S4.SS1.p1.1.m1.1.1.4.cmml" xref="S4.SS1.p1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math> for the two-stage fine-tuned model on the two manually validated reports is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.T5" title="TABLE V ‣ IV-A Validation and Test Prompts ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">V</span></a>. Here we observe that the FT <math alttext="Per" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">P</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">e</mi><mo id="S4.SS1.p1.2.m2.1.1.1a" xref="S4.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.2.m2.1.1.4" xref="S4.SS1.p1.2.m2.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></times><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝑃</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">𝑒</ci><ci id="S4.SS1.p1.2.m2.1.1.4.cmml" xref="S4.SS1.p1.2.m2.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_P italic_e italic_r</annotation></semantics></math> are consistently lower and hence better per section of the financial reports.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Average Sectional perplexity in fine-tuned validation reports.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T5.2" style="width:340.4pt;height:243pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.9pt,13.5pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.2.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.3.1.1.1">Report 1</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T5.2.2.3.1.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T5.2.2.3.1.3"></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.2.3.1">Section</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">vanlillaGPT3.5 <math alttext="Per" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T5.1.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.1.m1.1.1.2.cmml">P</mi><mo id="S4.T5.1.1.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.1.m1.1.1.3.cmml">e</mi><mo id="S4.T5.1.1.1.1.1.m1.1.1.1a" xref="S4.T5.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T5.1.1.1.1.1.m1.1.1.4" xref="S4.T5.1.1.1.1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1"><times id="S4.T5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.1"></times><ci id="S4.T5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2">𝑃</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.3">𝑒</ci><ci id="S4.T5.1.1.1.1.1.m1.1.1.4.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.2.2.1">Two-step FT, <math alttext="Per" class="ltx_Math" display="inline" id="S4.T5.2.2.2.2.1.m1.1"><semantics id="S4.T5.2.2.2.2.1.m1.1a"><mrow id="S4.T5.2.2.2.2.1.m1.1.1" xref="S4.T5.2.2.2.2.1.m1.1.1.cmml"><mi id="S4.T5.2.2.2.2.1.m1.1.1.2" xref="S4.T5.2.2.2.2.1.m1.1.1.2.cmml">P</mi><mo id="S4.T5.2.2.2.2.1.m1.1.1.1" xref="S4.T5.2.2.2.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.3" xref="S4.T5.2.2.2.2.1.m1.1.1.3.cmml">e</mi><mo id="S4.T5.2.2.2.2.1.m1.1.1.1a" xref="S4.T5.2.2.2.2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T5.2.2.2.2.1.m1.1.1.4" xref="S4.T5.2.2.2.2.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.1.m1.1b"><apply id="S4.T5.2.2.2.2.1.m1.1.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1"><times id="S4.T5.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.1"></times><ci id="S4.T5.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.2">𝑃</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.3">𝑒</ci><ci id="S4.T5.2.2.2.2.1.m1.1.1.4.cmml" xref="S4.T5.2.2.2.2.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.2.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.2.2.4.2.1">Introduction</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.2.4.2.2">1.58</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.2.4.2.3">1.25</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.5.3.1">Growth Outlook</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.5.3.2">1.24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.5.3.3">1.21</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.6.4.1">Service Group Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.6.4.2">1.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.6.4.3">1.26</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.7.5.1">Industry Performance</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.7.5.2">1.35</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.7.5.3">1.33</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.8.6.1">Performance Highlights</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.8.6.2">1.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.8.6.3">1.51</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.2.2.9.7.1"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.9.7.1.1">Report 2</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T5.2.2.9.7.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T5.2.2.9.7.3"></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T5.2.2.10.8.1">Introduction</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.2.10.8.2">1.245</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.2.2.10.8.3">1.21</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.11.9.1">Financial Review</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.11.9.2">1.186</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.11.9.3">1.08</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.12.10.1">New Bookings</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.12.10.2">1.225</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.12.10.3">1.07</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.13.11.1">Revenues by Geographic Market</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.13.11.2">1.083</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.13.11.3">1.07</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.14.12.1">Revenues by Industry Group</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.14.12.2">1.034</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.14.12.3">1.005</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S4.T5.2.2.15.13.1">Returning Cash to Shareholders</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.15.13.2">1.205</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.2.2.15.13.3">1.15</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2.16.14">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S4.T5.2.2.16.14.1">Business Outlook</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T5.2.2.16.14.2">1.163</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T5.2.2.16.14.3">1.109</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Additionally, the average scaled KDPS for vanlillaGPT3.5 and two-step FT model on both reports are 0.75 and 0.8 respectively. This demonstrates compound sentences and increased number of entity relations per sentence in the FT model.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Next, we assess the report generation performances for the following 3 prompts shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F7" title="Figure 7 ‣ IV-A Validation and Test Prompts ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F8" title="Figure 8 ‣ IV-A Validation and Test Prompts ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="144" id="S4.F7.g1" src="extracted/5864498/images/p12.png" width="339"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Prompt 1 and 2 used for testing performance of financial reports</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S4.F8.g1" src="extracted/5864498/images/p3.png" width="339"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Prompt 3 used for testing performance of financial reports</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Quantitative Analysis of Two-stage FT model</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To ensure style transfer and hallucination control, the one-stage FT model (style only) involves fine-tuning GPT3.5 model with 1000 prompt-completions generated using the method explained in section <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S3.SS2" title="III-B Data Preparation and FT-process ‣ III Methods and Data ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> over 100 epochs. Next, we query 112 questions to the one-stage FT model and check the performance for hallucinations. For the two-stage FT model (style and hallucination) an additional FT process involves 800 samples of hallucinations that are manually corrected and subjected to further tuning for 100 epochs. The quality of responses to the 112 user queries can be categorized as {Correct, Hallucinations, Incomplete} and shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F9" title="Figure 9 ‣ IV-B Quantitative Analysis of Two-stage FT model ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">9</span></a>. We observe that the two-stage FT model doubles correct answering capability (increase from 42% to 85%) and significantly reduces hallucinations (reduction from 20% to 7%) when compared to an untrained GPT3.5 model.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="S4.F9.g1" src="extracted/5864498/images/R2.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Question answering performances on the untrained, one-stage and two-stage FT models.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Additionally, the quality of generated text is further shown in terms of scaled metrics in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F10" title="Figure 10 ‣ IV-B Quantitative Analysis of Two-stage FT model ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">10</span></a>. Here we observe that the two-stage FT model has consistently high BLEU, ROUGE and chrF++ metrics when compared to the untrained and one-stage FT models.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="263" id="S4.F10.g1" src="extracted/5864498/images/R1.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Quality of generated text for the untrained, one-stage and two-stage FT models.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Qualitative Analysis: Hallucination, creativity monitoring</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In the previous subsection, we observe the importance in two-stage FT process to control for hallucinations and incomplete sentences. However, since creative and hallucinated words/tokens have similar probabilistic nature, we qualitatively assess the reports generated from the prompts shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F7" title="Figure 7 ‣ IV-A Validation and Test Prompts ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F8" title="Figure 8 ‣ IV-A Validation and Test Prompts ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">8</span></a> for hallucinations and creativity. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F11" title="Figure 11 ‣ IV-C Qualitative Analysis: Hallucination, creativity monitoring ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">11</span></a> demonstrates sample reports generated from untrained, one-stage FT and two-stage FT models, respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S4.F11.g1" src="extracted/5864498/images/two_stage_explain.png" width="648"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Examples of untrained, one-step FT and two-step FT outcomes. The last two lines from the untrained and one-stage FT model have minimal information and entities. Our goal is to reduce such sentences with low information content.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F12" title="Figure 12 ‣ IV-C Qualitative Analysis: Hallucination, creativity monitoring ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">12</span></a> we observe that the cross-entropy of the generated text significantly reduces after each stage of FT leading to less hallucinations and more certain text generation.</p>
</div>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F12.g1" src="extracted/5864498/images/two_stage_ce.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Scatter plot for cross entropy loss per sentence after each stage of fine-tuning.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.05365v2#S4.F13" title="Figure 13 ‣ IV-C Qualitative Analysis: Hallucination, creativity monitoring ‣ IV Experiments and Results ‣ FiSTECH: Financial Style Transfer to Enhance Creativity without Hallucinations in LLMs"><span class="ltx_text ltx_ref_tag">13</span></a> shows the ASLS metrics at sentence level for the untrained, one-stage FT and two-stage FT models, respectively. We observe that for the untrained and one-stage FT models, the last two lines have the lowest log-loss, or low certainty for text generation. These sentences have a higher tendency to contain hallucinations or creativity. However, for the two-stage FT model, the hallucinations significantly reduce, thereby leading to more certain and higher quality of text generation.</p>
</div>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S4.F13.g1" src="extracted/5864498/images/two_stage_scatter.png" width="319"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Scatter plot for ASLS before and after each stage of FT. The data points corresponding to the last two sentences in the untrained and one-stage FT models are circled with low ASLS. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusions and Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work we present a two-stage LLM fine tuning process, starting from data pre-processing to monitoring metrics to enable financial report generation that is similar in style to a financial analyst. Our goal is to minimize domain-specific fine tuning costs and explore data processing methods to enhance self learning and minimize hallucinations from LLMs. Our generalizable two-stage FT process incurs $16 costs for the final version of fine-tuned GPT3.5 LLM for the financial domain. The data pre-processing steps are augmented with minimal GPT4o prompts and the two-stage FT process doubles the correct response rate and halves the rate of hallucinations and incomplete responses. This makes the proposed two-stage FT model the preferred choice for generating accurate, coherent, and appropriately detailed financial report generation use-case.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The novelty of this work lies in the nature of the FT process, since we allow hallucinations in the first stage. In the second stage the hallucinations are corrected and the LLM is allowed to self-learn from the corrections. This process enhances the creativity and compound sentence generation capabilities of the LLM, that enables domain-specific fine-tuning at low costs. It is noteworthy that analysis on a variety of versions of LLM models on financial reports reveal the following nature of generated text.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Closed-ended questions generally have lower <math alttext="Per" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mrow id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml"><mi id="S5.I1.i1.p1.1.m1.1.1.2" xref="S5.I1.i1.p1.1.m1.1.1.2.cmml">P</mi><mo id="S5.I1.i1.p1.1.m1.1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.I1.i1.p1.1.m1.1.1.3" xref="S5.I1.i1.p1.1.m1.1.1.3.cmml">e</mi><mo id="S5.I1.i1.p1.1.m1.1.1.1a" xref="S5.I1.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.I1.i1.p1.1.m1.1.1.4" xref="S5.I1.i1.p1.1.m1.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><apply id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1"><times id="S5.I1.i1.p1.1.m1.1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1.1"></times><ci id="S5.I1.i1.p1.1.m1.1.1.2.cmml" xref="S5.I1.i1.p1.1.m1.1.1.2">𝑃</ci><ci id="S5.I1.i1.p1.1.m1.1.1.3.cmml" xref="S5.I1.i1.p1.1.m1.1.1.3">𝑒</ci><ci id="S5.I1.i1.p1.1.m1.1.1.4.cmml" xref="S5.I1.i1.p1.1.m1.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">Per</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_P italic_e italic_r</annotation></semantics></math>, higher ROUGE and lower TER scores indicating that they are easier for the model to predict based on reference context.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Open-ended questions tend to have lower BLEU, higher TER and perplexity suggesting more uncertainty in the generated text.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.p2.2">This work aims to generate close-ended text quality for open-ended questions where the prompts include minimal instructions and tabular data as input for multiple paragraph generation tasks. Additionally, we observe that for the labeled validation reports, the fine-tuned models generally outperform the untrained model with higher average coherence and correctness, while the relevance of data remains fairly unchanged. For question-answering from minimal prompts, the FT models excel in depth and creativity, although creativity is not a desired trait for financial reports. However, untrained models typically demonstrate more conciseness. Thus, without the need for creativity, the FT models demonstrate superior performance in coherence and correctness, combined with appropriate depth. This makes the proposed two-stage FT process the preferred choice for sectional financial report generation. Additionally, manual verification reveals that the untrained models tend to over-explain (low ASLS), leading to unnecessary depth, which is not preferred for financial reports.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Future works will be directed towards further controlling for hallucinations and creativity by monitoring weights and biases of the early layers of the LLMs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. I. Roumeliotis, N. D. Tselikas, and D. K. Nasiopoulos, “Llms in e-commerce: a comparative analysis of gpt and llama models in product review evaluation,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Natural Language Processing Journal</em>, vol. 6, p. 100056, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Roychowdhury, A. Alvarez, B. Moore, M. Krema, M. P. Gelpi, P. Agrawal, F. M. Rodríguez, Á. Rodríguez, J. R. Cabrejas, P. M. Serrano <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Hallucination-minimized data-to-answer framework for financial decision-makers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">2023 IEEE International Conference on Big Data (BigData)</em>.   IEEE, 2023, pp. 4693–4702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Ge, W. Hua, K. Mei, J. Tan, S. Xu, Z. Li, Y. Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>, “Openagi: When llm meets domain experts,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large language models and knowledge graphs: A roadmap,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
X.-Y. Liu, G. Wang, H. Yang, and D. Zha, “Fingpt: Democratizing internet-scale data for financial large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">NeurIPS Workshop on Instruction Tuning and Instruction Following</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large language model for finance,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2303.17564</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Kau, X. He, A. Nambissan, A. Astudillo, H. Yin, and A. Aryani, “Combining knowledge graphs and large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2407.06564</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Lee, “A mathematical investigation of hallucination and creativity in gpt models,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Mathematics</em>, vol. 11, no. 10, p. 2320, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X. Jiang, Y. Tian, F. Hua, C. Xu, Y. Wang, and J. Guo, “A survey on large language model hallucination via a creativity perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv e-prints</em>, pp. arXiv–2402, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. L. Dong, “The journey to a knowledgeable assistant with retrieval-augmented generation (rag),” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>, 2024, pp. 4–4.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Sun, Y. Xu, H. Zha, Y. Liu, and X. L. Dong, “Head-to-tail: How knowledgeable are large language models (llms)? aka will llms replace knowledge graphs?” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, 2024, pp. 311–325.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. H. Huang, H. Wang, and Y. Yang, “Finbert: A large language model for extracting information from financial text,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Contemporary Accounting Research</em>, vol. 40, no. 2, pp. 806–841, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Ouyang, Y. Liu, S. Li, R. Bao, K. Harimoto, and X. Sun, “Modal-adaptive knowledge-enhanced graph-based financial prediction from monetary policy conference calls with llm,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Xing, “Designing heterogeneous llm agents for financial sentiment analysis,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X. Deng, V. Bashlovkina, F. Han, S. Baumgartner, and M. Bendersky, “What do llms know about financial markets? a case study on reddit market sentiment analysis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Companion Proceedings of the ACM Web Conference 2023</em>, 2023, p. 107–110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X.-Y. Liu, G. Wang, H. Yang, and D. Zha, “Fingpt: Democratizing internet-scale data for financial large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Gupta, V. Dengre, H. A. Kheruwala, and M. Shah, “Comprehensive review of text-mining applications in finance,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Financial Innovation</em>, vol. 6, pp. 1–25, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
U. Gupta, “Gpt-investar: Enhancing stock investment strategies through annual report analysis with large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Faccia, J. McDonald, and B. George, “Nlp sentiment analysis and accounting transparency: A new era of financial record keeping,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Computers</em>, vol. 13, no. 1, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Rao and J. Tetreault, “Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>.   Association for Computational Linguistics, 2018, pp. 129–140.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Lai, A. Toral, and M. Nissim, “Multidimensional evaluation for text style transfer using chatgpt,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
V. John, L. Mou, H. Bahuleyan, and O. Vechtomova, “Disentangled representation learning for non-parallel text style transfer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>.   Association for Computational Linguistics, 2019, pp. 424–434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Pu and V. Demberg, “Chatgpt vs human-authored text: Insights into controllable text summarization and sentence style transfer,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G. Luo, Y. T. Han, L. Mou, and M. Firdaus, “Prompt-based editing for text style transfer,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Tao, D. Xi, Z. Li, L. Tang, and W. Xu, “Cat-llm: Prompting large language models with text style definition for chinese article-style transfer,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Rosenfeld and T. Lazebnik, “Whose llm is it anyway? linguistic comparison and llm attribution for gpt-3.5, gpt-4 and bard,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L. Pan, Y. Lan, Y. Li, and W. Qian, “Unsupervised text style transfer via llms and attention masking with multi-way interactions,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
G. Christopoulos, “The impact of language family on d2t generation in under-resourced languages,” Master’s thesis, Utrecht Unicversity, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.-Y. Yao, K.-P. Ning, Z.-H. Liu, M.-N. Ning, and L. Yuan, “Llm lies: Hallucinations are not bugs, but features as adversarial examples,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2310.01469</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>, “Gpt-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 01:22:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
