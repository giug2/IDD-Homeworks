<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1908.07170] Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data</title><meta property="og:description" content="Chest radiographs are frequently used to verify the correct intubation of patients in the emergency room. Fast and accurate identification and localization of the endotracheal (ET) tube is critical for the patient. In ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1908.07170">

<!--Generated on Sat Mar  2 15:00:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="ET tube Chest Radiographs Deep Learning CNN Classification Segmentation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>RADLogics Ltd., Tel-Aviv, Israel</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel
<span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{maayan,rula,hayit}@radlogics.com</span></span></span>
<br class="ltx_break"><span id="id2.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>hayit@eng.tau.ac.il</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maayan¬†Frid-Adar
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-8246-1411" title="ORCID identifier" class="ltx_ref">0000-0002-8246-1411</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rula¬†Amer
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-5630-827X" title="ORCID identifier" class="ltx_ref">0000-0002-5630-827X</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hayit¬†Greenspan
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-6908-7552" title="ORCID identifier" class="ltx_ref">0000-0001-6908-7552</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Chest radiographs are frequently used to verify the correct intubation of patients in the emergency room. Fast and accurate identification and localization of the endotracheal (ET) tube is critical for the patient. In this study we propose a novel automated deep learning scheme for accurate detection and segmentation of the ET tubes. Development of automatic systems using deep learning networks for classification and segmentation require large annotated data which is not always available. Here we present an approach for synthesizing ET tubes in real X-ray images. We suggest a method for training the network, first with synthetic data and then with real X-ray images in a fine-tuning phase, which allows the network to train on thousands of cases without annotating any data.
The proposed method was tested on 477 real chest radiographs from a public dataset and reached AUC of 0.99 in classifying the presence vs. absence of the ET tube, along with outputting high quality ET tube segmentation maps.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>ET tube Chest Radiographs Deep Learning CNN Classification Segmentation
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The American College of Radiology recommends acquisition of chest radiographs following intubation, to ensure proper positioning of inserted tubes, for patients in the Intensive Care Unit (ICU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This is justified by studies, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which show that following intubation, physical examination identified tube malposition in only 2% to 5% of patients, whereas the radiograph revealed suboptimal positioning in 10% to 25%.
The ideal endotracheal (ET) tube position is in the mid trachea if the patient‚Äôs head is in the neutral position. Malposition of the ET tube can cause serious complications if not detected, especially where the tube is too low and selective bronchial intubation occurs. Such complications include a segmental or complete collapse of the contralateral lung, pneumothorax and atelectasis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Using the acquired radiographs, Computer-Aided Detection (CAD) systems can assist physicians in automatic detection of the ET tubes. Previous studies used classical approaches to determine seed points followed by a line tracking algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
A more recent study used a convolutional neural network (CNN) classification system for the presence or absence identification of the ET tube, with reported area under curve (AUC) of 0.99; and a second classification network for identification of low vs normal positioning of the ET tube, with AUC of 0.81 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The above studies used private datasets of portable chest X-ray images with a relatively small amount of cases: 64 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and 87 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> were used for the classical approaches; 300 cases were used for the CNN based solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Collecting and labeling chest radiographs for presence of ET tubes requires collaboration with hospitals and data extraction methods. For the ET tube detection and ET tube segmentation annotation, expert physicians are needed.
In this paper, we present an innovative solution for the task of detection and segmentation of ET tubes in chest radiographs, in the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">scenario of limited expert labeled data</span>: We use a <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">public dataset of chest radiographs</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> which allows us to collect a large data of normal and ET tube examples required for training a deep learning network. We then <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">synthesize ET tubes</span> on top of the X-ray images to generate ground truth data for the ET tube segmentation. Finally, we present a combined CNN for ET tube detection and segmentation in chest radiographs showing promising results.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1908.07170/assets/nih_example.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="469" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples from the NIH public dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1908.07170/assets/synth_ett2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="279" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Original X-ray images (left column), Clavicles segmentation and synthesized ET tube (middle column) and Synthesized ET tubes blended into the original X-ray (right column) </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this study, we apply a technique to insert synthetic ET tubes as an overlay to the original X-ray images
taken from a publicly available dataset of chest radiographs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (hereon will be called the NIH dataset). This dataset contains over 100,000 frontal view images,
many of them coming from ICU patients. While annotations are provided
for 14 lung diseases, no annotations exist for the presence of ET tubes
(or other tubes). A few sample images from the NIH dataset are shown in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> - the cases have high variability and many have poor image quality. We only used cases in Anterior-Posterior (AP) positioning to simulate intubated patients.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the first step of our proposed solution, we propose a technique to generate new images with ground truth ET tube segmentation masks.
The new image set we form will be used in a follow-up step, for training a combined CNN for detection and segmentation of ET tubes in chest
radiographs.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generating Synthetic Data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Generating the synthetic ET tubes over real X-ray images includes the following main steps as shown in Fig. <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: a) Selection of cases from the NIH dataset that do not contain ET tubes but may include other tubes (such as nasogastric (NG) tube, drainage tubes, catheters); b) Segmentation of the clavicles in order to localize the synthetic ET tube in the trachea area; c) Blending of generated synthetic ET tubes onto real X-ray images.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Clavicles Segmentation:</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">ET tubes are inserted into the trachea to allow artificial ventilation of the lungs. X-ray images are mostly aligned with the trachea located between the clavicles. Therefore, correct segmentation of the clavicles assists in placing the synthetic ET tube in the trachea area. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> a methodology for organ segmentation within Chest radiographs was presented, and shown to outperform alternate schemes, when tested on a common benchmark of 247 chest radiographs from the JSRT dataset, with ground-truth segmentation masks from the SCR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The architecture proposed is based on a modified U-Net based architecture, in which pre-trained encoder weights were used, based on VGG16. In the current work, we use a similar scheme: For training we input <math id="S2.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mrow id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.SSS1.p1.1.m1.1.1.2" xref="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS1.p1.1.m1.1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS1.SSS1.p1.1.m1.1.1.3" xref="S2.SS1.SSS1.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><apply id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1"><times id="S2.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S2.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">224\times 224</annotation></semantics></math> images, each normalized by its mean and standard deviation. We train the single-class segmentation model using Dice loss and threshold the output score maps to generate binary segmentation masks of the clavicles structure. This model gives us Dice coefficient score of 93.1% and Mean average contour distance of 0.871 mm.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/1908.07170/assets/generate_ett3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Realistic ET tubes generation: (a) ET tube cross profile; (b) 2D projection of the tube; (c) Sampled profile at different angles; (d) ET tube profile drawing; (e) Drawing the ET tube relative to the location of the clavicles (using middle point x and lower point y)</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Realistic ET Tubes Generation:</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">We present next our methodology for generating synthetic ET tubes for adult X-ray images. In our solution, we were
inspired by the work of Yi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that generated synthetic catheters on pediatric X-ray images.
Figure <a href="#S2.F3" title="Figure 3 ‚Ä£ 2.1.1 Clavicles Segmentation: ‚Ä£ 2.1 Generating Synthetic Data ‚Ä£ 2 Methodology ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts the ET tube generation steps:
First, we created a 2D simulation of the ET tube, as a hollow tabular object with a rectangular marker made of a radiopaque material. The tube and the marker are made from different materials and therefore have different attenuation components (c1 and c2).
The tube outer and inner width, d1 and d2, were chosen to fit an adult ET tube with strip thickness t.
We defined <math id="S2.SS1.SSS2.p1.1.m1.10" class="ltx_Math" alttext="\{c1,c2,d1,d2,t\}=\{0.1,1,160,100,20\}" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.10a"><mrow id="S2.SS1.SSS2.p1.1.m1.10.10" xref="S2.SS1.SSS2.p1.1.m1.10.10.cmml"><mrow id="S2.SS1.SSS2.p1.1.m1.10.10.4.4" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.5" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">{</mo><mrow id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.1" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.1.cmml">‚Äã</mo><mn id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.6" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">,</mo><mrow id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.2" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.1" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.1.cmml">‚Äã</mo><mn id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.3" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.3.cmml">2</mn></mrow><mo id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.7" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">,</mo><mrow id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.2" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.1" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.1.cmml">‚Äã</mo><mn id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.3" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.3.cmml">1</mn></mrow><mo id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.8" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">,</mo><mrow id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.2" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.1" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.1.cmml">‚Äã</mo><mn id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.3" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.3.cmml">2</mn></mrow><mo id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.9" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">,</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.10" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml">}</mo></mrow><mo id="S2.SS1.SSS2.p1.1.m1.10.10.5" xref="S2.SS1.SSS2.p1.1.m1.10.10.5.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.1.m1.10.10.6.2" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.1" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">{</mo><mn id="S2.SS1.SSS2.p1.1.m1.2.2" xref="S2.SS1.SSS2.p1.1.m1.2.2.cmml">0.1</mn><mo id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.2" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">,</mo><mn id="S2.SS1.SSS2.p1.1.m1.3.3" xref="S2.SS1.SSS2.p1.1.m1.3.3.cmml">1</mn><mo id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.3" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">,</mo><mn id="S2.SS1.SSS2.p1.1.m1.4.4" xref="S2.SS1.SSS2.p1.1.m1.4.4.cmml">160</mn><mo id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.4" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">,</mo><mn id="S2.SS1.SSS2.p1.1.m1.5.5" xref="S2.SS1.SSS2.p1.1.m1.5.5.cmml">100</mn><mo id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.5" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">,</mo><mn id="S2.SS1.SSS2.p1.1.m1.6.6" xref="S2.SS1.SSS2.p1.1.m1.6.6.cmml">20</mn><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.10.10.6.2.6" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.10b"><apply id="S2.SS1.SSS2.p1.1.m1.10.10.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10"><eq id="S2.SS1.SSS2.p1.1.m1.10.10.5.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.5"></eq><set id="S2.SS1.SSS2.p1.1.m1.10.10.4.5.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4"><apply id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1"><times id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.1"></times><ci id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.2">ùëê</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.7.7.1.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2"><times id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.1"></times><ci id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.2">ùëê</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.8.8.2.2.2.3">2</cn></apply><apply id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3"><times id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.1"></times><ci id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.2">ùëë</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.9.9.3.3.3.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4"><times id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.1"></times><ci id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.2">ùëë</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.4.4.4.3">2</cn></apply><ci id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">ùë°</ci></set><set id="S2.SS1.SSS2.p1.1.m1.10.10.6.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.10.10.6.2"><cn type="float" id="S2.SS1.SSS2.p1.1.m1.2.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.2.2">0.1</cn><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.3.3">1</cn><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.4.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4">160</cn><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.5.5.cmml" xref="S2.SS1.SSS2.p1.1.m1.5.5">100</cn><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.6.6.cmml" xref="S2.SS1.SSS2.p1.1.m1.6.6">20</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.10c">\{c1,c2,d1,d2,t\}=\{0.1,1,160,100,20\}</annotation></semantics></math>.
All the parameters above were selected
based on true physical properties of ET tubes or based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.4" class="ltx_p">In order to simulate the ET tube from different rotations, we projected the 2D profile using a Radon transform and sampled the projection at 0<sup id="S2.SS1.SSS2.p2.4.1" class="ltx_sup"><span id="S2.SS1.SSS2.p2.4.1.1" class="ltx_text ltx_font_italic">‚àò</span></sup>, 30<sup id="S2.SS1.SSS2.p2.4.2" class="ltx_sup"><span id="S2.SS1.SSS2.p2.4.2.1" class="ltx_text ltx_font_italic">‚àò</span></sup>, 60<sup id="S2.SS1.SSS2.p2.4.3" class="ltx_sup"><span id="S2.SS1.SSS2.p2.4.3.1" class="ltx_text ltx_font_italic">‚àò</span></sup>, 90<sup id="S2.SS1.SSS2.p2.4.4" class="ltx_sup"><span id="S2.SS1.SSS2.p2.4.4.1" class="ltx_text ltx_font_italic">‚àò</span></sup>. For each synthetic ET tube we selected one of the four profiles and sampled the values of 15 pixels for drawing the tube.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.2" class="ltx_p">The trace of the ET tube was simulated over the trachea area using the clavicles segmentation. we extracted the middle point x between the clavicles and the lowest point y. Then, we randomly selected 4 points with x offset of <math id="S2.SS1.SSS2.p3.1.m1.2" class="ltx_Math" alttext="[-2,2]" display="inline"><semantics id="S2.SS1.SSS2.p3.1.m1.2a"><mrow id="S2.SS1.SSS2.p3.1.m1.2.2.1" xref="S2.SS1.SSS2.p3.1.m1.2.2.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p3.1.m1.2.2.1.2" xref="S2.SS1.SSS2.p3.1.m1.2.2.2.cmml">[</mo><mrow id="S2.SS1.SSS2.p3.1.m1.2.2.1.1" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1.cmml"><mo id="S2.SS1.SSS2.p3.1.m1.2.2.1.1a" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1.cmml">‚àí</mo><mn id="S2.SS1.SSS2.p3.1.m1.2.2.1.1.2" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1.2.cmml">2</mn></mrow><mo id="S2.SS1.SSS2.p3.1.m1.2.2.1.3" xref="S2.SS1.SSS2.p3.1.m1.2.2.2.cmml">,</mo><mn id="S2.SS1.SSS2.p3.1.m1.1.1" xref="S2.SS1.SSS2.p3.1.m1.1.1.cmml">2</mn><mo stretchy="false" id="S2.SS1.SSS2.p3.1.m1.2.2.1.4" xref="S2.SS1.SSS2.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.1.m1.2b"><interval closure="closed" id="S2.SS1.SSS2.p3.1.m1.2.2.2.cmml" xref="S2.SS1.SSS2.p3.1.m1.2.2.1"><apply id="S2.SS1.SSS2.p3.1.m1.2.2.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1"><minus id="S2.SS1.SSS2.p3.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1"></minus><cn type="integer" id="S2.SS1.SSS2.p3.1.m1.2.2.1.1.2.cmml" xref="S2.SS1.SSS2.p3.1.m1.2.2.1.1.2">2</cn></apply><cn type="integer" id="S2.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p3.1.m1.1.1">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.1.m1.2c">[-2,2]</annotation></semantics></math> pixels and y-axis samples starting from 0 to y+offset of <math id="S2.SS1.SSS2.p3.2.m2.2" class="ltx_Math" alttext="[0,30]" display="inline"><semantics id="S2.SS1.SSS2.p3.2.m2.2a"><mrow id="S2.SS1.SSS2.p3.2.m2.2.3.2" xref="S2.SS1.SSS2.p3.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p3.2.m2.2.3.2.1" xref="S2.SS1.SSS2.p3.2.m2.2.3.1.cmml">[</mo><mn id="S2.SS1.SSS2.p3.2.m2.1.1" xref="S2.SS1.SSS2.p3.2.m2.1.1.cmml">0</mn><mo id="S2.SS1.SSS2.p3.2.m2.2.3.2.2" xref="S2.SS1.SSS2.p3.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS1.SSS2.p3.2.m2.2.2" xref="S2.SS1.SSS2.p3.2.m2.2.2.cmml">30</mn><mo stretchy="false" id="S2.SS1.SSS2.p3.2.m2.2.3.2.3" xref="S2.SS1.SSS2.p3.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p3.2.m2.2b"><interval closure="closed" id="S2.SS1.SSS2.p3.2.m2.2.3.1.cmml" xref="S2.SS1.SSS2.p3.2.m2.2.3.2"><cn type="integer" id="S2.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p3.2.m2.1.1">0</cn><cn type="integer" id="S2.SS1.SSS2.p3.2.m2.2.2.cmml" xref="S2.SS1.SSS2.p3.2.m2.2.2">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p3.2.m2.2c">[0,30]</annotation></semantics></math> pixels.
The random points compose a line using B-spline interpolation. Finally, we draw the tube sampled profile over the line.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p">The last step for creating a realistic X-ray with an ET tube is to merge the synthetic tube with the real X-ray image. We selected AP X-ray images from the NIH dataset that do not contain ET tubes and blended the random synthetic ET tubes into the images. We used a simple blending with random weights in the range of <math id="S2.SS1.SSS2.p4.1.m1.2" class="ltx_Math" alttext="[0.1,0.2]" display="inline"><semantics id="S2.SS1.SSS2.p4.1.m1.2a"><mrow id="S2.SS1.SSS2.p4.1.m1.2.3.2" xref="S2.SS1.SSS2.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p4.1.m1.2.3.2.1" xref="S2.SS1.SSS2.p4.1.m1.2.3.1.cmml">[</mo><mn id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml">0.1</mn><mo id="S2.SS1.SSS2.p4.1.m1.2.3.2.2" xref="S2.SS1.SSS2.p4.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.SSS2.p4.1.m1.2.2" xref="S2.SS1.SSS2.p4.1.m1.2.2.cmml">0.2</mn><mo stretchy="false" id="S2.SS1.SSS2.p4.1.m1.2.3.2.3" xref="S2.SS1.SSS2.p4.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.2b"><interval closure="closed" id="S2.SS1.SSS2.p4.1.m1.2.3.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.2.3.2"><cn type="float" id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1">0.1</cn><cn type="float" id="S2.SS1.SSS2.p4.1.m1.2.2.cmml" xref="S2.SS1.SSS2.p4.1.m1.2.2">0.2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.2c">[0.1,0.2]</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Detection and Segmentation CNN</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.5" class="ltx_p">We propose a combined CNN architecture for ET tube detection and segmentation in chest radiographs, ETT-Net, as depicted in Figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2.2 Detection and Segmentation CNN ‚Ä£ 2 Methodology ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The architecture is built from a VGG16 style encoder followed by two paths: One is a decoder that continues the U-Net shape for addressing the ET tube segmentation task; The other path summarizes the features extracted at the end of the encoder using a global pooling layer followed by two dense layers and a sigmoid, for addressing the ET tube classification task. We used pre-trained VGG16 weights as initialization for the encoder.
The two paths of the network are trained simultaneously for both the classification task and the segmentation task using a combined weighted Binary Cross-Entropy (BCE) loss and a Dice (D) loss as follows:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="\mathcal{L}=BCE(\hat{L},L)+\lambda D(\hat{S},S)" display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.5" xref="S2.E1.m1.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.5.2" xref="S2.E1.m1.4.5.2.cmml">‚Ñí</mi><mo id="S2.E1.m1.4.5.1" xref="S2.E1.m1.4.5.1.cmml">=</mo><mrow id="S2.E1.m1.4.5.3" xref="S2.E1.m1.4.5.3.cmml"><mrow id="S2.E1.m1.4.5.3.2" xref="S2.E1.m1.4.5.3.2.cmml"><mi id="S2.E1.m1.4.5.3.2.2" xref="S2.E1.m1.4.5.3.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.2.1" xref="S2.E1.m1.4.5.3.2.1.cmml">‚Äã</mo><mi id="S2.E1.m1.4.5.3.2.3" xref="S2.E1.m1.4.5.3.2.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.2.1a" xref="S2.E1.m1.4.5.3.2.1.cmml">‚Äã</mo><mi id="S2.E1.m1.4.5.3.2.4" xref="S2.E1.m1.4.5.3.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.2.1b" xref="S2.E1.m1.4.5.3.2.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.4.5.3.2.5.2" xref="S2.E1.m1.4.5.3.2.5.1.cmml"><mo stretchy="false" id="S2.E1.m1.4.5.3.2.5.2.1" xref="S2.E1.m1.4.5.3.2.5.1.cmml">(</mo><mover accent="true" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">L</mi><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">^</mo></mover><mo id="S2.E1.m1.4.5.3.2.5.2.2" xref="S2.E1.m1.4.5.3.2.5.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">L</mi><mo stretchy="false" id="S2.E1.m1.4.5.3.2.5.2.3" xref="S2.E1.m1.4.5.3.2.5.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.5.3.1" xref="S2.E1.m1.4.5.3.1.cmml">+</mo><mrow id="S2.E1.m1.4.5.3.3" xref="S2.E1.m1.4.5.3.3.cmml"><mi id="S2.E1.m1.4.5.3.3.2" xref="S2.E1.m1.4.5.3.3.2.cmml">Œª</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.3.1" xref="S2.E1.m1.4.5.3.3.1.cmml">‚Äã</mo><mi id="S2.E1.m1.4.5.3.3.3" xref="S2.E1.m1.4.5.3.3.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.3.1a" xref="S2.E1.m1.4.5.3.3.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.4.5.3.3.4.2" xref="S2.E1.m1.4.5.3.3.4.1.cmml"><mo stretchy="false" id="S2.E1.m1.4.5.3.3.4.2.1" xref="S2.E1.m1.4.5.3.3.4.1.cmml">(</mo><mover accent="true" id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mi id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">S</mi><mo id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml">^</mo></mover><mo id="S2.E1.m1.4.5.3.3.4.2.2" xref="S2.E1.m1.4.5.3.3.4.1.cmml">,</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">S</mi><mo stretchy="false" id="S2.E1.m1.4.5.3.3.4.2.3" xref="S2.E1.m1.4.5.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.5.cmml" xref="S2.E1.m1.4.5"><eq id="S2.E1.m1.4.5.1.cmml" xref="S2.E1.m1.4.5.1"></eq><ci id="S2.E1.m1.4.5.2.cmml" xref="S2.E1.m1.4.5.2">‚Ñí</ci><apply id="S2.E1.m1.4.5.3.cmml" xref="S2.E1.m1.4.5.3"><plus id="S2.E1.m1.4.5.3.1.cmml" xref="S2.E1.m1.4.5.3.1"></plus><apply id="S2.E1.m1.4.5.3.2.cmml" xref="S2.E1.m1.4.5.3.2"><times id="S2.E1.m1.4.5.3.2.1.cmml" xref="S2.E1.m1.4.5.3.2.1"></times><ci id="S2.E1.m1.4.5.3.2.2.cmml" xref="S2.E1.m1.4.5.3.2.2">ùêµ</ci><ci id="S2.E1.m1.4.5.3.2.3.cmml" xref="S2.E1.m1.4.5.3.2.3">ùê∂</ci><ci id="S2.E1.m1.4.5.3.2.4.cmml" xref="S2.E1.m1.4.5.3.2.4">ùê∏</ci><interval closure="open" id="S2.E1.m1.4.5.3.2.5.1.cmml" xref="S2.E1.m1.4.5.3.2.5.2"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">^</ci><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">ùêø</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ùêø</ci></interval></apply><apply id="S2.E1.m1.4.5.3.3.cmml" xref="S2.E1.m1.4.5.3.3"><times id="S2.E1.m1.4.5.3.3.1.cmml" xref="S2.E1.m1.4.5.3.3.1"></times><ci id="S2.E1.m1.4.5.3.3.2.cmml" xref="S2.E1.m1.4.5.3.3.2">ùúÜ</ci><ci id="S2.E1.m1.4.5.3.3.3.cmml" xref="S2.E1.m1.4.5.3.3.3">ùê∑</ci><interval closure="open" id="S2.E1.m1.4.5.3.3.4.1.cmml" xref="S2.E1.m1.4.5.3.3.4.2"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><ci id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1">^</ci><ci id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2">ùëÜ</ci></apply><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">ùëÜ</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\mathcal{L}=BCE(\hat{L},L)+\lambda D(\hat{S},S)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.4" class="ltx_p">where <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">L</annotation></semantics></math> and <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\hat{L}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mover accent="true" id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">L</mi><mo id="S2.SS2.p1.2.m2.1.1.1" xref="S2.SS2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><ci id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1">^</ci><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ùêø</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\hat{L}</annotation></semantics></math> are the classification output label and the ground truth label, respectively; S and <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\hat{S}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mover accent="true" id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">S</mi><mo id="S2.SS2.p1.3.m3.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><ci id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1">^</ci><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\hat{S}</annotation></semantics></math> are the segmentation output mask and the ground truth mask, respectively.
<math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\lambda</annotation></semantics></math> is the weight to balance between the loss components and was chosen (empirically) as 0.1.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.2" class="ltx_p">The network inputs X-ray image of size <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mrow id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mn id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><times id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">224</cn><cn type="integer" id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">224\times 224</annotation></semantics></math> pixels duplicated 3 times (to fit the pre-trained encoder), the corresponding ET tube segmentation mask and a binary label for the presence or absence of ET tube.
The input images are preprocessed with contrast limited adaptive histogram equalization (CLAHE) and normalized by their mean and standard deviation.
The segmentation masks can be a blank ‚Äùall zero‚Äù image where no ET tube is present or a binary segmentation mask of the ET tube. For the training we augmented the data using horizontal flipping and small rotations of <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\pm 10^{\circ}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mrow id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mo id="S2.SS2.p2.2.m2.1.1a" xref="S2.SS2.p2.2.m2.1.1.cmml">¬±</mo><msup id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml"><mn id="S2.SS2.p2.2.m2.1.1.2.2" xref="S2.SS2.p2.2.m2.1.1.2.2.cmml">10</mn><mo id="S2.SS2.p2.2.m2.1.1.2.3" xref="S2.SS2.p2.2.m2.1.1.2.3.cmml">‚àò</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">plus-or-minus</csymbol><apply id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.p2.2.m2.1.1.2">superscript</csymbol><cn type="integer" id="S2.SS2.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2.2">10</cn><compose id="S2.SS2.p2.2.m2.1.1.2.3.cmml" xref="S2.SS2.p2.2.m2.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\pm 10^{\circ}</annotation></semantics></math>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/1908.07170/assets/architecture.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="459" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>ETT-Net: The proposed architecture for detection and segmentation of ET tubes</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Two Phase Training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to train our suggested CNN using the synthesized data and still benefit from the existence of hundreds of X-ray images containing ET tube in the public dataset, we used a two phase training methodology. First, we trained the CNN using the generated data as explained in Section <a href="#S2.SS1" title="2.1 Generating Synthetic Data ‚Ä£ 2 Methodology ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. Then, we used all the AP cases from the NIH dataset for inference: We extracted real cases to fine-tune the network to improve the classification and segmentation performance on real chest radiographs data.
In both training phases, we trained the network for 50 epochs using an Adam optimizer with default parameters.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The data for the first training phase includes 1669 X-ray images: 869 synthetic examples with ET tube and 800 without. The segmentation masks of the positive cases were obtained using a simple binary threshold of the synthetic tube before the blending operation. For the second phase, we used all NIH dataset AP cases and set conditions on the classification and segmentation outputs of the model: Images with classification prob. higher than 0.8 and non zero segmentation map were selected as positive examples; Images with classification prob. lower than 0.01 and a zero segmentation map were selected as negative examples. These conditions resulted in 3972 positive X-ray images with ET tube, and 36557 X-ray images without ET tube. Overall, after balancing the data, we trained the second phase using 7944 real chest radiographs.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/1908.07170/assets/ROC_curve2.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="476" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>ROC curve after the two phases of training</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/1908.07170/assets/result_examples6.png" id="S3.F6.2.g1" class="ltx_graphics ltx_img_square" width="538" height="538" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Cases correctly classified with ET tube and their corresponding segmentation as overlay (in green color)</figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S3.F7" class="ltx_figure"><img src="/html/1908.07170/assets/heatmap2.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="469" height="94" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Heatmap examples using ET tube classification CNN</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Test Set</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The test set includes 479 real chest radiographs from the NIH dataset that were collected manually one time during the development and entirely independent from all training data. All cases are in AP view position, 232 cases with ET tube and 247 without ET tube. After collecting the cases, we verified that the label for each case is consistent with the presence of an ET tube. It is important to note that as we didn‚Äôt use manual annotations for the segmentation of the tube, the ground truth segmentation maps are not pixel-wise accurate; still, they represent an expected range for the ET tube position in the images.
The classification accuracy is an quantitative measure we can use. Thus, we tested our model using the AUC for the classification accuracy. The segmentation output was examined qualitatively.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison to state-of-the-art methods for classification between presence vs. absence of ET tube ; ‚Äù-‚Äù means that the score was not reported</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_top ltx_border_t"></td>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:31.3pt;">AUC</span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:99.6pt;">Sensitivity, Specificity</span>
</span>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.4.1.1" class="ltx_p" style="width:91.0pt;">Testing Size [pos, neg]</span>
</span>
</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1.1" class="ltx_p" style="width:113.8pt;">Ramakrishna et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.2.1.1" class="ltx_p" style="width:31.3pt;">-</span>
</span>
</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.3.1.1" class="ltx_p" style="width:99.6pt;">92.9%, 97.2%</span>
</span>
</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.4.1.1" class="ltx_p" style="width:91.0pt;">64 [28, 36]</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.1.1.1" class="ltx_p" style="width:113.8pt;">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.2.1.1" class="ltx_p" style="width:31.3pt;">0.95</span>
</span>
</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.3.1.1" class="ltx_p" style="width:99.6pt;">-</span>
</span>
</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.4.1.1" class="ltx_p" style="width:91.0pt;">87 [44, 43]</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.1.1.1" class="ltx_p" style="width:113.8pt;">Lakhani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.2.1.1" class="ltx_p" style="width:31.3pt;">0.99</span>
</span>
</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.3.1.1" class="ltx_p" style="width:99.6pt;">-</span>
</span>
</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.4.1.1" class="ltx_p" style="width:91.0pt;">60 [30, 30]</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.1.1.1" class="ltx_p" style="width:113.8pt;">DenseNet</span>
</span>
</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.2.1.1" class="ltx_p" style="width:31.3pt;">0.97</span>
</span>
</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.3.1.1" class="ltx_p" style="width:99.6pt;">89.2%, 93.0%</span>
</span>
</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.4.1.1" class="ltx_p" style="width:91.0pt;">479 [232, 247]</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.1.1.1" class="ltx_p" style="width:113.8pt;">ETT-Net - Phase1</span>
</span>
</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.2.1.1" class="ltx_p" style="width:31.3pt;">0.96</span>
</span>
</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.3.1.1" class="ltx_p" style="width:99.6pt;">89.2%, 93.0%</span>
</span>
</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.4.1.1" class="ltx_p" style="width:91.0pt;">479 [232, 247]</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.1.1.1" class="ltx_p" style="width:113.8pt;">ETT-Net - Phase2</span>
</span>
</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.2.1.1" class="ltx_p" style="width:31.3pt;">0.99</span>
</span>
</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.3.1.1" class="ltx_p" style="width:99.6pt;">95.5%, 96.5%</span>
</span>
</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.4.1.1" class="ltx_p" style="width:91.0pt;">479 [232, 247]</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Training the combined model for classification and segmentation of ET tubes on synthetic X-ray images, we reached an AUC of 0.962 in classification accuracy. Using fine-tuning on real X-ray images, the accuracy improved to an AUC of 0.987 with both sensitivity and specificity over 95% (Figure <a href="#S3.F6" title="Figure 6 ‚Ä£ 3.1 Two Phase Training ‚Ä£ 3 Experiments and Results ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).
Figure <a href="#S3.F6" title="Figure 6 ‚Ä£ 3.1 Two Phase Training ‚Ä£ 3 Experiments and Results ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows real chest radiographs from the test set that were classified correctly for presence of ET tube and their output segmentation maps.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We conducted an additional experiment using a different CNN architecture only for the classification task: identification of the tubes in real case scenarios. We trained a DenseNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> architecture with the same dataset we used in the second phase of the combined model - real cases with and without ET tube (n=7944) for 50 epochs and Adam optimizer. Training only for classification using a large real training data, we reached a high accuracy with an AUC of 0.975. Figure <a href="#S3.F7" title="Figure 7 ‚Ä£ 3.1 Two Phase Training ‚Ä£ 3 Experiments and Results ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a heatmap visualization of the last convolutional layer of the network. This visualization clearly indicates the localization on the ET tube area.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‚Ä£ 3.2 Test Set ‚Ä£ 3 Experiments and Results ‚Ä£ Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares state-of-the-art methods for the classification of presence or absent ET tube to our methods - results after the first phase of training using the ETT-Net model, Second phase results of the final model after fine-tuning with real examples and the only classification method using DenseNet.
The table shows the amount of testing images used for testing each method, with separation for cases with ET tube (pos) and without (neg). Our best model, ETT-Net - Phase2, reached high performance with a test set size of one magnitude more than state-of-the-art methods. In addition our model was trained and tested using free public dataset (of real ICU patients) without the need for manual annotations, in contrast with the other methods were the cases were hand picked and annotated.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we proposed an approach for training a combined deep learning network for the tasks of detection and segmentation of the ET tube in adult chest radiographs without collecting and annotating data.
We used a public dataset of X-ray images and synthesized realistic ET tubes blended into those images. We used the synthetic data as a first phase of training our model. Collecting real X-ray cases using the trained model, we continued to a second phase of training.
Both stages are trained using the ETT-Net - a combined CNN architecture for ET tube detection and segmentation in chest radiographs.
The combined model achieved a very high accuracy for the presence of ET tube in real ICU patients (0.99 AUC) using a test set which is ten times larger compared to previous studies and also outputs high quality segmentation maps that can assist in detection of the misplacement of the tubes.
We also showed accurate results (0.97 AUC) using a CNN for classification only where the synthetic cases are used only for retrieval of real cases from the public dataset.
Future work can include exploring a similar method for other tube types and combining them together in a multi-class detection and segmentation method.
The ideas presented in our paper for synthesizing data over public dataset images, can be used in other medical imaging domains (for example generating tumors over healthy patients in X-ray or CT studies).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Godoy, M.C., Leitman, B.S., de Groot, P.M., Vlahos, I., Naidich, D.P.: Chest radiography in the icu: part 1, evaluation of airway, enteric, and pleural tubes. American Journal of Roentgenology <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">198</span>(3), 563‚Äì-571 (2012)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Trotman-Dickenson, B.: Radiology in the intensive care unit (part i). Journal of intensive care medicine <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">18</span>(4), 198‚Äì-210 (2003)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ramakrishna, B., Brown, M., Goldin, J., Cagnon, C., Enzmann, D.: An improved automatic computer aided tube detection and labeling system on chest radiographs. In: Medical Imaging 2012: Computer-Aided Diagnosis. vol. 8315, p. 83150R. Inter-national Society for Optics and Photonics (2012)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, S., Zhang, M., Yao, L., Xu, W.: Endotracheal tubes positioning detection in adult portable chest radiography for intensive care unit. International Journal of Computer Assisted Radiology and <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">Surgery11</span>(11), 2049-‚Äì2057 (2016)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Lakhani, P.: Deep convolutional neural networks for endotracheal tube position and x-ray image classification: Challenges and opportunities. Journal of Digital Imaging <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">30</span>(4), 460‚Äì-468 (2017). <span id="bib.bib5.2.2" class="ltx_ERROR undefined">\doi</span>10.10007/s10278-017-9980-77

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: The IEEE Conference on Computer Vision and Pattern Recognition (2017)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Frid-Adar, M., Ben-Cohen, A., Amer, R., Greenspan, H.: Improving the segmen-tation of anatomical structures in chest radiographs using u-net with an imagenet pre-trained encoder. In: Image Analysis for Moving Organ, Breast, and Thoracic Images, MICCAI. pp. 159‚Äì-168. Springer International Publishing, Cham (2018)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ginneken, B.V., Stegmann, M.B., Loog, M.: Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database. Medical Image Analysis <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">10</span>(1), 19‚Äì-40 (2006)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yi, X., Adams, S., Babyn, P., Elnajmi, A.: Automatic catheter detection in pediatric x-ray images using a scale-recurrent network and synthetic data. CoRR (2018). http://arxiv.org/abs/1806.00921

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Huang, G., Liu, Z., Weinberger, K.Q.: Densely connected convolutional networks. CoRR (2016). http://arxiv.org/abs/1608.069936

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1908.07168" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1908.07170" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1908.07170">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1908.07170" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1908.07171" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 15:00:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
