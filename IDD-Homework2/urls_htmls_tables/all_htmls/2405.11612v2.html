<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sociotechnical Implications of Generative Artificial Intelligence for Information Access</title>
<!--Generated on Tue Jul 16 15:46:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.11612v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S1" title="In Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2" title="In Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Implications of generative AI for information access</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1" title="In 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Consequences and mechanisms</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="In 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Consequence: Information ecosystem disruption</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="In 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Consequence: Concentration of power</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="In 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Consequence: Marginalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS4" title="In 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Consequence: Innovation decay</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="In 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.5 </span>Consequence: Ecological impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2" title="In 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Risks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS1" title="In 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Risks to society</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS2" title="In 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Risks to IR research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS3" title="In 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Risks to environment</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3" title="In Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods to evaluate risks and impact</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS1" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Evaluating the impact of generative IR applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS2" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Threat identification, assessment, and modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS3" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation during model development</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS3.SSS1" title="In 3.3 Evaluation during model development ‣ 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Model benchmarks vs. actual system context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS3.SSS2" title="In 3.3 Evaluation during model development ‣ 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Combining IR and generative AI evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS3.SSS3" title="In 3.3 Evaluation during model development ‣ 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>LLMs to evaluate LLM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS4" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Evaluation pre/post system release</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS4.SSS1" title="In 3.4 Evaluation pre/post system release ‣ 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Online evaluation using actual user behavior vs. offline evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS4.SSS2" title="In 3.4 Evaluation pre/post system release ‣ 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Stress testing, red-teaming and qualitative end-user evaluations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS5" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Societal impact of a system beyond its direct implementation and use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S3.SS6" title="In 3 Methods to evaluate risks and impact ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Sharing evaluation methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4" title="In Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Actors, incentives and ways of getting organized</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4.SS1" title="In 4 Actors, incentives and ways of getting organized ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Incentives towards misuse of AI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4.SS2" title="In 4 Actors, incentives and ways of getting organized ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Who can shift incentives, and how</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4.SS2.SSS1" title="In 4.2 Who can shift incentives, and how ‣ 4 Actors, incentives and ways of getting organized ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Organizational Factors</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4.SS2.SSS2" title="In 4.2 Who can shift incentives, and how ‣ 4 Actors, incentives and ways of getting organized ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Data-focused methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S5" title="In Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sociotechnical Implications of Generative Artificial Intelligence for Information Access</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<a class="ltx_ref ltx_href" href="https://orcid.org/my-orcid?orcid=0000-0002-5270-5550" title=""><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="8" id="id1.1.1.g1" src="x1.png" width="8"/> Bhaskar Mitra</a>
<br class="ltx_break"/>Microsoft Research 
<br class="ltx_break"/>Montréal, Canada 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id1">bmitra@microsoft.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henriette Cramer 
<br class="ltx_break"/>PaperMoon AI 
<br class="ltx_break"/>San Francisco, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1">henriette.cramer@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Olya Gurevich 
<br class="ltx_break"/>PaperMoon AI 
<br class="ltx_break"/>San Francisco, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1">olya.gurevich@gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Robust access to trustworthy information is a critical need for society with implications for knowledge production, public health education, and promoting informed citizenry in democratic societies.
Generative AI technologies may enable new ways to access information and improve effectiveness of existing information retrieval systems but we are only starting to understand and grapple with their long-term social implications.
In this chapter, we present an overview of some of the systemic consequences and risks of employing generative AI in the context of information access.
We also provide recommendations for evaluation and mitigation, and discuss challenges for future research.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Robust access to trustworthy information is a critical need for society including implications for knowledge production, public health education, and promoting informed citizenry in democratic societies.
Generative AI technologies such as large language models (LLMs) may enable new ways to access information and improve effectiveness of existing information retrieval (IR) systems.
More efficient basic task execution with the help of LLMs can also enable people to focus on the more challenging aspects of information retrieval related tasks and research.
However, the long-term social implications of deploying these technologies in the context of information access are not yet well-understood.
Existing research has focused on how these models may generate biased and harmful content <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib158" title="">158</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib124" title="">124</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib236" title="">236</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib80" title="">80</a>]</cite> as well as the environmental costs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib241" title="">241</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib167" title="">167</a>]</cite> of developing and deploying these models at scale.
In the context of information access, <cite class="ltx_cite ltx_citemacro_citet">Shah and Bender [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib187" title="">187</a>]</cite> have argued that certain framings of LLMs as “search engines” lack the necessary theoretical underpinnings and may constitute as a category error.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this current work, we present a broader perspective on the sociotechnical implications of generative AI for information access.
Our perspective is informed by existing literature and aims to provide a summary of known challenges viewed through a systemic lens that we hope will serve as a useful resource for future critical research in this area.
We present a summary of these implications next followed by recommendations for evaluation and mitigation later in this chapter.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Implications of generative AI for information access</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We present a reflection on the potential sociotechnical implications of generative AI, with an emphasis on LLMs, for information access.
Generative AI is still an emerging technology and our understanding of its sociotechnical impact today, and how it may evolve over time, is fairly limited.
Our treatment of this topic is therefore necessarily both incomplete and speculative.
We are informed by several recent works <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib234" title="">234</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib235" title="">235</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib199" title="">199</a>]</cite> that attempts to map the landscape of risks and harms from LLMs.
What distinguishes our treatment of this topic relative to this previous literature is the specific focus on information access.
There has also been work on the considerations for specific applications of LLMs in IR, such as for generating direct responses to users’ expressed information needs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib187" title="">187</a>]</cite>, which is relevant to our current discussion.
However, a thorough exploration of every potential application of LLMs in IR systems is beyond the scope of our current work.
Instead, we explore the implications for information access through a broader lens that encompasses considerations for content creation, content retrieval, sociopolitical power dynamics, geopolitical inequities, crowd-work, ecology, and future of IR research.
We reference relevant previous taxonomies and studies throughout this section to both support our claims and to establish meaningful connections in an attempt to present a more complete and consistent view on this topic to the reader.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">We adopt the consequences-mechanisms-risks (CMR) framework proposed by <cite class="ltx_cite ltx_citemacro_citet">Gausen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib76" title="">76</a>]</cite> to structure our presentation.
Gausen et al. introduce the CMR framework to support designers and developers of AI (and in general any computational) systems to identify and understand

<span class="ltx_inline-enumerate" id="S2.I1">
<span class="ltx_inline-item" id="S2.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I1.i1.1">the systemic consequences of developing and deploying the technology under study in the real world,
</span></span>
<span class="ltx_inline-item" id="S2.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I1.i2.1">the mechanisms introduced by said technology responsible for these consequences, and
</span></span>
<span class="ltx_inline-item" id="S2.I1.i3"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span class="ltx_text" id="S2.I1.i3.1">the corresponding risks to relevant stakeholders.
</span></span>
</span>
The framework intentionally explicates the higher-level consequences to motivate viewing the challenges through a more systemic lens.
The mechanisms, in turn, focus on more low-level system behaviors and aspects of the technology development process that contribute to the consequences and risks, and therefore represent sites for more actionable mitigation.
These consequences and mechanisms are mapped to relevant potential risks.
Through literature survey, in this work we identify the consequences, mechanisms, and risks of generative AI in the context of information access, and organize them according to the CMR framework as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.T1" title="Table 1 ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">1</span></a>.
While we acknowledge that this list of consequences-mechanisms-risks is incomplete, we hope that it provides a summary of the sociotechnical concerns already identified in existing literature and provokes new questions for critical future research.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of potential negative consequences for information access from generative AI, the related mechanisms introduced by these AI technologies, and corresponding risks.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:99.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1.1.1">Consequences</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:186.5pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1.1.1">Mechanisms</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:112.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1.1.1">Risks</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" rowspan="6" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.2.1.1.1.1" style="width:99.7pt;"><span class="ltx_text" id="S2.T1.1.2.1.1.1.1.1">Information ecosystem disruption (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1" style="width:186.5pt;">Content pollution (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3" rowspan="12" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.1.2.1.3.1.1" style="width:112.7pt;"><span class="ltx_text" id="S2.T1.1.2.1.3.1.1.1">Risks to society: democracy, health and wellbeing, and global inequity (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS1" title="2.2.1 Risks to society ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.3.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.2.1.1">
<span class="ltx_p" id="S2.T1.1.3.2.1.1.1" style="width:186.5pt;">The “Game of telephone” effect (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.4.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.3.1.1">
<span class="ltx_p" id="S2.T1.1.4.3.1.1.1" style="width:186.5pt;">Search engine manipulation (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.5.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.4.1.1">
<span class="ltx_p" id="S2.T1.1.5.4.1.1.1" style="width:186.5pt;">Degrading retrieval quality (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.6.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.5.1.1">
<span class="ltx_p" id="S2.T1.1.6.5.1.1.1" style="width:186.5pt;">Direct model access (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.7.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.6.1.1">
<span class="ltx_p" id="S2.T1.1.7.6.1.1.1" style="width:186.5pt;">The paradox of reuse (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.8.7.1" rowspan="3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.1.1">
<span class="ltx_p" id="S2.T1.1.8.7.1.1.1" style="width:99.7pt;"><span class="ltx_text" id="S2.T1.1.8.7.1.1.1.1">Concentration of power (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.8.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.7.2.1">
<span class="ltx_p" id="S2.T1.1.8.7.2.1.1" style="width:186.5pt;">Compute and data moat (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.9.8.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.8.1.1">
<span class="ltx_p" id="S2.T1.1.9.8.1.1.1" style="width:186.5pt;">AI persuasion (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.10.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.10.9.1.1">
<span class="ltx_p" id="S2.T1.1.10.9.1.1.1" style="width:186.5pt;">AI alignment (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.11.10.1" rowspan="3" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.11.10.1.1">
<span class="ltx_p" id="S2.T1.1.11.10.1.1.1" style="width:99.7pt;"><span class="ltx_text" id="S2.T1.1.11.10.1.1.1.1">Marginalization (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="2.1.3 Consequence: Marginalization ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.11.10.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.11.10.2.1">
<span class="ltx_p" id="S2.T1.1.11.10.2.1.1" style="width:186.5pt;">Appropriation of data labor (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="2.1.3 Consequence: Marginalization ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.12.11.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.11.1.1">
<span class="ltx_p" id="S2.T1.1.12.11.1.1.1" style="width:186.5pt;">Bias amplification (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="2.1.3 Consequence: Marginalization ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.13.12.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.13.12.1.1">
<span class="ltx_p" id="S2.T1.1.13.12.1.1.1" style="width:186.5pt;">AI exploitation and doxing (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="2.1.3 Consequence: Marginalization ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.14.13.1" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.14.13.1.1">
<span class="ltx_p" id="S2.T1.1.14.13.1.1.1" style="width:99.7pt;"><span class="ltx_text" id="S2.T1.1.14.13.1.1.1.1">Innovation decay (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS4" title="2.1.4 Consequence: Innovation decay ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.4</span></a>)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.14.13.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.14.13.2.1">
<span class="ltx_p" id="S2.T1.1.14.13.2.1.1" style="width:186.5pt;">Industry capture (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS4" title="2.1.4 Consequence: Innovation decay ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.4</span></a>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.14.13.3" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.14.13.3.1">
<span class="ltx_p" id="S2.T1.1.14.13.3.1.1" style="width:112.7pt;"><span class="ltx_text" id="S2.T1.1.14.13.3.1.1.1">Risks to IR research (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS2" title="2.2.2 Risks to IR research ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.15.14.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.15.14.1.1">
<span class="ltx_p" id="S2.T1.1.15.14.1.1.1" style="width:186.5pt;">Pollution of research artefacts (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS4" title="2.1.4 Consequence: Innovation decay ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.4</span></a>)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.16.15.1" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.16.15.1.1">
<span class="ltx_p" id="S2.T1.1.16.15.1.1.1" style="width:99.7pt;"><span class="ltx_text" id="S2.T1.1.16.15.1.1.1.1">Ecological impact (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="2.1.5 Consequence: Ecological impact ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>)</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.16.15.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.16.15.2.1">
<span class="ltx_p" id="S2.T1.1.16.15.2.1.1" style="width:186.5pt;">Resource demand and waste (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="2.1.5 Consequence: Ecological impact ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.16.15.3" rowspan="2" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.16.15.3.1">
<span class="ltx_p" id="S2.T1.1.16.15.3.1.1" style="width:112.7pt;"><span class="ltx_text" id="S2.T1.1.16.15.3.1.1.1">Risks to environment (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS3" title="2.2.3 Risks to environment ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.17.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.17.16.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.17.16.1.1">
<span class="ltx_p" id="S2.T1.1.17.16.1.1.1" style="width:186.5pt;">Persuasive advertising (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="2.1.5 Consequence: Ecological impact ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>)</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Consequences and mechanisms</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the context of information access, we identify five potential categories of negative consequences of generative AI, and corresponding mechanisms, that we discuss next.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Consequence: Information ecosystem disruption</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">To reflect on the implications of generative AI on information access, we must consider the information ecosystem as a whole, and not constrain our discussion only to the application of these emerging technologies directly in IR systems.
This ecosystem includes different actors and stakeholders such as information seekers, content producers, IR systems developers, advertisers, and other sociopolitical actors.
While the information ecosystem is constantly evolving, generative AI holds the potential to significantly disrupt how each of these actors operate on their own and how they relate to other actors and stakeholders.
This potential for disruption spans across how content is produced, consumed, monetized, and used towards specific ends.
By no means do we want to imply that these plausible changes are inherently bad but the scale of potential disruptions across the ecosystem should motivate careful and thoughtful considerations before these technologies are deployed at scale.
We discuss next some the underlying mechanisms introduced by generative AI that may contribute to these disruptions.
We encourage the reader to view these mechanisms not just in isolation but to also consider how they may interact with each other and how that may impact the ecosystem over time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">Mechanism: Content pollution

Generative AI enables low-cost generation of derivative low-quality content at unprecedented scale.
As a consequence, synthetic AI-generated content is rapidly and very widely appearing on the web <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib104" title="">104</a>]</cite>.
On Amazon,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amazon.com/" title="">https://www.amazon.com/</a></span></span></span> AI-generated content includes scammy derivatives of existing publications <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib159" title="">159</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib121" title="">121</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib135" title="">135</a>]</cite> and fake travel guides <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib125" title="">125</a>]</cite>.
On YouTube,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/" title="">https://www.youtube.com/</a></span></span></span> AI-generated video creators have targeted children <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib122" title="">122</a>]</cite>.
We are also witnessing a proliferation of news websites almost entirely generated by AI <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib182" title="">182</a>]</cite>, which are being surfaced in search results <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib51" title="">51</a>]</cite> and funded by online ads <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib33" title="">33</a>]</cite>.
Even reputable publishers have reportedly published AI-generated articles under fake AI-generated author profiles <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib62" title="">62</a>]</cite>.
Beyond news, other synthetic content such as AI-generated images is starting to pollute search results <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib63" title="">63</a>]</cite>.
According to another recent study <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib213" title="">213</a>]</cite>, a “shocking” amount of content on the web today is machine-translated text. The promise of machine translation is that it could make more content accessible to wider audiences. However, it also amplifies the influence of (sometimes questionable-quality) language technology choices. For example, <cite class="ltx_cite ltx_citemacro_citet">Thompson et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib213" title="">213</a>]</cite> found that more low quality content—rather than high quality content—was machine translated into lower resource languages, likely with the goal of generating ad revenue.
Concerns have also been raised about LLMs potentially serving as “Misinformation Superspreaders” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib162" title="">162</a>]</cite> as they make it trivially easy to inundate the web with “firehoses of falsehoods”.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Firehose_of_falsehood" title="">https://en.wikipedia.org/wiki/Firehose_of_falsehood</a></span></span></span>
<cite class="ltx_cite ltx_citemacro_citet">Hoel [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib103" title="">103</a>]</cite> points out that AI pollution of our information ecosystems is a “tragedy of the commons” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">Pollution of our information ecosystem at such scale has critical implications for people and society.
When authoring a document requires significant time and effort then the quality, style, and comprehensiveness are factors that readers may consider in deciding whether and how much to trust its content.
However, when the cost of writing an extensive article approaches zero, it becomes significantly harder for the reader to make that decision. They may not be able to distinguish between an article created based on extensive research, fact checking and thoughtful writing practices versus one generated instantly based on a short user prompt.
Furthermore, the increasing adoption of these same AI authoring tools by reputable publishers and content producers may homogenize the language and style of content on the web, making it even more difficult for readers to distinguish them from low-quality AI-generated content whose sole intent is to attract ad revenue or to mislead.
Such web pollution is also a concern for future AI models that require large web-scale datasets to train on.
Including AI-generated content in the training data for new AI models may have significant negative impact on model performance, what has been referred to as “Model collapse” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib192" title="">192</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib142" title="">142</a>]</cite>, “Model Autophagy Disorder” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib14" title="">14</a>]</cite>, and “Habsburg AI”.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://twitter.com/jathansadowski/status/1625245803211272194" title="">https://twitter.com/jathansadowski/status/1625245803211272194</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">Mechanism: The “Game of telephone” effect

LLMs have recently been employed in conversational search interfaces.
In systems such as Bing Copilot, the LLM has access to relevant web search results from which it can draw information to produce appropriate responses for the information needs expressed by a user.
In this scenario, the LLM performs a complex summarization task extracting relevant information from the retrieved documents to answer the search query.
In doing so, the LLM now inserts itself between the user and the retrieved web results.
This shifts the responsibility of inspecting the information in the documents and assessing their relevance, trustworthiness, and surrounding context from the user to the LLM.
Further, factual errors and inconsistencies may arise between what the LLM produces and what is in the retrieved documents.
Seeing the model through an anthropomorphic lens, these errors are sometimes referred to as “hallucinations”.
A more technical view may see this as a noisy translation akin to the children’s game of telephone.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Game_of_telephone" title="">https://en.wikipedia.org/wiki/Game_of_telephone</a></span></span></span>
Such errors, often subtle and hard to spot, may contribute to misinformation and reduce robustness of the information access system.
While the LLM-generated responses may cite relevant documents, it is unlikely that users diligently click the provided links and verify the information in the response is indeed supported by said sources.
Even if the LLM reproduces exact pieces of text from the source documents without error, taking these out of the context of the document may lead to unexpected negative consequences.
Such examples have previously been reported <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib216" title="">216</a>]</cite> in context of extracted answers that search engines display on the search result pages (SERPs) as response to the user query.
These issues may become more prevalent if conversational search interfaces become a popular way to access online information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1">In a more radical proposal, <cite class="ltx_cite ltx_citemacro_citet">Metzler et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib146" title="">146</a>]</cite> have suggested that LLMs could directly replace retrieval systems and respond directly to the user based on information in their training data.
LLMs are trained to produce statistically plausible text sequences and any semblance to an information retrieval system is likely an important mis-categorization of these models that we should be wary of <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib187" title="">187</a>]</cite>.
The game of telephone effect is likely to be more intense when LLMs are expected to produce information from their training data and not just the in-context information in its input.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1">The interjection of the LLM between the user and the search results may have other long term effects.
These interfaces may disincentivize users from the practice of verifying information sources and make them less skilled over time at discerning online misinformation.
If users get accustomed to information being presented neatly summarized and disconnected from original sources, the critical cognitive skills necessary to distinguish between trustworthy and untrustworthy information may atrophy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p7">
<p class="ltx_p" id="S2.SS1.SSS1.p7.1">Mechanism: Search engine manipulation

New applications of LLMs to the IR stack have exposed new attack vectors.
Prompt injection attacks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib138" title="">138</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib137" title="">137</a>]</cite> that try to blur the line between instructions and data have garnered specific interest.
In these types of attacks, website owners may inject what looks like instructions to the LLM.
When such documents are retrieved and included in the input of the LLM as augmentation, the LLM may mistake the injected prompt in the document content and be vulnerable to manipulation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p8">
<p class="ltx_p" id="S2.SS1.SSS1.p8.1">Recently, LLMs have also found application in relevance labeling for search <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib212" title="">212</a>]</cite>.
It is not well understood yet whether this may make the search engine vulnerable to improper ranking manipulation by website owners and search engine optimization experts.
For example, one may employ the same, or similar, LLMs to reproduce the labeling scheme externally and then adapt their website content and design to achieve undue high predicted relevance against queries to rank higher on SERPs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p9">
<p class="ltx_p" id="S2.SS1.SSS1.p9.1">Other attack vectors may include using LLMs to create effective content farms at low cost to manipulate the ranking of web results, or even use LLMs to artificially simulate users interacting with the search system to fake clicks and other user behavior signals, such as reformulations, that search engines depend on.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p10">
<p class="ltx_p" id="S2.SS1.SSS1.p10.1">Mechanism: Degrading retrieval quality

LLM usage can negatively impact search result quality in a number of (indirect) ways.
LLMs can contribute to new attack vectors, but more worryingly, in some cases the negative effect may be a result of the LLM behaving exactly as it is supposed to.
For example, one potential consequence of using conversational search interfaces,
is that the quality of feedback from user behavior signals on SERPs may significantly degrade. Historically, users of commercial web search engines have given search systems noisy implicit feedback through clicks and other actions on SERPs. These actions are one of the key secret sauce of any modern search systems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p11">
<p class="ltx_p" id="S2.SS1.SSS1.p11.1">However, conversational interfaces may discourage direct user clicks on web results and at best provide much weaker satisfaction signal that may be gleaned from the users’ next utterance in the conversation.
This over time may negatively impact the underlying retrieval quality.
This makes it important to invest in methods that can infer user satisfaction with high certainty from the natural language conversations.
However, methods for such signal interpretation are not yet at the level necessary to mitigate these impacts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p12">
<p class="ltx_p" id="S2.SS1.SSS1.p12.1">In conversational search interfaces and other applications, such as Microsoft Copilot for M365 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib145" title="">145</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib232" title="">232</a>]</cite>, the LLM may conduct the search on the user’s behalf.
In this process, the LLM generates search queries.
If these queries differ from those that are likely to be submitted by users then the underlying search system needs to optimize itself for both real user queries and LLM-generated queries.
This may have consequences that are not yet well understood.
Optimizing the search system directly to improve the LLMs natural language responses may also have unforeseen outcomes, especially in light of the fact that what makes for a good result set for retrieval-augmentation is not yet fully understood <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib56" title="">56</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p13">
<p class="ltx_p" id="S2.SS1.SSS1.p13.1">Mechanism: Direct model access

Another important consideration is the implications of open foundation models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib115" title="">115</a>]</cite>.
While centralized systems have their own negative implications, as discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>, open access generative AI models without any access moderation also pose certain challenges.
For example, there are many classes of harmful intents that systems should refuse to respond to.
This may include search queries seeking information on methods to self-harm or cause harms to others, or requests to generate harmful (and sometimes illegal) content such as child sex abuse material (CSAM) or non-consensual intimate information (NCII).
Publicly accessible LLMs trained on large web corpora may produce such irresponsible content in the absence of moderation.
Even if a model is trained to not respond to certain classes of queries, it is likely that there will be leakage, and the safety alignment may also be compromised if the model is further finetuned <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib176" title="">176</a>]</cite>.
Such leakage may also happen in the context of traditional search systems.
However, in the latter case, all queries are typically logged, allowing for post-hoc analysis and identification of critical gaps in the moderation system.
Unfortunately, no such mitigation is possible once these generative AI models are released into the wild.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p14">
<p class="ltx_p" id="S2.SS1.SSS1.p14.1">Mechanism: The paradox of reuse

Content producers and information access technologies are critically inter-dependant <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib144" title="">144</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib226" title="">226</a>]</cite>.
Websites such as Wikipedia,<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wikipedia.org/" title="">https://www.wikipedia.org/</a></span></span></span> StackExchange,<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackexchange.com/" title="">https://stackexchange.com/</a></span></span></span> and Reddit<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reddit.com/" title="">https://www.reddit.com/</a></span></span></span> produce critical content that is surfaced by information access platforms (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS1.p14.1.1">e.g.</em>, web search engines) and contribute to making these platforms significantly more useful to their users.
In return, these platforms have historically sent traffic back to the websites that contributes to their increased readership, subscriptions, and monetization.
However, when search platforms stop directing traffic back to websites—<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS1.p14.1.2">e.g.</em>, by instead surfacing relevant content directly on the search result pages (SERPs)—the relationship becomes less symbiotic towards the content producers, a phenomenon <cite class="ltx_cite ltx_citemacro_citet">Taraborelli [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib207" title="">207</a>]</cite> termed the “paradox of reuse”.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p15">
<p class="ltx_p" id="S2.SS1.SSS1.p15.1">The application of LLMs as conversational information access interfaces is likely to significantly intensify this problem.
For example, LLMs such as ChatGPT<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a></span></span></span> and Google Gemini<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gemini.google.com/app" title="">https://gemini.google.com/app</a></span></span></span> may gobble up large quantities of content from websites as part of their training data and later regurgitate the same information without any attribution back to the sources.
Even when models summarize information from multiple online sources with attribution—<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS1.p15.1.1">e.g.</em>, Bing Copliot<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bing.com/chat" title="">https://www.bing.com/chat</a></span></span></span>, they typically de-emphasize the references and reduce the likelihood of the searcher clicking through to the source websites as compared to the classic ten-blue-links interface.
There is evidence <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib57" title="">57</a>]</cite> to suggest that this phenomenon is already happening at scale and is jeopardizing the “grand bargain at the heart of the web” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib99" title="">99</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Consequence: Concentration of power</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p1">
<blockquote class="ltx_quote" id="S2.SS1.SSS2.p1.1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1.1">“We may have democracy, or we may have wealth concentrated in the hands of a few, but we can’t have both.”</p>
<p class="ltx_p ltx_align_right" id="S2.SS1.SSS2.p1.1.2">– Louis Brandeis</p>
<p class="ltx_p ltx_align_right" id="S2.SS1.SSS2.p1.1.3"><em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p1.1.3.1">As quoted by <cite class="ltx_cite ltx_citemacro_citet">Lonergan [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib139" title="">139</a>]</cite></em></p>
</blockquote>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">Technology shapes and is shaped by the sociopolitical power structures within which it exists.
The 2024 edition of the World Economic Forum’s Global Risks Report <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib10" title="">10</a>]</cite> lists “technological power concentration” as one of the top global risks for the coming decade and as the biggest upward mover in their annual ranking of global risks compared to the previous year. Deliberation on the social consequences of any technology must therefore include critical consideration of how the technology, and general narratives about said technology, shifts power and re-architects and codifies structures of hierarchy and control.
In this context, the politics and values of those in power to oversee what and how technology is built or regulated, especially when they reinforce hierarchy and authoritarianism (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p2.1.1">e.g.</em> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib126" title="">126</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib64" title="">64</a>]</cite>), becomes important to consider.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1">A report <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib112" title="">112</a>]</cite> from the research institute AI Now<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://ainowinstitute.org/</span></span></span> similarly asserts “the concentration of economic and political power in the hands of the tech industry—Big Tech in particular” as the core challenge posed by AI.
They further note that not just the technologies but the narratives (both the hype and the fear-mongering) around them questionably bolster claims of “foundational” advancements and their unassailable equivalence with scientific progress.
These concerns are complemented by the discourses within the AI community, such as the observations by <cite class="ltx_cite ltx_citemacro_citet">Birhane et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib27" title="">27</a>]</cite> that the prominent values expressed and operationalized in top cited AI papers generally have implications in support of centralization of power.
Even if platform owners act accountably to civil society, the concentration of power and control in their hands makes them vulnerable to other actors, such as autocratic governments, and allows that power to be potentially abused for oppressive and harmful intents.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p4">
<p class="ltx_p" id="S2.SS1.SSS2.p4.1">The popularization of generative AI can concentrate that power within large companies, since they emerge as some of the only institutions with the resources to develop and deploy these technologies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib117" title="">117</a>]</cite>.
The application of these technologies for information access may contribute to further concentration and growing inequities of wealth and power; we discuss three mechanisms in the context of generative AI that may contribute to concentration of power and control.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p5">
<p class="ltx_p" id="S2.SS1.SSS2.p5.1">Mechanism: Compute and data moat

The development of generative AI is heavily reliant on the availability of large swaths of training data and large-scale computing power for training and deployment.
Only a handful of institutions, largely in the private sector, own and control these necessary resources while simultaneously evangelizing AI as crucial geopolitical leverage and critical social infrastructure <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib112" title="">112</a>]</cite>.
Increased access to these models has sometimes been touted as potential paths to mitigation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib196" title="">196</a>]</cite>, where access may range from being heavily restricted over API to “open weight” models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib134" title="">134</a>]</cite>.
The ability to download models with their learned parameters allows others to further adapt for their own applications and opens the door to more meaningful analysis and audit of these models.
However, such “open access” also leads to severe limitations that we should recognize.
The availability of the trained models does little to challenge the predominant visions put forth by large technology companies of what AI fundamentally should look like.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p6">
<p class="ltx_p" id="S2.SS1.SSS2.p6.1">One potential direction would be to dismantle the data and compute moat by turning them over from private ownership into public infrastructure for independent researchers and developers and those affiliated with smaller institutions.
This also illustrates the importance of existing institutions such as archives, libraries and universities that have reliable, historical data.
The availability of public computer infrastructure would allow a broader set of developers to participate in the reimagination and development of diverse approaches to AI and not merely being forced to be satisfied with critiquing and finetuning artefacts produced by other institutions. However, there is no guarantee that without careful planning and incentives, a proliferation of smaller projects will lead to transformative new or more sustainable results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p7">
<p class="ltx_p" id="S2.SS1.SSS2.p7.1">Democratizing the control over computational resources provides a mechanism of checks and balances on the future directions of AI systems, and may allow for challenges to popular narratives and expectations about generative AI such as exponential growth in model size over time. Infrastructure is however also bound to the particular governing system, and local underlying goals and processes. Larger investments in existing research institutes, or new alternative companies or non-profits might in certain cases lead to faster results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p8">
<p class="ltx_p" id="S2.SS1.SSS2.p8.1">Similarly, the research community would benefit from easier access to industry models and APIs for critical studies and auditing. However, access to models or APIs alone is significantly limiting unless that access is also extended to the user-facing systems in which these technologies are deployed. The corresponding instrumentation data would provide context on how these systems are used by people and potential consequences. This can lead to practical privacy and security questions for platform teams. Practical support for decision making and for example the creation of standards to de-risk those concerns can help alleviate some of those concerns.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p9">
<p class="ltx_p" id="S2.SS1.SSS2.p9.1">Mechanism: AI persuasion

There is an emerging recognition of the dangers of <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p9.1.1">AI persuasion</em> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib67" title="">67</a>]</cite>, which <cite class="ltx_cite ltx_citemacro_citet">Burtell and Woodside [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib36" title="">36</a>]</cite> define as “a process by which AI systems alter the beliefs of their users”.
AI systems may persuade users by appealing to their reason and argument, or by using their cognitive biases and heuristics <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib67" title="">67</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet">El-Sayed et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib67" title="">67</a>]</cite> identify six mechanisms of generative AI persuasion—namely

<span class="ltx_inline-enumerate" id="S2.I2">
<span class="ltx_inline-item" id="S2.I2.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I2.i1.1">trust and rapport,
</span></span>
<span class="ltx_inline-item" id="S2.I2.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I2.i2.1">anthropomorphism,
</span></span>
<span class="ltx_inline-item" id="S2.I2.i3"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span class="ltx_text" id="S2.I2.i3.1">personalization,
</span></span>
<span class="ltx_inline-item" id="S2.I2.i4"><span class="ltx_tag ltx_tag_inline-item">(iv)</span> <span class="ltx_text" id="S2.I2.i4.1">deception and lack of transparency,
</span></span>
<span class="ltx_inline-item" id="S2.I2.i5"><span class="ltx_tag ltx_tag_inline-item">(v)</span> <span class="ltx_text" id="S2.I2.i5.1">manipulative strategies, and
</span></span>
<span class="ltx_inline-item" id="S2.I2.i6"><span class="ltx_tag ltx_tag_inline-item">(vi)</span> <span class="ltx_text" id="S2.I2.i6.1">alteration of choice environment
</span></span>
</span>—and corresponding model features that contribute to these mechanisms.
In the context of information access and advertising, these capabilities of generative AI can be powerful tools to hyper-target users and steer their behaviors.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p10">
<p class="ltx_p" id="S2.SS1.SSS2.p10.1">Modern online information access and communication platforms monetized with targeted advertising have been said to usher in an age of surveillance capitalism <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib247" title="">247</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib248" title="">248</a>]</cite>.
Information access systems increasingly collect detailed user behavior data that allow them to build accurate user profiles for audience targeting.
There is strong evidence that people are more likely to consume information that opposes their own personal views and beliefs when the it employs language similar to their own political leanings <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib243" title="">243</a>]</cite>.
So, combining users’ private preferences and behavioral data with the capabilities of generative AI to produce persuasive language could create worrying tools for mass behavioral manipulation.
The impact of such pervasive <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS2.p10.1.1">algorithmic nudging</em> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib151" title="">151</a>]</cite> may be further pronounced over longer time periods from continuous interactions between the user and the system.
Putting these capabilities in the hands of online platform owners, which typically tend to be large multinational for-profit institutions with largely hierarchical non-democratic internal governance structures, poses serious risks to functioning of democratic societies. At the same time, platforms must make decisions about what is acceptable on their platforms to avoid negative user experiences, spam, unwelcoming behavior, and other negative occurrences beyond those outlined in legal compliance alone. Platforms moderate content posted or accessible through the platform <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib83" title="">83</a>]</cite> and in doing so they unavoidably impose implementations of values on their users, or the values incentivized by, say, advertising needs or other business model related motivations.
For ads, this may mean an incentive to use generative AI to produce hyper-targeted highly-personalized persuasive advertisements which convince users to make certain buying decisions.
For content, when platforms optimize for increased user engagement, they may knowingly or unknowingly incentivize generative AI models to be producing highly charged content, such as “rage-bait” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib107" title="">107</a>]</cite>, because it tends to be more persuasive and engaging.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p11">
<p class="ltx_p" id="S2.SS1.SSS2.p11.1">Mechanism: AI alignment

To prevent generative AI models from producing harmful and offensive content, recent research has focused on how to align model outputs with “human values” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib204" title="">204</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib180" title="">180</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib73" title="">73</a>]</cite>.
Approaches such as reinforcement learning from
human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib246" title="">246</a>]</cite> have been effective in limiting certain types of problematic content from being produced.
However, this approach presupposes some notions of desirable values and puts the burden of determining and enforcing them on the shoulders of platform / model developers.
Any notions of universal values that might determine what type of content these models should generate—or, not generate <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib215" title="">215</a>]</cite>—is highly contested <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib111" title="">111</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib183" title="">183</a>]</cite>.
Placing these decisions in the exclusive domain of the platform developers, especially in the absence of democratic and civil society oversight, further concentrates power and responsibility. This is not an argument against content moderation itself but against the centralization of control over it without civil oversight or broader societal participation. As a pragmatic example, platforms may not necessarily have the necessary knowledge in-house, making it imperative for them to make successful connections to outside expertise.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Consequence: Marginalization</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Generative AI, both in its process of development and in its deployment in the context of information access, can marginalize groups and individuals by diminishing their value, power, and well-being.
Next, we discuss some the mechanisms that may contribute to this.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">Mechanism: Appropriation of data labor

<cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib129" title="">129</a>]</cite> define <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p2.1.1">data labor</em> as “activities that produce digital records useful for capital generation”.
The term encompasses both witting labor activities—as in the case of crowdwork <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib15" title="">15</a>]</cite>, peer production <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib208" title="">208</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib209" title="">209</a>]</cite>, and content moderation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib83" title="">83</a>]</cite>—and unwitting activities such as user behavior data and other data generated when users interact with and participate on the platforms.
Data labor also encompasses the creation of artefacts by writers <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib46" title="">46</a>]</cite>, artists <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib220" title="">220</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib221" title="">221</a>]</cite>, and programmers <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib219" title="">219</a>]</cite> etc. outside of the AI development process that are nonetheless extracted from the web and fed in as training data to generative AI models.
Appropriation of data labor in this context includes both

<span class="ltx_inline-enumerate" id="S2.I3">
<span class="ltx_inline-item" id="S2.I3.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I3.i1.1">the uncompensated appropriation of works by writers, authors, programmers, and peer production communities like Wikipedia <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib221" title="">221</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib219" title="">219</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib141" title="">141</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib225" title="">225</a>]</cite>, and
</span></span>
<span class="ltx_inline-item" id="S2.I3.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I3.i2.1">under-compensated crowdwork for data labeling that has been instrumental in the development of these technologies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib170" title="">170</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib239" title="">239</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib205" title="">205</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib242" title="">242</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib97" title="">97</a>]</cite>.
</span></span>
</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.1">It is particularly harmful when technology developed on appropriated labor is then employed to displace and automate the jobs of those whose labor was appropriated <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib224" title="">224</a>]</cite>.Introduction of such automation may involve vicious cycles of perceived skill-transfer from people to AI models whereby professional jobs are replaced by corresponding lesser-paid gigified equivalent as auditing and editing of model outputs only <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib87" title="">87</a>]</cite>. Proprietary AI model capabilities may then continue to improve by learning from workers’ inputs, while workers progressively lose their economic value and power, or are even relegated into the role of <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p3.1.1">moral crumple zones</em> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib68" title="">68</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p4">
<p class="ltx_p" id="S2.SS1.SSS3.p4.1">This is a critical challenge in the context of information access because

<span class="ltx_inline-enumerate" id="S2.I4">
<span class="ltx_inline-item" id="S2.I4.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I4.i1.1">the devaluation of writers and artists have direct implications for the quality of content on the web, and
</span></span>
<span class="ltx_inline-item" id="S2.I4.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I4.i2.1">these automated content generation tools are starting to get incorporated directly in information access platforms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib171" title="">171</a>]</cite>.
</span></span>
</span>
Similar concerns of commodification and appropriation have also been raised in other information and knowledge access contexts such as in the enterprise <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib76" title="">76</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p5">
<p class="ltx_p" id="S2.SS1.SSS3.p5.1"><em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p5.1.1">AI for me, data labor for thee.</em>
Another pernicious aspect of AI data labor dynamics discussed in the literature is how they can mirror and reify racial capitalism and coloniality, employ global labor exploitation and extractive practices, and reinforce the global north and south divide <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib161" title="">161</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib120" title="">120</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib154" title="">154</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib203" title="">203</a>]</cite>. While worldwide jobs might be created in certain cases, the workers are typically low-paid and deprived of any share of the profit made from technologies built with their labor.These dynamics encompass accruing the benefits of generative AI to privileged populations, while data labor is relegated to already marginalized populations, for example in the global south.
Communities that significantly contribute to AI data labor may even find their own linguistic styles being labeled AI-ese <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib101" title="">101</a>]</cite> and being forced to repeatedly prove their own humanity <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib143" title="">143</a>]</cite>.
Attempts to bridge the global north-south data gap also in turn may further intensify data extractive practices in the global south <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib44" title="">44</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p6">
<p class="ltx_p" id="S2.SS1.SSS3.p6.1">Mechanism: Bias amplification

LLMs and other generative models reproduce and amplify harmful biases and stereotypes from their training datasets <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib11" title="">11</a>]</cite> which can lead to allocative and representational harms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib54" title="">54</a>]</cite>.
Harms may also materialize from <em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p6.1.1">demographic blindness</em> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib76" title="">76</a>]</cite> when the model (or the system it is embedded in) treats different individuals and groups as alike when, in fact, it is unwarranted.
Examples may include the handling of certain languages as one homogeneous entity without regards for sociolects or dialects <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib28" title="">28</a>]</cite> or holding different perspectives as equally valid without considerations for historical context or structural dynamics of power.
These biases are concerning in the context of information access systems that are responsible for supporting informed citizenry and functioning democracies, health literacy, and knowledge production among other societal needs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p7">
<p class="ltx_p" id="S2.SS1.SSS3.p7.1">Mechanism: AI exploitation and doxing

<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p7.1.1">“AI doxing”</em> can describe the act of leaking people’s private information by an AI system.
<cite class="ltx_cite ltx_citemacro_citet">Weidinger et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib234" title="">234</a>]</cite> note that this may be caused by models leaking private information (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p7.1.2">e.g.</em>, address and telephone number) present in their training data <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib39" title="">39</a>]</cite> or when these models are employed to predict people’s sensitive attributes (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p7.1.3">e.g.</em>, political and sexual identities) based on what is known about them publicly <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib123" title="">123</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib163" title="">163</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib177" title="">177</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib244" title="">244</a>]</cite>.
Private information in the training data is a challenge even if the datasets have been sourced from the public web because models may continue to regurgitate that information after it has been removed from the web, or bypass safety measures that would prevent such information from surfacing through web search—<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS3.p7.1.4">e.g.</em>, the information may be protected by robots.txt that blocks popular search crawlers but misses crawler bots that specifically collect data for AI model training.
In many contexts, applications of these models to predict people’s private information may be based on shaky scientific grounds <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib218" title="">218</a>]</cite>, to put it mildly.
However, such applications may still contribute to serious harms and discrimination regardless of their accuracy as long as some people are convinced of their predictive power and employ them to marginalize others.
AI doxing may also take other forms such as reverse-image-search <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib20" title="">20</a>]</cite>, a functionality supported by some search engines, that may be abused for stalking and harassment.
In turn, exploitative materials produced with GenAI (such as deep fake revenge porn, or CSAM) might be amplified.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Consequence: Innovation decay</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">Generative AI may find innovative new applications in information access.
However, the excitement around these technologies and the significant investments from industry, government, and academia on corresponding research and development have broader implications for IR research.
Next, we discuss some of the mechanisms associated with the research and development of generative AI that may potentially throttle innovation in information access technologies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p2">
<p class="ltx_p" id="S2.SS1.SSS4.p2.1">Mechanism: Industry capture

The compute and data moat that concentrates power in the hands of big tech, as discussed earlier in §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>, also creates significant barriers to entry for academic research.
These barriers limits academic AI research to a handful of institutions that have the necessary means and connections to industry who provide access to compute and data resources to incentivize research in areas of their economic interests.
Academics who want to contribute to research on large scale AI systems or critique their sociotechnical impacts are pressured to play well with institutions holding monopolistic control over compute, data, and systems <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib155" title="">155</a>]</cite>.
Access to “open access” models—without the compute and data necessary to build them from scratch—allows academic researchers to invest in finding more effective applications of these technologies that serve industry interests, but not to reimagine / rearchitect them to in radically different ways.
Students and other academics who may someday want to work in industry are shepherded into integrating themselves into this homogenized research agenda.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p3">
<p class="ltx_p" id="S2.SS1.SSS4.p3.1">Such “industry capture” <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib238" title="">238</a>]</cite> allows for inordinate influence of the sociotechnical imaginaries<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><cite class="ltx_cite ltx_citemacro_citet">Jasanoff and Kim [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib109" title="">109</a>]</cite> define <em class="ltx_emph ltx_font_italic" id="footnote13.1">sociotechnical imaginaries</em> as “collectively held, institutionally stabilized, and publicly performed visions of desirable futures, animated by shared understandings of forms of social life and social order attainable through, and supportive of, advances in science and technology”.</span></span></span> of profit-driven
corporations over for example academic researchers <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite>. This can thwart research that may not be immediately monetizable or challenges the status quo of power concentration, and complements the “regulatory capture” by bigger tech companies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib136" title="">136</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib19" title="">19</a>]</cite>.
As <cite class="ltx_cite ltx_citemacro_citet">Mitra [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite> asks: “<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS4.p3.1.1">Whose sociotechnical imaginaries are granted normative status and what myriad of radically alternative futures are we overlooking?</em>”
Narratives of the inevitability of these technologies that are hyped up to be both transformative forces for society and simultaneously posing existential risks for humanity (often purported by the same actors) only bolster their imagined importance to accumulate increasing global investments, including from governments.
Researchers who care about sociotechnical impact and ecological sustainability are busy with enumerating the harms of rapidly emerging new AI technologies and chasing potential mitigations instead of having the full means to imagine and develop systems for social good.
While industry practitioners can contribute to both identifying new research challenges grounded in real-world systems and practical methods to mitigate some of the risks of emerging technologies, it is imperative that we create avenues for increasing independent research, while preserving the benefits of various modes of industry-academia collaborations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p4">
<p class="ltx_p" id="S2.SS1.SSS4.p4.1">Even as the grounded risks from these technologies (such as those discussed here) gather consensus from academic communities and civil society, it can be difficult to create space for alternative ways of development that are perceived as “slowing down”.
Critical research on sociotechnical harms of AI is also under risk when attempts are made to shift attention from concerns about real harms to marginalized people today to unsubstantiated imagined future concerns <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib77" title="">77</a>]</cite>.
Calls for regulations to address these imagined future harms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib79" title="">79</a>]</cite> further detract from real progress and contribute to reinforcement of monopolistic powers of those who have already added these technologies to their arsenals.
This has led some sociotechnical researchers in AI to explicitly draw attention to how these systems shift power (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS4.p4.1.1">e.g.</em>, <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib147" title="">147</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib76" title="">76</a>]</cite>), and to prioritize research guided by alternative visions for sociotechnical futures grounded in universal emancipation and social justice <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite>.
It is thus important that access to investments to enable development is also available to those trying to not only mitigate existing systems’ harms, but also develop new avenues, including work on social good and new business models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p5">
<p class="ltx_p" id="S2.SS1.SSS4.p5.1">As generative AI starts to accumulate the lion’s share of research investments, it may starve out other areas of information access research.
Generative AI has had exciting but limited deployments in information access systems today.
There are significant open challenges to making these models broadly useful, including but not limited to concerns of potential sociotechnical harms.
There is a risk that if these challenges are not mitigated in spite of the extensive resources already invested on them at present, there may be calls for even larger investments in future prompted by the sunk cost fallacy.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Sunk_cost#Fallacy_effect" title="">https://en.wikipedia.org/wiki/Sunk_cost#Fallacy_effect</a></span></span></span>
It would be astute for the IR community to consciously continue to invest in research on systems and applications that societies need beyond what existing AI technologies make plausible <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib187" title="">187</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p6">
<p class="ltx_p" id="S2.SS1.SSS4.p6.1">Mechanism: Pollution of research artefacts

Risks to academic research from generative AI may also emerge through the applications of generative AI models in IR scholarship—<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS4.p6.1.1">e.g.</em>, for authoring scientific papers and peer reviewing.
There is evidence that researchers in computational sciences are already leveraging these tools <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib133" title="">133</a>]</cite>, sometimes with hilariously terrible outcomes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib168" title="">168</a>]</cite>.
While the use of language models for light editing may (eventually) fall within the norms of socially acceptable behavior in research, their application in scholarship does raise concerns of plagiarism and scientific inaccuracies.
This is an area that currently has more questions than answers and the IR community would benefit from proactively considering potential implications of this trend on future IR research.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Consequence: Ecological impact</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS5.p1">
<p class="ltx_p" id="S2.SS1.SSS5.p1.1">Another important consequence of generative AI is its impact on the environment.
In this context it is important for us to consider the direct environmental cost of developing and deploying generative AI systems at scale as well as the potential impact of these technologies on the climate change discourse online.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS5.p2">
<p class="ltx_p" id="S2.SS1.SSS5.p2.3">Mechanism: Resource demand and waste

The ecological cost of deep learning models has been a subject of much concern and debate in the AI community <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib201" title="">201</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib241" title="">241</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib167" title="">167</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib24" title="">24</a>]</cite>.
Similar concerns have also been raised within the IR community with respect to the application of these models for information access <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib184" title="">184</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib249" title="">249</a>]</cite>.
By some estimates, the computing power being utilized for deep learning research has been doubling every 3.4 months since 2012 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib38" title="">38</a>]</cite>.
In the US, data centers consumed more than <math alttext="4\%" class="ltx_Math" display="inline" id="S2.SS1.SSS5.p2.1.m1.1"><semantics id="S2.SS1.SSS5.p2.1.m1.1a"><mrow id="S2.SS1.SSS5.p2.1.m1.1.1" xref="S2.SS1.SSS5.p2.1.m1.1.1.cmml"><mn id="S2.SS1.SSS5.p2.1.m1.1.1.2" xref="S2.SS1.SSS5.p2.1.m1.1.1.2.cmml">4</mn><mo id="S2.SS1.SSS5.p2.1.m1.1.1.1" xref="S2.SS1.SSS5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS5.p2.1.m1.1b"><apply id="S2.SS1.SSS5.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS5.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS5.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS5.p2.1.m1.1.1.1">percent</csymbol><cn id="S2.SS1.SSS5.p2.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.SSS5.p2.1.m1.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS5.p2.1.m1.1c">4\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS5.p2.1.m1.1d">4 %</annotation></semantics></math> of the total national electricity in 2022, and that number is projected to grow to <math alttext="6\%" class="ltx_Math" display="inline" id="S2.SS1.SSS5.p2.2.m2.1"><semantics id="S2.SS1.SSS5.p2.2.m2.1a"><mrow id="S2.SS1.SSS5.p2.2.m2.1.1" xref="S2.SS1.SSS5.p2.2.m2.1.1.cmml"><mn id="S2.SS1.SSS5.p2.2.m2.1.1.2" xref="S2.SS1.SSS5.p2.2.m2.1.1.2.cmml">6</mn><mo id="S2.SS1.SSS5.p2.2.m2.1.1.1" xref="S2.SS1.SSS5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS5.p2.2.m2.1b"><apply id="S2.SS1.SSS5.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS5.p2.2.m2.1.1"><csymbol cd="latexml" id="S2.SS1.SSS5.p2.2.m2.1.1.1.cmml" xref="S2.SS1.SSS5.p2.2.m2.1.1.1">percent</csymbol><cn id="S2.SS1.SSS5.p2.2.m2.1.1.2.cmml" type="integer" xref="S2.SS1.SSS5.p2.2.m2.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS5.p2.2.m2.1c">6\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS5.p2.2.m2.1d">6 %</annotation></semantics></math> by 2026 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib93" title="">93</a>]</cite>.
Another study <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib21" title="">21</a>]</cite> estimates that by 2040 Information and Communications Technology industry on the whole will account for <math alttext="14\%" class="ltx_Math" display="inline" id="S2.SS1.SSS5.p2.3.m3.1"><semantics id="S2.SS1.SSS5.p2.3.m3.1a"><mrow id="S2.SS1.SSS5.p2.3.m3.1.1" xref="S2.SS1.SSS5.p2.3.m3.1.1.cmml"><mn id="S2.SS1.SSS5.p2.3.m3.1.1.2" xref="S2.SS1.SSS5.p2.3.m3.1.1.2.cmml">14</mn><mo id="S2.SS1.SSS5.p2.3.m3.1.1.1" xref="S2.SS1.SSS5.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS5.p2.3.m3.1b"><apply id="S2.SS1.SSS5.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS5.p2.3.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS5.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS5.p2.3.m3.1.1.1">percent</csymbol><cn id="S2.SS1.SSS5.p2.3.m3.1.1.2.cmml" type="integer" xref="S2.SS1.SSS5.p2.3.m3.1.1.2">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS5.p2.3.m3.1c">14\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS5.p2.3.m3.1d">14 %</annotation></semantics></math> of global emissions.
Beyond emissions, data centers’ water consumption is also raising alarm bells <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib130" title="">130</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib178" title="">178</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib157" title="">157</a>]</cite>.
By 2027, global AI demand may be responsible for withdrawal of 1.1–1.7 trillion gallons of fresh water annually <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib130" title="">130</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib95" title="">95</a>]</cite>.
Serious concerns also revolve around the rising levels of electronic waste <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib118" title="">118</a>]</cite>.
Even as we make progress in reducing the ecological cost of training and deploying the current AI models, we risk encouraging the development of even larger models and their wider deployment worsening the overall ecological impact (<em class="ltx_emph ltx_font_italic" id="S2.SS1.SSS5.p2.3.1">i.e.</em>, Jevons paradox).<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Jevons_paradox" title="">https://en.wikipedia.org/wiki/Jevons_paradox</a></span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS5.p3">
<p class="ltx_p" id="S2.SS1.SSS5.p3.1">Mechanism: Persuasive advertising

Generative AI may not only negatively impact the environment through increasing demand for natural resources and increasing generation of waste, but may also supercharge climate change disinformation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib197" title="">197</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib48" title="">48</a>]</cite>.
For example, the fossil-fuel industry may attempt to sway public opinion through advertising that leverages generative AI’s persuasion capabilities discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>.
Persuasive advertising may also be employed by other environmentally-unfriendly business models like fast-fashion <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib47" title="">47</a>]</cite>.
While the direct ecological cost of generative AI justifiably garners lots of attention, its potential impact on related online discourse also deserves scrutiny.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Risks</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">We categorize the risks of generative AI broadly to our society, to IR research, and to the environment.
We map the first three consequences discussed earlier in this section—<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">i.e.</em>,

<span class="ltx_inline-enumerate" id="S2.I5">
<span class="ltx_inline-item" id="S2.I5.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S2.I5.i1.1">Information ecosystem disruption (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS1" title="2.1.1 Consequence: Information ecosystem disruption ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>),
</span></span>
<span class="ltx_inline-item" id="S2.I5.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S2.I5.i2.1">concentration of power (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS2" title="2.1.2 Consequence: Concentration of power ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.2</span></a>), and
</span></span>
<span class="ltx_inline-item" id="S2.I5.i3"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span class="ltx_text" id="S2.I5.i3.1">marginalization (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS3" title="2.1.3 Consequence: Marginalization ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>)—and
</span></span>
</span>
their corresponding mechanisms as potentially contributing to the risks to society.
We further map the last two consequences—<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">i.e.</em>,

<span class="ltx_inline-enumerate" id="S2.I6">
<span class="ltx_inline-item" id="S2.I6.i4"><span class="ltx_tag ltx_tag_inline-item">(iv)</span> <span class="ltx_text" id="S2.I6.i4.1">Innovation decay (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS4" title="2.1.4 Consequence: Innovation decay ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.4</span></a>) and
</span></span>
<span class="ltx_inline-item" id="S2.I6.i5"><span class="ltx_tag ltx_tag_inline-item">(v)</span> <span class="ltx_text" id="S2.I6.i5.1">Ecological impact (§<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="2.1.5 Consequence: Ecological impact ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>)—to
</span></span>
</span>
the risks to IR research and the environment, respectively.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Risks to society</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Information access is a critical need of any democratic society and a necessary ingredient for social transformation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib173" title="">173</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib86" title="">86</a>]</cite>.
It is also a social determinant of economic progress <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib245" title="">245</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib156" title="">156</a>]</cite> and health <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib152" title="">152</a>]</cite>.
Disruptions to the information ecosystem bears potentially grave risks to most aspects of our social lives.
A confluence of the pandemic <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib186" title="">186</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib211" title="">211</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib70" title="">70</a>]</cite>, rising global conflicts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib210" title="">210</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib214" title="">214</a>]</cite>, and escalating climate catastrophes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib165" title="">165</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib108" title="">108</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib174" title="">174</a>]</cite> are pushing the world towards precarious instability.
Our information ecosystems are already struggling under the weight of misinformation and disinformation that in this critical moment is eroding public trust in online platforms, institutions, and each other.
It is imperative that researchers and developers of information access systems prioritize safeguarding social interests and be vigilant in considering potential risks of disruption and ecosystem collapse when integrating generative AI technologies in the IR stack.
This includes identifying the necessary conditions under which these technologies can be safely deployed and developing practical safeguards and alternatives.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">The risks to society are not just from potential disruptions of the information ecosystem, but also from how these technologies simultaneously concentrate power away from those at the margins of society.
As institutions that develop and operate these technologies are themselves beneficiaries of this concentration, we need democratic oversights.
If technologies further exacerbate already worsening wealth and power inequities, this additionally may pose severe threats to democratic institutions and human rights. There is an opportunity cost of not re-imagining information access in light of sociotechnical ambitions of human emancipation, culture, and knowledge production, instead of being constrained solely by what these emerging technologies make plausible and the homogenized visions put forth by institutions who wield these technologies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Risks to IR research</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">IR research can suffer from a confluence of different factors including the distancing of academic researchers from the data and compute they need to do their work and how narratives about the inevitability of AI technologies shapes what computational research gets funded.
The concentration of access to the networks around these technologies in a subset of institutions shapes what is considered “foundational” or even “AI”.
Research on generative AI should not be performed only in the context of corporate economic interests while academia is hollowed out and prevented from exploring radical new methods that challenge the status quo.
This risk of homogenization of academic research agendas and the opportunity cost of not exploring more diverse approaches to online information access can have material consequences. Instead, the IR community must be empowered with both the space and the resources necessary to explore a diversity of these visions and critique dominant narratives. IR research should have a plurality of work, which includes work with access to industry to change current practices. However, we especially also need to ensure that not all IR research is simply an extension of industrial system development and risk the demise of fundamental research on alternative avenues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Risks to environment</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Information access provides one of the large scale application settings for generative AI.
However, the impact of such wide-scale deployment of these technologies on the impending climate crisis should be a critical consideration. Climate costs pose substantial existential risks for ecosystems and people, in more direct ways than some other “existential risks” that lack adequate scientific basis but have nonetheless been popular discourse in some parts of the AI community. This means both choosing what to deploy, and investment in methods to mitigate negative impacts that build on existing environmental work. As we discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS1.SSS5" title="2.1.5 Consequence: Ecological impact ‣ 2.1 Consequences and mechanisms ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.1.5</span></a>, these concerns include not just the ecological cost of developing and deploying generative AI technologies but also their impact on online discourse on societal priorities.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods to evaluate risks and impact</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluating the impact of generative IR applications</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Evaluating the impact of generative IR applications requires methods, as do data-informed interventions to steer that impact. Creating an LLM-based demo has become exceedingly easy. Understanding the impact of a system when it gets used in real life contexts, and getting to a high quality experience for a wide variety of users, is much harder. Standards for impact assessment have not kept up a similar pace as tech developments. Klaaf points out the need to carefully consider the differences in value alignment of the goals of a system, and safety considations, harms and risks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib119" title="">119</a>]</cite>. A wide range of online, offline, and human-assisted evaluations are possible -and necessary- to get a full sense of the impact of a system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">There are a number of frameworks that can provide helpful starting points for evaluating the impact of generative IR applications, and potential quality or safety improvements. Not surprisingly however, they can measure quite different aspects of a system and its underlying models. Distinctions have to be made between evaluating a model, a system, or a technology as a whole. For example, standards for foundation model evaluations might not take into account the impact of a system that uses such a model (or a combination of models) in a specific application context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Measurement and interventions are possible at every stage of the development life cycle of products, and their underlying models and data. In this regard, general insights around for example harm mitigation interventions being possible throughout the Machine Learning life cycle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib202" title="">202</a>]</cite> also apply to generative IR. To improve quality and safety, we need to be able to operationalize and measure the impact of potential interventions. This includes evaluations on aspects of that might be both system performance issues, but are also of societal importance, e.g., harmful/toxic output, hallucination, and differing model performance across languages/demographics.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Threat identification, assessment, and modeling</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">When the emergence of a new technology or application becomes apparent, the assessment of whether this poses risks or opportunities within specific domains poses a challenge. Before development of a system, threats and opportunities can be identified.
As <cite class="ltx_cite ltx_citemacro_citet">Kapoor et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib115" title="">115</a>]</cite> point out, it’s crucial not to evaluate the risks and impact of new systems in isolation, but rather in comparison with existing technologies. For example, the impact of usage of foundation models in <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">search</span> should be compared to existing web <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">search</span>. For this purpose, Kapoor et al., present an evaluation framework focus on marginal risks, applied to Open Foundation Models. Their framework is based on threat identification work from cybersecurity and consists of six steps necessary to demonstrate such marginal risk. These steps are: 1) threat identification, 2) evaluating existing risk absent open foundation models, 3) considering existing defenses absent open foundation models, 4) evidence of marginal risk of open foundation models, 5) ease of defending against new risks, and 6) outlining uncertainty and assumptions. Note that this framework does not set exact assessment criteria, but rather defines the steps to get to such evaluations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In practical settings, this might mean having to select standards for the development process (e.g. emerging standards from organizations such as NIST <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib8" title="">8</a>]</cite> or ISO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib1" title="">1</a>]</cite>, company-specific standards such as Microsoft’s Responsible AI Standard v2 General Requirements <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib2" title="">2</a>]</cite>, or following new (local) legal requirements). However, mapping out potential consequences and identifying mechanisms that introduce risks in the specific context of a system needs to go much further. How to disrupt potential negative mechanisms in order to mitigate those risks requires gauging a wide range of consumer-side impacts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib66" title="">66</a>]</cite>, but also wider societal impacts. That includes frameworks focused on worker consequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib76" title="">76</a>]</cite>, or practical methods focused on reducing the (legal) risks of using certain types of copyrighted or restricted training data vs. expected performance gains <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib148" title="">148</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation during model development</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Model benchmarks vs. actual system context</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">LLM benchmarks are widely used to compare the quality and safety progress made by new <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.1.1">model</span> releases, resulting in model leaderboards on different scenarios. The Stanford HELM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib200" title="">200</a>]</cite> leaderboard for example shows the performance of different LLM models on benchmarks, and these benchmarks include societal impact and bias-related measures. Their HELM (‘holistic framework for evaluating foundation models’) framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib132" title="">132</a>]</cite> uses scenarios, and measures seven metrics. Those are accuracy, calibration, robustness, efficiency, but also more social impact-oriented fairness, bias, and toxicity. Each scenario focuses on one use case, and consists of a dataset of instances, such as the LegalBench set of legal reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib91" title="">91</a>]</cite>, or medical board exam problem sets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib110" title="">110</a>]</cite>.
The larger BIG-bench (“Beyond the Imitation Game benchmark”) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib22" title="">22</a>]</cite> consists of 200+ tasks, contributed by hundreds of authors at a variety of institutes. More specific benchmarks for trustworthiness such as DecodingTrust, in turn focus on subsets such as toxicity, stereotyping, adversarial and out-of distribution robustness, privacy, machine ethics, and fairness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib231" title="">231</a>]</cite>, while for example the much more specific recurring TREC Fair Ranking track competitively evaluates systems according to how fairly they rank documents on a specific test task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">Paradoxically, while these benchmarks include aspects of societal impacts such as bias and toxicity, they do not necessarily cover the aspects that matter most in a specific application context in practice. Benchmarks are generally geared towards structured comparisons between models, <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p2.1.1">not</span> towards evaluating end-user applications in practice. This means that they may not be particularly suitable for a specific application and the people involved in its usage. In addition, using such large benchmarks can be quite resource intensive, making ‘lite’ versions necessary that are less comprehensive. Both Helm and BIG-Bench are also implemented as Lite versions. However, the evaluation differences that arise from specific, lighter implementations of benchmarks can significantly impact model comparison results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib198" title="">198</a>]</cite>. This makes it necessary to go beyond these benchmarks, and ensure suitable evaluations for the application at hand to avoid deriving conclusions about safety or responsibility devoid from actual application concerns.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Combining IR and generative AI evaluation metrics</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">It is challenging that standards for measuring societal impact, including bias, fairness and etc. are yet scarce in IR <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.1">product</span> settings. For example, <cite class="ltx_cite ltx_citemacro_citet">Smith et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib195" title="">195</a>]</cite> provide an overview of different metrics available for evaluating bias and fairness in recommendation systems, and the challenges practitioners face when choosing between them. In some cases, it may be more appropriate to for example focus on ‘traditional’ performance and accuracy metrics, but study the performance and subsequent quality of experiences for different groups of people by segmenting/slicing results by group. This approach assumes the ability to define relevant groups, or relies on more advanced methods to find clusters that may—or may not—have significant differences in performance or quality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">Specific methods might also be necessary to match new techniques. For example, Retrieval Augmented Generation (RAG) might be used to include more reliable information in a specific domain and reduce hallucinations in an LLM setting. However, RAG does not necessarily fully solve every hallucination-related issue. Specific frameworks that fit an application context are still necessary to evaluate these techniques and their actual impact on aspects such as factuality within that context. One example is <cite class="ltx_cite ltx_citemacro_citet">Saad-Falcon et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib181" title="">181</a>]</cite>, who present an evaluation framework, ARES, for RAG-assisted Question &amp; Answering settings. This framework uses three evaluation scores: context relevance of the retrieved information, answer faithfulness (the answer’s grounding in the retrieved context), and answer relevance to the question asked. These are similar to IR-evaluations, but might need adjustment to the setting at hand, and datasets used need to reflect actual needs in current circumstances.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>LLMs to evaluate LLM</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Beyond specific metrics, ongoing research is investigating the efficacy of LLMs to evaluate LLMs (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS3.p1.1.1">LLM-as-judge</span>)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib233" title="">233</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib190" title="">190</a>]</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib233" title="">233</a>]</cite> et al. use an LLM to rate the factuality of a long-form response to prompts, while also using Google Search. While promising, such more complex evaluation constellations also lead to additional complexity in understanding what is being evaluated, and changes therein as the evaluator LLM changes. This leads to having to validate the validation in itself <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib190" title="">190</a>]</cite>. While a human-and-LLM agent collaboration can help in this validation (as in e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib190" title="">190</a>]</cite>’s EvalGen approach), the evaluation criteria cannot be fully separated from observation of model outputs, resulting in a feedback loop from output to adjusted evaluation criteria.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation pre/post system release</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Online evaluation using actual user behavior vs. offline evaluation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Whether evaluations are done online or offline can deeply impact results. Offline evaluations—even when using thoughtful standards—might not reflect what actual end-users do in real-life settings, or system performance over time. Online evaluations similarly are limited to which metrics have been instrumented and how actual user interactions are captured. It involves field testing; getting an IR system online and out to actual users and analyzing their interactions with the system. It can include methods such as controlled experiments or extended A/B testing, and analysis of interactions; Hoffmann provides an overview of most common techniques used in IR settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib106" title="">106</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Stress testing, red-teaming and qualitative end-user evaluations</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Beyond metrics and quantitative analysis oriented methods, it is crucial to apply a combination of safety/security-inspired methods, user design and UX research methods to understand the actual reactions of users.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">The logistics around red teaming can provide a good glimpse into the importance of appropriate combinations of methods. Red teaming is a common way to test LLM applications for undesirable system responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib140" title="">140</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib149" title="">149</a>]</cite>.
Red teaming can be automated using for example sets of (generated) prompts, or done in full by human red teamers, including both the general public, or invited experts. Using LLMs as red teamers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib169" title="">169</a>]</cite> by generating risky prompts at scale, or using large-scale human red teaming efforts with thousands of participants who need access points, might yield different results. Human red team approaches in which “a group of people authorized and organized to emulate a potential adversary’s attack or exploitation capabilities against an enterprise’s security posture” (if we follow the definition from the National Institute of Standards and Technology, NIST) also lead to questions about tooling, recruiting and operational process design. <cite class="ltx_cite ltx_citemacro_citet">Markov et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib140" title="">140</a>]</cite> for example provide a helpful discussion of practical data challenges in content moderation use cases. In turn, model characteristics might have consequences on red teaming results. <cite class="ltx_cite ltx_citemacro_citet">Ganguli et al. [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib75" title="">75</a>]</cite>, for instance find that RLHF (reinforcement learning from human feedback) models are increasingly difficult to red team as they scale, while they don’t find similar challenges for other models. Interestingly this means that techniques such RLHF that are explicitly meant to help align agents with human preferences could also result in challenges in evaluating the systems that use them.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1">This means that like any evaluation method, red teaming has to be combined with other types of stress testing, assessment of security issues, as well as evaluation of experiences of actual users. Khlaaf points out the need for carefully considering what methods and terminology are appropriate for evaluations that probe for vulnerabilities of a specific system towards the outside world <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib119" title="">119</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Societal impact of a system beyond its direct implementation and use</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The impact of a system can reach much beyond its direct usage context. For example, the increasing demand for data and compute power of LLMs has environmental impact. However, such indirect impact can be hard to calculate without deep expertise. It is crucial to spend the time to evaluate evaluations methods for their suitability.
Methods have been developed in both the IR and LLM communities around reducing environmental harm<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib184" title="">184</a>]</cite> and sustainability industry teams exist to ensure more energy efficient data centers for both environmental as well as monetary reasons. Others in turn try to assess whether LLMs could help in generating more green code, and develop metrics to assess the code’s ‘green capacity’ based on earlier sustainability metrics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib217" title="">217</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">Similarly, a plethora of work points out the potential of amplifying and entrenching power structures through the usage of generative AI methods, or changing market conditions through releasing new models for free <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib179" title="">179</a>]</cite>, de-facto changing standards to the model that gets used most in practice. However, IR and ML evaluation methods are not generally suitable for the analysis of such impact that a particular technique or system might have. Methods from political analysis and behavioral economics might be more suitable, but are generally not shared in IR or ML venues. Challenging in the evaluation of systems is a deeper understanding of the long-term incentives that are created, and the resulting ‘rational’ use of LLMs in undesirable ways. A compounding challenge is that new incentives are also necessary to ensure that interventions from actual practice can be shared. Trust &amp; Safety teams might be doing scenario planning or prepare for incidents and crises.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Sharing evaluation methods</h3>
<div class="ltx_para ltx_noindent" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">From the above selection of methods, which is by no means comprehensive, it is clear that practitioners have to carefully pick and choose which methods work for them. However, different organizations come from different evaluation traditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Incentives to share methods and results might not align with practical product team incentives and pressures. Metrics and standards for evaluations from actual practice are often not shared in scientific literature. Security community-style (external) red and (internal) blue teams, Trust &amp; Safety incident monitoring approaches, IR-communities’ existing offline and online user feedback methods, or UX product testing approaches might be more (or less) top of mind depending on the organization and prior expertise. This means there is a gap in the generative IR literature in terms of shared understanding of actual practices and efficacy of methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib53" title="">53</a>]</cite>. If we as a community are to properly address the social risks as outlined in <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS1" title="2.2.1 Risks to society ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>, it is imperative we find fast and effective ways to share these methods and align them with practical needs. Especially with the increasing speed of the field, the variety of fields involved, and volume of new techniques.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Different types of existing evaluation frameworks relevant for generative IR impact &amp; safety. Note this is not an exhaustive overview, but rather a quick peek at the variety of methods evaluators can (and have to) choose from</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.1.1">Evaluation focus</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1.1.1">Examples</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1" style="width:195.1pt;">Marginal system impact, e.g. release decisions in comparison with existing technology</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1" style="width:195.1pt;">Kapoor et al., risk framework based on cybersecurity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib115" title="">115</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1" style="width:195.1pt;">Comparison benchmarks between LLM models that include fairness, bias, toxicity-type aspects</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1" style="width:195.1pt;">Benchmarks used in leaderboards, e.g., HELM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib200" title="">200</a>]</cite>, BIG-bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib22" title="">22</a>]</cite>, or trustworthiness benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib231" title="">231</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.3.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1.1" style="width:195.1pt;">Online or offline IR metrics, including
accuracy or quality across groups</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.2.1">
<span class="ltx_p" id="S3.T2.1.4.3.2.1.1" style="width:195.1pt;">Online IR-evaluation methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib105" title="">105</a>]</cite>, impact/fairness/bias metrics in recommendation systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib194" title="">194</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.5.4.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.1.1">
<span class="ltx_p" id="S3.T2.1.5.4.1.1.1" style="width:195.1pt;">Evaluation metrics using automated evaluation for specific LLM techniques or risks</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.2.1">
<span class="ltx_p" id="S3.T2.1.5.4.2.1.1" style="width:195.1pt;">E.g., LLMs as agents evaluating factuality of other LLMs’ statements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib233" title="">233</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib190" title="">190</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.6.5.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.1.1">
<span class="ltx_p" id="S3.T2.1.6.5.1.1.1" style="width:195.1pt;">Qualitative evaluation including human adversarial testing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.6.5.2" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.2.1">
<span class="ltx_p" id="S3.T2.1.6.5.2.1.1" style="width:195.1pt;">E.g., red teaming<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib75" title="">75</a>]</cite>, and UX evaluation</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Actors, incentives and ways of getting organized</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Incentives towards misuse of AI</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Emerging AI capabilities and their consequences (good or bad) are a hot topic of discussion. But it is just as important to talk about incentives, or why individuals or organizations might choose to use AI in certain ways.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Below are some examples of types of actors and their possible incentives that can lead to harmful uses of AI, along with ways in which some of them can be shifted in a more positive direction. AI can be transformative for human experience and quality of life, but only if incentives (both short-term and long-term) for its use are aligned with the benefits to humanity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Actor</span>: State actors and ideological groups.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.2">Incentive</span>: Geopolitical influence in favor or against something. This includes the use of extra-persuasive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib240" title="">240</a>]</cite>, micro-targeted content and deepfakes to sow malicious narratives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib193" title="">193</a>]</cite>, undermine support and trust in democratic institutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib153" title="">153</a>]</cite>, weaken social cohesion, etc.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.3">Modification</span>: The most effective way to modify this behavior is by making it prohibitively expensive or inconvenient to use AI for these purposes, through harsh legal consequences, content moderation, or counter-speech. The burden of implementing countermeasures falls on governments, content platforms, and community organizations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Actor</span>: Criminal or unscrupulous organizations.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.2">Incentive</span>: Financial gains from scams, ad-monetized website traffic, or product sales. This includes more legit-looking phishing content <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib230" title="">230</a>]</cite> and “Nigerian prince” letters; or gaming search engines via AI-generated SEO-friendly content <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib160" title="">160</a>]</cite>.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.3">Modification</span>: The incentives for financial gain are always going to exist and be exploited; protection against them can take the form of better (AI-enhanced) cybersecurity and anti-spam tools, implemented and deployed by most consumer-facing web surfaces.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Actor</span>: Commercial enterprises.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.2">Incentive</span>: Economic competitive advantage and increased shareholder value. Taken to its worst extreme, this incentive can lead to deceptive or discriminatory business practices, hasty deployment of cheaply developed AI to customers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib71" title="">71</a>]</cite>, premature restructuring of teams <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib127" title="">127</a>]</cite>, etc. In the case of social media platforms, the high engagement on polarizing or sensationalist content can lead the platforms to tolerate, encourage, and algorithmically amplify it.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.3">Modification</span>: The same drive for competitive advantage can also be a force for good, particularly when it is aligned with public opinion or customer sentiment. The best-case scenario is when trustworthy and safe AI makes products more usable, attracting more customers (akin to Apple’s “it just works” aesthetic that has no shortage of fans despite being more expensive than the competition). Government-led compliance requirements can also create positive incentives, like for food or car safety. And in some cases, a punitive legal strategy also works, like in the suing of tobacco companies or opiate producers, creating incentives for surviving companies to behave better.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Actor</span>: Individuals.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.2">Incentives</span>: Faster completion of work tasks, improved social status, revenge against perceived slights, or exploitation of the vulnerable. At worst, these can lead to cheating, misrepresentation of one’s identity of accomplishments, slander, deep-fake pornography, or AI-enhanced grooming.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.3">Modifications</span>: While some of these behaviors are illegal or fundamentally antisocial (and should be prosecuted as dictated by law), the urge to improve one’s work performance or social status can be a good thing. If AI tools are designed to enhance human productivity while rewarding our creative impulses, and feel fun, joyful, and satisfying to use, people will be more likely to employ them to good ends.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Who can shift incentives, and how</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In the broadest sense, it will take a whole-of-society approach to ensure that technological advances will align with the best interests of humans impacted by them (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S4.F1" title="Figure 1 ‣ 4.2 Who can shift incentives, and how ‣ 4 Actors, incentives and ways of getting organized ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">1</span></a>). Technology builders (company and individual), governments, academia, and civil society all bear responsibility for ensuring that technological advances in information access align with societal interests. The rest of this section focuses on what can be done at the intersection of these groups or actors, since inter-group coordination is most often where things go awry.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="420" id="S4.F1.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Primary actors responsible for aligning technology with societal interests</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Organizational Factors</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">While most of the literature and education in computer science by definition focuses on technical approaches, the impact of generative IR techniques can be influenced in other ways as well.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Changing work processes <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">within </span>organizations can have a direct impact on the expectations set on teams. This includes policies, explicit Go/No-Go procedures, roles and responsibilities to monitor systems, algorithmic impact assessments and model cards or other types of documentation. In different organizations, the responsibility for different measurement and mitigation might look very different. In one organization, a Machine Learning team may be expected to look at the energy consumption of their system design choices, whereas other organizations might have a technical sustainability team. In another organization, a Trust &amp; Safety or Integrity team might deliver evaluations of system output toxicity, whereas in another organization a separate Data Science team, or Product teams themselves, might have to do this work. In any case, if this responsibility is unclear, it is much harder to get this work done.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.1.1">External</span> engagement can help address internal deficiencies. Especially for audiences working on generative IR systems, some of these might not necessarily be familiar routes.
Examples include:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p4">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">External advice and safety boards. </span> increasingly created by companies to provide external advice for more complex safety or content moderation questions. This includes Facebook’s Oversight Board <span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>https://www.oversightboard.com</span></span></span>, which provides independent rulings on content moderation questions; parent company Meta’s Safety Advisory Council <span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>https://www.facebook.com/help/222332597793306/</span></span></span>
; or Spotify’s Safety Advisory Board <span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span>https://newsroom.spotify.com/2022-06-13/introducing-the-spotify-safety-advisory-council/</span></span></span>. These do not necessarily have decision making power, but provide a more formalized way to advise external organizations and researchers.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Regulatory advisory groups and expert consultations. </span> Organizations such as the UN, EU, various regions and countries working on future AI policy have all formed advisory boards (e.g., the UN AI advisory board <span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span>https://www.un.org/en/ai-advisory-body</span></span></span>, the Nordic AI advisory board). Apart from such official avenues, individual lawmakers and legal firms often consult experts. While regulatory capture is a very real concern <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib237" title="">237</a>]</cite>, this also allows for actually implementable regulation. This means owever that considering the potential overlap between advisory boards, as well as perhaps a lack of overlap with more specific AI experts, not all relevant expertise will be represented.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.1">Professional organizations. </span> Organizations such as ACM, IEEE, AAAI, the Trust &amp; Safety Professional Association allow for formal and informal exchange of best practices. A major challenge is ensuring that best practices in fast moving areas are also gathered and exchanged <span class="ltx_text ltx_font_italic" id="S4.I1.i3.p1.1.2">between</span> organizations and to the public at large.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">For the above arrangements, getting to collections of concrete examples of what has worked in the past is increasingly important. AI developments are speeding up, and increasingly diverse professional communities are both being impacted and getting involved. This makes efficient and effective coordination even more important. For policy makers, governmental agencies and journalists it may be hard to get an overview of which professional communities can provide actionable advice—especially with new AI developments being ‘louder’ than, for example, long-standing IR communities.
Inside of companies, in order to benefit from external advice or research, tech teams still have to navigate how to best work with external organizations. Researchers and non-governmental organizations in turn have to know where to invest their time and expertise most effectively, and how to offer actionable advice to appropriate individuals or teams in tech companies. This includes big picture scenario planning of where to best invest, and how to create incentives that truly will have a positive impact. Implicit hierarchies of the value of different types of produced knowledge (e.g. ’being the first’ or ’more technically complex’), but also a simple lack of knowledge about how certain processes work, can stand in the way of sharing of paved paths towards desired results, and of sharing these in accessible ways. It can also involve very pragmatic on-the-ground work, such as knowing how to set up contractual arrangements that work for all parties (not a skill commonly taught in IR or AI-related programs).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Data-focused methods</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">While a complete overview of all different mechanisms to positively affect AI development is outside the scope of this paper, one area does provide ample inspiration. Extensive literature exists on data labor and the need to understand how to effectively advocate for that labor’s value <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib129" title="">129</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib18" title="">18</a>]</cite>. Especially in the realm of training data concerns, multiple practical routes already exist, including:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">Business and partnership model development,</span> including developing new types of licensing and new types of business partnerships <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib3" title="">3</a>]</cite>, along with ways to get funding to data creators. There is also also research on the efficacy of suggested mechanisms, such as data dividends that are suggested as a means of AI profit sharing <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib228" title="">228</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">Collective action</span>. When new business models do not work out, coordinated action is imperative. These can be focused on data through data strikes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib227" title="">227</a>]</cite>, as well as large-scale labor organizing and strikes focused on treatment of data workers. More recently the Hollywood strikes illustrated how those particularly impacted by the ways their work and likeness can be used as data, can effectively organize, lay out clear demands and succeed through both technical and organizational competence. This included understanding what incentives are at play and what leverage data producers have <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib223" title="">223</a>]</cite>. Methods include data strikes to withhold data <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib227" title="">227</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib206" title="">206</a>]</cite>, data poisoning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib59" title="">59</a>]</cite> techniques such as NightShade <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib189" title="">189</a>]</cite>, Glaze <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib188" title="">188</a>]</cite> and Mist <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib131" title="">131</a>]</cite>.
Ways to empower end-users and the wider public in their relationship with tech companies are important <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib229" title="">229</a>]</cite>, as is understanding their potential leverage and means for protest through adjusted usage <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib128" title="">128</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">For effective research-informed mitigations, however, it is crucial that generative IR researchers have access to ways to learn how to effectively organize and navigate organizational and political structures, or how to communicate their results to others. Implicit hierarchies in what knowledge is appreciated in generative IR circles can become a hurdle in effectively identifying and addressing the risks outlined in earlier sections, §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS1" title="2.2.1 Risks to society ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>, §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS2" title="2.2.2 Risks to IR research ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>, and §<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S2.SS2.SSS3" title="2.2.3 Risks to environment ‣ 2.2 Risks ‣ 2 Implications of generative AI for information access ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>. A critical factor is knowing which concrete situations matter, what to ask for in those situations and how to assess whether impacts and risks are successfully steered.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S5.F2.g1" src="x3.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mitra’s <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite> hierarchy of IR stakeholder needs.
More critical needs are at the bottom of the pyramid.
This figure has been reproduced from the original paper with permission.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this chapter we have presented a discussion on the sociotechnical implications of generative AI for information access.
These deliberations are grounded in how these emerging technologies are currently being applied in IR applications as well as their future applications as being envisioned by practitioners and researchers.
It is important to recognize that sociotechnical visions of what information access should look like in the future are not just shaped by what emerging technologies like generative AI make plausible, but that visions for the future of information access in turn shape AI technologies themselves.
<cite class="ltx_cite ltx_citemacro_citet">Mitra [<a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#bib.bib150" title="">150</a>]</cite> proposed the hierarchy of IR stakeholder needs shown in <a class="ltx_ref" href="https://arxiv.org/html/2405.11612v2#S5.F2" title="Figure 2 ‣ 5 Conclusion ‣ Sociotechnical Implications of Generative Artificial Intelligence for Information Access"><span class="ltx_text ltx_ref_tag">2</span></a> and argued that IR research and system development require a fundamental shift towards re-centering societal needs and that we should reimagine information access as a vehicle for alternative futures.
When contemplating the implications of emerging technologies, we risk of falling in the trap of limiting ourselves to how the technology (and its process of development) is today, rather than how it can be or <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">should be</em> in the future.
Neither generative AI nor its application in the context of information access is predetermined.
So, while it is important that we consider potential harms of contemporary applications of generative AI in the context of information access, we close with some open question for the reader:
<em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">
If not this status quo, then what—and especially how?
What is the future of information access that we want to imagine for our collective wellbeing, and how can generative AI be another tool in the toolbox towards that transformation?
</em></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Artificial intelligence (ai) standards.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.iso.org/sectors/it-technologies/ai" title="">https://www.iso.org/sectors/it-technologies/ai</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">mic [2022]</span>
<span class="ltx_bibblock">
Microsoft responsible ai standard, v2, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl" title="">https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ap2 [2023]</span>
<span class="ltx_bibblock">
Ap, open ai agree to share select news content and technology in new collaboration, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ap.org/media-center/press-releases/2023/ap-open-ai-agree-to-share-select-news-content-and-technology-in-new-collaboration/" title="">https://www.ap.org/media-center/press-releases/2023/ap-open-ai-agree-to-share-select-news-content-and-technology-in-new-collaboration/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bbc [2023]</span>
<span class="ltx_bibblock">
Ai used to target kids with disinformation, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bbc.co.uk/newsround/66796495" title="">https://www.bbc.co.uk/newsround/66796495</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">eua [2023]</span>
<span class="ltx_bibblock">
Supporting open source and open science in the eu ai act, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/blog/assets/eu_ai_act_oss/supporting_OS_in_the_AIAct.pdf" title="">https://huggingface.co/blog/assets/eu_ai_act_oss/supporting_OS_in_the_AIAct.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">shu [2023]</span>
<span class="ltx_bibblock">
Shutterstock expands partnership with openai, signs new six-year agreement to provide high-quality training data, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year" title="">https://investor.shutterstock.com/news-releases/news-release-details/shutterstock-expands-partnership-openai-signs-new-six-year</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">foe [2024]</span>
<span class="ltx_bibblock">
Report: Artificial intelligence a threat to climate change, energy usage and disinformation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://foe.org/news/ai-threat-report/" title="">https://foe.org/news/ai-threat-report/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">nis [2024]</span>
<span class="ltx_bibblock">
Ai standards, nist, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nist.gov/artificial-intelligence/ai-standards" title="">https://www.nist.gov/artificial-intelligence/ai-standards</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">sus [2024]</span>
<span class="ltx_bibblock">
Report: Ai fueling climate change, energy usage and disinformation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sustainablebrands.com/read/product-service-design-innovation/ai-fueling-climate-change-energy-disinformation" title="">https://sustainablebrands.com/read/product-service-design-innovation/ai-fueling-climate-change-energy-disinformation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">wef [2024]</span>
<span class="ltx_bibblock">
World economic forum global risks report 2024, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.weforum.org/publications/global-risks-report-2024/" title="">https://www.weforum.org/publications/global-risks-report-2024/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abid et al. [2021]</span>
<span class="ltx_bibblock">
A. Abid, M. Farooqi, and J. Zou.

</span>
<span class="ltx_bibblock">Persistent anti-muslim bias in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</em>, pages 298–306, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agüera y Arcas et al. [2017]</span>
<span class="ltx_bibblock">
B. Agüera y Arcas, M. Mitchell, and A. Todorov.

</span>
<span class="ltx_bibblock">Physiognomy’s new clothes.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a" title="">https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Sibai [2023]</span>
<span class="ltx_bibblock">
N. Al-Sibai.

</span>
<span class="ltx_bibblock">The top google image for israel kamakawiwo’ole is ai-generated.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Futurism</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alemohammad et al. [2023]</span>
<span class="ltx_bibblock">
S. Alemohammad, J. Casco-Rodriguez, L. Luzi, A. I. Humayun, H. Babaei, D. LeJeune, A. Siahkoohi, and R. G. Baraniuk.

</span>
<span class="ltx_bibblock">Self-consuming generative models go mad.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2307.01850</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altenried [2020]</span>
<span class="ltx_bibblock">
M. Altenried.

</span>
<span class="ltx_bibblock">The platform as factory: Crowdwork and the hidden labour behind artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Capital &amp; Class</em>, 44(2):145–158, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ANGUIANO and BECKETT [2023]</span>
<span class="ltx_bibblock">
D. ANGUIANO and L. BECKETT.

</span>
<span class="ltx_bibblock">How hollywood writers triumphed over ai–and why it matters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The Guardian, October</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Appel et al. [2023]</span>
<span class="ltx_bibblock">
G. Appel, J. Neelbauer, and D. A. Schweidel.

</span>
<span class="ltx_bibblock">Generative ai has an intellectual property problem.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Harvard Business Review</em>, 7, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arrieta-Ibarra et al. [2018]</span>
<span class="ltx_bibblock">
I. Arrieta-Ibarra, L. Goff, D. Jiménez-Hernández, J. Lanier, and E. G. Weyl.

</span>
<span class="ltx_bibblock">Should we treat data as labor? moving beyond “free”.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">aea Papers and Proceedings</em>, volume 108, pages 38–42. American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asokan [2024]</span>
<span class="ltx_bibblock">
A. Asokan.

</span>
<span class="ltx_bibblock">Uk government warned of ai regulatory capture by big tech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">BankInfoSecurity</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baio [2024]</span>
<span class="ltx_bibblock">
A. Baio.

</span>
<span class="ltx_bibblock">’most disturbing ai site on internet’ can find every picture of you that exists.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Indy100</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belkhir and Elmeligi [2018]</span>
<span class="ltx_bibblock">
L. Belkhir and A. Elmeligi.

</span>
<span class="ltx_bibblock">Assessing ict global emissions footprint: Trends to 2040 &amp; recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Journal of cleaner production</em>, 177:448–463, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bench authors [2023]</span>
<span class="ltx_bibblock">
B. bench authors.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
<span class="ltx_bibblock">ISSN 2835-8856.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=uyTL5Bvosj" title="">https://openreview.net/forum?id=uyTL5Bvosj</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al. [2021]</span>
<span class="ltx_bibblock">
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?<span class="ltx_text" id="bib.bib23.1.1" style="position:relative; bottom:-5.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="21" id="bib.bib23.1.1.g1" src="extracted/5735636/parrot.png" width="19"/></span>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berreby [2024]</span>
<span class="ltx_bibblock">
D. Berreby.

</span>
<span class="ltx_bibblock">As use of a.i. soars, so does the energy and water it requires.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Yale Environment 360</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane [2020]</span>
<span class="ltx_bibblock">
A. Birhane.

</span>
<span class="ltx_bibblock">Algorithmic colonization of africa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">SCRIPTed</em>, 17:389, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane and Cummins [2019]</span>
<span class="ltx_bibblock">
A. Birhane and F. Cummins.

</span>
<span class="ltx_bibblock">Algorithmic injustices: Towards a relational ethics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1912.07376</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birhane et al. [2022]</span>
<span class="ltx_bibblock">
A. Birhane, P. Kalluri, D. Card, W. Agnew, R. Dotan, and M. Bao.

</span>
<span class="ltx_bibblock">The values encoded in machine learning research.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 173–184, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et al. [2016]</span>
<span class="ltx_bibblock">
S. L. Blodgett, L. Green, and B. O’Connor.

</span>
<span class="ltx_bibblock">Demographic dialectal variation in social media: A case study of african-american english.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:1608.08868</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et al. [2020]</span>
<span class="ltx_bibblock">
S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach.

</span>
<span class="ltx_bibblock">Language (technology) is power: A critical survey of" bias" in nlp.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2005.14050</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al. [2016]</span>
<span class="ltx_bibblock">
T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai.

</span>
<span class="ltx_bibblock">Man is to computer programmer as woman is to homemaker? debiasing word embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. [2021]</span>
<span class="ltx_bibblock">
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brewster et al. [2023a]</span>
<span class="ltx_bibblock">
J. Brewster, L. Arvanitis, and M. Sadeghi.

</span>
<span class="ltx_bibblock">Funding the next generation of content farms: Some of the world’s largest blue chip brands unintentionally support the spread of unreliable ai-generated news websites.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">NewsGuard</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brewster et al. [2023b]</span>
<span class="ltx_bibblock">
J. Brewster, Z. Fishman, and E. Xu.

</span>
<span class="ltx_bibblock">Funding the next generation of content farms: Some of the world’s largest blue chip brands unintentionally support the spread of unreliable ai-generated news websites.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">NewsGuard</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke [2023]</span>
<span class="ltx_bibblock">
K. Burke.

</span>
<span class="ltx_bibblock">’biggest act of copyright theft in history’: thousands of australian books allegedly used to train ai model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Guardian</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burke [2024]</span>
<span class="ltx_bibblock">
K. Burke.

</span>
<span class="ltx_bibblock">Generative ai is a marvel. is it also built on theft?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">The Economist</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burtell and Woodside [2023]</span>
<span class="ltx_bibblock">
M. Burtell and T. Woodside.

</span>
<span class="ltx_bibblock">Artificial influence: An analysis of ai-driven persuasion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2303.08721</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caliskan et al. [2017]</span>
<span class="ltx_bibblock">
A. Caliskan, J. J. Bryson, and A. Narayanan.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language corpora contain human-like biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Science</em>, 356(6334):183–186, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. Cantrell.

</span>
<span class="ltx_bibblock">The true cost of ai innovation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Scientific Computing World</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. [2021]</span>
<span class="ltx_bibblock">
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">30th USENIX Security Symposium (USENIX Security 21)</em>, pages 2633–2650, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carroll et al. [2023]</span>
<span class="ltx_bibblock">
M. Carroll, A. Chan, H. Ashton, and D. Krueger.

</span>
<span class="ltx_bibblock">Characterizing manipulation from ai systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</em>, pages 1–13, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chayka [2023]</span>
<span class="ltx_bibblock">
K. Chayka.

</span>
<span class="ltx_bibblock">Is a.i. art stealing from artists?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">The New Yorker</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chesterman [2024]</span>
<span class="ltx_bibblock">
S. Chesterman.

</span>
<span class="ltx_bibblock">Good models borrow, great models steal: intellectual property rights and generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Policy and Society</em>, page puae006, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. [2017]</span>
<span class="ltx_bibblock">
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coffey [2021]</span>
<span class="ltx_bibblock">
D. Coffey.

</span>
<span class="ltx_bibblock">Māori are trying to save their language from big tech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Wired UK</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohan [2023]</span>
<span class="ltx_bibblock">
W. D. Cohan.

</span>
<span class="ltx_bibblock">Ai is learning from stolen intellectual property. it needs to stop.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">The Washington Post</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coldewey [2023]</span>
<span class="ltx_bibblock">
D. Coldewey.

</span>
<span class="ltx_bibblock">Thousands of authors sign letter urging ai makers to stop stealing books.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">TechCrunch</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coleman [2023]</span>
<span class="ltx_bibblock">
J. Coleman.

</span>
<span class="ltx_bibblock">Ai’s climate impact goes beyond its emissions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Scientific American</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corbett [2024]</span>
<span class="ltx_bibblock">
J. Corbett.

</span>
<span class="ltx_bibblock">Report warns generative ai could turbocharge climate disinformation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Common Dreams</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Correia [2002]</span>
<span class="ltx_bibblock">
A. M. R. Correia.

</span>
<span class="ltx_bibblock">Information literacy for an active and effective citizenship.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">White Paper prepared for UNESCO, the US National Commission on Libraries and Information Science, and the National Forum on Information Literacy, for use at the Information Literacy Meeting of Experts, Prague, The Czech Republic</em>, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Couldry and Mejias [2019]</span>
<span class="ltx_bibblock">
N. Couldry and U. A. Mejias.

</span>
<span class="ltx_bibblock">Data colonialism: Rethinking big data’s relation to the contemporary subject.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Television &amp; New Media</em>, 20(4):336–349, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cox [2024]</span>
<span class="ltx_bibblock">
J. Cox.

</span>
<span class="ltx_bibblock">Google news is boosting garbage ai-generated articles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">404 Media</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coyle [2023]</span>
<span class="ltx_bibblock">
J. Coyle.

</span>
<span class="ltx_bibblock">In hollywood writers’ battle against ai, humans win (for now), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cramer [2022]</span>
<span class="ltx_bibblock">
H. Cramer.

</span>
<span class="ltx_bibblock">Practical routes in the ux of ai, or sharing more beaten paths.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Interactions</em>, 29(5):89–91, aug 2022.

</span>
<span class="ltx_bibblock">ISSN 1072-5520.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3555834</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3555834" title="">https://doi.org/10.1145/3555834</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crawford [2017]</span>
<span class="ltx_bibblock">
K. Crawford.

</span>
<span class="ltx_bibblock">The trouble with bias.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Conference on Neural Information Processing Systems, invited speaker</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Criddle and Bryan [2024]</span>
<span class="ltx_bibblock">
C. Criddle and K. Bryan.

</span>
<span class="ltx_bibblock">Ai boom sparks concern over big tech’s water consumption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">The Conversation</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al. [2024]</span>
<span class="ltx_bibblock">
F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri.

</span>
<span class="ltx_bibblock">The power of noise: Redefining retrieval for rag systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2401.14887</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">del Rio-Chanona et al. [2023]</span>
<span class="ltx_bibblock">
M. del Rio-Chanona, N. Laurentsyeva, and J. Wachs.

</span>
<span class="ltx_bibblock">Are large language models a threat to digital public goods? evidence from activity on stack overflow.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2307.07367</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhawan [2023]</span>
<span class="ltx_bibblock">
S. Dhawan.

</span>
<span class="ltx_bibblock">Universities leveraging ai detectors: International students fear they may be wrongly accused of cheating.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Financial Express</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dickson [2020]</span>
<span class="ltx_bibblock">
B. Dickson.

</span>
<span class="ltx_bibblock">What is machine learning data poisoning?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">The Verge</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Disinformation [2024]</span>
<span class="ltx_bibblock">
C. A. A. Disinformation.

</span>
<span class="ltx_bibblock">Artificial intelligence threats to climate change.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al. [2022]</span>
<span class="ltx_bibblock">
J. Dodge, T. Prewitt, R. Tachet des Combes, E. Odmark, R. Schwartz, E. Strubell, A. S. Luccioni, N. A. Smith, N. DeCario, and W. Buchanan.

</span>
<span class="ltx_bibblock">Measuring the carbon intensity of ai in cloud instances.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2022 ACM conference on fairness, accountability, and transparency</em>, pages 1877–1894, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupré’ [2023a]</span>
<span class="ltx_bibblock">
M. H. Dupré’.

</span>
<span class="ltx_bibblock">Sports illustrated published articles by fake, ai-generated writers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Futurism</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dupré’ [2023b]</span>
<span class="ltx_bibblock">
M. H. Dupré’.

</span>
<span class="ltx_bibblock">Top google result for "edward hopper" an ai-generated fake.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Futurism</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duran [2024]</span>
<span class="ltx_bibblock">
G. Duran.

</span>
<span class="ltx_bibblock">The tech baron seeking to “ethnically cleanse” san francisco.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">The New Republic</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekstrand et al. [2022]</span>
<span class="ltx_bibblock">
M. D. Ekstrand, G. McDonald, A. Raj, and I. Johnson.

</span>
<span class="ltx_bibblock">Overview of the trec 2021 fair ranking track.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">The Thirtieth Text REtrieval Conference (TREC 2021) Proceedings</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekstrand et al. [2024]</span>
<span class="ltx_bibblock">
M. D. Ekstrand, L. Beattie, M. S. Pera, and H. Cramer.

</span>
<span class="ltx_bibblock">Not just algorithms: Strategically addressing consumer impacts in information retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Advances in Information Retrieval: 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24–28, 2024, Proceedings, Part IV</em>, page 314–335, Berlin, Heidelberg, 2024. Springer-Verlag.

</span>
<span class="ltx_bibblock">ISBN 978-3-031-56065-1.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-031-56066-8_25</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-56066-8_25" title="">https://doi.org/10.1007/978-3-031-56066-8_25</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El-Sayed et al. [2024]</span>
<span class="ltx_bibblock">
S. El-Sayed, C. Akbulut, A. McCroskery, G. Keeling, Z. Kenton, Z. Jalan, N. Marchal, A. Manzini, T. Shevlane, S. Vallor, et al.

</span>
<span class="ltx_bibblock">A mechanism-based approach to mitigating harms from persuasive generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2404.15058</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elish [2019]</span>
<span class="ltx_bibblock">
M. C. Elish.

</span>
<span class="ltx_bibblock">Moral crumple zones: Cautionary tales in human-robot interaction (pre-print).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Engaging Science, Technology, and Society (pre-print)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferrara [2023]</span>
<span class="ltx_bibblock">
E. Ferrara.

</span>
<span class="ltx_bibblock">Should chatgpt be biased? challenges and risks of bias in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2304.03738</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">for Disease Control et al. [2022]</span>
<span class="ltx_bibblock">
C. for Disease Control, Prevention, et al.

</span>
<span class="ltx_bibblock">Cdc museum covid-19 timeline. 2022, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fowler [2024]</span>
<span class="ltx_bibblock">
J. A. Fowler.

</span>
<span class="ltx_bibblock">Turbotax and h&amp;r block now use ai for tax advice. it’s awful.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">The Washington Post</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.washingtonpost.com/technology/2024/03/04/ai-taxes-turbotax-hrblock-chatbot/" title="">https://www.washingtonpost.com/technology/2024/03/04/ai-taxes-turbotax-hrblock-chatbot/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabriel [2020]</span>
<span class="ltx_bibblock">
I. Gabriel.

</span>
<span class="ltx_bibblock">Artificial intelligence, values, and alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Minds and machines</em>, 30(3):411–437, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gabriel and Ghazavi [2021]</span>
<span class="ltx_bibblock">
I. Gabriel and V. Ghazavi.

</span>
<span class="ltx_bibblock">The challenge of value alignment: From fairer algorithms to ai safety.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2101.06060</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galaz et al. [2023]</span>
<span class="ltx_bibblock">
V. Galaz, H. Metzler, S. Daume, A. Olsson, B. Lindström, and A. Marklund.

</span>
<span class="ltx_bibblock">Ai could create a perfect storm of climate misinformation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2306.12807</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et al. [2022]</span>
<span class="ltx_bibblock">
D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K. Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gausen et al. [2024]</span>
<span class="ltx_bibblock">
A. Gausen, B. Mitra, and S. Lindley.

</span>
<span class="ltx_bibblock">A framework for exploring the consequences of ai-mediated enterprise knowledge access and identifying risks to workers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 2024 ACM conference on fairness, accountability, and transparency</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru [2022]</span>
<span class="ltx_bibblock">
T. Gebru.

</span>
<span class="ltx_bibblock">Effective altruism is pushing a dangerous brand of ‘ai safety’, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru and Torres [2023]</span>
<span class="ltx_bibblock">
T. Gebru and É. P. Torres.

</span>
<span class="ltx_bibblock">Eugenics and the promise of utopia through artificial general intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">First Monday</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gebru et al. [2023]</span>
<span class="ltx_bibblock">
T. Gebru, E. M. Bender, A. McMillan-Major, and M. Mitchell.

</span>
<span class="ltx_bibblock">Statement from the listed authors of stochastic parrots on the “ai pause” letter, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dair-institute.org/blog/letter-statement-March2023/" title="">https://www.dair-institute.org/blog/letter-statement-March2023/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehman et al. [2020]</span>
<span class="ltx_bibblock">
S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith.

</span>
<span class="ltx_bibblock">Realtoxicityprompts: Evaluating neural toxic degeneration in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2009.11462</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gershgorn [2021]</span>
<span class="ltx_bibblock">
D. Gershgorn.

</span>
<span class="ltx_bibblock">Github’s automatic coding tool rests on untested legal ground.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">The Verge</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gertner [2023]</span>
<span class="ltx_bibblock">
J. Gertner.

</span>
<span class="ltx_bibblock">Wikipedia’s moment of truth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">The New York Times Magazine</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gillespie [2018]</span>
<span class="ltx_bibblock">
T. Gillespie.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media</em>.

</span>
<span class="ltx_bibblock">Yale University Press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein [2020]</span>
<span class="ltx_bibblock">
S. Goldstein.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Informed Societies</em>.

</span>
<span class="ltx_bibblock">facet publishing, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonen and Goldberg [2019]</span>
<span class="ltx_bibblock">
H. Gonen and Y. Goldberg.

</span>
<span class="ltx_bibblock">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proc. NAACL</em>, pages 609–614, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">González [2021]</span>
<span class="ltx_bibblock">
M. González.

</span>
<span class="ltx_bibblock">A better-informed society is a freer society.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unesco.org/en/articles/better-informed-society-freer-society" title="">https://www.unesco.org/en/articles/better-informed-society-freer-society</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et al. [2023]</span>
<span class="ltx_bibblock">
A. D. Gordon, C. Negreanu, J. Cambronero, R. Chakravarthy, I. Drosos, H. Fang, B. Mitra, H. Richardson, A. Sarkar, S. Simmons, et al.

</span>
<span class="ltx_bibblock">Co-audit: tools to help humans double-check ai-generated content.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2310.01297</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon [2024]</span>
<span class="ltx_bibblock">
C. Gordon.

</span>
<span class="ltx_bibblock">Ai is accelerating the loss of our scarcest natural resource: Water.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Forbes</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greshake et al. [2023]</span>
<span class="ltx_bibblock">
K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz.

</span>
<span class="ltx_bibblock">Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security</em>, pages 79–90, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guerrini [2023]</span>
<span class="ltx_bibblock">
F. Guerrini.

</span>
<span class="ltx_bibblock">Ai’s unsustainable water use: How tech giants contribute to global water shortages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Forbes</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha et al. [2023]</span>
<span class="ltx_bibblock">
N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana, A. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, D. Zambrano, D. Talisman, E. Hoque, F. Surani, F. Fagan, G. Sarfaty, G. M. Dickinson, H. Porat, J. Hegland, J. Wu, J. Nudell, J. Niklaus, J. Nay, J. H. Choi, K. Tobia, M. Hagan, M. Ma, M. Livermore, N. Rasumov-Rahe, N. Holzenberger, N. Kolt, P. Henderson, S. Rehaag, S. Goel, S. Gao, S. Williams, S. Gandhi, T. Zur, V. Iyer, and Z. Li.

</span>
<span class="ltx_bibblock">Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. [2024]</span>
<span class="ltx_bibblock">
J. Gupta, H. Bosch, and L. v. Vliet.

</span>
<span class="ltx_bibblock">Ai’s excessive water consumption threatens to drown out its environmental contributions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">The Conversation</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halper [2024]</span>
<span class="ltx_bibblock">
E. Halper.

</span>
<span class="ltx_bibblock">Amid explosive demand, america is running out of power.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">The Washington Post</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao [2022]</span>
<span class="ltx_bibblock">
K. Hao.

</span>
<span class="ltx_bibblock">Artificial intelligence is creating a new colonial world order.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">MIT Technology Review</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao [2024]</span>
<span class="ltx_bibblock">
K. Hao.

</span>
<span class="ltx_bibblock">Ai is taking water from the desert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">The Atlantic</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao and Hernández [2022]</span>
<span class="ltx_bibblock">
K. Hao and A. P. Hernández.

</span>
<span class="ltx_bibblock">How the ai industry profits from catastrophe.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">MIT Technology Review</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao and Seetharaman [2023]</span>
<span class="ltx_bibblock">
K. Hao and D. Seetharaman.

</span>
<span class="ltx_bibblock">Cleaning up chatgpt takes heavy toll on human workers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">The Wall Street Journal</em>, 24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardin [2018]</span>
<span class="ltx_bibblock">
G. Hardin.

</span>
<span class="ltx_bibblock">The tragedy of the commons.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Classic Papers in Natural Resource Economics Revisited</em>, pages 145–156. Routledge, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hays and Barr [2024]</span>
<span class="ltx_bibblock">
K. Hays and A. Barr.

</span>
<span class="ltx_bibblock">Ai is killing the grand bargain at the heart of the web. ’we’re in a different world.’.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Business Insider</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heikkila [2023]</span>
<span class="ltx_bibblock">
M. Heikkila.

</span>
<span class="ltx_bibblock">This new data poisoning tool lets artists fight back against generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">MIT Technology Review</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hern [2024]</span>
<span class="ltx_bibblock">
A. Hern.

</span>
<span class="ltx_bibblock">Techscape: How cheap, outsourced labour in africa is shaping ai english, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt" title="">https://www.theguardian.com/technology/2024/apr/16/techscape-ai-gadgest-humane-ai-pin-chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Higgins and Gregory [2013]</span>
<span class="ltx_bibblock">
S. Higgins and L. Gregory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Information literacy and social justice: Radical professional praxis</em>.

</span>
<span class="ltx_bibblock">Library Juice Press, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoel [2024a]</span>
<span class="ltx_bibblock">
E. Hoel.

</span>
<span class="ltx_bibblock">Here lies the internet, murdered by generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">The Intrinsic Perspective</em>, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theintrinsicperspective.com/p/here-lies-the-internet-murdered-by" title="">https://www.theintrinsicperspective.com/p/here-lies-the-internet-murdered-by</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoel [2024b]</span>
<span class="ltx_bibblock">
E. Hoel.

</span>
<span class="ltx_bibblock">A.i.-generated garbage is polluting our culture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">The New York Times</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofmann et al. [2016a]</span>
<span class="ltx_bibblock">
K. Hofmann, L. Li, and F. Radlinski.

</span>
<span class="ltx_bibblock">Online evaluation for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Found. Trends Inf. Retr.</em>, 10(1):1–117, jun 2016a.

</span>
<span class="ltx_bibblock">ISSN 1554-0669.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1561/1500000051</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1561/1500000051" title="">https://doi.org/10.1561/1500000051</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofmann et al. [2016b]</span>
<span class="ltx_bibblock">
K. Hofmann, L. Li, and F. Radlinski.

</span>
<span class="ltx_bibblock">Online evaluation for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">Foundations and Trends® in Information Retrieval</em>, 10(1):1–117, 2016b.

</span>
<span class="ltx_bibblock">ISSN 1554-0669.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1561/1500000051</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1561/1500000051" title="">http://dx.doi.org/10.1561/1500000051</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hom [2015]</span>
<span class="ltx_bibblock">
K.-l. Hom.

</span>
<span class="ltx_bibblock">Rage baiting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Westside Seattle</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IPCC et al. [2013]</span>
<span class="ltx_bibblock">
C. C. IPCC et al.

</span>
<span class="ltx_bibblock">The physical science basis, the working group i contribution to the un ipcc’s fifth assessment report (wg1 ar5), 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jasanoff and Kim [2015]</span>
<span class="ltx_bibblock">
S. Jasanoff and S.-H. Kim.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Dreamscapes of modernity: Sociotechnical imaginaries and the fabrication of power</em>.

</span>
<span class="ltx_bibblock">University of Chicago Press, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2021]</span>
<span class="ltx_bibblock">
D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Applied Sciences</em>, 11(14), 2021.

</span>
<span class="ltx_bibblock">ISSN 2076-3417.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/app11146421</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2076-3417/11/14/6421" title="">https://www.mdpi.com/2076-3417/11/14/6421</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jobin et al. [2019]</span>
<span class="ltx_bibblock">
A. Jobin, M. Ienca, and E. Vayena.

</span>
<span class="ltx_bibblock">The global landscape of ai ethics guidelines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">Nature machine intelligence</em>, 1(9):389–399, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kak and West [2023]</span>
<span class="ltx_bibblock">
A. Kak and S. M. West.

</span>
<span class="ltx_bibblock">Ai now 2023 landscape: Confronting tech power, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ainowinstitute.org/2023-landscape" title="">https://ainowinstitute.org/2023-landscape</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalluri et al. [2020]</span>
<span class="ltx_bibblock">
P. Kalluri et al.

</span>
<span class="ltx_bibblock">Don’t ask if artificial intelligence is good or fair, ask how it shifts power.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Nature</em>, 583(7815):169–169, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanungo [2023]</span>
<span class="ltx_bibblock">
A. Kanungo.

</span>
<span class="ltx_bibblock">The green dilemma: Can ai fulfil its potential without harming the environment?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">Earth.Org</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kapoor et al. [2024]</span>
<span class="ltx_bibblock">
S. Kapoor, R. Bommasani, K. Klyman, S. Longpre, A. Ramaswami, P. Cihon, A. Hopkins, K. Bankston, S. Biderman, M. Bogen, et al.

</span>
<span class="ltx_bibblock">On the societal impact of open foundation models.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasirzadeh and Gabriel [2023]</span>
<span class="ltx_bibblock">
A. Kasirzadeh and I. Gabriel.

</span>
<span class="ltx_bibblock">In conversation with artificial intelligence: aligning language models with human values.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Philosophy &amp; Technology</em>, 36(2):1–24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanal et al. [2024]</span>
<span class="ltx_bibblock">
S. Khanal, H. Zhang, and A. Taeihagh.

</span>
<span class="ltx_bibblock">Why and how is the power of big tech increasing in the policy process? the case of generative ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Policy and Society</em>, page puae012, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattak [2023]</span>
<span class="ltx_bibblock">
R. Khattak.

</span>
<span class="ltx_bibblock">The environmental impact of e-waste.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Earth.Org</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khlaaf [2023]</span>
<span class="ltx_bibblock">
H. Khlaaf.

</span>
<span class="ltx_bibblock">Toward comprehensive risk assessments and assurance of ai-based systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">Trail of Bits</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein [2023]</span>
<span class="ltx_bibblock">
N. Klein.

</span>
<span class="ltx_bibblock">Ai machines aren’t ‘hallucinating’. but their makers are.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">The Guardian</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knibbs [2024a]</span>
<span class="ltx_bibblock">
K. Knibbs.

</span>
<span class="ltx_bibblock">Scammy ai-generated book rewrites are flooding amazon.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">Wired</em>, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/" title="">https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knibbs [2024b]</span>
<span class="ltx_bibblock">
K. Knibbs.

</span>
<span class="ltx_bibblock">Your kid may already be watching ai-generated videos on youtube.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Wired</em>, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wired.com/story/your-kid-may-be-watching-ai-generated-videos-on-youtube/" title="">https://www.wired.com/story/your-kid-may-be-watching-ai-generated-videos-on-youtube/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kosinski et al. [2013]</span>
<span class="ltx_bibblock">
M. Kosinski, D. Stillwell, and T. Graepel.

</span>
<span class="ltx_bibblock">Private traits and attributes are predictable from digital records of human behavior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Proceedings of the national academy of sciences</em>, 110(15):5802–5805, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotek et al. [2023]</span>
<span class="ltx_bibblock">
H. Kotek, R. Dockum, and D. Sun.

</span>
<span class="ltx_bibblock">Gender bias and stereotypes in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">Proceedings of The ACM Collective Intelligence Conference</em>, pages 12–24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kugel and Hiltner [2023]</span>
<span class="ltx_bibblock">
S. Kugel and S. Hiltner.

</span>
<span class="ltx_bibblock">A new frontier for travel scammers: A.i.-generated guidebooks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">The New York Times</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nytimes.com/2023/08/05/travel/amazon-guidebooks-artificial-intelligence.html" title="">https://www.nytimes.com/2023/08/05/travel/amazon-guidebooks-artificial-intelligence.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LaFrance [2024]</span>
<span class="ltx_bibblock">
A. LaFrance.

</span>
<span class="ltx_bibblock">The rise of techno-authoritarianism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">The Atlantic</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landymore [2023]</span>
<span class="ltx_bibblock">
F. Landymore.

</span>
<span class="ltx_bibblock">Sports illustrated lays off journalists after announcing pivot to ai content.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Futurism</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://futurism.com/the-byte/sports-illustrated-lays-off-journalists-ai-content" title="">https://futurism.com/the-byte/sports-illustrated-lays-off-journalists-ai-content</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
H. Li, N. Vincent, J. Tsai, J. Kaye, and B. Hecht.

</span>
<span class="ltx_bibblock">How do people change their technology use in protest? understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of the ACM on Human-Computer Interaction</em>, 3(CSCW):1–22, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
H. Li, N. Vincent, S. Chancellor, and B. Hecht.

</span>
<span class="ltx_bibblock">The dimensions of data labor: A road map for researchers, activists, and policymakers to empower data producers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 1151–1161, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
P. Li, J. Yang, M. A. Islam, and S. Ren.

</span>
<span class="ltx_bibblock">Making ai less" thirsty": Uncovering and addressing the secret water footprint of ai models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:2304.03271</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2023a]</span>
<span class="ltx_bibblock">
C. Liang, X. Wu, Y. Hua, J. Zhang, Y. Xue, T. Song, Z. Xue, R. Ma, and H. Guan.

</span>
<span class="ltx_bibblock">Adversarial example does good: Preventing painting imitation from diffusion models via adversarial examples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2302.04578</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2023b]</span>
<span class="ltx_bibblock">
P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Ré, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2024]</span>
<span class="ltx_bibblock">
W. Liang, Z. Izzo, Y. Zhang, H. Lepp, H. Cao, X. Zhao, L. Chen, H. Ye, S. Liu, Z. Huang, et al.

</span>
<span class="ltx_bibblock">Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2403.07183</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liesenfeld and Dingemanse [2024]</span>
<span class="ltx_bibblock">
A. Liesenfeld and M. Dingemanse.

</span>
<span class="ltx_bibblock">Rethinking open source generative ai: open washing and the eu ai act.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">The 2024 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 1774–1787, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Limbong [2024]</span>
<span class="ltx_bibblock">
A. Limbong.

</span>
<span class="ltx_bibblock">Authors push back on the growing number of ai ’scam’ books on amazon.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">National Public Radio</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.npr.org/2024/03/13/1237888126/growing-number-ai-scam-books-amazon" title="">https://www.npr.org/2024/03/13/1237888126/growing-number-ai-scam-books-amazon</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu [2023]</span>
<span class="ltx_bibblock">
L. Liu.

</span>
<span class="ltx_bibblock">Letter: Setting rules for ai must avoid regulatory capture by big tech.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">Financial Times</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024]</span>
<span class="ltx_bibblock">
X. Liu, Z. Yu, Y. Zhang, N. Zhang, and C. Xiao.

</span>
<span class="ltx_bibblock">Automatic and universal prompt injection attacks against large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">arXiv preprint arXiv:2403.04957</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu.

</span>
<span class="ltx_bibblock">Prompt injection attack against llm-integrated applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2306.05499</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lonergan [1941]</span>
<span class="ltx_bibblock">
R. Lonergan.

</span>
<span class="ltx_bibblock">Mr. justice brandeis, great american.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">Mr. Justice Brandeis, Great American: Press Opinion and Public Appraisal</em>, 1941.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Markov et al. [2023]</span>
<span class="ltx_bibblock">
T. Markov, C. Zhang, S. Agarwal, F. Eloundou Nekoul, T. Lee, S. Adler, A. Jiang, and L. Weng.

</span>
<span class="ltx_bibblock">A holistic approach to undesired content detection in the real world.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 37(12):15009–15018, Jun. 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1609/aaai.v37i12.26752</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/26752" title="">https://ojs.aaai.org/index.php/AAAI/article/view/26752</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marr [2023]</span>
<span class="ltx_bibblock">
B. Marr.

</span>
<span class="ltx_bibblock">Is generative ai stealing from artists?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Forbes</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martínez et al. [2023]</span>
<span class="ltx_bibblock">
G. Martínez, L. Watson, P. Reviriego, J. A. Hernández, M. Juarez, and R. Sarkar.

</span>
<span class="ltx_bibblock">Towards understanding the interplay of generative artificial intelligence and the internet.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2306.06130</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathewson [2023]</span>
<span class="ltx_bibblock">
T. Mathewson.

</span>
<span class="ltx_bibblock">Ai detection tools falsely accuse international students of cheating.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">The Markup</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahon et al. [2017]</span>
<span class="ltx_bibblock">
C. McMahon, I. Johnson, and B. Hecht.

</span>
<span class="ltx_bibblock">The substantial interdependence of wikipedia and google: A case study on the relationship between peer production communities and information technologies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Proceedings of the International AAAI Conference on Web and Social Media</em>, volume 11, pages 142–151, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehdi [2024]</span>
<span class="ltx_bibblock">
Y. Mehdi.

</span>
<span class="ltx_bibblock">Bringing the full power of copilot to more people and businesses, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.microsoft.com/blog/2024/01/15/bringing-the-full-power-of-copilot-to-more-people-and-businesses/" title="">https://blogs.microsoft.com/blog/2024/01/15/bringing-the-full-power-of-copilot-to-more-people-and-businesses/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Metzler et al. [2021]</span>
<span class="ltx_bibblock">
D. Metzler, Y. Tay, D. Bahri, and M. Najork.

</span>
<span class="ltx_bibblock">Rethinking search: making domain experts out of dilettantes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">Acm sigir forum</em>, volume 55, pages 1–27. ACM New York, NY, USA, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miceli et al. [2022]</span>
<span class="ltx_bibblock">
M. Miceli, J. Posada, and T. Yang.

</span>
<span class="ltx_bibblock">Studying up machine learning data: Why talk about bias when we mean power?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">Proceedings of the ACM on Human-Computer Interaction</em>, 6(GROUP):1–14, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. [2023]</span>
<span class="ltx_bibblock">
S. Min, S. Gururangan, E. Wallace, H. Hajishirzi, N. A. Smith, and L. Zettlemoyer.

</span>
<span class="ltx_bibblock">Silo language models: Isolating legal risk in a nonparametric datastore.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">arXiv preprint arXiv:2308.04430</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
P. Mishkin, L. Ahmad, M. Brundage, G. Krueger, and G. Sastry.

</span>
<span class="ltx_bibblock">Dalle-2-preview/system-card.md at main · openai/dalle-2-preview.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openai/dalle-2-preview/blob/main/system-card.md" title="">https://github.com/openai/dalle-2-preview/blob/main/system-card.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra [2024]</span>
<span class="ltx_bibblock">
B. Mitra.

</span>
<span class="ltx_bibblock">Search and society: Reimagining information access for radical futures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2403.17901</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Möhlmann [2021]</span>
<span class="ltx_bibblock">
M. Möhlmann.

</span>
<span class="ltx_bibblock">Algorithmic nudges don’t have to be unethical.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Harvard Business Review</em>, 22, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moretti et al. [2012]</span>
<span class="ltx_bibblock">
F. A. Moretti, V. E. d. Oliveira, and E. M. K. d. Silva.

</span>
<span class="ltx_bibblock">Access to health information on the internet: a public health issue?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">Revista da Associação Médica Brasileira</em>, 58:650–658, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mularczyk [2023]</span>
<span class="ltx_bibblock">
K. Mularczyk.

</span>
<span class="ltx_bibblock">Row over deepfake of polish pm in opposition-party broadcast.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">Brussels Signal</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://brusselssignal.eu/2023/08/row-over-deepfake-of-polish-pm-in-opposition-party-broadcast/" title="">https://brusselssignal.eu/2023/08/row-over-deepfake-of-polish-pm-in-opposition-party-broadcast/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muldoon and Wu [2023]</span>
<span class="ltx_bibblock">
J. Muldoon and B. A. Wu.

</span>
<span class="ltx_bibblock">Artificial intelligence in the colonial matrix of power.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Philosophy &amp; Technology</em>, 36(4):80, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murgia [2019]</span>
<span class="ltx_bibblock">
M. Murgia.

</span>
<span class="ltx_bibblock">Ai academics under pressure to do commercial research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">Financial Times</em>, 13, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mutula [2008]</span>
<span class="ltx_bibblock">
S. M. Mutula.

</span>
<span class="ltx_bibblock">Digital divide and economic development: Case study of sub-saharan africa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">The Electronic Library</em>, 26(4):468–489, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naughton [2024]</span>
<span class="ltx_bibblock">
J. Naughton.

</span>
<span class="ltx_bibblock">Ai’s craving for data is matched only by a runaway thirst for water and energy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">The Guardian</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Navigli et al. [2023]</span>
<span class="ltx_bibblock">
R. Navigli, S. Conia, and B. Ross.

</span>
<span class="ltx_bibblock">Biases in large language models: origins, inventory, and discussion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">ACM Journal of Data and Information Quality</em>, 15(2):1–21, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oremus [2023]</span>
<span class="ltx_bibblock">
W. Oremus.

</span>
<span class="ltx_bibblock">He wrote a book on a rare subject. then a chatgpt replica appeared on amazon.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">The Washington Post</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.washingtonpost.com/technology/2023/05/05/ai-spam-websites-books-chatgpt/" title="">https://www.washingtonpost.com/technology/2023/05/05/ai-spam-websites-books-chatgpt/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orland [2024]</span>
<span class="ltx_bibblock">
K. Orland.

</span>
<span class="ltx_bibblock">Lazy use of ai leads to amazon products called “i cannot fulfill that request”.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">Ars Technica</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/" title="">https://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Gorman [2023]</span>
<span class="ltx_bibblock">
M. O’Gorman.

</span>
<span class="ltx_bibblock">At the heart of artificial intelligence is racism and colonialism that we must excise.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">The Globe and Mail web edition</em>, pages NA–NA, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2023]</span>
<span class="ltx_bibblock">
Y. Pan, L. Pan, W. Chen, P. Nakov, M.-Y. Kan, and W. Y. Wang.

</span>
<span class="ltx_bibblock">On the risk of misinformation pollution with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">arXiv preprint arXiv:2305.13661</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2015]</span>
<span class="ltx_bibblock">
G. Park, H. A. Schwartz, J. C. Eichstaedt, M. L. Kern, M. Kosinski, D. J. Stillwell, L. H. Ungar, and M. E. Seligman.

</span>
<span class="ltx_bibblock">Automatic personality assessment through social media language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">Journal of personality and social psychology</em>, 108(6):934, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2023]</span>
<span class="ltx_bibblock">
P. S. Park, S. Goldstein, A. O’Gara, M. Chen, and D. Hendrycks.

</span>
<span class="ltx_bibblock">Ai deception: A survey of examples, risks, and potential solutions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">arXiv preprint arXiv:2308.14752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parmesan et al. [2022]</span>
<span class="ltx_bibblock">
C. Parmesan, M. D. Morecroft, and Y. Trisurat.

</span>
<span class="ltx_bibblock">Climate change 2022: Impacts, adaptation and vulnerability, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et al. [2021]</span>
<span class="ltx_bibblock">
D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et al. [2022]</span>
<span class="ltx_bibblock">
D. Patterson, J. Gonzalez, U. Hölzle, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. R. So, M. Texier, and J. Dean.

</span>
<span class="ltx_bibblock">The carbon footprint of machine learning training will plateau, then shrink.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">Computer</em>, 55(7):18–28, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pearson [2024]</span>
<span class="ltx_bibblock">
J. Pearson.

</span>
<span class="ltx_bibblock">Scientific journal publishes ai-generated rat with gigantic penis in worrying incident.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">Vice</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al. [2022]</span>
<span class="ltx_bibblock">
E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving.

</span>
<span class="ltx_bibblock">Red teaming language models with language models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perrigo [2023]</span>
<span class="ltx_bibblock">
B. Perrigo.

</span>
<span class="ltx_bibblock">Exclusive: Openai used kenyan workers on less than $2 per hour to make chatgpt less toxic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">Last accessed</em>, 19, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://time.com/6247678/openai-chatgpt-kenya-workers/" title="">https://time.com/6247678/openai-chatgpt-kenya-workers/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pierce [2023]</span>
<span class="ltx_bibblock">
D. Pierce.

</span>
<span class="ltx_bibblock">You can now use the dall-e 3 ai image generator inside bing chat.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">The Verge</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Png [2022]</span>
<span class="ltx_bibblock">
M.-T. Png.

</span>
<span class="ltx_bibblock">At the tensions of south and north: Critical roles of global south stakeholders in ai governance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 1434–1445, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polizzi [2020]</span>
<span class="ltx_bibblock">
G. Polizzi.

</span>
<span class="ltx_bibblock">Information literacy in the digital age: Why critical digital literacy matters for democracy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Informed Societies: Why information literacy matters for citizenship, participation and democracy</em>, pages 1–23, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poynting and Rivault [2024]</span>
<span class="ltx_bibblock">
M. Poynting and E. Rivault.

</span>
<span class="ltx_bibblock">2023 confirmed as world’s hottest year on record, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prabhakaran et al. [2022]</span>
<span class="ltx_bibblock">
V. Prabhakaran, M. Mitchell, T. Gebru, and I. Gabriel.

</span>
<span class="ltx_bibblock">A human rights-based approach to responsible ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2210.02667</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. [2023]</span>
<span class="ltx_bibblock">
X. Qi, Y. Zeng, T. Xie, P.-Y. Chen, R. Jia, P. Mittal, and P. Henderson.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">arXiv preprint arXiv:2310.03693</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quercia et al. [2011]</span>
<span class="ltx_bibblock">
D. Quercia, M. Kosinski, D. Stillwell, and J. Crowcroft.

</span>
<span class="ltx_bibblock">Our twitter profiles, our selves: Predicting personality with twitter.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing</em>, pages 180–185. IEEE, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren [2023]</span>
<span class="ltx_bibblock">
S. Ren.

</span>
<span class="ltx_bibblock">How much water does ai consume? the public deserves to know it.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robins-Early [2024]</span>
<span class="ltx_bibblock">
N. Robins-Early.

</span>
<span class="ltx_bibblock">New gpt-4o ai model is faster and free for all users, openai announces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">The Guardian</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell et al. [2015]</span>
<span class="ltx_bibblock">
S. Russell, D. Dewey, and M. Tegmark.

</span>
<span class="ltx_bibblock">Research priorities for robust and beneficial artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">AI magazine</em>, 36(4):105–114, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et al. [2024]</span>
<span class="ltx_bibblock">
J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadeghi and Arvanitis [2023]</span>
<span class="ltx_bibblock">
M. Sadeghi and L. Arvanitis.

</span>
<span class="ltx_bibblock">Rise of the newsbots: Ai-generated news websites proliferating online.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">NewsGuard</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sambasivan et al. [2021]</span>
<span class="ltx_bibblock">
N. Sambasivan, E. Arnesen, B. Hutchinson, T. Doshi, and V. Prabhakaran.

</span>
<span class="ltx_bibblock">Re-imagining algorithmic fairness in india and beyond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 315–328, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scells et al. [2022]</span>
<span class="ltx_bibblock">
H. Scells, S. Zhuang, and G. Zuccon.

</span>
<span class="ltx_bibblock">Reduce, reuse, recycle: Green information retrieval research.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 2825–2837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaake [2021]</span>
<span class="ltx_bibblock">
M. Schaake.

</span>
<span class="ltx_bibblock">Big tech calls for ‘regulation’ but is fuzzy on the details.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Financial Times</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scientist [2021]</span>
<span class="ltx_bibblock">
N. Scientist.

</span>
<span class="ltx_bibblock">Covid-19: the story of a pandemic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">New Scientist</em>, 10, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah and Bender [2022]</span>
<span class="ltx_bibblock">
C. Shah and E. M. Bender.

</span>
<span class="ltx_bibblock">Situating search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">Proceedings of the 2022 Conference on Human Information Interaction and Retrieval</em>, pages 221–232, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan et al. [2023a]</span>
<span class="ltx_bibblock">
S. Shan, J. Cryan, E. Wenger, H. Zheng, R. Hanocka, and B. Y. Zhao.

</span>
<span class="ltx_bibblock">Glaze: Protecting artists from style mimicry by <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib188.1.m1.1"><semantics id="bib.bib188.1.m1.1a"><mo id="bib.bib188.1.m1.1.1" stretchy="false" xref="bib.bib188.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib188.1.m1.1b"><ci id="bib.bib188.1.m1.1.1.cmml" xref="bib.bib188.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib188.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib188.1.m1.1d">{</annotation></semantics></math>Text-to-Image<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib188.2.m2.1"><semantics id="bib.bib188.2.m2.1a"><mo id="bib.bib188.2.m2.1.1" stretchy="false" xref="bib.bib188.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib188.2.m2.1b"><ci id="bib.bib188.2.m2.1.1.cmml" xref="bib.bib188.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib188.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib188.2.m2.1d">}</annotation></semantics></math> models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib188.3.1">32nd USENIX Security Symposium (USENIX Security 23)</em>, pages 2187–2204, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan et al. [2023b]</span>
<span class="ltx_bibblock">
S. Shan, W. Ding, J. Passananti, H. Zheng, and B. Y. Zhao.

</span>
<span class="ltx_bibblock">Prompt-specific poisoning attacks on text-to-image generative models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">arXiv preprint arXiv:2310.13828</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al. [2024]</span>
<span class="ltx_bibblock">
S. Shankar, J. D. Zamfirescu-Pereira, B. Hartmann, A. G. Parameswaran, and I. Arawjo.

</span>
<span class="ltx_bibblock">Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrivastava [2023]</span>
<span class="ltx_bibblock">
R. Shrivastava.

</span>
<span class="ltx_bibblock">Openai and microsoft sued by nonfiction writers for alleged ‘rampant theft’ of authors’ works.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">Forbes</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et al. [2023]</span>
<span class="ltx_bibblock">
I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson.

</span>
<span class="ltx_bibblock">The curse of recursion: Training on generated data makes models forget.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">arXiv preprint arXiv:2305.17493</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simchon et al. [2024]</span>
<span class="ltx_bibblock">
A. Simchon, M. Edwards, and S. Lewandowsky.

</span>
<span class="ltx_bibblock">The persuasive effects of political microtargeting in the age of generative artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">PNAS Nexus</em>, 3(2):pgae035, 01 2024.

</span>
<span class="ltx_bibblock">ISSN 2752-6542.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1093/pnasnexus/pgae035</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/pnasnexus/pgae035" title="">https://doi.org/10.1093/pnasnexus/pgae035</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith and Beattie [2022]</span>
<span class="ltx_bibblock">
J. J. Smith and L. Beattie.

</span>
<span class="ltx_bibblock">Recsys fairness metrics: Many to use but which one to choose?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">arXiv preprint arXiv:2209.04011</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. [2023]</span>
<span class="ltx_bibblock">
J. J. Smith, L. Beattie, and H. Cramer.

</span>
<span class="ltx_bibblock">Scoping fairness objectives and identifying fairness metrics for recommender systems: The practitioners’ perspective.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">Proceedings of the ACM Web Conference 2023</em>, WWW ’23, page 3648–3659, New York, NY, USA, 2023. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450394161.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3543507.3583204</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3543507.3583204" title="">https://doi.org/10.1145/3543507.3583204</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solaiman [2023]</span>
<span class="ltx_bibblock">
I. Solaiman.

</span>
<span class="ltx_bibblock">The gradient of generative ai release: Methods and considerations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">Proceedings of the 2023 ACM conference on fairness, accountability, and transparency</em>, pages 111–122, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Speare-Cole [2024]</span>
<span class="ltx_bibblock">
R. Speare-Cole.

</span>
<span class="ltx_bibblock">Generative ai could ‘supercharge’ climate disinformation, report warns.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">Independent</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. [2023]</span>
<span class="ltx_bibblock">
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Slone, A. Rahane, A. S. Iyer, A. Andreassen, A. Madotto, A. Santilli, A. Stuhlmüller, A. Dai, A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakaş, B. R. Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Özyurt, B. Hedayatnia, B. Neyshabur, B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. Howald, B. Orinion, C. Diao, C. Dour, C. Stinson, C. Argueta, C. F. Ramírez, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch, C. Waites, C. Voigt, C. D. Manning, C. Potts, C. Ramirez, C. E. Rivera, C. Siro, C. Raffel, C. Ashcraft,
C. Garbacea, D. Sileo, D. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Freeman, D. Khashabi, D. Levy, D. M. González, D. Perszyk, D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard, D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hupkes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, D. Schrader, E. Shutova, E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. Donoway, E. Pavlick, E. Rodola, E. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak, E. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Martínez-Plumed, F. Happé, F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo, G. Mariani, G. Wang, G. Jaimovitch-López, G. Betz, G. Gur-Ari, H. Galijasevic, H. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. Shevlin, H. Schütze, H. Yakura, H. Zhang, H. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton, J. Lee, J. F. Fisac, J. B. Simon, J. Koppel,
J. Zheng, J. Zou, J. Kocoń, J. Thompson, J. Wingfield, J. Kaplan, J. Radom, J. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim, J. Taal, J. Engel, J. Alabi, J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U. Balis, J. Batchelder, J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Guerr, J. Jones, J. B. Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan, K. Ignatyeva, K. Markert, K. D. Dhole, K. Gimpel, K. Omondi, K. Mathewson, K. Chiafullo, K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. O. Colón, L. Metz, L. K. Şenel, M. Bosma, M. Sap, M. ter Hoeve, M. Farooqi, M. Faruqui, M. Mazeika, M. Baturan, M. Marelli, M. Maru, M. J. R. Quintana, M. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast, M. L. Leavitt, M. Hagen, M. Schubert, M. O. Baitemirova,
M. Arnaud, M. McElrath, M. A. Yee, M. Cohen, M. Gu, M. Ivanitskiy, M. Starritt, M. Strube, M. Swędrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain, M. Xu, M. Suzgun, M. Walker, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini, M. V. T, N. Peng, N. A. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts, N. Doiron, N. Martinez, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S. Iyer, N. Constant, N. Fiedel, N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. A. M. Casares, P. Doshi, P. Fung, P. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. Chang, P. Eckersley, P. M. Htut, P. Hwang, P. Miłkowski, P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade, R. E. Rudolph, R. Gabriel, R. Habacker, R. Risco, R. Millière, R. Garg, R. Barnes, R. A. Saurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. LeBras, R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang, S. Singh, S. M.
Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. R. Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey, S. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi, S. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, Shyamolima, Debnath, S. Shakeri, S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar, S. Dehaene, S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. T. Piantadosi, S. M. Shieber, S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. Ali, T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick, T. Kornev, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot, T. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. Ramasesh, V. U. Prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders, W. Zhang, W. Vossen, X. Ren, X. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh,
Y. Lakretz, Y. Song, Y. Bahri, Y. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J. Wang, Z. Wang, and Z. Wu.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahl and Eke [2024]</span>
<span class="ltx_bibblock">
B. C. Stahl and D. Eke.

</span>
<span class="ltx_bibblock">The ethics of chatgpt–exploring the ethical issues of an emerging technology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">International Journal of Information Management</em>, 74:102700, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanford [2024]</span>
<span class="ltx_bibblock">
Stanford.

</span>
<span class="ltx_bibblock">Stanford helm, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://crfm.stanford.edu/helm/" title="">https://crfm.stanford.edu/helm/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et al. [2019]</span>
<span class="ltx_bibblock">
E. Strubell, A. Ganesh, and A. McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in nlp.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">arXiv preprint arXiv:1906.02243</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suresh and Guttag [2021]</span>
<span class="ltx_bibblock">
H. Suresh and J. Guttag.

</span>
<span class="ltx_bibblock">A framework for understanding sources of harm throughout the machine learning life cycle.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization</em>, EAAMO ’21, New York, NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450385534.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3465416.3483305</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3465416.3483305" title="">https://doi.org/10.1145/3465416.3483305</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tacheva and Ramasubramanian [2023]</span>
<span class="ltx_bibblock">
J. Tacheva and S. Ramasubramanian.

</span>
<span class="ltx_bibblock">Ai empire: Unraveling the interlocking systems of oppression in generative ai’s global order.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">Big Data &amp; Society</em>, 10(2):20539517231219241, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamkin et al. [2021]</span>
<span class="ltx_bibblock">
A. Tamkin, M. Brundage, J. Clark, and D. Ganguli.

</span>
<span class="ltx_bibblock">Understanding the capabilities, limitations, and societal impact of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">arXiv preprint arXiv:2102.02503</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Cabato [2023]</span>
<span class="ltx_bibblock">
R. Tan and R. Cabato.

</span>
<span class="ltx_bibblock">Behind the ai boom, an army of overseas workers in’digital sweatshops’.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">The Washington Post</em>, pages NA–NA, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tani [2023]</span>
<span class="ltx_bibblock">
M. Tani.

</span>
<span class="ltx_bibblock">New york times drops out of ai coalition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">Semafor</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taraborelli [2015]</span>
<span class="ltx_bibblock">
D. Taraborelli.

</span>
<span class="ltx_bibblock">The sum of all human knowledge in the age of machines: a new research agenda for wikimedia.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">ICWSM-15 Workshop on Wikipedia</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarkowski [2023a]</span>
<span class="ltx_bibblock">
A. Tarkowski.

</span>
<span class="ltx_bibblock">How wikipedia can shape the future of ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">Open Future</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarkowski [2023b]</span>
<span class="ltx_bibblock">
A. Tarkowski.

</span>
<span class="ltx_bibblock">Stewarding the sum of all knowledge in the age of ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">Open Future</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor [2023]</span>
<span class="ltx_bibblock">
A. Taylor.

</span>
<span class="ltx_bibblock">A historic rise in global conflict deaths suggests a violent new era.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.washingtonpost.com/world/2023/06/29/conflict-war-deaths-global-peace-rise-casualty/" title="">https://www.washingtonpost.com/world/2023/06/29/conflict-war-deaths-global-peace-rise-casualty/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor [2022]</span>
<span class="ltx_bibblock">
L. Taylor.

</span>
<span class="ltx_bibblock">Covid-19: True global death toll from pandemic is almost 15 million, says who.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">BMJ: British Medical Journal (Online)</em>, 377:o1144, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomas et al. [2023]</span>
<span class="ltx_bibblock">
P. Thomas, S. Spielman, N. Craswell, and B. Mitra.

</span>
<span class="ltx_bibblock">Large language models can accurately predict searcher preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">arXiv preprint arXiv:2309.10621</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson et al. [2024]</span>
<span class="ltx_bibblock">
B. Thompson, M. P. Dhaliwal, P. Frisch, T. Domhan, and M. Federico.

</span>
<span class="ltx_bibblock">A shocking amount of the web is machine translated: Insights from multi-way parallelism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">arXiv preprint arXiv:2401.05749</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
United Nations Meetings Coverage and Press Releases.

</span>
<span class="ltx_bibblock">With highest number of violent conflicts since second world war, united nations must rethink efforts to achieve, sustain peace, speakers tell security council.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://press.un.org/en/2023/sc15184.doc.htm" title="">https://press.un.org/en/2023/sc15184.doc.htm</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Urman and Makhortykh [2023]</span>
<span class="ltx_bibblock">
A. Urman and M. Makhortykh.

</span>
<span class="ltx_bibblock">The silence of the llms: Cross-lingual analysis of political bias and false information prevalence in chatgpt, google bard, and bing chat.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varghese [2021]</span>
<span class="ltx_bibblock">
S. Varghese.

</span>
<span class="ltx_bibblock">How a google search could end up endangering a life.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">iTWire</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vartziotis et al. [2024]</span>
<span class="ltx_bibblock">
T. Vartziotis, I. Dellatolas, G. Dasoulas, M. Schmidt, F. Schneider, T. Hoffmann, S. Kotsopoulos, and M. Keckeisen.

</span>
<span class="ltx_bibblock">Learn to code sustainably: An empirical study on llm-based green code generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2017]</span>
<span class="ltx_bibblock">
J. Vincent.

</span>
<span class="ltx_bibblock">The invention of ai ‘gaydar’could be the start of something much worse.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">The Verge</em>, 21, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2022a]</span>
<span class="ltx_bibblock">
J. Vincent.

</span>
<span class="ltx_bibblock">The lawsuit that could rewrite the rules of ai copyright.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">The Verge</em>, 22, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2022b]</span>
<span class="ltx_bibblock">
J. Vincent.

</span>
<span class="ltx_bibblock">Shutterstock will start selling ai-generated stock imagery with help from openai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">The Verge</em>, 25, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2023a]</span>
<span class="ltx_bibblock">
J. Vincent.

</span>
<span class="ltx_bibblock">Ai art tools stable diffusion and midjourney targeted with copyright lawsuit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">The Verge</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2020]</span>
<span class="ltx_bibblock">
N. Vincent.

</span>
<span class="ltx_bibblock">Don’t give openai all the credit for gpt-3: You might have helped create the latest “astonishing” advance in ai too, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.psagroup.org/blogposts/62" title="">https://www.psagroup.org/blogposts/62</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent [2023b]</span>
<span class="ltx_bibblock">
N. Vincent.

</span>
<span class="ltx_bibblock">The wga strike is a canary in the coal mine for ai labor concerns, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dataleverage.substack.com/p/the-wga-strike-is-a-canary-in-the" title="">https://dataleverage.substack.com/p/the-wga-strike-is-a-canary-in-the</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent and Li [2021]</span>
<span class="ltx_bibblock">
N. Vincent and H. Li.

</span>
<span class="ltx_bibblock">Github copilot and the exploitation of “data labor”: A wake-up call for the tech industry, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.psagroup.org/blogposts/62" title="">https://www.psagroup.org/blogposts/62</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent and Li [2023]</span>
<span class="ltx_bibblock">
N. Vincent and H. Li.

</span>
<span class="ltx_bibblock">Chatgpt stole your work. so what are you going to do?, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et al. [2018]</span>
<span class="ltx_bibblock">
N. Vincent, I. Johnson, and B. Hecht.

</span>
<span class="ltx_bibblock">Examining wikipedia with a broader lens: Quantifying the value of wikipedia’s relationships with other large-scale online communities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib226.1.1">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>, pages 1–13, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et al. [2019a]</span>
<span class="ltx_bibblock">
N. Vincent, B. Hecht, and S. Sen.

</span>
<span class="ltx_bibblock">“data strikes”: evaluating the effectiveness of a new form of collective action against technology companies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">The World Wide Web Conference</em>, pages 1931–1943, 2019a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et al. [2019b]</span>
<span class="ltx_bibblock">
N. Vincent, Y. Li, R. Zha, and B. Hecht.

</span>
<span class="ltx_bibblock">Mapping the potential and pitfalls of" data dividends" as a means of sharing the profits of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">arXiv preprint arXiv:1912.00757</em>, 2019b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et al. [2021]</span>
<span class="ltx_bibblock">
N. Vincent, H. Li, N. Tilly, S. Chancellor, and B. Hecht.

</span>
<span class="ltx_bibblock">Data leverage: A framework for empowering the public in its relationship with technology companies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 215–227, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Violino [2023]</span>
<span class="ltx_bibblock">
B. Violino.

</span>
<span class="ltx_bibblock">Ai tools such as chatgpt are generating a mammoth increase in malicious phishing emails.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib230.1.1">CNBC</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cnbc.com/2023/11/28/ai-like-chatgpt-is-creating-huge-increase-in-malicious-phishing-email.html" title="">https://www.cnbc.com/2023/11/28/ai-like-chatgpt-is-creating-huge-increase-in-malicious-phishing-email.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024]</span>
<span class="ltx_bibblock">
B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, S. T. Truong, S. Arora, M. Mazeika, D. Hendrycks, Z. Lin, Y. Cheng, S. Koyejo, D. Song, and B. Li.

</span>
<span class="ltx_bibblock">Decodingtrust: A comprehensive assessment of trustworthiness in gpt models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warren [2024]</span>
<span class="ltx_bibblock">
T. Warren.

</span>
<span class="ltx_bibblock">Microsoft’s new copilot pro brings ai-powered office features to the rest of us, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theverge.com/2024/1/15/24038711/microsoft-copilot-pro-office-ai-apps" title="">https://www.theverge.com/2024/1/15/24038711/microsoft-copilot-pro-office-ai-apps</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2024]</span>
<span class="ltx_bibblock">
J. Wei, C. Yang, X. Song, Y. Lu, N. Hu, J. Huang, D. Tran, D. Peng, R. Liu, D. Huang, C. Du, and Q. V. Le.

</span>
<span class="ltx_bibblock">Long-form factuality in large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et al. [2021]</span>
<span class="ltx_bibblock">
L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, et al.

</span>
<span class="ltx_bibblock">Ethical and social risks of harm from language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib234.1.1">arXiv preprint arXiv:2112.04359</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et al. [2022]</span>
<span class="ltx_bibblock">
L. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor, A. Glaese, M. Cheng, B. Balle, A. Kasirzadeh, et al.

</span>
<span class="ltx_bibblock">Taxonomy of risks posed by language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib235.1.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 214–229, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. [2021]</span>
<span class="ltx_bibblock">
J. Welbl, A. Glaese, J. Uesato, S. Dathathri, J. Mellor, L. A. Hendricks, K. Anderson, P. Kohli, B. Coppin, and P.-S. Huang.

</span>
<span class="ltx_bibblock">Challenges in detoxifying language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">arXiv preprint arXiv:2109.07445</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittaker [2021a]</span>
<span class="ltx_bibblock">
M. Whittaker.

</span>
<span class="ltx_bibblock">The steep cost of capture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib237.1.1">Interactions</em>, 28(6):50–55, nov 2021a.

</span>
<span class="ltx_bibblock">ISSN 1072-5520.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3488666</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3488666" title="">https://doi.org/10.1145/3488666</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Whittaker [2021b]</span>
<span class="ltx_bibblock">
M. Whittaker.

</span>
<span class="ltx_bibblock">The steep cost of capture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">Interactions</em>, 28(6):50–55, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al. [2022]</span>
<span class="ltx_bibblock">
A. Williams, M. Miceli, and T. Gebru.

</span>
<span class="ltx_bibblock">The exploited labor behind artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">Noema Magazine</em>, 13, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/" title="">https://www.noemamag.com/the-exploited-labor-behind-artificial-intelligence/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams [2023]</span>
<span class="ltx_bibblock">
R. Williams.

</span>
<span class="ltx_bibblock">Humans may be more likely to believe disinformation generated by ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib240.1.1">MIT Technology Review</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.technologyreview.com/2023/06/28/1075683/humans-may-be-more-likely-to-believe-disinformation-generated-by-ai/" title="">https://www.technologyreview.com/2023/06/28/1075683/humans-may-be-more-likely-to-believe-disinformation-generated-by-ai/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2022]</span>
<span class="ltx_bibblock">
C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang, F. Aga, J. Huang, C. Bai, et al.

</span>
<span class="ltx_bibblock">Sustainable AI: Environmental implications, challenges and opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">Proceedings of Machine Learning and Systems</em>, 4:795–813, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang [2023]</span>
<span class="ltx_bibblock">
C. Xiang.

</span>
<span class="ltx_bibblock">Openai used kenyan workers making $2 an hour to filter traumatic content from chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">VICE</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yom-Tov et al. [2014]</span>
<span class="ltx_bibblock">
E. Yom-Tov, S. Dumais, and Q. Guo.

</span>
<span class="ltx_bibblock">Promoting civil discourse through search engine diversity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">Social Science Computer Review</em>, 32(2):145–154, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Youyou et al. [2015]</span>
<span class="ltx_bibblock">
W. Youyou, M. Kosinski, and D. Stillwell.

</span>
<span class="ltx_bibblock">Computer-based personality judgments are more accurate than those made by humans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">Proceedings of the National Academy of Sciences</em>, 112(4):1036–1040, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu [2002]</span>
<span class="ltx_bibblock">
P. K. Yu.

</span>
<span class="ltx_bibblock">Bridging the digital divide: Equality in the information age.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">Cardozo Arts &amp; Ent. LJ</em>, 20:1, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al. [2019]</span>
<span class="ltx_bibblock">
D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.

</span>
<span class="ltx_bibblock">Fine-tuning language models from human preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib246.1.1">arXiv preprint arXiv:1909.08593</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuboff [2023]</span>
<span class="ltx_bibblock">
S. Zuboff.

</span>
<span class="ltx_bibblock">The age of surveillance capitalism.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">Social Theory Re-Wired</em>, pages 203–213. Routledge, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuboff et al. [2019]</span>
<span class="ltx_bibblock">
S. Zuboff, N. Möllers, D. M. Wood, and D. Lyon.

</span>
<span class="ltx_bibblock">Surveillance capitalism: an interview with shoshana zuboff.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">Surveillance &amp; Society</em>, 17(1/2):257–266, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuccon et al. [2023]</span>
<span class="ltx_bibblock">
G. Zuccon, H. Scells, and S. Zhuang.

</span>
<span class="ltx_bibblock">Beyond co2 emissions: The overlooked impact of water consumption of information retrieval models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval</em>, pages 283–289, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 16 15:46:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
