<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos</title>
<!--Generated on Wed Jul 31 15:41:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Eating Detection; Acoustic Sensing; Activity Recognition; Foundation Models" lang="en" name="keywords"/>
<base href="/html/2406.10750v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S1" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S2" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The System Design of EchoGuide</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS1" title="In 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Hardware Prototype</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS1.SSS1" title="In 3.1. Hardware Prototype ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Glasses with active acoustic Sensing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS1.SSS2" title="In 3.1. Hardware Prototype ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Head-mounted GoPro for egocentric video capture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS2" title="In 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Processing Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS2.SSS1" title="In 3.2. Data Processing Pipeline ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Using Active Acoustic Sensing to localize relevant actions and clip videos</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS2.SSS2" title="In 3.2. Data Processing Pipeline ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Generating activity records from video and active acoustic sensing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.SS2.SSS3" title="In 3.2. Data Processing Pipeline ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Answering questions given activity records</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S4" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>User Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluating quality of eating activity summaries and responses with LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.SS1" title="In 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.SS1.SSS1" title="In 5.1. Metrics ‣ 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Answer alignment with dense captioning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.SS1.SSS2" title="In 5.1. Metrics ‣ 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Recording reduction compared to dense sampling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.SS2" title="In 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation Procedure/Baseline Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.SS3" title="In 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Quantitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S6" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluating activity records’ ability to answer targeted eating questions with large image-language models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S6.SS1" title="In 6. Evaluating activity records’ ability to answer targeted eating questions with large image-language models ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Metrics/Evaluation Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S6.SS2" title="In 6. Evaluating activity records’ ability to answer targeted eating questions with large image-language models ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Quantitative Results and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S7" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S8" title="In EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vineet Parikh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vap43@cornell.edu">vap43@cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0000-8791-9340" title="ORCID identifier">0009-0000-8791-9340</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saif Mahmud
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sm2446@cornell.edu">sm2446@cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-5283-0765" title="ORCID identifier">0000-0002-5283-0765</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Devansh Agarwal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:da398@cornell.edu">da398@cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-1338-9275" title="ORCID identifier">0009-0005-1338-9275</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ke Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kl975@cornell.edu">kl975@cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-4208-7904" title="ORCID identifier">0000-0002-4208-7904</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">François Guimbretière
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:francois@cs.cornell.edu">francois@cs.cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-5510-6799" title="ORCID identifier">0000-0002-5510-6799</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cheng Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Cornell University</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Ithaca, NY</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chengzhang@cornell.edu">chengzhang@cornell.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-5079-5927" title="ORCID identifier">0000-0002-5079-5927</a></span>
</span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice of manually recording eating activities using paper records or smartphone apps is often unsustainable and inaccurate. Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors, but existing systems primarily identify when eating occurs without capturing details of the eating activities (E.g., what is being eaten). In this paper, we present EchoGuide, an application and system pipeline that leverages low-power active acoustic sensing to guide head-mounted cameras to capture egocentric videos, enabling efficient and detailed analysis of eating activities. By combining active acoustic sensing for eating detection with video captioning models and large-scale language models for retrieval augmentation, EchoGuide intelligently clips and analyzes videos to create concise, relevant activity records on eating. We evaluated EchoGuide with 9 participants in naturalistic settings involving eating activities, demonstrating high-quality summarization and significant reductions in video data needed, paving the way for practical, scalable eating activity tracking.</p>
</div>
<div class="ltx_keywords">Eating Detection; Acoustic Sensing; Activity Recognition; Foundation Models
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 2024
ACM International Symposium on Wearable Computers; October 5–9,
2024; Melbourne, VIC, Australia</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 2024 ACM International Symposium on
Wearable Computers (ISWC ’24), October 5–9, 2024, Melbourne, VIC, Australia</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3675095.3676611</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-1059-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing systems and tools</span></span></span>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\acmArticleType</span>
<p class="ltx_p" id="p1.2">Research</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice requires users to manually record their eating activities, including when and what they eat, using paper records or smartphone apps. This manual method is often unsustainable and sometimes inaccurate, as users frequently forget to record their activities.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors. To alleviate the need for manual recording, various sensing systems based on smart glasses have been developed to distinguish eating behavior from arm movements <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib42" title="">2022</a>)</cite>, ambient sound<cite class="ltx_cite ltx_citemacro_citep">(Thomaz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib32" title="">2015</a>)</cite> or facial muscle movements<cite class="ltx_cite ltx_citemacro_citep">(Shin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib29" title="">2022</a>)</cite>. However, most of these systems can only identify when eating occurs but not what is being eaten, which is critical information for interpreting eating behaviors. Conversely, sensing systems such as cameras, which can capture detailed information on eating (e.g., the type of food consumed), have high power consumption, making continuous operation impractical on commodity smart glasses.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we explore the research question:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Is it possible to use low-power active acoustic sensing on glasses to automatically guide the camera to capture activities, such as eating, in an energy-efficient manner without losing much critical information?</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Active acoustic sensing<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib34" title="">2018</a>)</cite> has been shown as a low-power and powerful sensing modality for tracking and interpret various types of fine-grained body poses on wearables, including facial expressions<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib19" title="">2022</a>)</cite>, gaze<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib18" title="">2024b</a>)</cite>, finger pose<cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib38" title="">2024</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib15" title="">2024</a>)</cite>, body pose<cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib22" title="">2023</a>)</cite>, tongue gesture<cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib30" title="">2023</a>)</cite>, silent speech recognition<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib41" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib40" title="">a</a>)</cite>, authentication<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib17" title="">2024a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib9" title="">2024b</a>)</cite> and physiological signal <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib8" title="">2024a</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib10" title="">2023</a>)</cite>. The latest work ActSonic<cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> has shown that using active-acoustic sensing on glasses can recognize over 28 types of everyday activities (including eating) in the wild with 89% F1 score at each second without the need for any training data from a new user. More specifically, it recognizes eating activities with an F1 score of 90% in completely unconstrained environments. However, this sensing modality doesn’t capture the full context of a given activity. For activity recording and downstream applications (such as calorie counting or recipe assistance), it’s important to understand not only <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">what action</span> (e.g., when eating happens) is performed given body motion, but also <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">what objects</span> (e.g., what is the food being eaten) the action is being performed with.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, we present the design and implementation of EchoGuide, an application and system pipeline that combines the strengths of active acoustic sensing for action detection, video captioning models for detailed egocentric action understanding, and large-scale language models with retrieval augmentation for conversational QA with action records, to enable efficient and seamless action recording and retrieval within specialized everyday domains such as eating.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">By leveraging efficient pre-trained models for action detection via active acoustic sensing from ActSonic<cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite>, EchoGuide can intelligently “clip” the videos to guide the camera and video models, creating an activity record that remains far shorter than naive dense-clip video captioning applications while additionally remaining far more relevant than inflexible sparse clipping methods.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">We evaluate the performance of EchoGuide with 9 participants wearing GoPros and ultrasonic sensors affixed to commodity eyeglasses to collect data about eating in the unconstrained environment of the participant’s choice. With customized acoustic data preprocessing, action detection, video captioning, and action retrieval QA pipeline, we efficiently build activity records with significant reductions in record length while maintaining high semantic correlation with densely captured records. We evaluate the system via a semi-in-the-wild user study with 9 participants focused on eating actions. Additionally, we discuss some of the challenges that EchoGuide must address to be deployed further at scale. We summarize the contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">To the best of our knowledge, we are the first to demonstrate the feasibility of leveraging active acoustic sensing on glass frames to guide the highly efficient capture and analysis of egocentric video for eating activity tracking.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We propose an end-to-end application pipeline enabling seamless and efficient action detection, video captioning, and action retrieval/QA leveraging a combination of active acoustic sensing and egocentric video.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">We evaluated the end-to-end pipeline on eating activities collected in naturalistic settings of 9 participants’ choices through a user-independent and session-independent study. Our system provided high-quality summarization (68% average reduction in activity records along with high alignment between reduced and original activity records given eating-focused videos) while significantly reducing the amount of video data needed.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Multimodal Image/Video Captioning and Summarization: </span> As larger-scale language and multimodal generative “foundation models” <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib6" title="">2021</a>)</cite> have been trained and released, image and video captioning has extended from simply determining the similarity of images/videos to a premade list of captions <cite class="ltx_cite ltx_citemacro_citep">(Wray et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib37" title="">2016</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib36" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib33" title="">2022</a>; Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib28" title="">2021</a>; Ni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib26" title="">2022</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib20" title="">2022</a>)</cite> towards generating captions for new videos based on either fine-tuning inexpensive smaller-scale Large Language Models with video encoders and captions <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib44" title="">2023</a>)</cite> or leveraging the emergent properties of natively multimodal Large Language models (such as the OpenAI GPT-4 multimodal series <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib27" title="">2024</a>; GPT, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib2" title="">[n. d.]</a>)</cite> and Google Gemini multimodal series <cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib31" title="">2023</a>)</cite>), truly “open-world” video captioning becomes more possible especially in a “zero-shot” paradigm without labeled examples. These systems can be applied offline throughout a video to create “activity records”: long documents which encode which activities a person might be completing within the course of a video, and which can be efficiently indexed and searched.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Extracting insights from preprocessed ”activity records” requires methods which can generate relevant answers to queries that are grounded in specific documents. Recent generative methods, especially in scenarios involving domain-specific information, leverage the Retrieval-Augmented Generation <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib16" title="">2020</a>)</cite> for returning helpful responses given queries and documents containing relevant information.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">However, the primary bottleneck when leveraging image/video captioning and summarization systems especially over longer videos is power and compute consumption: wearable cameras such as the GoPro HERO9 <cite class="ltx_cite ltx_citemacro_citep">(GoPro, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib12" title="">2020</a>)</cite> do not have sufficient battery life for continuous daily capture, and video-processing models which recognize activities and objects have high compute requirements.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Eating Recognition on Glasses: </span>
Various sensing modalities on eyeglasses form factors have been proposed to track eating events. These modalities include EMG electrodes <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Amft, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib39" title="">2017</a>)</cite>, piezoelectric sensing <cite class="ltx_cite ltx_citemacro_citep">(Shin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib29" title="">2022</a>; Farooq and Sazonov, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib11" title="">2016</a>)</cite>, contact microphones <cite class="ltx_cite ltx_citemacro_citep">(Bi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib5" title="">2018</a>)</cite>, microphones and IMUs <cite class="ltx_cite ltx_citemacro_citep">(Mirtchouk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib25" title="">2017</a>)</cite>, and sensor fusion <cite class="ltx_cite ltx_citemacro_citep">(Bedri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib3" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib4" title="">2022</a>)</cite>. While these systems track eating episodes, they lack the ability to provide detailed information related to eating activities, such as what food a person eats. This limitation is due to the absence of optimized access to an egocentric camera for extended monitoring periods.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The System Design of EchoGuide</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S3.F1.g1" src="extracted/5766746/paper/figures/SucreSystem.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Glasses and GoPro Hardware setup for EchoGuide</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we will present the design of EchoGuide including 1) the hardware prototype we used to collect egocentric acoustic and video data for eating activities; and 2) the software and deep learning pipeline we used to process the acoustic data for eating event segmentation and extract details of eating episodes from the segmented video clips.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Hardware Prototype</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Glasses with active acoustic Sensing</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We used a similar hardware prototype design of the glasses as ActSonic<cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.F1" title="Figure 1 ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. They include two OWR-05049T-38D<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bitfoic.com/detail/owr05049t38d-14578" title="">https://www.bitfoic.com/detail/owr05049t38d-14578</a></span></span></span> speakers for chirps and two ICS-43434<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://invensense.tdk.com/products/ics-43434/" title="">https://invensense.tdk.com/products/ics-43434/</a></span></span></span> microphones for receiving signals. The system uses a Teensy 4.1<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pjrc.com/store/teensy41.html" title="">https://www.pjrc.com/store/teensy41.html</a></span></span></span> microcontroller to store the transmitted signal and save the received signal on its SD card. Using a similar hardware prototype design will allow us to directly use ActSonic’s fully pre-trained deep learning model to identify eating moments in everyday activities without the need of any new training data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Head-mounted GoPro for egocentric video capture</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">:
To collect egocentric video of user’s activities, we used a head-mounted GoPro HERO-9<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gopro.com/en/us/news/hero9-black-announce" title="">https://gopro.com/en/us/news/hero9-black-announce</a></span></span></span>, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.F1" title="Figure 1 ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. The data was saved on the SD card within the GoPro.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Data Processing Pipeline</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="354" id="S3.F2.g1" src="extracted/5766746/paper/figures/SucrePipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span> EchoGuide vs Generic LLM Document Generation</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The EchoGuide Software and Deep Learning application follows a two-stage pipeline consisting of first processing synchronized egocentric videos and processed active acoustic data into “activity records” representing a history of user activities, and then building a question-answering framework leveraging large language models and Retrieval Augmented Generation for indexing, retrieving, and answering questions grounded in these “activity records”. This modular pipeline enables incremental improvement of individual components as they become more capable and is contrasted with a naive dense captioning pipeline which must process the entire video (as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S3.F2" title="Figure 2 ‣ 3.2. Data Processing Pipeline ‣ 3. The System Design of EchoGuide ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">2</span></a>), with natural language acting as an intermediate step between dense perceptual information and question-answering systems.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Using Active Acoustic Sensing to localize relevant actions and clip videos</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">We acquired the Resnet-18 model reported in ActSonic<cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> via contacting the authors. Our goal is to directly use the pretrained model in ActSonic to determine when an eating activity happens, leveraging the strong user-independent performance of ActSonic for detecting eating activities in in-the-wild settings <cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">We leverage ActSonic’s ResNet-18 model as an event detector by splitting the active acoustic differential echo profile (synchronized with the video) into consecutive 2-second windows which can be passed directly into the model. We define a set of “domain-specific classes” within the label space of ActSonic which capture important events for this particular domain, extract class predictions for all sliding windows (essentially treating the class prediction’s “timestamp” as the last timestamp of the corresponding window), and construct intervals of events by filtering for “domain-specific classes” and joining equivalent predictions in consecutive windows to create clips without requiring dense captions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Generating activity records from video and active acoustic sensing</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To generate activity records, we take a preprocessed and synchronized dataset containing egocentric videos and acoustic echo/differential profiles from user activities, and apply two modules: a <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p1.1.1">clipper</span> to each long untrimmed video/acoustic pair to convert the pair into a series of video clips with possible metadata, and a <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p1.1.2">captioner</span> which can take video clips and associated metadata (e.g. timestamp, acoustic classifier label, etc) and generate a “caption” for the clip in EGO-4D format (treating ”C” as the camera-wearer) <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib13" title="">2022</a>)</cite>, incorporating time metadata as well. We can then join the captions with timestamps to create an “activity record” for the given session. Within EchoGuide(), we primarily focus on proving out the combination of ActSonic as a “clipper” <cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> and LaViLa’s Narrator (a video-to-GPT2 model fine-tuned on EGO-4D <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib13" title="">2022</a>)</cite>, an egocentric vision dataset) as a “captioner” <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib44" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Answering questions given activity records</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">We leverage a Retrieval-Augmented Generation <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib16" title="">2020</a>)</cite> framework such as LlamaIndex<cite class="ltx_cite ltx_citemacro_citep">(Liu, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib21" title="">2022</a>)</cite>) for efficiently chunking and embedding a given series of documents (leveraging OpenAI’s “text-embedding-ada-002” embedding model) as well as input queries. Given an input query, we run a similarity search on the query embedding vs chunk embeddings (using cosine similarity) and pass the top “k” chunks into the context of a language model (in our case GPT3.5 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib7" title="">2020</a>)</cite>) to efficiently answer questions about the activity record via a chat/question-and-answer interface.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>User Study</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To collect data for evaluating EchoGuide, we conduct a semi-in-the-wild user study in various naturalistic locations (including participant homes and offices), focusing on capturing natural data of users eating while also performing other activities (such that only parts of each sequence relate to relevant actions). We leverage the activity set proposed in ActSonic <cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite>, which describes a wide collection of everyday activities.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Participants</span> The EchoGuide user study received approval from the Institutional Review Board for Human Participant Research (IRB) at our organization. We recruited 10 participants for a semi-in-the-wild user study at their homes. However, 1 participant’s data was lost during the user study. Therefore, we ended up with 9 valid participants in the study, ranging in age from 19 to 34. 6 participants self-reported as male while 3 self-reported as female. We collected basic demographic data and their ratings on the prototype through an IRB-approved questionnaire. The average comfort rating on a Likert scale of 0 to 5 was 2.62.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.4"><span class="ltx_text ltx_font_bold" id="S4.p3.4.1">Data Capture Apparatus</span> We captured acoustic data using the sensing system integrated into EchoGuide eyeglasses and recorded egocentric activity video data via the EchoGuide GoPro Hero9 <cite class="ltx_cite ltx_citemacro_citep">(GoPro, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib12" title="">2020</a>)</cite> camera mounted on the participants’ heads using a lightweight body mount from the same manufacturer. The camera’s horizontal and vertical field of view was set to <math alttext="118\degree" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">118</mn><mo id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p3.1.m1.1.1.3" mathvariant="normal" xref="S4.p3.1.m1.1.1.3.cmml">°</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn id="S4.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.p3.1.m1.1.1.2">118</cn><ci id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">118\degree</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">118 °</annotation></semantics></math> and <math alttext="69\degree" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mn id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">69</mn><mo id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.p3.2.m2.1.1.3" mathvariant="normal" xref="S4.p3.2.m2.1.1.3.cmml">°</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><cn id="S4.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.p3.2.m2.1.1.2">69</cn><ci id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">69\degree</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">69 °</annotation></semantics></math> respectively. It recorded egocentric videos at a resolution of <math alttext="720" class="ltx_Math" display="inline" id="S4.p3.3.m3.1"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn id="S4.p3.3.m3.1.1.cmml" type="integer" xref="S4.p3.3.m3.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">720</annotation><annotation encoding="application/x-llamapun" id="S4.p3.3.m3.1d">720</annotation></semantics></math>p and a frame rate of <math alttext="30" class="ltx_Math" display="inline" id="S4.p3.4.m4.1"><semantics id="S4.p3.4.m4.1a"><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><cn id="S4.p3.4.m4.1.1.cmml" type="integer" xref="S4.p3.4.m4.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">30</annotation><annotation encoding="application/x-llamapun" id="S4.p3.4.m4.1d">30</annotation></semantics></math> fps.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Study Procedure</span> We conducted a 9-participant semi-in-the-wild user study in unconstrained environments such as participants’ homes and offices. The recruited participants were equipped with eyeglasses and a head-mounted camera. We synchronized the acoustic and video data with a clap action performed by the researcher as the two sensors were physically separated. After synchronization, the participants could continue normal activities without interruption if they ate or drank at least one item within the 40-minute window. Upon completing the 40-minute study, the participants returned the devices to the researcher. We had 5 participants collect data at their homes and 4 in their office environments.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluating quality of eating activity summaries and responses with LLMs</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Metrics</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To measure the value of leveraging a supervised ultrasonic model to actively guide video captioners toward more efficient action captioning for retrieval, we define two primary metrics for evaluating system quality:</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Answer alignment with dense captioning</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Given a single question related to the domain, we find semantic similarity between the answer from a RAG QA agent that has indexed an activity record with alternative sampling (e.g. leveraging the ultrasonic modality to filter and caption fewer frames) and the answer from a RAG QA agent that has indexed an activity record with dense video sampling (e.g. captioning the entire video, which can reduce overall efficiency but captures all possible information). Semantic similarity is captured via BERT F-1 scores <cite class="ltx_cite ltx_citemacro_citep">(Zhang* et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib43" title="">2020</a>)</cite>, which captures pairwise cosine similarities (within the range -1 to 1) between BERT output embeddings to capture semantic and contextual information and which shows high correlation with human evaluations on summarization and captioning tasks (closely related to this work). Similarity scores are used to quantify information loss between the densely captioned model and the ActSonic-captioned model.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Recording reduction compared to dense sampling</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Different video clipping methods can lead to different “line counts” for an activity record (as each line of an activity record correlates to a clip in the video where a video captioner model was used). We can therefore find the size reduction between EchoGuide/ActSonic records (where clips are extracted using the ultrasonic modality) vs densely-captioned records (where clips are densely extracted at 1-second intervals).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Evaluation Procedure/Baseline Description</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We show per-participant metrics across the two studies across domains.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We focus on the following baselines and report per-participant metrics along with average metrics for both studies across all relevant domains.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">”1-second Dense Captioning with LaViLa” - this baseline densely splits the video into 1-second long clips and uses LaViLa <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib44" title="">2023</a>)</cite> on each clip to caption individual moments in the video.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">”Ultrasonic Action Captioning Without Video” - this baseline uses models trained on the active acoustic sensing modality to generate clips based on whether the ultrasonic classifier (in this case a pre-trained ActSonic <cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> model) classifies a particular 1-second clip as within the domain. The caption for this domain is derived from the classifier label (e.g. for a particular label “eating”, the caption would be extracted as “C performed the action: eating”). Notably, this method does not need to sample the video at all, but misses vital context which could be useful for understanding the details of the action.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">EchoGuide, using ultrasonic action detection (via a pre-trained ActSonic <cite class="ltx_cite ltx_citemacro_citep">(Mahmud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib23" title="">2024</a>)</cite> model) to efficiently clip a video before applying the LaViLa narrator to build an activity record.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Quantitative Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We report per-participant metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.T1" title="Table 1 ‣ 5.3. Quantitative Results ‣ 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. We find a relatively large reduction (avg 68%, max 95.9%, min 34.7%) in activity records using active acoustic sensing with relevant domain actions, though reductions are uneven due to the uneven distribution of eating activities (e.g. P06 spent most of the session eating, resulting in a low reduction of the activity record). We found a higher alignment score by combining both ultrasonic and video modalities to capture and record activities when compared to only using the cheaper ultrasonic modality (0.892 avg for EchoGuide vs 0.828 avg for ActSonic, with low alignment values primarily due to a lack of relevant details within the corresponding activity documents, preventing the LLM from giving a detailed response). Notably, these high correlations and significant % reductions are achieved without fine-tuning either the ultrasonic activity clipper or the visual captioning model on new videos, resulting in “session-independent/user-independent” performance metrics. In addition, these results are collected on user study data that is primarily centered around eating activities: if extended to longer “everyday recordings” where eating is comparatively sparse, future iterations of this system could achieve much higher record reduction metrics.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S5.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S5.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1">EchoGuide vs Dense</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S5.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.3.1">ActSonic vs Dense</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S5.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.4.1">% Reduction</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.2.1.1">P01</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.2.1.2">0.888</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.1.2.1.3">0.854</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.1.2.1.4">95.1%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.3.2.1">P02</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.3.2.2">0.897</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.3.2.3">0.823</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.3.2.4">53.4%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.4.3.1">P03</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.4.3.2">0.873</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.4.3.3">0.866</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.4.3.4">83.7%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.5.4.1">P04</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.5.4.2">0.888</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.5.4.3">0.862</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.5.4.4">95.5%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.6.5.1">P05</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.6.5.2">0.906</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.6.5.3">0.774</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.6.5.4">55.1%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.7.6.1">P06</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.7.6.2">0.890</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.7.6.3">0.805</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.7.6.4">34.7%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.8.7.1">P07</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.8.7.2">0.882</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.8.7.3">0.785</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.8.7.4">40%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.9.8.1">P08</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.9.8.2">0.897</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.9.8.3">0.853</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.9.8.4">95.9%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.10.9.1">P09</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.10.9.2">0.907</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.1.10.9.3">0.833</td>
<td class="ltx_td ltx_align_left" id="S5.T1.1.10.9.4">67.5%</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>EchoGuide metrics across participants in user study, including correlation with dense 1-second clipping (measured as mean BERT F1 score across all sessions per participant) vs ActSonic correlation with 1=second clipping, and % reduction in activity record using active acoustics vs 1-second clipping</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.T1.2">.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Evaluating activity records’ ability to answer targeted eating questions with large image-language models</h2>
<figure class="ltx_table" id="S6.T2">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r" id="S6.T2.1.1.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.2.1">Food type (1fps/0.5fps)</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.3.1">Utensil type</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.4.1">Container type</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.2.2.1">P01</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T2.1.2.2.2">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T2.1.2.2.3">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.1.2.2.4">1/0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.3.3.1">P02</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.3.3.2">1/1</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.3.3.3">1/1</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.3.3.4">1/1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.4.4.1">P03</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.4.4.2">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.4.4.3">1/0</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.4.4.4">1/0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.5.5.1">P04</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.5.5.2">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.5.5.3">0/0</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.5.5.4">0/0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.6.6.1">P05</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.6.6.2">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.6.6.3">1/0</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.6.6.4">0/0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.7.7.1">P06</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.7.7.2">1/1</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.7.7.3">1/1</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.7.7.4">1/1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.8.8.1">P07</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.8.8.2">1/0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.8.8.3">1/1</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.8.8.4">1/1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.9.9.1">P08</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.9.9.2">1/1</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.9.9.3">1/1</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.9.9.4">1/1</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S6.T2.1.10.10.1">P09</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.10.10.2">0/0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T2.1.10.10.3">1/1</td>
<td class="ltx_td ltx_align_left" id="S6.T2.1.10.10.4">1/1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Results of manual evaluation of EchoGuide + GPT4o given 1fps vs 0.5 fps sampling of frames from ActSonic-defined clips (based on zero-shot accuracy). Notation is defined as (X/Y) where X=accuracy at 1fps and Y=accuracy at 0.5fps</figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Metrics/Evaluation Procedure</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">EchoGuide, however, focuses not only on providing general summaries via activity records of an individual’s day from video and wearable sensor data, but also on answering targeted questions about these summaries by leveraging the image-text pertaining of large multimodal language models. We evaluate this method via manual review and annotation of the system’s answers to three eating questions (“What did C eat/drink? What utensils did C use while eating/drinking? What container did C eat or drink out of?”) when configured to use GPT4o <cite class="ltx_cite ltx_citemacro_citep">(GPT, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib2" title="">[n. d.]</a>)</cite> to caption images sampled at two varying FPS levels (1fps and 0.5fps) from clips proposed by ActSonic, and report accuracy metrics showing whether EchoGuide extracts correct values for these questions as compared to manually-determined ”ground truth” (taken by watching the reference video and determining which item is present): we’ve shown accuracy values given 1fps and 0.5fps sampling in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S6.T2" title="Table 2 ‣ 6. Evaluating activity records’ ability to answer targeted eating questions with large image-language models ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">2</span></a>. Accuracy values are defined as a 0/1 binary: 0 represents responses that do not overlap with the ground truth, while 1 represents responses that do completely overlap with the ground truth.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Quantitative Results and Discussion</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In general, we find that while 0.5fps results in a slow reduction in performance for some participants, we can attempt to leverage only a few frames along with metadata information (e.g. classifier outputs) from active acoustic sensors to output useful information, instead of having to process an entire video which could be full of redundant frames. For more ambiguous class types such as “food type” (which may not be easily determinable from appearance alone), we find a relatively low average F1 score (44% for 1fps and 30.5% for 0.5fps) across all participants, whereas for more recognizable/distinctive class type such as utensil type and container type, we find a relatively high average F-1 score (77% for utensils and containers for 1fps, 55% for utensils and containers for 0.5 fps). We find a clear performance drop as FPS is reduced (from 0.55 with 1 fps to 0.47 with 0.5 FPS), due to increased sparsity of frames causing reductions in visual detail for the models. As vision-language models and prompting techniques continue to improve, we expect these numbers will become more accurate over time.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Further Reduction on video recording when deployed in the wild Our study results showed that EchoGuide helped reduce video recordings by an average of 68% without significantly impacting the quality of summarization for eating activities. However, we want to point out that this percentage of reduction will likely be significantly higher if the system is deployed for full-day recording. In the user study, we only asked participants to collect data for 40 minutes, including the meal. Consequently, the ratio of eating activities in our dataset is significantly higher than it would be in a full day of recording. Therefore, if ActSonic is used to only activate the camera during eating activities in a full-day recording, the data reduction will likely be significantly higher than 68%. Additionally, the frame rate of recording can be further reduced to answer specific questions, saving energy and processing resources. We plan to explore these questions further in future works</p>
</div>
<figure class="ltx_figure" id="S7.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S7.F3.g1" src="extracted/5766746/paper/figures/fovs.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Example showing field of view between GoPro vs Meta Ray-Bans</figcaption>
</figure>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Comparison to Egocentric Video recorded on glasses</span> The initial hardware system for EchoGuide was not collected using “camera-enabled smartglasses” such as the Meta Ray-Bans <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib24" title="">2023</a>)</cite> due to limitations on recording videos with these off-the-shelf smart glasses. Instead, we used a head-mounted GoPro to easily capture egocentric activity videos. We found the information related to eating captured by smart glasses and our GoPro settings to be highly similar. To help readers understand the difference in images captured by these two devices, we used RayBan Smart glasses and a GoPro HERO9 mounted on the head to capture the same dietary scenario (a drink on the table), as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S7.F3" title="Figure 3 ‣ 7. Discussion ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">3</span></a>. We found that the GoPro has a much wider field of view and can capture more general scene details in the orientation used by EchoGuide compared to the camera on the Meta RayBan smart glasses. However, because most foods are present near the center of the field of view, the difference in view angle between the two cameras did not impact the captured information. Therefore, the results reported in the paper can still be referenced for egocentric video analysis captured on smart glasses.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Exploring additional domains for EchoGuide</span> While EchoGuide was adapted to focus primarily on localizing and understanding eating activities from video and acoustics, we also run a separate exploratory study with three participants operating in both the “eating” and “cooking” domains. Each participant engaged in 5 sessions of 4 minutes each within each domain with interventions between sessions to stop and restart data collection, for a total of 40 minutes per participant.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">The question for the ”eating” domain was “”What did the person C eat or drink?”, with ”relevant ultrasonic actions” being defined as the set of ”eating”, ”drinking”, and ”pickup/putdown” (referring to manipulated items) and the question for the “cooking” domain was “What did the person C cook?” with relevant ultrasonic actions being the set of “chopping”, “pouring”, “stirring”, “pickup/putdown”, or “walking”. We configure LlamaIndex with GPT3.5-turbo and a temperature value of 0, as well as the standard context prompt “You are a chatbot, able to have normal interactions, as well answer questions from the person about what they did today (walking, eating, cooking, etc). Here are the relevant documents for the context: {context_str}. Instruction: Use the previous chat history, or the context above, to interact and help the user. Format responses as a paragraph.”</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">We find a high average % reduction of 87% in record size across both domains by leveraging active acoustic sensing for clipping videos, along with higher correlation with dense captions (0.9 BERT F1 score) while using EchoGuide’s multimodal approach over only using active acoustic sensing (0.86 BERT F1 score). We find that combining the video and ultrasonic modalities additionally shows quantitative improvement (with respect to alignment with the dense caption summary of the original video) when compared to only using the ultrasonic modality, while still maintaining high reductions in the activity record. Though more thorough investigation needs to be done to show this system can work across a wider variety of everyday activities, improvements in unseen domains show the relatively task-agonstic nature of the EchoGuide software pipeline.</p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1"><span class="ltx_text ltx_font_bold" id="S7.p6.1.1">Improving comfort and reliability of hardware prototype</span> The current hardware prototype leverages a Teensy-based microcontroller on the left side of the eyeglasses which is connected to a phone for power and recording control, along with a Go-Pro head-mounted camera for video capture. The relative weight and complexity of the combined devices were cited in user surveys as the primary reason for the low comfort rating of the prototype (as it weighed more on the head and ears).</p>
</div>
<div class="ltx_para" id="S7.p7">
<p class="ltx_p" id="S7.p7.1">Leveraging a lower-power BLE with a LiPo battery (similar to Google Glass) as the primary microcontroller module for active acoustic recording, along with a Flex-PCB that reduces extraneous wiring, can reduce the unwieldy nature of the acoustic system. Developing custom low-power, high-FPS and high-resolution cameras (such as event-based cameras) that are purpose-built for eyeglass frames can also enable seamless video recording without a GoPro requirement, reducing the weight of the systems considerably. Building eyeglass frames that can swap lenses in a custom way, or building a system that can be seamlessly applied on any eyeglass, can reduce the likelihood of participants with custom prescriptions being unable to see through the provided eyeglasses.</p>
</div>
<div class="ltx_para" id="S7.p8">
<p class="ltx_p" id="S7.p8.1"><span class="ltx_text ltx_font_bold" id="S7.p8.1.1">Reducing software latency to enable real-time applications</span> Currently, EchoGuide processes and asks questions over activity records in an offline fashion, but many users may want to understand their activities in an online fashion (for instance, asking about previous meals while evaluating what food to get at a restaurant). As seen in other concurrent works <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib41" title="">2023b</a>)</cite>, active acoustic postprocessing could be completed on a smartphone, and with advancements in embedded AI chips and stronger networking modules for more robust cloud access, it may be possible to do end-to-end inference online with both edge-deployed and cloud-deployed models.</p>
</div>
<div class="ltx_para" id="S7.p9">
<p class="ltx_p" id="S7.p9.1"><span class="ltx_text ltx_font_bold" id="S7.p9.1.1">Improving overall model flexibility to new situations</span> While Sec <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5" title="5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#S5.T1" title="Table 1 ‣ 5.3. Quantitative Results ‣ 5. Evaluating quality of eating activity summaries and responses with LLMs ‣ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis from Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a> show promising results for EchoGuide usage (combining video and ultrasonic modalities) across two distinct domains and procedural styles in everyday activities, further improvements can be made to enhance overall system performance. Collecting and fine-tuning on a larger base dataset of ultrasonic captures of activities can enable more robust, user-independent detection of human body motion, while leveraging steadily more powerful large multimodal models can enable more robust and generalizable video captions that encode more domain-specific or estoeric information.</p>
</div>
<div class="ltx_para" id="S7.p10">
<p class="ltx_p" id="S7.p10.1">The EchoGuide pipeline leverages a Large Language Model with Retrieval Augmented Generation to enable document-based question-answering, along with a Large Multimodal Language Model to enable video captioning. Further optimization of system performance could be achieved via careful prompt engineering methods, such as chain-of-thought with few-shot exemplars <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib35" title="">2022</a>; Kojima et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10750v2#bib.bib14" title="">2022</a>)</cite>. We leave this in-depth exploration of prompt engineering methods to future work.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we present EchoGuide, an innovative application pipeline that combines low-power active acoustic sensing on eyeglasses, egocentric video analysis, and large-scale language models to efficiently detect and analyze eating activities. Our evaluation with 9 participants in naturalistic settings demonstrates that EchoGuide achieves high-quality summarization with a significant reduction in record size while maintaining high semantic correlation with densely-captioned records. As smart glasses become more widespread and equipped with various sensors, multi-stage pipelines like EchoGuide have the potential to be applied to a broader range of activities and contexts without requiring explicit fine-tuning for individual users.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This project was supported by the National Science Foundation Grant No. 2239569 and partially by the Cornell University IGNITE Innovation Acceleration Program. We also acknowledge and thank Prof. Bharath Hariharan at Cornell CS for help with computing resources.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GPT ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o" title="">https://openai.com/index/hello-gpt-4o</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedri et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Abdelkareem Bedri, Diana Li, Rushil Khurana, Kunal Bhuwalka, and Mayank Goel. 2020.

</span>
<span class="ltx_bibblock">FitByte: Automatic Diet Monitoring in Unconstrained Situations Using Multimodal Sensing on Eyeglasses. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> (¡conf-loc¿, ¡city¿Honolulu¡/city¿, ¡state¿HI¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(CHI ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1–12.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376869" title="">https://doi.org/10.1145/3313831.3376869</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bedri et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Abdelkareem Bedri, Yuchen Liang, Sudershan Boovaraghavan, Geoff Kaufman, and Mayank Goel. 2022.

</span>
<span class="ltx_bibblock">FitNibble: A Field Study to Evaluate the Utility and Usability of Automatic Diet Monitoring in Food Journaling Using an Eyeglasses-based Wearable. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 27th International Conference on Intelligent User Interfaces</em> (¡conf-loc¿, ¡city¿Helsinki¡/city¿, ¡country¿Finland¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.2">(IUI ’22)</em>. Association for Computing Machinery, New York, NY, USA, 79–92.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3490099.3511154" title="">https://doi.org/10.1145/3490099.3511154</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bi et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Shengjie Bi, Tao Wang, Nicole Tobias, Josephine Nordrum, Shang Wang, George Halvorsen, Sougata Sen, Ronald Peterson, Kofi Odame, Kelly Caine, Ryan Halter, Jacob Sorber, and David Kotz. 2018.

</span>
<span class="ltx_bibblock">Auracle: Detecting Eating Episodes with an Ear-mounted Sensor.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em> 2, 3, Article 92 (sep 2018), 27 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3264902" title="">https://doi.org/10.1145/3264902</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al<span class="ltx_text" id="bib.bib6.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.1">arXiv preprint arXiv:2108.07258</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib7.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Tao Chen, Yongjie Yang, Xiaoran Fan, Xiuzhen Guo, Jie Xiong, and Longfei Shangguan. 2024a.

</span>
<span class="ltx_bibblock">Exploring the Feasibility of Remote Cardiac Auscultation Using Earphones. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 30th Annual International Conference on Mobile Computing and Networking</em>. 357–372.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Tao Chen, Yongjie Yang, Chonghao Qiu, Xiaoran Fan, Xiuzhen Guo, and Longfei Shangguan. 2024b.

</span>
<span class="ltx_bibblock">Enabling Hands-Free Voice Assistant Activation on Earphones. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 22nd Annual International Conference on Mobile Systems, Applications and Services</em> (Minato-ku, Tokyo, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib9.4.2">(MOBISYS ’24)</em>. Association for Computing Machinery, New York, NY, USA, 155–168.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3643832.3661890" title="">https://doi.org/10.1145/3643832.3661890</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoran Fan, David Pearl, Richard Howard, Longfei Shangguan, and Trausti Thormundsson. 2023.

</span>
<span class="ltx_bibblock">Apg: Audioplethysmography for cardiac monitoring in hearables. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 29th Annual International Conference on Mobile Computing and Networking</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farooq and Sazonov (2016)</span>
<span class="ltx_bibblock">
Muhammad Farooq and Edward Sazonov. 2016.

</span>
<span class="ltx_bibblock">A novel wearable device for food intake and physical activity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Sensors</em> 16, 7 (2016), 1067.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GoPro (2020)</span>
<span class="ltx_bibblock">
GoPro. 2020.

</span>
<span class="ltx_bibblock">HERO9 Black.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gopro.com/en/us/shop/cameras/hero9-black/CHDHX-901-master.html" title="">https://gopro.com/en/us/shop/cameras/hero9-black/CHDHX-901-master.html</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Online; accessed 12-September-2023].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al<span class="ltx_text" id="bib.bib13.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Ego4d: Around the world in 3,000 hours of egocentric video. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18995–19012.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kojima et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Advances in neural information processing systems</em> 35 (2022), 22199–22213.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Chi-Jung Lee, Ruidong Zhang, Devansh Agarwal, Tianhong Catherine Yu, Vipin Gunda, Oliver Lopez, James Kim, Sicheng Yin, Boao Dong, Ke Li, Mose Sakashita, Francois Guimbretiere, and Cheng Zhang. 2024.

</span>
<span class="ltx_bibblock">EchoWrist: Continuous Hand Pose Tracking and Hand-Object Interaction Recognition Using Low-Power Active Acoustic Sensing On a Wristband. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 403, 21 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3642910" title="">https://doi.org/10.1145/3613904.3642910</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib16.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ke Li, Devansh Agarwal, Ruidong Zhang, Vipin Gunda, Tianjun Mo, Saif Mahmud, Boao Chen, François Guimbretière, and Cheng Zhang. 2024a.

</span>
<span class="ltx_bibblock">SonicID: User Identification on Smart Glasses with Acoustic Sensing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2406.08273</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Ke Li, Ruidong Zhang, Boao Chen, Siyuan Chen, Sicheng Yin, Saif Mahmud, Qikang Liang, Francois Guimbretiere, and Cheng Zhang. 2024b.

</span>
<span class="ltx_bibblock">GazeTrak: Exploring Acoustic-based Eye Tracking on a Glass Frame. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 30th Annual International Conference on Mobile Computing and Networking</em> (Washington D.C., DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.2">(ACM MobiCom ’24)</em>. Association for Computing Machinery, New York, NY, USA, 497–512.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3636534.3649376" title="">https://doi.org/10.1145/3636534.3649376</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ke Li, Ruidong Zhang, Bo Liang, François Guimbretière, and Cheng Zhang. 2022.

</span>
<span class="ltx_bibblock">EarIO: A Low-Power Acoustic Sensing Earable for Continuously Tracking Detailed Facial Movements.

</span>
<span class="ltx_bibblock">6, 2, Article 62 (jul 2022), 24 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534621" title="">https://doi.org/10.1145/3534621</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, et al<span class="ltx_text" id="bib.bib20.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Egocentric Video-Language Pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.4.1">arXiv preprint arXiv:2206.01670</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2022)</span>
<span class="ltx_bibblock">
Jerry Liu. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">LlamaIndex</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.1234" title="">https://doi.org/10.5281/zenodo.1234</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmud et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Saif Mahmud, Ke Li, Guilin Hu, Hao Chen, Richard Jin, Ruidong Zhang, François Guimbretière, and Cheng Zhang. 2023.

</span>
<span class="ltx_bibblock">PoseSonic: 3D Upper Body Pose Estimation Through Egocentric Acoustic Sensing on Smartglasses.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em> 7, 3, Article 111 (sep 2023), 28 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3610895" title="">https://doi.org/10.1145/3610895</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmud et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Saif Mahmud, Vineet Parikh, Qikang Liang, Ke Li, Ruidong Zhang, Ashwin Ajit, Vipin Gunda, Devansh Agarwal, François Guimbretière, and Cheng Zhang. 2024.

</span>
<span class="ltx_bibblock">ActSonic: Everyday Activity Recognition on Smart Glasses using Active Acoustic Sensing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2404.13924</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2023)</span>
<span class="ltx_bibblock">
Meta. 2023.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://about.fb.com/news/2023/09/new-ray-ban-meta-smart-glasses/" title="">https://about.fb.com/news/2023/09/new-ray-ban-meta-smart-glasses/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirtchouk et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Mark Mirtchouk, Drew Lustig, Alexandra Smith, Ivan Ching, Min Zheng, and Samantha Kleinberg. 2017.

</span>
<span class="ltx_bibblock">Recognizing Eating from Body-Worn Sensors: Combining Free-living and Laboratory Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em> 1, 3, Article 85 (sep 2017), 20 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3131894" title="">https://doi.org/10.1145/3131894</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. 2022.

</span>
<span class="ltx_bibblock">Expanding language-image pretrained models for general video recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">European Conference on Computer Vision</em>. Springer, 1–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich,
Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David
Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam
Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,
Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss,
Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,
William Zhuk, and Barret Zoph. 2024.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span class="ltx_text" id="bib.bib28.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.4.1">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jaemin Shin, Seungjoo Lee, Taesik Gong, Hyungjun Yoon, Hyunchul Roh, Andrea Bianchi, and Sung-Ju Lee. 2022.

</span>
<span class="ltx_bibblock">MyDJ: Sensing Food Intakes with an Attachable on Your Eyeglass Frame. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em> (, New Orleans, LA, USA,) <em class="ltx_emph ltx_font_italic" id="bib.bib29.4.2">(CHI ’22)</em>. Association for Computing Machinery, New York, NY, USA, Article 341, 17 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3502041" title="">https://doi.org/10.1145/3491102.3502041</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rujia Sun, Xiaohe Zhou, Benjamin Steeper, Ruidong Zhang, Sicheng Yin, Ke Li, Shengzhang Wu, Sam Tilsen, Francois Guimbretiere, and Cheng Zhang. 2023.

</span>
<span class="ltx_bibblock">EchoNose: Sensing Mouth, Breathing and Tongue Gestures inside Oral Cavity using a Non-contact Nose Interface. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 2023 ACM International Symposium on Wearable Computers</em> (Cancun, Quintana Roo, Mexico) <em class="ltx_emph ltx_font_italic" id="bib.bib30.4.2">(ISWC ’23)</em>. Association for Computing Machinery, New York, NY, USA, 22–26.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3594738.3611358" title="">https://doi.org/10.1145/3594738.3611358</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al<span class="ltx_text" id="bib.bib31.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.4.1">arXiv preprint arXiv:2312.11805</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomaz et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Edison Thomaz, Cheng Zhang, Irfan Essa, and Gregory D Abowd. 2015.

</span>
<span class="ltx_bibblock">Inferring meal eating activities in real world settings from ambient sounds: A feasibility study. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of the 20th International Conference on Intelligent User Interfaces</em>. 427–431.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu Yuan. 2022.

</span>
<span class="ltx_bibblock">Bevt: Bert pretraining of video transformers. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 14733–14743.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tianben Wang, Daqing Zhang, Yuanqing Zheng, Tao Gu, Xingshe Zhou, and Bernadette Dorizzi. 2018.

</span>
<span class="ltx_bibblock">C-FMCW Based Contactless Respiration Detection Using Acoustic Signal.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em> 1, 4, Article 170 (jan 2018), 20 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3161188" title="">https://doi.org/10.1145/3161188</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib35.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.4.1">Advances in neural information processing systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wray et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen. 2019.

</span>
<span class="ltx_bibblock">Fine-grained action retrieval through multiple parts-of-speech embeddings. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 450–459.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wray et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Michael Wray, Davide Moltisanti, Walterio Mayol-Cuevas, and Dima Damen. 2016.

</span>
<span class="ltx_bibblock">Sembed: Semantic embedding of egocentric action videos. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">European Conference on Computer Vision</em>. Springer, 532–545.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianhong Catherine Yu, Guilin Hu, Ruidong Zhang, Hyunchul Lim, Saif Mahmud, Chi-Jung Lee, Ke Li, Devansh Agarwal, Shuyang Nie, Jinseok Oh, et al<span class="ltx_text" id="bib.bib38.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Ring-a-Pose: A Ring for Continuous Hand Pose Tracking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.4.1">arXiv preprint arXiv:2404.12980</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Amft (2017)</span>
<span class="ltx_bibblock">
Rui Zhang and Oliver Amft. 2017.

</span>
<span class="ltx_bibblock">Monitoring chewing and eating in free-living using smart eyeglasses.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE journal of biomedical and health informatics</em> 22, 1 (2017), 23–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ruidong Zhang, Hao Chen, Devansh Agarwal, Richard Jin, Ke Li, François Guimbretière, and Cheng Zhang. 2023a.

</span>
<span class="ltx_bibblock">HPSpeech: Silent Speech Interface for Commodity Headphones. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the 2023 ACM International Symposium on Wearable Computers</em>. 60–65.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ruidong Zhang, Ke Li, Yihong Hao, Yufan Wang, Zhengnan Lai, François Guimbretière, and Cheng Zhang. 2023b.

</span>
<span class="ltx_bibblock">EchoSpeech: Continuous Silent Speech Recognition on Minimally-Obtrusive Eyewear Powered by Acoustic Sensing. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Hamburg, Germany) <em class="ltx_emph ltx_font_italic" id="bib.bib41.4.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 852, 18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580801" title="">https://doi.org/10.1145/3544548.3580801</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruidong Zhang, Jihai Zhang, Nitish Gade, Peng Cao, Seyun Kim, Junchi Yan, and Cheng Zhang. 2022.

</span>
<span class="ltx_bibblock">EatingTrak: Detecting Fine-Grained Eating Moments in the Wild Using a Wrist-Mounted IMU.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proc. ACM Hum.-Comput. Interact.</em> 6, MHCI, Article 214 (sep 2022), 22 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3546749" title="">https://doi.org/10.1145/3546749</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang* et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock">BERTScore: Evaluating Text Generation with BERT. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">https://openreview.net/forum?id=SkeHuCVFDr</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar. 2023.

</span>
<span class="ltx_bibblock">Learning video representations from large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 6586–6597.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 31 15:41:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
