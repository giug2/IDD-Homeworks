<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications</title>
<!--Generated on Thu Oct 10 21:46:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.20407v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S1" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS1" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS2" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Annotation in CVAT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS3" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Intra and Inter Grader Validation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS4" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Segmentation Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS5" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Prediction of Periorbital Distances</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.SS6" title="In 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.6 </span>Hardware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.SS1" title="In 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Description of the Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.SS2" title="In 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Segmentation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.SS3" title="In 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Intra and Intergrader Variation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S4" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S5" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Dataset and Code Availability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S6" title="In Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Supplemental Figures</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
George R.¬†Nahass 
<br class="ltx_break"/>Ophthalmology and Biomedical Engineering 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">gnahas2@uic.edu</span>
<br class="ltx_break"/>&amp;Emma¬†Koehler 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Nicholas¬†Tomares 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Danny¬†Lopez 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Madison¬†Cheung 
<br class="ltx_break"/>Plastic and Reconstructive Surgery 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Alexander¬†Palacios 
<br class="ltx_break"/>Plastic and Reconstructive Surgery 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Jefferey¬†Peterson 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Sasha¬†Hubschman 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Kelsey¬†Green 
<br class="ltx_break"/>Plastic and Reconstructive Surgery 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Chad¬†Purnell 
<br class="ltx_break"/>Plastic and Reconstructive Surgery 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Pete¬†Setabutr 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/>&amp;Ann Q.¬†Tran 
<br class="ltx_break"/>Ophthalmology 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">annqtran@uic.edu</span>
<br class="ltx_break"/>&amp;Darvin¬†Yi 
<br class="ltx_break"/>Ophthalmology and Biomedical Engineering 
<br class="ltx_break"/>University of Illinois Chicago College of Medicine 
<br class="ltx_break"/>Chicago, IL, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">dyi9@uic.edu</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">High quality segmentation of the eyes and lids is an essential step in developing clinically relevant deep learning models for oculoplastic and craniofacial surgery. However, there are currently no publicly available datasets suitable for this purpose. As such, we have developed and validated a novel dataset for oculoplastic segmentation and periorbital distance prediction. Using images from two open-source datasets, we segmented the iris, sclera, lid, caruncle, and brow from cropped eye images. Five trained annotators performed the segmentations, and intergrader reliability was assessed on 100 randomly selected images with a two-week interval, yielding an average Dice score of 0.82 ¬± 0.01. Intragrader reliability on 20 images averaged a Dice score of 0.81 ¬± 0.08. To demonstrate the dataset‚Äôs utility, we trained three DeepLabV3 models following standard procedures. This first-of-its-kind dataset, along with a toolkit for periorbital distance prediction, is publicly available to support the development of clinically useful segmentation models for oculoplastic and craniofacial applications.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In ophthalmology, multiple open-source datasets exist for classification and segmentation tasks using optical coherence tomography images, retinal fundus photographs, in vivo confocal microscopy images, and other imaging modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib4" title="">4</a>]</cite>. While the quality and quantity of open-source datasets for images of the inner eye is large, there are far fewer open-source datasets of external eye images for the purposes of training deep learning networks. There are independent datasets for iris segmentation with various labeling of self-reported gender and glasses status, however, these datasets are primarily for use in biometric identification and iris recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For a dataset to hold clinical relevance for segmentation tasks in oculoplastic and craniofacial surgery, detailed annotations of meaningful periorbital anatomy need to be created. Current publicly available datasets that have annotations for brows and lids were designed to train algorithms for face recognition and image generation using generative adversarial networks, and, out of the box, lack the appropriate detail required to obtain anatomically accurate segmentation at precise levels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib7" title="">7</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the literature, many papers have recently used segmentation of external ocular anatomy as an intermediate step in predicting periorbital distances <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib10" title="">10</a>]</cite>. Clinically, measuring periorbital distances is an important step to track disease progression and monitor treatment efficacy. However manual measurements is a time consuming and error prone process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib11" title="">11</a>]</cite>. While automatic prediction of periorbital distances via deep learning stands to significantly reduce the time burden of oculoplastic and craniofacial surgeons in the clinic, it also presents an attractive strategy to objectively measure periorbital distances to a high degree of accuracy. For research in this area to move forward, the development of open-source datasets with periorbital annotations created at the level of detail required for prediction of sub millimeter distances is required. As such, we have curated two open-source datasets of external eye images, created annotations of the caruncle, iris, lids, brows, and scleras, and demonstrated their efficacy in training deep learning models with them. Additionally, we have open-sourced a toolkit to predict periorbital distances from segmentation masks and ground truth measurements generated from our dataset to encourage future research and model development in this space.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Acquisition</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Open-source data was used as the foundation of the dataset. Images were sourced from the Chicago Facial dataset (CFD) and the CelebAMask-HQ dataset (Celeb) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib14" title="">14</a>]</cite>. All images were cropped to include the eyes and periorbital regions using Mediapipe Facemesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib15" title="">15</a>]</cite>. 827 images were included from the CFD, and 2015 images were included from Celeb.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Annotation in CVAT</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Five annotators were trained to use the Computer Vision Annotation Tool (CVAT) to draw masks denoting the iris, sclera, lids, caruncles, and brows. Existing brow and sclera annotations were used as starter masks for the Celeb dataset. For lid annotations, the lid crease was used as the superior margin and the superior rim of the sclera was used as the inferior margin. The lid annotation was bounded by the shortest line between the medial or lateral canthus to the lid crease, or the termination of the lid crease against the superior scleral margin. Upper eyelashes were included in the lid annotation, and epicanthal folds were not annotated. Iris annotations were bounded inferiorly and superiorly by the scleral margin. Scleral annotations were made from the lateral canthus to the caruncle, and the caruncle was annotated only if present. Annotations were exported from CVAT in COCO format and post-processed so every anatomical structure is denoted by a specific pixel value ranging 1-5. Representative images of eyes with full annotations from both CFD and the Celeb dataset can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.F1" title="Figure 1 ‚Ä£ 2.2 Annotation in CVAT ‚Ä£ 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="S2.F1.g1" src="extracted/5893670/eye_grid.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Representative images and annotations from the CFD and Celeb dataset used to construct the dataset described here.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Intra and Inter Grader Validation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Each annotator annotated an additional 100 images (60 from Celeb, 40 from CFD), where 20 images were ones they had annotated in the past. A minimum of two weeks had elapsed from the time of initial annotation to permit fair intra-grader evaluation. For the 100 images, the Dice score was computed pairwise between each annotator according to Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.E1" title="In 2.3 Intra and Inter Grader Validation ‚Ä£ 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Dice=\frac{2(\mathbf{X}\cap{\mathbf{Y}})}{|\mathbf{X}|+|\mathbf{Y}|}" class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.4" xref="S2.E1.m1.3.4.cmml"><mrow id="S2.E1.m1.3.4.2" xref="S2.E1.m1.3.4.2.cmml"><mi id="S2.E1.m1.3.4.2.2" xref="S2.E1.m1.3.4.2.2.cmml">D</mi><mo id="S2.E1.m1.3.4.2.1" xref="S2.E1.m1.3.4.2.1.cmml">‚Å¢</mo><mi id="S2.E1.m1.3.4.2.3" xref="S2.E1.m1.3.4.2.3.cmml">i</mi><mo id="S2.E1.m1.3.4.2.1a" xref="S2.E1.m1.3.4.2.1.cmml">‚Å¢</mo><mi id="S2.E1.m1.3.4.2.4" xref="S2.E1.m1.3.4.2.4.cmml">c</mi><mo id="S2.E1.m1.3.4.2.1b" xref="S2.E1.m1.3.4.2.1.cmml">‚Å¢</mo><mi id="S2.E1.m1.3.4.2.5" xref="S2.E1.m1.3.4.2.5.cmml">e</mi></mrow><mo id="S2.E1.m1.3.4.1" xref="S2.E1.m1.3.4.1.cmml">=</mo><mfrac id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mn id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">2</mn><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">ùêó</mi><mo id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">‚à©</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">ùêò</mi></mrow><mo id="S2.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mrow id="S2.E1.m1.3.3.3.4.2" xref="S2.E1.m1.3.3.3.4.1.cmml"><mo id="S2.E1.m1.3.3.3.4.2.1" stretchy="false" xref="S2.E1.m1.3.3.3.4.1.1.cmml">|</mo><mi id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml">ùêó</mi><mo id="S2.E1.m1.3.3.3.4.2.2" stretchy="false" xref="S2.E1.m1.3.3.3.4.1.1.cmml">|</mo></mrow><mo id="S2.E1.m1.3.3.3.3" xref="S2.E1.m1.3.3.3.3.cmml">+</mo><mrow id="S2.E1.m1.3.3.3.5.2" xref="S2.E1.m1.3.3.3.5.1.cmml"><mo id="S2.E1.m1.3.3.3.5.2.1" stretchy="false" xref="S2.E1.m1.3.3.3.5.1.1.cmml">|</mo><mi id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">ùêò</mi><mo id="S2.E1.m1.3.3.3.5.2.2" stretchy="false" xref="S2.E1.m1.3.3.3.5.1.1.cmml">|</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.4.cmml" xref="S2.E1.m1.3.4"><eq id="S2.E1.m1.3.4.1.cmml" xref="S2.E1.m1.3.4.1"></eq><apply id="S2.E1.m1.3.4.2.cmml" xref="S2.E1.m1.3.4.2"><times id="S2.E1.m1.3.4.2.1.cmml" xref="S2.E1.m1.3.4.2.1"></times><ci id="S2.E1.m1.3.4.2.2.cmml" xref="S2.E1.m1.3.4.2.2">ùê∑</ci><ci id="S2.E1.m1.3.4.2.3.cmml" xref="S2.E1.m1.3.4.2.3">ùëñ</ci><ci id="S2.E1.m1.3.4.2.4.cmml" xref="S2.E1.m1.3.4.2.4">ùëê</ci><ci id="S2.E1.m1.3.4.2.5.cmml" xref="S2.E1.m1.3.4.2.5">ùëí</ci></apply><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><divide id="S2.E1.m1.3.3.4.cmml" xref="S2.E1.m1.3.3"></divide><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><cn id="S2.E1.m1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.3">2</cn><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><intersect id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"></intersect><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">ùêó</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">ùêò</ci></apply></apply><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><plus id="S2.E1.m1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3"></plus><apply id="S2.E1.m1.3.3.3.4.1.cmml" xref="S2.E1.m1.3.3.3.4.2"><abs id="S2.E1.m1.3.3.3.4.1.1.cmml" xref="S2.E1.m1.3.3.3.4.2.1"></abs><ci id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1">ùêó</ci></apply><apply id="S2.E1.m1.3.3.3.5.1.cmml" xref="S2.E1.m1.3.3.3.5.2"><abs id="S2.E1.m1.3.3.3.5.1.1.cmml" xref="S2.E1.m1.3.3.3.5.2.1"></abs><ci id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">ùêò</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">Dice=\frac{2(\mathbf{X}\cap{\mathbf{Y}})}{|\mathbf{X}|+|\mathbf{Y}|}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_D italic_i italic_c italic_e = divide start_ARG 2 ( bold_X ‚à© bold_Y ) end_ARG start_ARG | bold_X | + | bold_Y | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Segmentation Pipeline</h3>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.4">We trained segmentation models on both the CFD and Celeb datasets. A DeepLabV3 segmentation network with a ResNet-101 backbone pretrained on ImageNet1K was implemented from Torchvision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib16" title="">16</a>]</cite>. The final layer was modified to output six output classes, and the model was trained for <math alttext="500" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><mn id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><cn id="S2.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS4.p1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">500</annotation></semantics></math> steps. A train test split of <math alttext="80/20" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><mrow id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml"><mn id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">80</mn><mo id="S2.SS4.p1.2.m2.1.1.1" xref="S2.SS4.p1.2.m2.1.1.1.cmml">/</mo><mn id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1"><divide id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1.1"></divide><cn id="S2.SS4.p1.2.m2.1.1.2.cmml" type="integer" xref="S2.SS4.p1.2.m2.1.1.2">80</cn><cn id="S2.SS4.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.SS4.p1.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">80/20</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">80 / 20</annotation></semantics></math> was used with cross-entropy loss and a batch size of 16. Adam optimization was used with a learning rate of .0001 and beta values of <math alttext=".9" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m3.1"><semantics id="S2.SS4.p1.3.m3.1a"><mn id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml">.9</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.1b"><cn id="S2.SS4.p1.3.m3.1.1.cmml" type="float" xref="S2.SS4.p1.3.m3.1.1">.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.1c">.9</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.3.m3.1d">.9</annotation></semantics></math> and <math alttext=".99" class="ltx_Math" display="inline" id="S2.SS4.p1.4.m4.1"><semantics id="S2.SS4.p1.4.m4.1a"><mn id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml">.99</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.1b"><cn id="S2.SS4.p1.4.m4.1.1.cmml" type="float" xref="S2.SS4.p1.4.m4.1.1">.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.1c">.99</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.4.m4.1d">.99</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Prior to training and prediction, images were split at the midline and resized to <math alttext="256x256" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mrow id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml"><mn id="S2.SS4.p2.1.m1.1.1.2" xref="S2.SS4.p2.1.m1.1.1.2.cmml">256</mn><mo id="S2.SS4.p2.1.m1.1.1.1" xref="S2.SS4.p2.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="S2.SS4.p2.1.m1.1.1.3" xref="S2.SS4.p2.1.m1.1.1.3.cmml">x</mi><mo id="S2.SS4.p2.1.m1.1.1.1a" xref="S2.SS4.p2.1.m1.1.1.1.cmml">‚Å¢</mo><mn id="S2.SS4.p2.1.m1.1.1.4" xref="S2.SS4.p2.1.m1.1.1.4.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><apply id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1"><times id="S2.SS4.p2.1.m1.1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1.1"></times><cn id="S2.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S2.SS4.p2.1.m1.1.1.2">256</cn><ci id="S2.SS4.p2.1.m1.1.1.3.cmml" xref="S2.SS4.p2.1.m1.1.1.3">ùë•</ci><cn id="S2.SS4.p2.1.m1.1.1.4.cmml" type="integer" xref="S2.SS4.p2.1.m1.1.1.4">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">256x256</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">256 italic_x 256</annotation></semantics></math>. At test time, the same process was applied, and the resulting segmentation maps of both halves of the image were recombined using the same aspect ratio as the initial image. Dice score on the test set was computed using the recombined image and the original segmentation mask according to Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.E1" title="In 2.3 Intra and Inter Grader Validation ‚Ä£ 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">1</span></a>. The entire segmentation pipeline can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S2.F2" title="Figure 2 ‚Ä£ 2.4 Segmentation Pipeline ‚Ä£ 2 Methods ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="S2.F2.g1" src="extracted/5893670/pipeline.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Schematic of preprocessing and training pipeline. Full details are described in the methods, but briefly, the dataset was split using an 80/20 train test split. The input image was split at the midline, and both halves of the image (and label) were resized to 256x256. A DeepLabV3 model with a ResNet101 backbone pretrained on ImageNet1K was trained for 500 steps. The same preprocessing procedure was used at test time. Following segmentation, the left and right halves of the image were resized and stitched back together such that the full segmentation mask was the same size as the input.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Prediction of Periorbital Distances</h3>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Periorbital distances were generated using the human annotated segmentation masks to provide a benchmark set of results. The iris diameter was set to a scale of 11.71 millimeters (mm), which was used to derive pixel to mm conversions as described by Van Brummen et.al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib8" title="">8</a>]</cite>. Scleral show was calculated only if present using the scleral and iris margins. Margin to Reflex Distance 1 and 2 (MRD 1 and 2) were calculated as the distance from the center of the iris to the superior or inferior lid.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">Inner and outer canthal distance (ICD, OCD) and interpupillary distance (IPD) were calculated as the distance between the medial canthus, lateral canthus, and iris center, respectively. Brow heights were obtained at the medial canthus, lateral canthus, and iris center. Canthal tilt, canthal height, and vertical dystopia were also measured using the medial canthus, lateral canthus, and iris center landmarks. Vertical palpebral fissure was computed as the sum of MRD 1 and 2 and the horizontal palpebral fissure was calculated using the x coordinates of the medial and lateral canthus.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Hardware</h3>
<div class="ltx_para ltx_noindent" id="S2.SS6.p1">
<p class="ltx_p" id="S2.SS6.p1.1">All batch experiments were performed on three 1080TI Nvidia GPUs. All code was written in Python 3.8 and standard machine learning packages were used for all training purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib19" title="">19</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Description of the Dataset</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Both the CFD and Celeb datasets are open source, so the only available metadata is that which was originally published with the dataset. A subset of the original Celeb dataset was randomly chosen for annotation. There are 827 and 2015 images and annotations for the CFD and Celeb dataset respectively. Within the CFD dataset, the counts of self-reported races are 109 Asian, 197 Black, 142 Indian, 108 Latin, 88 Mixed, and 183 White. Of the 827 images, 421 identified as female and 406 identified as male <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib14" title="">14</a>]</cite>. The Celeb dataset did not include any racial or gender metadata.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For both the CFD and Celeb datasets, the total number of anatomical objects annotated across all images can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.T1" title="Table 1 ‚Ä£ 3.1 Description of the Dataset ‚Ä£ 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">1</span></a>. Not all images had visible caruncles, and epicanthal lids were not annotated. An example of how different types of lids were annotated is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S6.F5" title="Figure 5 ‚Ä£ 6 Supplemental Figures ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">5</span></a>, and examples of images lacking certain annotations can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S6.F6" title="Figure 6 ‚Ä£ 6 Supplemental Figures ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:346.9pt;height:47.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.6pt,3.4pt) scale(0.875879004030638,0.875879004030638) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1" style="background-color:#FFFFFF;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.2.1" style="background-color:#FFFFFF;">Total Images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.3.1" style="background-color:#FFFFFF;">% Full Annotation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.4.1" style="background-color:#FFFFFF;">Sclera</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.5.1" style="background-color:#FFFFFF;">Iris</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.6.1" style="background-color:#FFFFFF;">Caruncle</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.7.1" style="background-color:#FFFFFF;">Brow</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.8" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.1.1.8.1" style="background-color:#FFFFFF;">Lid</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.1.1" style="background-color:#FFFFFF;">CFD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.2.1" style="background-color:#FFFFFF;">827</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.3.1" style="background-color:#FFFFFF;">0.97</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.4.1" style="background-color:#FFFFFF;">827</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.5.1" style="background-color:#FFFFFF;">827</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.6.1" style="background-color:#FFFFFF;">827</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.7.1" style="background-color:#FFFFFF;">827</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.8" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.2.1.8.1" style="background-color:#FFFFFF;">803</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.1" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.1.1" style="background-color:#FFFFFF;">Celeb</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.2.1" style="background-color:#FFFFFF;">2015</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.3.1" style="background-color:#FFFFFF;">0.89</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.4.1" style="background-color:#FFFFFF;">2002</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.5.1" style="background-color:#FFFFFF;">2002</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.6.1" style="background-color:#FFFFFF;">1884</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.7" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.7.1" style="background-color:#FFFFFF;">1994</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.8" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T1.1.1.3.2.8.1" style="background-color:#FFFFFF;">1892</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Metadata of our dataset from the two constituent open-source datasets showing the total number of images, the percent of images that have every class present, and the number of images containing each class.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Segmentation Results</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">We trained three deep learning networks for segmentation using 1) the CFD, 2) the Celeb dataset, and 3) both datasets combined. In all cases, segmentation of the iris, sclera, and brow was robust, with average Dice scores ranging from <math alttext=".78" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">.78</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn id="S3.SS2.p1.1.m1.1.1.cmml" type="float" xref="S3.SS2.p1.1.m1.1.1">.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">.78</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">.78</annotation></semantics></math> to <math alttext=".96" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">.96</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn id="S3.SS2.p1.2.m2.1.1.cmml" type="float" xref="S3.SS2.p1.2.m2.1.1">.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">.96</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">.96</annotation></semantics></math>. On the Celeb dataset, caruncle and lid segmentation were less robust on average, with Dice scores being <math alttext=".56" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">.56</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn id="S3.SS2.p1.3.m3.1.1.cmml" type="float" xref="S3.SS2.p1.3.m3.1.1">.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">.56</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">.56</annotation></semantics></math> and <math alttext=".75" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">.75</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn id="S3.SS2.p1.4.m4.1.1.cmml" type="float" xref="S3.SS2.p1.4.m4.1.1">.75</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">.75</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">.75</annotation></semantics></math> respectively. Caruncle and lid segmentation on the CFD was improved compared to the Celeb dataset, with average dice scores of <math alttext=".78" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">.78</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn id="S3.SS2.p1.5.m5.1.1.cmml" type="float" xref="S3.SS2.p1.5.m5.1.1">.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">.78</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">.78</annotation></semantics></math> and <math alttext=".87" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mn id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">.87</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><cn id="S3.SS2.p1.6.m6.1.1.cmml" type="float" xref="S3.SS2.p1.6.m6.1.1">.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">.87</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">.87</annotation></semantics></math>, respectively. Dice scores of a segmentation network trained and tested on both datasets were between those from the models trained on only the CFD or Celeb dataset. The Dice scores for all experiments can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.T2" title="Table 2 ‚Ä£ 3.2 Segmentation Results ‚Ä£ 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">2</span></a>, and histograms of all dice scores can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S6.F7" title="Figure 7 ‚Ä£ 6 Supplemental Figures ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:346.9pt;height:68.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.6pt,1.9pt) scale(0.94737361465291,0.94737361465291) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.2.1">Sclera</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.3.1">Iris</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.4.1">Brow</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.5.1">Caruncle</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.6.1">Lid</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.2.1.1">Celeb</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.2">0.78 ¬± 0.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.3">0.91 ¬± 0.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.4">0.83 ¬± 0.14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.5">0.57 ¬± 0.24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.6">0.75 ¬± 0.23</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.3.1.1">CFD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.2">0.88 ¬± 0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.3">0.96 ¬± 0.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.4">0.90 ¬± 0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.5">0.78 ¬± 0.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.6">0.87 ¬± 0.18</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.1.1">Combined</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.2">0.81 ¬± 0.11</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.3">0.93 ¬± 0.08</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.4">0.85 ¬± 0.14</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.5">0.65 ¬± 0.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.6">0.79 ¬± 0.21</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Average Dice scores for each class. Celeb denotes a model trained only on the Celeb annotations, CFD denotes a model trained only on the CFD annotations, and Combined denotes a model trained when the CFD and Celeb dataset was combined.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Intra and Intergrader Variation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.6">To evaluate the quality of our dataset, we randomly sampled 100 images and performed intergrader evaluation. When the same five annotators annotated the same 100 images, the average Dice Score between pairwise graders was <math alttext=".82\pm.01" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">.82</mn><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="float" xref="S3.SS3.p1.1.m1.1.1.2">.82</cn><cn id="S3.SS3.p1.1.m1.1.1.3.cmml" type="float" xref="S3.SS3.p1.1.m1.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">.82\pm.01</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">.82 ¬± .01</annotation></semantics></math>. The average dice score for iris, sclera, brow, lid, and caruncle annotations across all annotators was <math alttext=".94\pm.01" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mn id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">.94</mn><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.2.m2.1.1.2.cmml" type="float" xref="S3.SS3.p1.2.m2.1.1.2">.94</cn><cn id="S3.SS3.p1.2.m2.1.1.3.cmml" type="float" xref="S3.SS3.p1.2.m2.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">.94\pm.01</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">.94 ¬± .01</annotation></semantics></math>, <math alttext=".83\pm.02" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">.83</mn><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.3.m3.1.1.2.cmml" type="float" xref="S3.SS3.p1.3.m3.1.1.2">.83</cn><cn id="S3.SS3.p1.3.m3.1.1.3.cmml" type="float" xref="S3.SS3.p1.3.m3.1.1.3">.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">.83\pm.02</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">.83 ¬± .02</annotation></semantics></math>, <math alttext=".83\pm.02" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mn id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">.83</mn><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.4.m4.1.1.2.cmml" type="float" xref="S3.SS3.p1.4.m4.1.1.2">.83</cn><cn id="S3.SS3.p1.4.m4.1.1.3.cmml" type="float" xref="S3.SS3.p1.4.m4.1.1.3">.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">.83\pm.02</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">.83 ¬± .02</annotation></semantics></math>, <math alttext=".81\pm.03" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mn id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">.81</mn><mo id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.5.m5.1.1.2.cmml" type="float" xref="S3.SS3.p1.5.m5.1.1.2">.81</cn><cn id="S3.SS3.p1.5.m5.1.1.3.cmml" type="float" xref="S3.SS3.p1.5.m5.1.1.3">.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">.81\pm.03</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">.81 ¬± .03</annotation></semantics></math>, <math alttext=".71\pm.02" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mrow id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mn id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">.71</mn><mo id="S3.SS3.p1.6.m6.1.1.1" xref="S3.SS3.p1.6.m6.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="latexml" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p1.6.m6.1.1.2.cmml" type="float" xref="S3.SS3.p1.6.m6.1.1.2">.71</cn><cn id="S3.SS3.p1.6.m6.1.1.3.cmml" type="float" xref="S3.SS3.p1.6.m6.1.1.3">.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">.71\pm.02</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">.71 ¬± .02</annotation></semantics></math>. Pairwise matrices showing the Dice scores between all graders and a DeepLabV3 segmentation network can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.F3" title="Figure 3 ‚Ä£ 3.3 Intra and Intergrader Variation ‚Ä£ 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">3</span></a>. As the intergrader sample set consisted of both CFD and Celeb Dataset images, the DeepLabV3 model trained only on the individual dataset was used for each image. For example, all CFD images in the intergrader set were segmented using the DeepLabV3 model trained only on the CFD dataset. The agreement with the DeepLabV3 network between all graders was generally lower than agreement between graders, but not significantly.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="343" id="S3.F3.g1" src="extracted/5893670/intergrader.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Pairwise matrices representing intergrader agreement as the average Dice score between graders or DeepLabV3 over 100 randomly sampled images. A) The average pairwise Dice score between all graders, and B-F) represent the Dice score on the iris, sclera, brow, lid, and caruncle classes respectively.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6">We also asked each of our five annotators to annotate the same 20 images two times after at least a 2-week forgetting period. On average, the same grader annotated the same image very reproducibly with an average Dice Score of <math alttext=".81\pm.08" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">.81</mn><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">¬±</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S3.SS3.p2.1.m1.1.1.2.cmml" type="float" xref="S3.SS3.p2.1.m1.1.1.2">.81</cn><cn id="S3.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S3.SS3.p2.1.m1.1.1.3">.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">.81\pm.08</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">.81 ¬± .08</annotation></semantics></math> across all annotations (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S3.T3" title="Table 3 ‚Ä£ 3.3 Intra and Intergrader Variation ‚Ä£ 3 Results ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">3</span></a>). Across all graders, the iris was the most consistently reproduced with an average intragrader Dice score of <math alttext=".94" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">.94</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><cn id="S3.SS3.p2.2.m2.1.1.cmml" type="float" xref="S3.SS3.p2.2.m2.1.1">.94</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">.94</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">.94</annotation></semantics></math>. Sclera and lid annotations were reproduced with an average Dice score of <math alttext=".82" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mn id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">.82</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><cn id="S3.SS3.p2.3.m3.1.1.cmml" type="float" xref="S3.SS3.p2.3.m3.1.1">.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">.82</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">.82</annotation></semantics></math> and <math alttext=".83" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">.83</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><cn id="S3.SS3.p2.4.m4.1.1.cmml" type="float" xref="S3.SS3.p2.4.m4.1.1">.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">.83</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">.83</annotation></semantics></math> across all graders, while brow and caruncle were the most challenging anatomical part to reproduce having an average intragrader Dice score of <math alttext=".77" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">.77</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><cn id="S3.SS3.p2.5.m5.1.1.cmml" type="float" xref="S3.SS3.p2.5.m5.1.1">.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">.77</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">.77</annotation></semantics></math> and <math alttext=".71" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><mn id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">.71</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><cn id="S3.SS3.p2.6.m6.1.1.cmml" type="float" xref="S3.SS3.p2.6.m6.1.1">.71</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">.71</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">.71</annotation></semantics></math> respectively.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:216.8pt;height:116.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.9pt,4.8pt) scale(0.924287915953175,0.924287915953175) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.2" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.2.1" style="background-color:#FFFFFF;">Brow</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.3" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.3.1" style="background-color:#FFFFFF;">Sclera</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.4" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.4.1" style="background-color:#FFFFFF;">Iris</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.5" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.5.1" style="background-color:#FFFFFF;">Caruncle</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1.6" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.6.1" style="background-color:#FFFFFF;">Lid</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.2.2.1.1" style="background-color:#FFFFFF;">Grader 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.2.2.2.1" style="background-color:#FFFFFF;">0.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.2.2.3.1" style="background-color:#FFFFFF;">0.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.2.2.4.1" style="background-color:#FFFFFF;">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.2.2.5.1" style="background-color:#FFFFFF;">0.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.2.2.6.1" style="background-color:#FFFFFF;">0.85</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.3.1.1" style="background-color:#FFFFFF;">Grader 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.3.3.2.1" style="background-color:#FFFFFF;">0.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.3.3.3.1" style="background-color:#FFFFFF;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.3.3.4.1" style="background-color:#FFFFFF;">0.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.3.3.5.1" style="background-color:#FFFFFF;">0.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.3.3.6.1" style="background-color:#FFFFFF;">0.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.4.1.1" style="background-color:#FFFFFF;">Grader 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.4.4.2.1" style="background-color:#FFFFFF;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.4.4.3.1" style="background-color:#FFFFFF;">0.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.4.4.4.1" style="background-color:#FFFFFF;">0.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.4.4.5.1" style="background-color:#FFFFFF;">0.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.4.4.6.1" style="background-color:#FFFFFF;">0.72</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.1.1" style="background-color:#FFFFFF;">Grader 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.5.5.2.1" style="background-color:#FFFFFF;">0.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.5.5.3.1" style="background-color:#FFFFFF;">0.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.5.5.4.1" style="background-color:#FFFFFF;">0.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.5.5.5.1" style="background-color:#FFFFFF;">0.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.5.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.5.5.6.1" style="background-color:#FFFFFF;">0.90</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.6.1.1" style="background-color:#FFFFFF;">Grader 5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.6.6.2.1" style="background-color:#FFFFFF;">0.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.6.6.3.1" style="background-color:#FFFFFF;">0.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.6.6.4.1" style="background-color:#FFFFFF;">0.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.6.6.5.1" style="background-color:#FFFFFF;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.6.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.6.6.6.1" style="background-color:#FFFFFF;">0.88</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.1" style="background-color:#FFFFFF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.7.7.1.1" style="background-color:#FFFFFF;">Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.2" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.7.7.2.1" style="background-color:#FFFFFF;">0.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.3" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.7.7.3.1" style="background-color:#FFFFFF;">0.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.4" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.7.7.4.1" style="background-color:#FFFFFF;">0.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.5" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.7.7.5.1" style="background-color:#FFFFFF;">0.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.7.7.6" style="background-color:#FFFFFF;"><span class="ltx_text" id="S3.T3.1.1.7.7.6.1" style="background-color:#FFFFFF;">0.83</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Intragrader agreement represented as Dice score. Each grader annotated the same 20 images after a two-week forgetting period, and the Dice score is shown for each class. Average represents the average intragrader agreement for each class.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In the literature, periorbital segmentation is a primary intermediate step for periorbital distance prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib20" title="">20</a>]</cite>. Periorbital distances are highly time-consuming to obtain clinically, yet they provide valuable information in the clinical decision-making process with respect to disease tracking and treatment monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib21" title="">21</a>]</cite>. Prior studies using segmentation as an intermediate step for periorbital distance prediction currently rely on splitting relatively rare clinical data into train and test sets. This requires using a large amount of a precious resource for training purposes and limits the size of the test set, making it challenging to assess the generalizability of such models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib22" title="">22</a>]</cite>. Furthermore, it has been shown that training open-source datasets can lead to accurate segmentation on images of craniofacial and oculoplastic pathology collected both in the wild and using professional photographers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib23" title="">23</a>]</cite>. However, there is currently an absence of open-source datasets designed specifically for periorbital segmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In existing datasets, the lack of caruncle annotation often leads to the medial canthus not being well defined and lid margins that are not adequately demarcated by existing annotations. To solve this, we have leveraged existing open-source datasets to develop a comprehensive dataset designed for medical grade segmentation of relevant periorbital anatomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib14" title="">14</a>]</cite>. On the CFD and Celeb dataset we have created iris, lid, sclera, brow, and caruncle annotations in accordance with standard accepted definitions of this anatomy and through consultation with oculoplastic surgeons at our institution. We only annotated the object if it was present in the image. For example, if the eye was closed, no sclera or iris would be annotated. We also only annotated lids and portions of lids that were defined by a clear lid crease. As such, epicanthal lids were not annotated. An example of the criteria used for lid annotation can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S6.F5" title="Figure 5 ‚Ä£ 6 Supplemental Figures ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">5</span></a>. While the CFD consists of images that are directly facing forward, the Celeb dataset contains images of subjects at various angles and lighting conditions. For this reason, incorporating the Celeb dataset into training strategies may be a valuable approach for training models with the utility on in-the-wild clinical images as the distribution of images from this dataset is broader.
To demonstrate the efficacy of these annotations for training segmentation models, we trained networks on both the CFD and Celeb datasets. We achieved robust segmentation on all the anatomical objects of interest. In our prior work, we have shown that accurate sclera and brow segmentation can be achieved on images displaying pathology through training on as few as 1000 open-source images. We replicated this finding using our dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#bib.bib23" title="">23</a>]</cite>. The models we trained here did have lower performance on lids and caruncles relative to the iris, sclera, and brows. As such, we plan to expand this dataset to include more annotations for lids and caruncles and have provided all the relevant CVAT documentation to allow for community engagement in growing the dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">As our dataset was built on originally open-source datasets, we only include metadata available in the original publications. Of the two datasets we compiled, only the CFD dataset included metadata such as the self-reported racial and gender identity of the patients. The CFD component of our dataset is split <math alttext=".51" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">.51</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn id="S4.p3.1.m1.1.1.cmml" type="float" xref="S4.p3.1.m1.1.1">.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">.51</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">.51</annotation></semantics></math> to <math alttext=".49" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">.49</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn id="S4.p3.2.m2.1.1.cmml" type="float" xref="S4.p3.2.m2.1.1">.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">.49</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">.49</annotation></semantics></math> percent female to male, and there are a range of racial identities represented. This metadata may be useful in evaluating any potential bias of future segmentation models trained for periorbital segmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">We have validated the annotations through intra and intergrader comparisons and showed that our annotations are both high quality and reproducible. However, there is some inherent variability when the different graders annotate the same image and when the same annotator annotates the same image twice. This effect was most pronounced for the caruncle, brow, and lid annotations. Even though the graders were well trained, there is still inherent subjectivity in demarcating these anatomical structures on close-up images, particularly when the images are lower resolution (as is the case with the Celeb dataset) and the structures are relatively small compared to the entire image. Additionally, the agreement with the DeepLabV3 model with all graders was the lowest on average, indicating that while the deep learning models perform relatively well, there is still room for significant improvement in model development. On intragrader analysis, variation between two annotation sessions is most pronounced with the brow and caruncle classes, likely due to the nature of these classes.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S4.F4.g1" src="extracted/5893670/periorbital.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Periorbital distances on two images from the CFD dataset. These distances can be calculated using the toolkit, which we have made available via API, and the periorbital distances from the CFD dataset have been released as a benchmark dataset.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">In addition to open-sourcing our datasets, we have also created a toolkit for distance prediction from periorbital segmentation masks. This code has been released via an application programming interface (API) so others in the community can use it as a tool in future work related to the prediction of periorbital distances. Within the API, our models can be accessed, and full facial images or cropped images can be submitted for segmentation with or without periorbital distance calculation from the masks. Additionally, users can submit segmentation masks from their own models to obtain the periorbital distances using our analysis pipeline. The Python package can be found <a class="ltx_ref ltx_href" href="https://pypi.org/project/periorbital-package/" title="">here</a>. An example of an image with periorbital distances denoted by the toolkit can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.20407v2#S4.F4" title="Figure 4 ‚Ä£ 4 Discussion ‚Ä£ Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications"><span class="ltx_text ltx_ref_tag">4</span></a>. To provide a benchmark for iterative improvement, we have released the periorbital distances of the ground truth segmentation masks for the CFD images in our dataset as these images are of individuals facing forward in a standard position. We hope that the community will find utility in these annotations, the toolkit, and the benchmark distances for future research in this space.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Dataset and Code Availability</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The entire dataset is available to download <a class="ltx_ref ltx_href" href="https://zenodo.org/records/13916845" title="">from Zenodo</a>. Code used to train all models can be found at <a class="ltx_ref ltx_href" href="https://github.com/aiolab/periorbital-dataset" title="">the Artificial Intelligence in Ophthalmology center‚Äôs Github.</a></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Prasanna Porwal.

</span>
<span class="ltx_bibblock">Indian diabetic retinopathy image dataset (IDRiD), 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peyman Gholami, Priyanka Roy, Mohana¬†Kuppuswamy Parthasarathy, and Vasudevan Lakshminarayanan.

</span>
<span class="ltx_bibblock">OCTID: Optical coherence tomography image database, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Saad¬†M. Khan, Xiaoxuan Liu, Siddharth Nath, Edward Korot, Livia Faes, Siegfried¬†K. Wagner, Pearse¬†A. Keane, Neil¬†J. Sebire, Matthew¬†J. Burton, and Alastair¬†K. Denniston.

</span>
<span class="ltx_bibblock">A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">The Lancet Digital Health</span>, 3(1):e51‚Äìe66, 2021.

</span>
<span class="ltx_bibblock">Publisher: Elsevier.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bettina Selig, Koenraad¬†A. Vermeer, Bernd Rieger, Toine Hillenaar, and Cris¬†L. Luengo¬†Hendriks.

</span>
<span class="ltx_bibblock">Fully automatic evaluation of the corneal endothelium from in vivo confocal microscopy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">BMC Medical Imaging</span>, 15(1):13, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Radovan Fusek.

</span>
<span class="ltx_bibblock">Pupil localization using geodesic distance.

</span>
<span class="ltx_bibblock">In George Bebis, Richard Boyle, Bahram Parvin, Darko Koracin, Matt Turek, Srikumar Ramalingam, Kai Xu, Stephen Lin, Bilal Alsallakh, Jing Yang, Eduardo Cuervo, and Jonathan Ventura, editors, <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Advances in Visual Computing</span>, pages 433‚Äì444. Springer International Publishing, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hugo Proen√ßa, S√≠lvio Filipe, Ricardo Santos, Jo√£o Oliveira, and Lu√≠s¬†A. Alexandre.

</span>
<span class="ltx_bibblock">The UBIRIS.v2: a database of visible wavelength iris images captured on-the-move and at-a-distance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 32(8):1529‚Äì1535, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo.

</span>
<span class="ltx_bibblock">MaskGAN: Towards diverse and interactive facial image manipulation, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Alexandra Van¬†Brummen, Julia¬†P. Owen, Theodore Spaide, Colin Froines, Randy Lu, Megan Lacy, Marian Blazes, Emily Li, Cecilia¬†S. Lee, Aaron¬†Y. Lee, and Matthew Zhang.

</span>
<span class="ltx_bibblock">PeriorbitAI: Artificial intelligence automation of eyelid and periorbital measurements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">American journal of ophthalmology</span>, 230:285‚Äì296, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ji¬†Shao, Xingru Huang, Tao Gao, Jing Cao, Yaqi Wang, Qianni Zhang, Lixia Lou, and Juan Ye.

</span>
<span class="ltx_bibblock">Deep learning-based image analysis of eyelid morphology in thyroid-associated ophthalmopathy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Quantitative Imaging in Medicine and Surgery</span>, 13(3):1592‚Äì1604, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ji¬†Shao, Jing Cao, Changjun Wang, Peifang Xu, Lixia Lou, and Juan Ye.

</span>
<span class="ltx_bibblock">Automatic measurement and comparison of normal eyelid contour by age and gender using image-based deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Ophthalmology Science</span>, 4(5):100518, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K¬†Boboridis, A¬†Assi, A¬†Indar, C¬†Bunce, and A¬†Tyers.

</span>
<span class="ltx_bibblock">Repeatability and reproducibility of upper eyelid measurements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">The British Journal of Ophthalmology</span>, 85(1):99‚Äì101, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Debbie¬†S. Ma, Joshua Correll, and Bernd Wittenbrink.

</span>
<span class="ltx_bibblock">The chicago face database: A free stimulus set of faces and norming data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Behavior Research Methods</span>, 47(4):1122‚Äì1135, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Debbie¬†S. Ma, Justin Kantner, and Bernd Wittenbrink.

</span>
<span class="ltx_bibblock">Chicago face database: Multiracial expansion.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Behavior Research Methods</span>, 53(3):1289‚Äì1300, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Anjana Lakshmi, Bernd Wittenbrink, Joshua Correll, and Debbie¬†S. Ma.

</span>
<span class="ltx_bibblock">The india face set: International and cultural boundaries impact face impressions and perceptions of category membership.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Front. Psychol.</span>, 12, 2021.

</span>
<span class="ltx_bibblock">Publisher: Frontiers.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko, and Matthias Grundmann.

</span>
<span class="ltx_bibblock">Real-time facial surface geometry from monocular video on mobile GPUs, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
TorchVision Maintainers and contributors.

</span>
<span class="ltx_bibblock">TorchVision: PyTorch‚Äôs computer vision library, 2016.

</span>
<span class="ltx_bibblock">original-date: 2016-11-09T23:11:43Z.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu¬†Fang, Junjie Bai, and Soumith Chintala.

</span>
<span class="ltx_bibblock">PyTorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</span>, volume¬†32. Curran Associates, Inc., 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Charles¬†R. Harris, K.¬†Jarrod Millman, St√©fan¬†J. van¬†der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel¬†J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten¬†H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime¬†Fern√°ndez del R√≠o, Mark Wiebe, Pearu Peterson, Pierre G√©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis¬†E. Oliphant.

</span>
<span class="ltx_bibblock">Array programming with NumPy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Nature</span>, 585(7825):357‚Äì362, 2020.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and √âdouard Duchesnay.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine learning in python.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Journal of Machine Learning Research</span>, 12(85):2825‚Äì2830, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Khizar Rana, Mark Beecher, Carmelo Caltabiano, Carmelo Macri, Yang Zhao, Johan Verjans, and Dinesh Selva.

</span>
<span class="ltx_bibblock">Artificial intelligence to automate assessment of ocular and periocular measurements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">European Journal of Ophthalmology</span>, page 11206721241249773, 2024.

</span>
<span class="ltx_bibblock">Publisher: SAGE Publications.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Hung-Chang Chen, Shin-Shi Tzeng, Yen-Chang Hsiao, Ruei-Feng Chen, Erh-Chien Hung, and Oscar¬†K Lee.

</span>
<span class="ltx_bibblock">Smartphone-based artificial intelligence‚Äìassisted prediction for eyelid measurements: Algorithm development and observational validation study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">JMIR mHealth and uHealth</span>, 9(10):e32444, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mohammad Alauthman, Ahmad Al-qerem, Bilal Sowan, Ayoub Alsarhan, Mohammed Eshtay, Amjad Aldweesh, and Nauman Aslam.

</span>
<span class="ltx_bibblock">Enhancing small medical dataset classification performance using GAN.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Informatics</span>, 10(1):28, 2023.

</span>
<span class="ltx_bibblock">Number: 1 Publisher: Multidisciplinary Digital Publishing Institute.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
George¬†R. Nahass, Ghasem Yazdanpanah, Madison Cheung, Alex Palacios, Jeffery Peterson, Kevin Heinze, Sasha Hubschman, Chad¬†A. Purnell, Pete Setabutr, Ann¬†Q. Tran, and Darvin Yi.

</span>
<span class="ltx_bibblock">State-of-the-art periorbital distance prediction and disease classification using periorbital features, 2024.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Supplemental Figures</h2>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="171" id="S6.F5.g1" src="extracted/5893670/lid_variation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of how different types of lids were annotated using the lid crease as a guide. A) No visible lid crease, so no annotation was created. B) When the lid crease was present, the lid annotation included any visible lashes so the boundaries of the masks align with the sclera mask C) In the event of a partially visible lid crease, only regions of the lid inferior to the lid crease were annotated.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S6.F6.g1" src="extracted/5893670/no_annots.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples of images lacking certain annotations. A) Image lacking brow annotations, B) Image lacking caruncle annotations, C) Image lid annotations D) Image lacking sclera and iris annotations</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="384" id="S6.F7.g1" src="extracted/5893670/dice_histos.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Histograms of the Dice score for each class from each model trained.</figcaption>
</figure>
</section><div about="" class="ltx_rdf" content="David S.¬†Hippocampus, Elias D.¬†Striatum" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="First keyword, Second keyword, More" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="q-bio.NC, q-bio.QM" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="A template for the arxiv style" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 21:46:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
