<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling</title>
<!--Generated on Sat Sep 21 15:25:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Large Language Models,  Telecommunication,  RAG,  Question Masked Loss,  Option Batch-Shuffle Trick
" lang="en" name="keywords"/>
<base href="/html/2409.14175v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S1" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S2" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS1" title="In III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Retrieval-Augmented Generation Architecture</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS1.SSS1" title="In III-A Retrieval-Augmented Generation Architecture ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span>Splitting Documents into Chunks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS1.SSS2" title="In III-A Retrieval-Augmented Generation Architecture ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span>Creating chunk embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS1.SSS3" title="In III-A Retrieval-Augmented Generation Architecture ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span>Chunk retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS2" title="In III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Model Prompt</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS2.SSS1" title="In III-B Model Prompt ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Phi-2 model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.SS2.SSS2" title="In III-B Model Prompt ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Falcon-7B</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S4" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">QMOS Finetuning approach</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S4.SS1" title="In IV QMOS Finetuning approach ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Phi-2 Finetuning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S4.SS1.SSS1" title="In IV-A Phi-2 Finetuning ‣ IV QMOS Finetuning approach ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>QLoRa</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S4.SS1.SSS2" title="In IV-A Phi-2 Finetuning ‣ IV QMOS Finetuning approach ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>LoRa</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S4.SS2" title="In IV QMOS Finetuning approach ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Option Batch-Shuffle Trick</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.SS1" title="In V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Phi-2 model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.SS2" title="In V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Falcon-7B</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S6" title="In QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\UseRawInputEncoding</span>
</div>
<h1 class="ltx_title ltx_title_document">QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Blessed Guda<sup class="ltx_sup" id="id1.1.id1">*</sup>, Gabrial Zencha Ashungafac<sup class="ltx_sup" id="id2.2.id2">*</sup>, Lawrence Francis<sup class="ltx_sup" id="id3.3.id3">*</sup>, Carlee Joe-Wong<sup class="ltx_sup" id="id4.4.id4">**</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id5.5.id1">*</sup>These authors contributed equally to this work.<sup class="ltx_sup" id="id6.6.id1">**</sup>Mentor of the Team.
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id7.7.id1">College of Engineering, Carnegie Mellon University</span>
<br class="ltx_break"/>blessedg@andrew.cmu.edu, gzenchaa@andrew.cmu.edu, lfrancis@andrew.cmu.edu, cjoewong@andrew.cmu.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">Large Language models (LLMs) have brought about substantial advancements in the field of Question Answering (QA) systems. These models do remarkably well in addressing intricate inquiries in a variety of disciplines. However, because of domain-specific vocabulary, complex technological concepts, and the requirement for exact responses, applying LLMs to specialized sectors like telecommunications presents additional obstacles. GPT-3.5 has been used in recent work, to obtain noteworthy accuracy for telecom-related questions in a Retrieval Augmented Generation (RAG) framework. Notwithstanding these developments, the practical use of models such as GPT-3.5 is restricted by their proprietary nature and high computing demands. This paper introduces QMOS, an innovative approach which uses a Question-Masked loss and Option Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice Questions in the telecommunications domain. Our focus was on using open-source, smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework. Our multi-faceted approach involves several enhancements to the whole LLM-RAG pipeline of finetuning, retrieval, prompt engineering and inference.
Our approaches significantly outperform existing results, achieving accuracy improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07% to 84.65% with Phi-2.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large Language Models, Telecommunication, RAG, Question Masked Loss, Option Batch-Shuffle Trick

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The field of Question Answering (QA) systems has witnessed significant advancements with the advent of large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib1" title="">1</a>]</cite>. These models have demonstrated remarkable capabilities in understanding and responding to complex queries across various domains. However, their application to specialized fields, such as telecommunications, presents unique challenges due to domain-specific terminology, intricate technical concepts, and the need for precise, accurate responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib2" title="">2</a>]</cite>.
Recent research has shown promising results in applying LLMs to telecom-specific QA tasks.
A notable study, Telco-RAG achieved accuracies around 75% using GPT-3.5 within a Retrieval Augmented Generation (RAG) framework for telecom-related questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite>.
While this demonstrates the potential of LLMs in domain-specific applications, the computational demands and resource requirements of such models often pose significant challenges for practical, widespread implementation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib4" title="">4</a>]</cite>. Also, the use of GPT-3.5 deemphasizes open-source AI, as the training data details and architecture of GPT-3.5 are not open.
This paper explores an innovative approach to addressing multiple-choice questions (MCQs) in the telecommunications domain using <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">open, small language models within an enhanced RAG framework.</span>
Answering telecommunications standards questions presents a particularly challenging environment for LLMs due to its frequent use of unique abbreviations and rapidly evolving technologies and regulatory considerations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib2" title="">2</a>]</cite>. To further complicate the problem, LLMs have been shown to demonstrate selection bias when answering MCQs which significantly degrades performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib5" title="">5</a>]</cite>. In addition, due to rapid changes and data sparsity, large language models will require frequent fine-tuning to keep them up to date with latest telecommunications standard.
Our research aims to achieve competitive performance of larger models while maintaining efficiency and reducing computational costs. We focus on the Phi-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib6" title="">6</a>]</cite> and Falcon-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib7" title="">7</a>]</cite>, which have shown promise in achieving competitive performance with significantly fewer parameters than their larger counterparts.
Our work addresses these challenges through a multi-faceted approach called <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p1.1.2">QMOS</span> that combines several novel techniques:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We leverage multiple embedding models to diversify and enrich the documents retrieved by the RAG system, potentially capturing a broader range of relevant information.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Due to the high number of abbreviations used in Telecommunication standards, we enhance the dictionary of abbreviations used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite> to boost the hit rate of successful abbreviations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We meticulously design our prompts to guide the model to reason over the documents when selecting the answer.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We employ LoRA (Low-Rank Adaptation) fine-tuning on the Phi-2 model with <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">a question-masked loss function</span> to efficiently adapt it to the telecommunications domain.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We implement an innovative optimization technique: inference and train time option batch-shuffling, which enhances the accuracy of the Phi-2 model by eliminating bias in option position of correct answers.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">By focusing on small language models and employing these advanced techniques, our work addresses the growing need for efficient, scalable NLP solutions in specialized domains. This research has significant implications for developing cost-effective QA systems that can be deployed in resource-constrained environments while maintaining high accuracy.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Small language models have emerged as a promising alternative to large models for domain-specific applications. Models like Phi-2, Falcon-7B and TinyLlama-1.1B<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib8" title="">8</a>]</cite> offer advantages such as resource efficiency, faster inference times, and easier fine-tuning for specific domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib9" title="">9</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib10" title="">10</a>]</cite> Piovesan et al. (2024) conducted a comprehensive evaluation of the small language model Phi-2 in the telecommunications domain, comparing it to larger models like GPT-3.5 and GPT-4. Their findings demonstrate that Phi-2, despite being significantly smaller, achieved an overall accuracy of 52.30% on the TeleQnA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib2" title="">2</a>]</cite>, compared to 67.29% for GPT-3.5 and 74.91% for GPT-4. Notably, when enhanced with Retrieval-Augmented Generation (RAG), Phi-2’s performance in the challenging ’Standards Specifications’ category improved from 44.27% to 56.63%, nearly matching GPT-3.5’s 56.97%. In another study, Ahmed et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib11" title="">11</a>]</cite> conducted a comprehensive evaluation of several small language models, including Falcon 7B, in the telecommunications domain. Their findings demonstrate that Falcon 7B, despite having 7 billion parameters, achieved an overall accuracy of only 15.70% on the TeleQnA dataset, significantly lower than larger models like GPT-3.5 (67.29%) and GPT-4 (74.91%). While there is limited study on improving the small language model for telecom application, there has been an exhausive study in improving the performance or large language models. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib12" title="">12</a>]</cite> Soudani et al show that finetuning and RAG are both feasible methods in improving the performance of language models.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed Methodology</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Retrieval-Augmented Generation Architecture</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Retrieval-Augmented Generation is an interesting technique used to enhance the performance of large language models on tasks where the required knowledge was not present in the training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib13" title="">13</a>]</cite>.
RAG is achieved by integrating external knowledge sources in the prompt from which the LLM’s result is generated. In question answering tasks, questions are augmented with contextually relevant texts from external documents when creating prompts.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.4.1.1">III-A</span>1 </span>Splitting Documents into Chunks</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">For RAG, we only need relevant parts of a large number of documents, and this requires a search over these documents.
A common approach being employed by commercial tools for RAG such as LlamaIndex<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib14" title="">14</a>]</cite> and LangChain<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib15" title="">15</a>]</cite> splits the document such that each chunk contains a certain number of characters with an overlap between chunks. The parameters ’chunk-size’ and ’chunk-overlap’ are often adjusted such that the split leads to meaningful chunks (’chunk-size’ specifies the number of characters in a chunk of text while ’chunk-overlap’ specifies the overlap of texts between two contiguous chunks). We employed <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">a custom document splitting strategy </span>where we first split the documents into individual sections, excluding the table of contents. Each section is further split into chunks containing ‘chunk-size’ characters (in our setting, chunk-size = 1024). Additionally we ensured that each chunk begins with the heading of the document section to which it belongs by prepending this heading to each chunk.This chunking strategy is inspired by our analysis of the structure of the 3GPP standard documents. Also, we observe that the first few pages of the documents containing sections like the title page, scope, references and table of contents are not very informative and as such we do not include those sections when creating the chunks. Having obtained the necessary chunks, we proceed to create vector embeddings of these chunks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.4.1.1">III-A</span>2 </span>Creating chunk embeddings</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">To enable similarity search, we created associated embeddings for each chunk using an embedding model, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.F1" title="Figure 1 ‣ III-A2 Creating chunk embeddings ‣ III-A Retrieval-Augmented Generation Architecture ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">1</span></a>. The choice of embedding model was influenced by models on the Massive Text Embedding Benchmark (MTEB) leaderboard  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib16" title="">16</a>]</cite>. We opted for the best performing and easily accessible models, favouring the use of the models ’stella_en_400M_v5’ and ’gte-Qwen2-1.5B-instruct’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib17" title="">17</a>]</cite> with 400M and 1.5B parameters respectively. We used these text embedding models from the ’Sentence Transformer’ library  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib18" title="">18</a>]</cite> and sped up the embedding process by batching the chunks, using a batch size of 64. The chunks and their corresponding embeddings were saved to disk to be used for context retrieval for the question answering task.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="341" id="S3.F1.g1" src="extracted/5870255/chunking_drawio_1.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Document splitting and chunk embedding process</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.4.1.1">III-A</span>3 </span>Chunk retrieval</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">For the chunk retrieval process, we employed a k-Nearest Neighbors (KNN) approach on the similarity scores between a question/query embedding and the chunk embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib19" title="">19</a>]</cite>. The use of a dot product similarity score implies that higher scores denote higher similarity. Hence, we retrieve the top <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.1.m1.1"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mi id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><ci id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.1.m1.1d">italic_k</annotation></semantics></math> similar chunks for a given question/query. The number of retrieved chunks is chosen such that the context length of the language model is not exceeded when creating the prompt. We used the top 2 chunks retrieved with each embedding model. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S3.F2" title="Figure 2 ‣ III-A3 Chunk retrieval ‣ III-A Retrieval-Augmented Generation Architecture ‣ III Proposed Methodology ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">2</span></a> shows how chunks are retrieved to form a context in the input prompt to an LLM. Additionally, we employed the use of the BM25  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib20" title="">20</a>]</cite> algorithm which is a statistical approach for information retrieval that measures similarity based on the frequency of terms from the query that appear in the chunks. The motivation behind this is to ensure that the retrieval also includes chunks containing specific terms used in the query but are not necessarily enforced in the neural embedding models.This enabled us to create a context that consists of chunks from 2 embedding models (stella_en_400M_v5 and gte-Qwen2-1.5B-instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib17" title="">17</a>]</cite>) and BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="178" id="S3.F2.g1" src="extracted/5870255/systemdiag.drawio.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Chunk retrieval process: The top k chunks are retrieved using the KNN algorithm. Scores for each chunk is obtained from the dot product <math alttext="q.K^{T}" class="ltx_Math" display="inline" id="S3.F2.2.m1.2"><semantics id="S3.F2.2.m1.2b"><mrow id="S3.F2.2.m1.2.2.1" xref="S3.F2.2.m1.2.2.2.cmml"><mi id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">q</mi><mo id="S3.F2.2.m1.2.2.1.2" lspace="0em" rspace="0.167em" xref="S3.F2.2.m1.2.2.2a.cmml">.</mo><msup id="S3.F2.2.m1.2.2.1.1" xref="S3.F2.2.m1.2.2.1.1.cmml"><mi id="S3.F2.2.m1.2.2.1.1.2" xref="S3.F2.2.m1.2.2.1.1.2.cmml">K</mi><mi id="S3.F2.2.m1.2.2.1.1.3" xref="S3.F2.2.m1.2.2.1.1.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.2c"><apply id="S3.F2.2.m1.2.2.2.cmml" xref="S3.F2.2.m1.2.2.1"><csymbol cd="ambiguous" id="S3.F2.2.m1.2.2.2a.cmml" xref="S3.F2.2.m1.2.2.1.2">formulae-sequence</csymbol><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">𝑞</ci><apply id="S3.F2.2.m1.2.2.1.1.cmml" xref="S3.F2.2.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.F2.2.m1.2.2.1.1.1.cmml" xref="S3.F2.2.m1.2.2.1.1">superscript</csymbol><ci id="S3.F2.2.m1.2.2.1.1.2.cmml" xref="S3.F2.2.m1.2.2.1.1.2">𝐾</ci><ci id="S3.F2.2.m1.2.2.1.1.3.cmml" xref="S3.F2.2.m1.2.2.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.2d">q.K^{T}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.m1.2e">italic_q . italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math>. The retrieved chunks are then used to create the input prompt to the LLM</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Model Prompt</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Prompt engineering is crucial in RAG-based LLM systems as it significantly enhances their performance and reliability. By carefully crafting prompts, engineers can guide LLMs to produce more accurate, relevant, and contextually appropriate responses without the need for extensive finetuning  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib21" title="">21</a>]</cite>. This is particularly important in RAG systems, where the integration of external knowledge sources in the prompts helps mitigate issues like hallucinations and factual inaccuracies  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib22" title="">22</a>]</cite>. In designing our prompt we make the following considerations:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Question Repitition: </span> we draw from the observation of  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite>, which showed that in answering Telecommunication questions, repeating the question before and after the contexts helps make the Phi-2 model reason over the contexts for the answer.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Enhanced Abbreviation Expansion: </span> As we noticed that a lot of the questions in the TeleQnA dataset are about abbreviations, we also decide to include the abbreviations in the prompt like  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite>. However, we notice that the method in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite> missed a lot of abbreviations because of the insufficient dictionary of abbreviations. This is because the abbreviation dictionary used was generated by only considering the Vocabulary for 3GPP specifications document<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib23" title="">23</a>]</cite>. We expand this dictionary by searching for other abbreviations in the ”<span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">Definitions of terms, symbols and abbreviations</span>” sections of all the documents. With this enhanced abbreviation dictionary, we are able to achieve a hit rate of <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.3">95.16%</span> in the test set which is a significant improvement over the <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.4">63.74%</span> achieved using the approach in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Model Pretraining format</span>: We consider how the prompts in the training phase of the model were structured. This is crucial because presenting the model with prompts structured based on how it’s trained can help the model perform better.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Phi-2 model</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">With the above considerations, we design our prompt for Phi-2 as:</p>
</div>
<div class="ltx_logical-block ltx_framed ltx_framed_rectangle" id="S3.SS2.SSS1.1" style="border-color: #000000;padding:9pt;border-width:0.4pt;">
<div class="ltx_para" id="S3.SS2.SSS1.1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.1">Instruct: **<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.1.p1.1.1">Question</span>**</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.2">Abbreviations:</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.3">**<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.1.p1.3.1">abbreviation: full form</span>**</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.4">Considering the following retrieved contexts</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.5">context 1: context…</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.6">context 2: context…</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.7">**<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.1.p1.7.1">Question</span>**</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.8">** option 1: ….</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.9">** option 2: ….</p>
<p class="ltx_p" id="S3.SS2.SSS1.1.p1.10">Output :</p>
</div>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Falcon-7B</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">We make slight modifications to the prompt for the Falcon 7B based on its poor performance on the TeleQnA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib11" title="">11</a>]</cite>. We observed that when provided with the options, the model does not do well therefore, we do not provide the options to the model.</p>
</div>
<div class="ltx_logical-block ltx_framed ltx_framed_rectangle" id="S3.SS2.SSS2.1" style="border-color: #000000;padding:9pt;border-width:0.4pt;">
<div class="ltx_para" id="S3.SS2.SSS2.1.p1">
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.1">Youre a Telecommunication standards expert. Please answer the question first consider the given context for the answer.</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.2">**<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.1.p1.2.1">Question</span>**</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.3">Abbreviations:</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.4">**<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.1.p1.4.1">abbreviation: full form</span>**</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.5">Considering the following retrieved contexts</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.6">context 1: context…</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.7">context 2: context…</p>
<p class="ltx_p" id="S3.SS2.SSS2.1.p1.8">**<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.1.p1.8.1">Question</span>**</p>
</div>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">From the answer sentence generated by the model, we compute the embeddings of the generated answer sentence and take the answer option that has the highest similarity. We use a simple sentence BERT model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib24" title="">24</a>]</cite> for this answer extraction.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">QMOS Finetuning approach</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Phi-2 Finetuning</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To finetune the Phi-2 model, we used LoRA (Low-Rank Adaptation)  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib25" title="">25</a>]</cite> fine-tuning approaches instead of full finetuning due to the size of the model and the small size of the training data. We explored both LoRa and Quantized LoRa (QLoRa)  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib26" title="">26</a>]</cite> fine-tuning. We did not explore fine-tuning the Falcon 7B model, due to its large size which exceeds our computational budget.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>QLoRa</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning technique for large language models. It combines quantization and low-rank adaptation to significantly reduce memory requirements and computational costs. In QLoRA, the base model’s weights are quantized to 4-bit precision and frozen. Then, small trainable ”adapter” layers are added using low-rank decomposition. We envision that these adapters would capture the Telecommunication MCQ reasoning ability during fine-tuning while keeping most of the model fixed.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">We configure the model for QLoRA fine-tuning with the following parameters:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Low Rank: Set to 64 to balance between performance and computational efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Alpha: Set to 16 to scale the low-rank updates.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Dropout: Set to 0.05 to prevent overfitting during training.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Adapter Layers: We add adapter layers to the Query, Key, and Value and Feedforward Weights of the Transformer layers of the Phi-2 model.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>LoRa</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The use of QLoRA finetuning reduces the memory requirement during training. However, the training time is considerably increased due to the quantization and dequantization operations being performed during QLoRA finetuning. In order to investigate the influence of the finetuning objective on model’s performance on MCQs, we used LoRA since it offers faster training time compared to QLoRA. We compared the model’s performance when the training objective only considers the answers versus when the entire prompt and answer are considered in the objective.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">The original cross-entropy loss for next token prediction is defined as:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=-\sum_{t=1}^{T}y_{t}\log(\hat{y}_{t})" class="ltx_Math" display="block" id="S4.Ex1.m1.2"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">ℒ</mi><mo id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml">=</mo><mrow id="S4.Ex1.m1.2.2.1" xref="S4.Ex1.m1.2.2.1.cmml"><mo id="S4.Ex1.m1.2.2.1a" xref="S4.Ex1.m1.2.2.1.cmml">−</mo><mrow id="S4.Ex1.m1.2.2.1.1" xref="S4.Ex1.m1.2.2.1.1.cmml"><munderover id="S4.Ex1.m1.2.2.1.1.2" xref="S4.Ex1.m1.2.2.1.1.2.cmml"><mo id="S4.Ex1.m1.2.2.1.1.2.2.2" movablelimits="false" xref="S4.Ex1.m1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S4.Ex1.m1.2.2.1.1.2.2.3" xref="S4.Ex1.m1.2.2.1.1.2.2.3.cmml"><mi id="S4.Ex1.m1.2.2.1.1.2.2.3.2" xref="S4.Ex1.m1.2.2.1.1.2.2.3.2.cmml">t</mi><mo id="S4.Ex1.m1.2.2.1.1.2.2.3.1" xref="S4.Ex1.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S4.Ex1.m1.2.2.1.1.2.2.3.3" xref="S4.Ex1.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex1.m1.2.2.1.1.2.3" xref="S4.Ex1.m1.2.2.1.1.2.3.cmml">T</mi></munderover><mrow id="S4.Ex1.m1.2.2.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.cmml"><msub id="S4.Ex1.m1.2.2.1.1.1.3" xref="S4.Ex1.m1.2.2.1.1.1.3.cmml"><mi id="S4.Ex1.m1.2.2.1.1.1.3.2" xref="S4.Ex1.m1.2.2.1.1.1.3.2.cmml">y</mi><mi id="S4.Ex1.m1.2.2.1.1.1.3.3" xref="S4.Ex1.m1.2.2.1.1.1.3.3.cmml">t</mi></msub><mo id="S4.Ex1.m1.2.2.1.1.1.2" lspace="0.167em" xref="S4.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.2.2.1.1.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.1.2.cmml"><mi id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">log</mi><mo id="S4.Ex1.m1.2.2.1.1.1.1.1a" xref="S4.Ex1.m1.2.2.1.1.1.1.2.cmml">⁡</mo><mrow id="S4.Ex1.m1.2.2.1.1.1.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.1.2.cmml"><mo id="S4.Ex1.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.1" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.Ex1.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S4.Ex1.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><eq id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"></eq><ci id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3">ℒ</ci><apply id="S4.Ex1.m1.2.2.1.cmml" xref="S4.Ex1.m1.2.2.1"><minus id="S4.Ex1.m1.2.2.1.2.cmml" xref="S4.Ex1.m1.2.2.1"></minus><apply id="S4.Ex1.m1.2.2.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1"><apply id="S4.Ex1.m1.2.2.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.2.1.cmml" xref="S4.Ex1.m1.2.2.1.1.2">superscript</csymbol><apply id="S4.Ex1.m1.2.2.1.1.2.2.cmml" xref="S4.Ex1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.2.2.1.cmml" xref="S4.Ex1.m1.2.2.1.1.2">subscript</csymbol><sum id="S4.Ex1.m1.2.2.1.1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.1.1.2.2.2"></sum><apply id="S4.Ex1.m1.2.2.1.1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.1.1.2.2.3"><eq id="S4.Ex1.m1.2.2.1.1.2.2.3.1.cmml" xref="S4.Ex1.m1.2.2.1.1.2.2.3.1"></eq><ci id="S4.Ex1.m1.2.2.1.1.2.2.3.2.cmml" xref="S4.Ex1.m1.2.2.1.1.2.2.3.2">𝑡</ci><cn id="S4.Ex1.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S4.Ex1.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.Ex1.m1.2.2.1.1.2.3.cmml" xref="S4.Ex1.m1.2.2.1.1.2.3">𝑇</ci></apply><apply id="S4.Ex1.m1.2.2.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1"><times id="S4.Ex1.m1.2.2.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.2"></times><apply id="S4.Ex1.m1.2.2.1.1.1.3.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.1.3.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.2.2.1.1.1.3.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3.2">𝑦</ci><ci id="S4.Ex1.m1.2.2.1.1.1.3.3.cmml" xref="S4.Ex1.m1.2.2.1.1.1.3.3">𝑡</ci></apply><apply id="S4.Ex1.m1.2.2.1.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1"><log id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"></log><apply id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2"><ci id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.2.2">𝑦</ci></apply><ci id="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.2.2.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">\mathcal{L}=-\sum_{t=1}^{T}y_{t}\log(\hat{y}_{t})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.2d">caligraphic_L = - ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">where:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><math alttext="T" class="ltx_Math" display="inline" id="S4.I2.i1.p1.1.m1.1"><semantics id="S4.I2.i1.p1.1.m1.1a"><mi id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><ci id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i1.p1.1.m1.1d">italic_T</annotation></semantics></math> is the total number of tokens in the sequence (including both the question/prompt and the answer).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.2"><math alttext="y_{t}" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1"><semantics id="S4.I2.i2.p1.1.m1.1a"><msub id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml"><mi id="S4.I2.i2.p1.1.m1.1.1.2" xref="S4.I2.i2.p1.1.m1.1.1.2.cmml">y</mi><mi id="S4.I2.i2.p1.1.m1.1.1.3" xref="S4.I2.i2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><apply id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i2.p1.1.m1.1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.I2.i2.p1.1.m1.1.1.2.cmml" xref="S4.I2.i2.p1.1.m1.1.1.2">𝑦</ci><ci id="S4.I2.i2.p1.1.m1.1.1.3.cmml" xref="S4.I2.i2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.1.m1.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the actual token at position <math alttext="t" class="ltx_Math" display="inline" id="S4.I2.i2.p1.2.m2.1"><semantics id="S4.I2.i2.p1.2.m2.1a"><mi id="S4.I2.i2.p1.2.m2.1.1" xref="S4.I2.i2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.2.m2.1b"><ci id="S4.I2.i2.p1.2.m2.1.1.cmml" xref="S4.I2.i2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.2.m2.1d">italic_t</annotation></semantics></math> (one-hot encoded).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.2"><math alttext="\hat{y}_{t}" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><msub id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mover accent="true" id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml"><mi id="S4.I2.i3.p1.1.m1.1.1.2.2" xref="S4.I2.i3.p1.1.m1.1.1.2.2.cmml">y</mi><mo id="S4.I2.i3.p1.1.m1.1.1.2.1" xref="S4.I2.i3.p1.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S4.I2.i3.p1.1.m1.1.1.3" xref="S4.I2.i3.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">subscript</csymbol><apply id="S4.I2.i3.p1.1.m1.1.1.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2"><ci id="S4.I2.i3.p1.1.m1.1.1.2.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2.1">^</ci><ci id="S4.I2.i3.p1.1.m1.1.1.2.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2.2">𝑦</ci></apply><ci id="S4.I2.i3.p1.1.m1.1.1.3.cmml" xref="S4.I2.i3.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">\hat{y}_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted probability distribution over the vocabulary for token <math alttext="t" class="ltx_Math" display="inline" id="S4.I2.i3.p1.2.m2.1"><semantics id="S4.I2.i3.p1.2.m2.1a"><mi id="S4.I2.i3.p1.2.m2.1.1" xref="S4.I2.i3.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.2.m2.1b"><ci id="S4.I2.i3.p1.2.m2.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.2.m2.1d">italic_t</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p5">
<p class="ltx_p" id="S4.SS1.SSS2.p5.1">To focus only on answer generation, we introduce a masking vector <math alttext="m_{t}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p5.1.m1.1"><semantics id="S4.SS1.SSS2.p5.1.m1.1a"><msub id="S4.SS1.SSS2.p5.1.m1.1.1" xref="S4.SS1.SSS2.p5.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p5.1.m1.1.1.2" xref="S4.SS1.SSS2.p5.1.m1.1.1.2.cmml">m</mi><mi id="S4.SS1.SSS2.p5.1.m1.1.1.3" xref="S4.SS1.SSS2.p5.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p5.1.m1.1b"><apply id="S4.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p5.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p5.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.2">𝑚</ci><ci id="S4.SS1.SSS2.p5.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p5.1.m1.1c">m_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p5.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> such that:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m_{t}=\begin{cases}0&amp;\text{if }t\in\{1,2,\ldots,Q\}\\
1&amp;\text{if }t\in\{Q+1,Q+2,\ldots,T\}\end{cases}" class="ltx_Math" display="block" id="S4.Ex2.m1.4"><semantics id="S4.Ex2.m1.4a"><mrow id="S4.Ex2.m1.4.5" xref="S4.Ex2.m1.4.5.cmml"><msub id="S4.Ex2.m1.4.5.2" xref="S4.Ex2.m1.4.5.2.cmml"><mi id="S4.Ex2.m1.4.5.2.2" xref="S4.Ex2.m1.4.5.2.2.cmml">m</mi><mi id="S4.Ex2.m1.4.5.2.3" xref="S4.Ex2.m1.4.5.2.3.cmml">t</mi></msub><mo id="S4.Ex2.m1.4.5.1" xref="S4.Ex2.m1.4.5.1.cmml">=</mo><mrow id="S4.Ex2.m1.4.4" xref="S4.Ex2.m1.4.5.3.1.cmml"><mo id="S4.Ex2.m1.4.4.5" xref="S4.Ex2.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S4.Ex2.m1.4.4.4" rowspacing="0pt" xref="S4.Ex2.m1.4.5.3.1.cmml"><mtr id="S4.Ex2.m1.4.4.4a" xref="S4.Ex2.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4b" xref="S4.Ex2.m1.4.5.3.1.cmml"><mn id="S4.Ex2.m1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4c" xref="S4.Ex2.m1.4.5.3.1.cmml"><mrow id="S4.Ex2.m1.2.2.2.2.2.1" xref="S4.Ex2.m1.2.2.2.2.2.1.cmml"><mrow id="S4.Ex2.m1.2.2.2.2.2.1.6" xref="S4.Ex2.m1.2.2.2.2.2.1.6.cmml"><mtext id="S4.Ex2.m1.2.2.2.2.2.1.6.2" xref="S4.Ex2.m1.2.2.2.2.2.1.6.2a.cmml">if </mtext><mo id="S4.Ex2.m1.2.2.2.2.2.1.6.1" xref="S4.Ex2.m1.2.2.2.2.2.1.6.1.cmml">⁢</mo><mi id="S4.Ex2.m1.2.2.2.2.2.1.6.3" xref="S4.Ex2.m1.2.2.2.2.2.1.6.3.cmml">t</mi></mrow><mo id="S4.Ex2.m1.2.2.2.2.2.1.5" xref="S4.Ex2.m1.2.2.2.2.2.1.5.cmml">∈</mo><mrow id="S4.Ex2.m1.2.2.2.2.2.1.7.2" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml"><mo id="S4.Ex2.m1.2.2.2.2.2.1.7.2.1" stretchy="false" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml">{</mo><mn id="S4.Ex2.m1.2.2.2.2.2.1.1" xref="S4.Ex2.m1.2.2.2.2.2.1.1.cmml">1</mn><mo id="S4.Ex2.m1.2.2.2.2.2.1.7.2.2" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml">,</mo><mn id="S4.Ex2.m1.2.2.2.2.2.1.2" xref="S4.Ex2.m1.2.2.2.2.2.1.2.cmml">2</mn><mo id="S4.Ex2.m1.2.2.2.2.2.1.7.2.3" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml">,</mo><mi id="S4.Ex2.m1.2.2.2.2.2.1.3" mathvariant="normal" xref="S4.Ex2.m1.2.2.2.2.2.1.3.cmml">…</mi><mo id="S4.Ex2.m1.2.2.2.2.2.1.7.2.4" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml">,</mo><mi id="S4.Ex2.m1.2.2.2.2.2.1.4" xref="S4.Ex2.m1.2.2.2.2.2.1.4.cmml">Q</mi><mo id="S4.Ex2.m1.2.2.2.2.2.1.7.2.5" stretchy="false" xref="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml">}</mo></mrow></mrow></mtd></mtr><mtr id="S4.Ex2.m1.4.4.4d" xref="S4.Ex2.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4e" xref="S4.Ex2.m1.4.5.3.1.cmml"><mn id="S4.Ex2.m1.3.3.3.3.1.1" xref="S4.Ex2.m1.3.3.3.3.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4f" xref="S4.Ex2.m1.4.5.3.1.cmml"><mrow id="S4.Ex2.m1.4.4.4.4.2.1" xref="S4.Ex2.m1.4.4.4.4.2.1.cmml"><mrow id="S4.Ex2.m1.4.4.4.4.2.1.6" xref="S4.Ex2.m1.4.4.4.4.2.1.6.cmml"><mtext id="S4.Ex2.m1.4.4.4.4.2.1.6.2" xref="S4.Ex2.m1.4.4.4.4.2.1.6.2a.cmml">if </mtext><mo id="S4.Ex2.m1.4.4.4.4.2.1.6.1" xref="S4.Ex2.m1.4.4.4.4.2.1.6.1.cmml">⁢</mo><mi id="S4.Ex2.m1.4.4.4.4.2.1.6.3" xref="S4.Ex2.m1.4.4.4.4.2.1.6.3.cmml">t</mi></mrow><mo id="S4.Ex2.m1.4.4.4.4.2.1.5" xref="S4.Ex2.m1.4.4.4.4.2.1.5.cmml">∈</mo><mrow id="S4.Ex2.m1.4.4.4.4.2.1.4.2" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml"><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.3" stretchy="false" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml">{</mo><mrow id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.cmml"><mi id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.2" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.2.cmml">Q</mi><mo id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.1" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.1.cmml">+</mo><mn id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.3" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.3.cmml">1</mn></mrow><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.4" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml">,</mo><mrow id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.cmml"><mi id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.2" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.2.cmml">Q</mi><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.1" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.1.cmml">+</mo><mn id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.3" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.3.cmml">2</mn></mrow><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.5" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml">,</mo><mi id="S4.Ex2.m1.4.4.4.4.2.1.1" mathvariant="normal" xref="S4.Ex2.m1.4.4.4.4.2.1.1.cmml">…</mi><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.6" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml">,</mo><mi id="S4.Ex2.m1.4.4.4.4.2.1.2" xref="S4.Ex2.m1.4.4.4.4.2.1.2.cmml">T</mi><mo id="S4.Ex2.m1.4.4.4.4.2.1.4.2.7" stretchy="false" xref="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml">}</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.4b"><apply id="S4.Ex2.m1.4.5.cmml" xref="S4.Ex2.m1.4.5"><eq id="S4.Ex2.m1.4.5.1.cmml" xref="S4.Ex2.m1.4.5.1"></eq><apply id="S4.Ex2.m1.4.5.2.cmml" xref="S4.Ex2.m1.4.5.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.4.5.2.1.cmml" xref="S4.Ex2.m1.4.5.2">subscript</csymbol><ci id="S4.Ex2.m1.4.5.2.2.cmml" xref="S4.Ex2.m1.4.5.2.2">𝑚</ci><ci id="S4.Ex2.m1.4.5.2.3.cmml" xref="S4.Ex2.m1.4.5.2.3">𝑡</ci></apply><apply id="S4.Ex2.m1.4.5.3.1.cmml" xref="S4.Ex2.m1.4.4"><csymbol cd="latexml" id="S4.Ex2.m1.4.5.3.1.1.cmml" xref="S4.Ex2.m1.4.4.5">cases</csymbol><cn id="S4.Ex2.m1.1.1.1.1.1.1.cmml" type="integer" xref="S4.Ex2.m1.1.1.1.1.1.1">0</cn><apply id="S4.Ex2.m1.2.2.2.2.2.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1"><in id="S4.Ex2.m1.2.2.2.2.2.1.5.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.5"></in><apply id="S4.Ex2.m1.2.2.2.2.2.1.6.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.6"><times id="S4.Ex2.m1.2.2.2.2.2.1.6.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.6.1"></times><ci id="S4.Ex2.m1.2.2.2.2.2.1.6.2a.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.6.2"><mtext id="S4.Ex2.m1.2.2.2.2.2.1.6.2.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.6.2">if </mtext></ci><ci id="S4.Ex2.m1.2.2.2.2.2.1.6.3.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.6.3">𝑡</ci></apply><set id="S4.Ex2.m1.2.2.2.2.2.1.7.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.7.2"><cn id="S4.Ex2.m1.2.2.2.2.2.1.1.cmml" type="integer" xref="S4.Ex2.m1.2.2.2.2.2.1.1">1</cn><cn id="S4.Ex2.m1.2.2.2.2.2.1.2.cmml" type="integer" xref="S4.Ex2.m1.2.2.2.2.2.1.2">2</cn><ci id="S4.Ex2.m1.2.2.2.2.2.1.3.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.3">…</ci><ci id="S4.Ex2.m1.2.2.2.2.2.1.4.cmml" xref="S4.Ex2.m1.2.2.2.2.2.1.4">𝑄</ci></set></apply><cn id="S4.Ex2.m1.3.3.3.3.1.1.cmml" type="integer" xref="S4.Ex2.m1.3.3.3.3.1.1">1</cn><apply id="S4.Ex2.m1.4.4.4.4.2.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1"><in id="S4.Ex2.m1.4.4.4.4.2.1.5.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.5"></in><apply id="S4.Ex2.m1.4.4.4.4.2.1.6.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.6"><times id="S4.Ex2.m1.4.4.4.4.2.1.6.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.6.1"></times><ci id="S4.Ex2.m1.4.4.4.4.2.1.6.2a.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.6.2"><mtext id="S4.Ex2.m1.4.4.4.4.2.1.6.2.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.6.2">if </mtext></ci><ci id="S4.Ex2.m1.4.4.4.4.2.1.6.3.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.6.3">𝑡</ci></apply><set id="S4.Ex2.m1.4.4.4.4.2.1.4.3.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2"><apply id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1"><plus id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.1"></plus><ci id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.2.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.2">𝑄</ci><cn id="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.3.cmml" type="integer" xref="S4.Ex2.m1.4.4.4.4.2.1.3.1.1.3">1</cn></apply><apply id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2"><plus id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.1"></plus><ci id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.2.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.2">𝑄</ci><cn id="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.3.cmml" type="integer" xref="S4.Ex2.m1.4.4.4.4.2.1.4.2.2.3">2</cn></apply><ci id="S4.Ex2.m1.4.4.4.4.2.1.1.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.1">…</ci><ci id="S4.Ex2.m1.4.4.4.4.2.1.2.cmml" xref="S4.Ex2.m1.4.4.4.4.2.1.2">𝑇</ci></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.4c">m_{t}=\begin{cases}0&amp;\text{if }t\in\{1,2,\ldots,Q\}\\
1&amp;\text{if }t\in\{Q+1,Q+2,\ldots,T\}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.4d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { start_ROW start_CELL 0 end_CELL start_CELL if italic_t ∈ { 1 , 2 , … , italic_Q } end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL if italic_t ∈ { italic_Q + 1 , italic_Q + 2 , … , italic_T } end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p6">
<p class="ltx_p" id="S4.SS1.SSS2.p6.1">The modified cross-entropy loss, which we call question-masked loss <math alttext="\mathcal{L}_{\text{masked}}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p6.1.m1.1"><semantics id="S4.SS1.SSS2.p6.1.m1.1a"><msub id="S4.SS1.SSS2.p6.1.m1.1.1" xref="S4.SS1.SSS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS2.p6.1.m1.1.1.2" xref="S4.SS1.SSS2.p6.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS2.p6.1.m1.1.1.3" xref="S4.SS1.SSS2.p6.1.m1.1.1.3a.cmml">masked</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p6.1.m1.1b"><apply id="S4.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p6.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p6.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p6.1.m1.1.1.2">ℒ</ci><ci id="S4.SS1.SSS2.p6.1.m1.1.1.3a.cmml" xref="S4.SS1.SSS2.p6.1.m1.1.1.3"><mtext id="S4.SS1.SSS2.p6.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS1.SSS2.p6.1.m1.1.1.3">masked</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p6.1.m1.1c">\mathcal{L}_{\text{masked}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p6.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT masked end_POSTSUBSCRIPT</annotation></semantics></math> is then:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p7">
<table class="ltx_equation ltx_eqn_table" id="S4.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{masked}}=-\sum_{t=1}^{T}m_{t}\cdot y_{t}\log(\hat{y}_{t})" class="ltx_Math" display="block" id="S4.Ex3.m1.2"><semantics id="S4.Ex3.m1.2a"><mrow id="S4.Ex3.m1.2.2" xref="S4.Ex3.m1.2.2.cmml"><msub id="S4.Ex3.m1.2.2.3" xref="S4.Ex3.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.2.2.3.2" xref="S4.Ex3.m1.2.2.3.2.cmml">ℒ</mi><mtext id="S4.Ex3.m1.2.2.3.3" xref="S4.Ex3.m1.2.2.3.3a.cmml">masked</mtext></msub><mo id="S4.Ex3.m1.2.2.2" xref="S4.Ex3.m1.2.2.2.cmml">=</mo><mrow id="S4.Ex3.m1.2.2.1" xref="S4.Ex3.m1.2.2.1.cmml"><mo id="S4.Ex3.m1.2.2.1a" xref="S4.Ex3.m1.2.2.1.cmml">−</mo><mrow id="S4.Ex3.m1.2.2.1.1" xref="S4.Ex3.m1.2.2.1.1.cmml"><munderover id="S4.Ex3.m1.2.2.1.1.2" xref="S4.Ex3.m1.2.2.1.1.2.cmml"><mo id="S4.Ex3.m1.2.2.1.1.2.2.2" movablelimits="false" xref="S4.Ex3.m1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S4.Ex3.m1.2.2.1.1.2.2.3" xref="S4.Ex3.m1.2.2.1.1.2.2.3.cmml"><mi id="S4.Ex3.m1.2.2.1.1.2.2.3.2" xref="S4.Ex3.m1.2.2.1.1.2.2.3.2.cmml">t</mi><mo id="S4.Ex3.m1.2.2.1.1.2.2.3.1" xref="S4.Ex3.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S4.Ex3.m1.2.2.1.1.2.2.3.3" xref="S4.Ex3.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex3.m1.2.2.1.1.2.3" xref="S4.Ex3.m1.2.2.1.1.2.3.cmml">T</mi></munderover><mrow id="S4.Ex3.m1.2.2.1.1.1" xref="S4.Ex3.m1.2.2.1.1.1.cmml"><mrow id="S4.Ex3.m1.2.2.1.1.1.3" xref="S4.Ex3.m1.2.2.1.1.1.3.cmml"><msub id="S4.Ex3.m1.2.2.1.1.1.3.2" xref="S4.Ex3.m1.2.2.1.1.1.3.2.cmml"><mi id="S4.Ex3.m1.2.2.1.1.1.3.2.2" xref="S4.Ex3.m1.2.2.1.1.1.3.2.2.cmml">m</mi><mi id="S4.Ex3.m1.2.2.1.1.1.3.2.3" xref="S4.Ex3.m1.2.2.1.1.1.3.2.3.cmml">t</mi></msub><mo id="S4.Ex3.m1.2.2.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.Ex3.m1.2.2.1.1.1.3.1.cmml">⋅</mo><msub id="S4.Ex3.m1.2.2.1.1.1.3.3" xref="S4.Ex3.m1.2.2.1.1.1.3.3.cmml"><mi id="S4.Ex3.m1.2.2.1.1.1.3.3.2" xref="S4.Ex3.m1.2.2.1.1.1.3.3.2.cmml">y</mi><mi id="S4.Ex3.m1.2.2.1.1.1.3.3.3" xref="S4.Ex3.m1.2.2.1.1.1.3.3.3.cmml">t</mi></msub></mrow><mo id="S4.Ex3.m1.2.2.1.1.1.2" lspace="0.167em" xref="S4.Ex3.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex3.m1.2.2.1.1.1.1.1" xref="S4.Ex3.m1.2.2.1.1.1.1.2.cmml"><mi id="S4.Ex3.m1.1.1" xref="S4.Ex3.m1.1.1.cmml">log</mi><mo id="S4.Ex3.m1.2.2.1.1.1.1.1a" xref="S4.Ex3.m1.2.2.1.1.1.1.2.cmml">⁡</mo><mrow id="S4.Ex3.m1.2.2.1.1.1.1.1.1" xref="S4.Ex3.m1.2.2.1.1.1.1.2.cmml"><mo id="S4.Ex3.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S4.Ex3.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.1" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.Ex3.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S4.Ex3.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m1.2b"><apply id="S4.Ex3.m1.2.2.cmml" xref="S4.Ex3.m1.2.2"><eq id="S4.Ex3.m1.2.2.2.cmml" xref="S4.Ex3.m1.2.2.2"></eq><apply id="S4.Ex3.m1.2.2.3.cmml" xref="S4.Ex3.m1.2.2.3"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.3.1.cmml" xref="S4.Ex3.m1.2.2.3">subscript</csymbol><ci id="S4.Ex3.m1.2.2.3.2.cmml" xref="S4.Ex3.m1.2.2.3.2">ℒ</ci><ci id="S4.Ex3.m1.2.2.3.3a.cmml" xref="S4.Ex3.m1.2.2.3.3"><mtext id="S4.Ex3.m1.2.2.3.3.cmml" mathsize="70%" xref="S4.Ex3.m1.2.2.3.3">masked</mtext></ci></apply><apply id="S4.Ex3.m1.2.2.1.cmml" xref="S4.Ex3.m1.2.2.1"><minus id="S4.Ex3.m1.2.2.1.2.cmml" xref="S4.Ex3.m1.2.2.1"></minus><apply id="S4.Ex3.m1.2.2.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1"><apply id="S4.Ex3.m1.2.2.1.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.2">superscript</csymbol><apply id="S4.Ex3.m1.2.2.1.1.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.2.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.2">subscript</csymbol><sum id="S4.Ex3.m1.2.2.1.1.2.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2.2.2"></sum><apply id="S4.Ex3.m1.2.2.1.1.2.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.2.2.3"><eq id="S4.Ex3.m1.2.2.1.1.2.2.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.2.2.3.1"></eq><ci id="S4.Ex3.m1.2.2.1.1.2.2.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2.2.3.2">𝑡</ci><cn id="S4.Ex3.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S4.Ex3.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.Ex3.m1.2.2.1.1.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.2.3">𝑇</ci></apply><apply id="S4.Ex3.m1.2.2.1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1"><times id="S4.Ex3.m1.2.2.1.1.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.2"></times><apply id="S4.Ex3.m1.2.2.1.1.1.3.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3"><ci id="S4.Ex3.m1.2.2.1.1.1.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.1">⋅</ci><apply id="S4.Ex3.m1.2.2.1.1.1.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.1.3.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.2">subscript</csymbol><ci id="S4.Ex3.m1.2.2.1.1.1.3.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.2.2">𝑚</ci><ci id="S4.Ex3.m1.2.2.1.1.1.3.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.2.3">𝑡</ci></apply><apply id="S4.Ex3.m1.2.2.1.1.1.3.3.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.1.3.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.3">subscript</csymbol><ci id="S4.Ex3.m1.2.2.1.1.1.3.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.3.2">𝑦</ci><ci id="S4.Ex3.m1.2.2.1.1.1.3.3.3.cmml" xref="S4.Ex3.m1.2.2.1.1.1.3.3.3">𝑡</ci></apply></apply><apply id="S4.Ex3.m1.2.2.1.1.1.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1"><log id="S4.Ex3.m1.1.1.cmml" xref="S4.Ex3.m1.1.1"></log><apply id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2"><ci id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.2.2">𝑦</ci></apply><ci id="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex3.m1.2.2.1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m1.2c">\mathcal{L}_{\text{masked}}=-\sum_{t=1}^{T}m_{t}\cdot y_{t}\log(\hat{y}_{t})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex3.m1.2d">caligraphic_L start_POSTSUBSCRIPT masked end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⋅ italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p8">
<p class="ltx_p" id="S4.SS1.SSS2.p8.1">This loss function ensures that only the tokens corresponding to the answer part of the sequence contribute to the overall loss, effectively masking out the contributions from the question/prompt part.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Option Batch-Shuffle Trick</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.13">Recent research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib27" title="">27</a>]</cite> has unveiled a significant bias in LLMs when answering multiple-choice questions (MCQs). These models exhibit a strong sensitivity to the order of options, often selecting specific answer positions regardless of the content. This phenomenon, termed ”selection bias,” stems from the models’ tendency to assign higher probabilities to certain option labels (like ”A” or ”B” in options [”A”, ”B”, ”C”, ”D” and ”E”] ). Consequently, LLMs may prioritize these options even when logically incorrect, undermining the reliability of their performance on MCQ assessments. To avert this we employ a trick where we create multiple prompts for a question,
with each prompt having a different option order. The correct answer is thus determined by chosing the most selected answer by the model after observing the answers generated for all created prompts. Given that we have to permute these options to obtain all possible option ordering, the complexity of doing so is <math alttext="O(n!)" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">O</mi><mo id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo id="S4.SS2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S4.SS2.p1.1.m1.1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml">!</mo></mrow><mo id="S4.SS2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"></times><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑂</ci><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><factorial id="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1"></factorial><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">O(n!)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_O ( italic_n ! )</annotation></semantics></math> where <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_n</annotation></semantics></math> is the number of options present in an MCQ. This complexity significantly increases the inference time for a single question. For example, when an MCQ has 4 options, we create <math alttext="4!=24" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mrow id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.2.2.cmml">4</mn><mo id="S4.SS2.p1.3.m3.1.1.2.1" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">!</mo></mrow><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">24</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><apply id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><factorial id="S4.SS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.2.1"></factorial><cn id="S4.SS2.p1.3.m3.1.1.2.2.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.2.2">4</cn></apply><cn id="S4.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.p1.3.m3.1.1.3">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">4!=24</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">4 ! = 24</annotation></semantics></math> prompts instead of 1 prompt. For 5 options, we create <math alttext="5!=120" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mrow id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2.2" xref="S4.SS2.p1.4.m4.1.1.2.2.cmml">5</mn><mo id="S4.SS2.p1.4.m4.1.1.2.1" xref="S4.SS2.p1.4.m4.1.1.2.1.cmml">!</mo></mrow><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><eq id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></eq><apply id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2"><factorial id="S4.SS2.p1.4.m4.1.1.2.1.cmml" xref="S4.SS2.p1.4.m4.1.1.2.1"></factorial><cn id="S4.SS2.p1.4.m4.1.1.2.2.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.2.2">5</cn></apply><cn id="S4.SS2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS2.p1.4.m4.1.1.3">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">5!=120</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">5 ! = 120</annotation></semantics></math> prompts. To reduce this complexity, we randomly sample <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mi id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><ci id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">italic_k</annotation></semantics></math> prompts from the n! prompts, create a batch of <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">italic_k</annotation></semantics></math> prompts and generate answers for the batch using the Phi-2 model. Using <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.7.m7.1"><semantics id="S4.SS2.p1.7.m7.1a"><mi id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><ci id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.7.m7.1d">italic_k</annotation></semantics></math> prompts instead of <math alttext="n!" class="ltx_Math" display="inline" id="S4.SS2.p1.8.m8.1"><semantics id="S4.SS2.p1.8.m8.1a"><mrow id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml"><mi id="S4.SS2.p1.8.m8.1.1.2" xref="S4.SS2.p1.8.m8.1.1.2.cmml">n</mi><mo id="S4.SS2.p1.8.m8.1.1.1" xref="S4.SS2.p1.8.m8.1.1.1.cmml">!</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><apply id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"><factorial id="S4.SS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1.1"></factorial><ci id="S4.SS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.p1.8.m8.1.1.2">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">n!</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.8.m8.1d">italic_n !</annotation></semantics></math> reduces the complexity from <math alttext="O(n!)" class="ltx_Math" display="inline" id="S4.SS2.p1.9.m9.1"><semantics id="S4.SS2.p1.9.m9.1a"><mrow id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mi id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">O</mi><mo id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p1.9.m9.1.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.1.1.cmml"><mo id="S4.SS2.p1.9.m9.1.1.1.1.2" stretchy="false" xref="S4.SS2.p1.9.m9.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p1.9.m9.1.1.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.9.m9.1.1.1.1.1.2" xref="S4.SS2.p1.9.m9.1.1.1.1.1.2.cmml">n</mi><mo id="S4.SS2.p1.9.m9.1.1.1.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.1.1.1.cmml">!</mo></mrow><mo id="S4.SS2.p1.9.m9.1.1.1.1.3" stretchy="false" xref="S4.SS2.p1.9.m9.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><times id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2"></times><ci id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3">𝑂</ci><apply id="S4.SS2.p1.9.m9.1.1.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1.1"><factorial id="S4.SS2.p1.9.m9.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1.1.1.1"></factorial><ci id="S4.SS2.p1.9.m9.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">O(n!)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.9.m9.1d">italic_O ( italic_n ! )</annotation></semantics></math> to <math alttext="O(k)" class="ltx_Math" display="inline" id="S4.SS2.p1.10.m10.1"><semantics id="S4.SS2.p1.10.m10.1a"><mrow id="S4.SS2.p1.10.m10.1.2" xref="S4.SS2.p1.10.m10.1.2.cmml"><mi id="S4.SS2.p1.10.m10.1.2.2" xref="S4.SS2.p1.10.m10.1.2.2.cmml">O</mi><mo id="S4.SS2.p1.10.m10.1.2.1" xref="S4.SS2.p1.10.m10.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p1.10.m10.1.2.3.2" xref="S4.SS2.p1.10.m10.1.2.cmml"><mo id="S4.SS2.p1.10.m10.1.2.3.2.1" stretchy="false" xref="S4.SS2.p1.10.m10.1.2.cmml">(</mo><mi id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml">k</mi><mo id="S4.SS2.p1.10.m10.1.2.3.2.2" stretchy="false" xref="S4.SS2.p1.10.m10.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.2.cmml" xref="S4.SS2.p1.10.m10.1.2"><times id="S4.SS2.p1.10.m10.1.2.1.cmml" xref="S4.SS2.p1.10.m10.1.2.1"></times><ci id="S4.SS2.p1.10.m10.1.2.2.cmml" xref="S4.SS2.p1.10.m10.1.2.2">𝑂</ci><ci id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">O(k)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.10.m10.1d">italic_O ( italic_k )</annotation></semantics></math>, where <math alttext="k\ll n!" class="ltx_Math" display="inline" id="S4.SS2.p1.11.m11.1"><semantics id="S4.SS2.p1.11.m11.1a"><mrow id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml"><mi id="S4.SS2.p1.11.m11.1.1.2" xref="S4.SS2.p1.11.m11.1.1.2.cmml">k</mi><mo id="S4.SS2.p1.11.m11.1.1.1" xref="S4.SS2.p1.11.m11.1.1.1.cmml">≪</mo><mrow id="S4.SS2.p1.11.m11.1.1.3" xref="S4.SS2.p1.11.m11.1.1.3.cmml"><mi id="S4.SS2.p1.11.m11.1.1.3.2" xref="S4.SS2.p1.11.m11.1.1.3.2.cmml">n</mi><mo id="S4.SS2.p1.11.m11.1.1.3.1" xref="S4.SS2.p1.11.m11.1.1.3.1.cmml">!</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><apply id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1"><csymbol cd="latexml" id="S4.SS2.p1.11.m11.1.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1.1">much-less-than</csymbol><ci id="S4.SS2.p1.11.m11.1.1.2.cmml" xref="S4.SS2.p1.11.m11.1.1.2">𝑘</ci><apply id="S4.SS2.p1.11.m11.1.1.3.cmml" xref="S4.SS2.p1.11.m11.1.1.3"><factorial id="S4.SS2.p1.11.m11.1.1.3.1.cmml" xref="S4.SS2.p1.11.m11.1.1.3.1"></factorial><ci id="S4.SS2.p1.11.m11.1.1.3.2.cmml" xref="S4.SS2.p1.11.m11.1.1.3.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">k\ll n!</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.11.m11.1d">italic_k ≪ italic_n !</annotation></semantics></math>. The model generates answers for these <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.12.m12.1"><semantics id="S4.SS2.p1.12.m12.1a"><mi id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><ci id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.12.m12.1d">italic_k</annotation></semantics></math> prompts in a single batch, thus significantly reducing inference time while still benefiting from diverse option orderings. The selection of the most frequent answer from these <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.13.m13.1"><semantics id="S4.SS2.p1.13.m13.1a"><mi id="S4.SS2.p1.13.m13.1.1" xref="S4.SS2.p1.13.m13.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m13.1b"><ci id="S4.SS2.p1.13.m13.1.1.cmml" xref="S4.SS2.p1.13.m13.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m13.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.13.m13.1d">italic_k</annotation></semantics></math> prompts can be described as:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{a}=\arg\max_{a\in A}\sum_{i=1}^{k}\mathbb{I}(a_{i}=a)" class="ltx_Math" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mover accent="true" id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">a</mi><mo id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">^</mo></mover><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.3.1.cmml">arg</mi><mo id="S4.E1.m1.1.1.1.3a" lspace="0.167em" xref="S4.E1.m1.1.1.1.3.cmml">⁡</mo><munder id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.3.2.2.cmml">max</mi><mrow id="S4.E1.m1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.1.3.2.3.2.cmml">a</mi><mo id="S4.E1.m1.1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.1.3.2.3.1.cmml">∈</mo><mi id="S4.E1.m1.1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.1.3.2.3.3.cmml">A</mi></mrow></munder></mrow><mo id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><munderover id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml"><mo id="S4.E1.m1.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E1.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E1.m1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.2.3.cmml">k</mi></munderover><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">𝕀</mi><mo id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml">=</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml">a</mi></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><ci id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1">^</ci><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝑎</ci></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><times id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3"><arg id="S4.E1.m1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.3.1"></arg><apply id="S4.E1.m1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.3.2">subscript</csymbol><max id="S4.E1.m1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.3.2.2"></max><apply id="S4.E1.m1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.3.2.3"><in id="S4.E1.m1.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.1.3.2.3.1"></in><ci id="S4.E1.m1.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2.3.2">𝑎</ci><ci id="S4.E1.m1.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.1.3.2.3.3">𝐴</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><apply id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3"><eq id="S4.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S4.E1.m1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3">𝕀</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"></eq><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.2">𝑎</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3">𝑎</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\hat{a}=\arg\max_{a\in A}\sum_{i=1}^{k}\mathbb{I}(a_{i}=a)</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">over^ start_ARG italic_a end_ARG = roman_arg roman_max start_POSTSUBSCRIPT italic_a ∈ italic_A end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT blackboard_I ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_a )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.4">where <math alttext="\mathbb{I}(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.2" xref="S4.SS2.p3.1.m1.1.2.cmml"><mi id="S4.SS2.p3.1.m1.1.2.2" xref="S4.SS2.p3.1.m1.1.2.2.cmml">𝕀</mi><mo id="S4.SS2.p3.1.m1.1.2.1" xref="S4.SS2.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p3.1.m1.1.2.3.2" xref="S4.SS2.p3.1.m1.1.2.cmml"><mo id="S4.SS2.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S4.SS2.p3.1.m1.1.2.cmml">(</mo><mo id="S4.SS2.p3.1.m1.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.1.m1.1.1.cmml">⋅</mo><mo id="S4.SS2.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S4.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.2"><times id="S4.SS2.p3.1.m1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.2.1"></times><ci id="S4.SS2.p3.1.m1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.2.2">𝕀</ci><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathbb{I}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">blackboard_I ( ⋅ )</annotation></semantics></math> is the indicator function, <math alttext="a_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><msub id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">a</mi><mi id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">𝑎</ci><ci id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">a_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the answer chosen by the model for the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">italic_i</annotation></semantics></math>-th prompt, and <math alttext="A" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">italic_A</annotation></semantics></math> is the set of all possible answers.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.2">We call this the ’<span class="ltx_text ltx_font_italic" id="S4.SS2.p4.2.1">batch-shuffle trick</span>’. Using the batch-shuffle trick, we noticed over 6% at inference time and this increased to about 10% when we include the trick into the training phase. In the training phase, we only shuffle the options at the end of each epoch and do not use any explicit sampling. We hypothesize further improvement in performance as we increase <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_k</annotation></semantics></math>, the number of samples from n! prompts of an MCQ. In our case, we find <math alttext="k=20" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><eq id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></eq><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">𝑘</ci><cn id="S4.SS2.p4.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p4.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">k=20</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">italic_k = 20</annotation></semantics></math> to be a good balance between efficiency and accuracy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluated our approach using a subset of the TeleQnA dataset containing only two categories; Standards Specifications and Standards Overview. Matoouk <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">et al</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib2" title="">2</a>]</cite>
showed that GPT-3.5 and GPT-4 performed better in other question categories than in these two categories. The dataset, obtained from the ’Specializing Large Language Models for Telecom Networks by ITU AI/ML in 5G Challenge’ on Zindi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#bib.bib28" title="">28</a>]</cite> contains a train set of 1461 questions, a public test set 366 questions, and a private test set of 2000 questions. Evaluation results, as obtained from the leaderboard, are reported for the private test set. The train set was used for fine-tuning purposes. Additionally, we used the technical documents provided by the challenge as external knowledge sources for RAG.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We compare the performance of the base Phi-2 model and its performance with RAG, with fine-tuning and with the batch-shuffle trick both at inference and at training time. For the Falcon7B, we compare the base model performance and its performance with RAG and with the options excluded in the prompt.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Phi-2 model</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.T1" title="TABLE I ‣ V-A Phi-2 model ‣ V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">I</span></a> shows the performance of the Phi-2 model on the private test sets as obtained from the submissions on Zindi. The accuracy score measures the percentage of correctly answered questions. The base Phi-2 model has an accuracy of 42.07%, and this accuracy was increased to 66.39% with the introduction of RAG. The result obtained with RAG was further improved by fine-tuning the model using the training set. With fine-tuning, the accuracy increased to 76.90%. We had pointed out that LLMs exhibit a strong sensitivity to options ordering in MCQs and introduced the batch-shuffle trick in Section IV. Using the batch-shuffle trick with the fine-tuned Phi-2 model, we obtained an accuracy of 81.65% which is a 6.18% increase over the fine-tuned model and 84.65% when we shuffle the options during training.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">While fine-tuning the Phi-2 model on the training data, we discovered that the model’s performance stops improving after certain epochs of training. To investigate this we modified the objective (next token prediction) to only account for answer generation by masking the part of the cross entropy loss associated with question/prompt prediction.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Using a train-validation split (20% validation) while fine-tuning, it is expected that the validation accuracy increases as the validation loss decreases. However, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.F3" title="Figure 3 ‣ V-A Phi-2 model ‣ V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">3</span></a> shows no improvement in the validation accuracy even as the validation loss decreases. We suspected that this decrease in validation loss results from the model getting better at predicting the question and not the answers. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.F4" title="Figure 4 ‣ V-A Phi-2 model ‣ V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">4</span></a> shows the result obtained when the loss associated with questions is masked out, allowing the training objective to focus only on the answers. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.F4" title="Figure 4 ‣ V-A Phi-2 model ‣ V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the validation accuracy increases as the validation loss decreases. We, therefore, hypothesize that focusing solely on the answers during fine-tuning may yield better results. Validating this hypothesis is an interesting area of future work to be explored with additional experiments.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance of Phi-2 model on the test dataset as obtained from submissions to Zindi</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.1.1">
<span class="ltx_p" id="S5.T1.1.1.1.1.1.1" style="width:216.8pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1.1">Method</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.1.1.2.1">
<span class="ltx_p" id="S5.T1.1.1.1.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.2.1.1.1">Test Accuracy (%)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.1.1">
<span class="ltx_p" id="S5.T1.1.2.1.1.1.1" style="width:216.8pt;">Phi-2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.2.1.2.1">
<span class="ltx_p" id="S5.T1.1.2.1.2.1.1" style="width:130.1pt;">42.07</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.1.1">
<span class="ltx_p" id="S5.T1.1.3.2.1.1.1" style="width:216.8pt;">Phi-2 + RAG</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.3.2.2.1">
<span class="ltx_p" id="S5.T1.1.3.2.2.1.1" style="width:130.1pt;">66.39</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.1.1">
<span class="ltx_p" id="S5.T1.1.4.3.1.1.1" style="width:216.8pt;">Phi-2 + RAG + Fine-tuning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.4.3.2.1">
<span class="ltx_p" id="S5.T1.1.4.3.2.1.1" style="width:130.1pt;">76.90</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.1.1">
<span class="ltx_p" id="S5.T1.1.5.4.1.1.1" style="width:216.8pt;">Phi-2 + RAG + Fine-tuning + Inference batch-shuffle</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.5.4.2.1">
<span class="ltx_p" id="S5.T1.1.5.4.2.1.1" style="width:130.1pt;">81.65</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.1.1">
<span class="ltx_p" id="S5.T1.1.6.5.1.1.1" style="width:216.8pt;">Phi-2 + RAG + Fine-tuning + Inference &amp; Train batch-shuffle (<span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.1.1.1.1">QMOS</span>)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T1.1.6.5.2.1">
<span class="ltx_p" id="S5.T1.1.6.5.2.1.1" style="width:130.1pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.1.6.5.2.1.1.1">84.65</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S5.F3.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Loss and accuracy scores when fine-tuning the Phi-2 model with the standard next-token prediction objective</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S5.F4.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Loss and accuracy scores when fine-tuning the Phi-2 model and considering only the answers in the next-token prediction objective</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Falcon-7B</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The the performance of the Falcon-7B model is summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14175v1#S5.T2" title="TABLE II ‣ V-B Falcon-7B ‣ V Evaluation ‣ QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling"><span class="ltx_text ltx_ref_tag">II</span></a></p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance of Falcon7B model on the test dataset as obtained from submissions to Zindi</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.2.1.1">Baseline Falcon7B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.1.2">24.51</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.3.2.1">Falcon7B + RAG</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.2.2">36.61</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.1.4.3.1">Falcon7B + RAG + No Options</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.4.3.2.1">49.30</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For the Baseline 7B model, when prompted with the options we notice that in some cases the model does not output the options but just some unrelated texts.In that case, we randomly
choose an option. This yielded an accuracy of 24.51%. Adding the contexts from the RAG further enhanced the score to 36.61%. Finally, by removing the options and allowing the model to generate the answer freely. We then used the embedding of the generated answer with the embedding of the options to select the right option using a cosine similarity metric. With this strategy the model was able to achieve an accuracy of <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">49.93%</span>, which is significantly higher than the baseline of <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.2">24.51%</span>.
We note that the baseline Falcon-7B model is not fine-tuned considering it has more parameters (7B) than Phi-2 (2.7B). Since fine-tuning will increase the computational cost, we strictly rely on the effectiveness of prompting and RAG systems in a bid to improve its baseline performance on question answering in telecommunications domain.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this research, we have presented a comprehensive approach to addressing multiple-choice questions (MCQs) in the telecommunications domain using small, open-source language models within an enhanced Retrieval-Augmented Generation (RAG) framework. Our study demonstrates that small models such as Phi-2 and Falcon-7B, when combined with advanced techniques like LoRA fine-tuning, diversified embedding models for RAG, innovative prompt engineering, and batch-shuffle trick can achieve competitive performance compared to larger, proprietary models like GPT-3.5, while significantly reducing computational costs.Future work will involve fine-tuning the embedding models (used for RAG) for the telecommunication domain and also further investigate the performance of the proposed QMOS framework on the other language models and MCQs datasets.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Language models are few-shot learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">Advances in Neural Information Processing Systems</em>, vol. 33, 2020, pp. 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah, and Z.-Q. Luo, “Teleqna: A benchmark dataset to assess large language models telecommunications knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2308.06013</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A.-L. Bornea, F. Ayed, A. De Domenico, N. Piovesan, and A. Maatouk, “Telco-rag: Navigating the challenges of retrieval-augmented language models for telecommunications,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2404.15939</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Patterson, J. Gonzalez, Q. Le, P. Liang, V. Madhavan, J. Dean <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “Carbon emissions and large neural network training,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2104.10350</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. Pezeshkpour and E. Hruschka, “Large language models sensitivity to the order of options in multiple-choice questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2308.11483</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Microsoft Research. (2023) Phi-2: The surprising power of small language models. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, . Goffinet, D. Hesslow, J. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and G. Penedo, “The falcon series of open language models,” 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2311.16867</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source small language model,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2401.02385</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun, “Minicpm: Unveiling the potential of small language models with scalable training strategies,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2404.06395</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Piovesan, A. D. Domenico, and F. Ayed, “Telecom language models: Must they be large?” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2403.04666</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Ahmed, N. Piovesan, A. D. Domenico, and S. Choudhury, “Linguistic intelligence in large language models for telecommunications,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2402.15818</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Soudani, E. Kanoulas, and F. Hasibi, “Fine tuning vs. retrieval augmented generation for less popular knowledge,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2403.01432</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020, pp. 9459–9474. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper\_files/paper/2020/file/6b493230205f\\780e1bc26945df7481e5-Paper.pdf</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Llamaindex, “Token text splitter,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/token_text_splitter/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
LangChain, “Langchain Text Splitters,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
N. Muennighoff, N. Tazi, L. Magne, and N. Reimers, “Mteb: Massive text embedding benchmark,” 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2210.07316</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Z. Li, X. Zhang, Y. Zhang, D. Long, P. Xie, and M. Zhang, “Towards general text embeddings with multi-stage contrastive learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2308.03281</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>.   Association for Computational Linguistics, 11 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/1908.10084</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. Taunk, S. De, S. Verma, and A. Swetapadma, “A brief review of nearest neighbor algorithm for learning and classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">2019 International Conference on Intelligent Computing and Control Systems (ICCS)</em>, 2019, pp. 1255–1260.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Robertson and H. Zaragoza, “The probabilistic relevance framework: Bm25 and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Found. Trends Inf. Retr.</em>, vol. 3, no. 4, p. 333–389, apr 2009. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1561/1500000019</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G. Marvin, N. Hellen, D. Jjingo, and J. Nakatumba-Nabende, “Prompt engineering in large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International conference on data intelligence and cognitive informatics</em>.   Springer, 2023, pp. 387–402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Vatsal and H. Dubey, “A survey of prompt engineering methods in large language models for different nlp tasks,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.12994</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
3GPP-TR-21.905, “3rd generation partnership project; technical specification group services and system aspects; vocabulary for 3gpp specifications (release 17),” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">3GPP Release 17</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, vol. abs/1908.10084, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1908.10084</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang, “Large language models are not robust multiple choice selectors,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Africa, “Specializing large language models for telecom networks,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://zindi.africa/competitions/specializing-large-language-models-for-telecom-networks/data</span>, 2024, accessed: 2024-08-19.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 15:25:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
