<article class="ltx_document ltx_authors_1line">
 <div class="ltx_para" id="p1">
  <span class="ltx_ERROR undefined" id="p1.1">
   \pdfcolInitStack
  </span>
  <p class="ltx_p" id="p1.2">
   tcb@breakable
  </p>
 </div>
 <h1 class="ltx_title ltx_title_document">
  LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhuoling Li
    <sup class="ltx_sup" id="id10.9.id1">
     <span class="ltx_text ltx_font_italic" id="id10.9.id1.1">
      1
     </span>
    </sup>
    Xiaogang Xu
    <sup class="ltx_sup" id="id11.10.id2">
     <span class="ltx_text ltx_font_italic" id="id11.10.id2.1">
      2
     </span>
    </sup>
    Zhenhua Xu
    <sup class="ltx_sup" id="id12.11.id3">
     <span class="ltx_text ltx_font_italic" id="id12.11.id3.1">
      1
     </span>
    </sup>
    SerNam Lim
    <sup class="ltx_sup" id="id13.12.id4">
     <span class="ltx_text ltx_font_italic" id="id13.12.id4.1">
      3
     </span>
    </sup>
    Hengshuang Zhao
    <sup class="ltx_sup" id="id14.13.id5">
     <span class="ltx_text ltx_font_italic" id="id14.13.id5.1">
      1
     </span>
    </sup>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id15.14.id6">
     <span class="ltx_text ltx_font_italic" id="id15.14.id6.1">
      1
     </span>
    </sup>
    HKU
    <sup class="ltx_sup" id="id16.15.id7">
     <span class="ltx_text ltx_font_italic" id="id16.15.id7.1">
      2
     </span>
    </sup>
    CUHK
    <sup class="ltx_sup" id="id17.16.id8">
     <span class="ltx_text ltx_font_italic" id="id17.16.id8.1">
      3
     </span>
    </sup>
    UCF
    <br class="ltx_break"/>
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lizhuoling.github.io/LARM_webpage/" target="_blank" title="">
     https://lizhuoling.github.io/LARM_webpage/
    </a>
   </span>
   <span class="ltx_author_notes">
    Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id9.1">
   Due to the need to interact with the real world, embodied agents are required to possess comprehensive prior knowledge, long-horizon planning capability, and a swift response speed. Despite recent large language model (LLM) based agents achieving promising performance, they still exhibit several limitations. For instance, the output of LLMs is a descriptive sentence, which is ambiguous when determining specific actions. To address these limitations, we introduce the large auto-regressive model (LARM). LARM leverages both text and multi-view images as input and predicts subsequent actions in an auto-regressive manner. To train LARM, we develop a novel data format named auto-regressive node transmission structure and assemble a corresponding dataset. Adopting a two-phase training regimen, LARM successfully harvests enchanted equipment in Minecraft, which demands significantly more complex decision-making chains than the highest achievements of prior best methods. Besides, the speed of LARM is 6.8
   <math alttext="\times" class="ltx_Math" display="inline" id="id9.1.m1.1">
    <semantics id="id9.1.m1.1a">
     <mo id="id9.1.m1.1.1" xref="id9.1.m1.1.1.cmml">
      √ó
     </mo>
     <annotation-xml encoding="MathML-Content" id="id9.1.m1.1b">
      <times id="id9.1.m1.1.1.cmml" xref="id9.1.m1.1.1">
      </times>
     </annotation-xml>
     <annotation encoding="application/x-tex" id="id9.1.m1.1c">
      \times
     </annotation>
    </semantics>
   </math>
   faster.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    In recent years, remarkable progress has been achieved in various artificial intelligence (AI) fields
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ]
    </cite>
    like computer vision
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    and natural language processing
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    , but most of them lack the capacity to physically interact with the real world. To address this disconnect, the concept of embodied AI is introduced
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    . Early embodied agents are predominantly developed on simulation platforms for specific tasks such as object grasping and indoor navigation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ]
    </cite>
    . While notable advancements are achieved, these agents tend to be specialist models confined to isolated tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ]
    </cite>
    . To overcome this limitation, recent studies, including this work, employ Minecraft
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    as a benchmark to explore embodied agents with open-ended objectives and long-horizon reasoning chains.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.2">
    The initial methods for developing such agents primarily rely on reinforcement learning, the exploration of which is inefficient and results in limited performance
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ]
    </cite>
    . Recent works begin to investigate the use of large language models (LLMs) to build agents
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    . Owing to the extensive general knowledge and formidable reasoning capabilities of LLMs, these methods demonstrate promising results with significantly reduced domain-specific engineering efforts
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ]
    </cite>
    . Nevertheless, LLMs continue to exhibit a number of limitations, rendering them ill-suited for embodied AI tasks. The limitations of LLM can be divided into two categories, inadequate model structure and ineffective data utilization. Concerning the model structure, on the one hand, the output of LLM is text-based. As illustrated in Fig.
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , although LLM could describe the future plan well, the specific actions still remain ambiguous. On the other hand, the output of LLM is generated through iterative token prediction, necessitating
    <math alttext="N" class="ltx_Math" display="inline" id="S1.p2.1.m1.1">
     <semantics id="S1.p2.1.m1.1a">
      <mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">
       N
      </mi>
      <annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b">
       <ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">
        ùëÅ
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">
       N
      </annotation>
     </semantics>
    </math>
    inference operations for
    <math alttext="N" class="ltx_Math" display="inline" id="S1.p2.2.m2.1">
     <semantics id="S1.p2.2.m2.1a">
      <mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">
       N
      </mi>
      <annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b">
       <ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">
        ùëÅ
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">
       N
      </annotation>
     </semantics>
    </math>
    tokens, which results in a sluggish speed. Regarding data utilization, the training data for LLMs is predominantly sourced from the internet and lacks intricate dependency knowledge among embodied actions, agent status, and environmental information.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="228" id="S1.F1.g1" src="/html/2405.17424/assets/x1.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    LARM is the first method that achieves enchanted diamond equipment in Minecraft, surpassing previous SOTAs limited to crafting standard diamond tools. Compared with LLMs that generate ambiguous descriptive sentences for determining skills to execute, LARM directly predicts subsequent skills in an auto-regressive way.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To address the limitations of LLM, we propose
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     L
    </span>
    arge
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">
     A
    </span>
    uto-
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">
     R
    </span>
    egressive
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">
     M
    </span>
    odel (LARM). LARM engages in real-time interaction with the environment and predicts subsequent actions in an auto-regressive manner. Besides text information (target task description, historical action, agent position, inventory list, and environment feedback message), LARM incorporates multi-view images as input. Instead of generating a sentence composed of multiple tokens, LARM directly produces a single token to predict the next action. This design eradicates the ambiguity inherent in sentence description outputs and enhances the inference speed of LARM.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    To train LARM, we introduce a unique data organization structure termed as
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">
     A
    </span>
    uto-
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">
     R
    </span>
    egressive
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">
     N
    </span>
    ode
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">
     T
    </span>
    ransmission
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.5">
     S
    </span>
    tructure (ARNTS). Within ARNTS, each data sample is conceptualized as a node, encompassing text information, multi-view images, and the expected subsequent action. A chronological dependency exists among these nodes. The process of accomplishing a complex target can be interpreted as transmissions between different nodes. Adhering to this data structure, we manually collect an ARNTS dataset comprising 2,589 data samples, which cover diversified targets, weathers, and biomes. Leveraging this dataset to train the LARM agent, the agent can effectively learn the dependency knowledge among various information types. Additionally, we have devised three data augmentation strategies, each tailored to the unique characteristics of ARNTS. However, the ARNTS dataset alone is inadequate to train LARM due to its numerous parameters (e.g., 7 billion parameters). To mitigate this challenge, we employ a 34G multi-modal dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    , sourced from Wiki Minecraft webpages, to pre-train the model before proceeding with ARNTS training.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Through integrating the aforementioned efforts, we develop an agent capable of crafting enchanted diamond tools. As shown in Fig.
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    , crafting enchanted diamond tools requires a far more intricate action chain and long-horizon scheduling ability compared to existing state-of-the-art (SOTA) methods
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ]
    </cite>
    , which are only capable of crafting standard diamond tools. Besides, the inference speed of LARM is 6.8
    <math alttext="\times" class="ltx_Math" display="inline" id="S1.p5.1.m1.1">
     <semantics id="S1.p5.1.m1.1a">
      <mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">
       √ó
      </mo>
      <annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b">
       <times id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">
       </times>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">
       \times
      </annotation>
     </semantics>
    </math>
    faster on average compared to an LLM with comparable parameter volume.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     Embodied benchmarks.
    </span>
    Embodied AI recently garners significant attention due to its potential to bridge the gap between virtual algorithms and physical interactions
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    . The recent works in embodied AI primarily focus on enhancing agents‚Äô interaction with the environment, and many benchmarks are established
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ]
    </cite>
    . For instance, Anderson et al. propose a vision-and-language navigation task in photo-realistic environments
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    . Similarly, Chen et al. introduce Touchdown
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    , a game-like task that combines navigation and question-answering in urban environments. Moreover, the emergence of interactive environments like AI2-THOR
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ]
    </cite>
    and Habitat
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ]
    </cite>
    significantly contributes to embodied agent development. However, these benchmarks are mostly restricted to isolated tasks.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Minecraft.
    </span>
    Minecraft provides a platform for exploring models with long-horizon planning capability
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    . Initial methods typically develop Minecraft agents via reinforcement learning
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    . To mitigate the optimization challenges, these methods manually decompose a high-level task into simpler atomic tasks, subsequently training agents to execute these atomic tasks sequentially
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ]
    </cite>
    . However, the performance of these methods remains suboptimal due to the limited exploration efficiency
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib31" title="">
      31
     </a>
     ]
    </cite>
    . To rectify this issue, recent works construct agents based on LLMs. For example, Voyager
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ]
    </cite>
    successfully crafts diamond tools using a training-free system based on GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    . Similarly, DEPS
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib35" title="">
      35
     </a>
     ]
    </cite>
    develops a more comprehensive LLM system, yielding satisfactory results. However, LLMs solely support text input, which is inadequate for encapsulating environmental information. To surmount this hurdle, certain studies train large vision-language models through fine-tuning LLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    . Despite these advancements, the output of these models remains ambiguous text.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">
     Large language models.
    </span>
    GPT-3 emerges as a significant milestone in the evolution of LLMs, showcasing remarkable open-world generalization capabilities across diverse tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    . Subsequently, the fine-tuning of GPT-3 using reinforcement learning with human feedback leads to the creation of ChatGPT, a model that displays an impressive breadth of general knowledge
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    . Further advancements result in the creation of GPT-4, reinforcing the advantages of increasing model size and training data
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    . In addition, there are notable open-source works that train models with fewer parameters while still achieving promising results, such as LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib33" title="">
      33
     </a>
     ]
    </cite>
    . However, a significant limitation of LLMs is their inability to interpret information in images, which are vital information sources. To overcome this, large vision-language models like LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    and Flamingo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    are developed. While these models can describe the content in provided images, they lack the capability to interact with the real world.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Algorithm
  </h2>
  <figure class="ltx_figure" id="S3.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S3.F2.g1" src="/html/2405.17424/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    The overall framework of LARM. In this framework, the network takes the target task description, multi-view images, agent information, and environment feedback as input to predict a skill token. The skill token is matched with the skill embeddings, which are generated based on a pre-prepared skill library, to select the optimum skill. Then, the agent performs this skill, which helps the agent one step closer to completing the target and changes the environment.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Large Auto-Regressive Model
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     LARM is a large embodied AI model containing billions of parameters and predicting subsequent skills in an auto-regressive manner. Its framework is illustrated in Fig.
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . To save training cost, we generate skill embeddings before training LARM. Following Voyager
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     , we design prompts that describe the intended use of skills. Then, GPT-4 is employed to generate skill code based on these prompts and Mineflayer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     . Combining the generated skills, a skill library is derived. We utilize MineCLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     to extract the embeddings of these skills. Every skill is mapped to an embedding vector with the length of
     <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1">
      <semantics id="S3.SS1.p1.1.m1.1a">
       <mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">
        L
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b">
        <ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">
         ùêø
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">
        L
       </annotation>
      </semantics>
     </math>
     . All the embeddings are saved and directly used during training LARM.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     The input to LARM consists of both text and image information. The text information includes the target task description, the agent information, and the environment feedback. Specifically, the agent information contains the last skill performed by the agent, the current 3D position coordinate of the agent in the Minecraft world, the agent‚Äôs inventory list (a list of the resources that the agent owns). This information well describes the current status of the agent. For the environment feedback, two kinds of information are included, i.e., whether the last skill performed by the agent is executed successfully and the game response message to that skill. All the text information is tokenized into text tokens by a frozen tokenizer, the weight of which is initialized from BERT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib20" title="">
       20
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.7">
     Different from previous Minecraft agents that take a monocular image as input
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
      ]
     </cite>
     , LARM adopts
     <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1">
      <semantics id="S3.SS1.p3.1.m1.1a">
       <mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b">
        <ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">
        N
       </annotation>
      </semantics>
     </math>
     views of images in different directions, providing a more comprehensive description of the surrounding environment. The
     <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1">
      <semantics id="S3.SS1.p3.2.m2.1a">
       <mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b">
        <ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">
        N
       </annotation>
      </semantics>
     </math>
     multi-view images are tokenized as
     <math alttext="N\times N^{I}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1">
      <semantics id="S3.SS1.p3.3.m3.1a">
       <mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">
        <mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">
         N
        </mi>
        <mo id="S3.SS1.p3.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.3.m3.1.1.1.cmml">
         √ó
        </mo>
        <msup id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">
         <mi id="S3.SS1.p3.3.m3.1.1.3.2" xref="S3.SS1.p3.3.m3.1.1.3.2.cmml">
          N
         </mi>
         <mi id="S3.SS1.p3.3.m3.1.1.3.3" xref="S3.SS1.p3.3.m3.1.1.3.3.cmml">
          I
         </mi>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b">
        <apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">
         <times id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1">
         </times>
         <ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">
          ùëÅ
         </ci>
         <apply id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.1.1.3">
           superscript
          </csymbol>
          <ci id="S3.SS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.1.1.3.2">
           ùëÅ
          </ci>
          <ci id="S3.SS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3.3">
           ùêº
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">
        N\times N^{I}
       </annotation>
      </semantics>
     </math>
     image tokens by a frozen CLIP encoder. However, since there are
     <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1">
      <semantics id="S3.SS1.p3.4.m4.1a">
       <mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b">
        <ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">
        N
       </annotation>
      </semantics>
     </math>
     images, the number of obtained tokens is increased by
     <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1">
      <semantics id="S3.SS1.p3.5.m5.1a">
       <mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b">
        <ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">
        N
       </annotation>
      </semantics>
     </math>
     times and would result in a significant computational burden if using all these tokens as input to the subsequent decoders. To resolve this problem, we utilize a trainable Q-Former
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib23" title="">
       23
      </a>
      ]
     </cite>
     module to reduce the number of image tokens from
     <math alttext="N\times N^{I}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.1">
      <semantics id="S3.SS1.p3.6.m6.1a">
       <mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">
        <mi id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">
         N
        </mi>
        <mo id="S3.SS1.p3.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.6.m6.1.1.1.cmml">
         √ó
        </mo>
        <msup id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">
         <mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">
          N
         </mi>
         <mi id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml">
          I
         </mi>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b">
        <apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">
         <times id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1">
         </times>
         <ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">
          ùëÅ
         </ci>
         <apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3">
           superscript
          </csymbol>
          <ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">
           ùëÅ
          </ci>
          <ci id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3">
           ùêº
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">
        N\times N^{I}
       </annotation>
      </semantics>
     </math>
     to
     <math alttext="N^{I}" class="ltx_Math" display="inline" id="S3.SS1.p3.7.m7.1">
      <semantics id="S3.SS1.p3.7.m7.1a">
       <msup id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">
        <mi id="S3.SS1.p3.7.m7.1.1.2" xref="S3.SS1.p3.7.m7.1.1.2.cmml">
         N
        </mi>
        <mi id="S3.SS1.p3.7.m7.1.1.3" xref="S3.SS1.p3.7.m7.1.1.3.cmml">
         I
        </mi>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b">
        <apply id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2">
          ùëÅ
         </ci>
         <ci id="S3.SS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3">
          ùêº
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">
        N^{I}
       </annotation>
      </semantics>
     </math>
     , as shown in Fig.
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . Besides reducing image tokens, this Q-Former module is also helpful for fusing the feature existing in multi-view images. After the Q-Former, the image tokens are further transformed by a trainable projector linking the vision branch with decoders. The decoders take both text and image tokens as input to conduct feature interaction. Given that the decoders contain numerous parameters and are challenging to train, we initialize the parameters from the LLaVA weight
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib24" title="">
       24
      </a>
      ]
     </cite>
     and freeze them during training. Additionally, a trainable LoRA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib18" title="">
       18
      </a>
      ]
     </cite>
     is applied to assist the model in understanding Minecraft knowledge.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F4">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.1" style="width:173.4pt;">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="271" id="S3.F4.1.g1" src="/html/2405.17424/assets/x3.png" width="377"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         Figure 3:
        </span>
        An example of various recipes for crafting a stick.
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.F4.2" style="width:260.2pt;">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="182" id="S3.F4.2.g1" src="/html/2405.17424/assets/x4.png" width="406"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         Figure 4:
        </span>
        The multi-view images are added with view and image position embeddings.
       </figcaption>
      </figure>
     </div>
    </div>
   </figure>
   <div class="ltx_para" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     The output of the decoders is multiple tokens and we compress them into a single token with a trainable projector. We name the token produced by this projector as skill token, which contains the information about which skill should be executed as predicted by the LARM model. By conducting cosine distance
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib25" title="">
       25
      </a>
      ]
     </cite>
     based matching between the skill token and the many pre-generated skill embeddings, the most similar skill is selected for the agent to perform. The chosen skill alters both the status of the agent and the environment. LARM repeats the above process in an auto-regressive way until an
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">
      &lt;End&gt;
     </span>
     token is matched, marking the end of this skill execution chain.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Multi-view Vision Perception
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.10">
     In Minecraft, there are often multiple recipes for crafting the same item. For example, as shown in Fig.
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Large Auto-Regressive Model ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , different types of logs can be collected by chopping trees to craft sticks. When the agent is located in various biomes, the nearest tree category varies, and the optimal recipe is the one based on this nearest category of tree. Hence, it is important to perceive nearby environment information through images to determine the optimal recipe. To fully observe surrounding objects, we propose to use multi-view images as input. In LARM, by setting the agent observation pitch angle to
     <math alttext="0^{\circ}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1">
      <semantics id="S3.SS2.p1.1.m1.1a">
       <msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">
        <mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">
         0
        </mn>
        <mo id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">
         ‚àò
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b">
        <apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
          superscript
         </csymbol>
         <cn id="S3.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.2">
          0
         </cn>
         <compose id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">
         </compose>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">
        0^{\circ}
       </annotation>
      </semantics>
     </math>
     and the yaw angle to
     <math alttext="0^{\circ}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1">
      <semantics id="S3.SS2.p1.2.m2.1a">
       <msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">
        <mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">
         0
        </mn>
        <mo id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">
         ‚àò
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b">
        <apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
          superscript
         </csymbol>
         <cn id="S3.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2">
          0
         </cn>
         <compose id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">
         </compose>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">
        0^{\circ}
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="90^{\circ}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1">
      <semantics id="S3.SS2.p1.3.m3.1a">
       <msup id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">
        <mn id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">
         90
        </mn>
        <mo id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">
         ‚àò
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b">
        <apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">
          superscript
         </csymbol>
         <cn id="S3.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.p1.3.m3.1.1.2">
          90
         </cn>
         <compose id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">
         </compose>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">
        90^{\circ}
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="180^{\circ}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1">
      <semantics id="S3.SS2.p1.4.m4.1a">
       <msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">
        <mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">
         180
        </mn>
        <mo id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">
         ‚àò
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b">
        <apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">
          superscript
         </csymbol>
         <cn id="S3.SS2.p1.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p1.4.m4.1.1.2">
          180
         </cn>
         <compose id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">
         </compose>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">
        180^{\circ}
       </annotation>
      </semantics>
     </math>
     , and
     <math alttext="270^{\circ}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1">
      <semantics id="S3.SS2.p1.5.m5.1a">
       <msup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">
        <mn id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">
         270
        </mn>
        <mo id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">
         ‚àò
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b">
        <apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">
          superscript
         </csymbol>
         <cn id="S3.SS2.p1.5.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.2">
          270
         </cn>
         <compose id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">
         </compose>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">
        270^{\circ}
       </annotation>
      </semantics>
     </math>
     , the 4 multi-view images illustrated in Fig.
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Large Auto-Regressive Model ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     are obtained, respectively. To enable the LARM model to associate each pixel in these 4 images with voxels in the 3D environment, two kinds of learnable position embeddings are added, the image position embedding (IPE) and view position embedding (VPE) in Fig.
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Large Auto-Regressive Model ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . Denote the size of the multi-view image batch as
     <math alttext="N\times H\times W\times 3" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1">
      <semantics id="S3.SS2.p1.6.m6.1a">
       <mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">
        <mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">
         N
        </mi>
        <mo id="S3.SS2.p1.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.6.m6.1.1.1.cmml">
         √ó
        </mo>
        <mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">
         H
        </mi>
        <mo id="S3.SS2.p1.6.m6.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.6.m6.1.1.1.cmml">
         √ó
        </mo>
        <mi id="S3.SS2.p1.6.m6.1.1.4" xref="S3.SS2.p1.6.m6.1.1.4.cmml">
         W
        </mi>
        <mo id="S3.SS2.p1.6.m6.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.6.m6.1.1.1.cmml">
         √ó
        </mo>
        <mn id="S3.SS2.p1.6.m6.1.1.5" xref="S3.SS2.p1.6.m6.1.1.5.cmml">
         3
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b">
        <apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">
         <times id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1">
         </times>
         <ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">
          ùëÅ
         </ci>
         <ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">
          ùêª
         </ci>
         <ci id="S3.SS2.p1.6.m6.1.1.4.cmml" xref="S3.SS2.p1.6.m6.1.1.4">
          ùëä
         </ci>
         <cn id="S3.SS2.p1.6.m6.1.1.5.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.5">
          3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">
        N\times H\times W\times 3
       </annotation>
      </semantics>
     </math>
     , where
     <math alttext="H" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1">
      <semantics id="S3.SS2.p1.7.m7.1a">
       <mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">
        H
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b">
        <ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">
         ùêª
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">
        H
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="W" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1">
      <semantics id="S3.SS2.p1.8.m8.1a">
       <mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">
        W
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b">
        <ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">
         ùëä
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">
        W
       </annotation>
      </semantics>
     </math>
     represent image height and width. IPE is to distinguish pixels in the same image and its size is
     <math alttext="1\times H\times W\times 1" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1">
      <semantics id="S3.SS2.p1.9.m9.1a">
       <mrow id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">
        <mn id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">
         1
        </mn>
        <mo id="S3.SS2.p1.9.m9.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.9.m9.1.1.1.cmml">
         √ó
        </mo>
        <mi id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">
         H
        </mi>
        <mo id="S3.SS2.p1.9.m9.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.9.m9.1.1.1.cmml">
         √ó
        </mo>
        <mi id="S3.SS2.p1.9.m9.1.1.4" xref="S3.SS2.p1.9.m9.1.1.4.cmml">
         W
        </mi>
        <mo id="S3.SS2.p1.9.m9.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.9.m9.1.1.1.cmml">
         √ó
        </mo>
        <mn id="S3.SS2.p1.9.m9.1.1.5" xref="S3.SS2.p1.9.m9.1.1.5.cmml">
         1
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b">
        <apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">
         <times id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1">
         </times>
         <cn id="S3.SS2.p1.9.m9.1.1.2.cmml" type="integer" xref="S3.SS2.p1.9.m9.1.1.2">
          1
         </cn>
         <ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">
          ùêª
         </ci>
         <ci id="S3.SS2.p1.9.m9.1.1.4.cmml" xref="S3.SS2.p1.9.m9.1.1.4">
          ùëä
         </ci>
         <cn id="S3.SS2.p1.9.m9.1.1.5.cmml" type="integer" xref="S3.SS2.p1.9.m9.1.1.5">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">
        1\times H\times W\times 1
       </annotation>
      </semantics>
     </math>
     . Conversely, VPE is for classifying various views and thus its size is
     <math alttext="N\times 1\times 1\times 1" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1">
      <semantics id="S3.SS2.p1.10.m10.1a">
       <mrow id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">
        <mi id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml">
         N
        </mi>
        <mo id="S3.SS2.p1.10.m10.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.10.m10.1.1.1.cmml">
         √ó
        </mo>
        <mn id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml">
         1
        </mn>
        <mo id="S3.SS2.p1.10.m10.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.10.m10.1.1.1.cmml">
         √ó
        </mo>
        <mn id="S3.SS2.p1.10.m10.1.1.4" xref="S3.SS2.p1.10.m10.1.1.4.cmml">
         1
        </mn>
        <mo id="S3.SS2.p1.10.m10.1.1.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.10.m10.1.1.1.cmml">
         √ó
        </mo>
        <mn id="S3.SS2.p1.10.m10.1.1.5" xref="S3.SS2.p1.10.m10.1.1.5.cmml">
         1
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b">
        <apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">
         <times id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1">
         </times>
         <ci id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">
          ùëÅ
         </ci>
         <cn id="S3.SS2.p1.10.m10.1.1.3.cmml" type="integer" xref="S3.SS2.p1.10.m10.1.1.3">
          1
         </cn>
         <cn id="S3.SS2.p1.10.m10.1.1.4.cmml" type="integer" xref="S3.SS2.p1.10.m10.1.1.4">
          1
         </cn>
         <cn id="S3.SS2.p1.10.m10.1.1.5.cmml" type="integer" xref="S3.SS2.p1.10.m10.1.1.5">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">
        N\times 1\times 1\times 1
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Skill Library Preparation
   </h3>
   <figure class="ltx_figure" id="S3.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S3.F5.g1" src="/html/2405.17424/assets/x5.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     The skill library generation pipeline using two GPT-4 agents, the programmer and checker. The programmer is for writing code and the checker is to check the correctness of generated code.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     In this work, we manually design prompts for GPT-4 to generate the code of 106 skills in the skill library. As shown in Fig.
     <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , to ensure the correctness of code generation, two GPT-4 based agents are built, one serves as the programmer and the other one as the checker. The programmer takes a prompt that describes the skill information, a code format requirement, and a description about the atom functions based on Mineflayer as input to produce the corresponding skill code. The skill is then executed in the Minecraft environment. Afterwards, the execution result, along with the original code, is sent to the checker. If the checker judges that the skill code or execution result do not satisfy the requirement, it informs the programmer about the reason and asks the programmer to rewrite it. Otherwise, the skill code is added to the skill library. Notably, in Fig.
     <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , the skill is generated based on pre-defined prompts. However, the skill library can also be dynamically expanded following the procedures developed in Voyager
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="85" id="S3.F6.g1" src="/html/2405.17424/assets/x6.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     For ARTNS, the procedures of performing a multi-step task is interpreted as a graph. In different nodes, the agent perceives the environment information and selects a skill to execute. During an edge linking two nodes, the agent performs the selected skill, and its status transfers from the start node to the end node. Besides, there exist dependencies among various nodes. For example, as marked by the
     <span class="ltx_text" id="S3.F6.2.1" style="color:#4472C4;">
      blue
     </span>
     dotted lines, Node D depends on the first three nodes, because crafting 1 wooden axe requires the planks, sticks, and crafting table in Minecraft.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Data and Training
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    ARTNS Dataset
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     In ARTNS, we represent the performing a multi-step task process as a sequential chain, which includes nodes and edges that link these nodes. As illustrated in Fig.
     <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     , the nodes denote the moments when the agent perceives environmental information and selects the next skill. When the selected skill is performed, the status of the agent transitions from a node to the next node, represented by the edge linking these two nodes. Notably, there exists dependence relations among nodes, and the next node does not necessarily depend on the previous node. For example, as shown in Fig.
     <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     , making a crafting table requires planks but not sticks, which means Node C depends on the completion of the target in Node A but not Node B. In addition, a node could depend on multiple previous nodes, like Node D depends on Node A, Node B, and Node C simultaneously.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     In accordance with the ARTNS format, we manually operate an agent to complete various tasks in Minecraft and record the data. Through collecting data in diverse biomes and weather, we obtain a dataset consisting of 2,589 data pairs. Each pair contains the exploration trajectory ID, data pair ID, time, weather, biome name, target, agent position, agent inventory list, the name of the last performed skill and its corresponding execution result, the next skill, and multi-view images. Some multi-view image examples are visualized in Fig.
     <a class="ltx_ref" href="#S4.F7" title="Figure 7 ‚Ä£ 4.1 ARTNS Dataset ‚Ä£ 4 Data and Training ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     . Then, we annotate the dependency relations among nodes, which are represented as a graph like the blue dotted lines illustrated in Fig.
     <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     . This graph describes the skills in some nodes are prerequisites for other nodes, and the agent should learn this knowledge to understand how to schedule future skills. Refer to Section
     <a class="ltx_ref" href="#A6" title="Appendix F Data Collection Procedures ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       F
      </span>
     </a>
     in Appendix for data collection details.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F7">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="135" id="S4.F7.g1" src="/html/2405.17424/assets/x7.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     Some examples of the collected multi-view images, which covers diverse tasks, biomes, weather, etc. The bottom right group of images is a rainy night case so its original images are dim. We adjust its color contrast for clearer visualization, and the input to LARM is still the original form.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    ARTNS-based Augmentation
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Based on the characteristics of ARTNS, we develop three augmentation strategies: (1) target transmission, (2) target rephrasing, and (3) skill roll-back. They are beneficial to improving the robustness of the agent. The target transmission strategy is devised based on the dependency relations among nodes. As shown in Fig.
     <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     , making a wooden pickaxe in Node D requires the planks crafted in Node A. Thus, when an agent is in node A, even if the original target is replaced with the target ‚ÄúCraft 1 wooden pickaxe‚Äù from Node D, the next correct skill is still ‚ÄúCraft 16 planks‚Äù. Based on this concept, we transmit the targets of future nodes to the current node and train LARM to infer the next skill. This training is beneficial to improving the long-horizon scheduling ability of LARM. In the target rephrasing strategy, we utilize LLMs to rewrite the targets in various nodes. For example, the ‚ÄúCraft 4 sticks‚Äù can be augmented as ‚ÄúMake 4 sticks‚Äù or ‚ÄúGo to craft 4 sticks‚Äù. This strategy enhances the robustness in dealing with different target expressions. The skill roll-back strategy is to enable LARM to situations where a skill is executed unsuccessfully. For instance, when an agent is collecting oak logs, by setting the oak log number below expectation and the execution result of the last skill as ‚Äúfail‚Äù, we train LARM to know to re-implement the last skill. In this way, LARM becomes more robust to unexpected situations and can recover from failure automatically.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Two-phase Training
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Since the initial knowledge of LARM about Minecraft is limited and the ARTNS dataset volume is insufficient, we use 34G webpage data
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     crawled from Wiki to pre-train LARM before the ARTNS data training. Specifically, we remove the text comprising fewer than 30 letters in the webpage data. In this way, 63,666 sentences are obtained. The sentences are matched with the closest images on webpages and provided to the model. In Wiki pre-training, we mask a part of these sentences and train the model to predict the masked words. This helps the pre-trained model gain a better understanding of Minecraft. Refer to Section
     <a class="ltx_ref" href="#A4" title="Appendix D Wiki Pre-training Details ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       D
      </span>
     </a>
     in Appendix for more details. The model in Wiki pre-training shares the same network structure and training process as LLaVA. Subsequently, in the ARTNS training stage described in Section
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Large Auto-Regressive Model ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     , we employ the decoder weight of the pre-trained model to initialize the LARM decoders shown in Fig.
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . The experimental results suggest that the devised Wiki pre-training effectively improves the convergence speed of LARM.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.5">
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.5.1">
      Implementation details.
     </span>
     In our implementation, the parameters
     <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1">
      <semantics id="S5.SS1.p1.1.m1.1a">
       <mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b">
        <ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">
         ùëÅ
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">
        N
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="H" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1">
      <semantics id="S5.SS1.p1.2.m2.1a">
       <mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">
        H
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b">
        <ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">
         ùêª
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">
        H
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="W" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1">
      <semantics id="S5.SS1.p1.3.m3.1a">
       <mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">
        W
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b">
        <ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">
         ùëä
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">
        W
       </annotation>
      </semantics>
     </math>
     ,
     <math alttext="L" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1">
      <semantics id="S5.SS1.p1.4.m4.1a">
       <mi id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">
        L
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b">
        <ci id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">
         ùêø
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">
        L
       </annotation>
      </semantics>
     </math>
     , and
     <math alttext="N^{I}" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1">
      <semantics id="S5.SS1.p1.5.m5.1a">
       <msup id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">
        <mi id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">
         N
        </mi>
        <mi id="S5.SS1.p1.5.m5.1.1.3" xref="S5.SS1.p1.5.m5.1.1.3.cmml">
         I
        </mi>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b">
        <apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">
         <csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">
          superscript
         </csymbol>
         <ci id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2">
          ùëÅ
         </ci>
         <ci id="S5.SS1.p1.5.m5.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3">
          ùêº
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">
        N^{I}
       </annotation>
      </semantics>
     </math>
     are set to 4, 480, 640, 512, and 1,024, respectively. LARM is built with 32 decoders, which contain about 7B parameters. The epoch number and learning rate are set to 1 and 2e-4 for Wiki pre-training, and 6 and 2e-5 for the ARTNS training stage. The training batch size is 16 for both Wiki pre-training and ARNTS training. Other training details, like the choice of optimizer and learning scheduler, follow the LLaVA model. All training experiments are conducted on four RTX3090 GPUs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">
      Experimental settings.
     </span>
     Unless specifically stated otherwise, the agent is randomly initialized in a Minecraft environment with an empty inventory. A description of the target task is given to the agent, and the agent is expected to complete this task independently without human intervention. For each task, the agent is tested multiple times in various environments using different target descriptions. This is to verify whether it can complete the given task under a variety of challenging circumstances. The diverse tasks reflect both the long-term scheduling ability and robustness of validated models.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Comparison with Previous Methods
   </h3>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">
      Tech tree mastery comparison.
     </span>
     In this experiment, we compare the tech tree mastery of LARM with previous counterparts, including MineAgent
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     , Plan4MC
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib37" title="">
       37
      </a>
      ]
     </cite>
     , LLAMA-Rider
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     , ReAct
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib36" title="">
       36
      </a>
      ]
     </cite>
     , Reflection
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib32" title="">
       32
      </a>
      ]
     </cite>
     , AutoGPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib4" title="">
       4
      </a>
      ]
     </cite>
     , Voyager
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     , and STEVE
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
      ]
     </cite>
     . Among the compared methods, MineAgent and Plan4MC are based on reinforcement learning. Due to their inefficiency in exploring the environment, their performances are limited, and they can only conduct some simple tasks, like collecting logs and crafting wooden tools. Similarly, although LLAMA-Rider is an LLM-based method, it is established upon the skills learned from Plan4MC. Hence, its tech mastery achievement is also limited. ReAct, Reflection, and AutoGPT are LLMs specially designed for conducting task-specific skill scheduling, and previous works utilize them to build embodied agents in Minecraft
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     . Voyager is a competitive counterpart built based on GPT4 and obtains the diamond tool achievement using LLM for the first time, which confirms the superiority of LLM. Nevertheless, Voyager only supports text input, and thus its environment perception capability is restricted. To bridge this gap, STEVE leverages the power of large vision-language models to perform long-term skill scheduling and derives promising achievement.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     The task completion success rates of LARM and reinforcement learning skill based methods, which include MineAgent, Plan4MC, and LLaMA-Rider, are reported in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . A variety of tasks are tested and the best method is marked in light gray. It can be observed that the reinforcement learning skill based methods display significantly poorer performance compared with LARM, which can complete almost all these tasks with a success rate of 100%. Besides, except LARM, all these methods fail to craft a diamond sword and enchanted sword.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T1">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Performance comparison with reinforcement learning skill based methods. Following previous works
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ;
      <a class="ltx_ref" href="#bib.bib13" title="">
       13
      </a>
      ]
     </cite>
     , except for the tasks ‚Äúharvest diamond sword‚Äù and ‚Äúharvest enchanted sword‚Äù where the methods are tested 3 times, all the methods are run 30 times in other tasks. The shown numbers are the success rates of the methods in completing these tasks successfully. The best method is highlighted in light gray. Refer to Section
     <a class="ltx_ref" href="#A5" title="Appendix E Details of Compared Methods ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       E
      </span>
     </a>
     in Appendix for more experimental details.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.16" style="width:411.9pt;height:269.8pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-29.5pt,19.3pt) scale(0.874887401386689,0.874887401386689) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.16.16">
       <thead class="ltx_thead">
        <tr class="ltx_tr" id="S5.T1.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T1.1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          Task
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          MineAgent
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          Plan4MC
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          LLaMA-Rider Base
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          LLaMA-Rider
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1.1" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.1.1.1.1.1" style="background-color:#EBEBEB;">
           LARM
           <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="39" id="S5.T1.1.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/larm.png" width="22"/>
          </span>
         </th>
        </tr>
       </thead>
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S5.T1.2.2.2">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T1.2.2.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest stick
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.2.2.2.1.g1" src="/html/2405.17424/assets/Figure/minecraft/stick.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.30
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.23
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.43
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.2.2.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.2.2.2.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.3.3.3">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.3.3.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest crafting table
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.3.3.3.1.g1" src="/html/2405.17424/assets/Figure/minecraft/crafting_table.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.03
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.30
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.37
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.67
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.3.3.3.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.3.3.3.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.4.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.4.4.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest bowl
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.4.4.4.1.g1" src="/html/2405.17424/assets/Figure/minecraft/bowl.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.47
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.73
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.97
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.4.4.4.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.4.4.4.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.5.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.5.5.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest chest
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.5.5.5.1.g1" src="/html/2405.17424/assets/Figure/minecraft/chest.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.5.5.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.5.5.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.23
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.5.5.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.67
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.5.5.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.77
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.5.5.5.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.5.5.5.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.6.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.6.6.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest wooden sword
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.6.6.6.1.g1" src="/html/2405.17424/assets/Figure/minecraft/wooden_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.47
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.6.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.63
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.6.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.10
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.6.6.6.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.6.6.6.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.7.7.7">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.7.7.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest furnace
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.7.7.7.1.g1" src="/html/2405.17424/assets/Figure/minecraft/furnace.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.37
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.17
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.7.7.7.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.7.7.7.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.8.8.8">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.8.8.8.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest stone stairs
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.8.8.8.1.g1" src="/html/2405.17424/assets/Figure/minecraft/stone_stairs.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.8.8.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.8.8.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.47
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.8.8.8.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.8.8.8.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.57
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.8.8.8.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.8.8.8.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.9.9.9">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.9.9.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest stone sword
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.9.9.9.1.g1" src="/html/2405.17424/assets/Figure/minecraft/stone_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.10
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.9.9.9.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.9.9.9.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.10.10.10">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.10.10.10.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest iron ingot
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.10.10.10.1.g1" src="/html/2405.17424/assets/Figure/minecraft/iron_ingot.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.10.10.10.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.10.10.10.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.47
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.10.10.10.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.03
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.10.10.10.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.13
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.10.10.10.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.10.10.10.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.11.11.11">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.11.11.11.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest bucket
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.11.11.11.1.g1" src="/html/2405.17424/assets/Figure/minecraft/bucket.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.11.11.11.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.11.11.11.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.20
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.11.11.11.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.11.11.11.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.11.11.11.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.11.11.11.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.12.12.12">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.12.12.12.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest iron sword
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.12.12.12.1.g1" src="/html/2405.17424/assets/Figure/minecraft/iron_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.12.12.12.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.12.12.12.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.20
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.12.12.12.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.12.12.12.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.12.12.12.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.12.12.12.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.13.13.13">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.13.13.13.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest beef
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.13.13.13.1.g1" src="/html/2405.17424/assets/Figure/minecraft/beef.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.13.13.13.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.33
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.13.13.13.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.43
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.13.13.13.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.03
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.13.13.13.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.03
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.13.13.13.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.13.13.13.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.14.14.14">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.14.14.14.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest mutton
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.14.14.14.1.g1" src="/html/2405.17424/assets/Figure/minecraft/mutton.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.14.14.14.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.35
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.14.14.14.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.33
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.14.14.14.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.14.14.14.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.03
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.14.14.14.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.14.14.14.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.15.15.15">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T1.15.15.15.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest diamond sword
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T1.15.15.15.1.g1" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T1.15.15.15.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.15.15.15.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.15.15.15.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.15.15.15.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T1.15.15.15.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.15.15.15.6.1" style="background-color:#EBEBEB;">
           1.00
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T1.16.16.16">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T1.16.16.16.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Harvest enchanted sword
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="24" id="S5.T1.16.16.16.1.g1" src="/html/2405.17424/assets/Figure/minecraft/enchanted_diamond_sword.png" width="24"/>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.16.16.16.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.16.16.16.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.16.16.16.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.16.16.16.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          0.00
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.16.16.16.6" style="background-color:#EBEBEB;padding-left:4.3pt;padding-right:4.3pt;">
          <span class="ltx_text" id="S5.T1.16.16.16.6.1" style="background-color:#EBEBEB;">
           0.67
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <figure class="ltx_table" id="S5.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Performance comparison with LLM based methods on the tech tree mastery. Following previous works
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ]
     </cite>
     , all the methods are run 3 times. The fractions in this table show the successful trials of every method within 3 runs. Notably, all the compared methods demand tens of prompting iterations, while LARM obtains the enchanted tool achievement with only 1 prompt. The best method is highlighted in light gray. Refer to Section
     <a class="ltx_ref" href="#A5" title="Appendix E Details of Compared Methods ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       E
      </span>
     </a>
     in Appendix for more experimental details.
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.6" style="width:390.3pt;height:124.3pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-6.4pt,2.0pt) scale(0.96835540593082,0.96835540593082) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.6.6">
       <thead class="ltx_thead">
        <tr class="ltx_tr" id="S5.T2.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          Achievement
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          ReAct
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          Reflexion
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          AutoGPT
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          Voyager
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.7" style="padding-left:5.7pt;padding-right:5.7pt;">
          STEVE
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.1.1.1.1.1" style="background-color:#EBEBEB;">
           LARM
           <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="39" id="S5.T2.1.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/larm.png" width="22"/>
          </span>
         </th>
        </tr>
       </thead>
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S5.T2.2.2.2">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">
          Wooden Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T2.2.2.2.1.g1" src="/html/2405.17424/assets/Figure/minecraft/wooden_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.7" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.2.2.2.7.1" style="background-color:#EBEBEB;">
           3/3
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T2.3.3.3">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.3.3.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">
          Stone Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T2.3.3.3.1.g1" src="/html/2405.17424/assets/Figure/minecraft/stone_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.3.3.3.7" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.3.3.3.7.1" style="background-color:#EBEBEB;">
           3/3
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T2.4.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.4.4.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">
          Iron Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T2.4.4.4.1.g1" src="/html/2405.17424/assets/Figure/minecraft/iron_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.4.4.4.7" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.4.4.4.7.1" style="background-color:#EBEBEB;">
           3/3
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T2.5.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.5.5.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">
          Diamond Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T2.5.5.5.1.g1" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          1/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.7" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.5.5.5.7.1" style="background-color:#EBEBEB;">
           3/3
          </span>
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T2.6.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">
          Enchanted Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="24" id="S5.T2.6.6.6.1.g1" src="/html/2405.17424/assets/Figure/minecraft/enchanted_diamond_sword.png" width="24"/>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.4" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.5" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.6" style="padding-left:5.7pt;padding-right:5.7pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.6.7" style="background-color:#EBEBEB;padding-left:5.7pt;padding-right:5.7pt;">
          <span class="ltx_text" id="S5.T2.6.6.6.7.1" style="background-color:#EBEBEB;">
           2/3
          </span>
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
   <div class="ltx_para" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     We further compare LARM with LLM based methods in Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . It can be found that the results of the methods in Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     are significantly better than those in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . This observation indicates that LLM-based agents generally outperform reinforcement learning skill based ones. Moreover, LARM is the only method capable of crafting enchanted tools, which requires a complex skill execution chain. Therefore, the superiority of LARM is confirmed.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F8">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="161" id="S5.F8.g1" src="/html/2405.17424/assets/x8.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 8:
     </span>
     Analysis on the effect of the Wiki pre-training. (a) The most advanced items achieved by the models with and without Wiki pre-training after various iterations of ARTNS training. It can be observed that the model converges more quickly and presents better performance with the ARTNS training phase. (b) The model initialized from LLaVA lacks knowledge about Minecraft and gives a false answer to the question. By contrast, the model after Wiki pre-training answers correctly. The key points of the false and correct responses are highlighted in
     <span class="ltx_text" id="S5.F8.3.1" style="color:#C45912;">
      bronze
     </span>
     and
     <span class="ltx_text" id="S5.F8.4.2" style="color:#548236;">
      green
     </span>
     , respectively.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS2.p4">
    <p class="ltx_p" id="S5.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">
      Inference efficiency.
     </span>
     Besides the tech tree mastery achievement, inference efficiency is also important. However, many aforementioned methods are implemented based on LLM APIs. The inference speed of these methods is significantly affected by network latency, while LARM is a model deployed in local machines. Additionally, the GPUs used by these methods may vary and are uncontrollable. Therefore, directly comparing their inference speeds is meaningless. Thankfully, the methods built upon LLM share similar network architectures, i.e., the Transformer structure. Thus, it is reasonable to compare LARM with a LLM of the same parameter volume to validate inference efficiency. Following this thought, we compare the inference time costs of LARM and LLaVA-1.6 with both 7B parameters. They are tested in 100 cases to compute the average inference time, with each case corresponding to a node in Fig.
     <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     . The output of LLaVA is prompted to be as concise as possible. This experiment is performed using one RTX3090 GPU. Through this experiment, we find that the average inference time of LLaVA on the 100 cases is 5.78 seconds while LARM is 0.85 seconds, which indicates that LARM is 6.8
     <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1">
      <semantics id="S5.SS2.p4.1.m1.1a">
       <mo id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">
        √ó
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b">
        <times id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">
        </times>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">
        \times
       </annotation>
      </semantics>
     </math>
     more efficient. The efficiency of LARM stems from its characteristic that it generates a single token rather than a sequence of tokens at each step.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T3">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Ablation study on the ARTNS-based augmentation strategies including target transmission (TT), target rephrasing (TR), and skill roll-back (SR). The setting of this experiment follows Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
    </figcaption>
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.5" style="width:433.6pt;height:104.2pt;vertical-align:-0.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-17.7pt,4.3pt) scale(0.92452565133373,0.92452565133373) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.5.5">
       <thead class="ltx_thead">
        <tr class="ltx_tr" id="S5.T3.5.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.5.5.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          TT
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.5.5.5.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          TR
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.5.5.5.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          SR
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          Wooden Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T3.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/wooden_sword.png" width="15"/>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          Stone Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T3.2.2.2.2.g1" src="/html/2405.17424/assets/Figure/minecraft/stone_sword.png" width="15"/>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          Iron Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T3.3.3.3.3.g1" src="/html/2405.17424/assets/Figure/minecraft/iron_sword.png" width="15"/>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          Diamond Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="S5.T3.4.4.4.4.g1" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.5.5.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          Enchanted Tool
          <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="24" id="S5.T3.5.5.5.5.g1" src="/html/2405.17424/assets/Figure/minecraft/enchanted_diamond_sword.png" width="24"/>
         </th>
        </tr>
       </thead>
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S5.T3.5.5.6.1">
         <th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T3.5.5.6.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T3.5.5.6.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.5.5.6.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.6.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.6.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.6.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.6.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.6.1.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          0/3
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T3.5.5.7.2">
         <th class="ltx_td ltx_th ltx_th_row" id="S5.T3.5.5.7.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.5.5.7.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.5.5.7.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          1/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          0/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.7.2.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          0/3
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T3.5.5.8.3">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.5.5.8.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_th ltx_th_row" id="S5.T3.5.5.8.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.5.5.8.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.8.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.8.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.8.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.8.3.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.8.3.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          1/3
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T3.5.5.9.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.5.5.9.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.5.5.9.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S5.T3.5.5.9.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         </th>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          2/3
         </td>
         <td class="ltx_td ltx_align_center" id="S5.T3.5.5.9.4.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          1/3
         </td>
        </tr>
        <tr class="ltx_tr" id="S5.T3.5.5.10.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T3.5.5.10.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T3.5.5.10.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.5.5.10.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">
          ‚úì
         </th>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.5.10.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.5.10.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.5.10.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.5.10.5.7" style="padding-left:4.3pt;padding-right:4.3pt;">
          3/3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.5.5.10.5.8" style="padding-left:4.3pt;padding-right:4.3pt;">
          2/3
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Ablation Study
   </h3>
   <figure class="ltx_figure" id="S5.F9">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="209" id="S5.F9.g1" src="/html/2405.17424/assets/x9.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 9:
     </span>
     Qualitative study on the multi-view vision input. (a) LARM utilizes vision information to collect water and lava and then obtains obsidian. Due to the space limit, we only visualize the image view facing toward the target. (b) LARM can dynamically select the optimal recipe (acacia log, highlighted in
     <span class="ltx_text" id="S5.F9.3.1" style="color:#FFFF00;">
      yellow
     </span>
     ) for crafting sticks based on multi-view vision input. By contrast, previous methods often directly adopt the default recipe (oak log, highlighted in
     <span class="ltx_text" id="S5.F9.4.2" style="color:#FF8000;">
      orange
     </span>
     ), which causes them to explore a longer distance for gathering materials and this process could take quite a longer time.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">
      Effectiveness of Wiki pre-training.
     </span>
     LARM is initialized using the weight of LLaVA, which lacks knowledge about Minecraft. To bridge this gap, we introduce the Wiki pre-training phase to enrich the knowledge. In this part, we examine the impact of Wiki pre-training on the performance of the trained model and visualize the results in Fig.
     <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     In Fig.
     <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     (a), we compare the most advanced items obtained by the models after different iterations of ARTNS training. Achieving an upper item indicates more superior performance, and the two curves correspond to models with and without Wiki pre-training. From Fig.
     <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     (a), it can be inferred Wiki pre-training accelerates the convergence of LARM and contributes to better performance.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p3">
    <p class="ltx_p" id="S5.SS3.p3.1">
     Fig.
     <a class="ltx_ref" href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     (b) presents the responses of the models with and without Wiki pre-training to a question about Minecraft. The model without Wiki pre-training is exactly the original LLaVA-1.6-7B model. The key points of these two answers are marked in different colors. We can find that although the question is about how to combat a horse when a diamond sword is in the inventory, the model without Wiki pre-training provides an answer about riding a horse instead. Besides, the provided recipe for crafting a saddle is incorrect. By contrast, the model after Wiki pre-training accurately and in detail describes the steps for combating a horse. This demonstrates that Wiki pre-training effectively enriches the knowledge of LARM about Minecraft.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p4">
    <p class="ltx_p" id="S5.SS3.p4.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">
      Effectiveness of ARTNS-based augmentation.
     </span>
     In this part, we ablate the effectiveness of the three augmentation strategies, including target transmission (TT), target rephrasing (TR), and skill roll-back (SR). The results are reported in Table
     <a class="ltx_ref" href="#S5.T3" title="Table 3 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . Given the limited data volume of the collected ARTNS dataset, we can find that these augmentation strategies are crucial for the performance of LARM. Specifically, TR and SR boost the results significantly by improving the robustness of LARM. TT contributes the most to tech tree mastery. Without TT, LARM often chooses an incorrect skill to perform during the exploration process. This is because completing a complex task in Minecraft demands multiple steps of decision-making, and any wrong decision at any step could lead to final failure. TT effectively reduces the frequency of incorrect decision-making.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S5.SS3.p5">
    <p class="ltx_p" id="S5.SS3.p5.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p5.1.1">
      Effectiveness of multi-view vision input.
     </span>
     In this study, we qualitatively analyze the effectiveness of multi-view vision input. As shown in Fig.
     <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‚Ä£ 5.3 Ablation Study ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     (a), the agent aims to harvest obsidian, which requires the fusion of water and lava. The agent utilizes vision information to search for water and lava, and eventually successfully produces obsidian. If the input is limited to a single-view image instead of multi-view images, the agent may fail to detect the necessary targets.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p6">
    <p class="ltx_p" id="S5.SS3.p6.1">
     In Fig.
     <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‚Ä£ 5.3 Ablation Study ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     (b), we compare the behaviors of LARM and previous methods
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib34" title="">
       34
      </a>
      ;
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
      ]
     </cite>
     in selecting a recipe for crafting sticks. Thanks to the multi-view vision input, LARM can identify that the nearest source of logs is an acacia tree. By contrast, previous methods often default to the standard recipe, which involves using oak logs to craft sticks. However, finding oak trees may take a considerably longer time. The results confirm the effectiveness of multi-view image input and the superiority of LARM in dynamically choosing the optimal recipe.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion and Limitation
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this work, we have introduced LARM, an embodied intelligence predicting subsequent skills in an auto-regressive manner. To train LARM, a new data organization structure has been proposed and a corresponding dataset has been collected. Compared with previous methods, LARM presents stronger long-horizon scheduling ability and faster inference speed. LARM is the first method capable of crafting enchanted diamond tools in Minecraft. We hope this work significantly contributes to the development of embodied intelligence. At the current stage, LARM is mainly tested in Minecraft without deploying in real-world scenarios, which is the primary limitation of this work.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,
Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et¬†al.: Gpt-4
technical report. arXiv:2303.08774 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., et¬†al.: Flamingo: a visual language
model for few-shot learning. NeurIPS (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S√ºnderhauf, N.,
Reid, I., Gould, S., Van Den¬†Hengel, A.: Vision-and-language navigation:
Interpreting visually-grounded navigation instructions in real environments.
In: CVPR (2018)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     Significant-gravitas/auto-gpt: An experimental open-source attempt to make
gpt-4 fully autonomous. (2023),
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/Auto-GPT/tree/master" target="_blank" title="">
      https://github.com/Significant-Gravitas/Auto-GPT/tree/master
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A.,
Houghton, B., Sampedro, R., Clune, J.: Video pretraining (vpt): Learning to
act by watching unlabeled online videos. NeurIPS (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     Bredeche, N., Haasdijk, E., Prieto, A.: Embodied evolution in collective
robotics: a review. Front. Robot. AI (2018)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C.,
Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et¬†al.: Rt-1: Robotics
transformer for real-world control at scale. arXiv:2212.06817 (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et¬†al.: Language models
are few-shot learners. NeurIPS (2020)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     Chen, H., Suhr, A., Misra, D., Snavely, N., Artzi, Y.: Touchdown: Natural
language navigation and spatial reasoning in visual street environments. In:
CVPR (2019)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     Chrisley, R.: Embodied artificial intelligence. Artificial Intelligence (2003)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     Duan, J., Yu, S., Tan, H.L., Zhu, H., Tan, C.: A survey of embodied ai: From
simulators to research tasks. IETCI (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A.,
Huang, D.A., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied
agents with internet-scale knowledge. NeurIPS (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     Feng, Y., Wang, Y., Liu, J., Zheng, S., Lu, Z.: Llama rider: Spurring large
language models to explore the open world. arXiv:2310.08922 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     Frazier, S., Riedl, M.: Improving deep reinforcement learning in minecraft with
action advice. In: AAAI (2019)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     Guo, H., Wu, F., Qin, Y., Li, R., Li, K., Li, K.: Recent trends in task and
motion planning for robotics: A survey. ACM Computing Surveys (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     Guss, W.H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M.,
Salakhutdinov, R.: Minerl: a large-scale dataset of minecraft demonstrations.
In: IJCAI (2019)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: CVPR (2016)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.,
et¬†al.: Lora: Low-rank adaptation of large language models. In: ICLR (2021)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     Huang, C., Mees, O., Zeng, A., Burgard, W.: Visual language maps for robot
navigation. In: ICRA (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     Kenton, J.D.M.W.C., Toutanova, L.K.: Bert: Pre-training of deep bidirectional
transformers for language understanding. In: NAACL (2019)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A.,
Deitke, M., Ehsani, K., Gordon, D., Zhu, Y., et¬†al.: Ai2-thor: An interactive
3d environment for visual ai. arXiv:1712.05474 (2017)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature (2015)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. In: ICML
(2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv:2304.08485
(2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     Nguyen, H.V., Bai, L.: Cosine similarity metric learning for face verification.
In: ACCV (2010)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     OpenAI: Chatgpt.
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt/" target="_blank" title="">
      https://openai.com/blog/chatgpt/
     </a>
     (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     PrismarineJS.: Prismarinejs/mineflayer: Create minecraft bots with a powerful,
stable, and high level javascript api (2013)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et¬†al.: Learning transferable visual
models from natural language supervision. In: ICML (2021)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A.,
Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T.,
et¬†al.: A generalist agent. arXiv preprint arXiv:2205.06175 (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub,
J., Liu, J., Koltun, V., Malik, J., et¬†al.: Habitat: A platform for embodied
ai research. In: ICCV (2019)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [31]
    </span>
    <span class="ltx_bibblock">
     Scheller, C., Schraner, Y., Vogel, M.: Sample efficient reinforcement learning
through learning from demonstrations in minecraft. In: NeurIPS Workshop
(2020)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [32]
    </span>
    <span class="ltx_bibblock">
     Shinn, N., Labash, B., Gopinath, A.: Reflexion: an autonomous agent with
dynamic memory and self-reflection. arXiv:2303.11366 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [33]
    </span>
    <span class="ltx_bibblock">
     Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., et¬†al.: Llama: Open and
efficient foundation language models. arXiv:2302.13971 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [34]
    </span>
    <span class="ltx_bibblock">
     Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L.,
Anandkumar, A.: Voyager: An open-ended embodied agent with large language
models. arXiv:2305.16291 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [35]
    </span>
    <span class="ltx_bibblock">
     Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., Liang, Y.: Describe, explain,
plan and select: Interactive planning with large language models enables
open-world multi-task agents. arXiv:2302.01560 (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [36]
    </span>
    <span class="ltx_bibblock">
     Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.R., Cao, Y.:
React: Synergizing reasoning and acting in language models. In: ICLR (2022)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [37]
    </span>
    <span class="ltx_bibblock">
     Yuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H., Lu, Z.: Skill
reinforcement learning and planning for open-world long-horizon tasks. In:
NeurIPS Workshop (2023)
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     [38]
    </span>
    <span class="ltx_bibblock">
     Zhao, Z., Chai, W., Wang, X., Boyi, L., Hao, S., Cao, S., Ye, T., Hwang, J.N.,
Wang, G.: See and think: Embodied agent in virtual environment.
arXiv:2311.15209 (2023)
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <nav class="ltx_TOC ltx_list_toc ltx_toc_toc">
  <h6 class="ltx_title ltx_title_contents">
   Contents
  </h6>
  <ol class="ltx_toclist">
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S1" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       1
      </span>
      Introduction
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S2" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       2
      </span>
      Related Work
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S3" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       3
      </span>
      Algorithm
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS1" title="In 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.1
        </span>
        Large Auto-Regressive Model
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS2" title="In 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.2
        </span>
        Multi-view Vision Perception
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S3.SS3" title="In 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         3.3
        </span>
        Skill Library Preparation
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S4" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       4
      </span>
      Data and Training
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS1" title="In 4 Data and Training ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.1
        </span>
        ARTNS Dataset
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS2" title="In 4 Data and Training ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.2
        </span>
        ARTNS-based Augmentation
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S4.SS3" title="In 4 Data and Training ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         4.3
        </span>
        Two-phase Training
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S5" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       5
      </span>
      Experiments
     </span>
    </a>
    <ol class="ltx_toclist ltx_toclist_section">
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS1" title="In 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.1
        </span>
        Experimental Setup
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS2" title="In 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.2
        </span>
        Comparison with Previous Methods
       </span>
      </a>
     </li>
     <li class="ltx_tocentry ltx_tocentry_subsection">
      <a class="ltx_ref" href="#S5.SS3" title="In 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
       <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref">
         5.3
        </span>
        Ablation Study
       </span>
      </a>
     </li>
    </ol>
   </li>
   <li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="#S6" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       6
      </span>
      Conclusion and Limitation
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A1" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       A
      </span>
      Appendix Overview
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A2" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       B
      </span>
      Case Study
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A3" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       C
      </span>
      Study on training data volume
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A4" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       D
      </span>
      Wiki Pre-training Details
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A5" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       E
      </span>
      Details of Compared Methods
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A6" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       F
      </span>
      Data Collection Procedures
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A7" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       G
      </span>
      Analysis on LLM Output
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A8" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       H
      </span>
      LARM Text Input
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A9" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       I
      </span>
      Skill Library Description
     </span>
    </a>
   </li>
   <li class="ltx_tocentry ltx_tocentry_appendix">
    <a class="ltx_ref" href="#A10" title="In LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_title">
      <span class="ltx_tag ltx_tag_ref">
       J
      </span>
      Discussion on LARM Output
     </span>
    </a>
   </li>
  </ol>
 </nav>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Appendix Overview
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    Due to the page-limit of the main paper, we provide more sufficient analysis of LARM in this Appendix. The content in this Appendix is structured as follows:
   </p>
  </div>
  <div class="ltx_para" id="A1.p2">
   <p class="ltx_p" id="A1.p2.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p2.1.m1.1">
     <semantics id="A1.p2.1.m1.1a">
      <mo id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b">
       <ci id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A2" title="Appendix B Case Study ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      B
     </span>
    </a>
    : In the main paper, we mainly show the result of LARM on crafting an enchanted diamond sword. In this section, we show more behavior examples of LARM in performing other tasks, i.e., searching a village, building a nether portal and entering the nether, and multi-agent collaboration to combat zombies.
   </p>
  </div>
  <div class="ltx_para" id="A1.p3">
   <p class="ltx_p" id="A1.p3.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p3.1.m1.1">
     <semantics id="A1.p3.1.m1.1a">
      <mo id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b">
       <ci id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A3" title="Appendix C Study on training data volume ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      C
     </span>
    </a>
    : As the training data volumn is often important for deep learning based methods, we study how the data amount affects the performance of LARM in this section.
   </p>
  </div>
  <div class="ltx_para" id="A1.p4">
   <p class="ltx_p" id="A1.p4.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p4.1.m1.1">
     <semantics id="A1.p4.1.m1.1a">
      <mo id="A1.p4.1.m1.1.1" xref="A1.p4.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p4.1.m1.1b">
       <ci id="A1.p4.1.m1.1.1.cmml" xref="A1.p4.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p4.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A4" title="Appendix D Wiki Pre-training Details ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      D
     </span>
    </a>
    : In this part, we further describe the procedures of the Wiki pre-training stage.
   </p>
  </div>
  <div class="ltx_para" id="A1.p5">
   <p class="ltx_p" id="A1.p5.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p5.1.m1.1">
     <semantics id="A1.p5.1.m1.1a">
      <mo id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b">
       <ci id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A5" title="Appendix E Details of Compared Methods ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    : In Minecraft, various methods often adopt different training and testing protocols due to the lack of a widely accepted benchmark. Therefore, the compared methods reported in Table
    <a class="ltx_ref" href="#S5.T1" title="Table 1 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    and Table
    <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    may adopt different training and validation procedures from others. In this section, we elaborate on the details of these compared methods.
   </p>
  </div>
  <div class="ltx_para" id="A1.p6">
   <p class="ltx_p" id="A1.p6.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p6.1.m1.1">
     <semantics id="A1.p6.1.m1.1a">
      <mo id="A1.p6.1.m1.1.1" xref="A1.p6.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p6.1.m1.1b">
       <ci id="A1.p6.1.m1.1.1.cmml" xref="A1.p6.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p6.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A6" title="Appendix F Data Collection Procedures ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      F
     </span>
    </a>
    : This section describe how the ARTNS Dataset is collected.
   </p>
  </div>
  <div class="ltx_para" id="A1.p7">
   <p class="ltx_p" id="A1.p7.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p7.1.m1.1">
     <semantics id="A1.p7.1.m1.1a">
      <mo id="A1.p7.1.m1.1.1" xref="A1.p7.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p7.1.m1.1b">
       <ci id="A1.p7.1.m1.1.1.cmml" xref="A1.p7.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p7.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A7" title="Appendix G Analysis on LLM Output ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      G
     </span>
    </a>
    : We begin with an in-depth analysis on the output of large language models (LLMs). In this analysis, we compare the Minecraft knowledge of various popular large language models (LLMs), which include LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    , ChatGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    , and GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    . The experimental results reveal that the output description is not only ambiguous but also often incorrect.
   </p>
  </div>
  <div class="ltx_para" id="A1.p8">
   <p class="ltx_p" id="A1.p8.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p8.1.m1.1">
     <semantics id="A1.p8.1.m1.1a">
      <mo id="A1.p8.1.m1.1.1" xref="A1.p8.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p8.1.m1.1b">
       <ci id="A1.p8.1.m1.1.1.cmml" xref="A1.p8.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p8.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A8" title="Appendix H LARM Text Input ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      H
     </span>
    </a>
    : As explained in the main paper, the input to LAM consists of two parts, the text information and image information. The text information describes information about the agent status, surrounding environment information, and system feedback. In this part, we show how the different information is organized as the text input, and several examples are presented.
   </p>
  </div>
  <div class="ltx_para" id="A1.p9">
   <p class="ltx_p" id="A1.p9.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p9.1.m1.1">
     <semantics id="A1.p9.1.m1.1a">
      <mo id="A1.p9.1.m1.1.1" xref="A1.p9.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p9.1.m1.1b">
       <ci id="A1.p9.1.m1.1.1.cmml" xref="A1.p9.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p9.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A9" title="Appendix I Skill Library Description ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      I
     </span>
    </a>
    : As mentioned in the main paper, a skill library generation pipeline is devised to produce the skills in the skill library. These skills serve as basic actions for LARM. In this part, we explain these skills. Specifically, the code of a skill is presented as an example. In addition, the names and abstract descriptions of more skills are presented.
   </p>
  </div>
  <div class="ltx_para" id="A1.p10">
   <p class="ltx_p" id="A1.p10.2">
    <math alttext="\bullet" class="ltx_Math" display="inline" id="A1.p10.1.m1.1">
     <semantics id="A1.p10.1.m1.1a">
      <mo id="A1.p10.1.m1.1.1" xref="A1.p10.1.m1.1.1.cmml">
       ‚àô
      </mo>
      <annotation-xml encoding="MathML-Content" id="A1.p10.1.m1.1b">
       <ci id="A1.p10.1.m1.1.1.cmml" xref="A1.p10.1.m1.1.1">
        ‚àô
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A1.p10.1.m1.1c">
       \bullet
      </annotation>
     </semantics>
    </math>
    Section
    <a class="ltx_ref" href="#A10" title="Appendix J Discussion on LARM Output ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      J
     </span>
    </a>
    : This section presents a discussion on whether low-level control signals or high-level skill predictions are more suitable to serve as the output of an embodied model.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Case Study
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    In Fig.
    <a class="ltx_ref" href="#A3.F10" title="Figure 10 ‚Ä£ Appendix C Study on training data volume ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    , we present three additional examples of LARM behavior. In the first case, the agent travels through various biomes to find a village, which demonstrates the strong exploration capability of LARM. For the second one, the agent builds a nether portal and enters the Nether through it. This case suggests the promising construction capability of LARM. In the last example, two agents collaborate with each other to combat a large group of zombies, confirming that multiple LARM models can efficiently cooperate together. These three cases reveal the promising performance of LARM from different perspectives and implies that it can achieve more complex targets with stronger skill library and more training data.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Study on training data volume
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    In this experiment, we delve into how different ARTNS training data volumes influence the obtained highest achievement. Specifically, we separately train LARM using 10%, 50%, and 100% of the total training data, and the results are presented in Table
    <a class="ltx_ref" href="#A3.T4" title="Table 4 ‚Ä£ Appendix C Study on training data volume ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    . It can be observed that having sufficient training data is critical for the performance. When only 10% of data is employed, the model can only craft stone tools with a low success rate. In contrast, when 100% of the data is utilized, the model successfully successfully harvests enchanted tools.
   </p>
  </div>
  <figure class="ltx_figure" id="A3.F10">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="A3.F10.g1" src="/html/2405.17424/assets/x10.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 10:
    </span>
    More behavior example illustrations of LARM, which include searching a village, building a nether portal and entering the nether, and multi-agent collaboration to combat zombies.
   </figcaption>
  </figure>
  <figure class="ltx_table" id="A3.T4">
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 4:
    </span>
    Ablation study on the influence of ARTNS training data volume. The setting of this experiment follows the one in Table
    <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
   </figcaption>
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T4.5" style="width:390.3pt;height:65.3pt;vertical-align:-0.0pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-34.3pt,5.7pt) scale(0.850428727800118,0.850428727800118) ;">
     <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T4.5.5">
      <thead class="ltx_thead">
       <tr class="ltx_tr" id="A3.T4.5.5.5">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T4.5.5.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">
         Data Ratio
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         Wooden Tool
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A3.T4.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/wooden_sword.png" width="15"/>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.2.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         Stone Tool
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A3.T4.2.2.2.2.g1" src="/html/2405.17424/assets/Figure/minecraft/stone_sword.png" width="15"/>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.3.3.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         Iron Tool
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A3.T4.3.3.3.3.g1" src="/html/2405.17424/assets/Figure/minecraft/iron_sword.png" width="15"/>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.4.4.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">
         Diamond Tool
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A3.T4.4.4.4.4.g1" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
        </th>
        <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T4.5.5.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">
         Enchanted Tool
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="24" id="A3.T4.5.5.5.5.g1" src="/html/2405.17424/assets/Figure/minecraft/enchanted_diamond_sword.png" width="24"/>
        </th>
       </tr>
      </thead>
      <tbody class="ltx_tbody">
       <tr class="ltx_tr" id="A3.T4.5.5.6.1">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T4.5.5.6.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         10%
        </th>
        <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         1/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">
         0/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">
         0/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.5.5.6.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">
         0/3
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T4.5.5.7.2">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.5.5.7.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         50%
        </th>
        <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">
         2/3
        </td>
        <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">
         1/3
        </td>
        <td class="ltx_td ltx_align_center" id="A3.T4.5.5.7.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">
         0/3
        </td>
       </tr>
       <tr class="ltx_tr" id="A3.T4.5.5.8.3">
        <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T4.5.5.8.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">
         100%
        </th>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.8.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.8.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.8.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.8.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">
         3/3
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T4.5.5.8.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">
         2/3
        </td>
       </tr>
      </tbody>
     </table>
    </span>
   </div>
  </figure>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Wiki Pre-training Details
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    To explain our method clearly, we further explain the training details of the Wiki Pre-training stage in this section. The procedures are visualized in Fig.
    <a class="ltx_ref" href="#A4.F11" title="Figure 11 ‚Ä£ Appendix D Wiki Pre-training Details ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      11
     </span>
    </a>
    for clarity. As shown, the data in Wiki webpages include both text and image information. The work Minddojo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    crawls 34G of Minecraft data from Internet and composes it as a multi-modal dataset. However, this dataset contains many short phrases that offer limited useful information. To resolve this problem, we exclude text with fewer than 30 characters. For the remaining data, we match every text with its closest image as a multi-modal data pair and uses it to train LARM. As depicted in Fig.
    <a class="ltx_ref" href="#A4.F11" title="Figure 11 ‚Ä£ Appendix D Wiki Pre-training Details ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      11
     </span>
    </a>
    , we mask a portion of the text information and construct a prompt from the remaining text and image. The model is trained to fill in the masked words. The model output format, training loss, and hyper-parameter details follow LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    . Each data pair is utilized twice in the Wiki pre-training stage, equating to two epochs of training.
   </p>
  </div>
  <figure class="ltx_figure" id="A4.F11">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="185" id="A4.F11.g1" src="/html/2405.17424/assets/x11.png" width="415"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 11:
    </span>
    LARM is pre-trained based on Wiki data to enrich the knowledge about Minecraft. In this stage, the model is asked to predict the remaining words in a sentence based on the given context.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Details of Compared Methods
  </h2>
  <div class="ltx_para" id="A5.p1">
   <p class="ltx_p" id="A5.p1.1">
    In Minecraft, as there is no widely accepted benchmark, different methods often employ varying training pipelines, testing protocals and training data. Therefore, the methods compared in Table
    <a class="ltx_ref" href="#S5.T1" title="Table 1 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    and Table
    <a class="ltx_ref" href="#S5.T2" title="Table 2 ‚Ä£ 5.2 Comparison with Previous Methods ‚Ä£ 5 Experiments ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    are not always trained in the same setting. However, this problem does not affect the significant contribution of LARM, because the main focus of embodied AI research is how intelligent the agent can achieve, and LARM obtains the achievement significantly better than all previous counterparts. Besides, we described below, the testing protocal of LARM is more challenging than all compared methods. To be more transparent about the difference among various methods, we elaborate on the details of all compared methods in this section one by one.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p2">
   <p class="ltx_p" id="A5.p2.1">
    <span class="ltx_text ltx_font_bold" id="A5.p2.1.1">
     MineAgent
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    . MineAgent is the baseline method provided by Minddojo. This work first fine-tunes the CLIP model
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      28
     </a>
     ]
    </cite>
    based on numerous Internet data and uses the data to guide the training of reinforcement learning based algorithms. The method is test with Minddojo. In each testing iteration, the agent is initialized with some tools and asked to complete a task described by given text.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p3">
   <p class="ltx_p" id="A5.p3.1">
    <span class="ltx_text ltx_font_bold" id="A5.p3.1.1">
     Plan4MC
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ]
    </cite>
    . Plan4MC is a reinformcement learning based method. It splits the tasks of an agent into basic skills and trains an agent to learn them one by one in a hierarchical way. The model is test based on Minddojo. In each testing iteration, the agent is initialized with some tools and asked to complete a task described by given text.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p4">
   <p class="ltx_p" id="A5.p4.1">
    <span class="ltx_text ltx_font_bold" id="A5.p4.1.1">
     LLaMA-Rider
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    . LLaMA-Rider is a LLM obtained by fine-tuning LLaMA. For its developing model process, this work first makes the agent explore the environment by itself and collect some data during this process. Then, they complile the collected data into a dataset to fine-tune the LARM model in a supervised manner. The output of LLaMA-Rider is text and the text is used to call the skills generated by Plan4MC. The model is test using Minddojo. In each testing iteration, the agent is initialized with some tools and asked to complete a task described by given text.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p5">
   <p class="ltx_p" id="A5.p5.1">
    <span class="ltx_text ltx_font_bold" id="A5.p5.1.1">
     Voyager
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ]
    </cite>
    . Voyager is a method implemented based on GPT4 and has no training process. Its main contribution is designing a multi-step prompt generation pipeline. The skills required by it are implemented based on Mineflayer. When a target task is given, the method prompts GPT4 to know which skill should be executed and gradually realizes the target. This method is test in the standard Minecraft game environment. In the begining of testing, the agent is initialized with no any tool.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p6">
   <p class="ltx_p" id="A5.p6.1">
    <span class="ltx_text ltx_font_bold" id="A5.p6.1.1">
     ReAct
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib36" title="">
      36
     </a>
     ]
    </cite>
    ,
    <span class="ltx_text ltx_font_bold" id="A5.p6.1.2">
     Reflexion
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      32
     </a>
     ]
    </cite>
    , and
    <span class="ltx_text ltx_font_bold" id="A5.p6.1.3">
     AutoGPT
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    . All the three methods are LLMs and can reason about what skill should be performed through multi-step question answering. Their performances in Minecraft are test in the work Voyager. Their called skills are also implemented based on Mineflayer and the testing protocols are the same as Voyager.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A5.p7">
   <p class="ltx_p" id="A5.p7.1">
    <span class="ltx_text ltx_font_bold" id="A5.p7.1.1">
     STEVE
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ]
    </cite>
    . STEVE is large vision-language model. The authors of this work collects a training dataset including both viedos and test-image pairs and uses the data to fine-tune a LLaMA model. The obtained model is used to conduct multi-step reasoning and call skills implemented based on Mineflayer. Its testing protocol follows Voyager.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A6">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix F
   </span>
   Data Collection Procedures
  </h2>
  <div class="ltx_para" id="A6.p1">
   <p class="ltx_p" id="A6.p1.1">
    As described in the main paper, the data is organized in the ARTNS format and we collect the data manually. Specifically, for each given target task, we first decompose the task into multiple steps and each step corresponds to a basic skill. We operate the agent to execute these skills one by one. After each skill is performed, the moment of which corresponds to a node in Fig.
    <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‚Ä£ 3.3 Skill Library Preparation ‚Ä£ 3 Algorithm ‚Ä£ LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    , we record all the concerned information, such as multi-view images, agent position information, etc. After collecting the data, we label the dependence relations among all nodes.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A7">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix G
   </span>
   Analysis on LLM Output
  </h2>
  <div class="ltx_para" id="A7.p1">
   <p class="ltx_p" id="A7.p1.1">
    As discussed in the main paper, the output of LLMs is often ambiguous for agents to selecting specific actions. In addition, we find that the generated response of existing LLMs is often incorrect due to their limited knowledge about Minecraft. In the following, we compare three various popular LLMs, i.e., LLaVA-1.6-7B, ChatGPT, and GPT-4 in Minecraft related question-answer test.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A7.p2">
   <svg class="ltx_picture" height="407.44" id="A7.p2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,407.44) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 0 L 0 407.44 L 600 407.44 L 600 0 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 0.69 0.69 L 0.69 335.49 L 599.31 335.49 L 599.31 0.69 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 340.12)">
      <foreignobject color="#FFFFFF" height="62.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:404.2pt;">
        <span class="ltx_p" id="A7.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
         Dialogue 1: Combat one horse
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="60" id="A7.p2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/horse.png" width="54"/>
         with a diamond sword
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A7.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g2" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
         .
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)">
      <foreignobject color="#000000" height="311.18" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.p2.pic1.3.3.3.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A7.p2.pic1.3.3.3.1.1.1.1">
          Prompt:
         </span>
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.2">
         You are an expert of playing Minecraft. I will tell you a target task, some information about your Minecraft agent and environment. You need to tell me the next action the agent should perform to complete the target task. Your answer should be concise and describe the next action clearly. For crafting items, you should specify the expected item number.
The target task is to kill one horse. The time is night and the weather is clear. The agent position in the world coordinate system is (-584, 67, 384). The inventory information includes: ‚Äúchicken:1. mutton:2. sand:1. cobblestone:44. birch_planks:4. dark_oak_planks:1. cobblestone_slab:1. apple:1. stick:2. diamond_pickaxe:1. diamond_shovel:1. beef:7. white_wool:1. porkchop:2. diamond_sword:1. leather:4. feather:2. dirt:13.‚Äù.
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.3">
         <span class="ltx_text ltx_font_bold" id="A7.p2.pic1.3.3.3.1.1.3.1">
          LLaVA-1.6-7B:
         </span>
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.4">
         The next action the agent should perform is to gather materials for crafting a saddle, which is required to ride the horse. The agent should first gather 2 birch planks, 1 cobblestone, and 1 stick from the surrounding area. Once the materials are gathered, the agent should then craft a saddle using the gathered materials. The saddle can be crafted by placing the birch planks on the cobblestone, followed by the stick on top of the birch planks. After crafting the saddle, the agent can then mount the horse and ride it.
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.5">
         <span class="ltx_text ltx_font_bold" id="A7.p2.pic1.3.3.3.1.1.5.1">
          ChatGPT:
         </span>
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.6">
         The next action the agent should perform is to locate a horse and attack it to kill it.
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.7">
         <span class="ltx_text ltx_font_bold" id="A7.p2.pic1.3.3.3.1.1.7.1">
          GPT-4:
         </span>
        </span>
        <span class="ltx_p" id="A7.p2.pic1.3.3.3.1.1.8">
         Given the current circumstances and the target task, the next action the agent should perform is to equip the diamond sword. After equipping the sword, the agent should then proceed to locate and approach the horse. Once the horse is within range, the agent should use the diamond sword to attack and kill the horse.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para ltx_noindent" id="A7.p3">
   <svg class="ltx_picture" height="307.81" id="A7.p3.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,307.81) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 0 L 0 307.81 L 600 307.81 L 600 0 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 0.69 0.69 L 0.69 235.87 L 599.31 235.87 L 599.31 0.69 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 240.5)">
      <foreignobject color="#FFFFFF" height="62.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:404.2pt;">
        <span class="ltx_p" id="A7.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
         Dialogue 2: Combat one horse
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="60" id="A7.p3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="/html/2405.17424/assets/Figure/minecraft/horse.png" width="54"/>
         without a sword
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="A7.p3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g2" src="/html/2405.17424/assets/Figure/minecraft/diamond_sword.png" width="15"/>
         .
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)">
      <foreignobject color="#000000" height="211.55" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.p3.pic1.3.3.3.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A7.p3.pic1.3.3.3.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A7.p3.pic1.3.3.3.1.1.1.1">
          Prompt:
         </span>
        </span>
        <span class="ltx_p" id="A7.p3.pic1.3.3.3.1.1.2">
         You are an expert of playing Minecraft. I will tell you a target task, some information about your Minecraft agent and environment. You need to tell me the next action the agent should perform to complete the target task. Your answer should be concise and describe the next action clearly. For crafting items, you should specify the expected item number.
The target task is to kill one horse. The time is night and the weather is clear. The agent position in the world coordinate system is (-584, 67, 384). The inventory information includes: ‚Äúchicken:1. mutton:2. sand:1. cobblestone:44. birch_planks:4. dark_oak_planks:1. cobblestone_slab:1. apple:1. stick:2. diamond_pickaxe:1. diamond_shovel:1. beef:7. white_wool:1. porkchop:2. leather:4. feather:2. dirt:13.‚Äù.
        </span>
        <span class="ltx_p" id="A7.p3.pic1.3.3.3.1.1.3">
         <span class="ltx_text ltx_font_bold" id="A7.p3.pic1.3.3.3.1.1.3.1">
          GPT-4:
         </span>
        </span>
        <span class="ltx_p" id="A7.p3.pic1.3.3.3.1.1.4">
         Given the task and the current inventory, the agent does not have the appropriate equipment to kill a horse efficiently. Therefore, the next action should be to craft a stone sword. To do this, the agent needs to use 2 cobblestones from the inventory and create a crafting table if one doesn‚Äôt exist. Once the sword is crafted, the agent can then proceed to locate and kill the horse.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
  <div class="ltx_para" id="A7.p4">
   <p class="ltx_p" id="A7.p4.1">
    According the above two dialogues, the output of LLaVA is significantly incorrect. First of all, the prompt is about combating a horse, while the response of LLaVA is about how to ride a horse. In addition, saddle cannot be crafted by the agent itself in Minecraft, the given recipe is also false. By contrast, ChatGPT gives a relatively better answer, locate a horse first and then combat it. Nevertheless, ChatGPT overlooks the fact that there is a diamond sword in the inventory list and combating a horse without any weapon is quite inefficient. In contrast to LLaVA and ChatGPT, the knowledge of GPT-4 about Minecraft is much more richer. When a diamond sword is in the inventory list, GPT-4 knows the agent should first equip the sword, and then find a horse and combat it. Conversely, if there is no weapon in the inventory list, GPT-4 understands a stone sword should be crafted first based on the available resources in the inventory list. GPT-4 does not selects to craft a iron sword because there is iron ingot. Additionally, crafting a wooden sword is not chosen because its damage is weaker than the stone sword.
   </p>
  </div>
  <div class="ltx_para" id="A7.p5">
   <p class="ltx_p" id="A7.p5.1">
    Based on the aforementioned result and analysis, we can observe that GPT-4 behaves the best among these three LLMs. This observation suggests that the model size and training data volumn are also crucial for the LLMs applied to Minecraft. However, although GPT-4 presents promising performance, it shows slow inference speed and the demands numerous GPUs for inference. These characteristics make it unsuitable for embodied applications, which require real-time operations and have limited local computing resource. Compared with these LLMs, LARM is much more fast, computing economical, and presents superior performance. Therefore, LARM is more suitable for developing embodied intelligence than the many previous LLMs.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A8">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix H
   </span>
   LARM Text Input
  </h2>
  <div class="ltx_para" id="A8.p1">
   <p class="ltx_p" id="A8.p1.1">
    As mentioned in the main paper, the text input is important for LARM to perceive information. In the following, we introduce how different text information is organized as an input sentence, which is shown below.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="A8.p2">
   <svg class="ltx_picture" height="192.54" id="A8.p2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,192.54) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 0 L 0 192.54 L 600 192.54 L 600 0 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 0.69 0.69 L 0.69 170.99 L 599.31 170.99 L 599.31 0.69 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 175.61)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A8.p2.pic1.1.1.1.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A8.p2.pic1.1.1.1.1.1.1">
         An example of how different text information is organized as an sentence input to LARM.
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)">
      <foreignobject color="#000000" height="146.67" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A8.p2.pic1.2.2.2.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.1.1">
          Original Information:
         </span>
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.2">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.2.1">
          target
         </span>
         : Create 1 iron shovel.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.3">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.3.1">
          time
         </span>
         : day.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.4">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.4.1">
          weather
         </span>
         : clear.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.5">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.5.1">
          last action
         </span>
         : craft_16_oak_planks.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.6">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.6.1">
          position
         </span>
         : (-54, 64, 56).
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.7">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.7.1">
          last action execution result
         </span>
         : success.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.8">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.8.1">
          inventory list
         </span>
         : oak_log:12. oak_planks:16.
        </span>
        <span class="ltx_p" id="A8.p2.pic1.2.2.2.1.1.9">
         <span class="ltx_text ltx_font_bold" id="A8.p2.pic1.2.2.2.1.1.9.1">
          The Sentence Input to LARM
         </span>
         : You are an expert of playing Minecraft. I will tell you a target task, some information about your Minecraft agent and environment. You need to tell me the next action the agent should perform to complete the target task. Your answer should be concise and describe the next action clearly. For crafting items, you should specify the expected item number.
The target task is to create 1 iron shovel. The time is day and the weather is clear. The agent position in the world coordinate system is (-54, 64, 56). The inventory information includes: ‚Äúoak_log:12. oak_planks:16.‚Äù.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
 </section>
 <section class="ltx_appendix" id="A9">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix I
   </span>
   Skill Library Description
  </h2>
  <div class="ltx_para" id="A9.p1">
   <p class="ltx_p" id="A9.p1.1">
    As mentioned before, the skill library is important for LARM, as the skills included in this library serve as the basic actions for LARM. To clearly explain the details of these skills, the code of one skill is given as an example. In addition, we present the titles and description of 10 skills in this part.
   </p>
  </div>
  <figure class="ltx_float ltx_lstlisting" id="LST1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_float">
     Skill-Code example¬†1:
    </span>
    "The code of combating one mob".
   </figcaption>
   <div class="ltx_listing ltx_lst_language_JavaScript ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="LST1.1" style="background-color:#F5F5F5;">
    <div class="ltx_listing_data">
     <a download="skill_demo.js" href="data:text/plain;base64,Ly8gRGVzY3JpcHRpb246IFRoZSBmdW5jdGlvbiB0byBjb21iYXQgb25lIG1vYi4KYXN5bmMgZnVuY3Rpb24gY29tYmF0X29uZV9tb2IoYm90LCBhbmltYWxfbmFtZSkKewogICAgdHJ5CiAgICB7CiAgICAgICAgLy8gRmluZCB0aGUgZW50aXR5IHRoYXQgbWF0Y2hlcyB0aGUgbmFtZSBvZiB0aGUgYW5pbWFsCiAgICAgICAgbGV0IHRndF9hbmltYWwgPSBib3QubmVhcmVzdEVudGl0eShlbnRpdHkgPT4gZW50aXR5Lm5hbWUudG9Mb3dlckNhc2UoKSA9PT0gYW5pbWFsX25hbWUpCiAgICAgICAgCiAgICAgICAgLy8gSWYgdGhlIGVudGl0eSBpcyBub3QgZm91bmQsIGRpc3BsYXkgYW4gZXJyb3IgbWVzc2FnZQogICAgICAgIGlmKHRndF9hbmltYWwgPT0gbnVsbCkKICAgICAgICB7CiAgICAgICAgICAgIGJvdC5jaGF0KCdEbyBub3QgZmluZCB0aGUgZW50aXR5ICR7YW5pbWFsX25hbWV9LicpCiAgICAgICAgICAgIHJldHVybiBmYWxzZQogICAgICAgIH0KICAgICAgICAvLyBJZiB0aGUgZW50aXR5IGlzIGZvdW5kLCBhdHRhY2sgaXQKICAgICAgICBlbHNlCiAgICAgICAgewogICAgICAgICAgICBhd2FpdCBib3QucHZwLmF0dGFjayh0Z3RfYW5pbWFsKQogICAgICAgIH0KICAgIH0KICAgIC8vIElmIGFuIGVycm9yIG9jY3VycywgbG9nIHRoZSBlcnJvciBhbmQgcmV0dXJuIGZhbHNlCiAgICBjYXRjaChlcnIpCiAgICB7CiAgICAgICAgY29uc29sZS5sb2coZXJyKQogICAgICAgIHJldHVybiBmYWxzZQogICAgfQp9">
      ‚¨á
     </a>
    </div>
    <div class="ltx_listingline" id="lstnumberx1">
     <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx1.1" style="font-size:80%;color:#922192;">
      //
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.1">
      </span>
      Description:
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.2">
      </span>
      The
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.3">
      </span>
      function
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.4">
      </span>
      to
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.5">
      </span>
      combat
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.6">
      </span>
      one
      <span class="ltx_text ltx_lst_space" id="lstnumberx1.1.7">
      </span>
      mob.
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx2">
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx2.1" style="font-size:80%;color:#000000;">
      async
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx2.2" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx2.3" style="font-size:80%;color:#0000FF;">
      function
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx2.4" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx2.5" style="font-size:80%;color:#000000;">
      combat_one_mob
     </span>
     <span class="ltx_text" id="lstnumberx2.6" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx2.7" style="font-size:80%;color:#000000;">
      bot
     </span>
     <span class="ltx_text" id="lstnumberx2.8" style="font-size:80%;">
      ,
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx2.9" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx2.10" style="font-size:80%;color:#000000;">
      animal_name
     </span>
     <span class="ltx_text" id="lstnumberx2.11" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx3">
     <span class="ltx_text" id="lstnumberx3.1" style="font-size:80%;">
      {
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx4">
     <span class="ltx_text ltx_lst_space" id="lstnumberx4.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx4.2" style="font-size:80%;color:#000000;">
      try
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx5">
     <span class="ltx_text ltx_lst_space" id="lstnumberx5.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx5.2" style="font-size:80%;">
      {
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx6">
     <span class="ltx_text ltx_lst_space" id="lstnumberx6.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx6.2" style="font-size:80%;color:#922192;">
      //
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.1">
      </span>
      Find
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.2">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.3">
      </span>
      entity
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.4">
      </span>
      that
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.5">
      </span>
      matches
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.6">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.7">
      </span>
      name
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.8">
      </span>
      of
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.9">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx6.2.10">
      </span>
      animal
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx7">
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.2" style="font-size:80%;color:#000000;">
      let
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.3" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.4" style="font-size:80%;color:#000000;">
      tgt_animal
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.5" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx7.6" style="font-size:80%;">
      =
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.7" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.8" style="font-size:80%;color:#000000;">
      bot
     </span>
     <span class="ltx_text" id="lstnumberx7.9" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.10" style="font-size:80%;color:#000000;">
      nearestEntity
     </span>
     <span class="ltx_text" id="lstnumberx7.11" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.12" style="font-size:80%;color:#000000;">
      entity
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.13" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx7.14" style="font-size:80%;">
      =&gt;
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.15" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.16" style="font-size:80%;color:#000000;">
      entity
     </span>
     <span class="ltx_text" id="lstnumberx7.17" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.18" style="font-size:80%;color:#000000;">
      name
     </span>
     <span class="ltx_text" id="lstnumberx7.19" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.20" style="font-size:80%;color:#000000;">
      toLowerCase
     </span>
     <span class="ltx_text" id="lstnumberx7.21" style="font-size:80%;">
      ()
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.22" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx7.23" style="font-size:80%;">
      ===
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx7.24" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx7.25" style="font-size:80%;color:#000000;">
      animal_name
     </span>
     <span class="ltx_text" id="lstnumberx7.26" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx8">
    </div>
    <div class="ltx_listingline" id="lstnumberx9">
     <span class="ltx_text ltx_lst_space" id="lstnumberx9.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx9.2" style="font-size:80%;color:#922192;">
      //
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.1">
      </span>
      If
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.2">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.3">
      </span>
      entity
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.4">
      </span>
      is
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.5">
      </span>
      not
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.6">
      </span>
      found,
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.7">
      </span>
      display
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.8">
      </span>
      an
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.9">
      </span>
      error
      <span class="ltx_text ltx_lst_space" id="lstnumberx9.2.10">
      </span>
      message
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx10">
     <span class="ltx_text ltx_lst_space" id="lstnumberx10.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx10.2" style="font-size:80%;color:#0000FF;">
      if
     </span>
     <span class="ltx_text" id="lstnumberx10.3" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx10.4" style="font-size:80%;color:#000000;">
      tgt_animal
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx10.5" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx10.6" style="font-size:80%;">
      ==
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx10.7" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx10.8" style="font-size:80%;color:#0000FF;">
      null
     </span>
     <span class="ltx_text" id="lstnumberx10.9" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx11">
     <span class="ltx_text ltx_lst_space" id="lstnumberx11.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx11.2" style="font-size:80%;">
      {
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx12">
     <span class="ltx_text ltx_lst_space" id="lstnumberx12.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx12.2" style="font-size:80%;color:#000000;">
      bot
     </span>
     <span class="ltx_text" id="lstnumberx12.3" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx12.4" style="font-size:80%;color:#000000;">
      chat
     </span>
     <span class="ltx_text" id="lstnumberx12.5" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx12.6" style="font-size:80%;color:#FF0000;">
      ‚ÄôDo
      <span class="ltx_text ltx_lst_space" id="lstnumberx12.6.1">
      </span>
      not
      <span class="ltx_text ltx_lst_space" id="lstnumberx12.6.2">
      </span>
      find
      <span class="ltx_text ltx_lst_space" id="lstnumberx12.6.3">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx12.6.4">
      </span>
      entity
      <span class="ltx_text ltx_lst_space" id="lstnumberx12.6.5">
      </span>
      ${animal_name}.‚Äô
     </span>
     <span class="ltx_text" id="lstnumberx12.7" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx13">
     <span class="ltx_text ltx_lst_space" id="lstnumberx13.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx13.2" style="font-size:80%;color:#0000FF;">
      return
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx13.3" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx13.4" style="font-size:80%;color:#0000FF;">
      false
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx14">
     <span class="ltx_text ltx_lst_space" id="lstnumberx14.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx14.2" style="font-size:80%;">
      }
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx15">
     <span class="ltx_text ltx_lst_space" id="lstnumberx15.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx15.2" style="font-size:80%;color:#922192;">
      //
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.1">
      </span>
      If
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.2">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.3">
      </span>
      entity
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.4">
      </span>
      is
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.5">
      </span>
      found,
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.6">
      </span>
      attack
      <span class="ltx_text ltx_lst_space" id="lstnumberx15.2.7">
      </span>
      it
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx16">
     <span class="ltx_text ltx_lst_space" id="lstnumberx16.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx16.2" style="font-size:80%;color:#0000FF;">
      else
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx17">
     <span class="ltx_text ltx_lst_space" id="lstnumberx17.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx17.2" style="font-size:80%;">
      {
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx18">
     <span class="ltx_text ltx_lst_space" id="lstnumberx18.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx18.2" style="font-size:80%;color:#000000;">
      await
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx18.3" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx18.4" style="font-size:80%;color:#000000;">
      bot
     </span>
     <span class="ltx_text" id="lstnumberx18.5" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx18.6" style="font-size:80%;color:#000000;">
      pvp
     </span>
     <span class="ltx_text" id="lstnumberx18.7" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx18.8" style="font-size:80%;color:#000000;">
      attack
     </span>
     <span class="ltx_text" id="lstnumberx18.9" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx18.10" style="font-size:80%;color:#000000;">
      tgt_animal
     </span>
     <span class="ltx_text" id="lstnumberx18.11" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx19">
     <span class="ltx_text ltx_lst_space" id="lstnumberx19.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx19.2" style="font-size:80%;">
      }
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx20">
     <span class="ltx_text ltx_lst_space" id="lstnumberx20.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx20.2" style="font-size:80%;">
      }
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx21">
     <span class="ltx_text ltx_lst_space" id="lstnumberx21.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx21.2" style="font-size:80%;color:#922192;">
      //
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.1">
      </span>
      If
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.2">
      </span>
      an
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.3">
      </span>
      error
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.4">
      </span>
      occurs,
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.5">
      </span>
      log
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.6">
      </span>
      the
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.7">
      </span>
      error
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.8">
      </span>
      and
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.9">
      </span>
      return
      <span class="ltx_text ltx_lst_space" id="lstnumberx21.2.10">
      </span>
      false
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx22">
     <span class="ltx_text ltx_lst_space" id="lstnumberx22.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx22.2" style="font-size:80%;color:#0000FF;">
      catch
     </span>
     <span class="ltx_text" id="lstnumberx22.3" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx22.4" style="font-size:80%;color:#000000;">
      err
     </span>
     <span class="ltx_text" id="lstnumberx22.5" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx23">
     <span class="ltx_text ltx_lst_space" id="lstnumberx23.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx23.2" style="font-size:80%;">
      {
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx24">
     <span class="ltx_text ltx_lst_space" id="lstnumberx24.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx24.2" style="font-size:80%;color:#000000;">
      console
     </span>
     <span class="ltx_text" id="lstnumberx24.3" style="font-size:80%;">
      .
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx24.4" style="font-size:80%;color:#000000;">
      log
     </span>
     <span class="ltx_text" id="lstnumberx24.5" style="font-size:80%;">
      (
     </span>
     <span class="ltx_text ltx_lst_identifier" id="lstnumberx24.6" style="font-size:80%;color:#000000;">
      err
     </span>
     <span class="ltx_text" id="lstnumberx24.7" style="font-size:80%;">
      )
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx25">
     <span class="ltx_text ltx_lst_space" id="lstnumberx25.1" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx25.2" style="font-size:80%;color:#0000FF;">
      return
     </span>
     <span class="ltx_text ltx_lst_space" id="lstnumberx25.3" style="font-size:80%;">
     </span>
     <span class="ltx_text ltx_lst_keyword ltx_font_typewriter ltx_font_bold" id="lstnumberx25.4" style="font-size:80%;color:#0000FF;">
      false
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx26">
     <span class="ltx_text ltx_lst_space" id="lstnumberx26.1" style="font-size:80%;">
     </span>
     <span class="ltx_text" id="lstnumberx26.2" style="font-size:80%;">
      }
     </span>
    </div>
    <div class="ltx_listingline" id="lstnumberx27">
     <span class="ltx_text" id="lstnumberx27.1" style="font-size:80%;">
      }
     </span>
    </div>
   </div>
  </figure>
  <div class="ltx_para ltx_noindent" id="A9.p2">
   <svg class="ltx_picture" height="124.74" id="A9.p2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,124.74) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 0 L 0 124.74 L 600 124.74 L 600 0 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 0.69 0.69 L 0.69 103.18 L 599.31 103.18 L 599.31 0.69 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 107.81)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A9.p2.pic1.1.1.1.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A9.p2.pic1.1.1.1.1.1.1">
         The function names and descriptions of 10 representative skills.
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)">
      <foreignobject color="#000000" height="78.87" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A9.p2.pic1.2.2.2.1.1" style="width:404.2pt;">
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.1.1">
          collect_1_oak_log
         </span>
         : Collect 1 oak log.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.2">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.2.1">
          craft_16_spruce_planks
         </span>
         : Craft 16 spruce planks.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.3">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.3.1">
          craft_1_wooden_shovel
         </span>
         : Craft 1 wooden shovel.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.4">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.4.1">
          collect_32_stone
         </span>
         : Collect 32 stone blocks.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.5">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.5.1">
          collect_1_lava_bucket
         </span>
         : Collect 1 bucket of lava.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.6">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.6.1">
          craft_1_diamond_sword
         </span>
         : Collect 1 diamond sword.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.7">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.7.1">
          equip_diamond_sword
         </span>
         : Equip the owned diamond sword.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.8">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.8.1">
          combat_one_zombie
         </span>
         : Combat 1 zombie.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.9">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.9.1">
          go_ground
         </span>
         : Go to the ground.
        </span>
        <span class="ltx_p" id="A9.p2.pic1.2.2.2.1.1.10">
         <span class="ltx_text ltx_font_bold" id="A9.p2.pic1.2.2.2.1.1.10.1">
          eat_1_steak
         </span>
         : Eat 1 steak.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </div>
 </section>
 <section class="ltx_appendix" id="A10">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix J
   </span>
   Discussion on LARM Output
  </h2>
  <div class="ltx_para" id="A10.p1">
   <p class="ltx_p" id="A10.p1.1">
    As described in the main paper, LARM takes a skill (high-level action) as one atomic operation. Simultaneously, there is also some methods that directly output a low-level action, like the mouse click or keyboard press
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    . In this section, we present a discussion on whether we should employ high-level or low-level actions in embodied artificial intelligence (AI).
   </p>
  </div>
  <div class="ltx_para" id="A10.p2">
   <p class="ltx_p" id="A10.p2.1">
    In practical embodied applications, differing agents usually have significantly different mechanical structures. Although there are works that try unifying them by training end-to-end models with a large amount of parameters and collecting numerous data, the obtained performance is still limited and cannot be deployed in real industrial environments
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib29" title="">
      29
     </a>
     ;
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    . Conversely, existing deployed robots are mainly implemented based on many traditional control modules
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    , which are similar to the pre-generated skills in Minecraft. Therefore, making models output high-level actions is beneficial to alleviate the deployment difficulty.
   </p>
  </div>
  <div class="ltx_para" id="A10.p3">
   <p class="ltx_p" id="A10.p3.1">
    Similarly, in Minecraft, although building VPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    consumes numerous resource, its obtained highest achievement is much weaker than LARM, while LARM is trained using only 4 RTX3090 GPUs. Therefore, when the data and computing resource is not very abundant, exploring models with high-level output is a promising choice.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
