<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Recognizing Lawyers as AI Creators and Intermediaries in Contestability</title>
<!--Generated on Thu Sep 26 08:17:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="algorithmic decision-making,  lawyers,  cross-disciplinary design,  contestability " lang="en" name="keywords"/>
<base href="/html/2409.17626v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S1" title="In Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S2" title="In Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Lawyers as AI Creators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S3" title="In Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Lawyers as Intermediaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S4" title="In Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Designing with Lawyers</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S4.SS1" title="In 4. Designing with Lawyers ‣ Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Opportunity to Create Co-Design Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S4.SS2" title="In 4. Designing with Lawyers ‣ Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Creating Boundary Objects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S4.SS3" title="In 4. Designing with Lawyers ‣ Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Differences that Co-Design Methods and Boundary Objects Must Navigate</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#S5" title="In Recognizing Lawyers as AI Creators and Intermediaries in Contestability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Recognizing Lawyers as AI Creators and Intermediaries in Contestability</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gennie Mansi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gmansi3@gatech.edu">gmansi3@gatech.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6186-5102" title="ORCID identifier">0000-0001-6186-5102</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mark Riedl
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:riedl@cc.gatech.edu">riedl@cc.gatech.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Georgia Institute of Technology</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">GA</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024; 15 September 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id5.id1">Laws play a key role in the complex socio-technical system impacting contestability: they create the regulations shaping the way AI systems are designed, evaluated, and used. Despite their role in the AI value chain, lawyers’ impact on contestability has gone largely unrecognized in the design of AI systems. In this paper, we highlight two main roles lawyers play that impact contestability: (1) as AI Creators because the regulations they create shape the design and evaluation of AI systems before they are deployed; and (2) as Intermediaries because they interpret regulations when harm occurs, navigating the gap between stakeholders, instutions, and harmful outcomes. We use these two roles to illuminate new opportunities and challenges for including lawyers in the design of AI systems, contributing a significant first step in practical recommendations to amplify the power to contest systems through cross-disciplinary design.</p>
</div>
<div class="ltx_keywords">algorithmic decision-making, lawyers, cross-disciplinary design, contestability 
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>workshop “From Stem to Stern: Contestability Along AI Value Chains” at the Conference for Computer Supported Collaborative Work; 09 November
2024; San José, Costa Rica</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing HCI theory, concepts and models</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">From design to deployment, the impacts of laws and regulations on the AI life cycle dramatically shape <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">contestability</span>—how an AI system is open and responsive to human dispute and intervention <cite class="ltx_cite ltx_citemacro_citep">(Karusala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib19" title="">2024</a>)</cite>. Prior to deployment, laws and regulations impact how AI systems are created, what algorithms can be selected, and how safety is measured and determined. For example, the Federal Drug Administration (FDA) requires AI-based devices remain “on leash” such that developers can always provide a detailed description of how the system will evolve over time, significantly impacting choices around how the kinds of algorithms used in AI systems <cite class="ltx_cite ltx_citemacro_citep">(FDA Center for Devices and Radiological Health, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib12" title="">2023</a>)</cite>. Regulations also govern required procedures to prevent risks <cite class="ltx_cite ltx_citemacro_citep">(Chan, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib8" title="">2021</a>)</cite>, train AI systems to meet localized needs <cite class="ltx_cite ltx_citemacro_citep">(Maliha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib23" title="">2021</a>)</cite>, and how their application must be justified and presented to consumers <cite class="ltx_cite ltx_citemacro_citep">(Mulligan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib24" title="">2019</a>)</cite>. After deployment, regulations impact
how different actors contribute to their deployment and use, further shaping AI systems’ sociotechnical environment. Regulations determine procedures for how to complete audits <cite class="ltx_cite ltx_citemacro_citep">(Mulligan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib24" title="">2019</a>; FDA Center for Devices and Radiological Health, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib12" title="">2023</a>)</cite> and for litigating harms once they have been caused <cite class="ltx_cite ltx_citemacro_citep">(Sundholm, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib32" title="">2024</a>; Price II and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib30" title="">2023</a>; Gerke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib13" title="">2020</a>)</cite>, which can cause users’ to change or adopt new behaviors as they try to mitigate perceived legal risks from the system <cite class="ltx_cite ltx_citemacro_citep">(Nash et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib25" title="">2004</a>)</cite>. Consequently, laws and regulations influence how contestability is enacted across the sociotechnical environment, directly shaping AI systems in ways already recognized as critical to contestability <cite class="ltx_cite ltx_citemacro_citep">(Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib6" title="">[n. d.]</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although laws and regulations play a significant role in how contestability is enacted throughout the AI life cycle, there is limited recognition for lawyers as the actors who create these legal structures.
Lawyers, then, are people who, through the regulations they create, directly shape how stakeholders are empowered to prevent and recover from AI system harms. Therefore, they are significant actors who should be included when designing for contestability. <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">There is a need to investigate how to include lawyers as a part of design efforts for contestable AI systems.</span>
However, there is limited understanding around the roles lawyers play in creating the legal context for AI systems. More recent work in the Contestability Community has begun to interview lawyers, bringing attention to the need to design with them in order to improve AI systems’ contestability <cite class="ltx_cite ltx_citemacro_citep">(Jin and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib18" title="">2024</a>; Warren and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib37" title="">2022</a>)</cite>. Consequently, there is a need to understand the capacities through which we can understand, communicate with, and leverage lawyers’ perspectives and skills when creating AI systems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Towards this end, our work contributes a first step in elucidating specific ways lawyers impact the AI value chain and identifying opportunities for cross-disciplinary design.</span>
In this paper, we argue that recognizing how lawyers shape the contestability of AI systems through two key roles—(1) as <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">AI Creators</span> and as (2) <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">Intermediaries</span>—allows us to understand how to direct efforts to design AI systems <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">with</span> them. We discuss how designing for lawyers as AI Creators and lawyers as AI Intermediaries can better enable the development of contestable systems. We also present anticipated challenges and opportunities when designing AI systems with lawyers in these two roles. We will predominantly use AI-based medical technology for examples of the legal terms. However, these principles are present in other domains as well, including finance, criminal justice, and privacy among others <cite class="ltx_cite ltx_citemacro_citep">(Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib6" title="">[n. d.]</a>; Karusala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib19" title="">2024</a>)</cite>. Through our work we aim to include and provide practical recommendations to leverage the experiences and knowledge of lawyers as actors who powerfully influence the implementation of contestability.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Lawyers as AI Creators</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Lawyers act as <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">AI Creators</span> because the laws and regulations impact AI systems’ designs, including mechanisms for contestability. Lawyers create laws to directly shape the design and deployment of AI systems to protect consumers. AI is what lawyers call a “credence good”—a consumer can’t know the quality of the good until after the purchase. It requires blind faith in the quality or some rigorous, systemic evaluation in order to ensure it works properly <cite class="ltx_cite ltx_citemacro_citep">(Price, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib29" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Due to the risk consumers assume with AI as a credence good, lawyers regulate its development at multiple points through <span class="ltx_text ltx_font_italic" id="S2.p2.1.1">ex ante</span> and <span class="ltx_text ltx_font_italic" id="S2.p2.1.2">ex post</span> laws.
<span class="ltx_text ltx_font_italic" id="S2.p2.1.3">Ex ante</span> laws impact technology design before it enters stakeholders’ workflows. For example, current FDA regulations ensure and validate the safety of technology before physicians’ use.
<span class="ltx_text ltx_font_italic" id="S2.p2.1.4">Ex post</span> laws define a procedures for determining and mitigating harm after a technology is distributed to consumers. Medical malpractice laws that determine fault when patients are harmed are examples of <span class="ltx_text ltx_font_italic" id="S2.p2.1.5">ex post</span> regulations. Further, <span class="ltx_text ltx_font_italic" id="S2.p2.1.6">ex ante</span> and <span class="ltx_text ltx_font_italic" id="S2.p2.1.7">ex post</span> regulations can interact with each other. Under previous FDA regulations, achieving Class III FDA approval of a medical technology could shield a company from certain degrees of liability claims if a consumer was harmed <cite class="ltx_cite ltx_citemacro_citep">(FDA, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib2" title="">2008</a>)</cite>. In this way, <span class="ltx_text ltx_font_italic" id="S2.p2.1.8">ex ante</span> and <span class="ltx_text ltx_font_italic" id="S2.p2.1.9">ex post</span> laws change the AI value chain from design and development to deployment. When creating a new AI-based technology, developers and companies must consider both <span class="ltx_text ltx_font_italic" id="S2.p2.1.10">ex ante</span> and <span class="ltx_text ltx_font_italic" id="S2.p2.1.11">ex post</span> laws in their designs and verification processes.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Lawyers’ roles as AI Creators are reflected in their robust conversations around how to create laws that direct how AI systems are designed, deployed, and developed to ensure the safe and effective use of AI systems. This includes discussions around the kinds of datasets should be allowed for training models <cite class="ltx_cite ltx_citemacro_citep">(Gerke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib13" title="">2020</a>)</cite>, protocols and system requirements when evaluating systems before and during deployment <cite class="ltx_cite ltx_citemacro_citep">(Gerke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib13" title="">2020</a>)</cite>, how governance and maintenance of these systems should be distributed <cite class="ltx_cite ltx_citemacro_citep">(Price, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib29" title="">2022</a>)</cite>, required methods for validation in localized settings <cite class="ltx_cite ltx_citemacro_citep">(Abràmoff et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib4" title="">2020</a>)</cite>, and the responsibility of individual care providers in training and using AI systems <cite class="ltx_cite ltx_citemacro_citep">(Chan, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib8" title="">2021</a>; Maliha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib23" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Limited work has investigated how to coordinate work between legal and technical experts to create effective policies and systems that together support contestability. Within the HCI community, some conversations draw attention to lawyers’ roles as AI Creators in creating policy and laws to enforce ethical design principles and ensure contestability. Alfrink et al. <cite class="ltx_cite ltx_citemacro_citep">(Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib6" title="">[n. d.]</a>)</cite> emphasize the need for <span class="ltx_text ltx_font_italic" id="S2.p4.1.1">ex ante</span> safeguards to ensure contestability, placing policy level constraints to protect against potential harms through requirements on certifications, including specific aspects of design or requiring certain outputs that enable monitoring and evaluation. Others use the term “policy-software-decisions loop” to describe the direct impact of policy on the design of software and subsequently how decisions are made with and around AI systems <cite class="ltx_cite ltx_citemacro_citep">(Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib5" title="">2023</a>)</cite>. Unfortunately, lawyers have been shown to lack understanding around AI systems’ capabilities <cite class="ltx_cite ltx_citemacro_citep">(Gorski and Ramakrishna, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib14" title="">2021</a>)</cite>, which can impair their ability to create laws to effectively support contestability. Collaboration across the policy-software-decisions loop could help address this, but limited work has explored methods for including and leveraging lawyers’ roles as AI Creators to support contestability through the intentional co-design of laws and technology.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Lawyers as Intermediaries</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">After AI systems are deployed, lawyers continue to play an important part in shaping their contestability.
Lawyers are integral to interpreting the law and determining legal consequences across the network of people tied to harms from decisions with the AI system. Lawyers examine evidence from a case, integrating it with legal information to construct a narrative that can persuade a judge or jury to act in favor of their client’s perspective. For example, in a medical malpractice suit against a physician, a lawyer advocating for the patient might work to build a case clearly depicting the physician’s negligence in care. A lawyer for the physician might aim to form and solidify an image of the physician as a professional with domain knowledge, skills, and intentional care without oversight. In both cases, the lawyer serves as a conduit for their client’s position, acting as a kind of legal prism influencing subsequent legal consequences. Thus, lawyers not only stand between the legal system and clients—directly impacting legal consequences for clients—but also between clients and other people or institutions who have conflicting interests. Consequently, we use the term <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Intermediaries</span> to describe lawyers’ roles in navigating legal structures between institutions and stakeholders, interpreting the connections between the harms from using the AI system and the related network of people tied to its use.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Within the Contestability Community, attention around lawyers’ roles as Intermediaries is growing. Warren and Salehi <cite class="ltx_cite ltx_citemacro_citep">(Warren and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib37" title="">2022</a>)</cite> describe the role of public defenders in interpreting AI systems’ data and their significant impact on those they represent, “public defenders are bulwark[s] between a poor person charged with a crime and the most consequential harms of state surveillance.” Other studies have highlighted how lawyers come alongside those they represent, emphasizing their role in the “political process” (i.e. power-related process) of contesting an AI decision <cite class="ltx_cite ltx_citemacro_citep">(Karusala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib19" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Although others may accompany stakeholders in pushing back against AI systems, it is important to specifically design for lawyers’ capacities as Intermediaries because of their unique legal position.
The term “intermediaries” has been used more widely to describe all who come alongside others to push back against unjust or incorrect decisions, including but not restricted to lawyers, NGO’s, and other institutions <cite class="ltx_cite ltx_citemacro_citep">(Karusala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib19" title="">2024</a>)</cite>. However, lawyers are uniquely positioned with respect to the law and their ability to navigate legal structures to push back against AI systems. Lawyers are trained to cultivate skills that allow them to make effective arguments in court and to navigate legal procedures. One study on Contestability underscored how lawyers’ abilities to practically enact contestability through the legal system stems from their “institutional knowledge of in-house experts, attorney-analyst liaisons, and learning from other public defenders” <cite class="ltx_cite ltx_citemacro_citep">(Jin and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib18" title="">2024</a>)</cite>. Specific design requirements addressing lawyers’ needs as Intermediaries can help ensure AI systems are contestable, but limited work has investigated how to understand or design for how they exercise their unique capacities to advocate for clients.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Designing with Lawyers</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Specific recommendations around opportunities for engaging lawyers in co-designing AI systems and contestability mechanisms need to be created <cite class="ltx_cite ltx_citemacro_citep">(Jin and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib18" title="">2024</a>)</cite>. There’s a growing movement to integrate design thinking in the law to support creative human-centered problem solving around such challenges. Proponents argue that design thinking can help lawyers think “not what the law is, but what it can be” <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib38" title="">2023</a>)</cite> and even advocate for the integration of design thinking into law schools’ pedagogy <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib38" title="">2023</a>; Ursel, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib33" title="">2017</a>)</cite>. In line with this movement is the field of <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Legal Design</span>, the application of design thinking to assess and recreate legal systems, documents, and services <cite class="ltx_cite ltx_citemacro_citep">(Haapio, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib15" title="">2014</a>; Hagan, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib16" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Legal design draws on a variety of human-centered design methods and has been used to support community- and user-centered revisions of legal services <cite class="ltx_cite ltx_citemacro_citep">(Passera and Haapio, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib28" title="">2013</a>; Haapio, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib15" title="">2014</a>)</cite>. For example, the Wikimedia foundation used Legal Design as a part of re-creating their trademark policy, so they could support cross-field discussion among its lawyers, contributors, and users around the new policy <cite class="ltx_cite ltx_citemacro_citep">(Haapio, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib15" title="">2014</a>)</cite>. Others have used Legal Design to approach redesigning contracts, so users more easily understand what they are agreeing to <cite class="ltx_cite ltx_citemacro_citep">(Passera and Haapio, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib28" title="">2013</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Work in Legal Design demonstrates the potential for using design thinking with lawyers to tackle challenges around technology use. But it has largely been used to innovate around legal changes with new technology, rather than technical methods, and there is a need for co-designing with lawyers to create specific recommendations for the design of AI systems <cite class="ltx_cite ltx_citemacro_citep">(Jin and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib18" title="">2024</a>)</cite>. Because lawyers act as AI Creators and Intermediaries, tailoring design methods for these two roles can open ways to coordinate policy and technical efforts for contestability. Here we discuss several areas where design innovations could support cross-disciplinary work to improve AI systems’ designs. We also articulate corresponding challenges to help focus the development and refinement of these design innovations <cite class="ltx_cite ltx_citemacro_citep">(Hirsch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib17" title="">2017</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Opportunity to Create Co-Design Methods</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Creating co-design methods that leverage lawyers’ roles as AI Creators and Intermediaries can coordinate work between the communities, expanding the design space by aligning technical and legal pathways for contestability. Research has recognized the need not just to investigate technical methods for contestable AI systems, but the importance of laws in enforcing them to ensure ethical and safe designs <cite class="ltx_cite ltx_citemacro_citep">(Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib6" title="">[n. d.]</a>)</cite>. In order to coordinate work between the lawyers and other AI Creators, there is a need for cross-disciplinary communication <cite class="ltx_cite ltx_citemacro_citep">(van Otterlo and Atzmueller, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib34" title="">2018</a>; Alfrink et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib6" title="">[n. d.]</a>)</cite> to “continually assessing the changing risk environment” and address it <cite class="ltx_cite ltx_citemacro_citep">(Oswald, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib27" title="">2013</a>)</cite>. Co-design methods can provide space for experts in diverse fields to contribute and share knowledge <cite class="ltx_cite ltx_citemacro_citep">(Vines et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib36" title="">2013</a>)</cite>, supporting this kind of cross-disciplinary communication to improve contestability.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Co-design methods that recognize lawyers’ roles as AI Creators and Intermediaries can enhance the design of both laws and technology to enable actionability on several fronts. First, they can help developers understand how laws translate to technical requirements for AI systems, so they can better anticipate and design for their impact on the sociotechnical environment. For example, co-design methods could help with tailoring AI explanations for requirements around contestability <cite class="ltx_cite ltx_citemacro_citep">(Leofante et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib21" title="">2024</a>)</cite>, ensuring AI systems have the legally required explanations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Co-design methods can also help ensure laws have their intended effect on technology design. For example, some have highlighted the role that speculative co-design practices could play in understanding the potential impact of technologies and discussing the policy dimensions of their implementation <cite class="ltx_cite ltx_citemacro_citep">(Spaa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib31" title="">2019</a>; Warren and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib37" title="">2022</a>)</cite>. While speculative design holds a lot of promise, it is important to account for “issue spotting”—a skill taught in law school where lawyers identify specific features of a case that are legally problematic <cite class="ltx_cite ltx_citemacro_citep">(Cuthill and McCartney, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib9" title="">1993</a>)</cite>. Without carefully considering what kinds of details are included in speculative activities, lawyers will likely turn to issue spotting, leading them to fixate on details in the interfaces, rather than thinking about the broader context in which the system is used and how that connects to users’ and current challenges in the law. But well-tailored descriptions, visuals, and prototypes can help lawyers generate ideas without turning to issue spotting and design more effective laws.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Co-design methods can also help pinpoint new areas for innovation beyond explanations. Lawyers act as Intermediaries representing those impacted by decisions made with AI systems. Prior work has shown how challenges with technology tied to AI systems can undermine lawyers’ legal defense, translating to significant consequences for clients and negatively impacting the contestability of AI systems <cite class="ltx_cite ltx_citemacro_citep">(Warren and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib37" title="">2022</a>)</cite>. Consequently, there’s a need to address barriers in engaging lawyers in “evaluation design and building tools” <cite class="ltx_cite ltx_citemacro_citep">(Jin and Salehi, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib18" title="">2024</a>)</cite>. Co-design methods with lawyers could meet these calls and clarify AI systems’ designs to more clearly communicate rights and liabilities when using a system. This could help users and lawyers more easily understand and pursue avenues for change when adverse outcomes happen.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Creating Boundary Objects</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The other design opportunity is to create design artifacts (e.g. visualizations, graphics, etc) that can serve as <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">boundary objects</span>, promoting shared understandings for legal and technical communities. Boundary objects help communicate shared articulations and common understandings between knowledge communities that have different cultures.
Boundary objects have been used in both the HCI and Legal Design Communities to help critique and innovate around infrastructures. In HCI, boundary objects are central to coorperative and participatory forms of design <cite class="ltx_cite ltx_citemacro_citep">(Vines et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib36" title="">2013</a>)</cite> and have been used to critique technology <cite class="ltx_cite ltx_citemacro_citep">(Vines et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib35" title="">2012</a>)</cite> and direct new avenues for research <cite class="ltx_cite ltx_citemacro_citep">(Spaa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib31" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Similarly, in Legal Design, boundary objects have been used in community-centered work between researchers institutions, lawyers, and clients to re-make policies in response to new uses of technology. For example, the Wikimedia Foundation collaborated with researchers to create a set of boundary objects to communicate about and elicit feedback from the Wikimedia community about the current and proposed trademark policy. These boundary objects took the form of graphics that used plain language and visualization techniques to clearly communicate how trademarks might differ between the policies. It pinpointed the most salient aspects of different trademark uses, and then linked back to the legal text for additional information. In this way, it helped people understand the different actions they could take with the trademarks and how they would differ between the policies, so they could provide feedback on changes for the new policy.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We advocate for the continued and expanded use of boundary objects to communicate with lawyers throughout the co-design process and to summarize findings from collaborations between the communities. Doing so can help expand and develop best practices around designing AI systems and promote innovation around contestability. For example, a boundary object could take the form of a study artifact that is annotated to support others in creating their own inter-disciplinary co-design methods; prototypes of systems could be used to generate discussions around legal or technical challenges; and visual summaries of findings could help others more easily tackle outstanding challenges around state-of-the-art contestability findings.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Differences that Co-Design Methods and Boundary Objects Must Navigate</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">There are several differences between technical and legal communities that co-design methods and boundary objects must navigate. Here we articulate two challenges to help focus the development and refinement of these methods: (1) lawyers potential lack of understanding around AI systems’ capabilities; and (2) fundamental terminological differences between the communities that may cause confusions.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">First, lawyers may not understand how the technological capabilities of AI systems relate to contestability. For example, many legal scholars argue that more transparent, inherently explainable systems translate to trust and accountability <cite class="ltx_cite ltx_citemacro_citep">(Babic and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib7" title="">2023</a>; Doshi-Velez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib10" title="">2017</a>; Ehsan and Riedl, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib11" title="">2024</a>)</cite>, but they have incorrect expectations around what explainability methods can do or how they contribute to transparency. One set of researchers reported that lawyers, when working with an explainable AI tool, expected the system to support its conclusions with citations and facts relevant to the system’s decision <cite class="ltx_cite ltx_citemacro_citep">(Gorski and Ramakrishna, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib14" title="">2021</a>)</cite>.
This can be a problematic interpretation of explainability for many high performing AI systems that are not inherently explainable and could not provide this information.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">There are also fundamental terminology differences between the communities. The computer science <cite class="ltx_cite ltx_citemacro_citep">(Yurrita et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib39" title="">2023</a>; Leofante et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib21" title="">2024</a>)</cite> and legal communities <cite class="ltx_cite ltx_citemacro_citep">(Babic and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib7" title="">2023</a>)</cite> both see explanations as key mechanisms for contestability. However, they do not agree on the definition of “explanations”.
In the computer science community, there is no overarching definition of “explainability” <cite class="ltx_cite ltx_citemacro_citep">(Nyrup and Robinson, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib26" title="">2022</a>)</cite>. Explainable, interpretable, intelligible, and scrutable <cite class="ltx_cite ltx_citemacro_citep">(Abdul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib3" title="">2018</a>; Khosravi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib20" title="">2022</a>)</cite> have been used across the field to describe a variety of overlapping approaches to AI systems. Even among computer scientists, the two most widely used terms—“explainable” and “interpretable”—are criticized for the ambiguity with which they are used, described, and evaluated <cite class="ltx_cite ltx_citemacro_citep">(Nyrup and Robinson, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib26" title="">2022</a>; Lipton, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib22" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">On the other hand, legal scholars are defining their own ways of using and distinguishing explainability approaches. For instance, Babic and Cohen <cite class="ltx_cite ltx_citemacro_citep">(Babic and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib7" title="">2023</a>)</cite> define “interpretable” algorithms in a way that maps to inherently explainable AI algorithmic approaches, and they use “explainable” to refer to post-hoc methods of explainability. In direct contrast, Abdul et al. <cite class="ltx_cite ltx_citemacro_citep">(Abdul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.17626v1#bib.bib3" title="">2018</a>)</cite>, a group of computer science researchers, use “interpretable machine learning (iML)” to refer to post-hoc methods of explainability. Later the same paper uses “Explainable AI” as a term encompassing iML—a double conflict of terminology with Babic and Cohen. When developing co-design methods and creating boundary objects, it is important to take into account these fundamental differences in terminological standards. Differences can significantly impact the participation of legal participants and can cause confusion as to what regulations and proposed policies mean for the development of AI applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Laws and regulations dramatically shape the creation of AI systems. However, lawyers’ roles in the AI value chain remains unrecognized. Recognizing lawyers as AI Creators and Intermediaries pinpoints areas of co-design that can improve and expand the contestability of AI systems.
Our work contributes a first step in illuminating the specific ways in which lawyers’ impact the AI value chain as AI Creators and Intermediaries, and in advocating for cross-disciplinary design with lawyers. We articulate challenges in engaging cross-disciplinary design with lawyers and suggest new opportunities for design innovations to meet this challenges, leverage lawyers’ skills, and increase the contestability of AI systems.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This material is based upon work supported by the National Science Foundation GRFP under Grant No. DGE-2039655. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FDA (2008)</span>
<span class="ltx_bibblock">
2008.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Recent Supreme Court Decision Bars Product Liability Claims Involving…</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reinhartlaw.com/news-insights/recent-supreme-court-decision-bars-product-liability-claims-involving-fda-approved-class-iii-medical-devices" title="">https://www.reinhartlaw.com/news-insights/recent-supreme-court-decision-bars-product-liability-claims-involving-fda-approved-class-iii-medical-devices</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdul et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankanhalli. 2018.

</span>
<span class="ltx_bibblock">Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> (Montreal QC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(CHI ’18)</em>. Association for Computing Machinery, New York, NY, USA, 1–18.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3173574.3174156" title="">https://doi.org/10.1145/3173574.3174156</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abràmoff et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Michael D. Abràmoff, Danny Tobey, and Danton S. Char. 2020.

</span>
<span class="ltx_bibblock">Lessons Learned About Autonomous AI: Finding a Safe, Efficacious, and Ethical Path Through the Development Process.

</span>
<span class="ltx_bibblock">214 (2020), 134–142.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.ajo.2020.02.022" title="">https://doi.org/10.1016/j.ajo.2020.02.022</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alfrink et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kars Alfrink, Ianus Keller, Neelke Doorn, and Gerd Kortuem. 2023.

</span>
<span class="ltx_bibblock">Contestable Camera Cars: A Speculative Design Exploration of Public AI That Is Open and Responsive to Dispute. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Hamburg, Germany) <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 8, 16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580984" title="">https://doi.org/10.1145/3544548.3580984</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alfrink et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Kars Alfrink, Ianus Keller, Gerd Kortuem, and Neelke Doorn. [n. d.].

</span>
<span class="ltx_bibblock">Contestable AI by Design: Towards a Framework.

</span>
<span class="ltx_bibblock">33, 4 ([n. d.]), 613–639.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11023-022-09611-z" title="">https://doi.org/10.1007/s11023-022-09611-z</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babic and Cohen (2023)</span>
<span class="ltx_bibblock">
Boris Babic and I. Glenn Cohen. 2023.

</span>
<span class="ltx_bibblock">The Algorithmic Explainability ’Bait and Switch’.

</span>
<span class="ltx_bibblock">4541487 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://papers.ssrn.com/abstract=4541487" title="">https://papers.ssrn.com/abstract=4541487</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan (2021)</span>
<span class="ltx_bibblock">
Benny Chan. 2021.

</span>
<span class="ltx_bibblock">Applying a Common Enterprise Theory of Liability to Clinical AI Systems.

</span>
<span class="ltx_bibblock">47, 4 (2021), 351–385.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1017/amj.2022.1" title="">https://doi.org/10.1017/amj.2022.1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuthill and McCartney (1993)</span>
<span class="ltx_bibblock">
Barbara Cuthill and Robert McCartney. 1993.

</span>
<span class="ltx_bibblock">Issue spotting in legal cases. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 4th International Conference on Artificial Intelligence and Law</em> (New York, NY, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">(ICAIL ’93)</em>. Association for Computing Machinery, 245–253.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/158976.159007" title="">https://doi.org/10.1145/158976.159007</a>
</span>
<span class="ltx_bibblock">event-place: Amsterdam, The Netherlands.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doshi-Velez et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Finale Doshi-Velez, Mason Kortz, Ryan Budish, Christopher Bavitz, Samuel J. Gershman, David O’Brien, Kate Scott, Stuart Shieber, Jim Waldo, David Weinberger, Adrian Weller, and Alexandra Wood. 2017.

</span>
<span class="ltx_bibblock">Accountability of AI Under the Law: The Role of Explanation.

</span>
<span class="ltx_bibblock">3064761 (2017).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2139/ssrn.3064761" title="">https://doi.org/10.2139/ssrn.3064761</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsan and Riedl (2024)</span>
<span class="ltx_bibblock">
Upol Ehsan and Mark O. Riedl. 2024.

</span>
<span class="ltx_bibblock">Social construction of XAI: Do we need one definition to rule them all?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Patterns</em> 5, 2 (2024), 100926.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.patter.2024.100926" title="">https://doi.org/10.1016/j.patter.2024.100926</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FDA Center for Devices and Radiological Health (2023)</span>
<span class="ltx_bibblock">
FDA Center for Devices and Radiological Health. 2023.

</span>
<span class="ltx_bibblock">Artificial Intelligence and Machine Learning in Software as a Medical Device. (2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device" title="">https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-software-medical-device</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerke et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Sara Gerke, Boris Babic, Theodoros Evgeniou, and I. Glenn Cohen. 2020.

</span>
<span class="ltx_bibblock">The need for a system view to regulate artificial intelligence/machine learning-based software as medical device.

</span>
<span class="ltx_bibblock">3, 1 (2020), 53.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41746-020-0262-2" title="">https://doi.org/10.1038/s41746-020-0262-2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gorski and Ramakrishna (2021)</span>
<span class="ltx_bibblock">
Lukasz Gorski and Shashishekar Ramakrishna. 2021.

</span>
<span class="ltx_bibblock">Explainable artificial intelligence, lawyer’s perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law</em> (New York, NY, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">(ICAIL ’21)</em>. Association for Computing Machinery, 60–68.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3462757.3466145" title="">https://doi.org/10.1145/3462757.3466145</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haapio (2014)</span>
<span class="ltx_bibblock">
Helena Haapio. 2014.

</span>
<span class="ltx_bibblock">Lawyers as Designers, Engineers and Innovators: Better Legal Documents Through Information Design and Visualization. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 17th International Legal Informatics Symposium IRIS 2014</em> (Rochester, NY).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://papers.ssrn.com/abstract=2651066" title="">https://papers.ssrn.com/abstract=2651066</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hagan (2020)</span>
<span class="ltx_bibblock">
Margaret Hagan. 2020.

</span>
<span class="ltx_bibblock">Legal Design as a Thing: A Theory of Change and a Set of Methods to Craft a Human-Centered Legal System.

</span>
<span class="ltx_bibblock">36, 3 (2020), 3–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/desi_a_00600" title="">https://doi.org/10.1162/desi_a_00600</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirsch et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Tad Hirsch, Kritzia Merced, Shrikanth Narayanan, Zac E. Imel, and David C. Atkins. 2017.

</span>
<span class="ltx_bibblock">Designing Contestability: Interaction Design, Machine Learning, and Mental Health. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 2017 Conference on Designing Interactive Systems</em> (Edinburgh, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(DIS ’17)</em>. Association for Computing Machinery, New York, NY, USA, 95–99.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3064663.3064703" title="">https://doi.org/10.1145/3064663.3064703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin and Salehi (2024)</span>
<span class="ltx_bibblock">
Angela Jin and Niloufar Salehi. 2024.

</span>
<span class="ltx_bibblock">(Beyond) Reasonable Doubt: Challenges that Public Defenders Face in Scrutinizing AI in Court. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 467, 19 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3641902" title="">https://doi.org/10.1145/3613904.3641902</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karusala et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Naveena Karusala, Sohini Upadhyay, Rajesh Veeraraghavan, and Krzysztof Z. Gajos. 2024.

</span>
<span class="ltx_bibblock">Understanding Contestability on the Margins: Implications for the Design of Algorithmic Decision-making in Public Services. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib19.4.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 478, 16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3641898" title="">https://doi.org/10.1145/3613904.3641898</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosravi et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hassan Khosravi, Simon Buckingham Shum, Guanliang Chen, Cristina Conati, Yi-Shan Tsai, Judy Kay, Simon Knight, Roberto Martinez-Maldonado, Shazia Sadiq, and Dragan Gašević. 2022.

</span>
<span class="ltx_bibblock">Explainable Artificial Intelligence in education.

</span>
<span class="ltx_bibblock">3 (2022), 100074.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.caeai.2022.100074" title="">https://doi.org/10.1016/j.caeai.2022.100074</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leofante et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Francesco Leofante, Hamed Ayoobi, Adam Dejl, Gabriel Freedman, Deniz Gorur, Junqi Jiang, Guilherme Paulino-Passos, Antonio Rago, Anna Rapberger, Fabrizio Russo, Xiang Yin, Dekai Zhang, and Francesca Toni. 2024.

</span>
<span class="ltx_bibblock">Contestable AI needs Computational Argumentation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.10729 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.10729" title="">https://arxiv.org/abs/2405.10729</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipton (2018)</span>
<span class="ltx_bibblock">
Zachary Chase Lipton. 2018.

</span>
<span class="ltx_bibblock">The Mythos of Model Interpretability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Commun. ACM</em> 61 (2018), 36 – 43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maliha et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
George Maliha, Sara Gerke, I. Glenn Cohen, and Ravi B. Parikh. 2021.

</span>
<span class="ltx_bibblock">Artificial Intelligence and Liability in Medicine: Balancing Safety and Innovation.

</span>
<span class="ltx_bibblock">99, 3 (2021), 629–647.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111/1468-0009.12504" title="">https://doi.org/10.1111/1468-0009.12504</a>
</span>
<span class="ltx_bibblock">Publisher: John Wiley &amp; Sons, Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mulligan et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Deirdre K. Mulligan, Daniel Kluttz, and Nitin Kohli. 2019.

</span>
<span class="ltx_bibblock">Shaping Our Tools: Contestability as a Means to Promote Responsible Algorithmic Decision Making in the Professions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2139/ssrn.3311894" title="">https://doi.org/10.2139/ssrn.3311894</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nash et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Louise Nash, Christopher Tennant, and Merrilyn Walton. 2004.

</span>
<span class="ltx_bibblock">The Psychological Impact of Complaints and Negligence Suits on Doctors.

</span>
<span class="ltx_bibblock">12, 3 (2004), 278–281.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/j.1039-8562.2004.02079.x" title="">https://doi.org/10.1080/j.1039-8562.2004.02079.x</a>
</span>
<span class="ltx_bibblock">Publisher: SAGE Publications Ltd.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nyrup and Robinson (2022)</span>
<span class="ltx_bibblock">
Rune Nyrup and Diana Robinson. 2022.

</span>
<span class="ltx_bibblock">Explanatory pragmatism: a context-sensitive framework for explainable medical AI.

</span>
<span class="ltx_bibblock">24, 1 (2022), 13.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s10676-022-09632-3" title="">https://doi.org/10.1007/s10676-022-09632-3</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oswald (2013)</span>
<span class="ltx_bibblock">
Marion Oswald. 2013.

</span>
<span class="ltx_bibblock">Something Bad Might Happen: Lawyers, anonymization and risk.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">XRDS</em> 20, 1 (9 2013), 22–26.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2508970" title="">https://doi.org/10.1145/2508970</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Passera and Haapio (2013)</span>
<span class="ltx_bibblock">
Stefania Passera and Helena Haapio. 2013.

</span>
<span class="ltx_bibblock">Transforming contracts from legal rules to user-centered communication tools: a human-information interaction challenge.

</span>
<span class="ltx_bibblock">1, 3 (2013), 38–45.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2466489.2466498" title="">https://doi.org/10.1145/2466489.2466498</a>
</span>
<span class="ltx_bibblock">Place: New York, NY, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Price (2022)</span>
<span class="ltx_bibblock">
W. Nicholson Price. 2022.

</span>
<span class="ltx_bibblock">Distributed Governance of Medical AI.

</span>
<span class="ltx_bibblock">25, 1 (2022), 3.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.25172/smustlr.25.1.2" title="">https://doi.org/10.25172/smustlr.25.1.2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Price II and Cohen (2023)</span>
<span class="ltx_bibblock">
W. Nicholson Price II and I. Glenn Cohen. 2023.

</span>
<span class="ltx_bibblock">Locating Liability for Medical AI.

</span>
<span class="ltx_bibblock">4517740 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2139/ssrn.4517740" title="">https://doi.org/10.2139/ssrn.4517740</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spaa et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Anne Spaa, Abigail Durrant, Chris Elsden, and John Vines. 2019.

</span>
<span class="ltx_bibblock">Understanding the Boundaries between Policymaking and HCI. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em> (Glasgow, Scotland Uk) <em class="ltx_emph ltx_font_italic" id="bib.bib31.4.2">(CHI ’19)</em>. Association for Computing Machinery, New York, NY, USA, 1–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300314" title="">https://doi.org/10.1145/3290605.3300314</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundholm (2024)</span>
<span class="ltx_bibblock">
Benjamin Sundholm. 2024.

</span>
<span class="ltx_bibblock">Trust the Process.

</span>
<span class="ltx_bibblock">56 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ursel (2017)</span>
<span class="ltx_bibblock">
Susan Ursel. 2017.

</span>
<span class="ltx_bibblock">Building Better Law: How Design Thinking can Help Us be Better Lawyers, Meet New Challenges, and Create the Future of Law.

</span>
<span class="ltx_bibblock">34, 1 (2017), 28–59.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.22329/wyaj.v34i1.4999" title="">https://doi.org/10.22329/wyaj.v34i1.4999</a>
</span>
<span class="ltx_bibblock">Publisher: Faculty of Law, University of Windsor.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Otterlo and Atzmueller (2018)</span>
<span class="ltx_bibblock">
Martijn van Otterlo and Martin Atzmueller. 2018.

</span>
<span class="ltx_bibblock">On Requirements and Design Criteria for Explainability in Legal AI: The 31st International Conference on Legal Knowledge and Information Systems.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vines et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
John Vines, Mark Blythe, Stephen Lindsay, Paul Dunphy, Andrew Monk, and Patrick Olivier. 2012.

</span>
<span class="ltx_bibblock">Questionable concepts: critique as resource for designing with eighty somethings. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (Austin, Texas, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib35.4.2">(CHI ’12)</em>. Association for Computing Machinery, New York, NY, USA, 1169–1178.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2207676.2208567" title="">https://doi.org/10.1145/2207676.2208567</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vines et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
John Vines, Rachel Clarke, Peter Wright, John McCarthy, and Patrick Olivier. 2013.

</span>
<span class="ltx_bibblock">Configuring participation: on how we involve people in design. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (Paris, France) <em class="ltx_emph ltx_font_italic" id="bib.bib36.4.2">(CHI ’13)</em>. Association for Computing Machinery, New York, NY, USA, 429–438.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2470654.2470716" title="">https://doi.org/10.1145/2470654.2470716</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warren and Salehi (2022)</span>
<span class="ltx_bibblock">
Rachel B. Warren and Niloufar Salehi. 2022.

</span>
<span class="ltx_bibblock">Trial by File Formats: Exploring Public Defenders’ Challenges Working with Novel Surveillance Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proc. ACM Hum.-Comput. Interact.</em> 6, CSCW1, Article 67 (apr 2022), 26 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3512914" title="">https://doi.org/10.1145/3512914</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (2023)</span>
<span class="ltx_bibblock">
Pengju Xu. 2023.

</span>
<span class="ltx_bibblock">Lawyers’ creative confidence and thinking ability: the significance of design thinking in legal education. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Science of Law Journal</em>, Vol. 2. 1–11.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.23977/law.2023.021201" title="">https://doi.org/10.23977/law.2023.021201</a>
</span>
<span class="ltx_bibblock">Publisher: Clausius Scientific Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yurrita et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mireia Yurrita, Tim Draws, Agathe Balayn, Dave Murray-Rust, Nava Tintarev, and Alessandro Bozzon. 2023.

</span>
<span class="ltx_bibblock">Disentangling Fairness Perceptions in Algorithmic Decision-Making: the Effects of Explanations, Human Oversight, and Contestability. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Hamburg, Germany) <em class="ltx_emph ltx_font_italic" id="bib.bib39.4.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 134, 21 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3581161" title="">https://doi.org/10.1145/3544548.3581161</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 26 08:17:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
