<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Fully Aligned Network for Referring Image Segmentation</title>
<!--Generated on Sun Sep 29 06:12:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.19569v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S1" title="In Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S2" title="In Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3" title="In Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS1" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS2" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Image and Language Encoding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS3" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Activation Module</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS4" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Vision-to-Language Decoder</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS5" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Language-to-Vision Decoder</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.SS6" title="In III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">Discussion of Framework and Principles</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4" title="In Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS1" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Datasets and Metrics.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS2" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Implementation Details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS3" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Comparison with State-of-the-arts</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS4" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS4.SSS0.Px1" title="In IV-D Ablation Study ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title">Interaction Principles.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS4.SSS0.Px2" title="In IV-D Ablation Study ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title">Structure of Language-to-Vision Decoder.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.SS4.SSS0.Px3" title="In IV-D Ablation Study ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title">Structure of Vision Projection Module.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S5" title="In Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fully Aligned Network for Referring 
<br class="ltx_break"/>Image Segmentation </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Liu<sup class="ltx_sup" id="id1.1.id1">1</sup> ,
Ruihao Xu<sup class="ltx_sup" id="id2.2.id2">1</sup> ,
Yansong Tang<sup class="ltx_sup" id="id3.3.id3">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id4.4.id4">1</sup>Tsinghua Shenzhen International Graduate School, Tsinghua University
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">This paper focuses on the Referring Image Segmentation (RIS) task, which aims to segment objects from an image based on a given language description.
The critical problem of RIS is achieving fine-grained alignment between different modalities to recognize and segment the target object.
Recent advances using the attention mechanism for cross-modal interaction have achieved excellent progress. However, current methods tend to lack explicit principles of interaction design as guidelines, leading to inadequate cross-modal comprehension. Additionally, most previous works use a single-modal mask decoder for prediction, losing the advantage of full cross-modal alignment. To address these challenges, we present a Fully Aligned Network (FAN) that follows four cross-modal interaction principles. Under the guidance of reasonable rules, our FAN achieves state-of-the-art performance on the prevalent RIS benchmarks (RefCOCO, RefCOCO+, G-Ref) with a simple architecture.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Referring Image Segmentation (RIS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib2" title="">2</a>]</cite> aims to segment the target object in an image based on a given text description. RIS requires understanding the content of different modalities to identify and segment the target accurately. This task is crucial in multi-modal research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib6" title="">6</a>]</cite>, with applications in human-robot interaction and image processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The main challenge in RIS is aligning different modalities due to varied image content and unrestricted language expression. Early methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib12" title="">12</a>]</cite> concatenated linguistic features with vision features but performed poorly due to lack of cross-modal interaction. Later methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib14" title="">14</a>]</cite> used multi-modal graph reasoning to localize referred objects based on detailed descriptions. With the development of transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib17" title="">17</a>]</cite>, taking cross-attention operation for vision and language alignment has received growing interest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a>]</cite>.
However, there remain two potential problems that constrain the development of this field.
Firstly, almost all current methods take a single-modal mask decoder to output the prediction mask. Due to the lack of vision-and-language interaction, the mask decoder tends to lose the advantage of fully utilizing multi-modal guidance.
Secondly, the design of previous models lacks explicit alignment principles as guidance, which may lead to insufficient cross-modal alignment.
As a result, they usually design respective auxiliary modules to improve performance. But these auxiliary modules are often not generalizable.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To this end, we summarize four cross-modal interaction principles and present a simple, clean yet strong Fully Aligned Network (FAN).
The structure design of FAN is guided by the following principles:
<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Encoding Interaction:</span> performing preliminary activation of visual features, which helps to alleviate the effect of background pixels. <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">Coarse and Fine-Grained Interaction:</span> utilizing both word-level and sentence-level features for detailed target object highlighting.
<span class="ltx_text ltx_font_italic" id="S1.p3.1.3">Multi-Scale Interaction:</span> leveraging diverse information from visual features at hierarchical scales.
<span class="ltx_text ltx_font_italic" id="S1.p3.1.4">Bidirectional Interaction:</span> updating visual and linguistic features simultaneously to create a joint space by producing implicit content-aware expressions that are more suitable for model understanding.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">With these principles, FAN builds a well-aligned visual and textual common space using attention operations, which allows the prediction mask can be generated by simple similarity calculation without the need for a complex operation. Our experiments on RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite>, RefCOCO+<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite>, and G-Ref <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib22" title="">22</a>]</cite> datasets show that FAN achieves excellent performance. Our contributions can be concluded as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose explicit interaction principles that help to build deep cross-modal relationships between image content and language description. Guided by that, we design a conceptually simple, clean, yet strong framework named Fully Aligned Network (FAN), which achieves fully cross-modal alignment with a attention mechanism.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our FAN achieves state-of-the-art performance on the popular dataset: RefCOCO, RefCOCO+, and G-Ref.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Referring image segmentation (RIS) segments pixels into masks based on natural language expressions, requiring effective cross-modal alignment. Initial baselines include <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib23" title="">23</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib24" title="">24</a>]</cite>. Subsequent methods generally fall into two main categories.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The first idea is to utilize text structure to excavate linguistic relationships further for object targeting.
MAttNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib25" title="">25</a>]</cite> proposes to decompose the description into different modular components related to appearance, location, and relationships.
Some other methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib14" title="">14</a>]</cite> leverage the graph networks to model the internal structure of the text.
However, the above methods do not model well-aligned cross-modal common space, and their pipelines tend to be complex.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The other idea is to model the cross-modal relations between image and language by various attention operations.
KWAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib27" title="">27</a>]</cite> utilizes the cross-modal cross-attention to build the joint space.
EFN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib28" title="">28</a>]</cite> and LAVT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib19" title="">19</a>]</cite> propose to fuse inside the visual backbone.
CRIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a>]</cite> leverages the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib29" title="">29</a>]</cite> pre-trained weights with a contrastive loss.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Recent advances <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib6" title="">6</a>]</cite> have achieved excellent performance but lack explicit alignment principles. Additionally, most previous works use a single-modal mask decoder for prediction, which misses the benefits of full cross-modal alignment. To this end, we propose explicit interaction principles and introduce a conceptually simple, clean, yet strong framework called the Fully Aligned Network (FAN).</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S2.F1.g1" src="x1.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pipeline of our FAN. Taking an image and the corresponding language expression as input, the vision and language encoder extract corresponding features, respectively. Then a multi-scale activation module performs preliminary fusion between them to highlight the referred region roughly. For the decoding process, we update visual and linguistic features simultaneously to project them into the common space. Finally, the output mask is obtained by simple similarity calculation and binarization.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Overview</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S2.F1" title="In II Related Work ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> illustrates the pipeline of our Fully Aligned Network (FAN). Given an image and a descriptive language expression, a vision encoder and a language encoder extract visual and linguistic features. The image is encoded into hierarchical features <math alttext="f_{v}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">f_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, and the text into fine-grained word embeddings <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑓</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> and coarse-grained sentence embeddings <math alttext="f_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑓</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">f_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. A multi-scale activation module fuses these features to highlight the referent region and reduce background noise.
Subsequently, the model embeds these features into a joint space, updating both of them with attention mechanisms in vision-to-language and language-to-vision decoders. Finally, the target region is isolated from the background using matrix multiplication.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Image and Language Encoding</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">For the input image <math alttext="I\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">I</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3.3.4" xref="S3.SS2.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐼</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">𝑊</ci><cn id="S3.SS2.p1.1.m1.1.1.3.3.4.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">I\in\mathbb{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_I ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, a pyramidal vision encoder extracts hierarchical features <math alttext="f_{v}^{i}\in\mathbb{R}^{\frac{H}{2^{i}}\times\frac{W}{2^{i}}\times C_{v}^{i}}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msubsup id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.2.cmml">f</mi><mi id="S3.SS2.p1.2.m2.1.1.2.2.3" xref="S3.SS2.p1.2.m2.1.1.2.2.3.cmml">v</mi><mi id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml"><mfrac id="S3.SS2.p1.2.m2.1.1.3.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml">H</mi><msup id="S3.SS2.p1.2.m2.1.1.3.3.2.3" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml"><mn id="S3.SS2.p1.2.m2.1.1.3.3.2.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.2.cmml">2</mn><mi id="S3.SS2.p1.2.m2.1.1.3.3.2.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.3.cmml">i</mi></msup></mfrac><mo id="S3.SS2.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mfrac id="S3.SS2.p1.2.m2.1.1.3.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.3.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.3.2.cmml">W</mi><msup id="S3.SS2.p1.2.m2.1.1.3.3.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3.cmml"><mn id="S3.SS2.p1.2.m2.1.1.3.3.3.3.2" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3.2.cmml">2</mn><mi id="S3.SS2.p1.2.m2.1.1.3.3.3.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3.3.cmml">i</mi></msup></mfrac><mo id="S3.SS2.p1.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.2.m2.1.1.3.3.1.cmml">×</mo><msubsup id="S3.SS2.p1.2.m2.1.1.3.3.4" xref="S3.SS2.p1.2.m2.1.1.3.3.4.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.3.4.2.2" xref="S3.SS2.p1.2.m2.1.1.3.3.4.2.2.cmml">C</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3.4.2.3" xref="S3.SS2.p1.2.m2.1.1.3.3.4.2.3.cmml">v</mi><mi id="S3.SS2.p1.2.m2.1.1.3.3.4.3" xref="S3.SS2.p1.2.m2.1.1.3.3.4.3.cmml">i</mi></msubsup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><in id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></in><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">superscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2.2">𝑓</ci><ci id="S3.SS2.p1.2.m2.1.1.2.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2.3">𝑣</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3"><times id="S3.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.1"></times><apply id="S3.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2"><divide id="S3.SS2.p1.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2"></divide><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.2">𝐻</ci><apply id="S3.SS2.p1.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.2.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3">superscript</csymbol><cn id="S3.SS2.p1.2.m2.1.1.3.3.2.3.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.2">2</cn><ci id="S3.SS2.p1.2.m2.1.1.3.3.2.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.2.3.3">𝑖</ci></apply></apply><apply id="S3.SS2.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3"><divide id="S3.SS2.p1.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3"></divide><ci id="S3.SS2.p1.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3.2">𝑊</ci><apply id="S3.SS2.p1.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.3.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3">superscript</csymbol><cn id="S3.SS2.p1.2.m2.1.1.3.3.3.3.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3.2">2</cn><ci id="S3.SS2.p1.2.m2.1.1.3.3.3.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.3.3.3">𝑖</ci></apply></apply><apply id="S3.SS2.p1.2.m2.1.1.3.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.4.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4">superscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.3.3.4.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.3.4.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.3.4.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4.2.2">𝐶</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.4.2.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4.2.3">𝑣</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.3.3.4.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3.4.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">f_{v}^{i}\in\mathbb{R}^{\frac{H}{2^{i}}\times\frac{W}{2^{i}}\times C_{v}^{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_H end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG × divide start_ARG italic_W end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG × italic_C start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="i\in" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">i</mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">∈</mo><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><in id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></in><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑖</ci><csymbol cd="latexml" id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">i\in</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_i ∈</annotation></semantics></math> [2,3,4,5]. Here, <math alttext="H" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_H</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_W</annotation></semantics></math> denote the height and width of the image, and <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_C</annotation></semantics></math> denotes the channel dimension.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.6">For the input text <math alttext="L\in\mathbb{R}^{l}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">L</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐿</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">ℝ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">L\in\mathbb{R}^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_L ∈ blackboard_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>, a transformer-based text encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib31" title="">31</a>]</cite> encodes it into a word embedding <math alttext="f_{w}\in\mathbb{R}^{l\times C_{t}}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><msub id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml">w</mi></msub><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">l</mi><mo id="S3.SS2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.3.2.cmml">C</mi><mi id="S3.SS2.p2.2.m2.1.1.3.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.3.cmml">t</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">𝑓</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3">𝑤</ci></apply><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><times id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">𝑙</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.2">𝐶</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">f_{w}\in\mathbb{R}^{l\times C_{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_l × italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and a sentence embedding <math alttext="f_{s}\in\mathbb{R}^{1\times C_{t}}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msub id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml"><mn id="S3.SS2.p2.3.m3.1.1.3.3.2" xref="S3.SS2.p2.3.m3.1.1.3.3.2.cmml">1</mn><mo id="S3.SS2.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.SS2.p2.3.m3.1.1.3.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.3.3.2" xref="S3.SS2.p2.3.m3.1.1.3.3.3.2.cmml">C</mi><mi id="S3.SS2.p2.3.m3.1.1.3.3.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.3.3.cmml">t</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><in id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></in><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝑓</ci><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">𝑠</ci></apply><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3"><times id="S3.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.1"></times><cn id="S3.SS2.p2.3.m3.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.3.3.2">1</cn><apply id="S3.SS2.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3.2">𝐶</ci><ci id="S3.SS2.p2.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">f_{s}\in\mathbb{R}^{1\times C_{t}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT 1 × italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_l</annotation></semantics></math> is the length of the text. The sentence embedding <math alttext="f_{s}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑓</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">f_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> represents the overall characteristics of the target object, while the word embedding <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">f</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑓</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> provides detailed information for precise segmentation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Activation Module</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We use a multi-scale activation module to preliminarily activate visual features with word embeddings <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math>, highlighting the referred region. This reduces the background pixel influence on later alignment, aiding in the updating of linguistic and visual features. Our exploration showed that a multi-head cross-attention layer suffices for this activation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6">The module takes word feature <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑓</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> and hierarchical vision feature <math alttext="f_{v}^{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msubsup id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">f</mi><mi id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">v</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">𝑓</ci><ci id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3">𝑣</ci></apply><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">f_{v}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="i\in" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">i</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">∈</mo><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><in id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></in><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝑖</ci><csymbol cd="latexml" id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">i\in</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_i ∈</annotation></semantics></math> [2,3,4,5] as input. For the i-th scale, the visual feature <math alttext="f_{v}^{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><msubsup id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml">f</mi><mi id="S3.SS3.p2.4.m4.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.2.3.cmml">v</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">superscript</csymbol><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2">𝑓</ci><ci id="S3.SS3.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.2.3">𝑣</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">f_{v}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> serves as the query, and the word vector <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">𝑓</ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> as the key and value. The process involves projecting input features to the corresponding space, applying multi-head attention to these projections, and then generating the activated cross-modal features <math alttext="f_{c}^{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><msubsup id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.cmml">f</mi><mi id="S3.SS3.p2.6.m6.1.1.2.3" xref="S3.SS3.p2.6.m6.1.1.2.3.cmml">c</mi><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">superscript</csymbol><apply id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2">𝑓</ci><ci id="S3.SS3.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.2.3">𝑐</ci></apply><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">f_{c}^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Vision-to-Language Decoder</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.8">We use the Vision-to-Language Decoder and Language-to-Vision Decoder to align visual and linguistic embeddings in a shared space.
The Vision-to-Language Decoder (V2L) takes an FPN-like architecture with a cross-modal alignment module. The Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib32" title="">32</a>]</cite>, often used in object detection and segmentation, fuses multi-scale information and upsamples output features. We input multi-scale activated vision features <math alttext="f_{c}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">f_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> with strides from 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mo id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><times id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">×</annotation></semantics></math> to 32<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mo id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><times id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">×</annotation></semantics></math>. It outputs decoded 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><mo id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><times id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">×</annotation></semantics></math> features. Fusion is performed from <math alttext="f^{5}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><msup id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">f</mi><mn id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">𝑓</ci><cn id="S3.SS4.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS4.p1.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">f^{5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">italic_f start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="f^{2}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><msup id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">f</mi><mn id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">𝑓</ci><cn id="S3.SS4.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.SS4.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">f^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">italic_f start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="f^{2}" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><msup id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2" xref="S3.SS4.p1.7.m7.1.1.2.cmml">f</mi><mn id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2">𝑓</ci><cn id="S3.SS4.p1.7.m7.1.1.3.cmml" type="integer" xref="S3.SS4.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">f^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">italic_f start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> is 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.p1.8.m8.1"><semantics id="S3.SS4.p1.8.m8.1a"><mo id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><times id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.8.m8.1d">×</annotation></semantics></math> downsampled.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="498" id="S3.F2.g1" src="x2.png" width="498"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The structure of the Vision Projection Module (VPM).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Unlike vanilla FPN, our V2L decoder integrates linguistic guidance into visual features using a Vision Projection Module (VPM) before multi-scale fusion, aiding in transferring visual features into a multi-modal space. The VPM structure (see <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S3.F2" title="In III-D Vision-to-Language Decoder ‣ III Method ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>) includes multi-modal self-attention and cross-attention layers. For the i-th level feature, we flatten it along the spatial dimension, add fixed positional embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib33" title="">33</a>]</cite>, and concatenate the flattened tokens with word features <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">𝑓</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> to form multi-modal tokens. A multi-head self attention layer is applied to these tokens to extract relevant information and only vision tokens are selected for later cross-attention alignment.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">This process allows the model to integrate information from both modalities while modeling the shared space. Fused vision tokens then serve as the query, and word embeddings <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑓</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> as key and value for multi-head cross attention, aiding in locating the target object. Finally, the i-th level aligned vision features are output after residual connection and FFN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib15" title="">15</a>]</cite> layers.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Language-to-Vision Decoder</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">For referring image segmentation, a common method involves fusing language embeddings with visual features and using the activated features for segmentation. However, this method does not fully utilize the representational ability of linguistic features. Unrestricted language expression can be ambiguous, especially in challenging scenes where language alone cannot clearly express the target object. For instance, the term “pink” is vague until combined with an image context, such as a picture of two people, one wearing a pink dress, making “pink dress” more informative.
Even if the description is detailed, it is given by humans based on their prior knowledge.
Due to differences in knowledge domain, models may not understand given descriptions well.
This is somewhat similar to the recent research of prompt mechanism, which finds that learnable prompt embeddings work better than prompt defined by humans based on their own knowledge frameworks.
Inspired by CLIP, which jointly learns visual and textual spaces, we use a Language-to-Vision Decoder (L2V) to align linguistic features to a multi-modal common space. By aligning linguistic features with the visual space, the output textual embedding becomes more perceptive to image content, providing a more informative description that better identifies the target object and distinguishes it from others in the image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.5.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.6.2">Discussion of Framework and Principles</span>
</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.2">Our FAN adheres to the proposed cross-modal alignment principles. The activation module corresponds to the <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.2.1">encoding interaction principle</span>, highlighting the referring region. Unlike LAVT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib19" title="">19</a>]</cite> and EFN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib28" title="">28</a>]</cite>, which perform interaction within the visual backbone, we perform encoding interaction on the output feature maps. This preserves the pre-trained weights of the backbone, leveraging models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib29" title="">29</a>]</cite>.Besides,
both the activation module and vision projection module use hierarchical visual features, adhering to the <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.2.2">multi-scale interaction principle</span>. Guided by the <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.2.3">bidirectional interaction principle</span>, we update visual and textual embeddings in the vision-to-language and language-to-vision decoders to create a multi-modal common space. For the <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.2.4">coarse and fine-grained interaction principle</span>, we use fine-grained word embeddings <math alttext="f_{w}" class="ltx_Math" display="inline" id="S3.SS6.p1.1.m1.1"><semantics id="S3.SS6.p1.1.m1.1a"><msub id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">f_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math> and coarse-grained sentence embeddings <math alttext="f_{s}" class="ltx_Math" display="inline" id="S3.SS6.p1.2.m2.1"><semantics id="S3.SS6.p1.2.m2.1a"><msub id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><mi id="S3.SS6.p1.2.m2.1.1.2" xref="S3.SS6.p1.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2">𝑓</ci><ci id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">f_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> in the V2L and L2V decoders, respectively. This enables the use of detailed and holistic linguistic information to identify the target object.
Experiment results in <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T2" title="In IV-C Comparison with State-of-the-arts ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">II</span></a> demonstrate the validity and effectiveness of these principles.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiment</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison with state-of-the-art methods in terms of the IoU metric on three popular benchmarks. We have experimented different visual backbone to perform fair comparison with other methods. To show the comparison more clearly, we mark the results of same level backbone with same color. Best viewed in color.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.1" rowspan="2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.1.1.2.1" style="font-size:90%;">Vision</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T1.1.1.1.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.1.1.3.1" style="font-size:90%;">RefCOCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T1.1.1.1.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.1.1.4.1" style="font-size:90%;">RefCOCO+</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.1.1.1.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.1.1.5.1" style="font-size:90%;">G-Ref</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.2.2.1" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.1.1" style="font-size:90%;">Backbone</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.2.1" style="font-size:90%;">val</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.3.1" style="font-size:90%;">test A</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.4.1" style="font-size:90%;">test B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.5.1" style="font-size:90%;">val</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.6.1" style="font-size:90%;">test A</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.7.1" style="font-size:90%;">test B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.8.1" style="font-size:90%;">val</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.2.2.9.1" style="font-size:90%;">test</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.3.3.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.3.3.1.1" style="font-size:90%;">CAC </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.3.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib34" title="">34</a><span class="ltx_text" id="S4.T1.1.3.3.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.3.3.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.3.1" style="font-size:90%;">58.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.4.1" style="font-size:90%;">61.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.5.1" style="font-size:90%;">53.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.6.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.7.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.8.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.9.1" style="font-size:90%;">46.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.3.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.3.3.10.1" style="font-size:90%;">46.95</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.4.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.4.4.1.1" style="font-size:90%;">STEP </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.4.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib30" title="">30</a><span class="ltx_text" id="S4.T1.1.4.4.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.4.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.3.1" style="font-size:90%;">60.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.4.1" style="font-size:90%;">63.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.5.1" style="font-size:90%;">57.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.6.1" style="font-size:90%;">48.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.7.1" style="font-size:90%;">52.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.8.1" style="font-size:90%;">40.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.4.4.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.4.4.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.5.5.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.5.5.1.1" style="font-size:90%;">BRINet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.5.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib35" title="">35</a><span class="ltx_text" id="S4.T1.1.5.5.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.5.5.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.3.1" style="font-size:90%;">60.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.4.1" style="font-size:90%;">62.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.5.1" style="font-size:90%;">59.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.6.1" style="font-size:90%;">48.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.7.1" style="font-size:90%;">52.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.8.1" style="font-size:90%;">42.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.5.5.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.5.5.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.6.6.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.6.6.1.1" style="font-size:90%;">CMPC </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.6.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib14" title="">14</a><span class="ltx_text" id="S4.T1.1.6.6.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.6.6.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.3.1" style="font-size:90%;">61.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.4.1" style="font-size:90%;">64.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.5.1" style="font-size:90%;">59.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.6.1" style="font-size:90%;">49.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.7.1" style="font-size:90%;">53.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.8.1" style="font-size:90%;">43.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.6.6.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.6.6.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.7.7.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.7.7.1.1" style="font-size:90%;">LSCM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.7.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib26" title="">26</a><span class="ltx_text" id="S4.T1.1.7.7.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.7.7.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.3.1" style="font-size:90%;">61.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.4.1" style="font-size:90%;">64.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.5.1" style="font-size:90%;">59.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.6.1" style="font-size:90%;">49.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.7.1" style="font-size:90%;">53.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.8.1" style="font-size:90%;">43.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.7.7.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.7.7.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.8.8.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.8.8.1.1" style="font-size:90%;">CMPC+ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.8.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib36" title="">36</a><span class="ltx_text" id="S4.T1.1.8.8.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.8.8.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.3.1" style="font-size:90%;">62.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.4.1" style="font-size:90%;">65.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.5.1" style="font-size:90%;">60.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.6.1" style="font-size:90%;">50.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.7.1" style="font-size:90%;">54.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.8.1" style="font-size:90%;">43.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.8.8.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.8.8.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.9.9.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.9.9.1.1" style="font-size:90%;">MCN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.9.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib37" title="">37</a><span class="ltx_text" id="S4.T1.1.9.9.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.9.9.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.2.1" style="font-size:90%;">DarkNet53</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.3.1" style="font-size:90%;">62.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.4.1" style="font-size:90%;">64.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.5.1" style="font-size:90%;">59.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.6.1" style="font-size:90%;">50.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.7.1" style="font-size:90%;">54.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.8.1" style="font-size:90%;">44.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.9.9.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.9.1" style="font-size:90%;">49.22</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.9.9.10.1" style="font-size:90%;">49.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.10.10.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.10.10.1.1" style="font-size:90%;">EFN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.10.10.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib28" title="">28</a><span class="ltx_text" id="S4.T1.1.10.10.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.10.10.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.3.1" style="font-size:90%;">62.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.4.1" style="font-size:90%;">65.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.5.1" style="font-size:90%;">59.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.6.1" style="font-size:90%;">51.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.7.1" style="font-size:90%;">55.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.8.1" style="font-size:90%;">43.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.10.10.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.10.10.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.11.11.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.11.11.1.1" style="font-size:90%;">BUSNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.11.11.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib13" title="">13</a><span class="ltx_text" id="S4.T1.1.11.11.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.11.11.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.3.1" style="font-size:90%;">63.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.4.1" style="font-size:90%;">66.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.5.1" style="font-size:90%;">61.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.6.1" style="font-size:90%;">51.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.7.1" style="font-size:90%;">56.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.8.1" style="font-size:90%;">44.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.11.11.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.9.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.11.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.11.11.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.12.12.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.12.12.1.1" style="font-size:90%;">CGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.12.12.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib38" title="">38</a><span class="ltx_text" id="S4.T1.1.12.12.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.12.12.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.2.1" style="font-size:90%;">DarkNet53</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.3.1" style="font-size:90%;">64.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.4.1" style="font-size:90%;">68.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.5.1" style="font-size:90%;">62.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.6.1" style="font-size:90%;">51.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.7.1" style="font-size:90%;">55.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.8.1" style="font-size:90%;">44.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.12.12.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.9.1" style="font-size:90%;">51.01</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.12.12.10.1" style="font-size:90%;">51.69</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.13.13.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.13.13.1.1" style="font-size:90%;">LTS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.13.13.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib39" title="">39</a><span class="ltx_text" id="S4.T1.1.13.13.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.13.13.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.2.1" style="font-size:90%;">DarkNet53</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.3.1" style="font-size:90%;">65.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.4.1" style="font-size:90%;">67.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.5.1" style="font-size:90%;">63.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.6.1" style="font-size:90%;">54.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.7.1" style="font-size:90%;">58.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.8.1" style="font-size:90%;">48.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.13.13.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.9.1" style="font-size:90%;">54.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.13.13.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.13.13.10.1" style="font-size:90%;">54.25</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.14.14.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.14.14.1.1" style="font-size:90%;">VLT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.14.14.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib18" title="">18</a><span class="ltx_text" id="S4.T1.1.14.14.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.14.14.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.2.1" style="font-size:90%;">DarkNet56</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.3.1" style="font-size:90%;">65.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.4.1" style="font-size:90%;">68.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.5.1" style="font-size:90%;">62.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.6.1" style="font-size:90%;">55.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.7.1" style="font-size:90%;">59.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.8.1" style="font-size:90%;">49.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.14.14.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.9.1" style="font-size:90%;">52.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.14.14.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.14.14.10.1" style="font-size:90%;">56.65</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.15.15.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.15.15.1.1" style="font-size:90%;">ResTR </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.15.15.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib40" title="">40</a><span class="ltx_text" id="S4.T1.1.15.15.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.15.15.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.2.1" style="font-size:90%;">ViT-B</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.3.1" style="font-size:90%;">67.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.4.1" style="font-size:90%;">69.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.5.1" style="font-size:90%;">64.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.6.1" style="font-size:90%;">55.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.7.1" style="font-size:90%;">60.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.8.1" style="font-size:90%;">48.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.15.15.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.9.1" style="font-size:90%;">54.48</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.15.15.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.15.15.10.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.16.16.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.16.16.1.1" style="font-size:90%;">CRIS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.16.16.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a><span class="ltx_text" id="S4.T1.1.16.16.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.16.16.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.2.1" style="font-size:90%;">CLIP-ResNet50</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.3.1" style="font-size:90%;color:#0000FF;">69.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.4.1" style="font-size:90%;color:#0000FF;">72.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.5.1" style="font-size:90%;color:#0000FF;">64.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.6.1" style="font-size:90%;color:#0000FF;">61.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.7.1" style="font-size:90%;color:#0000FF;">67.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.8.1" style="font-size:90%;color:#0000FF;">52.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.16.16.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.9.1" style="font-size:90%;color:#0000FF;">59.35</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.16.16.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.16.16.10.1" style="font-size:90%;color:#0000FF;">59.39</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.17.17.1" style="padding-left:9.1pt;padding-right:9.1pt;">
<span class="ltx_text" id="S4.T1.1.17.17.1.1" style="font-size:90%;">LAVT </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.1.17.17.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib19" title="">19</a><span class="ltx_text" id="S4.T1.1.17.17.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.17.17.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.2.1" style="font-size:90%;">Swin-B</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.3.1" style="font-size:90%;color:#FF0000;">72.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.4.1" style="font-size:90%;color:#FF0000;">75.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.5.1" style="font-size:90%;color:#FF0000;">68.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.6.1" style="font-size:90%;color:#FF0000;">62.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.7.1" style="font-size:90%;color:#FF0000;">68.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.8.1" style="font-size:90%;color:#FF0000;">55.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.17.17.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.9.1" style="font-size:90%;color:#FF0000;">61.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.17.17.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.17.17.10.1" style="font-size:90%;color:#FF0000;">62.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.18.18.1" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.18.18.1.1" style="font-size:90%;">FAN (Ours)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.18.18.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.18.18.2.1" style="font-size:90%;">ResNet101</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.3.1" style="font-size:90%;">69.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.4.1" style="font-size:90%;">71.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.5.1" style="font-size:90%;">64.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.6.1" style="font-size:90%;">58.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.7.1" style="font-size:90%;">62.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.8.1" style="font-size:90%;">51.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.18.18.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.9.1" style="font-size:90%;">58.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.18.18.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.18.18.10.1" style="font-size:90%;">58.93</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.19.19.1" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.19.19.1.1" style="font-size:90%;">FAN (Ours)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.19.19.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.19.19.2.1" style="font-size:90%;">CLIP-ResNet50</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.3.1" style="font-size:90%;color:#0000FF;">71.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.4.1" style="font-size:90%;color:#0000FF;">74.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.5.1" style="font-size:90%;color:#0000FF;">66.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.6.1" style="font-size:90%;color:#0000FF;">62.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.7.1" style="font-size:90%;color:#0000FF;">68.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.8.1" style="font-size:90%;color:#0000FF;">53.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.1.19.19.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.9.1" style="font-size:90%;color:#0000FF;">60.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.19.19.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.19.19.10.1" style="font-size:90%;color:#0000FF;">61.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.1" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.20.20.1.1" style="font-size:90%;">FAN (Ours)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.2" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text" id="S4.T1.1.20.20.2.1" style="font-size:90%;">Swin-B</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.3" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.3.1" style="font-size:90%;color:#FF0000;">74.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.4" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.4.1" style="font-size:90%;color:#FF0000;">75.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.5" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.5.1" style="font-size:90%;color:#FF0000;">70.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.6" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.6.1" style="font-size:90%;color:#FF0000;">64.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.7" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.7.1" style="font-size:90%;color:#FF0000;">69.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.8" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.8.1" style="font-size:90%;color:#FF0000;">58.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.1.20.20.9" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.9.1" style="font-size:90%;color:#FF0000;">65.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.20.20.10" style="padding-left:9.1pt;padding-right:9.1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.20.20.10.1" style="font-size:90%;color:#FF0000;">65.51</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Datasets and Metrics.</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We used the following datasets: RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite>, derived from MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib41" title="">41</a>]</cite>, is a key dataset for image segmentation and visual grounding, divided into training, validation, and test sets. RefCOCO<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">+</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite> excludes certain location words and follows a similar split. G-Ref <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib22" title="">22</a>]</cite> features longer expressions with more location and appearance words, collected from Amazon Mechanical Turk.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2">For metrics, we use IoU and Precision<math alttext="@" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p2.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">@</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">@</annotation></semantics></math>X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib2" title="">2</a>]</cite>, where IoU measures segmentation accuracy and Precision<math alttext="@" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" mathvariant="normal" xref="S4.SS1.p2.2.m2.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">@</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">@</annotation></semantics></math>X evaluates the location ability at various IoU thresholds.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Implementation Details</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The model is implemented in Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib42" title="">42</a>]</cite>. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a>]</cite>, we initialize the vision and language encoders with CLIP-ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib29" title="">29</a>]</cite> by default. We also experiment with other vision encoders like DeepLabV3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib43" title="">43</a>]</cite> pretrained ResNet101 and ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib44" title="">44</a>]</cite> pretrained Swin-B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib45" title="">45</a>]</cite> for fair comparison, with results shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T1" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">I</span></a>.
The Language-to-Vision decoder includes 6 transformer decoder layers, each with 8 heads and a feed-forward hidden dimension of 2048. The model is optimized using cross-entropy and dice loss. Considering extra [SOS] and [EOS] tokens, the maximum sentence length is 17 for RefCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite> and RefCOCO+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib21" title="">21</a>]</cite>, and 22 for G-Ref <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib22" title="">22</a>]</cite>. Input images are resized to 416 × 416.
We train the model with the Adam <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib46" title="">46</a>]</cite> optimizer for 50 epochs on 8 Tesla V100 GPUs with a batch size of 64, taking about 7 hours. The initial learning rate is 0.0001, reduced by a factor of 0.1 at epoch 35. A smaller learning rate (scaling factor of 0.1) is set for the backbone.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For inference, the output mask is upsampled to the input image size by bilinear interpolation. Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib20" title="">20</a>]</cite>, we binarize the prediction masks with a 0.35 threshold and do not use other post-processing operations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Comparison with State-of-the-arts</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T1" title="In IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">I</span></a>, we compare our FAN with previous state-of-the-art methods on the popular datasets RefCOCO, RefCOCO+, and G-Ref using the IoU metric. To enhance clarity, results using the same visual backbone are marked with the same color.
Our FAN achieves the best performance across all datasets. With the Swin-B backbone, FAN exceeds the previous SOTA method LAVT by 2%. On the challenging G-Ref dataset, the margin extends to 4% (<span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">65.28</span> vs. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">61.24</span>). Using the CLIP backbone, FAN surpasses previous methods significantly. Additionally, our model with ResNet-101 outperforms previous approaches using DarkNet and ViT. Notably, FAN with the CLIP-ResNet50 backbone even surpasses LAVT using Swin-B on some datasets, such as <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">62.83</span> vs. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">62.14</span> on the RefCOCO+ val set.
These results demonstrate that our FAN, through effective alignment principles and simple attention operations, establishes a well-aligned vision-and-language common space, enhancing language-guided segmentation performance and simplifying the overall pipeline.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Ablation studies about the proposed interaction principles on the RefCOCO validation set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.1.1.2.1" style="font-size:90%;">IoU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.1.1.3.1" style="font-size:90%;">P@0.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.1.1.4.1" style="font-size:90%;">P@0.9</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.2.1.1.1" style="font-size:90%;">Simple Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.2.1.2.1" style="font-size:90%;">59.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.2.1.3.1" style="font-size:90%;">66.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.2.1.4.1" style="font-size:90%;">10.85</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.3.2.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.3.2.1.1" style="font-size:90%;">+ Language-to-Vision Decoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.2.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.3.2.2.1" style="font-size:90%;">64.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.2.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.3.2.3.1" style="font-size:90%;">72.88</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.3.2.4.1" style="font-size:90%;">16.28</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.4.3.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.4.3.1.1" style="font-size:90%;">+ Single-Scale Vision Projection Module</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.3.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.4.3.2.1" style="font-size:90%;">67.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.3.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.4.3.3.1" style="font-size:90%;">77.94</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.4.3.4.1" style="font-size:90%;">18.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.5.4.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.5.4.1.1" style="font-size:90%;">+ Multi-Scale Vision Projection Module</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.5.4.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.5.4.2.1" style="font-size:90%;">68.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.5.4.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.5.4.3.1" style="font-size:90%;">79.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.5.4.4.1" style="font-size:90%;">19.65</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.1.6.5.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.6.5.1.1" style="font-size:90%;">+ Activation Module</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.6.5.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.2.1" style="font-size:90%;">71.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.6.5.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.3.1" style="font-size:90%;">82.80</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.4.1" style="font-size:90%;">21.91</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T2.1.7.6.1" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.7.6.1.1" style="font-size:90%;">Only utilize sentence embedding</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.7.6.2" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.7.6.2.1" style="font-size:90%;">69.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.7.6.3" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.7.6.3.1" style="font-size:90%;">81.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.7.6.4" style="padding:0.45pt 3.0pt;"><span class="ltx_text" id="S4.T2.1.7.6.4.1" style="font-size:90%;">19.84</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Ablation Study</span>
</h3>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Interaction Principles.</h4>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T2" title="In IV-C Comparison with State-of-the-arts ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">II</span></a> demonstrates the importance of various types of interaction. Bidirectional Interaction enhances linguistic embeddings by integrating high-level visual information (row 1 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px1.p1.1.1">vs</span> row 2). Multi-scale Interaction, which fuses linguistic and visual features at various scales, ensures segmentation accuracy and superior multi-modal understanding, with performance decreasing when fusion is limited to the highest level (row 3 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px1.p1.1.2">vs</span> row 4). Encoding Interaction, involving preliminary activation of visual features, is crucial for coarse localization and minimizing background interference, with a 3% performance drop observed without the Activation Module (row 4 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px1.p1.1.3">vs</span> row 5). Lastly, Coarse and Fine-grained Interaction, utilizing both sentence-level and word-level features, provides better linguistic guidance than using sentence features alone (row 5 <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px1.p1.1.4">vs</span> row 6).</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Experiments about structure of Language-to-Vision Decoder. The vision encoder used is CLIP-ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#bib.bib29" title="">29</a>]</cite>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.1.1.2.1" style="font-size:90%;">IoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.1.1.3.1" style="font-size:90%;">P@0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.1.1.4.1" style="font-size:90%;">P@0.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2" style="background-color:#E8E8E8;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T3.1.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.1.2.2.1.1" style="font-size:90%;background-color:#E8E8E8;">(a) Structure of Language-to-Vision Decoder</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.3.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.1.1" style="font-size:90%;">1 Decoder Layer</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.2.1" style="font-size:90%;">71.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.3.1" style="font-size:90%;">82.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.3.3.4.1" style="font-size:90%;">21.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.4.4.1.1" style="font-size:90%;">3 Decoder Layers</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.4.4.2.1" style="font-size:90%;">71.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.4.4.3.1" style="font-size:90%;">82.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.4.4.4.1" style="font-size:90%;">21.33</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.5.5.1.1" style="font-size:90%;">6 Decoder Layers</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.5.5.2.1" style="font-size:90%;">71.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.5.5.3.1" style="font-size:90%;">82.80</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.5.5.4.1" style="font-size:90%;">21.91</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.6.6.1.1" style="font-size:90%;">+ Encoder Layers</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.6.6.2.1" style="font-size:90%;">71.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.6.6.3.1" style="font-size:90%;">82.92</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.6.6.4.1" style="font-size:90%;">21.93</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.7" style="background-color:#E8E8E8;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T3.1.7.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.1.7.7.1.1" style="font-size:90%;background-color:#E8E8E8;">(b) Structure of Vision Projection Module</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.8.8.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.8.8.1.1" style="font-size:90%;">Only Cross-Attention Fusion</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.8.8.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.8.8.2.1" style="font-size:90%;">68.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.8.8.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.8.8.3.1" style="font-size:90%;">78.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.8.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.8.8.4.1" style="font-size:90%;">19.38</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.1.9.9.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.9.9.1.1" style="font-size:90%;">Both Self and Cross-Attention Fusion</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.9.9.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.9.9.2.1" style="font-size:90%;">71.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.9.9.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.9.9.3.1" style="font-size:90%;">82.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.9.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T3.1.9.9.4.1" style="font-size:90%;">21.93</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Structure of Language-to-Vision Decoder.</h4>
<div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T3" title="In Interaction Principles. ‣ IV-D Ablation Study ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">III</span></a> shows that the number of transformer decoder layers has minimal impact on results, with one layer achieving 71.38 IoU, highlighting the lightweight nature of our FAN. Besides, using a transformer encoder is unnecessary since preliminary activation provides sufficient target objects. Our default setting uses no encoder layer and 6 decoder layers.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Structure of Vision Projection Module.</h4>
<div class="ltx_para" id="S4.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px3.p1.1">The results of the ablation experiments summarized in <a class="ltx_ref" href="https://arxiv.org/html/2409.19569v1#S4.T3" title="In Interaction Principles. ‣ IV-D Ablation Study ‣ IV Experiment ‣ Fully Aligned Network for Referring Image Segmentation"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">III</span></a> demonstrate that the Vision Projection Module’s structure, which adopts a transformer decoder layer approach, is superior when integrating textual guidance into visual features through concatenation in the self-attention section followed by multi-modal information fusion via cross-attention, compared to using cross-attention alone.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we address the referring image segmentation task by fully cross-modal alignment with eleborate attention mechanism.
We explicitly propose four interaction principles for aligning visual and textual information: encoding interaction, multi-scale interaction, coarse and fine-grained interaction, and bidirectional interaction.
Guided by the interaction principles, we propose a simple yet strong Fully Aligned Network (FAN), which achieves state-of-the-art performance on prevalent RIS benchmarks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
R. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib1.3.3" style="font-size:90%;">, 2016, pp. 108–124.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
R. Li, K. Li, Y.-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib2.3.3" style="font-size:90%;">, 2018, pp. 5745–5753.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Z. Luo, Y. Xiao, Y. Liu, S. Li, Y. Wang, Y. Tang, X. Li, and Y. Yang, “Soc: Semantic-assisted object cluster for referring video object segmentation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="font-size:90%;">NeurIPS</em><span class="ltx_text" id="bib.bib3.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
K. Han, Y. Liu, J. H. Liew, H. Ding, J. Liu, Y. Wang, Y. Tang, Y. Yang, J. Feng, Y. Zhao </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib4.3.3" style="font-size:90%;">, “Global knowledge calibration for fast open-vocabulary segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.4.4" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib4.5.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Y. Liu, S. Bai, G. Li, Y. Wang, and Y. Tang, “Open-vocabulary segmentation with semantic-assisted calibration,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib5.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Y. Liu, C. Zhang, Y. Wang, J. Wang, Y. Yang, and Y. Tang, “Universal segmentation at arbitrary granularity with language instruction,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib6.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Y. Liu, R. Yu, F. Yin, X. Zhao, W. Zhao, W. Xia, and Y. Yang, “Learning quality-aware dynamic memory for video object segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib7.3.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Y. Liu, R. Yu, J. Wang, X. Zhao, Y. Wang, Y. Tang, and Y. Yang, “Global spectral filter memory network for video object segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib8.3.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Y. Liu, R. Yu, X. Zhao, and Y. Yang, “Quality-aware and selective prior enhancement memory network for video object segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2" style="font-size:90%;">CVPR Workshop</em><span class="ltx_text" id="bib.bib9.3.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
X. Ni, Y. Liu, H. Wen, Y. Ji, J. Xiao, and Y. Yang, “Multimodal prototype-enhanced network for few-shot action recognition,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2" style="font-size:90%;">ICMR</em><span class="ltx_text" id="bib.bib10.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Y. Xiao, Z. Luo, Y. Liu, Y. Ma, H. Bian, Y. Ji, Y. Yang, and X. Li, “Bridging the gap: A unified video comprehension framework for moment retrieval and highlight detection,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib11.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
E. Margffoy-Tuay, J. C. Pérez, E. Botero, and P. Arbeláez, “Dynamic multimodal instance segmentation guided by natural language queries,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib12.3.3" style="font-size:90%;">, 2018, pp. 630–645.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
S. Yang, M. Xia, G. Li, H.-Y. Zhou, and Y. Yu, “Bottom-up shift and reasoning for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib13.3.3" style="font-size:90%;">, 2021, pp. 11 266–11 275.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
S. Huang, T. Hui, S. Liu, G. Li, Y. Wei, J. Han, L. Liu, and B. Li, “Referring image segmentation via cross-modal progressive comprehension,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib14.3.3" style="font-size:90%;">, 2020, pp. 10 488–10 497.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib15.3.3" style="font-size:90%;">, 2017, pp. 5998–6008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
J. Wang, S. Zhang, Y. Liu, T. Wu, Y. Yang, X. Liu, K. Chen, P. Luo, and D. Lin, “Riformer: Keep your vision backbone effective but removing token mixer,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib16.3.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
H. Zhang, Y. Wang, Y. Tang, Y. Liu, J. Feng, J. Dai, and X. Jin, “Flash-vstream: Memory-based real-time understanding for long video streams,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2" style="font-size:90%;">arXiv preprint arXiv:2406.08085</em><span class="ltx_text" id="bib.bib17.3.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
H. Ding, C. Liu, S. Wang, and X. Jiang, “Vision-language transformer and query generation for referring segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib18.3.3" style="font-size:90%;">, 2021, pp. 16 321–16 330.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Z. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib19.3.3" style="font-size:90%;">, 2022, pp. 18 155–18 165.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Z. Wang, Y. Lu, Q. Li, X. Tao, Y. Guo, M. Gong, and T. Liu, “Cris: Clip-driven referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib20.3.3" style="font-size:90%;">, 2022, pp. 11 686–11 695.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring to objects in photographs of natural scenes,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2" style="font-size:90%;">EMNLP</em><span class="ltx_text" id="bib.bib21.3.3" style="font-size:90%;">, 2014, pp. 787–798.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
V. K. Nagaraja, V. I. Morariu, and L. S. Davis, “Modeling context between objects for referring expression understanding,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib22.3.3" style="font-size:90%;">, 2016, pp. 792–807.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
R. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib23.3.3" style="font-size:90%;">, 2016, pp. 108–124.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
C. Liu, Z. Lin, X. Shen, J. Yang, X. Lu, and A. L. Yuille, “Recurrent multimodal interaction for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib24.3.3" style="font-size:90%;">, 2017, pp. 1280–1289.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg, “Mattnet: Modular attention network for referring expression comprehension,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib25.3.3" style="font-size:90%;">, 2018, pp. 1307–1315.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, “Linguistic structure guided context modeling for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib26.3.3" style="font-size:90%;">, 2020, pp. 59–75.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
H. Shi, H. Li, F. Meng, and Q. Wu, “Key-word-aware network for referring expression image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib27.3.3" style="font-size:90%;">, 2018, pp. 38–54.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
G. Feng, Z. Hu, L. Zhang, and H. Lu, “Encoder fusion network with co-attention embedding for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib28.3.3" style="font-size:90%;">, 2021, pp. 15 506–15 515.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2" style="font-size:90%;">ICML</em><span class="ltx_text" id="bib.bib29.3.3" style="font-size:90%;">, 2021, pp. 8748–8763.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
D.-J. Chen, S. Jia, Y.-C. Lo, H.-T. Chen, and T.-L. Liu, “See-through-text grouping for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib30.3.3" style="font-size:90%;">, 2019, pp. 7454–7463.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understanding,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2" style="font-size:90%;">NAACL</em><span class="ltx_text" id="bib.bib31.3.3" style="font-size:90%;">, 2019, pp. 4171–4186.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
T. Lin, P. Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, “Feature pyramid networks for object detection,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib32.3.3" style="font-size:90%;">, 2017, pp. 936–944.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib33.3.3" style="font-size:90%;">, 2020, pp. 213–229.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Y. Chen, Y. Tsai, T. Wang, Y. Lin, and M. Yang, “Referring expression object segmentation with caption-aware consistency,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2" style="font-size:90%;">BMVC</em><span class="ltx_text" id="bib.bib34.3.3" style="font-size:90%;">, 2019, p. 263.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu, “Bi-directional relationship inferring network for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib35.3.3" style="font-size:90%;">, 2020, pp. 4424–4433.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, “Cross-modal progressive comprehension for referring segmentation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2" style="font-size:90%;">TPAMI</em><span class="ltx_text" id="bib.bib36.3.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
G. Luo, Y. Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, “Multi-task collaborative network for joint referring expression comprehension and segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib37.3.3" style="font-size:90%;">, 2020, pp. 10 034–10 043.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
G. Luo, Y. Zhou, R. Ji, X. Sun, J. Su, C.-W. Lin, and Q. Tian, “Cascade grouped attention network for referring expression segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2" style="font-size:90%;">ACM MM</em><span class="ltx_text" id="bib.bib38.3.3" style="font-size:90%;">, 2020, pp. 1274–1282.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Y. Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, “Locate then segment: A strong pipeline for referring image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib39.3.3" style="font-size:90%;">, 2021, pp. 9858–9867.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, “Restr: Convolution-free referring image segmentation using transformers,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib40.3.3" style="font-size:90%;">, 2022, pp. 18 145–18 154.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in context,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2" style="font-size:90%;">ECCV</em><span class="ltx_text" id="bib.bib41.3.3" style="font-size:90%;">, 2014, pp. 740–755.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-performance deep learning library,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.2.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib42.3.3" style="font-size:90%;">, 2019, pp. 8024–8035.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
L. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous convolution for semantic image segmentation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2" style="font-size:90%;">arXiv preprint arXiv:1706.05587</em><span class="ltx_text" id="bib.bib43.3.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2" style="font-size:90%;">CVPR</em><span class="ltx_text" id="bib.bib44.3.3" style="font-size:90%;">, 2009, pp. 248–255.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2" style="font-size:90%;">ICCV</em><span class="ltx_text" id="bib.bib45.3.3" style="font-size:90%;">, 2021, pp. 9992–10 002.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib46.3.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 29 06:12:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
