<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning</title>
<!--Generated on Wed Jul  3 12:41:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.03076v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S1" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S2" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.SS1" title="In 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.SS2" title="In 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Cascade Multi-Task Learning Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.SS3" title="In 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Context Selection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.SS0.SSS0.Px1" title="In 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title">Vanilla-Sent:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.SS0.SSS0.Px2" title="In 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title">Concat-Context:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.SS0.SSS0.Px3" title="In 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title">Inside-Context:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.SS1" title="In 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.SS2" title="In 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>NMT Model Setups</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS1" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Results of MTL and Contrastive Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS2" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Analysis of Reconstruction Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS3" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>MTL vs. Multi-Encoder Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS4" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Effect of Context in MTL setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS5" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Results of MTL and Multi-Encoder models without Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS6" title="In 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Pronoun Translation Accuracy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S6" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S7" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1" title="In A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1" title="In Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Preliminary Investigation on Auxiliary Objectives</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1.SSS1" title="In A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>Effect of Random Context</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ramakrishna Appicharla<sup class="ltx_sup" id="id10.10.id1">1</sup>, Baban Gain<sup class="ltx_sup" id="id11.11.id2">1</sup>, Santanu Pal<sup class="ltx_sup" id="id12.12.id3">2</sup>, Asif Ekbal<sup class="ltx_sup" id="id13.13.id4">3</sup>, Pushpak Bhattacharyya<sup class="ltx_sup" id="id14.14.id5">4</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id6">1</sup>Department of Computer Science and Engineering, Indian Institute of Technology Patna, India 
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.id7">2</sup>Wipro AI, Lab45, London, UK 
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id8">3</sup>School of AI and Data Science, Indian Institute of Technology Jodhpur, India 
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id9">4</sup>Department of Computer Science and Engineering, Indian Institute of Technology Bombay, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id19.19.id10">{ramakrishnaappicharla, gainbaban, santanu.pal.ju,</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id20.20.id11">asif.ekbal, pushpakbh} @gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id21.id1">In document-level neural machine translation (DocNMT), multi-encoder approaches are common in encoding context and source sentences. Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> have shown that the context encoder generates noise and makes the model robust to the choice of context. This paper further investigates this observation by explicitly modelling context encoding through multi-task learning (MTL) to make the model sensitive to the choice of context. We conduct experiments on cascade MTL architecture, which consists of one encoder and two decoders. Generation of the source from the context is considered an auxiliary task, and generation of the target from the source is the main task. We experimented with German–English language pairs on News, TED, and Europarl corpora. Evaluation results show that the proposed MTL approach performs better than concatenation-based and multi-encoder DocNMT models in low-resource settings and is sensitive to the choice of context. However, we observe that the MTL models are failing to generate the source from the context. These observations align with the previous studies, and this might suggest that the available document-level parallel corpora are not context-aware, and a robust sentence-level model can outperform the context-aware models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Context-aware neural machine translation gained much attention due to the ability to incorporate context, which helps in producing more consistent translations than sentence-level models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx23" title="">Maruf and Haffari, 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx41" title="">Zhang et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx5" title="">Bawden et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx1" title="">Agrawal et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx34" title="">Voita et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx9" title="">Huo et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx7" title="">Donato et al., 2021</a>]</cite>. There are mainly two approaches to incorporating context. The first one is to create a context-aware input sentence by concatenating context and current input sentence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx31" title="">Tiedemann and Scherrer, 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx1" title="">Agrawal et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx43" title="">Zhang et al., 2020b</a>]</cite> and using it as the input to the encoder. The second approach uses an additional context-aware component to encode the source or target context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx41" title="">Zhang et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx33" title="">Voita et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx12" title="">Kim et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx22" title="">Ma et al., 2020</a>]</cite> and the entire model is jointly optimized. Typically, the current sentence’s neighbouring sentences (either previous or next) are used as the context.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.4">The context-aware models are trained to maximize the log-likelihood of the target sentence given the source sentence and context. Most of the existing works on DocNMT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx41" title="">Zhang et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx23" title="">Maruf and Haffari, 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx34" title="">Voita et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> focus on encoding the context through context-specific encoders. Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> show that, in the multi-encoder DocNMT models, the performance improvement is not due to specific context encoding but rather the context-encoder acts like a noise generator, which, in turn, improves the robustness of the model. In this work, we explore whether the context encoding can be modelled explicitly through multi-task learning (MTL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx21" title="">Luong et al., 2015</a>]</cite>. Specifically, we aim to study the effectiveness of the MTL framework for DocNMT rather than proposing a state-of-the-art system. The availability of document-level corpora is less compared to sentence-level corpora. Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite> use the sentence-level corpora to warm-start the document-level model, which can be further tuned with the existing limited amount of document-level data. However, in this work, we focus only on improving the performance of DocNMT models with available document-level corpora. We consider the source reconstruction from the context as the auxiliary task and the target translation from the source as the main task. We conduct experiments on cascade MTL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx2" title="">Anastasopoulos and Chiang, 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx44" title="">Zhou et al., 2019</a>]</cite> architecture. The cascade MTL architecture comprises one encoder and two decoders (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.F1" title="Figure 1 ‣ 3.2 Cascade Multi-Task Learning Transformer ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a>). The intermediate (first) decoder attends over the output of the encoder, and the final (second) decoder attends over the output of the intermediate decoder. The input consists of <math alttext="\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle" class="ltx_Math" display="inline" id="S1.p2.1.m1.3"><semantics id="S1.p2.1.m1.3a"><mrow id="S1.p2.1.m1.3.3.1" xref="S1.p2.1.m1.3.3.2.cmml"><mo id="S1.p2.1.m1.3.3.1.2" stretchy="false" xref="S1.p2.1.m1.3.3.2.cmml">⟨</mo><msub id="S1.p2.1.m1.3.3.1.1" xref="S1.p2.1.m1.3.3.1.1.cmml"><mi id="S1.p2.1.m1.3.3.1.1.2" mathvariant="normal" xref="S1.p2.1.m1.3.3.1.1.2.cmml">c</mi><mi id="S1.p2.1.m1.3.3.1.1.3" mathvariant="normal" xref="S1.p2.1.m1.3.3.1.1.3.cmml">x</mi></msub><mo id="S1.p2.1.m1.3.3.1.3" xref="S1.p2.1.m1.3.3.2.cmml">,</mo><mi id="S1.p2.1.m1.1.1" mathvariant="normal" xref="S1.p2.1.m1.1.1.cmml">x</mi><mo id="S1.p2.1.m1.3.3.1.4" xref="S1.p2.1.m1.3.3.2.cmml">,</mo><mi id="S1.p2.1.m1.2.2" mathvariant="normal" xref="S1.p2.1.m1.2.2.cmml">y</mi><mo id="S1.p2.1.m1.3.3.1.5" stretchy="false" xref="S1.p2.1.m1.3.3.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.3b"><list id="S1.p2.1.m1.3.3.2.cmml" xref="S1.p2.1.m1.3.3.1"><apply id="S1.p2.1.m1.3.3.1.1.cmml" xref="S1.p2.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.3.3.1.1.1.cmml" xref="S1.p2.1.m1.3.3.1.1">subscript</csymbol><ci id="S1.p2.1.m1.3.3.1.1.2.cmml" xref="S1.p2.1.m1.3.3.1.1.2">c</ci><ci id="S1.p2.1.m1.3.3.1.1.3.cmml" xref="S1.p2.1.m1.3.3.1.1.3">x</ci></apply><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">x</ci><ci id="S1.p2.1.m1.2.2.cmml" xref="S1.p2.1.m1.2.2">y</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.3c">\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.3d">⟨ roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT , roman_x , roman_y ⟩</annotation></semantics></math> triplets, where <math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><msub id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml"><mi id="S1.p2.2.m2.1.1.2" mathvariant="normal" xref="S1.p2.2.m2.1.1.2.cmml">c</mi><mi id="S1.p2.2.m2.1.1.3" mathvariant="normal" xref="S1.p2.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S1.p2.2.m2.1.1.1.cmml" xref="S1.p2.2.m2.1.1">subscript</csymbol><ci id="S1.p2.2.m2.1.1.2.cmml" xref="S1.p2.2.m2.1.1.2">c</ci><ci id="S1.p2.2.m2.1.1.3.cmml" xref="S1.p2.2.m2.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S1.p2.3.m3.1"><semantics id="S1.p2.3.m3.1a"><mi id="S1.p2.3.m3.1.1" mathvariant="normal" xref="S1.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><ci id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.3.m3.1d">roman_x</annotation></semantics></math> and <math alttext="\mathrm{y}" class="ltx_Math" display="inline" id="S1.p2.4.m4.1"><semantics id="S1.p2.4.m4.1a"><mi id="S1.p2.4.m4.1.1" mathvariant="normal" xref="S1.p2.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">y</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">\mathrm{y}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.4.m4.1d">roman_y</annotation></semantics></math> represents the context, source, and target sentences, respectively. The model is trained to optimize both translation and reconstruction objectives jointly. We also train two baseline models as contrastive models, namely sentence-level vanilla baseline and single encoder-decoder model, by concatenating the context and source <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx31" title="">Tiedemann and Scherrer, 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx1" title="">Agrawal et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite>. We additionally train multi-encoder single-decoder models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> to study how context affects the DocNMT models. We conduct experiments on German-English direction with three different types of contexts (<span class="ltx_text ltx_font_italic" id="S1.p2.4.1">viz.</span> previous two source sentences, previous two target sentences, and previous-next source sentences) on News-commentary v14 and TED corpora. We report BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx27" title="">Papineni et al., 2002</a>]</cite> calculated with sacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx29" title="">Post, 2018</a>]</cite> and APT (accuracy of pronoun translation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx26" title="">Miculicich Werlen and Popescu-Belis, 2017</a>]</cite> scores.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To summarize, the specific attributes of our current work are as follows:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We explore whether the MTL approach can improve the performance of context-aware NMT by introducing additional training objectives along with the main translation objective.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose an MTL approach where the reconstruction of the source sentence given the context is used as an auxiliary task and the translation of the target sentence from the source sentence as the main task, jointly optimized during the training.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The results show that in the MTL approach, the context encoder generates noise similar to the multi-encoder approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite>, which makes the model robust to the choice of the context.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Previous studies have proposed various document-level NMT models and achieved great success. The main goal of these approaches is to efficiently model context representation, which can lead to better translation quality.
Towards this goal to represent context, Tiedemann and Scherrer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx31" title="">Tiedemann and Scherrer, 2017</a>]</cite> concatenate consecutive sentences and use them as input to the single-encoder-based DocNMT model. Agrawal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx1" title="">Agrawal et al., 2018</a>]</cite> conducted experiments on varying neighbouring contexts and concatenated with the current sentence as input to their model. With these similar trends, Junczys-Dowmunt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite> conducted experiments considering the entire document as context. Further progress on context representation in DocNMT, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx41" title="">Zhang et al., 2018</a>]</cite> and Voita et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx33" title="">Voita et al., 2018</a>]</cite> proposed transformer-based multi-encoder NMT models where the additional encoder is used to encode the context. While Miculicich et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx25" title="">Miculicich et al., 2018</a>]</cite> proposed a hierarchical attention network to encode the context, a more recent approach Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx11" title="">Kang et al., 2020</a>]</cite> proposed a reinforcement learning-based dynamic context selection module for DocNMT. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx12" title="">Kim et al., 2019</a>]</cite> and Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> conducted experiments on multi-encoder DocNMT models and reported that the performance improvement is not due to context encoding; instead, the context encoder acts as a noise generator, which improves the robustness of the DocNMT model. Junczys-Dowmunt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite> conducted experiments on a single encoder model with masked language model objective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx6" title="">Devlin et al., 2019</a>]</cite> to incorporate document-level monolingual source-side data. Since the multi-encoder models are trained to optimize the translation objective only, it might be possible for the model to pay less attention to the context, and Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> report the same.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">MTL strategies in NMT trained on other auxiliary tasks along with the main translation task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx21" title="">Luong et al., 2015</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx8" title="">Dong et al., 2015</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx40" title="">Zaremoodi et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx35" title="">Wang et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx38" title="">Yang et al., 2020</a>]</cite> achieved significant improvements in translation quality so far. The other auxiliary tasks include autoencoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx21" title="">Luong et al., 2015</a>]</cite>, denoising autoencoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx35" title="">Wang et al., 2020</a>]</cite>, parsing and named entity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx39" title="">Zaremoodi and Haffari, 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx40" title="">Zaremoodi et al., 2018</a>]</cite>. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx44" title="">Zhou et al., 2019</a>]</cite> proposed a cascade MTL network to improve the robustness of the NMT model. They considered denoising the noisy text as an auxiliary task and the translation as the main task. They achieved a significant BLEU score improvement (up to 7.1 BLEU) on the WMT robustness shared task on the French-English dataset.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">However, most multi-task models are proposed only for sentence-level NMT models. Multi-task learning is relatively unexplored in context-aware NMT settings. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx36" title="">Wang et al., 2021</a>]</cite> proposed an MTL framework for dialogue translation tasks that jointly correct the sentences having issues such as pronoun dropping, punctuation dropping, and typos and translate them into the target language. Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx19" title="">Liang et al., 2022</a>]</cite> proposed a three-stage training framework for the neural chat translation task. The model is trained on auxiliary tasks such as monolingual cross-lingual response generation tasks to generate coherent translation and the next utterance discrimination task. Lei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx17" title="">Lei et al., 2022</a>]</cite> proposed an MTL system to force the model to attend over relevant cohesion devices while translating the current sentence. In this work, we propose a multi-task learning objective, i.e., reconstruction of source sentences given the source context in a cascade multi-task learning setting to study the effect of context in document-level NMT systems.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Statement</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">Our document-level NMT is based on a cascade MTL framework to force the model to consider the context while generating translation. Given a source sentence <math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">roman_x</annotation></semantics></math> and context <math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS1.p1.2.m2.1.1.3" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">c</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>, the translation probability of the target sentence <math alttext="\mathrm{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">y</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathrm{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">roman_y</annotation></semantics></math> in the DocNMT setting is calculated as in Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E1" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathrm{y})=p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}})\times p(\mathrm{x},%
\mathrm{c_{x}})" class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml"><mi id="S3.E1.m1.5.5.4.2" xref="S3.E1.m1.5.5.4.2.cmml">p</mi><mo id="S3.E1.m1.5.5.4.1" xref="S3.E1.m1.5.5.4.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.4.3.2" xref="S3.E1.m1.5.5.4.cmml"><mo id="S3.E1.m1.5.5.4.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.4.cmml">(</mo><mi id="S3.E1.m1.1.1" mathvariant="normal" xref="S3.E1.m1.1.1.cmml">y</mi><mo id="S3.E1.m1.5.5.4.3.2.2" stretchy="false" xref="S3.E1.m1.5.5.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml">p</mi><mo id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.cmml">y</mi><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">x</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">x</mi></msub></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.2" rspace="0.222em" xref="S3.E1.m1.4.4.1.1.2.cmml">×</mo><mi id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml">p</mi></mrow><mo id="S3.E1.m1.5.5.2.3" xref="S3.E1.m1.5.5.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.2.2.1" xref="S3.E1.m1.5.5.2.2.2.cmml"><mo id="S3.E1.m1.5.5.2.2.1.2" stretchy="false" xref="S3.E1.m1.5.5.2.2.2.cmml">(</mo><mi id="S3.E1.m1.3.3" mathvariant="normal" xref="S3.E1.m1.3.3.cmml">x</mi><mo id="S3.E1.m1.5.5.2.2.1.3" xref="S3.E1.m1.5.5.2.2.2.cmml">,</mo><msub id="S3.E1.m1.5.5.2.2.1.1" xref="S3.E1.m1.5.5.2.2.1.1.cmml"><mi id="S3.E1.m1.5.5.2.2.1.1.2" mathvariant="normal" xref="S3.E1.m1.5.5.2.2.1.1.2.cmml">c</mi><mi id="S3.E1.m1.5.5.2.2.1.1.3" mathvariant="normal" xref="S3.E1.m1.5.5.2.2.1.1.3.cmml">x</mi></msub><mo id="S3.E1.m1.5.5.2.2.1.4" stretchy="false" xref="S3.E1.m1.5.5.2.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"></eq><apply id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4"><times id="S3.E1.m1.5.5.4.1.cmml" xref="S3.E1.m1.5.5.4.1"></times><ci id="S3.E1.m1.5.5.4.2.cmml" xref="S3.E1.m1.5.5.4.2">𝑝</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">y</ci></apply><apply id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"><times id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.3"></times><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1"><times id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></times><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2"></times><ci id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3">𝑝</ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2">conditional</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3">y</ci><list id="S3.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">x</ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2">c</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3">x</ci></apply></list></apply></apply><ci id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3">𝑝</ci></apply><interval closure="open" id="S3.E1.m1.5.5.2.2.2.cmml" xref="S3.E1.m1.5.5.2.2.1"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">x</ci><apply id="S3.E1.m1.5.5.2.2.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.2.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.2.2.1.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1.2">c</ci><ci id="S3.E1.m1.5.5.2.2.1.1.3.cmml" xref="S3.E1.m1.5.5.2.2.1.1.3">x</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">p(\mathrm{y})=p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}})\times p(\mathrm{x},%
\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_p ( roman_y ) = italic_p ( roman_y | roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) × italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4">We consider <math alttext="p(\mathrm{x},\mathrm{c_{x}})" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.2"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.2" xref="S3.SS1.p3.1.m1.2.2.cmml"><mi id="S3.SS1.p3.1.m1.2.2.3" xref="S3.SS1.p3.1.m1.2.2.3.cmml">p</mi><mo id="S3.SS1.p3.1.m1.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.cmml">⁢</mo><mrow id="S3.SS1.p3.1.m1.2.2.1.1" xref="S3.SS1.p3.1.m1.2.2.1.2.cmml"><mo id="S3.SS1.p3.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS1.p3.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p3.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p3.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p3.1.m1.2.2.1.1.3" xref="S3.SS1.p3.1.m1.2.2.1.2.cmml">,</mo><msub id="S3.SS1.p3.1.m1.2.2.1.1.1" xref="S3.SS1.p3.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.1.m1.2.2.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p3.1.m1.2.2.1.1.1.3" mathvariant="normal" xref="S3.SS1.p3.1.m1.2.2.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.p3.1.m1.2.2.1.1.4" stretchy="false" xref="S3.SS1.p3.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><apply id="S3.SS1.p3.1.m1.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2"><times id="S3.SS1.p3.1.m1.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2"></times><ci id="S3.SS1.p3.1.m1.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.3">𝑝</ci><interval closure="open" id="S3.SS1.p3.1.m1.2.2.1.2.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">x</ci><apply id="S3.SS1.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1.1.2">c</ci><ci id="S3.SS1.p3.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.2.2.1.1.1.3">x</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">p(\mathrm{x},\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.2d">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math> as the auxiliary task of source (<math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">roman_x</annotation></semantics></math>) reconstruction from <math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" mathvariant="normal" xref="S3.SS1.p3.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS1.p3.3.m3.1.1.3" mathvariant="normal" xref="S3.SS1.p3.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">c</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math> (as <math alttext="p(\mathrm{x}|\mathrm{c_{x}})" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">p</mi><mo id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.4.m4.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.4.m4.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS1.p3.4.m4.1.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS1.p3.4.m4.1.1.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.1.1.1.3.2" mathvariant="normal" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.SS1.p3.4.m4.1.1.1.1.1.3.3" mathvariant="normal" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S3.SS1.p3.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><times id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2"></times><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">𝑝</ci><apply id="S3.SS1.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p3.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2">x</ci><apply id="S3.SS1.p3.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.2">c</ci><ci id="S3.SS1.p3.4.m4.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.3">x</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">p(\mathrm{x}|\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math>) <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Since the joint probability of <math alttext="p(\mathrm{x},\mathrm{c_{x}})" class="ltx_Math" display="inline" id="footnote1.m1.2"><semantics id="footnote1.m1.2b"><mrow id="footnote1.m1.2.2" xref="footnote1.m1.2.2.cmml"><mi id="footnote1.m1.2.2.3" xref="footnote1.m1.2.2.3.cmml">p</mi><mo id="footnote1.m1.2.2.2" xref="footnote1.m1.2.2.2.cmml">⁢</mo><mrow id="footnote1.m1.2.2.1.1" xref="footnote1.m1.2.2.1.2.cmml"><mo id="footnote1.m1.2.2.1.1.2" stretchy="false" xref="footnote1.m1.2.2.1.2.cmml">(</mo><mi id="footnote1.m1.1.1" mathvariant="normal" xref="footnote1.m1.1.1.cmml">x</mi><mo id="footnote1.m1.2.2.1.1.3" xref="footnote1.m1.2.2.1.2.cmml">,</mo><msub id="footnote1.m1.2.2.1.1.1" xref="footnote1.m1.2.2.1.1.1.cmml"><mi id="footnote1.m1.2.2.1.1.1.2" mathvariant="normal" xref="footnote1.m1.2.2.1.1.1.2.cmml">c</mi><mi id="footnote1.m1.2.2.1.1.1.3" mathvariant="normal" xref="footnote1.m1.2.2.1.1.1.3.cmml">x</mi></msub><mo id="footnote1.m1.2.2.1.1.4" stretchy="false" xref="footnote1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m1.2c"><apply id="footnote1.m1.2.2.cmml" xref="footnote1.m1.2.2"><times id="footnote1.m1.2.2.2.cmml" xref="footnote1.m1.2.2.2"></times><ci id="footnote1.m1.2.2.3.cmml" xref="footnote1.m1.2.2.3">𝑝</ci><interval closure="open" id="footnote1.m1.2.2.1.2.cmml" xref="footnote1.m1.2.2.1.1"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">x</ci><apply id="footnote1.m1.2.2.1.1.1.cmml" xref="footnote1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="footnote1.m1.2.2.1.1.1.1.cmml" xref="footnote1.m1.2.2.1.1.1">subscript</csymbol><ci id="footnote1.m1.2.2.1.1.1.2.cmml" xref="footnote1.m1.2.2.1.1.1.2">c</ci><ci id="footnote1.m1.2.2.1.1.1.3.cmml" xref="footnote1.m1.2.2.1.1.1.3">x</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.2d">p(\mathrm{x},\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.2e">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math> can be calculated as <math alttext="p(\mathrm{c_{x}}|\mathrm{x})\times p(\mathrm{x})" class="ltx_Math" display="inline" id="footnote1.m2.2"><semantics id="footnote1.m2.2b"><mrow id="footnote1.m2.2.2" xref="footnote1.m2.2.2.cmml"><mrow id="footnote1.m2.2.2.1" xref="footnote1.m2.2.2.1.cmml"><mrow id="footnote1.m2.2.2.1.1" xref="footnote1.m2.2.2.1.1.cmml"><mi id="footnote1.m2.2.2.1.1.3" xref="footnote1.m2.2.2.1.1.3.cmml">p</mi><mo id="footnote1.m2.2.2.1.1.2" xref="footnote1.m2.2.2.1.1.2.cmml">⁢</mo><mrow id="footnote1.m2.2.2.1.1.1.1" xref="footnote1.m2.2.2.1.1.1.1.1.cmml"><mo id="footnote1.m2.2.2.1.1.1.1.2" stretchy="false" xref="footnote1.m2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="footnote1.m2.2.2.1.1.1.1.1" xref="footnote1.m2.2.2.1.1.1.1.1.cmml"><msub id="footnote1.m2.2.2.1.1.1.1.1.2" xref="footnote1.m2.2.2.1.1.1.1.1.2.cmml"><mi id="footnote1.m2.2.2.1.1.1.1.1.2.2" mathvariant="normal" xref="footnote1.m2.2.2.1.1.1.1.1.2.2.cmml">c</mi><mi id="footnote1.m2.2.2.1.1.1.1.1.2.3" mathvariant="normal" xref="footnote1.m2.2.2.1.1.1.1.1.2.3.cmml">x</mi></msub><mo fence="false" id="footnote1.m2.2.2.1.1.1.1.1.1" xref="footnote1.m2.2.2.1.1.1.1.1.1.cmml">|</mo><mi id="footnote1.m2.2.2.1.1.1.1.1.3" mathvariant="normal" xref="footnote1.m2.2.2.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="footnote1.m2.2.2.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="footnote1.m2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="footnote1.m2.2.2.1.2" rspace="0.222em" xref="footnote1.m2.2.2.1.2.cmml">×</mo><mi id="footnote1.m2.2.2.1.3" xref="footnote1.m2.2.2.1.3.cmml">p</mi></mrow><mo id="footnote1.m2.2.2.2" xref="footnote1.m2.2.2.2.cmml">⁢</mo><mrow id="footnote1.m2.2.2.3.2" xref="footnote1.m2.2.2.cmml"><mo id="footnote1.m2.2.2.3.2.1" stretchy="false" xref="footnote1.m2.2.2.cmml">(</mo><mi id="footnote1.m2.1.1" mathvariant="normal" xref="footnote1.m2.1.1.cmml">x</mi><mo id="footnote1.m2.2.2.3.2.2" stretchy="false" xref="footnote1.m2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m2.2c"><apply id="footnote1.m2.2.2.cmml" xref="footnote1.m2.2.2"><times id="footnote1.m2.2.2.2.cmml" xref="footnote1.m2.2.2.2"></times><apply id="footnote1.m2.2.2.1.cmml" xref="footnote1.m2.2.2.1"><times id="footnote1.m2.2.2.1.2.cmml" xref="footnote1.m2.2.2.1.2"></times><apply id="footnote1.m2.2.2.1.1.cmml" xref="footnote1.m2.2.2.1.1"><times id="footnote1.m2.2.2.1.1.2.cmml" xref="footnote1.m2.2.2.1.1.2"></times><ci id="footnote1.m2.2.2.1.1.3.cmml" xref="footnote1.m2.2.2.1.1.3">𝑝</ci><apply id="footnote1.m2.2.2.1.1.1.1.1.cmml" xref="footnote1.m2.2.2.1.1.1.1"><csymbol cd="latexml" id="footnote1.m2.2.2.1.1.1.1.1.1.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.1">conditional</csymbol><apply id="footnote1.m2.2.2.1.1.1.1.1.2.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="footnote1.m2.2.2.1.1.1.1.1.2.1.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="footnote1.m2.2.2.1.1.1.1.1.2.2.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.2.2">c</ci><ci id="footnote1.m2.2.2.1.1.1.1.1.2.3.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.2.3">x</ci></apply><ci id="footnote1.m2.2.2.1.1.1.1.1.3.cmml" xref="footnote1.m2.2.2.1.1.1.1.1.3">x</ci></apply></apply><ci id="footnote1.m2.2.2.1.3.cmml" xref="footnote1.m2.2.2.1.3">𝑝</ci></apply><ci id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.2d">p(\mathrm{c_{x}}|\mathrm{x})\times p(\mathrm{x})</annotation><annotation encoding="application/x-llamapun" id="footnote1.m2.2e">italic_p ( roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT | roman_x ) × italic_p ( roman_x )</annotation></semantics></math>, we also explored this setting. We observed that the performance of the model is poor in this setting compared to the other setting. More details can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1" title="A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</span></span></span>, calculated as in Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E2" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{x}|\mathrm{c_{x}})\times p(\mathrm{c_{x%
}})" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">p</mi><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" mathvariant="normal" xref="S3.E2.m1.1.1.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.2.cmml">,</mo><msub id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.2" mathvariant="normal" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">c</mi><mi id="S3.E2.m1.2.2.1.1.1.1.3" mathvariant="normal" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.E2.m1.2.2.1.1.1.4" stretchy="false" xref="S3.E2.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml">=</mo><mrow id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml"><mrow id="S3.E2.m1.3.3.2.1" xref="S3.E2.m1.3.3.2.1.cmml"><mrow id="S3.E2.m1.3.3.2.1.1" xref="S3.E2.m1.3.3.2.1.1.cmml"><mi id="S3.E2.m1.3.3.2.1.1.3" xref="S3.E2.m1.3.3.2.1.1.3.cmml">p</mi><mo id="S3.E2.m1.3.3.2.1.1.2" xref="S3.E2.m1.3.3.2.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.2.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.2.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.3.3.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.2.1.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.2" mathvariant="normal" xref="S3.E2.m1.3.3.2.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.E2.m1.3.3.2.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E2.m1.3.3.2.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.3.2" mathvariant="normal" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.E2.m1.3.3.2.1.1.1.1.1.3.3" mathvariant="normal" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S3.E2.m1.3.3.2.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E2.m1.3.3.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.2.1.2" rspace="0.222em" xref="S3.E2.m1.3.3.2.1.2.cmml">×</mo><mi id="S3.E2.m1.3.3.2.1.3" xref="S3.E2.m1.3.3.2.1.3.cmml">p</mi></mrow><mo id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.3.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.3.2.1" xref="S3.E2.m1.4.4.3.2.1.1.cmml"><mo id="S3.E2.m1.4.4.3.2.1.2" stretchy="false" xref="S3.E2.m1.4.4.3.2.1.1.cmml">(</mo><msub id="S3.E2.m1.4.4.3.2.1.1" xref="S3.E2.m1.4.4.3.2.1.1.cmml"><mi id="S3.E2.m1.4.4.3.2.1.1.2" mathvariant="normal" xref="S3.E2.m1.4.4.3.2.1.1.2.cmml">c</mi><mi id="S3.E2.m1.4.4.3.2.1.1.3" mathvariant="normal" xref="S3.E2.m1.4.4.3.2.1.1.3.cmml">x</mi></msub><mo id="S3.E2.m1.4.4.3.2.1.3" stretchy="false" xref="S3.E2.m1.4.4.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"></eq><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><times id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></times><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">𝑝</ci><interval closure="open" id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">x</ci><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2">c</ci><ci id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">x</ci></apply></interval></apply><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"><times id="S3.E2.m1.4.4.3.3.cmml" xref="S3.E2.m1.4.4.3.3"></times><apply id="S3.E2.m1.3.3.2.1.cmml" xref="S3.E2.m1.3.3.2.1"><times id="S3.E2.m1.3.3.2.1.2.cmml" xref="S3.E2.m1.3.3.2.1.2"></times><apply id="S3.E2.m1.3.3.2.1.1.cmml" xref="S3.E2.m1.3.3.2.1.1"><times id="S3.E2.m1.3.3.2.1.1.2.cmml" xref="S3.E2.m1.3.3.2.1.1.2"></times><ci id="S3.E2.m1.3.3.2.1.1.3.cmml" xref="S3.E2.m1.3.3.2.1.1.3">𝑝</ci><apply id="S3.E2.m1.3.3.2.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.2">x</ci><apply id="S3.E2.m1.3.3.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3.2">c</ci><ci id="S3.E2.m1.3.3.2.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.2.1.1.1.1.1.3.3">x</ci></apply></apply></apply><ci id="S3.E2.m1.3.3.2.1.3.cmml" xref="S3.E2.m1.3.3.2.1.3">𝑝</ci></apply><apply id="S3.E2.m1.4.4.3.2.1.1.cmml" xref="S3.E2.m1.4.4.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.2.1.1.1.cmml" xref="S3.E2.m1.4.4.3.2.1">subscript</csymbol><ci id="S3.E2.m1.4.4.3.2.1.1.2.cmml" xref="S3.E2.m1.4.4.3.2.1.1.2">c</ci><ci id="S3.E2.m1.4.4.3.2.1.1.3.cmml" xref="S3.E2.m1.4.4.3.2.1.1.3">x</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{x}|\mathrm{c_{x}})\times p(\mathrm{c_{x%
}})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) = italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) × italic_p ( roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.3">The training data <math alttext="\mathrm{D}" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p5.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">D</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\mathrm{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">roman_D</annotation></semantics></math> consists of triplets <math alttext="\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.3"><semantics id="S3.SS1.p5.2.m2.3a"><mrow id="S3.SS1.p5.2.m2.3.3.1" xref="S3.SS1.p5.2.m2.3.3.2.cmml"><mo id="S3.SS1.p5.2.m2.3.3.1.2" stretchy="false" xref="S3.SS1.p5.2.m2.3.3.2.cmml">⟨</mo><msub id="S3.SS1.p5.2.m2.3.3.1.1" xref="S3.SS1.p5.2.m2.3.3.1.1.cmml"><mi id="S3.SS1.p5.2.m2.3.3.1.1.2" mathvariant="normal" xref="S3.SS1.p5.2.m2.3.3.1.1.2.cmml">c</mi><mi id="S3.SS1.p5.2.m2.3.3.1.1.3" mathvariant="normal" xref="S3.SS1.p5.2.m2.3.3.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.p5.2.m2.3.3.1.3" xref="S3.SS1.p5.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS1.p5.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p5.2.m2.1.1.cmml">x</mi><mo id="S3.SS1.p5.2.m2.3.3.1.4" xref="S3.SS1.p5.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS1.p5.2.m2.2.2" mathvariant="normal" xref="S3.SS1.p5.2.m2.2.2.cmml">y</mi><mo id="S3.SS1.p5.2.m2.3.3.1.5" stretchy="false" xref="S3.SS1.p5.2.m2.3.3.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.3b"><list id="S3.SS1.p5.2.m2.3.3.2.cmml" xref="S3.SS1.p5.2.m2.3.3.1"><apply id="S3.SS1.p5.2.m2.3.3.1.1.cmml" xref="S3.SS1.p5.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.p5.2.m2.3.3.1.1">subscript</csymbol><ci id="S3.SS1.p5.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.p5.2.m2.3.3.1.1.2">c</ci><ci id="S3.SS1.p5.2.m2.3.3.1.1.3.cmml" xref="S3.SS1.p5.2.m2.3.3.1.1.3">x</ci></apply><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">x</ci><ci id="S3.SS1.p5.2.m2.2.2.cmml" xref="S3.SS1.p5.2.m2.2.2">y</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.3c">\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.3d">⟨ roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT , roman_x , roman_y ⟩</annotation></semantics></math>. Given the parameters of the model <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mi id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_θ</annotation></semantics></math>, the translation (Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E1" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a>) and reconstruction (Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E2" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a>) objectives can be modeled as Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E3" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">3</span></a> and Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E4" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}};\theta)=\prod_{t=1}^{T}p(y_{t}|\mathrm{%
x},\mathrm{c_{x}},\mathrm{y}_{&lt;t};\theta)" class="ltx_Math" display="block" id="S3.E3.m1.6"><semantics id="S3.E3.m1.6a"><mrow id="S3.E3.m1.6.6" xref="S3.E3.m1.6.6.cmml"><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.cmml"><mi id="S3.E3.m1.5.5.1.3" xref="S3.E3.m1.5.5.1.3.cmml">p</mi><mo id="S3.E3.m1.5.5.1.2" xref="S3.E3.m1.5.5.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.5.5.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><mo id="S3.E3.m1.5.5.1.1.1.2" stretchy="false" xref="S3.E3.m1.5.5.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.3" mathvariant="normal" xref="S3.E3.m1.5.5.1.1.1.1.3.cmml">y</mi><mo fence="false" id="S3.E3.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml">|</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1" mathvariant="normal" xref="S3.E3.m1.1.1.cmml">x</mi><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S3.E3.m1.5.5.1.1.1.3" stretchy="false" xref="S3.E3.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.3" rspace="0.111em" xref="S3.E3.m1.6.6.3.cmml">=</mo><mrow id="S3.E3.m1.6.6.2" xref="S3.E3.m1.6.6.2.cmml"><munderover id="S3.E3.m1.6.6.2.2" xref="S3.E3.m1.6.6.2.2.cmml"><mo id="S3.E3.m1.6.6.2.2.2.2" movablelimits="false" xref="S3.E3.m1.6.6.2.2.2.2.cmml">∏</mo><mrow id="S3.E3.m1.6.6.2.2.2.3" xref="S3.E3.m1.6.6.2.2.2.3.cmml"><mi id="S3.E3.m1.6.6.2.2.2.3.2" xref="S3.E3.m1.6.6.2.2.2.3.2.cmml">t</mi><mo id="S3.E3.m1.6.6.2.2.2.3.1" xref="S3.E3.m1.6.6.2.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.6.6.2.2.2.3.3" xref="S3.E3.m1.6.6.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.6.6.2.2.3" xref="S3.E3.m1.6.6.2.2.3.cmml">T</mi></munderover><mrow id="S3.E3.m1.6.6.2.1" xref="S3.E3.m1.6.6.2.1.cmml"><mi id="S3.E3.m1.6.6.2.1.3" xref="S3.E3.m1.6.6.2.1.3.cmml">p</mi><mo id="S3.E3.m1.6.6.2.1.2" xref="S3.E3.m1.6.6.2.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.6.6.2.1.1.1" xref="S3.E3.m1.6.6.2.1.1.1.1.cmml"><mo id="S3.E3.m1.6.6.2.1.1.1.2" stretchy="false" xref="S3.E3.m1.6.6.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.6.6.2.1.1.1.1" xref="S3.E3.m1.6.6.2.1.1.1.1.cmml"><msub id="S3.E3.m1.6.6.2.1.1.1.1.4" xref="S3.E3.m1.6.6.2.1.1.1.1.4.cmml"><mi id="S3.E3.m1.6.6.2.1.1.1.1.4.2" xref="S3.E3.m1.6.6.2.1.1.1.1.4.2.cmml">y</mi><mi id="S3.E3.m1.6.6.2.1.1.1.1.4.3" xref="S3.E3.m1.6.6.2.1.1.1.1.4.3.cmml">t</mi></msub><mo fence="false" id="S3.E3.m1.6.6.2.1.1.1.1.3" xref="S3.E3.m1.6.6.2.1.1.1.1.3.cmml">|</mo><mrow id="S3.E3.m1.6.6.2.1.1.1.1.2.2" xref="S3.E3.m1.6.6.2.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.3.3" mathvariant="normal" xref="S3.E3.m1.3.3.cmml">x</mi><mo id="S3.E3.m1.6.6.2.1.1.1.1.2.2.3" xref="S3.E3.m1.6.6.2.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.E3.m1.6.6.2.1.1.1.1.2.2.4" xref="S3.E3.m1.6.6.2.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.2" mathvariant="normal" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.2.cmml">y</mi><mrow id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.2" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.2.cmml"></mi><mo id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.1" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.3" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.3.cmml">t</mi></mrow></msub><mo id="S3.E3.m1.6.6.2.1.1.1.1.2.2.5" xref="S3.E3.m1.6.6.2.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml">θ</mi></mrow></mrow><mo id="S3.E3.m1.6.6.2.1.1.1.3" stretchy="false" xref="S3.E3.m1.6.6.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.6b"><apply id="S3.E3.m1.6.6.cmml" xref="S3.E3.m1.6.6"><eq id="S3.E3.m1.6.6.3.cmml" xref="S3.E3.m1.6.6.3"></eq><apply id="S3.E3.m1.5.5.1.cmml" xref="S3.E3.m1.5.5.1"><times id="S3.E3.m1.5.5.1.2.cmml" xref="S3.E3.m1.5.5.1.2"></times><ci id="S3.E3.m1.5.5.1.3.cmml" xref="S3.E3.m1.5.5.1.3">𝑝</ci><apply id="S3.E3.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2">conditional</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3">y</ci><list id="S3.E3.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">x</ci><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2">c</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3">x</ci></apply><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝜃</ci></list></apply></apply><apply id="S3.E3.m1.6.6.2.cmml" xref="S3.E3.m1.6.6.2"><apply id="S3.E3.m1.6.6.2.2.cmml" xref="S3.E3.m1.6.6.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.2.2.1.cmml" xref="S3.E3.m1.6.6.2.2">superscript</csymbol><apply id="S3.E3.m1.6.6.2.2.2.cmml" xref="S3.E3.m1.6.6.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.2.2.2.1.cmml" xref="S3.E3.m1.6.6.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E3.m1.6.6.2.2.2.2.cmml" xref="S3.E3.m1.6.6.2.2.2.2">product</csymbol><apply id="S3.E3.m1.6.6.2.2.2.3.cmml" xref="S3.E3.m1.6.6.2.2.2.3"><eq id="S3.E3.m1.6.6.2.2.2.3.1.cmml" xref="S3.E3.m1.6.6.2.2.2.3.1"></eq><ci id="S3.E3.m1.6.6.2.2.2.3.2.cmml" xref="S3.E3.m1.6.6.2.2.2.3.2">𝑡</ci><cn id="S3.E3.m1.6.6.2.2.2.3.3.cmml" type="integer" xref="S3.E3.m1.6.6.2.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.6.6.2.2.3.cmml" xref="S3.E3.m1.6.6.2.2.3">𝑇</ci></apply><apply id="S3.E3.m1.6.6.2.1.cmml" xref="S3.E3.m1.6.6.2.1"><times id="S3.E3.m1.6.6.2.1.2.cmml" xref="S3.E3.m1.6.6.2.1.2"></times><ci id="S3.E3.m1.6.6.2.1.3.cmml" xref="S3.E3.m1.6.6.2.1.3">𝑝</ci><apply id="S3.E3.m1.6.6.2.1.1.1.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.6.6.2.1.1.1.1.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.3">conditional</csymbol><apply id="S3.E3.m1.6.6.2.1.1.1.1.4.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.2.1.1.1.1.4.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.6.6.2.1.1.1.1.4.2.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.4.2">𝑦</ci><ci id="S3.E3.m1.6.6.2.1.1.1.1.4.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.4.3">𝑡</ci></apply><list id="S3.E3.m1.6.6.2.1.1.1.1.2.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2"><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">x</ci><apply id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.2">c</ci><ci id="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.1.1.1.3">x</ci></apply><apply id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.2">y</ci><apply id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3"><lt id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.2">absent</csymbol><ci id="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E3.m1.6.6.2.1.1.1.1.2.2.2.3.3">𝑡</ci></apply></apply><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">𝜃</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.6c">p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}};\theta)=\prod_{t=1}^{T}p(y_{t}|\mathrm{%
x},\mathrm{c_{x}},\mathrm{y}_{&lt;t};\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.6d">italic_p ( roman_y | roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ; italic_θ ) = ∏ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_p ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT , roman_y start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(\mathrm{x}|\mathrm{c_{x}};\theta)=\prod_{s=1}^{S}p(x_{s}|\mathrm{c_{x}},%
\mathrm{x}_{&lt;s};\theta)" class="ltx_Math" display="block" id="S3.E4.m1.4"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.cmml"><mi id="S3.E4.m1.3.3.1.3" xref="S3.E4.m1.3.3.1.3.cmml">p</mi><mo id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.3" mathvariant="normal" xref="S3.E4.m1.3.3.1.1.1.1.3.cmml">x</mi><mo fence="false" id="S3.E4.m1.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">|</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.2.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">θ</mi></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.4.4.3" rspace="0.111em" xref="S3.E4.m1.4.4.3.cmml">=</mo><mrow id="S3.E4.m1.4.4.2" xref="S3.E4.m1.4.4.2.cmml"><munderover id="S3.E4.m1.4.4.2.2" xref="S3.E4.m1.4.4.2.2.cmml"><mo id="S3.E4.m1.4.4.2.2.2.2" movablelimits="false" xref="S3.E4.m1.4.4.2.2.2.2.cmml">∏</mo><mrow id="S3.E4.m1.4.4.2.2.2.3" xref="S3.E4.m1.4.4.2.2.2.3.cmml"><mi id="S3.E4.m1.4.4.2.2.2.3.2" xref="S3.E4.m1.4.4.2.2.2.3.2.cmml">s</mi><mo id="S3.E4.m1.4.4.2.2.2.3.1" xref="S3.E4.m1.4.4.2.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.4.4.2.2.2.3.3" xref="S3.E4.m1.4.4.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.4.4.2.2.3" xref="S3.E4.m1.4.4.2.2.3.cmml">S</mi></munderover><mrow id="S3.E4.m1.4.4.2.1" xref="S3.E4.m1.4.4.2.1.cmml"><mi id="S3.E4.m1.4.4.2.1.3" xref="S3.E4.m1.4.4.2.1.3.cmml">p</mi><mo id="S3.E4.m1.4.4.2.1.2" xref="S3.E4.m1.4.4.2.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.4.4.2.1.1.1" xref="S3.E4.m1.4.4.2.1.1.1.1.cmml"><mo id="S3.E4.m1.4.4.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.4.4.2.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.4.4.2.1.1.1.1" xref="S3.E4.m1.4.4.2.1.1.1.1.cmml"><msub id="S3.E4.m1.4.4.2.1.1.1.1.4" xref="S3.E4.m1.4.4.2.1.1.1.1.4.cmml"><mi id="S3.E4.m1.4.4.2.1.1.1.1.4.2" xref="S3.E4.m1.4.4.2.1.1.1.1.4.2.cmml">x</mi><mi id="S3.E4.m1.4.4.2.1.1.1.1.4.3" xref="S3.E4.m1.4.4.2.1.1.1.1.4.3.cmml">s</mi></msub><mo fence="false" id="S3.E4.m1.4.4.2.1.1.1.1.3" xref="S3.E4.m1.4.4.2.1.1.1.1.3.cmml">|</mo><mrow id="S3.E4.m1.4.4.2.1.1.1.1.2.2" xref="S3.E4.m1.4.4.2.1.1.1.1.2.3.cmml"><msub id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.E4.m1.4.4.2.1.1.1.1.2.2.3" xref="S3.E4.m1.4.4.2.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.2" mathvariant="normal" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.2" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.2.cmml"></mi><mo id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.1" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.3" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.3.cmml">s</mi></mrow></msub><mo id="S3.E4.m1.4.4.2.1.1.1.1.2.2.4" xref="S3.E4.m1.4.4.2.1.1.1.1.2.3.cmml">;</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S3.E4.m1.4.4.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.4.4.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4"><eq id="S3.E4.m1.4.4.3.cmml" xref="S3.E4.m1.4.4.3"></eq><apply id="S3.E4.m1.3.3.1.cmml" xref="S3.E4.m1.3.3.1"><times id="S3.E4.m1.3.3.1.2.cmml" xref="S3.E4.m1.3.3.1.2"></times><ci id="S3.E4.m1.3.3.1.3.cmml" xref="S3.E4.m1.3.3.1.3">𝑝</ci><apply id="S3.E4.m1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.2">conditional</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3">x</ci><list id="S3.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2">c</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3">x</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝜃</ci></list></apply></apply><apply id="S3.E4.m1.4.4.2.cmml" xref="S3.E4.m1.4.4.2"><apply id="S3.E4.m1.4.4.2.2.cmml" xref="S3.E4.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.1.cmml" xref="S3.E4.m1.4.4.2.2">superscript</csymbol><apply id="S3.E4.m1.4.4.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.2.1.cmml" xref="S3.E4.m1.4.4.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E4.m1.4.4.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.2">product</csymbol><apply id="S3.E4.m1.4.4.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3"><eq id="S3.E4.m1.4.4.2.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.2.2.3.1"></eq><ci id="S3.E4.m1.4.4.2.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.2.2.3.2">𝑠</ci><cn id="S3.E4.m1.4.4.2.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.4.4.2.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.4.4.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.3">𝑆</ci></apply><apply id="S3.E4.m1.4.4.2.1.cmml" xref="S3.E4.m1.4.4.2.1"><times id="S3.E4.m1.4.4.2.1.2.cmml" xref="S3.E4.m1.4.4.2.1.2"></times><ci id="S3.E4.m1.4.4.2.1.3.cmml" xref="S3.E4.m1.4.4.2.1.3">𝑝</ci><apply id="S3.E4.m1.4.4.2.1.1.1.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.4.4.2.1.1.1.1.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.3">conditional</csymbol><apply id="S3.E4.m1.4.4.2.1.1.1.1.4.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.1.1.1.1.4.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.4.4.2.1.1.1.1.4.2.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.4.2">𝑥</ci><ci id="S3.E4.m1.4.4.2.1.1.1.1.4.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.4.3">𝑠</ci></apply><list id="S3.E4.m1.4.4.2.1.1.1.1.2.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2"><apply id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.2">c</ci><ci id="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.1.1.1.3">x</ci></apply><apply id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.2">x</ci><apply id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3"><lt id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.2">absent</csymbol><ci id="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E4.m1.4.4.2.1.1.1.1.2.2.2.3.3">𝑠</ci></apply></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">p(\mathrm{x}|\mathrm{c_{x}};\theta)=\prod_{s=1}^{S}p(x_{s}|\mathrm{c_{x}},%
\mathrm{x}_{&lt;s};\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.4d">italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ; italic_θ ) = ∏ start_POSTSUBSCRIPT italic_s = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT , roman_x start_POSTSUBSCRIPT &lt; italic_s end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p7.5">where, <math alttext="\mathrm{S,Z,T}" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.3"><semantics id="S3.SS1.p7.1.m1.3a"><mrow id="S3.SS1.p7.1.m1.3.4.2" xref="S3.SS1.p7.1.m1.3.4.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p7.1.m1.1.1.cmml">S</mi><mo id="S3.SS1.p7.1.m1.3.4.2.1" xref="S3.SS1.p7.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.p7.1.m1.2.2" mathvariant="normal" xref="S3.SS1.p7.1.m1.2.2.cmml">Z</mi><mo id="S3.SS1.p7.1.m1.3.4.2.2" xref="S3.SS1.p7.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS1.p7.1.m1.3.3" mathvariant="normal" xref="S3.SS1.p7.1.m1.3.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.3b"><list id="S3.SS1.p7.1.m1.3.4.1.cmml" xref="S3.SS1.p7.1.m1.3.4.2"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">S</ci><ci id="S3.SS1.p7.1.m1.2.2.cmml" xref="S3.SS1.p7.1.m1.2.2">Z</ci><ci id="S3.SS1.p7.1.m1.3.3.cmml" xref="S3.SS1.p7.1.m1.3.3">T</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.3c">\mathrm{S,Z,T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.3d">roman_S , roman_Z , roman_T</annotation></semantics></math> denote the lengths of <math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="S3.SS1.p7.2.m2.1"><semantics id="S3.SS1.p7.2.m2.1a"><mi id="S3.SS1.p7.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p7.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><ci id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.2.m2.1d">roman_x</annotation></semantics></math>, <math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="S3.SS1.p7.3.m3.1"><semantics id="S3.SS1.p7.3.m3.1a"><msub id="S3.SS1.p7.3.m3.1.1" xref="S3.SS1.p7.3.m3.1.1.cmml"><mi id="S3.SS1.p7.3.m3.1.1.2" mathvariant="normal" xref="S3.SS1.p7.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS1.p7.3.m3.1.1.3" mathvariant="normal" xref="S3.SS1.p7.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.3.m3.1b"><apply id="S3.SS1.p7.3.m3.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.3.m3.1.1.1.cmml" xref="S3.SS1.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p7.3.m3.1.1.2.cmml" xref="S3.SS1.p7.3.m3.1.1.2">c</ci><ci id="S3.SS1.p7.3.m3.1.1.3.cmml" xref="S3.SS1.p7.3.m3.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.3.m3.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.3.m3.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathrm{y}" class="ltx_Math" display="inline" id="S3.SS1.p7.4.m4.1"><semantics id="S3.SS1.p7.4.m4.1a"><mi id="S3.SS1.p7.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p7.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.4.m4.1b"><ci id="S3.SS1.p7.4.m4.1.1.cmml" xref="S3.SS1.p7.4.m4.1.1">y</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.4.m4.1c">\mathrm{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.4.m4.1d">roman_y</annotation></semantics></math> respectively and <math alttext="\mathrm{x}_{&lt;s},\mathrm{c_{x}}_{&lt;z},y_{&lt;t}" class="ltx_Math" display="inline" id="S3.SS1.p7.5.m5.3"><semantics id="S3.SS1.p7.5.m5.3a"><mrow id="S3.SS1.p7.5.m5.3.3.3" xref="S3.SS1.p7.5.m5.3.3.4.cmml"><msub id="S3.SS1.p7.5.m5.1.1.1.1" xref="S3.SS1.p7.5.m5.1.1.1.1.cmml"><mi id="S3.SS1.p7.5.m5.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p7.5.m5.1.1.1.1.2.cmml">x</mi><mrow id="S3.SS1.p7.5.m5.1.1.1.1.3" xref="S3.SS1.p7.5.m5.1.1.1.1.3.cmml"><mi id="S3.SS1.p7.5.m5.1.1.1.1.3.2" xref="S3.SS1.p7.5.m5.1.1.1.1.3.2.cmml"></mi><mo id="S3.SS1.p7.5.m5.1.1.1.1.3.1" xref="S3.SS1.p7.5.m5.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p7.5.m5.1.1.1.1.3.3" xref="S3.SS1.p7.5.m5.1.1.1.1.3.3.cmml">s</mi></mrow></msub><mo id="S3.SS1.p7.5.m5.3.3.3.4" xref="S3.SS1.p7.5.m5.3.3.4.cmml">,</mo><mmultiscripts id="S3.SS1.p7.5.m5.2.2.2.2" xref="S3.SS1.p7.5.m5.2.2.2.2.cmml"><mi id="S3.SS1.p7.5.m5.2.2.2.2.2.2" mathvariant="normal" xref="S3.SS1.p7.5.m5.2.2.2.2.2.2.cmml">c</mi><mi id="S3.SS1.p7.5.m5.2.2.2.2.2.3" mathvariant="normal" xref="S3.SS1.p7.5.m5.2.2.2.2.2.3.cmml">x</mi><mrow id="S3.SS1.p7.5.m5.2.2.2.2a" xref="S3.SS1.p7.5.m5.2.2.2.2.cmml"></mrow><mrow id="S3.SS1.p7.5.m5.2.2.2.2.3" xref="S3.SS1.p7.5.m5.2.2.2.2.3.cmml"><mi id="S3.SS1.p7.5.m5.2.2.2.2.3.2" xref="S3.SS1.p7.5.m5.2.2.2.2.3.2.cmml"></mi><mo id="S3.SS1.p7.5.m5.2.2.2.2.3.1" xref="S3.SS1.p7.5.m5.2.2.2.2.3.1.cmml">&lt;</mo><mi id="S3.SS1.p7.5.m5.2.2.2.2.3.3" xref="S3.SS1.p7.5.m5.2.2.2.2.3.3.cmml">z</mi></mrow><mrow id="S3.SS1.p7.5.m5.2.2.2.2b" xref="S3.SS1.p7.5.m5.2.2.2.2.cmml"></mrow></mmultiscripts><mo id="S3.SS1.p7.5.m5.3.3.3.5" xref="S3.SS1.p7.5.m5.3.3.4.cmml">,</mo><msub id="S3.SS1.p7.5.m5.3.3.3.3" xref="S3.SS1.p7.5.m5.3.3.3.3.cmml"><mi id="S3.SS1.p7.5.m5.3.3.3.3.2" xref="S3.SS1.p7.5.m5.3.3.3.3.2.cmml">y</mi><mrow id="S3.SS1.p7.5.m5.3.3.3.3.3" xref="S3.SS1.p7.5.m5.3.3.3.3.3.cmml"><mi id="S3.SS1.p7.5.m5.3.3.3.3.3.2" xref="S3.SS1.p7.5.m5.3.3.3.3.3.2.cmml"></mi><mo id="S3.SS1.p7.5.m5.3.3.3.3.3.1" xref="S3.SS1.p7.5.m5.3.3.3.3.3.1.cmml">&lt;</mo><mi id="S3.SS1.p7.5.m5.3.3.3.3.3.3" xref="S3.SS1.p7.5.m5.3.3.3.3.3.3.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.5.m5.3b"><list id="S3.SS1.p7.5.m5.3.3.4.cmml" xref="S3.SS1.p7.5.m5.3.3.3"><apply id="S3.SS1.p7.5.m5.1.1.1.1.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p7.5.m5.1.1.1.1.2.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1.2">x</ci><apply id="S3.SS1.p7.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1.3"><lt id="S3.SS1.p7.5.m5.1.1.1.1.3.1.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p7.5.m5.1.1.1.1.3.2.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1.3.2">absent</csymbol><ci id="S3.SS1.p7.5.m5.1.1.1.1.3.3.cmml" xref="S3.SS1.p7.5.m5.1.1.1.1.3.3">𝑠</ci></apply></apply><apply id="S3.SS1.p7.5.m5.2.2.2.2.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p7.5.m5.2.2.2.2.1.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2">subscript</csymbol><apply id="S3.SS1.p7.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p7.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p7.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.2.2">c</ci><ci id="S3.SS1.p7.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.2.3">x</ci></apply><apply id="S3.SS1.p7.5.m5.2.2.2.2.3.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.3"><lt id="S3.SS1.p7.5.m5.2.2.2.2.3.1.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p7.5.m5.2.2.2.2.3.2.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.3.2">absent</csymbol><ci id="S3.SS1.p7.5.m5.2.2.2.2.3.3.cmml" xref="S3.SS1.p7.5.m5.2.2.2.2.3.3">𝑧</ci></apply></apply><apply id="S3.SS1.p7.5.m5.3.3.3.3.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p7.5.m5.3.3.3.3.1.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p7.5.m5.3.3.3.3.2.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3.2">𝑦</ci><apply id="S3.SS1.p7.5.m5.3.3.3.3.3.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3.3"><lt id="S3.SS1.p7.5.m5.3.3.3.3.3.1.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p7.5.m5.3.3.3.3.3.2.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3.3.2">absent</csymbol><ci id="S3.SS1.p7.5.m5.3.3.3.3.3.3.cmml" xref="S3.SS1.p7.5.m5.3.3.3.3.3.3">𝑡</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.5.m5.3c">\mathrm{x}_{&lt;s},\mathrm{c_{x}}_{&lt;z},y_{&lt;t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.5.m5.3d">roman_x start_POSTSUBSCRIPT &lt; italic_s end_POSTSUBSCRIPT , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT start_POSTSUBSCRIPT &lt; italic_z end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denote partially generated sequences.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.7">Given translation objective <math alttext="p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}})" class="ltx_Math" display="inline" id="S3.SS1.p8.1.m1.2"><semantics id="S3.SS1.p8.1.m1.2a"><mrow id="S3.SS1.p8.1.m1.2.2" xref="S3.SS1.p8.1.m1.2.2.cmml"><mi id="S3.SS1.p8.1.m1.2.2.3" xref="S3.SS1.p8.1.m1.2.2.3.cmml">p</mi><mo id="S3.SS1.p8.1.m1.2.2.2" xref="S3.SS1.p8.1.m1.2.2.2.cmml">⁢</mo><mrow id="S3.SS1.p8.1.m1.2.2.1.1" xref="S3.SS1.p8.1.m1.2.2.1.1.1.cmml"><mo id="S3.SS1.p8.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS1.p8.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p8.1.m1.2.2.1.1.1" xref="S3.SS1.p8.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.2.2.1.1.1.3" mathvariant="normal" xref="S3.SS1.p8.1.m1.2.2.1.1.1.3.cmml">y</mi><mo fence="false" id="S3.SS1.p8.1.m1.2.2.1.1.1.2" xref="S3.SS1.p8.1.m1.2.2.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.2.cmml"><mi id="S3.SS1.p8.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p8.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.2" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.3.cmml">x</mi></msub></mrow></mrow><mo id="S3.SS1.p8.1.m1.2.2.1.1.3" stretchy="false" xref="S3.SS1.p8.1.m1.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.2b"><apply id="S3.SS1.p8.1.m1.2.2.cmml" xref="S3.SS1.p8.1.m1.2.2"><times id="S3.SS1.p8.1.m1.2.2.2.cmml" xref="S3.SS1.p8.1.m1.2.2.2"></times><ci id="S3.SS1.p8.1.m1.2.2.3.cmml" xref="S3.SS1.p8.1.m1.2.2.3">𝑝</ci><apply id="S3.SS1.p8.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p8.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.2">conditional</csymbol><ci id="S3.SS1.p8.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.3">y</ci><list id="S3.SS1.p8.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1"><ci id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">x</ci><apply id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.2">c</ci><ci id="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p8.1.m1.2.2.1.1.1.1.1.1.3">x</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.2c">p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.1.m1.2d">italic_p ( roman_y | roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math> and reconstruction objective <math alttext="p(\mathrm{x}|\mathrm{c_{x}})" class="ltx_Math" display="inline" id="S3.SS1.p8.2.m2.1"><semantics id="S3.SS1.p8.2.m2.1a"><mrow id="S3.SS1.p8.2.m2.1.1" xref="S3.SS1.p8.2.m2.1.1.cmml"><mi id="S3.SS1.p8.2.m2.1.1.3" xref="S3.SS1.p8.2.m2.1.1.3.cmml">p</mi><mo id="S3.SS1.p8.2.m2.1.1.2" xref="S3.SS1.p8.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p8.2.m2.1.1.1.1" xref="S3.SS1.p8.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p8.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p8.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p8.2.m2.1.1.1.1.1" xref="S3.SS1.p8.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p8.2.m2.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p8.2.m2.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.SS1.p8.2.m2.1.1.1.1.1.1" xref="S3.SS1.p8.2.m2.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS1.p8.2.m2.1.1.1.1.1.3" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p8.2.m2.1.1.1.1.1.3.2" mathvariant="normal" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.SS1.p8.2.m2.1.1.1.1.1.3.3" mathvariant="normal" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S3.SS1.p8.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p8.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.2.m2.1b"><apply id="S3.SS1.p8.2.m2.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1"><times id="S3.SS1.p8.2.m2.1.1.2.cmml" xref="S3.SS1.p8.2.m2.1.1.2"></times><ci id="S3.SS1.p8.2.m2.1.1.3.cmml" xref="S3.SS1.p8.2.m2.1.1.3">𝑝</ci><apply id="S3.SS1.p8.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p8.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p8.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.2">x</ci><apply id="S3.SS1.p8.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p8.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p8.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3.2">c</ci><ci id="S3.SS1.p8.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p8.2.m2.1.1.1.1.1.3.3">x</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.2.m2.1c">p(\mathrm{x}|\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.2.m2.1d">italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math>, the model is jointly trained and optimized the loss, <math alttext="\mathcal{L}" class="ltx_Math" display="inline" id="S3.SS1.p8.3.m3.1"><semantics id="S3.SS1.p8.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p8.3.m3.1.1" xref="S3.SS1.p8.3.m3.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.3.m3.1b"><ci id="S3.SS1.p8.3.m3.1.1.cmml" xref="S3.SS1.p8.3.m3.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.3.m3.1c">\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.3.m3.1d">caligraphic_L</annotation></semantics></math> using parameter <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p8.4.m4.1"><semantics id="S3.SS1.p8.4.m4.1a"><mi id="S3.SS1.p8.4.m4.1.1" xref="S3.SS1.p8.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.4.m4.1b"><ci id="S3.SS1.p8.4.m4.1.1.cmml" xref="S3.SS1.p8.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.4.m4.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.4.m4.1d">italic_θ</annotation></semantics></math> (cf. Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E5" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">5</span></a>); where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p8.5.m5.1"><semantics id="S3.SS1.p8.5.m5.1a"><mi id="S3.SS1.p8.5.m5.1.1" xref="S3.SS1.p8.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.5.m5.1b"><ci id="S3.SS1.p8.5.m5.1.1.cmml" xref="S3.SS1.p8.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.5.m5.1d">italic_α</annotation></semantics></math> is a hyper-parameter used to control the loss. We set <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p8.6.m6.1"><semantics id="S3.SS1.p8.6.m6.1a"><mi id="S3.SS1.p8.6.m6.1.1" xref="S3.SS1.p8.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.6.m6.1b"><ci id="S3.SS1.p8.6.m6.1.1.cmml" xref="S3.SS1.p8.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.6.m6.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.6.m6.1d">italic_α</annotation></semantics></math> to <math alttext="0.5" class="ltx_Math" display="inline" id="S3.SS1.p8.7.m7.1"><semantics id="S3.SS1.p8.7.m7.1a"><mn id="S3.SS1.p8.7.m7.1.1" xref="S3.SS1.p8.7.m7.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.7.m7.1b"><cn id="S3.SS1.p8.7.m7.1.1.cmml" type="float" xref="S3.SS1.p8.7.m7.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.7.m7.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p8.7.m7.1d">0.5</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\mathcal{L}=\alpha*\log p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}};%
\theta)+\\
(1-\alpha)*\log p(\mathrm{x}|\mathrm{c_{x}};\theta)\end{split}" class="ltx_Math" display="block" id="S3.E5.m1.39"><semantics id="S3.E5.m1.39a"><mtable displaystyle="true" id="S3.E5.m1.39.39.6" rowspacing="0pt" xref="S3.E5.m1.36.36.3.cmml"><mtr id="S3.E5.m1.39.39.6a" xref="S3.E5.m1.36.36.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.39.39.6b" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.37.37.4.34.18.18" xref="S3.E5.m1.36.36.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">ℒ</mi><mo id="S3.E5.m1.2.2.2.2.2.2" xref="S3.E5.m1.2.2.2.2.2.2.cmml">=</mo><mrow id="S3.E5.m1.37.37.4.34.18.18.18" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1.3" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.cmml">α</mi><mo id="S3.E5.m1.4.4.4.4.4.4" lspace="0.222em" rspace="0.222em" xref="S3.E5.m1.4.4.4.4.4.4.cmml">∗</mo><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1.3.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.5.5.5.5.5.5" xref="S3.E5.m1.5.5.5.5.5.5.cmml">log</mi><mo id="S3.E5.m1.37.37.4.34.18.18.18.1.3.1a" lspace="0.167em" xref="S3.E5.m1.36.36.3.cmml">⁡</mo><mi id="S3.E5.m1.6.6.6.6.6.6" xref="S3.E5.m1.6.6.6.6.6.6.cmml">p</mi></mrow></mrow><mo id="S3.E5.m1.37.37.4.34.18.18.18.1.2" xref="S3.E5.m1.36.36.3.cmml">⁢</mo><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mo id="S3.E5.m1.7.7.7.7.7.7" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">(</mo><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.8.8.8.8.8.8" mathvariant="normal" xref="S3.E5.m1.8.8.8.8.8.8.cmml">y</mi><mo fence="false" id="S3.E5.m1.9.9.9.9.9.9" xref="S3.E5.m1.9.9.9.9.9.9.cmml">|</mo><mrow id="S3.E5.m1.37.37.4.34.18.18.18.1.1.1.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.10.10.10.10.10.10" mathvariant="normal" xref="S3.E5.m1.10.10.10.10.10.10.cmml">x</mi><mo id="S3.E5.m1.11.11.11.11.11.11" xref="S3.E5.m1.36.36.3.cmml">,</mo><msub id="S3.E5.m1.37.37.4.34.18.18.18.1.1.1.1.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.12.12.12.12.12.12" mathvariant="normal" xref="S3.E5.m1.12.12.12.12.12.12.cmml">c</mi><mi id="S3.E5.m1.13.13.13.13.13.13.1" mathvariant="normal" xref="S3.E5.m1.13.13.13.13.13.13.1.cmml">x</mi></msub><mo id="S3.E5.m1.14.14.14.14.14.14" xref="S3.E5.m1.36.36.3.cmml">;</mo><mi id="S3.E5.m1.15.15.15.15.15.15" xref="S3.E5.m1.15.15.15.15.15.15.cmml">θ</mi></mrow></mrow><mo id="S3.E5.m1.16.16.16.16.16.16" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.17.17.17.17.17.17" xref="S3.E5.m1.17.17.17.17.17.17.cmml">+</mo></mrow></mrow></mtd></mtr><mtr id="S3.E5.m1.39.39.6c" xref="S3.E5.m1.36.36.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.39.39.6d" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.39.39.6.36.18.18" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.38.38.5.35.17.17.17" xref="S3.E5.m1.36.36.3.cmml"><mrow id="S3.E5.m1.38.38.5.35.17.17.17.1.1" xref="S3.E5.m1.36.36.3.cmml"><mo id="S3.E5.m1.18.18.18.1.1.1" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">(</mo><mrow id="S3.E5.m1.38.38.5.35.17.17.17.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mn id="S3.E5.m1.19.19.19.2.2.2" xref="S3.E5.m1.19.19.19.2.2.2.cmml">1</mn><mo id="S3.E5.m1.20.20.20.3.3.3" xref="S3.E5.m1.20.20.20.3.3.3.cmml">−</mo><mi id="S3.E5.m1.21.21.21.4.4.4" xref="S3.E5.m1.21.21.21.4.4.4.cmml">α</mi></mrow><mo id="S3.E5.m1.22.22.22.5.5.5" rspace="0.055em" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">)</mo></mrow><mo id="S3.E5.m1.23.23.23.6.6.6" rspace="0.222em" xref="S3.E5.m1.23.23.23.6.6.6.cmml">∗</mo><mrow id="S3.E5.m1.38.38.5.35.17.17.17.2" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.24.24.24.7.7.7" xref="S3.E5.m1.24.24.24.7.7.7.cmml">log</mi><mo id="S3.E5.m1.38.38.5.35.17.17.17.2a" lspace="0.167em" xref="S3.E5.m1.36.36.3.cmml">⁡</mo><mi id="S3.E5.m1.25.25.25.8.8.8" xref="S3.E5.m1.25.25.25.8.8.8.cmml">p</mi></mrow></mrow><mo id="S3.E5.m1.39.39.6.36.18.18.19" xref="S3.E5.m1.36.36.3.cmml">⁢</mo><mrow id="S3.E5.m1.39.39.6.36.18.18.18.1" xref="S3.E5.m1.36.36.3.cmml"><mo id="S3.E5.m1.26.26.26.9.9.9" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">(</mo><mrow id="S3.E5.m1.39.39.6.36.18.18.18.1.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.27.27.27.10.10.10" mathvariant="normal" xref="S3.E5.m1.27.27.27.10.10.10.cmml">x</mi><mo fence="false" id="S3.E5.m1.28.28.28.11.11.11" xref="S3.E5.m1.28.28.28.11.11.11.cmml">|</mo><mrow id="S3.E5.m1.39.39.6.36.18.18.18.1.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><msub id="S3.E5.m1.39.39.6.36.18.18.18.1.1.1.1.1" xref="S3.E5.m1.36.36.3.cmml"><mi id="S3.E5.m1.29.29.29.12.12.12" mathvariant="normal" xref="S3.E5.m1.29.29.29.12.12.12.cmml">c</mi><mi id="S3.E5.m1.30.30.30.13.13.13.1" mathvariant="normal" xref="S3.E5.m1.30.30.30.13.13.13.1.cmml">x</mi></msub><mo id="S3.E5.m1.31.31.31.14.14.14" xref="S3.E5.m1.36.36.3.cmml">;</mo><mi id="S3.E5.m1.32.32.32.15.15.15" xref="S3.E5.m1.32.32.32.15.15.15.cmml">θ</mi></mrow></mrow><mo id="S3.E5.m1.33.33.33.16.16.16" stretchy="false" xref="S3.E5.m1.36.36.3.cmml">)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E5.m1.39b"><apply id="S3.E5.m1.36.36.3.cmml" xref="S3.E5.m1.39.39.6"><eq id="S3.E5.m1.2.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.2.2"></eq><ci id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">ℒ</ci><apply id="S3.E5.m1.36.36.3.3.cmml" xref="S3.E5.m1.39.39.6"><plus id="S3.E5.m1.17.17.17.17.17.17.cmml" xref="S3.E5.m1.17.17.17.17.17.17"></plus><apply id="S3.E5.m1.34.34.1.1.1.cmml" xref="S3.E5.m1.39.39.6"><times id="S3.E5.m1.34.34.1.1.1.2.cmml" xref="S3.E5.m1.39.39.6"></times><apply id="S3.E5.m1.34.34.1.1.1.3.cmml" xref="S3.E5.m1.39.39.6"><times id="S3.E5.m1.4.4.4.4.4.4.cmml" xref="S3.E5.m1.4.4.4.4.4.4"></times><ci id="S3.E5.m1.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3">𝛼</ci><apply id="S3.E5.m1.34.34.1.1.1.3.3.cmml" xref="S3.E5.m1.39.39.6"><log id="S3.E5.m1.5.5.5.5.5.5.cmml" xref="S3.E5.m1.5.5.5.5.5.5"></log><ci id="S3.E5.m1.6.6.6.6.6.6.cmml" xref="S3.E5.m1.6.6.6.6.6.6">𝑝</ci></apply></apply><apply id="S3.E5.m1.34.34.1.1.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6"><csymbol cd="latexml" id="S3.E5.m1.9.9.9.9.9.9.cmml" xref="S3.E5.m1.9.9.9.9.9.9">conditional</csymbol><ci id="S3.E5.m1.8.8.8.8.8.8.cmml" xref="S3.E5.m1.8.8.8.8.8.8">y</ci><list id="S3.E5.m1.34.34.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.39.39.6"><ci id="S3.E5.m1.10.10.10.10.10.10.cmml" xref="S3.E5.m1.10.10.10.10.10.10">x</ci><apply id="S3.E5.m1.34.34.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E5.m1.34.34.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6">subscript</csymbol><ci id="S3.E5.m1.12.12.12.12.12.12.cmml" xref="S3.E5.m1.12.12.12.12.12.12">c</ci><ci id="S3.E5.m1.13.13.13.13.13.13.1.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1">x</ci></apply><ci id="S3.E5.m1.15.15.15.15.15.15.cmml" xref="S3.E5.m1.15.15.15.15.15.15">𝜃</ci></list></apply></apply><apply id="S3.E5.m1.36.36.3.3.3.cmml" xref="S3.E5.m1.39.39.6"><times id="S3.E5.m1.36.36.3.3.3.3.cmml" xref="S3.E5.m1.39.39.6"></times><apply id="S3.E5.m1.35.35.2.2.2.1.cmml" xref="S3.E5.m1.39.39.6"><times id="S3.E5.m1.23.23.23.6.6.6.cmml" xref="S3.E5.m1.23.23.23.6.6.6"></times><apply id="S3.E5.m1.35.35.2.2.2.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6"><minus id="S3.E5.m1.20.20.20.3.3.3.cmml" xref="S3.E5.m1.20.20.20.3.3.3"></minus><cn id="S3.E5.m1.19.19.19.2.2.2.cmml" type="integer" xref="S3.E5.m1.19.19.19.2.2.2">1</cn><ci id="S3.E5.m1.21.21.21.4.4.4.cmml" xref="S3.E5.m1.21.21.21.4.4.4">𝛼</ci></apply><apply id="S3.E5.m1.35.35.2.2.2.1.3.cmml" xref="S3.E5.m1.39.39.6"><log id="S3.E5.m1.24.24.24.7.7.7.cmml" xref="S3.E5.m1.24.24.24.7.7.7"></log><ci id="S3.E5.m1.25.25.25.8.8.8.cmml" xref="S3.E5.m1.25.25.25.8.8.8">𝑝</ci></apply></apply><apply id="S3.E5.m1.36.36.3.3.3.2.1.1.cmml" xref="S3.E5.m1.39.39.6"><csymbol cd="latexml" id="S3.E5.m1.28.28.28.11.11.11.cmml" xref="S3.E5.m1.28.28.28.11.11.11">conditional</csymbol><ci id="S3.E5.m1.27.27.27.10.10.10.cmml" xref="S3.E5.m1.27.27.27.10.10.10">x</ci><list id="S3.E5.m1.36.36.3.3.3.2.1.1.1.2.cmml" xref="S3.E5.m1.39.39.6"><apply id="S3.E5.m1.36.36.3.3.3.2.1.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E5.m1.36.36.3.3.3.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.39.39.6">subscript</csymbol><ci id="S3.E5.m1.29.29.29.12.12.12.cmml" xref="S3.E5.m1.29.29.29.12.12.12">c</ci><ci id="S3.E5.m1.30.30.30.13.13.13.1.cmml" xref="S3.E5.m1.30.30.30.13.13.13.1">x</ci></apply><ci id="S3.E5.m1.32.32.32.15.15.15.cmml" xref="S3.E5.m1.32.32.32.15.15.15">𝜃</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.39c">\begin{split}\mathcal{L}=\alpha*\log p(\mathrm{y}|\mathrm{x},\mathrm{c_{x}};%
\theta)+\\
(1-\alpha)*\log p(\mathrm{x}|\mathrm{c_{x}};\theta)\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.39d">start_ROW start_CELL caligraphic_L = italic_α ∗ roman_log italic_p ( roman_y | roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ; italic_θ ) + end_CELL end_ROW start_ROW start_CELL ( 1 - italic_α ) ∗ roman_log italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ; italic_θ ) end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.1">We hypothesize that forcing the model to learn reconstruction and translation objectives jointly will enable the model to encode the context effectively. The output of the reconstruction task can verify this during testing. If the context encoder generates noise, then the model might be unable to reconstruct the source and vice-versa.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Cascade Multi-Task Learning Transformer</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The cascade multi-task learning architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx44" title="">Zhou et al., 2019</a>]</cite> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.F1" title="Figure 1 ‣ 3.2 Cascade Multi-Task Learning Transformer ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a>) consists of one encoder and two decoders based on the transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx32" title="">Vaswani et al., 2017</a>]</cite> architecture. The model takes three inputs: <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Source</span>: Current source sentence, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Context</span>: Context of the current source sentence, and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Target</span>: Current target sentence. The input to the encoder is context, and the input to the intermediate decoder is the source. The intermediate decoder is trained to reconstruct the source given context by attending to the output of the encoder. The final decoder attends over the output of the intermediate decoder. In the non-MTL setting, the model is trained only on the translation objective (output of the final decoder), and the intermediate decoder is not trained with the reconstruction objective.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="108" id="S3.F1.g1" src="x1.png" width="288"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overview of our MTL architecture. The input to the model is a triplet. The triplet consist of (<span class="ltx_text ltx_font_italic" id="S3.F1.10.1">Context</span>, <span class="ltx_text ltx_font_italic" id="S3.F1.11.2">Source</span>, <span class="ltx_text ltx_font_italic" id="S3.F1.12.3">Target</span>). The Intermediate Decoder is trained to reconstruct the <span class="ltx_text ltx_font_italic" id="S3.F1.13.4">Source</span> given <span class="ltx_text ltx_font_italic" id="S3.F1.14.5">Context</span>, and the Final Decoder is trained to translate the <span class="ltx_text ltx_font_italic" id="S3.F1.15.6">Source</span>. Here, <span class="ltx_text ltx_font_italic" id="S3.F1.16.7">Source</span>: Current source sentence, <span class="ltx_text ltx_font_italic" id="S3.F1.17.8">Context</span>: Context for the current source sentence, and <span class="ltx_text ltx_font_italic" id="S3.F1.18.9">Target</span>: Translation of current source sentence. None of the layers are shared.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Context Selection</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3">We conduct experiments on different settings of the source context. The term “source context” is defined as considering related or dependent sentences directly related to the input sentence. Based on the findings of Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx41" title="">Zhang et al., 2018</a>]</cite>, we select two sentences as context and concatenate them with a special token ‘<math alttext="\langle\mathrm{break}\rangle" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.2.2" xref="S3.SS3.p1.1.m1.1.2.1.cmml"><mo id="S3.SS3.p1.1.m1.1.2.2.1" stretchy="false" xref="S3.SS3.p1.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">break</mi><mo id="S3.SS3.p1.1.m1.1.2.2.2" stretchy="false" xref="S3.SS3.p1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.2.2"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.2.1.1.cmml" xref="S3.SS3.p1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">break</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\langle\mathrm{break}\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">⟨ roman_break ⟩</annotation></semantics></math>’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite>. For a given input source sentence (<math alttext="\mathrm{x_{i}}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" mathvariant="normal" xref="S3.SS3.p1.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.2.m2.1.1.3" mathvariant="normal" xref="S3.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">x</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">i</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathrm{x_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">roman_x start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT</annotation></semantics></math>) and target sentence (<math alttext="\mathrm{y_{i}}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" mathvariant="normal" xref="S3.SS3.p1.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS3.p1.3.m3.1.1.3" mathvariant="normal" xref="S3.SS3.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">y</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">i</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathrm{y_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">roman_y start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT</annotation></semantics></math>), contexts selected for the experiments are:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Previous-2 Source (<span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">P@2-SRC</span>): Two previous source sentences (<math alttext="\mathrm{x_{i-2},x_{i-1}}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.2"><semantics id="S3.I1.i1.p1.1.m1.2a"><mrow id="S3.I1.i1.p1.1.m1.2.2.2" xref="S3.I1.i1.p1.1.m1.2.2.3.cmml"><msub id="S3.I1.i1.p1.1.m1.1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.1.1.2" mathvariant="normal" xref="S3.I1.i1.p1.1.m1.1.1.1.1.2.cmml">x</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.1.1.3.2" mathvariant="normal" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.I1.i1.p1.1.m1.1.1.1.1.3.1" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.1.m1.1.1.1.1.3.3" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.3.cmml">2</mn></mrow></msub><mo id="S3.I1.i1.p1.1.m1.2.2.2.3" xref="S3.I1.i1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.I1.i1.p1.1.m1.2.2.2.2" xref="S3.I1.i1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.I1.i1.p1.1.m1.2.2.2.2.2" mathvariant="normal" xref="S3.I1.i1.p1.1.m1.2.2.2.2.2.cmml">x</mi><mrow id="S3.I1.i1.p1.1.m1.2.2.2.2.3" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.cmml"><mi id="S3.I1.i1.p1.1.m1.2.2.2.2.3.2" mathvariant="normal" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.2.cmml">i</mi><mo id="S3.I1.i1.p1.1.m1.2.2.2.2.3.1" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.1.cmml">−</mo><mn id="S3.I1.i1.p1.1.m1.2.2.2.2.3.3" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.2b"><list id="S3.I1.i1.p1.1.m1.2.2.3.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2"><apply id="S3.I1.i1.p1.1.m1.1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1.2">x</ci><apply id="S3.I1.i1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3"><minus id="S3.I1.i1.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.1"></minus><ci id="S3.I1.i1.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.2">i</ci><cn id="S3.I1.i1.p1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.I1.i1.p1.1.m1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.I1.i1.p1.1.m1.2.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2.2">x</ci><apply id="S3.I1.i1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3"><minus id="S3.I1.i1.p1.1.m1.2.2.2.2.3.1.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.1"></minus><ci id="S3.I1.i1.p1.1.m1.2.2.2.2.3.2.cmml" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.2">i</ci><cn id="S3.I1.i1.p1.1.m1.2.2.2.2.3.3.cmml" type="integer" xref="S3.I1.i1.p1.1.m1.2.2.2.2.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.2c">\mathrm{x_{i-2},x_{i-1}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.2d">roman_x start_POSTSUBSCRIPT roman_i - 2 end_POSTSUBSCRIPT , roman_x start_POSTSUBSCRIPT roman_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Previous-2 Target (<span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">P@2-TGT</span>): Two previous target sentences (<math alttext="\mathrm{y_{i-2},y_{i-1}}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.2"><semantics id="S3.I1.i2.p1.1.m1.2a"><mrow id="S3.I1.i2.p1.1.m1.2.2.2" xref="S3.I1.i2.p1.1.m1.2.2.3.cmml"><msub id="S3.I1.i2.p1.1.m1.1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.1.1.2" mathvariant="normal" xref="S3.I1.i2.p1.1.m1.1.1.1.1.2.cmml">y</mi><mrow id="S3.I1.i2.p1.1.m1.1.1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.1.1.3.2" mathvariant="normal" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.I1.i2.p1.1.m1.1.1.1.1.3.1" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.I1.i2.p1.1.m1.1.1.1.1.3.3" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.3.cmml">2</mn></mrow></msub><mo id="S3.I1.i2.p1.1.m1.2.2.2.3" xref="S3.I1.i2.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.I1.i2.p1.1.m1.2.2.2.2" xref="S3.I1.i2.p1.1.m1.2.2.2.2.cmml"><mi id="S3.I1.i2.p1.1.m1.2.2.2.2.2" mathvariant="normal" xref="S3.I1.i2.p1.1.m1.2.2.2.2.2.cmml">y</mi><mrow id="S3.I1.i2.p1.1.m1.2.2.2.2.3" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.cmml"><mi id="S3.I1.i2.p1.1.m1.2.2.2.2.3.2" mathvariant="normal" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.2.cmml">i</mi><mo id="S3.I1.i2.p1.1.m1.2.2.2.2.3.1" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.1.cmml">−</mo><mn id="S3.I1.i2.p1.1.m1.2.2.2.2.3.3" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.2b"><list id="S3.I1.i2.p1.1.m1.2.2.3.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2"><apply id="S3.I1.i2.p1.1.m1.1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1.2">y</ci><apply id="S3.I1.i2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3"><minus id="S3.I1.i2.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.1"></minus><ci id="S3.I1.i2.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.2">i</ci><cn id="S3.I1.i2.p1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.I1.i2.p1.1.m1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.I1.i2.p1.1.m1.2.2.2.2.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.2.2.2.2.1.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2.2">y</ci><apply id="S3.I1.i2.p1.1.m1.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3"><minus id="S3.I1.i2.p1.1.m1.2.2.2.2.3.1.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.1"></minus><ci id="S3.I1.i2.p1.1.m1.2.2.2.2.3.2.cmml" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.2">i</ci><cn id="S3.I1.i2.p1.1.m1.2.2.2.2.3.3.cmml" type="integer" xref="S3.I1.i2.p1.1.m1.2.2.2.2.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.2c">\mathrm{y_{i-2},y_{i-1}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.2d">roman_y start_POSTSUBSCRIPT roman_i - 2 end_POSTSUBSCRIPT , roman_y start_POSTSUBSCRIPT roman_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Previous-Next Source (<span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">P-N-SRC</span>): Previous and next source sentences (<math alttext="\mathrm{x_{i-1},x_{i+1}}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.2"><semantics id="S3.I1.i3.p1.1.m1.2a"><mrow id="S3.I1.i3.p1.1.m1.2.2.2" xref="S3.I1.i3.p1.1.m1.2.2.3.cmml"><msub id="S3.I1.i3.p1.1.m1.1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.1.1.2" mathvariant="normal" xref="S3.I1.i3.p1.1.m1.1.1.1.1.2.cmml">x</mi><mrow id="S3.I1.i3.p1.1.m1.1.1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.1.1.3.2" mathvariant="normal" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.I1.i3.p1.1.m1.1.1.1.1.3.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.I1.i3.p1.1.m1.1.1.1.1.3.3" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.I1.i3.p1.1.m1.2.2.2.3" xref="S3.I1.i3.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.I1.i3.p1.1.m1.2.2.2.2" xref="S3.I1.i3.p1.1.m1.2.2.2.2.cmml"><mi id="S3.I1.i3.p1.1.m1.2.2.2.2.2" mathvariant="normal" xref="S3.I1.i3.p1.1.m1.2.2.2.2.2.cmml">x</mi><mrow id="S3.I1.i3.p1.1.m1.2.2.2.2.3" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.cmml"><mi id="S3.I1.i3.p1.1.m1.2.2.2.2.3.2" mathvariant="normal" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.2.cmml">i</mi><mo id="S3.I1.i3.p1.1.m1.2.2.2.2.3.1" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.1.cmml">+</mo><mn id="S3.I1.i3.p1.1.m1.2.2.2.2.3.3" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><list id="S3.I1.i3.p1.1.m1.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2"><apply id="S3.I1.i3.p1.1.m1.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.2">x</ci><apply id="S3.I1.i3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3"><minus id="S3.I1.i3.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.1"></minus><ci id="S3.I1.i3.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.2">i</ci><cn id="S3.I1.i3.p1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.I1.i3.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.I1.i3.p1.1.m1.2.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.2.2.2.2.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2.2">x</ci><apply id="S3.I1.i3.p1.1.m1.2.2.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3"><plus id="S3.I1.i3.p1.1.m1.2.2.2.2.3.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.1"></plus><ci id="S3.I1.i3.p1.1.m1.2.2.2.2.3.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.2">i</ci><cn id="S3.I1.i3.p1.1.m1.2.2.2.2.3.3.cmml" type="integer" xref="S3.I1.i3.p1.1.m1.2.2.2.2.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">\mathrm{x_{i-1},x_{i+1}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.2d">roman_x start_POSTSUBSCRIPT roman_i - 1 end_POSTSUBSCRIPT , roman_x start_POSTSUBSCRIPT roman_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math>)</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment Setup</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We train our models with the proposed cascade MTL approach. The model is trained on <math alttext="\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle" class="ltx_Math" display="inline" id="S4.p1.1.m1.3"><semantics id="S4.p1.1.m1.3a"><mrow id="S4.p1.1.m1.3.3.1" xref="S4.p1.1.m1.3.3.2.cmml"><mo id="S4.p1.1.m1.3.3.1.2" stretchy="false" xref="S4.p1.1.m1.3.3.2.cmml">⟨</mo><msub id="S4.p1.1.m1.3.3.1.1" xref="S4.p1.1.m1.3.3.1.1.cmml"><mi id="S4.p1.1.m1.3.3.1.1.2" mathvariant="normal" xref="S4.p1.1.m1.3.3.1.1.2.cmml">c</mi><mi id="S4.p1.1.m1.3.3.1.1.3" mathvariant="normal" xref="S4.p1.1.m1.3.3.1.1.3.cmml">x</mi></msub><mo id="S4.p1.1.m1.3.3.1.3" xref="S4.p1.1.m1.3.3.2.cmml">,</mo><mi id="S4.p1.1.m1.1.1" mathvariant="normal" xref="S4.p1.1.m1.1.1.cmml">x</mi><mo id="S4.p1.1.m1.3.3.1.4" xref="S4.p1.1.m1.3.3.2.cmml">,</mo><mi id="S4.p1.1.m1.2.2" mathvariant="normal" xref="S4.p1.1.m1.2.2.cmml">y</mi><mo id="S4.p1.1.m1.3.3.1.5" stretchy="false" xref="S4.p1.1.m1.3.3.2.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.3b"><list id="S4.p1.1.m1.3.3.2.cmml" xref="S4.p1.1.m1.3.3.1"><apply id="S4.p1.1.m1.3.3.1.1.cmml" xref="S4.p1.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.3.3.1.1.1.cmml" xref="S4.p1.1.m1.3.3.1.1">subscript</csymbol><ci id="S4.p1.1.m1.3.3.1.1.2.cmml" xref="S4.p1.1.m1.3.3.1.1.2">c</ci><ci id="S4.p1.1.m1.3.3.1.1.3.cmml" xref="S4.p1.1.m1.3.3.1.1.3">x</ci></apply><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">x</ci><ci id="S4.p1.1.m1.2.2.cmml" xref="S4.p1.1.m1.2.2">y</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.3c">\langle\mathrm{c_{x}},\mathrm{x},\mathrm{y}\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.3d">⟨ roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT , roman_x , roman_y ⟩</annotation></semantics></math> triplet to jointly optimize both translation and source reconstruction objectives (Figure: <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.F1" title="Figure 1 ‣ 3.2 Cascade Multi-Task Learning Transformer ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a>). We also train three other contrastive models to show the effect of context in the MTL setting.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Vanilla-Sent:</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">A vanilla sentence-level baseline model is trained without context on a single encoder-decoder network.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Concat-Context:</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.2">This model is trained on a single encoder-decoder network where context is concatenated with the source <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx31" title="">Tiedemann and Scherrer, 2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx1" title="">Agrawal et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx10" title="">Junczys-Dowmunt, 2019</a>]</cite> and fed to the encoder as input. In this setting, sentences within the context are concatenated with a unique token, ‘<math alttext="\langle\mathrm{break}\rangle" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px2.p1.1.m1.1.2.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.2.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.1.m1.1.2.2.1" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.2.1.1.cmml">⟨</mo><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">break</mi><mo id="S4.SS0.SSS0.Px2.p1.1.m1.1.2.2.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p1.1.m1.1.2.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.1.m1.1.2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">break</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">\langle\mathrm{break}\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">⟨ roman_break ⟩</annotation></semantics></math>’. The context and the source are concatenated with another special symbol, ‘<math alttext="\langle\mathrm{concat}\rangle" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p1.2.m2.1.2.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.2.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.2.m2.1.2.2.1" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.2.1.1.cmml">⟨</mo><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">concat</mi><mo id="S4.SS0.SSS0.Px2.p1.2.m2.1.2.2.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.2.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.2.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.2.2"><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.2.m2.1.2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.2.2.1">delimited-⟨⟩</csymbol><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1">concat</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">\langle\mathrm{concat}\rangle</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">⟨ roman_concat ⟩</annotation></semantics></math>’. The special symbol helps the model to distinguish between context and source sentences.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Inside-Context:</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">We re-implemented the ‘Inside-Context’ model proposed by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite>, a multi-encoder approach. This model consists of two encoders and one decoder. The decoder is modified to include two cross-attention layers to attend over the outputs of both encoders before passing through the position-wise feed-forward layer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx32" title="">Vaswani et al., 2017</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Statistics</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct experiments on WMT news-commentary, IWSLT‘17 TED, and Europarl-v7 German-English corpora. For the WMT news-commentary, we use news-commentary v14 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx4" title="">Barrault et al., 2019</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://data.statmt.org/news-commentary/v14/training/</span></span></span></span> as the train set, newstest2017 as the validation set, and newstest2018 as the test set. For IWSLT‘17 TED and Europarl-v7 corpora, we follow the train, validation, and test set splits mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx24" title="">Maruf et al., 2019</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/sameenmaruf/selective-attn/tree/master/data</span></span></span></span>. All models are trained on German to English. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.T1" title="Table 1 ‣ 4.1 Data Statistics ‣ 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a> shows data statistics of the train, validation, and test sets.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:124.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(91.8pt,-26.4pt) scale(1.73389781665714,1.73389781665714) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.2.1"># Sent</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.3.1"># Doc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.1">News</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.2">329,000/3,004/2,998</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.3">8,462/130/122</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.3.2.1">TED</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.2">206,112/8,967/2,271</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.3">1,698/93/23</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.4.3.1">Europarl</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.4.3.2">1,666,904/3,587/5,134</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.4.3.3">117,855/240/360</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data statistics for our experiments. <span class="ltx_text ltx_font_bold" id="S4.T1.4.1"># Sent</span>, <span class="ltx_text ltx_font_bold" id="S4.T1.5.2"># Doc</span> represent the number of sentences and documents, respectively. The numbers are shown in the Train/Validation/Test set order.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.4.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.2.1">News</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.3.4.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.4.1">TED</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.3.4.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.6.1">Europarl</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.1">s-BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.2">d-BLEU</th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T2.3.5.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.4">s-BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.5">d-BLEU</th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T2.3.5.2.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.7">s-BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.5.2.8">d-BLEU</th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.3.6.3.1">Vanilla-Sent</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.2">18.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.3">20.9</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.5">19.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.6">24.9</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.8">32.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.3.6.3.9">35.1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.7.1.1">Concat-Context: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.2">18.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.3">20.5</td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.7.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.5">17.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.6">22.4</td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.7.1.7"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.8">32.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.7.1.9">35.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.8.2.1">Concat-Context: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.2">18.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.3">20.7</td>
<td class="ltx_td" id="S4.T2.3.8.2.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.5">17.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.6">22.5</td>
<td class="ltx_td" id="S4.T2.3.8.2.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.8">32.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.8.2.9">35.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.9.3.1">Concat-Context: P@2-TGT</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.2">14.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.3">17.2</td>
<td class="ltx_td" id="S4.T2.3.9.3.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.5">15.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.6">20.4</td>
<td class="ltx_td" id="S4.T2.3.9.3.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.8"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.3.8.1">36.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.9.3.9"><span class="ltx_text ltx_font_bold" id="S4.T2.3.9.3.9.1">39.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.10.4.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.2">19.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.3">21.7</td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.10.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.5">20.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.6">24.8</td>
<td class="ltx_td ltx_border_t" id="S4.T2.3.10.4.7"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.8">29.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.10.4.9">32.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.3">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1"><math alttext="\textbf{20.1}^{{\dagger}}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2a.cmml">20.1</mtext><mo id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2a.cmml" xref="S4.T2.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">20.1</mtext></ci><ci id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\textbf{20.1}^{{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">20.1 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.1">22.5</span></td>
<td class="ltx_td" id="S4.T2.2.2.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6">20.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7">25.2</td>
<td class="ltx_td" id="S4.T2.2.2.8"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2"><math alttext="32.5^{{\dagger}}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><msup id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml"><mn id="S4.T2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.m1.1.1.2.cmml">32.5</mn><mo id="S4.T2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">superscript</csymbol><cn id="S4.T2.2.2.2.m1.1.1.2.cmml" type="float" xref="S4.T2.2.2.2.m1.1.1.2">32.5</cn><ci id="S4.T2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">32.5^{{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">32.5 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9">35.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.3.3.2">MTL: P@2-TGT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3">19.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.4">21.7</td>
<td class="ltx_td ltx_border_bb" id="S4.T2.3.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.1"><math alttext="\textbf{20.7}^{{\dagger}}" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><msup id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T2.3.3.1.m1.1.1.2" xref="S4.T2.3.3.1.m1.1.1.2a.cmml">20.7</mtext><mo id="S4.T2.3.3.1.m1.1.1.3" xref="S4.T2.3.3.1.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><apply id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">superscript</csymbol><ci id="S4.T2.3.3.1.m1.1.1.2a.cmml" xref="S4.T2.3.3.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.T2.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.1.m1.1.1.2">20.7</mtext></ci><ci id="S4.T2.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\textbf{20.7}^{{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">20.7 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.6.1">25.4</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T2.3.3.7"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.8">28.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.9">31.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>BLEU scores of Vanilla-Sent, Concat-Context, and proposed MTL DocNMT models trained with different source contexts for German to English direction on News-commentary v14, IWSTL-17 TED, and Europarl corpora. <span class="ltx_text ltx_font_bold" id="S4.T2.10.1">s-BLEU</span> and <span class="ltx_text ltx_font_bold" id="S4.T2.11.2">d-BLEU</span> represent sentence-level and document-level BLEU respectively. The best results are shown in bold. ‘<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S4.T2.6.m1.1"><semantics id="S4.T2.6.m1.1b"><mo id="S4.T2.6.m1.1.1" xref="S4.T2.6.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.m1.1c"><ci id="S4.T2.6.m1.1.1.cmml" xref="S4.T2.6.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m1.1d">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m1.1e">†</annotation></semantics></math>’ denotes the statistically significant results than Vanilla-Sent and Concat-Context models with <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.T2.7.m2.1"><semantics id="S4.T2.7.m2.1b"><mrow id="S4.T2.7.m2.1.1" xref="S4.T2.7.m2.1.1.cmml"><mi id="S4.T2.7.m2.1.1.2" xref="S4.T2.7.m2.1.1.2.cmml">p</mi><mo id="S4.T2.7.m2.1.1.1" xref="S4.T2.7.m2.1.1.1.cmml">&lt;</mo><mn id="S4.T2.7.m2.1.1.3" xref="S4.T2.7.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.m2.1c"><apply id="S4.T2.7.m2.1.1.cmml" xref="S4.T2.7.m2.1.1"><lt id="S4.T2.7.m2.1.1.1.cmml" xref="S4.T2.7.m2.1.1.1"></lt><ci id="S4.T2.7.m2.1.1.2.cmml" xref="S4.T2.7.m2.1.1.2">𝑝</ci><cn id="S4.T2.7.m2.1.1.3.cmml" type="float" xref="S4.T2.7.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.m2.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.m2.1e">italic_p &lt; 0.05</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>NMT Model Setups</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We conduct all the experiments on transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx32" title="">Vaswani et al., 2017</a>]</cite>. All the models are implemented in PyTorch<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://pytorch.org/</span></span></span></span>. We use 6-layer encoder-decoder stacks with 8 attention heads. Positional token embedding sizes are set to 512, and the feed-forward layer consists of 2048 cells. Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx13" title="">Kingma and Ba, 2015</a>]</cite> is used for training with a noam learning rate scheduler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx32" title="">Vaswani et al., 2017</a>]</cite> with an initial learning rate of 0.2. We use warmup steps of 16,000 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx28" title="">Popel and Bojar, 2018</a>]</cite>, and dropout is set to 0.1. Due to the GPU memory restrictions, we use a mini-batch of 40 sentences for the models trained on News and TED corpora and 25 for the models trained on Europarl corpus. We create joint subword vocabularies of size 32k for each training corpus. We use the BPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx30" title="">Sennrich et al., 2016</a>]</cite> to create subword vocabularies with SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx15" title="">Kudo and Richardson, 2018</a>]</cite> implementation. We also learn the positional encoding of tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx6" title="">Devlin et al., 2019</a>]</cite>, and the maximum sequence length is set to 140 tokens for all models and 160 for <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Concat-Context</span> models.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">All the models are trained till convergence. We use the perplexity of the validation set as an early stopping criterion with the patience of 10 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx28" title="">Popel and Bojar, 2018</a>]</cite>. We report results on the best model checkpoint saved during the training. We perform beam search during inference with beam size 4 and length penalty of 0.6 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx37" title="">Wu et al., 2016</a>]</cite>. For DocNMT models, we use the same source context with which the models are trained. Since the input to the intermediate decoder (source sentence) is also given during the testing phase, the representation of the intermediate decoder can be calculated in parallel, similar to the training phase.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">All the experiments are conducted on a single Nvidia GTX 2080ti GPU. The number of parameters and training time of the models is as follows: <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">Vanilla-Sent</span>: 76M, 76.5 hours, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">Concat-Context</span>: 76M, 81 hours, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">Inside-Context</span>: 118M, 125 hours and proposed <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.4">MTL</span>: 130M, 160 hours. The parameters and training times are approximately the same for all the corpora.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section discusses the results of the trained models and the context’s effect on Multi-Encoder and MTL settings. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.T2" title="Table 2 ‣ 4.1 Data Statistics ‣ 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a> shows the sentence-BLEU (s-BLEU) and document-BLEU (d-BLEU) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx20" title="">Liu et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx3" title="">Bao et al., 2021</a>]</cite> scores of the proposed multi-task learning model along with the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Vanilla-Sent</span> and <span class="ltx_text ltx_font_italic" id="S5.p1.1.2">Concat-Context</span> models.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We report all models’ BLEU scores on German <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" stretchy="false" xref="S5.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">→</annotation></semantics></math> English direction, calculated with sacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx29" title="">Post, 2018</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Results of MTL and Contrastive Models</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We report the BLEU scores of the models on German <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">→</annotation></semantics></math> English direction, calculated with sacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx29" title="">Post, 2018</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>sacreBLEU signature:“nrefs:1<math alttext="|" class="ltx_Math" display="inline" id="footnote5.m1.1"><semantics id="footnote5.m1.1b"><mo fence="false" id="footnote5.m1.1.1" stretchy="false" xref="footnote5.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="footnote5.m1.1c"><ci id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.1d">|</annotation><annotation encoding="application/x-llamapun" id="footnote5.m1.1e">|</annotation></semantics></math>case:mixed<math alttext="|" class="ltx_Math" display="inline" id="footnote5.m2.1"><semantics id="footnote5.m2.1b"><mo fence="false" id="footnote5.m2.1.1" stretchy="false" xref="footnote5.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="footnote5.m2.1c"><ci id="footnote5.m2.1.1.cmml" xref="footnote5.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m2.1d">|</annotation><annotation encoding="application/x-llamapun" id="footnote5.m2.1e">|</annotation></semantics></math>eff:no<math alttext="|" class="ltx_Math" display="inline" id="footnote5.m3.1"><semantics id="footnote5.m3.1b"><mo fence="false" id="footnote5.m3.1.1" stretchy="false" xref="footnote5.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="footnote5.m3.1c"><ci id="footnote5.m3.1.1.cmml" xref="footnote5.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m3.1d">|</annotation><annotation encoding="application/x-llamapun" id="footnote5.m3.1e">|</annotation></semantics></math>tok:13a<math alttext="|" class="ltx_Math" display="inline" id="footnote5.m4.1"><semantics id="footnote5.m4.1b"><mo fence="false" id="footnote5.m4.1.1" stretchy="false" xref="footnote5.m4.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="footnote5.m4.1c"><ci id="footnote5.m4.1.1.cmml" xref="footnote5.m4.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m4.1d">|</annotation><annotation encoding="application/x-llamapun" id="footnote5.m4.1e">|</annotation></semantics></math>
<br class="ltx_break"/>smooth:exp<math alttext="|" class="ltx_Math" display="inline" id="footnote5.m5.1"><semantics id="footnote5.m5.1b"><mo fence="false" id="footnote5.m5.1.1" stretchy="false" xref="footnote5.m5.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="footnote5.m5.1c"><ci id="footnote5.m5.1.1.cmml" xref="footnote5.m5.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m5.1d">|</annotation><annotation encoding="application/x-llamapun" id="footnote5.m5.1e">|</annotation></semantics></math>version:2.3.1”</span></span></span>. The proposed MTL model can outperform both <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">Vanilla-Sent</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">Concat-Context</span> models by achieving s-BLEU scores of 20.1 (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">MTL: P-N-SRC</span>) and 20.7 (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.4">MTL: P@2-TGT</span>) with an improvement of +1.8 and +0.8 BLEU improvement for News and TED corpora respectively. However, in the case of the Europarl data set, <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.5">Concat-Context</span> models outperform both <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.6">Vanilla-Sent</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.7">MTL</span> models. This shows that the <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.8">Concat-Context</span> model requires more data to perform well, unlike the MTL models, which can also work effectively in low-resource settings. We observe that the performance of the models is almost uniform across the three different context settings with a maximum BLEU difference of +1.0 (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.9">P-N-SRC</span> vs. <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.10">P@2-SRC</span>) on News, +0.5 (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.11">P@2-TGT</span> vs. <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.12">P@2-SRC</span>) on TED and +4.3 (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.13">P-N-SRC</span> vs <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.14">P@2-TGT</span>) on Europarl corpora respectively.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We also report d-BLEU (document-level BLEU) scores <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx20" title="">Liu et al., 2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx3" title="">Bao et al., 2021</a>]</cite> by converting each document into one single sequence (paragraph) by concatenating all sentences from that document and calculate BLEU scores on the resulting corpus. This results in slightly higher scores than the sentence level by matching n-grams over the whole document instead of at the sentence level. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S4.T2" title="Table 2 ‣ 4.1 Data Statistics ‣ 4 Experiment Setup ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a> also shows d-BLEU scores. Like s-BLEU scores, proposed MTL models achieve the best d-BLEU scores of 22.5 and 25.4 for News and TED corpora, respectively. We report the paired bootstrap resampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx14" title="">Koehn, 2004</a>]</cite> results, calculated with sacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx29" title="">Post, 2018</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">News</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">TED</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">Europarl</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">1.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.3">1.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.4">4.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.2.1">MTL: P@2-TGT</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.2">1.2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.3">1.6</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.4">3.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.4.3.1">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3.2">1.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3.3">1.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3.4">3.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>s-BLEU scores for the reconstruction objective of the MTL models on test set for News, TED, and Europarl corpora.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Analysis of Reconstruction Objective</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We analyze the performance of the MTL model on the reconstruction objective on the test set to verify if the context encoder is generating noise. If the context encoder generates noise by the suboptimal encoding of context, the intermediate decoder will fail to reconstruct the source sentence from the context; otherwise, the intermediate decoder can reconstruct the source sentence to a similar extent as the final translated sentence. We perform greedy decoding on the intermediate decoder to generate the source from the context. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.T3" title="Table 3 ‣ 5.1 Results of MTL and Contrastive Models ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">3</span></a> shows the BLEU scores of the reconstruction objective on the test set for News, TED, and Europarl corpora. The results show that the MTL models fail to reconstruct the source from the context. Based on this, we conclude that the context encoder cannot encode the context, leading to poor reconstruction performance of the models. However, we hypothesize that the model cannot reconstruct the source from the context because the corpora used to train context-aware models might not be context-aware. This observation aligns with the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx12" title="">Kim et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite>, and with enough data, vanilla sentence-level NMT models can outperform the document-level NMT models.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="115" id="S5.F2.g1" src="extracted/5708283/inside_attn.png" width="178"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview of the Inside-Context model. The input to the model is a triplet consisting of (<span class="ltx_text ltx_font_italic" id="S5.F2.6.1">Context, Source, Target</span>). The multi-head attention layer of the decoder is modified to attend to both the context encoders (<math alttext="\mathrm{Encoder_{c}}" class="ltx_Math" display="inline" id="S5.F2.3.m1.1"><semantics id="S5.F2.3.m1.1b"><msub id="S5.F2.3.m1.1.1" xref="S5.F2.3.m1.1.1.cmml"><mi id="S5.F2.3.m1.1.1.2" xref="S5.F2.3.m1.1.1.2.cmml">Encoder</mi><mi id="S5.F2.3.m1.1.1.3" mathvariant="normal" xref="S5.F2.3.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F2.3.m1.1c"><apply id="S5.F2.3.m1.1.1.cmml" xref="S5.F2.3.m1.1.1"><csymbol cd="ambiguous" id="S5.F2.3.m1.1.1.1.cmml" xref="S5.F2.3.m1.1.1">subscript</csymbol><ci id="S5.F2.3.m1.1.1.2.cmml" xref="S5.F2.3.m1.1.1.2">Encoder</ci><ci id="S5.F2.3.m1.1.1.3.cmml" xref="S5.F2.3.m1.1.1.3">c</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.3.m1.1d">\mathrm{Encoder_{c}}</annotation><annotation encoding="application/x-llamapun" id="S5.F2.3.m1.1e">roman_Encoder start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT</annotation></semantics></math>) and the source encoder (<math alttext="\mathrm{Encoder_{s}}" class="ltx_Math" display="inline" id="S5.F2.4.m2.1"><semantics id="S5.F2.4.m2.1b"><msub id="S5.F2.4.m2.1.1" xref="S5.F2.4.m2.1.1.cmml"><mi id="S5.F2.4.m2.1.1.2" xref="S5.F2.4.m2.1.1.2.cmml">Encoder</mi><mi id="S5.F2.4.m2.1.1.3" mathvariant="normal" xref="S5.F2.4.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F2.4.m2.1c"><apply id="S5.F2.4.m2.1.1.cmml" xref="S5.F2.4.m2.1.1"><csymbol cd="ambiguous" id="S5.F2.4.m2.1.1.1.cmml" xref="S5.F2.4.m2.1.1">subscript</csymbol><ci id="S5.F2.4.m2.1.1.2.cmml" xref="S5.F2.4.m2.1.1.2">Encoder</ci><ci id="S5.F2.4.m2.1.1.3.cmml" xref="S5.F2.4.m2.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.4.m2.1d">\mathrm{Encoder_{s}}</annotation><annotation encoding="application/x-llamapun" id="S5.F2.4.m2.1e">roman_Encoder start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT</annotation></semantics></math>).</figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:433.6pt;height:215.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(90.1pt,-44.8pt) scale(1.71043657069557,1.71043657069557) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T4.3.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.4.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.4.1.2.1">News</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.3.4.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.4.1.3.1">TED</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.3.4.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.4.1.4.1">Europarl</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.3.3.5.2.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.2.2">19.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.2.3">20.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.5.2.4">29.5</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.2.2.2.3">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.1.1"><math alttext="\textbf{20.1}^{\dagger}" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><msup id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.T4.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.m1.1.1.2a.cmml">20.1</mtext><mo id="S5.T4.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S5.T4.1.1.1.1.m1.1.1.2a.cmml" xref="S5.T4.1.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.T4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.m1.1.1.2">20.1</mtext></ci><ci id="S5.T4.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\textbf{20.1}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">20.1 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.2.4">20.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.2.2">32.5<sup class="ltx_sup" id="S5.T4.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.2.2.2.2.1.1">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.3.3.2">MTL: P@2-TGT</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3.3">19.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3.1"><math alttext="\textbf{20.7}^{\dagger}" class="ltx_Math" display="inline" id="S5.T4.3.3.3.1.m1.1"><semantics id="S5.T4.3.3.3.1.m1.1a"><msup id="S5.T4.3.3.3.1.m1.1.1" xref="S5.T4.3.3.3.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.T4.3.3.3.1.m1.1.1.2" xref="S5.T4.3.3.3.1.m1.1.1.2a.cmml">20.7</mtext><mo id="S5.T4.3.3.3.1.m1.1.1.3" xref="S5.T4.3.3.3.1.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><apply id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.3.3.3.1.m1.1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1">superscript</csymbol><ci id="S5.T4.3.3.3.1.m1.1.1.2a.cmml" xref="S5.T4.3.3.3.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.T4.3.3.3.1.m1.1.1.2.cmml" xref="S5.T4.3.3.3.1.m1.1.1.2">20.7</mtext></ci><ci id="S5.T4.3.3.3.1.m1.1.1.3.cmml" xref="S5.T4.3.3.3.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\textbf{20.7}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.1.m1.1d">20.7 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3.4">28.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.3.3.6.3.1">Inside-Context: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.2">18.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.3">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.4">33.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.3.7.4.1">Inside-Context: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.4.2">19.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.4.3">19.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.7.4.4">33.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.3.3.8.5.1">Inside-Context: P@2-TGT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.8.5.2">18.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.8.5.3">20.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.8.5.4.1">33.6</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of s-BLEU scores of MTL and Inside-Context Multi-Encoder models. The best results are shown in bold. ‘<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S5.T4.6.m1.1"><semantics id="S5.T4.6.m1.1b"><mo id="S5.T4.6.m1.1.1" xref="S5.T4.6.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.m1.1c"><ci id="S5.T4.6.m1.1.1.cmml" xref="S5.T4.6.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.m1.1d">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.m1.1e">†</annotation></semantics></math>’ denotes the statistically significant results than Vanilla-Sent and Concat-Context models with <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.T4.7.m2.1"><semantics id="S5.T4.7.m2.1b"><mrow id="S5.T4.7.m2.1.1" xref="S5.T4.7.m2.1.1.cmml"><mi id="S5.T4.7.m2.1.1.2" xref="S5.T4.7.m2.1.1.2.cmml">p</mi><mo id="S5.T4.7.m2.1.1.1" xref="S5.T4.7.m2.1.1.1.cmml">&lt;</mo><mn id="S5.T4.7.m2.1.1.3" xref="S5.T4.7.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.7.m2.1c"><apply id="S5.T4.7.m2.1.1.cmml" xref="S5.T4.7.m2.1.1"><lt id="S5.T4.7.m2.1.1.1.cmml" xref="S5.T4.7.m2.1.1.1"></lt><ci id="S5.T4.7.m2.1.1.2.cmml" xref="S5.T4.7.m2.1.1.2">𝑝</ci><cn id="S5.T4.7.m2.1.1.3.cmml" type="float" xref="S5.T4.7.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.m2.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.m2.1e">italic_p &lt; 0.05</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>MTL vs. Multi-Encoder Approach</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We compare the proposed MTL approach to the existing Multi-Encoder approach to study how the model will perform in a single-task setting. Specifically, we compare our MTL approach (single-encoder multi-decoder network) with <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">Inside-Context</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> architecture. This model consists of two transformer encoders and one transformer decoder. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.F2" title="Figure 2 ‣ 5.2 Analysis of Reconstruction Objective ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a> shows the model’s architecture. The decoder is modified to attend to the outputs of both encoders. The model follows the transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx32" title="">Vaswani et al., 2017</a>]</cite> architecture. An element-wise addition is performed on the outputs of both cross-attention layers before passing through layer-norm and position-wise feed-forward layers. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.T4" title="Table 4 ‣ 5.2 Analysis of Reconstruction Objective ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">4</span></a> shows the s-BLEU scores of the MTL and Inside-Context models. We observe that the performance of multi-encoder models is similar to MTL models, with MTL models achieving +1.1 (P-N-SRC models), +0.3 (P@2-TGT models) BLEU points improvement over Inside-Context models for News and TED corpora respectively. In the case of Europarl, inside-context models achieve better performance than the MTL models, with the P@2-TGT model achieving +5.4 BLEU points improvement compared to the MTL model. Based on the results, we conclude that the MTL setting is more effective for low-resource scenarios.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:433.6pt;height:179pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.2pt,-26.5pt) scale(1.42073213470981,1.42073213470981) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.2.1">News</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.3.1">TED</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.4.1">Europarl</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.1.2.1.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.2">1.2 (-17.9)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.3">0.8 (-19.4)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.4">4.5 (-25.0)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.3.2.1">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.2">1.2 (-18.9)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.3">0.8 (-19.5)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.4">4.0 (-28.5)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.4.3.1">MTL: P@2-TGT</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.2">0.5 (-18.7)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.3">0.3 (-20.4)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.4">3.9 (-24.3)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.1.5.4.1">Inside-Context: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.2">18.7 (-0.1)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.3">19.4 (-0.2)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.4">33.2 (0.0)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.6.5.1">Inside-Context: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.6.5.2">18.9 (-0.1)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.6.5.3">19.8 (0.0)</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.6.5.4">33.2 (0.0)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.1.7.6.1">Inside-Context: P@2-TGT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.7.6.2">18.3 (0.0)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.7.6.3">20.3 (-0.1)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.7.6.4">33.1 (-0.5)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of s-BLEU scores of MTL models tested with random context. The difference in scores over the models trained with the selected context is shown inside the parentheses.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Effect of Context in MTL setting</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Since the BLEU scores of our MTL models are almost the same for all three context settings, we check whether the MTL models are affected by the choice of context. To this end, we test the MTL models with random context. Here, random context denotes two randomly selected sentences from the entire corpus. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.T5" title="Table 5 ‣ 5.3 MTL vs. Multi-Encoder Approach ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results of MTL and Inside-Context models tested with random context. Results show that the MTL models fail to translate source sentences when the context is random. However, Inside-Context models are agnostic to context as models can translate well even if the context is random. Our findings in the case of multi-encoder models are in line with the findings of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite>. Based on the results, we conclude that MTL models are sensitive to the choice of context. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1.SSS1" title="A.1.1 Effect of Random Context ‣ A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">A.1.1</span></a> describes a similar experiment where the MTL models are tested with random context. However, the architecture used in the main experiments differs slightly from the one used in the preliminary investigation. We observe that feeding the Intermediate Decoder output to the Final Decoder makes the model sensitive to the choice of context (cf. Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.F1" title="Figure 1 ‣ 3.2 Cascade Multi-Task Learning Transformer ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.F3" title="Figure 3 ‣ A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">3</span></a> in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1" title="A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">A.1</span></a>). We hypothesize that a weighted combination of the Context Encoder output and Intermediate Decoder output is desired as it performs slightly better than the model used in the main experimental setup. However, it also makes the model agnostic to the choice of context. We plan to explore this behaviour in detail in our future work.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.3" style="width:433.6pt;height:117.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(51.3pt,-13.9pt) scale(1.30958810149744,1.30958810149744) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T6.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.2.1">News</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.3.1">TED</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T6.3.1.1.1.4.1">Europarl</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.3.1.2.1.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.2">13.7 (+12.5)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.3">11.2 (+10.4)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.2.1.4">22.3 (+17.8)</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.1.3.2.1">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T6.3.1.3.2.2">14.5 (+13.3)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.3.1.3.2.3">11.3 (+10.5)</td>
<td class="ltx_td ltx_align_center" id="S5.T6.3.1.3.2.4">19.7 (+15.7)</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.3.1.4.3.1">Inside-Context: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.4.3.2">18.7 (0.0)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.4.3.3">19.6 (+0.2)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.3.1.4.3.4">33.1 (-0.1)</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.3.1.5.4.1">Inside-Context: P-N-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.5.4.2">19.0 (+0.1)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.5.4.3">19.7 (-0.1)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.3.1.5.4.4">33.0 (-0.2)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>s-BLEU scores of the MTL and Inside-Context models are tested by giving the same source sentences as context and input. The change of s-BLEU scores over the models tested with random context is shown in (<math alttext="\pm x" class="ltx_Math" display="inline" id="S5.T6.2.m1.1"><semantics id="S5.T6.2.m1.1b"><mrow id="S5.T6.2.m1.1.1" xref="S5.T6.2.m1.1.1.cmml"><mo id="S5.T6.2.m1.1.1b" xref="S5.T6.2.m1.1.1.cmml">±</mo><mi id="S5.T6.2.m1.1.1.2" xref="S5.T6.2.m1.1.1.2.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T6.2.m1.1c"><apply id="S5.T6.2.m1.1.1.cmml" xref="S5.T6.2.m1.1.1"><csymbol cd="latexml" id="S5.T6.2.m1.1.1.1.cmml" xref="S5.T6.2.m1.1.1">plus-or-minus</csymbol><ci id="S5.T6.2.m1.1.1.2.cmml" xref="S5.T6.2.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.m1.1d">\pm x</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.m1.1e">± italic_x</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Results of MTL and Multi-Encoder models without Context</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We conduct experiments on MTL and Inside-Context models by using the same source sentence as the context. Since the proposed MTL models fail when tested with random context (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.SS4" title="5.4 Effect of Context in MTL setting ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">5.4</span></a>), we observe how the MTL and Multi-Encoder models are performing when the same source sentence is given as context. This setting presents a scenario where the context is not random but also not the type of context with which the models are trained. We conduct experiments for <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">P@2-SRC</span> and <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">P-N-SRC</span> context settings only as the <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.3">P@2-TGT</span> context setting requires the current target sentence, which is unavailable during testing. We observe that MTL models can perform well compared to the random context setting, which shows that the MTL models are sensitive to the choice of context. The performance of Inside-Context models is almost the same as those tested with random context. This shows that the Inside-Context model is agnostic to the choice of the context. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.T6" title="Table 6 ‣ 5.4 Effect of Context in MTL setting ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">6</span></a> shows the s-BLEU scores of the MTL and Inside-Context models.</p>
</div>
<figure class="ltx_table" id="S5.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T7.1" style="width:433.6pt;height:240.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(86.9pt,-48.1pt) scale(1.66874728740747,1.66874728740747) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T7.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T7.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.2.1">News</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.3.1">TED</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.4.1">Europarl</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.2.2.1">Vanilla-Sent</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.2">40.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.3">31.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2.4">37.22</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.3.3.1">Concat-Context: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.2">39.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.3">30.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.3.3.4">36.42</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.4.4.1">Concat-Context: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.2">39.99</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.3">29.57</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.4.4.4">36.78</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.5.5.1">Concat-Context: P@2-TGT</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.2">38.50</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.3">28.82</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.5.4"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.5.4.1">37.27</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T7.1.1.6.6.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.6.2">40.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.6.3">31.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.6.4">35.96</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T7.1.1.7.7.1">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.2">40.50</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.3">31.24</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.7.4">36.94</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T7.1.1.8.8.1">MTL: P@2-TGT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.2.1">40.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.8.3.1">31.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.8.8.4">33.91</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Accuracy of Pronoun Translation (APT) scores. The best results are shown in bold.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Pronoun Translation Accuracy</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">We also evaluate our proposed models’ performance on pronoun translation accuracy. We calculate the pronoun translation accuracy with APT (accuracy of pronoun translation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx26" title="">Miculicich Werlen and Popescu-Belis, 2017</a>]</cite> metric<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/idiap/APT</span></span></span></span>. This metric requires a list of pronouns from the source language (German) with a list of pronouns from the target language (English) as an optional argument. We use spaCy<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://spacy.io/models</span></span></span></span> to tag both source and target sentences from the test set and extract pronouns. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S5.T7" title="Table 7 ‣ 5.5 Results of MTL and Multi-Encoder models without Context ‣ 5 Results and Analysis ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">7</span></a> shows the APT scores of <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.1">Vanilla-Sent</span>, <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.2">Concat-Context</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.3">MTL</span> DocNMT models. The APT scores correlate with the s-BLEU and d-BLEU scores, achieving the highest APT score of 40.99 in <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.4">MTL: P@2-TGT</span> setting with an improvement of +0.82 over <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.5">Vanilla-Sent</span> and +1.0 over <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.6">Concat-Context</span> (<span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.7">P-N-SRC</span>) models on News corpus. Similarly, the <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.8">MTL: P@2-TGT</span> model achieves the highest APT score of 31.90 with an improvement of +0.68 and +1.89 over <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.9">Vanilla-Sent</span> and <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.10">Concat-Context</span> (<span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.11">P@2-SRC</span>) on TED. For the Europarl corpus, <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.12">Concat-Context</span> (<span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.13">P@2-TGT</span>) achieved the highest APT score of 37.27 with an improvement of +0.05 and +0.33 over <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.14">Vanilla-Sent</span> and <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.15">MTL</span> (<span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.16">P-N-SRC</span>) models respectively.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work explored the MTL approach for document-level NMT (DocNMT). Our proposed MTL approach is based on cascade MTL architecture, where the model consists of one encoder (for context encoding) and two decoders (for the representation of the current source and target sentences). Reconstruction of the source sentence given the context is considered the auxiliary task, along with the translation of the current source sentence as the main task. We conducted experiments for German–English for News-commentary v14, IWSLT‘17 TED, and Europarl v7 corpora with three different types of contexts <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">viz.</span> two previous sources, two previous targets, and previous-next source sentences with respect to the current input source sentence.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our proposed MTL approaches outperform the sentence-level baseline and concatenated-context models in low-resource (for News and TED corpora) settings. However, all models perform well in the high resource setting (Europarl corpus), with proposed MTL models slightly underperforming the rest. Our MTL models are more sensitive to the choice of context than the multi-encoder models when tested with random context. We observe that the context encoder cannot encode context sufficiently and performs poorly reconstruction tasks. Finally, we reported APT (accuracy of pronoun translation) scores, and the proposed MTL models outperformed the sentence-level baseline and concatenated-context models. Our empirical analysis concludes that our approach is more sensitive to the choice of context and improves the overall translation performance in low-resource context-aware settings. We plan to explore other tasks, such as gap sentence generation (GSG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx42" title="">Zhang et al., 2020a</a>]</cite> as an auxiliary task for better context encoding, different training curricula to prioritize one objective over the other during the training, and dynamic context selection.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our study poses two main limitations. First, our primary motivation is to understand the effect of context and if the context encoding can be modelled as an auxiliary task but not to propose a model to achieve state-of-the-art results. We have followed the findings of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite> and used one of their approach to understanding the effect of context. Our observations are also in line with their findings.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Second, even though our proposed MTL approach can outperform the models in other settings, the auxiliary task (reconstruction) is not very effective as it improves the BLEU scores in the range of [0.1-1.8] over the Multi-encoder models. We hypothesize that, in the loss function, we are giving equal weights to both the objectives (0.5 for both reconstruction and translation objectives), which might lead to significantly less improvement in overall translation quality. We plan to explore different training curricula to adjust the weight of the objectives dynamically during the training.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We gratefully acknowledge the support from the “NLTM: VIDYAAPATI” project, sponsored by Electronics and IT, Ministry of Electronics and Information Technology (MeiTY), Government of India. Santanu Pal acknowledges the support from Wipro AI. We also thank the anonymous reviewers for their insightful comments.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Agrawal et al., 2018] </span>
<span class="ltx_bibblock">
Agrawal, Ruchit Rajeshkumar, Marco Turchi, and Matteo Negri.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Contextual handling in neural machine translation: Look behind, ahead and on both sides.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">21st Annual Conference of the European Association for Machine Translation</span>, pages 11–20.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Anastasopoulos and Chiang, 2018] </span>
<span class="ltx_bibblock">
Anastasopoulos, Antonios and David Chiang.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Tied multitask learning for neural speech translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 82–91, New Orleans, Louisiana, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Bao et al., 2021] </span>
<span class="ltx_bibblock">
Bao, Guangsheng, Yue Zhang, Zhiyang Teng, Boxing Chen, and Weihua Luo.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">G-transformer for document-level machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 3442–3455, Online, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Barrault et al., 2019] </span>
<span class="ltx_bibblock">
Barrault, Loïc, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Findings of the 2019 conference on machine translation (WMT19).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</span>, pages 1–61, Florence, Italy, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Bawden et al., 2018] </span>
<span class="ltx_bibblock">
Bawden, Rachel, Rico Sennrich, Alexandra Birch, and Barry Haddow.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Evaluating discourse phenomena in neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 1304–1313, New Orleans, Louisiana, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al., 2019] </span>
<span class="ltx_bibblock">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx6.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Donato et al., 2021] </span>
<span class="ltx_bibblock">
Donato, Domenic, Lei Yu, and Chris Dyer.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Diverse pretrained context encodings improve document translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 1299–1311, Online, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Dong et al., 2015] </span>
<span class="ltx_bibblock">
Dong, Daxiang, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Multi-task learning for multiple language translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 1723–1732, Beijing, China, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Huo et al., 2020] </span>
<span class="ltx_bibblock">
Huo, Jingjing, Christian Herold, Yingbo Gao, Leonard Dahlmann, Shahram Khadivi, and Hermann Ney.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Diving deep into context-aware neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">Proceedings of the Fifth Conference on Machine Translation</span>, pages 604–616, Online, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Junczys-Dowmunt, 2019] </span>
<span class="ltx_bibblock">
Junczys-Dowmunt, Marcin.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Microsoft translator at WMT 2019: Towards large-scale document-level neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</span>, pages 225–233, Florence, Italy, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Kang et al., 2020] </span>
<span class="ltx_bibblock">
Kang, Xiaomian, Yang Zhao, Jiajun Zhang, and Chengqing Zong.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Dynamic context selection for document-level neural machine translation via reinforcement learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 2242–2254, Online, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Kim et al., 2019] </span>
<span class="ltx_bibblock">
Kim, Yunsu, Duc Thanh Tran, and Hermann Ney.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">When and why is document-level context useful in neural machine translation?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)</span>, pages 24–34, Hong Kong, China, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Kingma and Ba, 2015] </span>
<span class="ltx_bibblock">
Kingma, Diederik P. and Jimmy Ba.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In Bengio, Yoshua and Yann LeCun, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Koehn, 2004] </span>
<span class="ltx_bibblock">
Koehn, Philipp.

</span>
<span class="ltx_bibblock">2004.

</span>
<span class="ltx_bibblock">Statistical significance tests for machine translation evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</span>, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Kudo and Richardson, 2018] </span>
<span class="ltx_bibblock">
Kudo, Taku and John Richardson.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</span>, pages 66–71, Brussels, Belgium, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Kudo, 2018] </span>
<span class="ltx_bibblock">
Kudo, Taku.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Subword regularization: Improving neural network translation models with multiple subword candidates.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 66–75, Melbourne, Australia, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Lei et al., 2022] </span>
<span class="ltx_bibblock">
Lei, Yikun, Yuqi Ren, and Deyi Xiong.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">CoDoNMT: Modeling cohesion devices for document-level neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Proceedings of the 29th International Conference on Computational Linguistics</span>, pages 5205–5216, Gyeongju, Republic of Korea, October. International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2020] </span>
<span class="ltx_bibblock">
Li, Bei, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, and Changliang Li.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Does multi-encoder help? a case study on context-aware neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 3512–3518, Online, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Liang et al., 2022] </span>
<span class="ltx_bibblock">
Liang, Yunlong, Fandong Meng, Jinan Xu, Yufeng Chen, and Jie Zhou.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Scheduled multi-task learning for neural chat translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 4375–4388, Dublin, Ireland, May. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Liu et al., 2020] </span>
<span class="ltx_bibblock">
Liu, Yinhan, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Multilingual denoising pre-training for neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Transactions of the Association for Computational Linguistics</span>, 8:726–742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Luong et al., 2015] </span>
<span class="ltx_bibblock">
Luong, Minh-Thang, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Multi-task sequence to sequence learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">arXiv preprint arXiv:1511.06114</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Ma et al., 2020] </span>
<span class="ltx_bibblock">
Ma, Shuming, Dongdong Zhang, and Ming Zhou.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">A simple and effective unified encoder for document-level machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 3505–3511, Online, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[Maruf and Haffari, 2018] </span>
<span class="ltx_bibblock">
Maruf, Sameen and Gholamreza Haffari.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Document context neural machine translation with memory networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 1275–1284, Melbourne, Australia, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Maruf et al., 2019] </span>
<span class="ltx_bibblock">
Maruf, Sameen, André F. T. Martins, and Gholamreza Haffari.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Selective attention for context-aware neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 3092–3102, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Miculicich et al., 2018] </span>
<span class="ltx_bibblock">
Miculicich, Lesly, Dhananjay Ram, Nikolaos Pappas, and James Henderson.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Document-level neural machine translation with hierarchical attention networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 2947–2954, Brussels, Belgium, October-November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Miculicich Werlen and Popescu-Belis, 2017] </span>
<span class="ltx_bibblock">
Miculicich Werlen, Lesly and Andrei Popescu-Belis.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Validation of an automatic metric for the accuracy of pronoun translation (APT).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Proceedings of the Third Workshop on Discourse in Machine Translation</span>, pages 17–25, Copenhagen, Denmark, September. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Papineni et al., 2002] </span>
<span class="ltx_bibblock">
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span>, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Popel and Bojar, 2018] </span>
<span class="ltx_bibblock">
Popel, Martin and Ondřej Bojar.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Training tips for the transformer model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">arXiv preprint arXiv:1804.00247</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Post, 2018] </span>
<span class="ltx_bibblock">
Post, Matt.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 186–191, Brussels, Belgium, October. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Sennrich et al., 2016] </span>
<span class="ltx_bibblock">
Sennrich, Rico, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Neural machine translation of rare words with subword units.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 1715–1725, Berlin, Germany, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Tiedemann and Scherrer, 2017] </span>
<span class="ltx_bibblock">
Tiedemann, Jörg and Yves Scherrer.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Neural machine translation with extended context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">Proceedings of the Third Workshop on Discourse in Machine Translation</span>, pages 82–92, Copenhagen, Denmark, September. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Advances in neural information processing systems</span>, pages 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[Voita et al., 2018] </span>
<span class="ltx_bibblock">
Voita, Elena, Pavel Serdyukov, Rico Sennrich, and Ivan Titov.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Context-aware neural machine translation learns anaphora resolution.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx33.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 1264–1274, Melbourne, Australia, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[Voita et al., 2019] </span>
<span class="ltx_bibblock">
Voita, Elena, Rico Sennrich, and Ivan Titov.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 1198–1212, Florence, Italy, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al., 2020] </span>
<span class="ltx_bibblock">
Wang, Yiren, ChengXiang Zhai, and Hany Hassan.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Multi-task learning for multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 1022–1034, Online, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al., 2021] </span>
<span class="ltx_bibblock">
Wang, Tao, Chengqi Zhao, Mingxuan Wang, Lei Li, and Deyi Xiong.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Autocorrect in the process of translation — multi-task learning improves dialogue machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers</span>, pages 105–112, Online, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[Wu et al., 2016] </span>
<span class="ltx_bibblock">
Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx37.1.1">CoRR</span>, abs/1609.08144.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al., 2020] </span>
<span class="ltx_bibblock">
Yang, Jiacheng, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Weinan Zhang, Yong Yu, and Lei Li.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Towards making the most of bert in neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx38.1.1">Proceedings of the AAAI conference on artificial intelligence</span>, volume 34, pages 9378–9385.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[Zaremoodi and Haffari, 2018] </span>
<span class="ltx_bibblock">
Zaremoodi, Poorya and Gholamreza Haffari.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Neural machine translation for bilingually scarce scenarios: a deep multi-task learning approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx39.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</span>, pages 1356–1365, New Orleans, Louisiana, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[Zaremoodi et al., 2018] </span>
<span class="ltx_bibblock">
Zaremoodi, Poorya, Wray Buntine, and Gholamreza Haffari.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Adaptive knowledge sharing in multi-task learning: Improving low-resource neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx40.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 656–661, Melbourne, Australia, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2018] </span>
<span class="ltx_bibblock">
Zhang, Jiacheng, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Improving the transformer translation model with document-level context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx41.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</span>, pages 533–542, Brussels, Belgium, October-November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2020a] </span>
<span class="ltx_bibblock">
Zhang, Jingqing, Yao Zhao, Mohammad Saleh, and Peter Liu.

</span>
<span class="ltx_bibblock">2020a.

</span>
<span class="ltx_bibblock">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx42.1.1">International Conference on Machine Learning</span>, pages 11328–11339. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 2020b] </span>
<span class="ltx_bibblock">
Zhang, Pei, Boxing Chen, Niyu Ge, and Kai Fan.

</span>
<span class="ltx_bibblock">2020b.

</span>
<span class="ltx_bibblock">Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx43.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 1081–1087, Online, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[Zhou et al., 2019] </span>
<span class="ltx_bibblock">
Zhou, Shuyan, Xiangkai Zeng, Yingqi Zhou, Antonios Anastasopoulos, and Graham Neubig.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Improving robustness of neural machine translation with multi-task learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx44.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</span>, pages 565–571, Florence, Italy, August. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Preliminary Investigation on Auxiliary Objectives</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The joint probability in Equation <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.E2" title="In 3.1 Problem Statement ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">2</span></a> (<math alttext="p(\mathrm{x},\mathrm{c_{x}}" class="ltx_math_unparsed" display="inline" id="A1.SS1.p1.1.m1.1"><semantics id="A1.SS1.p1.1.m1.1a"><mrow id="A1.SS1.p1.1.m1.1b"><mi id="A1.SS1.p1.1.m1.1.2">p</mi><mrow id="A1.SS1.p1.1.m1.1.3"><mo id="A1.SS1.p1.1.m1.1.3.1" stretchy="false">(</mo><mi id="A1.SS1.p1.1.m1.1.1" mathvariant="normal">x</mi><mo id="A1.SS1.p1.1.m1.1.3.2">,</mo><msub id="A1.SS1.p1.1.m1.1.3.3"><mi id="A1.SS1.p1.1.m1.1.3.3.2" mathvariant="normal">c</mi><mi id="A1.SS1.p1.1.m1.1.3.3.3" mathvariant="normal">x</mi></msub></mrow></mrow><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">p(\mathrm{x},\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.m1.1d">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>)) can be calculated in two ways such as:</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A1.EGx1">
<tbody id="A1.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{x}|\mathrm{c_{x}})\times p%
(\mathrm{c_{x}})" class="ltx_Math" display="inline" id="A1.E6.m1.4"><semantics id="A1.E6.m1.4a"><mrow id="A1.E6.m1.4.4" xref="A1.E6.m1.4.4.cmml"><mrow id="A1.E6.m1.2.2.1" xref="A1.E6.m1.2.2.1.cmml"><mi id="A1.E6.m1.2.2.1.3" xref="A1.E6.m1.2.2.1.3.cmml">p</mi><mo id="A1.E6.m1.2.2.1.2" xref="A1.E6.m1.2.2.1.2.cmml">⁢</mo><mrow id="A1.E6.m1.2.2.1.1.1" xref="A1.E6.m1.2.2.1.1.2.cmml"><mo id="A1.E6.m1.2.2.1.1.1.2" stretchy="false" xref="A1.E6.m1.2.2.1.1.2.cmml">(</mo><mi id="A1.E6.m1.1.1" mathvariant="normal" xref="A1.E6.m1.1.1.cmml">x</mi><mo id="A1.E6.m1.2.2.1.1.1.3" xref="A1.E6.m1.2.2.1.1.2.cmml">,</mo><msub id="A1.E6.m1.2.2.1.1.1.1" xref="A1.E6.m1.2.2.1.1.1.1.cmml"><mi id="A1.E6.m1.2.2.1.1.1.1.2" mathvariant="normal" xref="A1.E6.m1.2.2.1.1.1.1.2.cmml">c</mi><mi id="A1.E6.m1.2.2.1.1.1.1.3" mathvariant="normal" xref="A1.E6.m1.2.2.1.1.1.1.3.cmml">x</mi></msub><mo id="A1.E6.m1.2.2.1.1.1.4" stretchy="false" xref="A1.E6.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="A1.E6.m1.4.4.4" xref="A1.E6.m1.4.4.4.cmml">=</mo><mrow id="A1.E6.m1.4.4.3" xref="A1.E6.m1.4.4.3.cmml"><mrow id="A1.E6.m1.3.3.2.1" xref="A1.E6.m1.3.3.2.1.cmml"><mrow id="A1.E6.m1.3.3.2.1.1" xref="A1.E6.m1.3.3.2.1.1.cmml"><mi id="A1.E6.m1.3.3.2.1.1.3" xref="A1.E6.m1.3.3.2.1.1.3.cmml">p</mi><mo id="A1.E6.m1.3.3.2.1.1.2" xref="A1.E6.m1.3.3.2.1.1.2.cmml">⁢</mo><mrow id="A1.E6.m1.3.3.2.1.1.1.1" xref="A1.E6.m1.3.3.2.1.1.1.1.1.cmml"><mo id="A1.E6.m1.3.3.2.1.1.1.1.2" stretchy="false" xref="A1.E6.m1.3.3.2.1.1.1.1.1.cmml">(</mo><mrow id="A1.E6.m1.3.3.2.1.1.1.1.1" xref="A1.E6.m1.3.3.2.1.1.1.1.1.cmml"><mi id="A1.E6.m1.3.3.2.1.1.1.1.1.2" mathvariant="normal" xref="A1.E6.m1.3.3.2.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="A1.E6.m1.3.3.2.1.1.1.1.1.1" xref="A1.E6.m1.3.3.2.1.1.1.1.1.1.cmml">|</mo><msub id="A1.E6.m1.3.3.2.1.1.1.1.1.3" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3.cmml"><mi id="A1.E6.m1.3.3.2.1.1.1.1.1.3.2" mathvariant="normal" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3.2.cmml">c</mi><mi id="A1.E6.m1.3.3.2.1.1.1.1.1.3.3" mathvariant="normal" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="A1.E6.m1.3.3.2.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="A1.E6.m1.3.3.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.E6.m1.3.3.2.1.2" rspace="0.222em" xref="A1.E6.m1.3.3.2.1.2.cmml">×</mo><mi id="A1.E6.m1.3.3.2.1.3" xref="A1.E6.m1.3.3.2.1.3.cmml">p</mi></mrow><mo id="A1.E6.m1.4.4.3.3" xref="A1.E6.m1.4.4.3.3.cmml">⁢</mo><mrow id="A1.E6.m1.4.4.3.2.1" xref="A1.E6.m1.4.4.3.2.1.1.cmml"><mo id="A1.E6.m1.4.4.3.2.1.2" stretchy="false" xref="A1.E6.m1.4.4.3.2.1.1.cmml">(</mo><msub id="A1.E6.m1.4.4.3.2.1.1" xref="A1.E6.m1.4.4.3.2.1.1.cmml"><mi id="A1.E6.m1.4.4.3.2.1.1.2" mathvariant="normal" xref="A1.E6.m1.4.4.3.2.1.1.2.cmml">c</mi><mi id="A1.E6.m1.4.4.3.2.1.1.3" mathvariant="normal" xref="A1.E6.m1.4.4.3.2.1.1.3.cmml">x</mi></msub><mo id="A1.E6.m1.4.4.3.2.1.3" stretchy="false" xref="A1.E6.m1.4.4.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.E6.m1.4b"><apply id="A1.E6.m1.4.4.cmml" xref="A1.E6.m1.4.4"><eq id="A1.E6.m1.4.4.4.cmml" xref="A1.E6.m1.4.4.4"></eq><apply id="A1.E6.m1.2.2.1.cmml" xref="A1.E6.m1.2.2.1"><times id="A1.E6.m1.2.2.1.2.cmml" xref="A1.E6.m1.2.2.1.2"></times><ci id="A1.E6.m1.2.2.1.3.cmml" xref="A1.E6.m1.2.2.1.3">𝑝</ci><interval closure="open" id="A1.E6.m1.2.2.1.1.2.cmml" xref="A1.E6.m1.2.2.1.1.1"><ci id="A1.E6.m1.1.1.cmml" xref="A1.E6.m1.1.1">x</ci><apply id="A1.E6.m1.2.2.1.1.1.1.cmml" xref="A1.E6.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A1.E6.m1.2.2.1.1.1.1.1.cmml" xref="A1.E6.m1.2.2.1.1.1.1">subscript</csymbol><ci id="A1.E6.m1.2.2.1.1.1.1.2.cmml" xref="A1.E6.m1.2.2.1.1.1.1.2">c</ci><ci id="A1.E6.m1.2.2.1.1.1.1.3.cmml" xref="A1.E6.m1.2.2.1.1.1.1.3">x</ci></apply></interval></apply><apply id="A1.E6.m1.4.4.3.cmml" xref="A1.E6.m1.4.4.3"><times id="A1.E6.m1.4.4.3.3.cmml" xref="A1.E6.m1.4.4.3.3"></times><apply id="A1.E6.m1.3.3.2.1.cmml" xref="A1.E6.m1.3.3.2.1"><times id="A1.E6.m1.3.3.2.1.2.cmml" xref="A1.E6.m1.3.3.2.1.2"></times><apply id="A1.E6.m1.3.3.2.1.1.cmml" xref="A1.E6.m1.3.3.2.1.1"><times id="A1.E6.m1.3.3.2.1.1.2.cmml" xref="A1.E6.m1.3.3.2.1.1.2"></times><ci id="A1.E6.m1.3.3.2.1.1.3.cmml" xref="A1.E6.m1.3.3.2.1.1.3">𝑝</ci><apply id="A1.E6.m1.3.3.2.1.1.1.1.1.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1"><csymbol cd="latexml" id="A1.E6.m1.3.3.2.1.1.1.1.1.1.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.1">conditional</csymbol><ci id="A1.E6.m1.3.3.2.1.1.1.1.1.2.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.2">x</ci><apply id="A1.E6.m1.3.3.2.1.1.1.1.1.3.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A1.E6.m1.3.3.2.1.1.1.1.1.3.1.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3">subscript</csymbol><ci id="A1.E6.m1.3.3.2.1.1.1.1.1.3.2.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3.2">c</ci><ci id="A1.E6.m1.3.3.2.1.1.1.1.1.3.3.cmml" xref="A1.E6.m1.3.3.2.1.1.1.1.1.3.3">x</ci></apply></apply></apply><ci id="A1.E6.m1.3.3.2.1.3.cmml" xref="A1.E6.m1.3.3.2.1.3">𝑝</ci></apply><apply id="A1.E6.m1.4.4.3.2.1.1.cmml" xref="A1.E6.m1.4.4.3.2.1"><csymbol cd="ambiguous" id="A1.E6.m1.4.4.3.2.1.1.1.cmml" xref="A1.E6.m1.4.4.3.2.1">subscript</csymbol><ci id="A1.E6.m1.4.4.3.2.1.1.2.cmml" xref="A1.E6.m1.4.4.3.2.1.1.2">c</ci><ci id="A1.E6.m1.4.4.3.2.1.1.3.cmml" xref="A1.E6.m1.4.4.3.2.1.1.3">x</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E6.m1.4c">\displaystyle p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{x}|\mathrm{c_{x}})\times p%
(\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="A1.E6.m1.4d">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) = italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) × italic_p ( roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
<tbody id="A1.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{c_{x}}|\mathrm{x})\times p%
(\mathrm{x})" class="ltx_Math" display="inline" id="A1.E7.m1.4"><semantics id="A1.E7.m1.4a"><mrow id="A1.E7.m1.4.4" xref="A1.E7.m1.4.4.cmml"><mrow id="A1.E7.m1.3.3.1" xref="A1.E7.m1.3.3.1.cmml"><mi id="A1.E7.m1.3.3.1.3" xref="A1.E7.m1.3.3.1.3.cmml">p</mi><mo id="A1.E7.m1.3.3.1.2" xref="A1.E7.m1.3.3.1.2.cmml">⁢</mo><mrow id="A1.E7.m1.3.3.1.1.1" xref="A1.E7.m1.3.3.1.1.2.cmml"><mo id="A1.E7.m1.3.3.1.1.1.2" stretchy="false" xref="A1.E7.m1.3.3.1.1.2.cmml">(</mo><mi id="A1.E7.m1.1.1" mathvariant="normal" xref="A1.E7.m1.1.1.cmml">x</mi><mo id="A1.E7.m1.3.3.1.1.1.3" xref="A1.E7.m1.3.3.1.1.2.cmml">,</mo><msub id="A1.E7.m1.3.3.1.1.1.1" xref="A1.E7.m1.3.3.1.1.1.1.cmml"><mi id="A1.E7.m1.3.3.1.1.1.1.2" mathvariant="normal" xref="A1.E7.m1.3.3.1.1.1.1.2.cmml">c</mi><mi id="A1.E7.m1.3.3.1.1.1.1.3" mathvariant="normal" xref="A1.E7.m1.3.3.1.1.1.1.3.cmml">x</mi></msub><mo id="A1.E7.m1.3.3.1.1.1.4" stretchy="false" xref="A1.E7.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="A1.E7.m1.4.4.3" xref="A1.E7.m1.4.4.3.cmml">=</mo><mrow id="A1.E7.m1.4.4.2" xref="A1.E7.m1.4.4.2.cmml"><mrow id="A1.E7.m1.4.4.2.1" xref="A1.E7.m1.4.4.2.1.cmml"><mrow id="A1.E7.m1.4.4.2.1.1" xref="A1.E7.m1.4.4.2.1.1.cmml"><mi id="A1.E7.m1.4.4.2.1.1.3" xref="A1.E7.m1.4.4.2.1.1.3.cmml">p</mi><mo id="A1.E7.m1.4.4.2.1.1.2" xref="A1.E7.m1.4.4.2.1.1.2.cmml">⁢</mo><mrow id="A1.E7.m1.4.4.2.1.1.1.1" xref="A1.E7.m1.4.4.2.1.1.1.1.1.cmml"><mo id="A1.E7.m1.4.4.2.1.1.1.1.2" stretchy="false" xref="A1.E7.m1.4.4.2.1.1.1.1.1.cmml">(</mo><mrow id="A1.E7.m1.4.4.2.1.1.1.1.1" xref="A1.E7.m1.4.4.2.1.1.1.1.1.cmml"><msub id="A1.E7.m1.4.4.2.1.1.1.1.1.2" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2.cmml"><mi id="A1.E7.m1.4.4.2.1.1.1.1.1.2.2" mathvariant="normal" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2.2.cmml">c</mi><mi id="A1.E7.m1.4.4.2.1.1.1.1.1.2.3" mathvariant="normal" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2.3.cmml">x</mi></msub><mo fence="false" id="A1.E7.m1.4.4.2.1.1.1.1.1.1" xref="A1.E7.m1.4.4.2.1.1.1.1.1.1.cmml">|</mo><mi id="A1.E7.m1.4.4.2.1.1.1.1.1.3" mathvariant="normal" xref="A1.E7.m1.4.4.2.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="A1.E7.m1.4.4.2.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="A1.E7.m1.4.4.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.E7.m1.4.4.2.1.2" rspace="0.222em" xref="A1.E7.m1.4.4.2.1.2.cmml">×</mo><mi id="A1.E7.m1.4.4.2.1.3" xref="A1.E7.m1.4.4.2.1.3.cmml">p</mi></mrow><mo id="A1.E7.m1.4.4.2.2" xref="A1.E7.m1.4.4.2.2.cmml">⁢</mo><mrow id="A1.E7.m1.4.4.2.3.2" xref="A1.E7.m1.4.4.2.cmml"><mo id="A1.E7.m1.4.4.2.3.2.1" stretchy="false" xref="A1.E7.m1.4.4.2.cmml">(</mo><mi id="A1.E7.m1.2.2" mathvariant="normal" xref="A1.E7.m1.2.2.cmml">x</mi><mo id="A1.E7.m1.4.4.2.3.2.2" stretchy="false" xref="A1.E7.m1.4.4.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.E7.m1.4b"><apply id="A1.E7.m1.4.4.cmml" xref="A1.E7.m1.4.4"><eq id="A1.E7.m1.4.4.3.cmml" xref="A1.E7.m1.4.4.3"></eq><apply id="A1.E7.m1.3.3.1.cmml" xref="A1.E7.m1.3.3.1"><times id="A1.E7.m1.3.3.1.2.cmml" xref="A1.E7.m1.3.3.1.2"></times><ci id="A1.E7.m1.3.3.1.3.cmml" xref="A1.E7.m1.3.3.1.3">𝑝</ci><interval closure="open" id="A1.E7.m1.3.3.1.1.2.cmml" xref="A1.E7.m1.3.3.1.1.1"><ci id="A1.E7.m1.1.1.cmml" xref="A1.E7.m1.1.1">x</ci><apply id="A1.E7.m1.3.3.1.1.1.1.cmml" xref="A1.E7.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="A1.E7.m1.3.3.1.1.1.1.1.cmml" xref="A1.E7.m1.3.3.1.1.1.1">subscript</csymbol><ci id="A1.E7.m1.3.3.1.1.1.1.2.cmml" xref="A1.E7.m1.3.3.1.1.1.1.2">c</ci><ci id="A1.E7.m1.3.3.1.1.1.1.3.cmml" xref="A1.E7.m1.3.3.1.1.1.1.3">x</ci></apply></interval></apply><apply id="A1.E7.m1.4.4.2.cmml" xref="A1.E7.m1.4.4.2"><times id="A1.E7.m1.4.4.2.2.cmml" xref="A1.E7.m1.4.4.2.2"></times><apply id="A1.E7.m1.4.4.2.1.cmml" xref="A1.E7.m1.4.4.2.1"><times id="A1.E7.m1.4.4.2.1.2.cmml" xref="A1.E7.m1.4.4.2.1.2"></times><apply id="A1.E7.m1.4.4.2.1.1.cmml" xref="A1.E7.m1.4.4.2.1.1"><times id="A1.E7.m1.4.4.2.1.1.2.cmml" xref="A1.E7.m1.4.4.2.1.1.2"></times><ci id="A1.E7.m1.4.4.2.1.1.3.cmml" xref="A1.E7.m1.4.4.2.1.1.3">𝑝</ci><apply id="A1.E7.m1.4.4.2.1.1.1.1.1.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1"><csymbol cd="latexml" id="A1.E7.m1.4.4.2.1.1.1.1.1.1.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.1">conditional</csymbol><apply id="A1.E7.m1.4.4.2.1.1.1.1.1.2.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.E7.m1.4.4.2.1.1.1.1.1.2.1.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2">subscript</csymbol><ci id="A1.E7.m1.4.4.2.1.1.1.1.1.2.2.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2.2">c</ci><ci id="A1.E7.m1.4.4.2.1.1.1.1.1.2.3.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.2.3">x</ci></apply><ci id="A1.E7.m1.4.4.2.1.1.1.1.1.3.cmml" xref="A1.E7.m1.4.4.2.1.1.1.1.1.3">x</ci></apply></apply><ci id="A1.E7.m1.4.4.2.1.3.cmml" xref="A1.E7.m1.4.4.2.1.3">𝑝</ci></apply><ci id="A1.E7.m1.2.2.cmml" xref="A1.E7.m1.2.2">x</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E7.m1.4c">\displaystyle p(\mathrm{x},\mathrm{c_{x}})=p(\mathrm{c_{x}}|\mathrm{x})\times p%
(\mathrm{x})</annotation><annotation encoding="application/x-llamapun" id="A1.E7.m1.4d">italic_p ( roman_x , roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT ) = italic_p ( roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT | roman_x ) × italic_p ( roman_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.6">Since the joint probability can be computed in two different ways, we conduct an initial study to select the optimal auxiliary objective that improves the overall translation performance of the model. Specifically, we consider <math alttext="p(\mathrm{x}|\mathrm{c_{x}})" class="ltx_Math" display="inline" id="A1.SS1.p3.1.m1.1"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.1" xref="A1.SS1.p3.1.m1.1.1.cmml"><mi id="A1.SS1.p3.1.m1.1.1.3" xref="A1.SS1.p3.1.m1.1.1.3.cmml">p</mi><mo id="A1.SS1.p3.1.m1.1.1.2" xref="A1.SS1.p3.1.m1.1.1.2.cmml">⁢</mo><mrow id="A1.SS1.p3.1.m1.1.1.1.1" xref="A1.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo id="A1.SS1.p3.1.m1.1.1.1.1.2" stretchy="false" xref="A1.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="A1.SS1.p3.1.m1.1.1.1.1.1" xref="A1.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="A1.SS1.p3.1.m1.1.1.1.1.1.2" mathvariant="normal" xref="A1.SS1.p3.1.m1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="A1.SS1.p3.1.m1.1.1.1.1.1.1" xref="A1.SS1.p3.1.m1.1.1.1.1.1.1.cmml">|</mo><msub id="A1.SS1.p3.1.m1.1.1.1.1.1.3" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="A1.SS1.p3.1.m1.1.1.1.1.1.3.2" mathvariant="normal" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml">c</mi><mi id="A1.SS1.p3.1.m1.1.1.1.1.1.3.3" mathvariant="normal" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="A1.SS1.p3.1.m1.1.1.1.1.3" stretchy="false" xref="A1.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><apply id="A1.SS1.p3.1.m1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1"><times id="A1.SS1.p3.1.m1.1.1.2.cmml" xref="A1.SS1.p3.1.m1.1.1.2"></times><ci id="A1.SS1.p3.1.m1.1.1.3.cmml" xref="A1.SS1.p3.1.m1.1.1.3">𝑝</ci><apply id="A1.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="A1.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="A1.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.2">x</ci><apply id="A1.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="A1.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3.2">c</ci><ci id="A1.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="A1.SS1.p3.1.m1.1.1.1.1.1.3.3">x</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">p(\mathrm{x}|\mathrm{c_{x}})</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.1.m1.1d">italic_p ( roman_x | roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT )</annotation></semantics></math> as one auxiliary task where source (<math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="A1.SS1.p3.2.m2.1"><semantics id="A1.SS1.p3.2.m2.1a"><mi id="A1.SS1.p3.2.m2.1.1" mathvariant="normal" xref="A1.SS1.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.2.m2.1b"><ci id="A1.SS1.p3.2.m2.1.1.cmml" xref="A1.SS1.p3.2.m2.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.2.m2.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.2.m2.1d">roman_x</annotation></semantics></math>) is autoregressively reconstructed (denoted as <span class="ltx_text ltx_font_bold" id="A1.SS1.p3.6.1">Re-Src</span>) from the encoded context (<math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="A1.SS1.p3.3.m3.1"><semantics id="A1.SS1.p3.3.m3.1a"><msub id="A1.SS1.p3.3.m3.1.1" xref="A1.SS1.p3.3.m3.1.1.cmml"><mi id="A1.SS1.p3.3.m3.1.1.2" mathvariant="normal" xref="A1.SS1.p3.3.m3.1.1.2.cmml">c</mi><mi id="A1.SS1.p3.3.m3.1.1.3" mathvariant="normal" xref="A1.SS1.p3.3.m3.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.3.m3.1b"><apply id="A1.SS1.p3.3.m3.1.1.cmml" xref="A1.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="A1.SS1.p3.3.m3.1.1.1.cmml" xref="A1.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="A1.SS1.p3.3.m3.1.1.2.cmml" xref="A1.SS1.p3.3.m3.1.1.2">c</ci><ci id="A1.SS1.p3.3.m3.1.1.3.cmml" xref="A1.SS1.p3.3.m3.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.3.m3.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.3.m3.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>) and <math alttext="p(\mathrm{c_{x}}|\mathrm{x})" class="ltx_Math" display="inline" id="A1.SS1.p3.4.m4.1"><semantics id="A1.SS1.p3.4.m4.1a"><mrow id="A1.SS1.p3.4.m4.1.1" xref="A1.SS1.p3.4.m4.1.1.cmml"><mi id="A1.SS1.p3.4.m4.1.1.3" xref="A1.SS1.p3.4.m4.1.1.3.cmml">p</mi><mo id="A1.SS1.p3.4.m4.1.1.2" xref="A1.SS1.p3.4.m4.1.1.2.cmml">⁢</mo><mrow id="A1.SS1.p3.4.m4.1.1.1.1" xref="A1.SS1.p3.4.m4.1.1.1.1.1.cmml"><mo id="A1.SS1.p3.4.m4.1.1.1.1.2" stretchy="false" xref="A1.SS1.p3.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="A1.SS1.p3.4.m4.1.1.1.1.1" xref="A1.SS1.p3.4.m4.1.1.1.1.1.cmml"><msub id="A1.SS1.p3.4.m4.1.1.1.1.1.2" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2.cmml"><mi id="A1.SS1.p3.4.m4.1.1.1.1.1.2.2" mathvariant="normal" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2.2.cmml">c</mi><mi id="A1.SS1.p3.4.m4.1.1.1.1.1.2.3" mathvariant="normal" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2.3.cmml">x</mi></msub><mo fence="false" id="A1.SS1.p3.4.m4.1.1.1.1.1.1" xref="A1.SS1.p3.4.m4.1.1.1.1.1.1.cmml">|</mo><mi id="A1.SS1.p3.4.m4.1.1.1.1.1.3" mathvariant="normal" xref="A1.SS1.p3.4.m4.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="A1.SS1.p3.4.m4.1.1.1.1.3" stretchy="false" xref="A1.SS1.p3.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.4.m4.1b"><apply id="A1.SS1.p3.4.m4.1.1.cmml" xref="A1.SS1.p3.4.m4.1.1"><times id="A1.SS1.p3.4.m4.1.1.2.cmml" xref="A1.SS1.p3.4.m4.1.1.2"></times><ci id="A1.SS1.p3.4.m4.1.1.3.cmml" xref="A1.SS1.p3.4.m4.1.1.3">𝑝</ci><apply id="A1.SS1.p3.4.m4.1.1.1.1.1.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1"><csymbol cd="latexml" id="A1.SS1.p3.4.m4.1.1.1.1.1.1.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.1">conditional</csymbol><apply id="A1.SS1.p3.4.m4.1.1.1.1.1.2.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.p3.4.m4.1.1.1.1.1.2.1.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2">subscript</csymbol><ci id="A1.SS1.p3.4.m4.1.1.1.1.1.2.2.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2.2">c</ci><ci id="A1.SS1.p3.4.m4.1.1.1.1.1.2.3.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.2.3">x</ci></apply><ci id="A1.SS1.p3.4.m4.1.1.1.1.1.3.cmml" xref="A1.SS1.p3.4.m4.1.1.1.1.1.3">x</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.4.m4.1c">p(\mathrm{c_{x}}|\mathrm{x})</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.4.m4.1d">italic_p ( roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT | roman_x )</annotation></semantics></math> as the other auxiliary task where context (<math alttext="\mathrm{x}" class="ltx_Math" display="inline" id="A1.SS1.p3.5.m5.1"><semantics id="A1.SS1.p3.5.m5.1a"><mi id="A1.SS1.p3.5.m5.1.1" mathvariant="normal" xref="A1.SS1.p3.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.5.m5.1b"><ci id="A1.SS1.p3.5.m5.1.1.cmml" xref="A1.SS1.p3.5.m5.1.1">x</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.5.m5.1c">\mathrm{x}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.5.m5.1d">roman_x</annotation></semantics></math>) is autoregressively reconstructed (denoted as <span class="ltx_text ltx_font_bold" id="A1.SS1.p3.6.2">Re-Cntx</span>) from the encoded source (<math alttext="\mathrm{c_{x}}" class="ltx_Math" display="inline" id="A1.SS1.p3.6.m6.1"><semantics id="A1.SS1.p3.6.m6.1a"><msub id="A1.SS1.p3.6.m6.1.1" xref="A1.SS1.p3.6.m6.1.1.cmml"><mi id="A1.SS1.p3.6.m6.1.1.2" mathvariant="normal" xref="A1.SS1.p3.6.m6.1.1.2.cmml">c</mi><mi id="A1.SS1.p3.6.m6.1.1.3" mathvariant="normal" xref="A1.SS1.p3.6.m6.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.6.m6.1b"><apply id="A1.SS1.p3.6.m6.1.1.cmml" xref="A1.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="A1.SS1.p3.6.m6.1.1.1.cmml" xref="A1.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="A1.SS1.p3.6.m6.1.1.2.cmml" xref="A1.SS1.p3.6.m6.1.1.2">c</ci><ci id="A1.SS1.p3.6.m6.1.1.3.cmml" xref="A1.SS1.p3.6.m6.1.1.3">x</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.6.m6.1c">\mathrm{c_{x}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.6.m6.1d">roman_c start_POSTSUBSCRIPT roman_x end_POSTSUBSCRIPT</annotation></semantics></math>. We conducted experiments to verify which auxiliary task is performing better.</p>
</div>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="105" id="A1.F3.g1" src="x2.png" width="301"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The overview of modified MTL architecture with residual connection. The input to the model is a triplet. The triplet consist of (<span class="ltx_text ltx_font_italic" id="A1.F3.12.1">Context</span>, <span class="ltx_text ltx_font_italic" id="A1.F3.13.2">Source</span>, <span class="ltx_text ltx_font_italic" id="A1.F3.14.3">Target</span>) in <span class="ltx_text ltx_font_bold" id="A1.F3.15.4">Re-Src</span> setting and (<span class="ltx_text ltx_font_italic" id="A1.F3.16.5">Source</span>, <span class="ltx_text ltx_font_italic" id="A1.F3.17.6">Context</span>, <span class="ltx_text ltx_font_italic" id="A1.F3.18.7">Target</span>) in <span class="ltx_text ltx_font_bold" id="A1.F3.19.8">Re-Cntx</span> setting. Here, <span class="ltx_text ltx_font_italic" id="A1.F3.20.9">Source</span>: Current source sentence, <span class="ltx_text ltx_font_italic" id="A1.F3.21.10">Context</span>: Context for the current source sentence, and <span class="ltx_text ltx_font_italic" id="A1.F3.22.11">Target</span>: Translation of current source sentence. None of the layers are shared.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1">The experimental setup and model architecture are slightly different for this comparison study than those used in the main experiments.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We modified the experimental setup and model architecture during our main experiments. In this preliminary investigation, the capacity of models with independent subword vocabularies is slightly larger. Due to this, the s-BLEU scores are slightly better than the main results.</span></span></span> The Context Encoder and Intermediate Decoder output are combined with a linear layer with ReLU activation. The main experimental setup does not use this linear layer + ReLU combination. We hypothesize that adding this layer might make the model agnostic to the choice of context. We test this by training the model with random context (cf. Section <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.SS1.SSS1" title="A.1.1 Effect of Random Context ‣ A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">A.1.1</span></a>. Specifically, we use two context settings <span class="ltx_text ltx_font_italic" id="A1.SS1.p4.1.1">viz.</span> <span class="ltx_text ltx_font_italic" id="A1.SS1.p4.1.2">P@2-SRC</span> and <span class="ltx_text ltx_font_italic" id="A1.SS1.p4.1.3">P-N-SRC</span> settings (cf. <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#S3.SS3" title="3.3 Context Selection ‣ 3 Methodology ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">3.3</span></a>). We use a fixed learning rate of <math alttext="10^{-5}" class="ltx_Math" display="inline" id="A1.SS1.p4.1.m1.1"><semantics id="A1.SS1.p4.1.m1.1a"><msup id="A1.SS1.p4.1.m1.1.1" xref="A1.SS1.p4.1.m1.1.1.cmml"><mn id="A1.SS1.p4.1.m1.1.1.2" xref="A1.SS1.p4.1.m1.1.1.2.cmml">10</mn><mrow id="A1.SS1.p4.1.m1.1.1.3" xref="A1.SS1.p4.1.m1.1.1.3.cmml"><mo id="A1.SS1.p4.1.m1.1.1.3a" xref="A1.SS1.p4.1.m1.1.1.3.cmml">−</mo><mn id="A1.SS1.p4.1.m1.1.1.3.2" xref="A1.SS1.p4.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.p4.1.m1.1b"><apply id="A1.SS1.p4.1.m1.1.1.cmml" xref="A1.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p4.1.m1.1.1.1.cmml" xref="A1.SS1.p4.1.m1.1.1">superscript</csymbol><cn id="A1.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="A1.SS1.p4.1.m1.1.1.2">10</cn><apply id="A1.SS1.p4.1.m1.1.1.3.cmml" xref="A1.SS1.p4.1.m1.1.1.3"><minus id="A1.SS1.p4.1.m1.1.1.3.1.cmml" xref="A1.SS1.p4.1.m1.1.1.3"></minus><cn id="A1.SS1.p4.1.m1.1.1.3.2.cmml" type="integer" xref="A1.SS1.p4.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p4.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p4.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> instead of the warmup schedule. The output from this layer is given as input to the Final Decoder.</p>
</div>
<figure class="ltx_table" id="A1.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T8.1" style="width:433.6pt;height:162.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(48.7pt,-18.3pt) scale(1.28989415301533,1.28989415301533) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T8.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.2.1">Vanilla-Sent</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.3.1">MTL: P@2-SRC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.4.1">MTL: P-N-SRC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.1" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.2.1.1.1">News</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.2">Re-Src</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.3" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.2.1.3.1">16.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.4">20.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.2.1.5.1">20.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.3.2">
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.3.2.1">Re-Cntx</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.3.2.2">16.7 (-3.9)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.3.2.3">17.9 (-3.0)</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.4.3.1" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.4.3.1.1">TED</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.4.3.2">Re-Src</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.4.3.3" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.4.3.3.1">12.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.4.3.4">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.4.3.5.1">22.0</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.5.4">
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.5.4.1">Re-Cntx</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.5.4.2">18.0 (-3.6)</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.5.4.3">17.8 (-4.2)</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T8.1.1.6.5.1" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.6.5.1.1">Europarl</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.6.5.2">Re-Src</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T8.1.1.6.5.3" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.6.5.3.1">35.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.6.5.4">35.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.6.5.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.6.5.5.1">35.8</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.1.1.7.6.1">Re-Cntx</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.1.1.7.6.2">33.2 (-1.9)</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.1.1.7.6.3">33.6 (-2.2)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of s-BLEU scores of Baseline and proposed MTL DocNMT models trained with different source contexts for German to English direction. Differences in the scores over <span class="ltx_text ltx_font_bold" id="A1.T8.3.1">Re-Src</span> are shown inside the parentheses.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1">We use a mini-batch of 18 sentences to train all the models. We create two separate subword vocabularies for each training corpus. The created subword vocabulary is 40k in both German and English. We use the unigram language model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx16" title="">Kudo, 2018</a>]</cite> to create subword vocabularies with SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx15" title="">Kudo and Richardson, 2018</a>]</cite>, and the maximum sequence length is set to 160 tokens. During inference, we perform greedy decoding. The rest of the experimental setup is the same as the one used in the main experiments.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T9.1" style="width:433.6pt;height:110.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.9pt,-19.4pt) scale(1.53811280608167,1.53811280608167) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T9.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T9.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T9.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.2.1">Random-Train</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T9.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.4.1">Random-Infer</span></th>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.2.2.1">Re-Src</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.2.2.2">Re-Cntx</th>
<th class="ltx_td ltx_th ltx_th_column" id="A1.T9.1.1.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.2.2.4">Re-Src</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.2.2.5">Re-Cntx</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T9.1.1.3.1.1">MTL: P@2-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.3.1.2">20.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.3.1.3">16.6</td>
<td class="ltx_td ltx_border_t" id="A1.T9.1.1.3.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.3.1.5">20.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.3.1.6">16.8</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T9.1.1.4.2.1">MTL: P-N-SRC</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.4.2.2">20.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.4.2.3">16.4</td>
<td class="ltx_td ltx_border_bb" id="A1.T9.1.1.4.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.4.2.5">20.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.4.2.6">17.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>s-BLEU scores of <span class="ltx_text ltx_font_italic" id="A1.T9.4.1">Random-Train</span> and <span class="ltx_text ltx_font_italic" id="A1.T9.5.2">Random-Infer</span> experiments on News-commentary corpus.</figcaption>
</figure>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Effect of Random Context</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p" id="A1.SS1.SSS1.p1.1">We also conduct experiments to study how the random context affects the MTL models. Specifically, we evaluate the MTL models in two settings. The model is trained on the random context in the <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.1">Random-Train</span> setting by concatenating two randomly sampled sentences from the train set and testing with<span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.2"> P@2-SRC</span> and <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.3">P-N-SRC</span> context settings. In <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.4">Random-Infer</span> setting, the model is trained on <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.5">P@2-SRC</span> and <span class="ltx_text ltx_font_italic" id="A1.SS1.SSS1.p1.1.6">P-N-SRC</span> context settings and tested with random context. We train the models on the news-commentary corpus. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#A1.T9" title="Table 9 ‣ A.1 Preliminary Investigation on Auxiliary Objectives ‣ Appendix A Appendix ‣ A Case Study on Context-Aware Neural Machine Translation with Multi-Task Learning"><span class="ltx_text ltx_ref_tag">9</span></a> shows the s-BLEU scores of the MTL models trained and tested in the random context setting. Based on the results, we conclude that the model trained with random context improves the robustness of the model. This observation aligns with the findings of Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.03076v1#bib.bibx18" title="">Li et al., 2020</a>]</cite>, but they conducted experiments in the non-MTL setting with multiple encoders. As the model largely ignores the choice of the context, we remove this linear + ReLU combination and feed the output of the Intermediate Decoder to the Final Decoder. We hypothesize that this forces the model to consider the context while generating the target sentence.</p>
</div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul  3 12:41:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
