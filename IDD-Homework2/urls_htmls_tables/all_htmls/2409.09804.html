<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09804] Abnormal Event Detection In Videos Using Deep Embedding</title><meta property="og:description" content="Abnormal event detection or anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. Due to the lack of anomalous events at training time, anomaly detection requireâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Abnormal Event Detection In Videos Using Deep Embedding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Abnormal Event Detection In Videos Using Deep Embedding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09804">

<!--Generated on Sun Oct  6 00:23:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Abnormal Event Detection In Videos Using Deep Embedding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Darshan Venkatrayappa 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_italic">darsh.venkat@gmail.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Abnormal event detection or anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without supervision. In this work we propose an unsupervised approach for video anomaly detection with the aim to jointly optimize the objectives of the deep neural network and the anomaly detection task using a hybrid architecture. Initially, a convolutional autoencoder is pre-trained in an unsupervised manner with a fusion of depth, motion and appearance features. In the second step, we utilize the encoder part of the pre-trained autoencoder and extract the embeddings of the fused input. Now, we jointly train/ fine tune the encoder to map the embeddings to a hypercenter. Thus, embeddings of normal data fall near the hypercenter, whereas embeddings of anomalous data fall far away from the hypercenter.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">To ensure the safety and security of public spaces, there is a need for swift and precise detection of abnormal events, including incidents such as altercations or urgent situations like fires. Achieving this goal involves strategically placing surveillance cameras in various locations such as airports, malls, and public streets, resulting in a significant increase in the volume of video data. However, manually detecting these events, or anomalies, is an extremely meticulous task that often demands more manpower than is readily available. This challenge is exacerbated by the low probability of these abnormal events occurring, making manual detection a laborious and time-consuming endeavor. Consequently, there is an urgent requirement for automated systems capable of identifying rare or unusual incidents and activities within surveillance videos.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Defining anomalies can pose a significant challenge, primarily because their determination relies heavily on the specific context in which they occur. For example, a person crossing the road might be considered anomalous if it happens outside of a designated crosswalk. Furthermore, the precise definition of what constitutes an anomaly can often be vague and subject to interpretation. Different individuals may have varying opinions on whether activities like walking around on a subway platform should be categorized as normal or flagged as anomalous, potentially due to suspicions or different perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Chong and Tay, 2017</a>]</cite>. These complexities pose significant obstacles when attempting to construct a supervised learning model for distinguishing anomalies from regular occurrences, mainly because abnormal events represent only a small fraction of the total dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this challenge, a part of the computer vision community approaches the anomaly detection problem as an outlier detection task. They construct a normality model based on training data representing normal events and label deviations from this model as anomalous. In our work, we adopt a similar approach by mapping the embeddings of our hybrid architecture to a hypercenter. Consequently, embeddings of normal data cluster closely around the hypercenter, while those of anomalous data are positioned far away from it. This enables us to effectively identify anomalies within the data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RELATED WORK</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the realm of computer vision, various approaches have been proposed to tackle the issue of abnormal event detection. The choice of a specific approach often hinges on several factors, including the nature of the input data (whether itâ€™s sequential or non-sequential) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Chalapathy and Chawla, 2019</a>]</cite>, the type of labels available (supervised, unsupervised, or semi-supervised), and the desired output format (binary values or normality scores). Broadly speaking, we can categorize these approaches into two primary groups: those designed for sequential data, such as videos, and those tailored for non-sequential data, like images. When dealing with sequential data, such as videos, the focus typically shifts towards techniques rooted in CNNs (Convolutional Neural Networks), RNNs (Recurrent Neural Networks), or LSTM networks (Long Short-Term Memory Networks). On the other hand, approaches geared toward non-sequential data, like images, tend to favor the utilization of CNNs or AEs (Auto-encoders).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Anomaly detection can be broadly categorized along two axes: supervised approaches and unsupervised or semi-supervised approaches . In supervised methods, a dataset is meticulously labeled to distinguish normal from abnormal instances, effectively transforming the problem into a conventional classification task. While supervised approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">GÃ¶rnitz etÂ al., 2013</a>]</cite> are generally more effective than unsupervised ones, they demand a substantial number of annotations, which, in our specific context, are seldom obtainable for abnormal events. Conversely, unsupervised approaches do not depend on pre-existing labels and instead rely on the inherent characteristics of the dataset to pinpoint anomalous instances. These techniques typically operate on the premise that abnormal data points are sparsely distributed within the dataset, prompting the search for examples that exhibit substantial deviations from the overall data distribution. Unsupervised methods frequently employ dimensionality reduction techniques like PCA, auto-encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Zhou and Paffenroth, 2017</a>]</cite>, or generative models to achieve this goal. The output of the anomaly detector can be either a binary value or a â€normalityâ€ score. This can be done either globally on the signal studied, or locally to indicate the position of the anomaly. Generally binary approaches are based on thresholding a score and we consider here only approaches returning a score. Most methods calculate distances between the data to be tested and a central point of a â€normalityâ€ sphere <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Ruff etÂ al., 2018</a>]</cite>; any point sufficiently far from this center is considered abnormal.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Kiran etÂ al., 2018</a>]</cite> abnormal event detection in videos can be categorized into three main approaches. Firstly, there are reconstruction-based methods that focus on reducing data dimensionality, often through techniques like PCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Kudo etÂ al., 2013</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Wang etÂ al., 2019</a>]</cite> or auto-encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Chalapathy and Chawla, 2019</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Sabokrou etÂ al., 2016</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Hasan etÂ al., 2016</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Akhriev and Marecek, 2019</a>]</cite>. These methods assume that anomalies are in-compressible and thus cannot be effectively reconstructed from low-dimensional projections. These methods demonstrate promising results when the anomaly ratio is fairly low. Although the reconstruction of anomalous samples, based on a reconstruction scheme optimized for normal data, tends to generate a higher error, a significant amount of anomalous samples could mislead the autoencoders to learn the correlations in the anomalous data instead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Li etÂ al., 2021</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Secondly, prediction-based methods take a different approach by employing auto-regressive models or generative models to predict successive video frames based on prior frames. Anomalies are identified when these predictions deviate significantly from the actual frames. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Zhao etÂ al., 2017</a>]</cite> thus uses a 3D autoencoder for anomaly detection. Its decoder is composed of two parts, one allowing the reconstruction of the input sequence and the other predicting the following sequence. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Liu etÂ al., 2018</a>]</cite> train a frame prediction network by incorporating different techniques including gradient loss, optical flow, and adversarial training. Authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Hu etÂ al., 2016</a>]</cite> uses Slow Feature Analysis (SFA) to detect anomalies. However, itâ€™s noteworthy that many prediction-based techniques may not fully exploit the temporal context and high-level semantic information of video anomalies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Zhang etÂ al., 2023</a>]</cite>. Furthermore, these sequential predictions can be computationally intensive, and the learned representations may not be optimized for anomaly detection, as their primary objective revolves around sequential prediction rather than anomaly identification.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Finally, generative-based methods utilize models like VAEs and GANs to understand the distribution of â€normalâ€ examples, aiding in anomaly detection. Authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Fan etÂ al., 2020</a>]</cite> have used VAEs for video anomaly detection. Although these variational approaches are able to generate various plausible outcomes, the predictions are blurrier and of lower quality compared to state-of-theart GAN-based models.
Authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Ravanbakhsh etÂ al., 2017</a>]</cite> proposed to learn the generator as a reconstructor of normal events, and
hence if it cannot properly reconstruct a chunk of the input frames, that chunk is considered an anomaly. However, adversarial training is unstable. Without an explicit latent variable interpretation, GANs are prone to mode collapse as generator fails to cover the space of possible predictions by getting stuck into a single mode.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">The drawback of these above methods is that they do not detect anomalies directly. They instead leverage proxy tasks for anomaly detection, e.g., reconstructing input frames or predicting future frames, to extract general feature representations rather than normal patterns. To address these issues, we exploit the one-class classification objective to map normal data to the center of hypersphere. Specifically, we minimize the distance to the center of the hypersphere such that normal samples are mapped closely to the center of the sphere. We achieve this by training a hybrid architecture with a fusion of motion, depth and appearance features belonging to the normal data. The embeddings from the encoder of the hybrid architecture are mapped to the center of the hypersphere. During the test time the normal data are mapped near the hypercenter, where as the abnormal data are mapped away from the hypercenter.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>METHODOLOGY</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.09804/assets/images/METHODOLOGY.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="667" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the method</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Overview of our method is shown in Fig. <a href="#S3.F1" title="Figure 1 â€£ 3 METHODOLOGY â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The proposed hybrid architecture is split in to 3 parts.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Latent Feature Extraction.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Feature Fusion.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">One class classification</p>
</div>
</li>
</ul>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Latent Feature Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In our work we make use of 3 different input modalities :</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Depth maps : HR-Depth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Lyu etÂ al., 2020</a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Optical flow : RAFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Teed and Deng, 2020</a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Appearance features : CAE(Convolution Auto encoder).</p>
</div>
</li>
</ul>
<p id="S3.SS1.p1.2" class="ltx_p">The architectures of these modalities are based on the encoder/decoder principle. We will use the outputs of the encoders to have a latent representation of each modality which will then be fused to detect anomalies. We use pre-trained models on modality-specific bases. More information about these modalities can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Lyu etÂ al., 2020</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Teed and Deng, 2020</a>]</cite>. Instead of Appearance features from the CAE, Latent features of Semantic maps from Mask-RCNN can also be used. In order to ensure that the modality extractor networks continue to fulfil this role, we decided to freeze their weights for learning. As the anomaly bases do not have any labelling on the modalities used, updating the modalities in an end-to-end network is made more difficult. A future extension of the work would be to use unsupervised cost functions to perform this task.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Feature Fusion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">The optimal fusion of different modalities is a critical consideration, with the literature on multi-modal fusion offering numerous approaches but no consensus on the best fusion level. Instead, the ideal fusion position seems to be task-dependent, making it a parameter to be optimized like any other. Following the methodology proposed by the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Vielzeuf etÂ al., 2018</a>]</cite>, we implement feature fusion. As illustrated in Figure <a href="#S3.F2" title="Figure 2 â€£ 3.2 Feature Fusion â€£ 3 METHODOLOGY â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the fusion block integrates features from various modalities through multiple branches. Each modality includes a standard neural network handling the modality. In addition, there is a central network that merges the modalities. At each layer of this central branch we combine via a weighted sum the values of the previous layer and the values of the layers of the same depth of the networks handling the modality. These fusion weights are learned like the other parameters of the architecture. Thus the input to the fusion layer corresponds to the following equation:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="h_{C_{i+1}}=\alpha_{C_{i}}h_{C_{i}}+\sum_{k=1}^{m}\alpha_{M_{i}^{k}}h_{M_{i}^{k}}." display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.cmml">h</mi><msub id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.3.2" xref="S3.Ex1.m1.1.1.1.1.2.3.2.cmml">C</mi><mrow id="S3.Ex1.m1.1.1.1.1.2.3.3" xref="S3.Ex1.m1.1.1.1.1.2.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.3.3.2" xref="S3.Ex1.m1.1.1.1.1.2.3.3.2.cmml">i</mi><mo id="S3.Ex1.m1.1.1.1.1.2.3.3.1" xref="S3.Ex1.m1.1.1.1.1.2.3.3.1.cmml">+</mo><mn id="S3.Ex1.m1.1.1.1.1.2.3.3.3" xref="S3.Ex1.m1.1.1.1.1.2.3.3.3.cmml">1</mn></mrow></msub></msub><mo id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.3.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.cmml"><msub id="S3.Ex1.m1.1.1.1.1.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.2.2" xref="S3.Ex1.m1.1.1.1.1.3.2.2.2.cmml">Î±</mi><msub id="S3.Ex1.m1.1.1.1.1.3.2.2.3" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.2.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.2.cmml">C</mi><mi id="S3.Ex1.m1.1.1.1.1.3.2.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.3.cmml">i</mi></msub></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.3.2.1" xref="S3.Ex1.m1.1.1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.1.1.1.1.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.3.2.cmml">h</mi><msub id="S3.Ex1.m1.1.1.1.1.3.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.3.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.2.cmml">C</mi><mi id="S3.Ex1.m1.1.1.1.1.3.2.3.3.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.3.cmml">i</mi></msub></msub></mrow><mo rspace="0.055em" id="S3.Ex1.m1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.cmml"><munderover id="S3.Ex1.m1.1.1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.Ex1.m1.1.1.1.1.3.3.1.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.2" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.2.cmml">k</mi><mo id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.1" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex1.m1.1.1.1.1.3.3.1.3" xref="S3.Ex1.m1.1.1.1.1.3.3.1.3.cmml">m</mi></munderover><mrow id="S3.Ex1.m1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.cmml"><msub id="S3.Ex1.m1.1.1.1.1.3.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.2.cmml">Î±</mi><msubsup id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.2.cmml">M</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.3.cmml">i</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.3.cmml">k</mi></msubsup></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.3.3.2.1" xref="S3.Ex1.m1.1.1.1.1.3.3.2.1.cmml">â€‹</mo><msub id="S3.Ex1.m1.1.1.1.1.3.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.3.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.2.cmml">h</mi><msubsup id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.2.cmml">M</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.3.cmml">i</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.3.cmml">k</mi></msubsup></msub></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"></eq><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2">â„</ci><apply id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.2">ğ¶</ci><apply id="S3.Ex1.m1.1.1.1.1.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.3"><plus id="S3.Ex1.m1.1.1.1.1.2.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.3.1"></plus><ci id="S3.Ex1.m1.1.1.1.1.2.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.3.2">ğ‘–</ci><cn type="integer" id="S3.Ex1.m1.1.1.1.1.2.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3"><plus id="S3.Ex1.m1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.1"></plus><apply id="S3.Ex1.m1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2"><times id="S3.Ex1.m1.1.1.1.1.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.1"></times><apply id="S3.Ex1.m1.1.1.1.1.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.2">ğ›¼</ci><apply id="S3.Ex1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.2">ğ¶</ci><ci id="S3.Ex1.m1.1.1.1.1.3.2.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.2">â„</ci><apply id="S3.Ex1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.2">ğ¶</ci><ci id="S3.Ex1.m1.1.1.1.1.3.2.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.3">ğ‘–</ci></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3"><apply id="S3.Ex1.m1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1">subscript</csymbol><sum id="S3.Ex1.m1.1.1.1.1.3.3.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.2"></sum><apply id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3"><eq id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.1"></eq><ci id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex1.m1.1.1.1.1.3.3.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1.3">ğ‘š</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2"><times id="S3.Ex1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.1"></times><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.2">ğ›¼</ci><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.2">ğ‘€</ci><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.2.3">ğ‘–</ci></apply><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.3.3">ğ‘˜</ci></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.2">â„</ci><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3">superscript</csymbol><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.2">ğ‘€</ci><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.2.3">ğ‘–</ci></apply><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.3.3">ğ‘˜</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">h_{C_{i+1}}=\alpha_{C_{i}}h_{C_{i}}+\sum_{k=1}^{m}\alpha_{M_{i}^{k}}h_{M_{i}^{k}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.5" class="ltx_p">Where <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">m</annotation></semantics></math> is the number of modalities, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\alpha</annotation></semantics></math> a learnable scalar, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="h_{M_{i}^{k}}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">h</mi><msubsup id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2.2" xref="S3.SS2.p1.3.m3.1.1.3.2.2.cmml">M</mi><mi id="S3.SS2.p1.3.m3.1.1.3.2.3" xref="S3.SS2.p1.3.m3.1.1.3.2.3.cmml">i</mi><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">k</mi></msubsup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">â„</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">superscript</csymbol><apply id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.2.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.3.2.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2.2">ğ‘€</ci><ci id="S3.SS2.p1.3.m3.1.1.3.2.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2.3">ğ‘–</ci></apply><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">h_{M_{i}^{k}}</annotation></semantics></math> the hidden representation of each modality of depth <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">i</annotation></semantics></math> and <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="h_{C_{i}}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">h</mi><msub id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">C</mi><mi id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">â„</ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">ğ¶</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">h_{C_{i}}</annotation></semantics></math> the central hidden representation. For each branch of the fusion block, we propose to use the convolutional neural network.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.09804/assets/images/CentralNet.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="510" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of fusion block</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The encoder outputs of each modality are of different sizes so in order to process them in fusion block we need to align their dimensions. To do this, we use interpolation to obtain maps of the same size.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>One Class Classification</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The final stage of our approach is to use a one-class learning algorithm to detect anomalies. The goal of one-class learning is to build a classifier that identifies objects of the same nature as those presented during training. For this type of approach, the training set is only composed of the objects of interest/ normal data. Unlike traditional classification approaches, there is no attempt to distinguish between two or more classes with objects in each class. The focus here is on the class of interest and the aim is to delimit its boundaries. Historically, this type of classification has been studied on classifiers such as SVMs by seeking to identify the smallest hypersphere encompassing the training data. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Xu etÂ al., 2015</a>]</cite> use this approach for anomaly detection by using auto-encoder. More recently, one-class learning has been extended to deep neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Ruff etÂ al., 2018</a>]</cite>. In this article, we propose to use the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">One-Class Deep SVDD</span> approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Ruff etÂ al., 2018</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The final stage is split in to two parts 1) Pre-training and 2) Fine tuning. In the pretraining stage we train a convolution autoencoder with the fused feature maps as the input. In the finetuning stage we use the encoder part of the pretrained CAE to extract the features and map the features to the center of the hypersphere such that the distance between the center of the hypersphere and the features are minimized. We use the following regularised loss function:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.4" class="ltx_Math" alttext="loss_{m}(W,c)=\dfrac{1}{n}\sum_{i=1}^{n}\|\phi(x_{i},W)-c\|^{2}+\dfrac{\lambda}{2}\sum_{\ell=1}^{L}\|W^{\ell}\|^{2}_{F}," display="block"><semantics id="S3.Ex2.m1.4a"><mrow id="S3.Ex2.m1.4.4.1" xref="S3.Ex2.m1.4.4.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1" xref="S3.Ex2.m1.4.4.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.4" xref="S3.Ex2.m1.4.4.1.1.4.cmml"><mi id="S3.Ex2.m1.4.4.1.1.4.2" xref="S3.Ex2.m1.4.4.1.1.4.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.4.1" xref="S3.Ex2.m1.4.4.1.1.4.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.4.4.1.1.4.3" xref="S3.Ex2.m1.4.4.1.1.4.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.4.1a" xref="S3.Ex2.m1.4.4.1.1.4.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.4.4.1.1.4.4" xref="S3.Ex2.m1.4.4.1.1.4.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.4.1b" xref="S3.Ex2.m1.4.4.1.1.4.1.cmml">â€‹</mo><msub id="S3.Ex2.m1.4.4.1.1.4.5" xref="S3.Ex2.m1.4.4.1.1.4.5.cmml"><mi id="S3.Ex2.m1.4.4.1.1.4.5.2" xref="S3.Ex2.m1.4.4.1.1.4.5.2.cmml">s</mi><mi id="S3.Ex2.m1.4.4.1.1.4.5.3" xref="S3.Ex2.m1.4.4.1.1.4.5.3.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.4.1c" xref="S3.Ex2.m1.4.4.1.1.4.1.cmml">â€‹</mo><mrow id="S3.Ex2.m1.4.4.1.1.4.6.2" xref="S3.Ex2.m1.4.4.1.1.4.6.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.4.6.2.1" xref="S3.Ex2.m1.4.4.1.1.4.6.1.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">W</mi><mo id="S3.Ex2.m1.4.4.1.1.4.6.2.2" xref="S3.Ex2.m1.4.4.1.1.4.6.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">c</mi><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.4.6.2.3" xref="S3.Ex2.m1.4.4.1.1.4.6.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.4.4.1.1.3" xref="S3.Ex2.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.Ex2.m1.4.4.1.1.2" xref="S3.Ex2.m1.4.4.1.1.2.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.cmml"><mfrac id="S3.Ex2.m1.4.4.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.3.cmml"><mn id="S3.Ex2.m1.4.4.1.1.1.1.3.2" xref="S3.Ex2.m1.4.4.1.1.1.1.3.2.cmml">1</mn><mi id="S3.Ex2.m1.4.4.1.1.1.1.3.3" xref="S3.Ex2.m1.4.4.1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.cmml"><munderover id="S3.Ex2.m1.4.4.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.cmml"><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.2.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="S3.Ex2.m1.4.4.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml">W</mi><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mi id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.Ex2.m1.4.4.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo id="S3.Ex2.m1.4.4.1.1.2.3" xref="S3.Ex2.m1.4.4.1.1.2.3.cmml">+</mo><mrow id="S3.Ex2.m1.4.4.1.1.2.2" xref="S3.Ex2.m1.4.4.1.1.2.2.cmml"><mfrac id="S3.Ex2.m1.4.4.1.1.2.2.3" xref="S3.Ex2.m1.4.4.1.1.2.2.3.cmml"><mi id="S3.Ex2.m1.4.4.1.1.2.2.3.2" xref="S3.Ex2.m1.4.4.1.1.2.2.3.2.cmml">Î»</mi><mn id="S3.Ex2.m1.4.4.1.1.2.2.3.3" xref="S3.Ex2.m1.4.4.1.1.2.2.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.4.4.1.1.2.2.2" xref="S3.Ex2.m1.4.4.1.1.2.2.2.cmml">â€‹</mo><mrow id="S3.Ex2.m1.4.4.1.1.2.2.1" xref="S3.Ex2.m1.4.4.1.1.2.2.1.cmml"><munderover id="S3.Ex2.m1.4.4.1.1.2.2.1.2" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.2" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.cmml"><mi mathvariant="normal" id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.2" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.2.cmml">â„“</mi><mo id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.1" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex2.m1.4.4.1.1.2.2.1.2.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.3.cmml">L</mi></munderover><msubsup id="S3.Ex2.m1.4.4.1.1.2.2.1.1" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.2.1.cmml">â€–</mo><msup id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.2" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.2.cmml">W</mi><mi mathvariant="normal" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.3.cmml">â„“</mi></msup><mo stretchy="false" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.2.1.cmml">â€–</mo></mrow><mi id="S3.Ex2.m1.4.4.1.1.2.2.1.1.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.3.cmml">F</mi><mn id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.3" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.4.4.1.2" xref="S3.Ex2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.4b"><apply id="S3.Ex2.m1.4.4.1.1.cmml" xref="S3.Ex2.m1.4.4.1"><eq id="S3.Ex2.m1.4.4.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.3"></eq><apply id="S3.Ex2.m1.4.4.1.1.4.cmml" xref="S3.Ex2.m1.4.4.1.1.4"><times id="S3.Ex2.m1.4.4.1.1.4.1.cmml" xref="S3.Ex2.m1.4.4.1.1.4.1"></times><ci id="S3.Ex2.m1.4.4.1.1.4.2.cmml" xref="S3.Ex2.m1.4.4.1.1.4.2">ğ‘™</ci><ci id="S3.Ex2.m1.4.4.1.1.4.3.cmml" xref="S3.Ex2.m1.4.4.1.1.4.3">ğ‘œ</ci><ci id="S3.Ex2.m1.4.4.1.1.4.4.cmml" xref="S3.Ex2.m1.4.4.1.1.4.4">ğ‘ </ci><apply id="S3.Ex2.m1.4.4.1.1.4.5.cmml" xref="S3.Ex2.m1.4.4.1.1.4.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.4.5.1.cmml" xref="S3.Ex2.m1.4.4.1.1.4.5">subscript</csymbol><ci id="S3.Ex2.m1.4.4.1.1.4.5.2.cmml" xref="S3.Ex2.m1.4.4.1.1.4.5.2">ğ‘ </ci><ci id="S3.Ex2.m1.4.4.1.1.4.5.3.cmml" xref="S3.Ex2.m1.4.4.1.1.4.5.3">ğ‘š</ci></apply><interval closure="open" id="S3.Ex2.m1.4.4.1.1.4.6.1.cmml" xref="S3.Ex2.m1.4.4.1.1.4.6.2"><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">ğ‘Š</ci><ci id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">ğ‘</ci></interval></apply><apply id="S3.Ex2.m1.4.4.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2"><plus id="S3.Ex2.m1.4.4.1.1.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.3"></plus><apply id="S3.Ex2.m1.4.4.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1"><times id="S3.Ex2.m1.4.4.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.2"></times><apply id="S3.Ex2.m1.4.4.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.3"><divide id="S3.Ex2.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.3"></divide><cn type="integer" id="S3.Ex2.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.3.2">1</cn><ci id="S3.Ex2.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.3.3">ğ‘›</ci></apply><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1"><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.2"></sum><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3"><eq id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.2.3">ğ‘›</ci></apply><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1"><minus id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1"><times id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">italic-Ï•</ci><interval closure="open" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3">ğ‘Š</ci></interval></apply><ci id="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.3">ğ‘</ci></apply></apply><cn type="integer" id="S3.Ex2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S3.Ex2.m1.4.4.1.1.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2"><times id="S3.Ex2.m1.4.4.1.1.2.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.2"></times><apply id="S3.Ex2.m1.4.4.1.1.2.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.3"><divide id="S3.Ex2.m1.4.4.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.3"></divide><ci id="S3.Ex2.m1.4.4.1.1.2.2.3.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.3.2">ğœ†</ci><cn type="integer" id="S3.Ex2.m1.4.4.1.1.2.2.3.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.3.3">2</cn></apply><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1"><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.2.1.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2">superscript</csymbol><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2">subscript</csymbol><sum id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.2"></sum><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3"><eq id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.1"></eq><ci id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.2">â„“</ci><cn type="integer" id="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex2.m1.4.4.1.1.2.2.1.2.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.2.3">ğ¿</ci></apply><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1">subscript</csymbol><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1">superscript</csymbol><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.2">ğ‘Š</ci><ci id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.1.1.1.3">â„“</ci></apply></apply><cn type="integer" id="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.1.3">2</cn></apply><ci id="S3.Ex2.m1.4.4.1.1.2.2.1.1.3.cmml" xref="S3.Ex2.m1.4.4.1.1.2.2.1.1.3">ğ¹</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.4c">loss_{m}(W,c)=\dfrac{1}{n}\sum_{i=1}^{n}\|\phi(x_{i},W)-c\|^{2}+\dfrac{\lambda}{2}\sum_{\ell=1}^{L}\|W^{\ell}\|^{2}_{F},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.9" class="ltx_p">where <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">W</annotation></semantics></math> are the weights of the finetuned architecture, <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">x_{i}</annotation></semantics></math> the <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">n</annotation></semantics></math> examples of the mini-batch. <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">\phi</annotation></semantics></math> is the inference of the network on the example <math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><msub id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml"><mi id="S3.SS3.p4.5.m5.1.1.2" xref="S3.SS3.p4.5.m5.1.1.2.cmml">x</mi><mi id="S3.SS3.p4.5.m5.1.1.3" xref="S3.SS3.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><apply id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.2.cmml" xref="S3.SS3.p4.5.m5.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p4.5.m5.1.1.3.cmml" xref="S3.SS3.p4.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">x_{i}</annotation></semantics></math>, for the weights <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><mi id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><ci id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">W</annotation></semantics></math>. <math id="S3.SS3.p4.7.m7.1" class="ltx_Math" alttext="\lambda&gt;0" display="inline"><semantics id="S3.SS3.p4.7.m7.1a"><mrow id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml"><mi id="S3.SS3.p4.7.m7.1.1.2" xref="S3.SS3.p4.7.m7.1.1.2.cmml">Î»</mi><mo id="S3.SS3.p4.7.m7.1.1.1" xref="S3.SS3.p4.7.m7.1.1.1.cmml">&gt;</mo><mn id="S3.SS3.p4.7.m7.1.1.3" xref="S3.SS3.p4.7.m7.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><apply id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1"><gt id="S3.SS3.p4.7.m7.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1.1"></gt><ci id="S3.SS3.p4.7.m7.1.1.2.cmml" xref="S3.SS3.p4.7.m7.1.1.2">ğœ†</ci><cn type="integer" id="S3.SS3.p4.7.m7.1.1.3.cmml" xref="S3.SS3.p4.7.m7.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">\lambda&gt;0</annotation></semantics></math> is the weight of the regularisation and <math id="S3.SS3.p4.8.m8.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p4.8.m8.1a"><mi id="S3.SS3.p4.8.m8.1.1" xref="S3.SS3.p4.8.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.1b"><ci id="S3.SS3.p4.8.m8.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.1c">c</annotation></semantics></math> the hypercentre of the reference distribution. We thus seek the same hypercentre <math id="S3.SS3.p4.9.m9.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p4.9.m9.1a"><mi id="S3.SS3.p4.9.m9.1.1" xref="S3.SS3.p4.9.m9.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m9.1b"><ci id="S3.SS3.p4.9.m9.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m9.1c">c</annotation></semantics></math> surrounding the reference data set. In the case of learning the parameters of the fusion block, we use a cost function per network branch. Consequently, the learning process consists in solving the following problem:</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.8" class="ltx_Math" alttext="\arg\min_{W,c_{f},c_{1},\cdots,c_{m}}loss_{f}(W,c_{f})+\sum_{k=1}^{m}loss_{M^{k}}(W,c_{k})," display="block"><semantics id="S3.Ex3.m1.8a"><mrow id="S3.Ex3.m1.8.8.1" xref="S3.Ex3.m1.8.8.1.1.cmml"><mrow id="S3.Ex3.m1.8.8.1.1" xref="S3.Ex3.m1.8.8.1.1.cmml"><mrow id="S3.Ex3.m1.8.8.1.1.1" xref="S3.Ex3.m1.8.8.1.1.1.cmml"><mrow id="S3.Ex3.m1.8.8.1.1.1.3" xref="S3.Ex3.m1.8.8.1.1.1.3.cmml"><mi id="S3.Ex3.m1.8.8.1.1.1.3.1" xref="S3.Ex3.m1.8.8.1.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.Ex3.m1.8.8.1.1.1.3a" xref="S3.Ex3.m1.8.8.1.1.1.3.cmml">â¡</mo><mrow id="S3.Ex3.m1.8.8.1.1.1.3.2" xref="S3.Ex3.m1.8.8.1.1.1.3.2.cmml"><munder id="S3.Ex3.m1.8.8.1.1.1.3.2.1" xref="S3.Ex3.m1.8.8.1.1.1.3.2.1.cmml"><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.1.2" xref="S3.Ex3.m1.8.8.1.1.1.3.2.1.2.cmml">min</mi><mrow id="S3.Ex3.m1.5.5.5.5" xref="S3.Ex3.m1.5.5.5.6.cmml"><mi id="S3.Ex3.m1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml">W</mi><mo id="S3.Ex3.m1.5.5.5.5.4" xref="S3.Ex3.m1.5.5.5.6.cmml">,</mo><msub id="S3.Ex3.m1.3.3.3.3.1" xref="S3.Ex3.m1.3.3.3.3.1.cmml"><mi id="S3.Ex3.m1.3.3.3.3.1.2" xref="S3.Ex3.m1.3.3.3.3.1.2.cmml">c</mi><mi id="S3.Ex3.m1.3.3.3.3.1.3" xref="S3.Ex3.m1.3.3.3.3.1.3.cmml">f</mi></msub><mo id="S3.Ex3.m1.5.5.5.5.5" xref="S3.Ex3.m1.5.5.5.6.cmml">,</mo><msub id="S3.Ex3.m1.4.4.4.4.2" xref="S3.Ex3.m1.4.4.4.4.2.cmml"><mi id="S3.Ex3.m1.4.4.4.4.2.2" xref="S3.Ex3.m1.4.4.4.4.2.2.cmml">c</mi><mn id="S3.Ex3.m1.4.4.4.4.2.3" xref="S3.Ex3.m1.4.4.4.4.2.3.cmml">1</mn></msub><mo id="S3.Ex3.m1.5.5.5.5.6" xref="S3.Ex3.m1.5.5.5.6.cmml">,</mo><mi mathvariant="normal" id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml">â‹¯</mi><mo id="S3.Ex3.m1.5.5.5.5.7" xref="S3.Ex3.m1.5.5.5.6.cmml">,</mo><msub id="S3.Ex3.m1.5.5.5.5.3" xref="S3.Ex3.m1.5.5.5.5.3.cmml"><mi id="S3.Ex3.m1.5.5.5.5.3.2" xref="S3.Ex3.m1.5.5.5.5.3.2.cmml">c</mi><mi id="S3.Ex3.m1.5.5.5.5.3.3" xref="S3.Ex3.m1.5.5.5.5.3.3.cmml">m</mi></msub></mrow></munder><mo lspace="0.167em" id="S3.Ex3.m1.8.8.1.1.1.3.2a" xref="S3.Ex3.m1.8.8.1.1.1.3.2.cmml">â¡</mo><mrow id="S3.Ex3.m1.8.8.1.1.1.3.2.2" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.cmml"><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.2.2" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.1.3.2.2.1" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.1.cmml">â€‹</mo><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.2.3" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.1.3.2.2.1a" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.1.cmml">â€‹</mo><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.2.4" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.1.3.2.2.1b" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.cmml"><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.2" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.2.cmml">s</mi><mi id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.3" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.3.cmml">f</mi></msub></mrow></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.1.2" xref="S3.Ex3.m1.8.8.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex3.m1.8.8.1.1.1.1.1" xref="S3.Ex3.m1.8.8.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex3.m1.8.8.1.1.1.1.1.2" xref="S3.Ex3.m1.8.8.1.1.1.1.2.cmml">(</mo><mi id="S3.Ex3.m1.6.6" xref="S3.Ex3.m1.6.6.cmml">W</mi><mo id="S3.Ex3.m1.8.8.1.1.1.1.1.3" xref="S3.Ex3.m1.8.8.1.1.1.1.2.cmml">,</mo><msub id="S3.Ex3.m1.8.8.1.1.1.1.1.1" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.8.8.1.1.1.1.1.1.2" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.Ex3.m1.8.8.1.1.1.1.1.1.3" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1.3.cmml">f</mi></msub><mo stretchy="false" id="S3.Ex3.m1.8.8.1.1.1.1.1.4" xref="S3.Ex3.m1.8.8.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.055em" id="S3.Ex3.m1.8.8.1.1.3" xref="S3.Ex3.m1.8.8.1.1.3.cmml">+</mo><mrow id="S3.Ex3.m1.8.8.1.1.2" xref="S3.Ex3.m1.8.8.1.1.2.cmml"><munderover id="S3.Ex3.m1.8.8.1.1.2.2" xref="S3.Ex3.m1.8.8.1.1.2.2.cmml"><mo movablelimits="false" id="S3.Ex3.m1.8.8.1.1.2.2.2.2" xref="S3.Ex3.m1.8.8.1.1.2.2.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex3.m1.8.8.1.1.2.2.2.3" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.cmml"><mi id="S3.Ex3.m1.8.8.1.1.2.2.2.3.2" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.2.cmml">k</mi><mo id="S3.Ex3.m1.8.8.1.1.2.2.2.3.1" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.1.cmml">=</mo><mn id="S3.Ex3.m1.8.8.1.1.2.2.2.3.3" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex3.m1.8.8.1.1.2.2.3" xref="S3.Ex3.m1.8.8.1.1.2.2.3.cmml">m</mi></munderover><mrow id="S3.Ex3.m1.8.8.1.1.2.1" xref="S3.Ex3.m1.8.8.1.1.2.1.cmml"><mi id="S3.Ex3.m1.8.8.1.1.2.1.3" xref="S3.Ex3.m1.8.8.1.1.2.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.2.1.2" xref="S3.Ex3.m1.8.8.1.1.2.1.2.cmml">â€‹</mo><mi id="S3.Ex3.m1.8.8.1.1.2.1.4" xref="S3.Ex3.m1.8.8.1.1.2.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.2.1.2a" xref="S3.Ex3.m1.8.8.1.1.2.1.2.cmml">â€‹</mo><mi id="S3.Ex3.m1.8.8.1.1.2.1.5" xref="S3.Ex3.m1.8.8.1.1.2.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.2.1.2b" xref="S3.Ex3.m1.8.8.1.1.2.1.2.cmml">â€‹</mo><msub id="S3.Ex3.m1.8.8.1.1.2.1.6" xref="S3.Ex3.m1.8.8.1.1.2.1.6.cmml"><mi id="S3.Ex3.m1.8.8.1.1.2.1.6.2" xref="S3.Ex3.m1.8.8.1.1.2.1.6.2.cmml">s</mi><msup id="S3.Ex3.m1.8.8.1.1.2.1.6.3" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3.cmml"><mi id="S3.Ex3.m1.8.8.1.1.2.1.6.3.2" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3.2.cmml">M</mi><mi id="S3.Ex3.m1.8.8.1.1.2.1.6.3.3" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3.3.cmml">k</mi></msup></msub><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.8.8.1.1.2.1.2c" xref="S3.Ex3.m1.8.8.1.1.2.1.2.cmml">â€‹</mo><mrow id="S3.Ex3.m1.8.8.1.1.2.1.1.1" xref="S3.Ex3.m1.8.8.1.1.2.1.1.2.cmml"><mo stretchy="false" id="S3.Ex3.m1.8.8.1.1.2.1.1.1.2" xref="S3.Ex3.m1.8.8.1.1.2.1.1.2.cmml">(</mo><mi id="S3.Ex3.m1.7.7" xref="S3.Ex3.m1.7.7.cmml">W</mi><mo id="S3.Ex3.m1.8.8.1.1.2.1.1.1.3" xref="S3.Ex3.m1.8.8.1.1.2.1.1.2.cmml">,</mo><msub id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.cmml"><mi id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.2" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.2.cmml">c</mi><mi id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.3" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.Ex3.m1.8.8.1.1.2.1.1.1.4" xref="S3.Ex3.m1.8.8.1.1.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex3.m1.8.8.1.2" xref="S3.Ex3.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.8b"><apply id="S3.Ex3.m1.8.8.1.1.cmml" xref="S3.Ex3.m1.8.8.1"><plus id="S3.Ex3.m1.8.8.1.1.3.cmml" xref="S3.Ex3.m1.8.8.1.1.3"></plus><apply id="S3.Ex3.m1.8.8.1.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1"><times id="S3.Ex3.m1.8.8.1.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.2"></times><apply id="S3.Ex3.m1.8.8.1.1.1.3.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3"><arg id="S3.Ex3.m1.8.8.1.1.1.3.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.1"></arg><apply id="S3.Ex3.m1.8.8.1.1.1.3.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2"><apply id="S3.Ex3.m1.8.8.1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.1.3.2.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.1">subscript</csymbol><min id="S3.Ex3.m1.8.8.1.1.1.3.2.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.1.2"></min><list id="S3.Ex3.m1.5.5.5.6.cmml" xref="S3.Ex3.m1.5.5.5.5"><ci id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1">ğ‘Š</ci><apply id="S3.Ex3.m1.3.3.3.3.1.cmml" xref="S3.Ex3.m1.3.3.3.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.3.3.3.3.1.1.cmml" xref="S3.Ex3.m1.3.3.3.3.1">subscript</csymbol><ci id="S3.Ex3.m1.3.3.3.3.1.2.cmml" xref="S3.Ex3.m1.3.3.3.3.1.2">ğ‘</ci><ci id="S3.Ex3.m1.3.3.3.3.1.3.cmml" xref="S3.Ex3.m1.3.3.3.3.1.3">ğ‘“</ci></apply><apply id="S3.Ex3.m1.4.4.4.4.2.cmml" xref="S3.Ex3.m1.4.4.4.4.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.4.4.2.1.cmml" xref="S3.Ex3.m1.4.4.4.4.2">subscript</csymbol><ci id="S3.Ex3.m1.4.4.4.4.2.2.cmml" xref="S3.Ex3.m1.4.4.4.4.2.2">ğ‘</ci><cn type="integer" id="S3.Ex3.m1.4.4.4.4.2.3.cmml" xref="S3.Ex3.m1.4.4.4.4.2.3">1</cn></apply><ci id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2">â‹¯</ci><apply id="S3.Ex3.m1.5.5.5.5.3.cmml" xref="S3.Ex3.m1.5.5.5.5.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.5.5.5.5.3.1.cmml" xref="S3.Ex3.m1.5.5.5.5.3">subscript</csymbol><ci id="S3.Ex3.m1.5.5.5.5.3.2.cmml" xref="S3.Ex3.m1.5.5.5.5.3.2">ğ‘</ci><ci id="S3.Ex3.m1.5.5.5.5.3.3.cmml" xref="S3.Ex3.m1.5.5.5.5.3.3">ğ‘š</ci></apply></list></apply><apply id="S3.Ex3.m1.8.8.1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2"><times id="S3.Ex3.m1.8.8.1.1.1.3.2.2.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.1"></times><ci id="S3.Ex3.m1.8.8.1.1.1.3.2.2.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.2">ğ‘™</ci><ci id="S3.Ex3.m1.8.8.1.1.1.3.2.2.3.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.3">ğ‘œ</ci><ci id="S3.Ex3.m1.8.8.1.1.1.3.2.2.4.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.4">ğ‘ </ci><apply id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5">subscript</csymbol><ci id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.2">ğ‘ </ci><ci id="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.3.cmml" xref="S3.Ex3.m1.8.8.1.1.1.3.2.2.5.3">ğ‘“</ci></apply></apply></apply></apply><interval closure="open" id="S3.Ex3.m1.8.8.1.1.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.1.1"><ci id="S3.Ex3.m1.6.6.cmml" xref="S3.Ex3.m1.6.6">ğ‘Š</ci><apply id="S3.Ex3.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.Ex3.m1.8.8.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.8.8.1.1.1.1.1.1.3">ğ‘“</ci></apply></interval></apply><apply id="S3.Ex3.m1.8.8.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2"><apply id="S3.Ex3.m1.8.8.1.1.2.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.2.2.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2">superscript</csymbol><apply id="S3.Ex3.m1.8.8.1.1.2.2.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.2.2.2.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2">subscript</csymbol><sum id="S3.Ex3.m1.8.8.1.1.2.2.2.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.2.2"></sum><apply id="S3.Ex3.m1.8.8.1.1.2.2.2.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3"><eq id="S3.Ex3.m1.8.8.1.1.2.2.2.3.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.1"></eq><ci id="S3.Ex3.m1.8.8.1.1.2.2.2.3.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.Ex3.m1.8.8.1.1.2.2.2.3.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex3.m1.8.8.1.1.2.2.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.2.3">ğ‘š</ci></apply><apply id="S3.Ex3.m1.8.8.1.1.2.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1"><times id="S3.Ex3.m1.8.8.1.1.2.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.2"></times><ci id="S3.Ex3.m1.8.8.1.1.2.1.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.3">ğ‘™</ci><ci id="S3.Ex3.m1.8.8.1.1.2.1.4.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.4">ğ‘œ</ci><ci id="S3.Ex3.m1.8.8.1.1.2.1.5.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.5">ğ‘ </ci><apply id="S3.Ex3.m1.8.8.1.1.2.1.6.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.2.1.6.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6">subscript</csymbol><ci id="S3.Ex3.m1.8.8.1.1.2.1.6.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6.2">ğ‘ </ci><apply id="S3.Ex3.m1.8.8.1.1.2.1.6.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.2.1.6.3.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3">superscript</csymbol><ci id="S3.Ex3.m1.8.8.1.1.2.1.6.3.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3.2">ğ‘€</ci><ci id="S3.Ex3.m1.8.8.1.1.2.1.6.3.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.6.3.3">ğ‘˜</ci></apply></apply><interval closure="open" id="S3.Ex3.m1.8.8.1.1.2.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1"><ci id="S3.Ex3.m1.7.7.cmml" xref="S3.Ex3.m1.7.7">ğ‘Š</ci><apply id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.1.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.2.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.2">ğ‘</ci><ci id="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.3.cmml" xref="S3.Ex3.m1.8.8.1.1.2.1.1.1.1.3">ğ‘˜</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.8c">\arg\min_{W,c_{f},c_{1},\cdots,c_{m}}loss_{f}(W,c_{f})+\sum_{k=1}^{m}loss_{M^{k}}(W,c_{k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.4" class="ltx_p">where <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="loss_{f}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mrow id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.1.m1.1.1.1" xref="S3.SS3.p6.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.1.m1.1.1.1a" xref="S3.SS3.p6.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p6.1.m1.1.1.4" xref="S3.SS3.p6.1.m1.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.1.m1.1.1.1b" xref="S3.SS3.p6.1.m1.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p6.1.m1.1.1.5" xref="S3.SS3.p6.1.m1.1.1.5.cmml"><mi id="S3.SS3.p6.1.m1.1.1.5.2" xref="S3.SS3.p6.1.m1.1.1.5.2.cmml">s</mi><mi id="S3.SS3.p6.1.m1.1.1.5.3" xref="S3.SS3.p6.1.m1.1.1.5.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><times id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1"></times><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">ğ‘™</ci><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">ğ‘œ</ci><ci id="S3.SS3.p6.1.m1.1.1.4.cmml" xref="S3.SS3.p6.1.m1.1.1.4">ğ‘ </ci><apply id="S3.SS3.p6.1.m1.1.1.5.cmml" xref="S3.SS3.p6.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.5.1.cmml" xref="S3.SS3.p6.1.m1.1.1.5">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.5.2.cmml" xref="S3.SS3.p6.1.m1.1.1.5.2">ğ‘ </ci><ci id="S3.SS3.p6.1.m1.1.1.5.3.cmml" xref="S3.SS3.p6.1.m1.1.1.5.3">ğ‘“</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">loss_{f}</annotation></semantics></math> is the loss function of the fusion branch and <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="loss_{M^{k}}" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mrow id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mi id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.2.m2.1.1.1" xref="S3.SS3.p6.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.2.m2.1.1.1a" xref="S3.SS3.p6.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p6.2.m2.1.1.4" xref="S3.SS3.p6.2.m2.1.1.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p6.2.m2.1.1.1b" xref="S3.SS3.p6.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p6.2.m2.1.1.5" xref="S3.SS3.p6.2.m2.1.1.5.cmml"><mi id="S3.SS3.p6.2.m2.1.1.5.2" xref="S3.SS3.p6.2.m2.1.1.5.2.cmml">s</mi><msup id="S3.SS3.p6.2.m2.1.1.5.3" xref="S3.SS3.p6.2.m2.1.1.5.3.cmml"><mi id="S3.SS3.p6.2.m2.1.1.5.3.2" xref="S3.SS3.p6.2.m2.1.1.5.3.2.cmml">M</mi><mi id="S3.SS3.p6.2.m2.1.1.5.3.3" xref="S3.SS3.p6.2.m2.1.1.5.3.3.cmml">k</mi></msup></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><times id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1.1"></times><ci id="S3.SS3.p6.2.m2.1.1.2.cmml" xref="S3.SS3.p6.2.m2.1.1.2">ğ‘™</ci><ci id="S3.SS3.p6.2.m2.1.1.3.cmml" xref="S3.SS3.p6.2.m2.1.1.3">ğ‘œ</ci><ci id="S3.SS3.p6.2.m2.1.1.4.cmml" xref="S3.SS3.p6.2.m2.1.1.4">ğ‘ </ci><apply id="S3.SS3.p6.2.m2.1.1.5.cmml" xref="S3.SS3.p6.2.m2.1.1.5"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m2.1.1.5.1.cmml" xref="S3.SS3.p6.2.m2.1.1.5">subscript</csymbol><ci id="S3.SS3.p6.2.m2.1.1.5.2.cmml" xref="S3.SS3.p6.2.m2.1.1.5.2">ğ‘ </ci><apply id="S3.SS3.p6.2.m2.1.1.5.3.cmml" xref="S3.SS3.p6.2.m2.1.1.5.3"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m2.1.1.5.3.1.cmml" xref="S3.SS3.p6.2.m2.1.1.5.3">superscript</csymbol><ci id="S3.SS3.p6.2.m2.1.1.5.3.2.cmml" xref="S3.SS3.p6.2.m2.1.1.5.3.2">ğ‘€</ci><ci id="S3.SS3.p6.2.m2.1.1.5.3.3.cmml" xref="S3.SS3.p6.2.m2.1.1.5.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">loss_{M^{k}}</annotation></semantics></math> the functions associated with each modality. During the inferrence, we will use only the central branch of the fusion. The output is therefore <math id="S3.SS3.p6.3.m3.2" class="ltx_Math" alttext="|\phi_{f}(x_{i},W)-c_{f}\|^{2}" display="inline"><semantics id="S3.SS3.p6.3.m3.2a"><msup id="S3.SS3.p6.3.m3.2.2" xref="S3.SS3.p6.3.m3.2.2.cmml"><mrow id="S3.SS3.p6.3.m3.2.2.1.1" xref="S3.SS3.p6.3.m3.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p6.3.m3.2.2.1.1.2" xref="S3.SS3.p6.3.m3.2.2.1.2.1.cmml">|</mo><mrow id="S3.SS3.p6.3.m3.2.2.1.1.1" xref="S3.SS3.p6.3.m3.2.2.1.1.1.cmml"><mrow id="S3.SS3.p6.3.m3.2.2.1.1.1.1" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.cmml"><msub id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.cmml"><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.2.cmml">Ï•</mi><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.3.cmml">f</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.2.cmml">(</mo><msub id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.2.cmml">,</mo><mi id="S3.SS3.p6.3.m3.1.1" xref="S3.SS3.p6.3.m3.1.1.cmml">W</mi><mo stretchy="false" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.4" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p6.3.m3.2.2.1.1.1.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.2.cmml">âˆ’</mo><msub id="S3.SS3.p6.3.m3.2.2.1.1.1.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3.cmml"><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.3.2" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3.2.cmml">c</mi><mi id="S3.SS3.p6.3.m3.2.2.1.1.1.3.3" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3.3.cmml">f</mi></msub></mrow><mo stretchy="false" id="S3.SS3.p6.3.m3.2.2.1.1.3" xref="S3.SS3.p6.3.m3.2.2.1.2.1.cmml">â€–</mo></mrow><mn id="S3.SS3.p6.3.m3.2.2.3" xref="S3.SS3.p6.3.m3.2.2.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m3.2b"><apply id="S3.SS3.p6.3.m3.2.2.cmml" xref="S3.SS3.p6.3.m3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.2.2.2.cmml" xref="S3.SS3.p6.3.m3.2.2">superscript</csymbol><apply id="S3.SS3.p6.3.m3.2.2.1.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1"><csymbol cd="latexml" id="S3.SS3.p6.3.m3.2.2.1.2.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.2">delimited-|â€–</csymbol><apply id="S3.SS3.p6.3.m3.2.2.1.1.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1"><minus id="S3.SS3.p6.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.2"></minus><apply id="S3.SS3.p6.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1"><times id="S3.SS3.p6.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.2"></times><apply id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.2">italic-Ï•</ci><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.3.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.3.3">ğ‘“</ci></apply><interval closure="open" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1"><apply id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S3.SS3.p6.3.m3.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1">ğ‘Š</ci></interval></apply><apply id="S3.SS3.p6.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.2.2.1.1.1.3.1.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.3.2.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3.2">ğ‘</ci><ci id="S3.SS3.p6.3.m3.2.2.1.1.1.3.3.cmml" xref="S3.SS3.p6.3.m3.2.2.1.1.1.3.3">ğ‘“</ci></apply></apply></apply><cn type="integer" id="S3.SS3.p6.3.m3.2.2.3.cmml" xref="S3.SS3.p6.3.m3.2.2.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m3.2c">|\phi_{f}(x_{i},W)-c_{f}\|^{2}</annotation></semantics></math>, where <math id="S3.SS3.p6.4.m4.1" class="ltx_Math" alttext="\phi_{f}" display="inline"><semantics id="S3.SS3.p6.4.m4.1a"><msub id="S3.SS3.p6.4.m4.1.1" xref="S3.SS3.p6.4.m4.1.1.cmml"><mi id="S3.SS3.p6.4.m4.1.1.2" xref="S3.SS3.p6.4.m4.1.1.2.cmml">Ï•</mi><mi id="S3.SS3.p6.4.m4.1.1.3" xref="S3.SS3.p6.4.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m4.1b"><apply id="S3.SS3.p6.4.m4.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m4.1.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p6.4.m4.1.1.2.cmml" xref="S3.SS3.p6.4.m4.1.1.2">italic-Ï•</ci><ci id="S3.SS3.p6.4.m4.1.1.3.cmml" xref="S3.SS3.p6.4.m4.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m4.1c">\phi_{f}</annotation></semantics></math> is the output only of the central branch.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EXPERIMENTS</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We validate our approach over several benchmark datasets portraying complex anomalous events in various scenarios involving multiple scenes captured from different angles. All datasets comprise â€˜normalâ€™ video frames for training and a combination of anomalous and
non-anomalous frames for testing. Their features are summarised in Table <a href="#S4.T1" title="Table 1 â€£ 4.1 Dataset â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The CHUK Avenue dataset contains 16 normal videos for training and 21 videos for testing, for a total of 30,652 frames. Test videos include anomalies like the throwing of objects, walking in the wrong direction, running, and loitering. The UCSD anomaly detection dataset contains surveillance videos of pedestrian walkways. Anomalies include presence of skaters, bikers, small carts and people walking sideways in walkways. The dataset is divided into two parts: Ped1 and Ped2. Ped1 contains 34 normal video samples for training with some perspective distortion and 36 videos samples for testing. Ped2 portrays pedestrians walking parallely to the camera plane, with 16 videos for training and 12
for testing. The complex ShanghaiTech Campus dataset is specifically used to validate the robustness of the model. The corpus contains video sources from 13 different scenes, under various lighting conditions and camera angles. It has 330 video samples for training and 107 for testing, with a total of 130 abnormal events. Anomalies include complex unusual human behaviors, presence of unusual objects and movements in the wrong direction.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:449.6pt;height:80.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.7pt,5.4pt) scale(0.87992,0.87992) ;">
<p id="S4.T1.1.1" class="ltx_p"><span id="S4.T1.1.1.1" class="ltx_text">
<span id="S4.T1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:510.9pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.1.1.1.1.1" class="ltx_p"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text">
<span id="S4.T1.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T1.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Datasets</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Anomalous Events</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Sources</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Normal Frames</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Anomalous Frames</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Training/Testing Frames</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T1.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">UCSD Ped1</span>
<span id="S4.T1.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40</span>
<span id="S4.T1.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</span>
<span id="S4.T1.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9,995</span>
<span id="S4.T1.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,005</span>
<span id="S4.T1.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,800 / 7,200</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">UCSD Ped2</span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12</span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,924</span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,636</span>
<span id="S4.T1.1.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,550 / 2,010</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CHUK Avenue</span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47</span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26,832</span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3,820</span>
<span id="S4.T1.1.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15,328 / 15,324</span></span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">ShanghaiTech</span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">130</span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">13</span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">300,308</span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17,090</span>
<span id="S4.T1.1.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">274,515 / 42,883</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Characteristics of the datasets used in this work.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results &amp; Discussions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate our method on two benchmark datasets. 1) The UCSD Ped2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Li etÂ al., 2014</a>]</cite> which contains 16 training and 12 test videos with 12 irregular events, including riding a bike and driving a vehicle. 2) The CUHK Avenue dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Lu etÂ al., 2013</a>]</cite> consists of 16 training and 21 test videos with 47 abnormal events such as running and throwing stuff.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.7" class="ltx_p">By construction the CAE used for pretraining is symmetric. Both encoder and decoder is made of <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">4</annotation></semantics></math> convolution layers with bias set to zero followed by Relu as the activation function. Each convolution layer is followed by a BatchNorm2d layer. The weights are initialized using Kaiming initialization. The filter are of size <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">3</annotation></semantics></math> x <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">3</annotation></semantics></math> with padding and stride set to <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mn id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><cn type="integer" id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">1</annotation></semantics></math>. We pretrain the auto-encoder for <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mn id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><cn type="integer" id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">100</annotation></semantics></math> epochs and fine tune anomaly detector(encoder) for 75 epochs. We use Adam optimizer to optimize the parameters of both the CAE and the encoder. The learning rate and weight decay hyper-parameter are set to <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="1\times 10^{-2}" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mn id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">Ã—</mo><msup id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml"><mn id="S4.SS2.p2.6.m6.1.1.3.2" xref="S4.SS2.p2.6.m6.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p2.6.m6.1.1.3.3" xref="S4.SS2.p2.6.m6.1.1.3.3.cmml"><mo id="S4.SS2.p2.6.m6.1.1.3.3a" xref="S4.SS2.p2.6.m6.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS2.p2.6.m6.1.1.3.3.2" xref="S4.SS2.p2.6.m6.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><times id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></times><cn type="integer" id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">1</cn><apply id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.1.1.3.1.cmml" xref="S4.SS2.p2.6.m6.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.2.cmml" xref="S4.SS2.p2.6.m6.1.1.3.2">10</cn><apply id="S4.SS2.p2.6.m6.1.1.3.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3.3"><minus id="S4.SS2.p2.6.m6.1.1.3.3.1.cmml" xref="S4.SS2.p2.6.m6.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.3.2.cmml" xref="S4.SS2.p2.6.m6.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">1\times 10^{-2}</annotation></semantics></math> and <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mn id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><cn type="float" id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">0.1</annotation></semantics></math> respectively. Initially, we conducted experiments without pretraining, which led to poor outcomes. However, incorporating both pretraining and fine-tuning significantly improved the results.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The quantitative results of our approach are tabulated in Table.<a href="#S4.T2" title="Table 2 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We evaluate the algorithm using the Area under the curve metric. We use the sklearn metric ROC AUC score function to evaluate our algorithm. We find the AUC for individual videos and the average AUC for all the videos. We compare our method with other unsupervised methods such as MPPCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Kim and Grauman, 2009</a>]</cite>, MPPC+SFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Mahadevan etÂ al., 2010</a>]</cite>, ConvLSTM-AE and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Luo etÂ al., 2017</a>]</cite>. From Table.<a href="#S4.T2" title="Table 2 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> it can be seen that the performance of our method is almost similar to or better than most of the methods.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The qualitative results of our approach are shown in Figure.<a href="#S4.F3" title="Figure 3 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure.<a href="#S4.F4" title="Figure 4 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> respectively. In both the figures, the blue curve represents the ground truth data. The anomalous frame in ground truth is indicated by the value of 1 whereas the normal frames are indicated by 0 value. In Figure.<a href="#S4.F3" title="Figure 3 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> it can be seen that the green curve starts ascending as soon as the van enters the cameras field of view and keeps fluctuating till the end of the sequence but never drops back to the zero value. Similarly, In Figure.<a href="#S4.F4" title="Figure 4 â€£ 4.2 Results &amp; Discussions â€£ 4 EXPERIMENTS â€£ Abnormal Event Detection In Videos Using Deep Embedding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we can see that the green curve remains zero till a person starts behaving randomly by repeatedly throwing his bag up in the air.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Ped2</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">CUHK Avenue</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">Our Method</span></td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.883</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.758</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.1.1" class="ltx_text ltx_font_bold">MPPCA</span></td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NA</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.69</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">MPPC+SFA</span></td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.613</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NA</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.5.4.1.1" class="ltx_text ltx_font_bold">ConvLSTM-AE</span></td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.881</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.77</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>AUC of different methodsk.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.09804/assets/images/UANOMALY_DETECTION_RESULTS_Ped2_new.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Anomaly detection result on Ped2 dataset, Sequence 04. GT denotes
the ground truth(Blue curve). The Green curve shows the detection from our
approach.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.09804/assets/images/UANOMALY_DETECTION_RESULTS_Avenue_new_new.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Anomaly detection result on Avenue dataset, Sequence 05. GT denotes
the ground truth(Blue curve). The Green curve shows the detection from our
approach</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSIONS &amp; FUTURE WORK</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In our work we have proposed an unsupervised approach to detect anomalies in videos using a fusion of motion, depth and appearance features. Experimental evaluations on standard benchmarks demonstrate the our model performs similar to other unsupervised methods. We believe that the performance of our method can be further improved by incorporating other modalities like pose maps and audio. In our current work we just train the parameters for the fusion block. In the future we would like to simultaneously train and update the parameters of the different modalities along with the fusion block.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">REFERENCES</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akhriev and Marecek, 2019</span>
<span class="ltx_bibblock">
Akhriev, A. and Marecek, J. (2019).

</span>
<span class="ltx_bibblock">Deep autoencoders with value-at-risk thresholding for unsupervised
anomaly detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">IEEE International Symposium on Multimedia, ISM 2019, San
Diego, CA, USA, December 9-11, 2019</span>, pages 208â€“211. IEEE.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chalapathy and Chawla, 2019</span>
<span class="ltx_bibblock">
Chalapathy, R. and Chawla, S. (2019).

</span>
<span class="ltx_bibblock">Deep learning for anomaly detection: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1901.03407.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chong and Tay, 2017</span>
<span class="ltx_bibblock">
Chong, Y.Â S. and Tay, Y.Â H. (2017).

</span>
<span class="ltx_bibblock">Abnormal event detection in videos using spatiotemporal autoencoder.

</span>
<span class="ltx_bibblock">In Cong, F., Leung, A.Â C., and Wei, Q., editors, <span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">Advances in
Neural Networks - ISNN 2017 - 14th International Symposium, ISNN 2017,
Sapporo, Hakodate, and Muroran, Hokkaido, Japan, June 21-26, 2017,
Proceedings, Part II</span>, volume 10262 of <span id="bib.bibx3.2.2" class="ltx_text ltx_font_italic">Lecture Notes in Computer
Science</span>, pages 189â€“196. Springer.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al., 2020</span>
<span class="ltx_bibblock">
Fan, Y., Wen, G., Li, D., Qiu, S., Levine, M.Â D., and Xiao, F. (2020).

</span>
<span class="ltx_bibblock">Video anomaly detection and localization via gaussian mixture fully
convolutional variational autoencoder.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, 195:102920.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GÃ¶rnitz etÂ al., 2013</span>
<span class="ltx_bibblock">
GÃ¶rnitz, N., Kloft, M., Rieck, K., and Brefeld, U. (2013).

</span>
<span class="ltx_bibblock">Toward supervised anomaly detection.

</span>
<span class="ltx_bibblock"><span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">J. Artif. Intell. Res.</span>, 46:235â€“262.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan etÂ al., 2016</span>
<span class="ltx_bibblock">
Hasan, M., Choi, J., Neumann, J., Roy-Chowdhury, A.Â K., and Davis, L.Â S.
(2016).

</span>
<span class="ltx_bibblock">Learning temporal regularity in video sequences.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</span>, pages
733â€“742. IEEE Computer Society.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al., 2016</span>
<span class="ltx_bibblock">
Hu, X., Hu, S., Huang, Y., Zhang, H., and Wu, H. (2016).

</span>
<span class="ltx_bibblock">Video anomaly detection using deep incremental slow feature analysis
network.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">IET Comput. Vis.</span>, 10(4):258â€“265.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Grauman, 2009</span>
<span class="ltx_bibblock">
Kim, J. and Grauman, K. (2009).

</span>
<span class="ltx_bibblock">Observe locally, infer globally: A space-time MRF for detecting
abnormal activities with incremental updates.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida,
USA</span>, pages 2921â€“2928. IEEE Computer Society.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiran etÂ al., 2018</span>
<span class="ltx_bibblock">
Kiran, B.Â R., Thomas, D.Â M., and Parakkal, R. (2018).

</span>
<span class="ltx_bibblock">An overview of deep learning based methods for unsupervised and
semi-supervised anomaly detection in videos.

</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">J. Imaging</span>, 4(2):36.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo etÂ al., 2013</span>
<span class="ltx_bibblock">
Kudo, T., Morita, T., Matsuda, T., and Takine, T. (2013).

</span>
<span class="ltx_bibblock">Pca-based robust anomaly detection using periodic traffic behavior.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Communications, ICC
2013, Budapest, Hungary, June 9-13, 2013, Workshops Proceedings</span>, pages
1330â€“1334. IEEE.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al., 2021</span>
<span class="ltx_bibblock">
Li, T., Wang, Z., Liu, S., and Lin, W.-Y. (2021).

</span>
<span class="ltx_bibblock">Deep unsupervised anomaly detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">2021 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span>, pages 3635â€“3644.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al., 2014</span>
<span class="ltx_bibblock">
Li, W., Mahadevan, V., and Vasconcelos, N. (2014).

</span>
<span class="ltx_bibblock">Anomaly detection and localization in crowded scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 36(1):18â€“32.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al., 2018</span>
<span class="ltx_bibblock">
Liu, W., Luo, W., Lian, D., and Gao, S. (2018).

</span>
<span class="ltx_bibblock">Future frame prediction for anomaly detection - A new baseline.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</span>, pages
6536â€“6545. Computer Vision Foundation / IEEE Computer Society.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al., 2013</span>
<span class="ltx_bibblock">
Lu, C., Shi, J., and Jia, J. (2013).

</span>
<span class="ltx_bibblock">Abnormal event detection at 150 FPS in MATLAB.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2013, Sydney, Australia, December 1-8, 2013</span>, pages 2720â€“2727. IEEE
Computer Society.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al., 2017</span>
<span class="ltx_bibblock">
Luo, W., Liu, W., and Gao, S. (2017).

</span>
<span class="ltx_bibblock">Remembering history with convolutional LSTM for anomaly detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Multimedia and Expo,
ICME 2017, Hong Kong, China, July 10-14, 2017</span>, pages 439â€“444. IEEE
Computer Society.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al., 2020</span>
<span class="ltx_bibblock">
Lyu, X., Liu, L., Wang, M., Kong, X., Liu, L., Liu, Y., Chen, X., and Yuan, Y.
(2020).

</span>
<span class="ltx_bibblock">Hr-depth: High resolution self-supervised monocular depth estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2012.07356.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahadevan etÂ al., 2010</span>
<span class="ltx_bibblock">
Mahadevan, V., Li, W., Bhalodia, V., and Vasconcelos, N. (2010).

</span>
<span class="ltx_bibblock">Anomaly detection in crowded scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">2010 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition</span>, pages 1975â€“1981.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravanbakhsh etÂ al., 2017</span>
<span class="ltx_bibblock">
Ravanbakhsh, M., Nabi, M., Sangineto, E., Marcenaro, L., Regazzoni, C.Â S., and
Sebe, N. (2017).

</span>
<span class="ltx_bibblock">Abnormal event detection in videos using generative adversarial nets.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Image Processing,
ICIP 2017, Beijing, China, September 17-20, 2017</span>, pages 1577â€“1581.
IEEE.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruff etÂ al., 2018</span>
<span class="ltx_bibblock">
Ruff, L., GÃ¶rnitz, N., Deecke, L., Siddiqui, S.Â A., Vandermeulen, R.Â A.,
Binder, A., MÃ¼ller, E., and Kloft, M. (2018).

</span>
<span class="ltx_bibblock">Deep one-class classification.

</span>
<span class="ltx_bibblock">In Dy, J.Â G. and Krause, A., editors, <span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">Proceedings of the 35th
International Conference on Machine Learning, ICML 2018,
StockholmsmÃ¤ssan, Stockholm, Sweden, July 10-15, 2018</span>, volumeÂ 80 of
<span id="bib.bibx19.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 4390â€“4399. PMLR.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabokrou etÂ al., 2016</span>
<span class="ltx_bibblock">
Sabokrou, M., Fathy, M., and Hoseini, M. (2016).

</span>
<span class="ltx_bibblock">Video anomaly detection and localisation based on the sparsity and
reconstruction error of auto-encoder.

</span>
<span class="ltx_bibblock"><span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">Electronics Letters</span>, 52:1122â€“1124.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teed and Deng, 2020</span>
<span class="ltx_bibblock">
Teed, Z. and Deng, J. (2020).

</span>
<span class="ltx_bibblock">RAFT: recurrent all-pairs field transforms for optical flow.

</span>
<span class="ltx_bibblock">In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J., editors, <span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August
23-28, 2020, Proceedings, Part II</span>, volume 12347 of <span id="bib.bibx21.2.2" class="ltx_text ltx_font_italic">Lecture Notes in
Computer Science</span>, pages 402â€“419. Springer.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vielzeuf etÂ al., 2018</span>
<span class="ltx_bibblock">
Vielzeuf, V., Lechervy, A., Pateux, S., and Jurie, F. (2018).

</span>
<span class="ltx_bibblock">Centralnet: a multilayer approach for multimodal fusion.

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1808.07275.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al., 2019</span>
<span class="ltx_bibblock">
Wang, T., Miao, Z., Chen, Y., Zhou, Y., Shan, G., and Snoussi, H. (2019).

</span>
<span class="ltx_bibblock">Aed-net: An abnormal event detection network.

</span>
<span class="ltx_bibblock"><span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1903.11891.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al., 2015</span>
<span class="ltx_bibblock">
Xu, D., Ricci, E., Yan, Y., Song, J., and Sebe, N. (2015).

</span>
<span class="ltx_bibblock">Learning deep representations of appearance and motion for anomalous
event detection.

</span>
<span class="ltx_bibblock">In Xie, X., Jones, M.Â W., and Tam, G. K.Â L., editors, <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">Proceedings of the British Machine Vision Conference 2015, BMVC 2015,
Swansea, UK, September 7-10, 2015</span>, pages 8.1â€“8.12. BMVA Press.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al., 2023</span>
<span class="ltx_bibblock">
Zhang, L., Chang, X., Liu, J., Luo, M., Li, Z., Yao, L., and Hauptmann, A.
(2023).

</span>
<span class="ltx_bibblock">TN-ZSTAD: transferable network for zero-shot temporal activity
detection.

</span>
<span class="ltx_bibblock"><span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 45(3):3848â€“3861.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al., 2017</span>
<span class="ltx_bibblock">
Zhao, Y., Deng, B., Shen, C., Liu, Y., Lu, H., and Hua, X. (2017).

</span>
<span class="ltx_bibblock">Spatio-temporal autoencoder for video anomaly detection.

</span>
<span class="ltx_bibblock">In Liu, Q., Lienhart, R., Wang, H., Chen, S.Â K., Boll, S., Chen,
Y.Â P., Friedland, G., Li, J., and Yan, S., editors, <span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">Proceedings of the
2017 ACM on Multimedia Conference, MM 2017, Mountain View, CA, USA,
October 23-27, 2017</span>, pages 1933â€“1941. ACM.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Paffenroth, 2017</span>
<span class="ltx_bibblock">
Zhou, C. and Paffenroth, R.Â C. (2017).

</span>
<span class="ltx_bibblock">Anomaly detection with robust deep autoencoders.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx27.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada,
August 13 - 17, 2017</span>, pages 665â€“674. ACM.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09803" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09804" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09804">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09804" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09805" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:23:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
