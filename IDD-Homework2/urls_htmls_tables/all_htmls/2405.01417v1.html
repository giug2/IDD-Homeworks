<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Modeling Activity-Driven Music Listening with PACE</title>
<!--Generated on Thu May  2 16:04:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="user embedding,  regularity,  pattern,  soundtracking" lang="en" name="keywords"/>
<base href="/html/2405.01417v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S1" title="In Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S2" title="In Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S3" title="In Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S3.SS1" title="In 3. Methodology â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Encoding Consumption Histories</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S3.SS2" title="In 3. Methodology â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Detecting Listening Behavior Patterns</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4" title="In Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Framework evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4.SS1" title="In 4. Framework evaluation â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4.SS2" title="In 4. Framework evaluation â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Classification Scores</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4.SS3" title="In 4. Framework evaluation â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Model Interpretation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S5" title="In Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Modeling Activity-Driven Music Listening with PACE</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lilian Marey
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mareylilian@gmail.com">mareylilian@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Deezer Research - LTCI, TÃ©lÃ©com Paris</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Paris</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">France</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bruno Sguerra
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:research@deezer.com">research@deezer.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Deezer Research</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Paris</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">France</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Manuel Moussallam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:research@deezer.com">research@deezer.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Deezer Research</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Paris</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">France</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">While the topic of listening context is widely studied in the literature of music recommender systems, the integration of regular user behavior is often omitted.
In this paper, we propose PACE (PAttern-based user Consumption Embedding), a framework for building user embeddings that takes advantage of periodic listening behaviors.
PACE leverages usersâ€™ multichannel time-series consumption patterns to build understandable user vectors.
We believe the embeddings learned with PACE unveil much about the repetitive nature of user listening dynamics.
By applying this framework on long-term user histories, we evaluate the embeddings through a predictive task of activities performed while listening to music.
The validation taskâ€™s interest is two-fold, while it shows the relevance of our approach, it also offers an insightful way of understanding usersâ€™ musical consumption habits.</p>
</div>
<div class="ltx_keywords">user embedding, regularity, pattern, soundtracking
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>Conference on Human Information
Interaction and Retrieval; March 10â€“14, 2024; Sheffield, UK</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id3"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval; March 10â€“14, 2024; Sheffield, United Kingdom</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR â€™24), March 10â€“14, 2024, Sheffield, United Kingdom</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id6"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3627508.3638299</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>979-8-4007-0434-5/24/03</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Human-centered computingÂ HCI theory, concepts and models</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Human-centered computingÂ Empirical studies in HCI</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Multimedia streaming</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The Oxford English Dictionary defines taste as â€œThe sense of what is appropriate, harmonious, or beautiful; the discernment and appreciation of the beautiful in nature or artâ€. Traditionally, musical taste is often declarative. Answers to questions such as â€œDo you like the music of this artist?â€ or â€œWhat do you think of this kind of music?â€ are considered to give a general idea of a personâ€™s taste.
However, peopleâ€™s taste for art is shaped by the technological context, in particular by the accessibility of the medium in question.
In recent years, most music listening has been done online, through streaming platforms.
This change has profoundly impacted peopleâ€™s relationship with music <cite class="ltx_cite ltx_citemacro_citep">(Datta etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib7" title="">2018</a>; Fuentes etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib10" title="">2019</a>)</cite> for at least two reasons: first, the simplicity of listening (potentially everywhere and at any time), and second, the vast range of tracks now available at low cost.
In order to offer their users personalized recommendations, these services make the underlying assumption that interactions between the user and the catalog serve as accurate and exhaustive traces of musical taste. In fact, these interaction logs, collected during the use of streaming services, are precious data insofar as they can reveal behaviors listeners themselves are sometimes not aware of, or which they would not declare when asked during discussions.
However, interactions between users and the catalog are shaped by different modalities of music consumption. For example, the practice of soundtracking (listening to music to accompany an activity) is a relatively recent one and is becoming more widespread <cite class="ltx_cite ltx_citemacro_citep">(Fuentes etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib10" title="">2019</a>)</cite>. When soundtracking, users show a preference for songs that differ from their usual, e.g., a student might listen to lofi music to accompany their studying sessions <cite class="ltx_cite ltx_citemacro_citep">(Wang, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib24" title="">2020</a>)</cite>. In this situation, the student employs music as a means of helping them focus, so the consumed music might be different from the one the student listens to for leisure or with friends. We call those streams <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">activity-driven listening</span>. In these situations, the choice to listen to music in the first place, and the consequent listened content, are highly dependent on the activity performed <cite class="ltx_cite ltx_citemacro_citep">(Fuentes etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib10" title="">2019</a>; North etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib16" title="">2004</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Given the widespread use of soundtracking, distinguishing the logs generated by this practice from others is crucial for enhancing user taste models and refining recommendation methods. Being closely associated with specific activities, soundtracking allows us to leverage a fundamental aspect of human behavior: the inclination to follow recurring patterns. For instance, individuals commonly dedicate weekdays to work-related tasks, reserving weekends for leisure and relaxation <cite class="ltx_cite ltx_citemacro_citep">(Ginoux etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib11" title="">2020</a>; North etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib16" title="">2004</a>)</cite>. This inherent pattern facilitates the extraction of listening trends driven by usersâ€™ activities from the recorded logs. In this paper, thus, we focus on the notion of listening regularity. The hypothesis we want to challenge is the following: can patterns in usersâ€™ logs be leveraged for characterizing activity-driven listening?</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We therefore propose PACE (PAttern-based user Consumption Embedding), a new method for embedding user consumption histories, based on the encoding of regular listening behaviors.
PACE leverages common weekly patterns of consumption that are used to generate user embeddings, characterizing the different consumption uses found in music streaming services.
As some soundtracking activities are particularly regular, one way of evaluating our embeddings is to access if they capture activity-driven behaviors. Thus, we evaluate PACE through an activity prediction task. This step was made possible thanks to a survey carried out among Deezer (a major music streaming service) customers, which asks users what activities they use to practice while listening to music.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To summarize, our paper includes two major contributions:</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">(1)</span>: the introduction of PACE, a framework for encoding usersâ€™ histories, specializing embeddings on regular consumption patterns;</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">(2)</span>: the design of predictive models that shed light on the intrinsic relationships between regular listening behavior and the practice of soundtracking.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In order to answer the question â€œwhat determines the choice to listen to music?â€, one might refer to the idea of intention. According to <cite class="ltx_cite ltx_citemacro_citep">(North etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib16" title="">2004</a>; Volokhin and Agichtein, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib22" title="">2018</a>)</cite>, listeners may choose what they listen to according to the expected effect the music will have on their psyche (relaxation, concentration, motivation, etc.). The notion of intention is perhaps the most fundamental, insofar as it stems from psychological impulses. However, these intentions can be conditioned by many external factors of the user experience, such as the time of day, weather, location, or an activity being performed. This observation paves the way for the most widespread approaches in the music consumption literature, all grouped together in the so-called study of <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">listening context</span>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In <cite class="ltx_cite ltx_citemacro_citep">(Adomavicius and Tuzhilin, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib2" title="">2010</a>)</cite>, Adomavicius et al. establish a first taxonomy of the general field of  <span class="ltx_glossaryref" title="Context-Aware Recommender Systems"><span class="ltx_text ltx_glossary_long">Context-Aware Recommender Systems</span></span> (<abbr class="ltx_glossaryref" title="Context-Aware Recommender Systems"><span class="ltx_text ltx_glossary_short">CARS</span></abbr>), classifying on the one hand the different types of contextual knowledge according to their observability (e.g., the listening timestamp is often an observable context, whereas the userâ€™s mood is in general unobservable) and according to the static/dynamic dimension, as well as the ways of integrating context in algorithms. In the literature, a number of variables are used to model the context of listening events. In <cite class="ltx_cite ltx_citemacro_citep">(Yang and Capra, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib25" title="">2023</a>)</cite>, the authors build a framework where several classes of contextual factors are nested (information retrieval, situational, personal, and social and cultural contexts). Accordingly, listening context can be inferred by a large set of features that characterize the user/item interactions.
In <cite class="ltx_cite ltx_citemacro_citep">(Hansen etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib12" title="">2020</a>)</cite>, C. Hansen et al. illustrate the influence of time and device context in the embedding of listening sessions. In a similar approach, <cite class="ltx_cite ltx_citemacro_citep">(Braunhofer etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib5" title="">2013</a>; Cheng and Shen, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib6" title="">2016</a>)</cite> use listener location, and <cite class="ltx_cite ltx_citemacro_citep">(Sen and Larson, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib18" title="">2015</a>)</cite> uses sensor data, as weather or light levels. These contextual variables are observable and so relatively easy to collect, which is why they are present in many context-aware approaches. Beyond employing accessible data, several methods have been proposed to capture more sophisticated contexts: <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib23" title="">2018</a>)</cite> predicts user emotion, and <cite class="ltx_cite ltx_citemacro_citep">(Oh etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib17" title="">2022</a>)</cite> infers context in a latent space without predefined labels.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">These works generally deal with user histories over short periods of time and do not benefit from a long-term approach. Also, in the methods described above, user representations are a collection of successive snapshots containing contextual information, but the choice of time windows are often quite coarse, and a full temporal comprehensive profile of the userâ€™s behavior is not created. This is a shortfall, as it fails to take into account the regular aspects of human activity with precision.
As we show over the next sections, our approach, on the other hand, highlights temporal hourly weekly consumption patterns that we might be unaware of, as it has rarely been studied in the literature.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">For activity-driven listening, the activity practiced can of course be seen as part of the context. Some recent works seek to predict activity, but mainly with a content activity-tagging approach <cite class="ltx_cite ltx_citemacro_citep">(Dias etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib8" title="">2014</a>; Ibrahim etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib14" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib13" title="">2022</a>)</cite>. On the latter, although some tracks are particularly well known for being appropriate in certain contexts (e.g., <span class="ltx_text ltx_font_italic" id="S2.p4.1.1">Eye of the Tiger</span> by Survivor for doing sports), it seems quite likely that the association between tracks and activities is highly user-dependent (some people will prefer calm rather than energetic music to wake up).
This is why this paper focuses on a user modeling-based approach.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In PACE, each userâ€™s consumption history is encoded as a multivariate time series, i.e. time series composed of several dimensions, called channels.
Each channel captures specific insights from the userâ€™s history, as detailed in the next section.
The choice of these channels is shaped by our knowledge regarding common user consumption aspects.
From these time series, we derive a dictionary containing a fixed number of stereotyped behaviors, or â€œatomsâ€.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">While detecting the atoms, userâ€™s historical consumption time series are projected onto the dictionary to generate user embeddings. Unlike conventional user embedding techniques like matrix factorization, where users are represented in a latent space, each component of our vectors directly corresponds to an atom, which is, essentially, a time series. These time series encode distinct listening patterns on a weekly scale across various channels, surfacing information to help understand user behavior. Since the atomsâ€™ channels are understandable, interpreting these embeddings is doable given common weekly activity patterns and knowledge on music consumption modes from the literature.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Encoding Consumption Histories</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The collected userâ€™s stream histories are issued from Deezer, and cover a period of a year and a half (January 2022 to May 2023).
In the collected data, for each user, every music listening event has been recorded.
For each event, we have access to several pieces of information, such as the listening timestamp, the song identifier, and the origin of the stream (if the user accessed the song by themselves, or it came from algorithmic recommendation).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Our goal is to capture temporal regularities in user behavior, and as <cite class="ltx_cite ltx_citemacro_citep">(Taylor and Letham, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib21" title="">2018</a>)</cite> shows, the most important seasonal effects in business time series are in general at yearly and weekly scales. Given the length of our data, we focus on weekly cycles. Therefore, the built signals are based on counts of subsamples of streams, over one-hour windows, and they are aggregated on a weekly hourly scale (e.g., a value is assigned to Tuesdays from 10 to 11 a.m.).
This aggregation allows regular activity patterns to reveal themselves (one example is shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S3.F1" title="Figure 1 â€£ 3.1. Encoding Consumption Histories â€£ 3. Methodology â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Aggregation process for a particular user: stream count over the whole dataset (top), weekly scale aggregation (middle), after convolution and normalization (bottom).
</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S3.F1.g1" src="x1.png" width="373"/>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Since the collected listening logs contain a wide range of behavioral information, we encode histories on several channels to better represent different user consumption patterns:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Volume</span>: the first channel we choose is a simple count of the number of streams. This highlights periods of regular listening, and encodes the userâ€™s general weekly behavior.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Repetition</span>: a whole literature focuses on repeat consumptionÂ <cite class="ltx_cite ltx_citemacro_citep">(Benson etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib4" title="">2016</a>; Anderson etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib3" title="">2014</a>)</cite>. For example, inÂ <cite class="ltx_cite ltx_citemacro_citep">(Sguerra etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib19" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib20" title="">2023</a>)</cite> authors show the phenomenon of the <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.2">Mere Exposure Effect</span>, where multiple repetitions affect the evolution of usersâ€™ interest, for example by increasing the temporal gaps between two successive consumption of the same song, over the repetitions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">In this context, we use repeat behavior to gain insight into a userâ€™s intent when listening to music. On the one hand, choosing new content signifies an intent to discover, perhaps exploring songs by an artist or within a preferred genre. On the other hand, opting for known songs implies an awareness of the emotional impact they can have. Repeated listening indicates a stronger intent to control musicâ€™s influence on oneâ€™s mood. Therefore, we define a channel as a measurement for the repetitiveness of listening behavior.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Organicity</span>: the music consumption in streaming platforms have different â€œoriginsâ€, for instance, users might look for songs and albums themselves from their library or from the search bar, or enjoy automatic algorithmic music recommendation.
This distinguishes <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.2">organic</span> streams from <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.3">algorithmic</span> ones.
In an organic context, the user makes a choice about the music listened to, and is in a decision-making position with regard to the content consumed, while in a recommendation driven session, the user can trade some of the control they have for discoverability or convenience.
From the considered listening histories, about 80% of the streams come from organic usage.
However, this rate is subject to variations among users and time of the week, which is why we build a channel to account for it.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">Liked</span>: furthermore, <cite class="ltx_cite ltx_citemacro_citep">(North etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib16" title="">2004</a>)</cite> shows that the music chosen in collective listening contexts differs strongly from the userâ€™s fundamental taste (approaching a collective taste).
As a proxy for this information, we account for streams of usersâ€™ liked content, i.e. songs the user assigned as favorite tracks, or songs from albums favorited by the user.
Therefore, a measure of the alignment of the music listened to with the userâ€™s more precise fundamental taste can be computed. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.6">For each user, we build a multivariate time series, with the <math alttext="4" class="ltx_Math" display="inline" id="S3.SS1.p9.1.m1.1"><semantics id="S3.SS1.p9.1.m1.1a"><mn id="S3.SS1.p9.1.m1.1.1" xref="S3.SS1.p9.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.1.m1.1b"><cn id="S3.SS1.p9.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p9.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.1.m1.1c">4</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.1.m1.1d">4</annotation></semantics></math> channels described above, each with a length of 168 (number of hours in a week).
We define user signals tensor <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.SS1.p9.6.1">S</span>, such that for a user <math alttext="u" class="ltx_Math" display="inline" id="S3.SS1.p9.3.m3.1"><semantics id="S3.SS1.p9.3.m3.1a"><mi id="S3.SS1.p9.3.m3.1.1" xref="S3.SS1.p9.3.m3.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.3.m3.1b"><ci id="S3.SS1.p9.3.m3.1.1.cmml" xref="S3.SS1.p9.3.m3.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.3.m3.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.3.m3.1d">italic_u</annotation></semantics></math>, a channel <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p9.4.m4.1"><semantics id="S3.SS1.p9.4.m4.1a"><mi id="S3.SS1.p9.4.m4.1.1" xref="S3.SS1.p9.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.4.m4.1b"><ci id="S3.SS1.p9.4.m4.1.1.cmml" xref="S3.SS1.p9.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.4.m4.1d">italic_c</annotation></semantics></math>, and a weekly <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p9.5.m5.1"><semantics id="S3.SS1.p9.5.m5.1a"><mn id="S3.SS1.p9.5.m5.1.1" xref="S3.SS1.p9.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.5.m5.1b"><cn id="S3.SS1.p9.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p9.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.5.m5.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.5.m5.1d">1</annotation></semantics></math>-hour window <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p9.6.m6.1"><semantics id="S3.SS1.p9.6.m6.1a"><mi id="S3.SS1.p9.6.m6.1.1" xref="S3.SS1.p9.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.6.m6.1b"><ci id="S3.SS1.p9.6.m6.1.1.cmml" xref="S3.SS1.p9.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.6.m6.1d">italic_t</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textbf{S}_{u,c,t}=\frac{1}{|\{a\in A,a\sim t\}|}\sum_{a\in A,a\sim t}F_{c}(u,%
a)," class="ltx_Math" display="block" id="S3.Ex1.m1.9"><semantics id="S3.Ex1.m1.9a"><mrow id="S3.Ex1.m1.9.9.1" xref="S3.Ex1.m1.9.9.1.1.cmml"><mrow id="S3.Ex1.m1.9.9.1.1" xref="S3.Ex1.m1.9.9.1.1.cmml"><msub id="S3.Ex1.m1.9.9.1.1.2" xref="S3.Ex1.m1.9.9.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.9.9.1.1.2.2" xref="S3.Ex1.m1.9.9.1.1.2.2a.cmml">S</mtext><mrow id="S3.Ex1.m1.3.3.3.5" xref="S3.Ex1.m1.3.3.3.4.cmml"><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">u</mi><mo id="S3.Ex1.m1.3.3.3.5.1" xref="S3.Ex1.m1.3.3.3.4.cmml">,</mo><mi id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">c</mi><mo id="S3.Ex1.m1.3.3.3.5.2" xref="S3.Ex1.m1.3.3.3.4.cmml">,</mo><mi id="S3.Ex1.m1.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.cmml">t</mi></mrow></msub><mo id="S3.Ex1.m1.9.9.1.1.1" xref="S3.Ex1.m1.9.9.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.9.9.1.1.3" xref="S3.Ex1.m1.9.9.1.1.3.cmml"><mfrac id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mn id="S3.Ex1.m1.4.4.3" xref="S3.Ex1.m1.4.4.3.cmml">1</mn><mrow id="S3.Ex1.m1.4.4.1.1" xref="S3.Ex1.m1.4.4.1.2.cmml"><mo id="S3.Ex1.m1.4.4.1.1.2" stretchy="false" xref="S3.Ex1.m1.4.4.1.2.1.cmml">|</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.2.cmml"><mo id="S3.Ex1.m1.4.4.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.4.4.1.1.1.2.cmml">{</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.2.cmml">a</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.3.cmml">A</mi></mrow><mo id="S3.Ex1.m1.4.4.1.1.1.1.1.2.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.2.cmml">a</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.1.cmml">âˆ¼</mo><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.3.cmml">t</mi></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.4.4.1.1.1.2.cmml">}</mo></mrow><mo id="S3.Ex1.m1.4.4.1.1.3" stretchy="false" xref="S3.Ex1.m1.4.4.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.Ex1.m1.9.9.1.1.3.1" xref="S3.Ex1.m1.9.9.1.1.3.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.9.9.1.1.3.2" xref="S3.Ex1.m1.9.9.1.1.3.2.cmml"><munder id="S3.Ex1.m1.9.9.1.1.3.2.1" xref="S3.Ex1.m1.9.9.1.1.3.2.1.cmml"><mo id="S3.Ex1.m1.9.9.1.1.3.2.1.2" movablelimits="false" xref="S3.Ex1.m1.9.9.1.1.3.2.1.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m1.6.6.2.2" xref="S3.Ex1.m1.6.6.2.3.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.2.cmml">a</mi><mo id="S3.Ex1.m1.5.5.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex1.m1.5.5.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.1.3.cmml">A</mi></mrow><mo id="S3.Ex1.m1.6.6.2.2.3" xref="S3.Ex1.m1.6.6.2.3a.cmml">,</mo><mrow id="S3.Ex1.m1.6.6.2.2.2" xref="S3.Ex1.m1.6.6.2.2.2.cmml"><mi id="S3.Ex1.m1.6.6.2.2.2.2" xref="S3.Ex1.m1.6.6.2.2.2.2.cmml">a</mi><mo id="S3.Ex1.m1.6.6.2.2.2.1" xref="S3.Ex1.m1.6.6.2.2.2.1.cmml">âˆ¼</mo><mi id="S3.Ex1.m1.6.6.2.2.2.3" xref="S3.Ex1.m1.6.6.2.2.2.3.cmml">t</mi></mrow></mrow></munder><mrow id="S3.Ex1.m1.9.9.1.1.3.2.2" xref="S3.Ex1.m1.9.9.1.1.3.2.2.cmml"><msub id="S3.Ex1.m1.9.9.1.1.3.2.2.2" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2.cmml"><mi id="S3.Ex1.m1.9.9.1.1.3.2.2.2.2" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2.2.cmml">F</mi><mi id="S3.Ex1.m1.9.9.1.1.3.2.2.2.3" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2.3.cmml">c</mi></msub><mo id="S3.Ex1.m1.9.9.1.1.3.2.2.1" xref="S3.Ex1.m1.9.9.1.1.3.2.2.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.9.9.1.1.3.2.2.3.2" xref="S3.Ex1.m1.9.9.1.1.3.2.2.3.1.cmml"><mo id="S3.Ex1.m1.9.9.1.1.3.2.2.3.2.1" stretchy="false" xref="S3.Ex1.m1.9.9.1.1.3.2.2.3.1.cmml">(</mo><mi id="S3.Ex1.m1.7.7" xref="S3.Ex1.m1.7.7.cmml">u</mi><mo id="S3.Ex1.m1.9.9.1.1.3.2.2.3.2.2" xref="S3.Ex1.m1.9.9.1.1.3.2.2.3.1.cmml">,</mo><mi id="S3.Ex1.m1.8.8" xref="S3.Ex1.m1.8.8.cmml">a</mi><mo id="S3.Ex1.m1.9.9.1.1.3.2.2.3.2.3" stretchy="false" xref="S3.Ex1.m1.9.9.1.1.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m1.9.9.1.2" xref="S3.Ex1.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.9b"><apply id="S3.Ex1.m1.9.9.1.1.cmml" xref="S3.Ex1.m1.9.9.1"><eq id="S3.Ex1.m1.9.9.1.1.1.cmml" xref="S3.Ex1.m1.9.9.1.1.1"></eq><apply id="S3.Ex1.m1.9.9.1.1.2.cmml" xref="S3.Ex1.m1.9.9.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.9.9.1.1.2.1.cmml" xref="S3.Ex1.m1.9.9.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.9.9.1.1.2.2a.cmml" xref="S3.Ex1.m1.9.9.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.9.9.1.1.2.2.cmml" xref="S3.Ex1.m1.9.9.1.1.2.2">S</mtext></ci><list id="S3.Ex1.m1.3.3.3.4.cmml" xref="S3.Ex1.m1.3.3.3.5"><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">ğ‘¢</ci><ci id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2">ğ‘</ci><ci id="S3.Ex1.m1.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3">ğ‘¡</ci></list></apply><apply id="S3.Ex1.m1.9.9.1.1.3.cmml" xref="S3.Ex1.m1.9.9.1.1.3"><times id="S3.Ex1.m1.9.9.1.1.3.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.1"></times><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><divide id="S3.Ex1.m1.4.4.2.cmml" xref="S3.Ex1.m1.4.4"></divide><cn id="S3.Ex1.m1.4.4.3.cmml" type="integer" xref="S3.Ex1.m1.4.4.3">1</cn><apply id="S3.Ex1.m1.4.4.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1"><abs id="S3.Ex1.m1.4.4.1.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.2"></abs><set id="S3.Ex1.m1.4.4.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1"><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.1.1.1.3a.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1"><in id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.1"></in><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.1.3">ğ´</ci></apply><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.1">similar-to</csymbol><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.2.3">ğ‘¡</ci></apply></apply></set></apply></apply><apply id="S3.Ex1.m1.9.9.1.1.3.2.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2"><apply id="S3.Ex1.m1.9.9.1.1.3.2.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.9.9.1.1.3.2.1.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.1">subscript</csymbol><sum id="S3.Ex1.m1.9.9.1.1.3.2.1.2.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.1.2"></sum><apply id="S3.Ex1.m1.6.6.2.3.cmml" xref="S3.Ex1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.6.6.2.3a.cmml" xref="S3.Ex1.m1.6.6.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.5.5.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1"><in id="S3.Ex1.m1.5.5.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1"></in><ci id="S3.Ex1.m1.5.5.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.2">ğ‘</ci><ci id="S3.Ex1.m1.5.5.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.3">ğ´</ci></apply><apply id="S3.Ex1.m1.6.6.2.2.2.cmml" xref="S3.Ex1.m1.6.6.2.2.2"><csymbol cd="latexml" id="S3.Ex1.m1.6.6.2.2.2.1.cmml" xref="S3.Ex1.m1.6.6.2.2.2.1">similar-to</csymbol><ci id="S3.Ex1.m1.6.6.2.2.2.2.cmml" xref="S3.Ex1.m1.6.6.2.2.2.2">ğ‘</ci><ci id="S3.Ex1.m1.6.6.2.2.2.3.cmml" xref="S3.Ex1.m1.6.6.2.2.2.3">ğ‘¡</ci></apply></apply></apply><apply id="S3.Ex1.m1.9.9.1.1.3.2.2.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2"><times id="S3.Ex1.m1.9.9.1.1.3.2.2.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.1"></times><apply id="S3.Ex1.m1.9.9.1.1.3.2.2.2.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.9.9.1.1.3.2.2.2.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.9.9.1.1.3.2.2.2.2.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2.2">ğ¹</ci><ci id="S3.Ex1.m1.9.9.1.1.3.2.2.2.3.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.2.3">ğ‘</ci></apply><interval closure="open" id="S3.Ex1.m1.9.9.1.1.3.2.2.3.1.cmml" xref="S3.Ex1.m1.9.9.1.1.3.2.2.3.2"><ci id="S3.Ex1.m1.7.7.cmml" xref="S3.Ex1.m1.7.7">ğ‘¢</ci><ci id="S3.Ex1.m1.8.8.cmml" xref="S3.Ex1.m1.8.8">ğ‘</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.9c">\textbf{S}_{u,c,t}=\frac{1}{|\{a\in A,a\sim t\}|}\sum_{a\in A,a\sim t}F_{c}(u,%
a),</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.9d">S start_POSTSUBSCRIPT italic_u , italic_c , italic_t end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | { italic_a âˆˆ italic_A , italic_a âˆ¼ italic_t } | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_a âˆˆ italic_A , italic_a âˆ¼ italic_t end_POSTSUBSCRIPT italic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_u , italic_a ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p9.17">where <math alttext="\{a\in A,a\sim t\}" class="ltx_Math" display="inline" id="S3.SS1.p9.7.m1.1"><semantics id="S3.SS1.p9.7.m1.1a"><mrow id="S3.SS1.p9.7.m1.1.1.1" xref="S3.SS1.p9.7.m1.1.1.2.cmml"><mo id="S3.SS1.p9.7.m1.1.1.1.2" stretchy="false" xref="S3.SS1.p9.7.m1.1.1.2.cmml">{</mo><mrow id="S3.SS1.p9.7.m1.1.1.1.1.2" xref="S3.SS1.p9.7.m1.1.1.1.1.3.cmml"><mrow id="S3.SS1.p9.7.m1.1.1.1.1.1.1" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p9.7.m1.1.1.1.1.1.1.2" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.2.cmml">a</mi><mo id="S3.SS1.p9.7.m1.1.1.1.1.1.1.1" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.SS1.p9.7.m1.1.1.1.1.1.1.3" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.3.cmml">A</mi></mrow><mo id="S3.SS1.p9.7.m1.1.1.1.1.2.3" xref="S3.SS1.p9.7.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.SS1.p9.7.m1.1.1.1.1.2.2" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.cmml"><mi id="S3.SS1.p9.7.m1.1.1.1.1.2.2.2" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.2.cmml">a</mi><mo id="S3.SS1.p9.7.m1.1.1.1.1.2.2.1" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.1.cmml">âˆ¼</mo><mi id="S3.SS1.p9.7.m1.1.1.1.1.2.2.3" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.3.cmml">t</mi></mrow></mrow><mo id="S3.SS1.p9.7.m1.1.1.1.3" stretchy="false" xref="S3.SS1.p9.7.m1.1.1.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.7.m1.1b"><set id="S3.SS1.p9.7.m1.1.1.2.cmml" xref="S3.SS1.p9.7.m1.1.1.1"><apply id="S3.SS1.p9.7.m1.1.1.1.1.3.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p9.7.m1.1.1.1.1.3a.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p9.7.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1"><in id="S3.SS1.p9.7.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.1"></in><ci id="S3.SS1.p9.7.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS1.p9.7.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.1.1.3">ğ´</ci></apply><apply id="S3.SS1.p9.7.m1.1.1.1.1.2.2.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.SS1.p9.7.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.1">similar-to</csymbol><ci id="S3.SS1.p9.7.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.SS1.p9.7.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p9.7.m1.1.1.1.1.2.2.3">ğ‘¡</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.7.m1.1c">\{a\in A,a\sim t\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.7.m1.1d">{ italic_a âˆˆ italic_A , italic_a âˆ¼ italic_t }</annotation></semantics></math> is the set of all the <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p9.8.m2.1"><semantics id="S3.SS1.p9.8.m2.1a"><mn id="S3.SS1.p9.8.m2.1.1" xref="S3.SS1.p9.8.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.8.m2.1b"><cn id="S3.SS1.p9.8.m2.1.1.cmml" type="integer" xref="S3.SS1.p9.8.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.8.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.8.m2.1d">1</annotation></semantics></math>-hour windows of the database corresponding to their weekly equivalent <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p9.9.m3.1"><semantics id="S3.SS1.p9.9.m3.1a"><mi id="S3.SS1.p9.9.m3.1.1" xref="S3.SS1.p9.9.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.9.m3.1b"><ci id="S3.SS1.p9.9.m3.1.1.cmml" xref="S3.SS1.p9.9.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.9.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.9.m3.1d">italic_t</annotation></semantics></math>, and <math alttext="F_{c}" class="ltx_Math" display="inline" id="S3.SS1.p9.10.m4.1"><semantics id="S3.SS1.p9.10.m4.1a"><msub id="S3.SS1.p9.10.m4.1.1" xref="S3.SS1.p9.10.m4.1.1.cmml"><mi id="S3.SS1.p9.10.m4.1.1.2" xref="S3.SS1.p9.10.m4.1.1.2.cmml">F</mi><mi id="S3.SS1.p9.10.m4.1.1.3" xref="S3.SS1.p9.10.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.10.m4.1b"><apply id="S3.SS1.p9.10.m4.1.1.cmml" xref="S3.SS1.p9.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p9.10.m4.1.1.1.cmml" xref="S3.SS1.p9.10.m4.1.1">subscript</csymbol><ci id="S3.SS1.p9.10.m4.1.1.2.cmml" xref="S3.SS1.p9.10.m4.1.1.2">ğ¹</ci><ci id="S3.SS1.p9.10.m4.1.1.3.cmml" xref="S3.SS1.p9.10.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.10.m4.1c">F_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.10.m4.1d">italic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is the function coding channel <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p9.11.m5.1"><semantics id="S3.SS1.p9.11.m5.1a"><mi id="S3.SS1.p9.11.m5.1.1" xref="S3.SS1.p9.11.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.11.m5.1b"><ci id="S3.SS1.p9.11.m5.1.1.cmml" xref="S3.SS1.p9.11.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.11.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.11.m5.1d">italic_c</annotation></semantics></math>, defined as follows: <math alttext="F_{\textbf{volume}}(u,a)" class="ltx_Math" display="inline" id="S3.SS1.p9.12.m6.2"><semantics id="S3.SS1.p9.12.m6.2a"><mrow id="S3.SS1.p9.12.m6.2.3" xref="S3.SS1.p9.12.m6.2.3.cmml"><msub id="S3.SS1.p9.12.m6.2.3.2" xref="S3.SS1.p9.12.m6.2.3.2.cmml"><mi id="S3.SS1.p9.12.m6.2.3.2.2" xref="S3.SS1.p9.12.m6.2.3.2.2.cmml">F</mi><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.12.m6.2.3.2.3" xref="S3.SS1.p9.12.m6.2.3.2.3a.cmml">volume</mtext></msub><mo id="S3.SS1.p9.12.m6.2.3.1" xref="S3.SS1.p9.12.m6.2.3.1.cmml">â¢</mo><mrow id="S3.SS1.p9.12.m6.2.3.3.2" xref="S3.SS1.p9.12.m6.2.3.3.1.cmml"><mo id="S3.SS1.p9.12.m6.2.3.3.2.1" stretchy="false" xref="S3.SS1.p9.12.m6.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p9.12.m6.1.1" xref="S3.SS1.p9.12.m6.1.1.cmml">u</mi><mo id="S3.SS1.p9.12.m6.2.3.3.2.2" xref="S3.SS1.p9.12.m6.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p9.12.m6.2.2" xref="S3.SS1.p9.12.m6.2.2.cmml">a</mi><mo id="S3.SS1.p9.12.m6.2.3.3.2.3" stretchy="false" xref="S3.SS1.p9.12.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.12.m6.2b"><apply id="S3.SS1.p9.12.m6.2.3.cmml" xref="S3.SS1.p9.12.m6.2.3"><times id="S3.SS1.p9.12.m6.2.3.1.cmml" xref="S3.SS1.p9.12.m6.2.3.1"></times><apply id="S3.SS1.p9.12.m6.2.3.2.cmml" xref="S3.SS1.p9.12.m6.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p9.12.m6.2.3.2.1.cmml" xref="S3.SS1.p9.12.m6.2.3.2">subscript</csymbol><ci id="S3.SS1.p9.12.m6.2.3.2.2.cmml" xref="S3.SS1.p9.12.m6.2.3.2.2">ğ¹</ci><ci id="S3.SS1.p9.12.m6.2.3.2.3a.cmml" xref="S3.SS1.p9.12.m6.2.3.2.3"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.12.m6.2.3.2.3.cmml" mathsize="70%" xref="S3.SS1.p9.12.m6.2.3.2.3">volume</mtext></ci></apply><interval closure="open" id="S3.SS1.p9.12.m6.2.3.3.1.cmml" xref="S3.SS1.p9.12.m6.2.3.3.2"><ci id="S3.SS1.p9.12.m6.1.1.cmml" xref="S3.SS1.p9.12.m6.1.1">ğ‘¢</ci><ci id="S3.SS1.p9.12.m6.2.2.cmml" xref="S3.SS1.p9.12.m6.2.2">ğ‘</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.12.m6.2c">F_{\textbf{volume}}(u,a)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.12.m6.2d">italic_F start_POSTSUBSCRIPT volume end_POSTSUBSCRIPT ( italic_u , italic_a )</annotation></semantics></math> counts the number of items listened by <math alttext="u" class="ltx_Math" display="inline" id="S3.SS1.p9.13.m7.1"><semantics id="S3.SS1.p9.13.m7.1a"><mi id="S3.SS1.p9.13.m7.1.1" xref="S3.SS1.p9.13.m7.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.13.m7.1b"><ci id="S3.SS1.p9.13.m7.1.1.cmml" xref="S3.SS1.p9.13.m7.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.13.m7.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.13.m7.1d">italic_u</annotation></semantics></math> in <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.p9.14.m8.1"><semantics id="S3.SS1.p9.14.m8.1a"><mi id="S3.SS1.p9.14.m8.1.1" xref="S3.SS1.p9.14.m8.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.14.m8.1b"><ci id="S3.SS1.p9.14.m8.1.1.cmml" xref="S3.SS1.p9.14.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.14.m8.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.14.m8.1d">italic_a</annotation></semantics></math>, and <math alttext="F_{\textbf{repetition}},F_{\textbf{organicity}},F_{\textbf{liked}}" class="ltx_Math" display="inline" id="S3.SS1.p9.15.m9.3"><semantics id="S3.SS1.p9.15.m9.3a"><mrow id="S3.SS1.p9.15.m9.3.3.3" xref="S3.SS1.p9.15.m9.3.3.4.cmml"><msub id="S3.SS1.p9.15.m9.1.1.1.1" xref="S3.SS1.p9.15.m9.1.1.1.1.cmml"><mi id="S3.SS1.p9.15.m9.1.1.1.1.2" xref="S3.SS1.p9.15.m9.1.1.1.1.2.cmml">F</mi><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.1.1.1.1.3" xref="S3.SS1.p9.15.m9.1.1.1.1.3a.cmml">repetition</mtext></msub><mo id="S3.SS1.p9.15.m9.3.3.3.4" xref="S3.SS1.p9.15.m9.3.3.4.cmml">,</mo><msub id="S3.SS1.p9.15.m9.2.2.2.2" xref="S3.SS1.p9.15.m9.2.2.2.2.cmml"><mi id="S3.SS1.p9.15.m9.2.2.2.2.2" xref="S3.SS1.p9.15.m9.2.2.2.2.2.cmml">F</mi><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.2.2.2.2.3" xref="S3.SS1.p9.15.m9.2.2.2.2.3a.cmml">organicity</mtext></msub><mo id="S3.SS1.p9.15.m9.3.3.3.5" xref="S3.SS1.p9.15.m9.3.3.4.cmml">,</mo><msub id="S3.SS1.p9.15.m9.3.3.3.3" xref="S3.SS1.p9.15.m9.3.3.3.3.cmml"><mi id="S3.SS1.p9.15.m9.3.3.3.3.2" xref="S3.SS1.p9.15.m9.3.3.3.3.2.cmml">F</mi><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.3.3.3.3.3" xref="S3.SS1.p9.15.m9.3.3.3.3.3a.cmml">liked</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.15.m9.3b"><list id="S3.SS1.p9.15.m9.3.3.4.cmml" xref="S3.SS1.p9.15.m9.3.3.3"><apply id="S3.SS1.p9.15.m9.1.1.1.1.cmml" xref="S3.SS1.p9.15.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p9.15.m9.1.1.1.1.1.cmml" xref="S3.SS1.p9.15.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p9.15.m9.1.1.1.1.2.cmml" xref="S3.SS1.p9.15.m9.1.1.1.1.2">ğ¹</ci><ci id="S3.SS1.p9.15.m9.1.1.1.1.3a.cmml" xref="S3.SS1.p9.15.m9.1.1.1.1.3"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.1.1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p9.15.m9.1.1.1.1.3">repetition</mtext></ci></apply><apply id="S3.SS1.p9.15.m9.2.2.2.2.cmml" xref="S3.SS1.p9.15.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p9.15.m9.2.2.2.2.1.cmml" xref="S3.SS1.p9.15.m9.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p9.15.m9.2.2.2.2.2.cmml" xref="S3.SS1.p9.15.m9.2.2.2.2.2">ğ¹</ci><ci id="S3.SS1.p9.15.m9.2.2.2.2.3a.cmml" xref="S3.SS1.p9.15.m9.2.2.2.2.3"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.2.2.2.2.3.cmml" mathsize="70%" xref="S3.SS1.p9.15.m9.2.2.2.2.3">organicity</mtext></ci></apply><apply id="S3.SS1.p9.15.m9.3.3.3.3.cmml" xref="S3.SS1.p9.15.m9.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p9.15.m9.3.3.3.3.1.cmml" xref="S3.SS1.p9.15.m9.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p9.15.m9.3.3.3.3.2.cmml" xref="S3.SS1.p9.15.m9.3.3.3.3.2">ğ¹</ci><ci id="S3.SS1.p9.15.m9.3.3.3.3.3a.cmml" xref="S3.SS1.p9.15.m9.3.3.3.3.3"><mtext class="ltx_mathvariant_bold" id="S3.SS1.p9.15.m9.3.3.3.3.3.cmml" mathsize="70%" xref="S3.SS1.p9.15.m9.3.3.3.3.3">liked</mtext></ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.15.m9.3c">F_{\textbf{repetition}},F_{\textbf{organicity}},F_{\textbf{liked}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.15.m9.3d">italic_F start_POSTSUBSCRIPT repetition end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT organicity end_POSTSUBSCRIPT , italic_F start_POSTSUBSCRIPT liked end_POSTSUBSCRIPT</annotation></semantics></math> respectively compute the ratio in <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.p9.16.m10.1"><semantics id="S3.SS1.p9.16.m10.1a"><mi id="S3.SS1.p9.16.m10.1.1" xref="S3.SS1.p9.16.m10.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.16.m10.1b"><ci id="S3.SS1.p9.16.m10.1.1.cmml" xref="S3.SS1.p9.16.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.16.m10.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.16.m10.1d">italic_a</annotation></semantics></math> of streams of music the user listened more than <math alttext="3" class="ltx_Math" display="inline" id="S3.SS1.p9.17.m11.1"><semantics id="S3.SS1.p9.17.m11.1a"><mn id="S3.SS1.p9.17.m11.1.1" xref="S3.SS1.p9.17.m11.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.17.m11.1b"><cn id="S3.SS1.p9.17.m11.1.1.cmml" type="integer" xref="S3.SS1.p9.17.m11.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.17.m11.1c">3</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.17.m11.1d">3</annotation></semantics></math> times overall, streams tagged <span class="ltx_text ltx_font_italic" id="S3.SS1.p9.17.1">organic</span>, and streams liked by the user.
Note that streams listened to for less than 30 seconds are not considered.</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.4">In order to fade the strict <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p10.1.m1.1"><semantics id="S3.SS1.p10.1.m1.1a"><mn id="S3.SS1.p10.1.m1.1.1" xref="S3.SS1.p10.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.1.m1.1b"><cn id="S3.SS1.p10.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p10.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p10.1.m1.1d">1</annotation></semantics></math>-hour delimitation, a convolution by a constant filter of length <math alttext="3" class="ltx_Math" display="inline" id="S3.SS1.p10.2.m2.1"><semantics id="S3.SS1.p10.2.m2.1a"><mn id="S3.SS1.p10.2.m2.1.1" xref="S3.SS1.p10.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.2.m2.1b"><cn id="S3.SS1.p10.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p10.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p10.2.m2.1d">3</annotation></semantics></math> is applied to each time series channel.
In addition, to focus on finding patterns in these time series, we normalize the series on each channel for each user, fixing its mean to <math alttext="0" class="ltx_Math" display="inline" id="S3.SS1.p10.3.m3.1"><semantics id="S3.SS1.p10.3.m3.1a"><mn id="S3.SS1.p10.3.m3.1.1" xref="S3.SS1.p10.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.3.m3.1b"><cn id="S3.SS1.p10.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p10.3.m3.1.1">0</cn></annotation-xml></semantics></math> and its maximum absolute value to <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p10.4.m4.1"><semantics id="S3.SS1.p10.4.m4.1a"><mn id="S3.SS1.p10.4.m4.1.1" xref="S3.SS1.p10.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p10.4.m4.1b"><cn id="S3.SS1.p10.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p10.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p10.4.m4.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p10.4.m4.1d">1</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Detecting Listening Behavior Patterns</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To highlight typical behaviors, we use dictionary learning <cite class="ltx_cite ltx_citemacro_citep">(Mairal etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib15" title="">2009</a>)</cite>.
This allows the detection of a fixed number of time series (called atoms), that are computed as minimizers of a signal reconstruction error combined with a sparsity constraint.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As our data are multichannel series, we use the implementation of multivariate dictionary learning given by <cite class="ltx_cite ltx_citemacro_citep">(DuprÃ©Â la Tour etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib9" title="">2018</a>)</cite>.
This implies learning multivariate atoms, stored in a tensor dictionary <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.SS2.p2.1.1">D</span>. Thus, the optimization problem we tackle is the following:</p>
<p class="ltx_p ltx_align_center" id="S3.SS2.p2.2.1"><math alttext="\hat{\gamma},\hat{\textbf{D}}=argmin_{\gamma,\textbf{D}}\sum_{c\in C}||\textbf%
{S}^{c}-\textbf{D}^{c}\gamma||_{F}^{2}+\lambda||\gamma||_{1}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.1.m1.6"><semantics id="S3.SS2.p2.2.1.m1.6a"><mrow id="S3.SS2.p2.2.1.m1.6.6" xref="S3.SS2.p2.2.1.m1.6.6.cmml"><mrow id="S3.SS2.p2.2.1.m1.6.6.3.2" xref="S3.SS2.p2.2.1.m1.6.6.3.1.cmml"><mover accent="true" id="S3.SS2.p2.2.1.m1.4.4" xref="S3.SS2.p2.2.1.m1.4.4.cmml"><mi id="S3.SS2.p2.2.1.m1.4.4.2" xref="S3.SS2.p2.2.1.m1.4.4.2.cmml">Î³</mi><mo id="S3.SS2.p2.2.1.m1.4.4.1" xref="S3.SS2.p2.2.1.m1.4.4.1.cmml">^</mo></mover><mo id="S3.SS2.p2.2.1.m1.6.6.3.2.1" xref="S3.SS2.p2.2.1.m1.6.6.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.p2.2.1.m1.5.5" xref="S3.SS2.p2.2.1.m1.5.5.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.5.5.2" xref="S3.SS2.p2.2.1.m1.5.5.2a.cmml">D</mtext><mo id="S3.SS2.p2.2.1.m1.5.5.1" xref="S3.SS2.p2.2.1.m1.5.5.1.cmml">^</mo></mover></mrow><mo id="S3.SS2.p2.2.1.m1.6.6.2" xref="S3.SS2.p2.2.1.m1.6.6.2.cmml">=</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1" xref="S3.SS2.p2.2.1.m1.6.6.1.cmml"><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.cmml"><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.3.cmml">a</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.4" xref="S3.SS2.p2.2.1.m1.6.6.1.1.4.cmml">r</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2a" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.5" xref="S3.SS2.p2.2.1.m1.6.6.1.1.5.cmml">g</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2b" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.6" xref="S3.SS2.p2.2.1.m1.6.6.1.1.6.cmml">m</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2c" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.7" xref="S3.SS2.p2.2.1.m1.6.6.1.1.7.cmml">i</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2d" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><msub id="S3.SS2.p2.2.1.m1.6.6.1.1.8" xref="S3.SS2.p2.2.1.m1.6.6.1.1.8.cmml"><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.8.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.8.2.cmml">n</mi><mrow id="S3.SS2.p2.2.1.m1.2.2.2.4" xref="S3.SS2.p2.2.1.m1.2.2.2.3.cmml"><mi id="S3.SS2.p2.2.1.m1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.1.1.1.1.cmml">Î³</mi><mo id="S3.SS2.p2.2.1.m1.2.2.2.4.1" xref="S3.SS2.p2.2.1.m1.2.2.2.3.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.2.2.2.2" xref="S3.SS2.p2.2.1.m1.2.2.2.2a.cmml">D</mtext></mrow></msub><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.2e" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml">â¢</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.cmml"><msub id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.cmml"><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.2" rspace="0em" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.cmml"><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.2.cmml">c</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.1.cmml">âˆˆ</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.3.cmml">C</mi></mrow></msub><msubsup id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.cmml"><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.2.cmml"><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.cmml"><msup id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2a.cmml">S</mtext><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.3.cmml">c</mi></msup><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.cmml"><msup id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2a.cmml">D</mtext><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.3.cmml">c</mi></msup><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.1" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.3.cmml">Î³</mi></mrow></mrow><mo id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mi id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.3.cmml">F</mi><mn id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.3" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S3.SS2.p2.2.1.m1.6.6.1.2" xref="S3.SS2.p2.2.1.m1.6.6.1.2.cmml">+</mo><mrow id="S3.SS2.p2.2.1.m1.6.6.1.3" xref="S3.SS2.p2.2.1.m1.6.6.1.3.cmml"><mi id="S3.SS2.p2.2.1.m1.6.6.1.3.2" xref="S3.SS2.p2.2.1.m1.6.6.1.3.2.cmml">Î»</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.3.1" xref="S3.SS2.p2.2.1.m1.6.6.1.3.1.cmml">â¢</mo><msub id="S3.SS2.p2.2.1.m1.6.6.1.3.3" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.cmml"><mrow id="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.2" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.1.cmml"><mo id="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.2.1" stretchy="false" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.1.1.cmml">â€–</mo><mi id="S3.SS2.p2.2.1.m1.3.3" xref="S3.SS2.p2.2.1.m1.3.3.cmml">Î³</mi><mo id="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.2.2" stretchy="false" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.1.1.cmml">â€–</mo></mrow><mn id="S3.SS2.p2.2.1.m1.6.6.1.3.3.3" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.3.cmml">1</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.1.m1.6b"><apply id="S3.SS2.p2.2.1.m1.6.6.cmml" xref="S3.SS2.p2.2.1.m1.6.6"><eq id="S3.SS2.p2.2.1.m1.6.6.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.2"></eq><list id="S3.SS2.p2.2.1.m1.6.6.3.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.3.2"><apply id="S3.SS2.p2.2.1.m1.4.4.cmml" xref="S3.SS2.p2.2.1.m1.4.4"><ci id="S3.SS2.p2.2.1.m1.4.4.1.cmml" xref="S3.SS2.p2.2.1.m1.4.4.1">^</ci><ci id="S3.SS2.p2.2.1.m1.4.4.2.cmml" xref="S3.SS2.p2.2.1.m1.4.4.2">ğ›¾</ci></apply><apply id="S3.SS2.p2.2.1.m1.5.5.cmml" xref="S3.SS2.p2.2.1.m1.5.5"><ci id="S3.SS2.p2.2.1.m1.5.5.1.cmml" xref="S3.SS2.p2.2.1.m1.5.5.1">^</ci><ci id="S3.SS2.p2.2.1.m1.5.5.2a.cmml" xref="S3.SS2.p2.2.1.m1.5.5.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.5.5.2.cmml" xref="S3.SS2.p2.2.1.m1.5.5.2">D</mtext></ci></apply></list><apply id="S3.SS2.p2.2.1.m1.6.6.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1"><plus id="S3.SS2.p2.2.1.m1.6.6.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.2"></plus><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1"><times id="S3.SS2.p2.2.1.m1.6.6.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.2"></times><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.3">ğ‘</ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.4.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.4">ğ‘Ÿ</ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.5.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.5">ğ‘”</ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.6.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.6">ğ‘š</ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.7.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.7">ğ‘–</ci><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.8.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.8"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.8.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.8">subscript</csymbol><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.8.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.8.2">ğ‘›</ci><list id="S3.SS2.p2.2.1.m1.2.2.2.3.cmml" xref="S3.SS2.p2.2.1.m1.2.2.2.4"><ci id="S3.SS2.p2.2.1.m1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.1.1.1.1">ğ›¾</ci><ci id="S3.SS2.p2.2.1.m1.2.2.2.2a.cmml" xref="S3.SS2.p2.2.1.m1.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.2.2.2.2.cmml" mathsize="70%" xref="S3.SS2.p2.2.1.m1.2.2.2.2">D</mtext></ci></list></apply><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1"><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2">subscript</csymbol><sum id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.2"></sum><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3"><in id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.1"></in><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.2">ğ‘</ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.2.3.3">ğ¶</ci></apply></apply><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1"><minus id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.2">S</mtext></ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3"><times id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.1"></times><apply id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2a.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.2">D</mtext></ci><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.2.3">ğ‘</ci></apply><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.1.1.1.3.3">ğ›¾</ci></apply></apply></apply><ci id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.1.3">ğ¹</ci></apply><cn id="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p2.2.1.m1.6.6.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S3.SS2.p2.2.1.m1.6.6.1.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3"><times id="S3.SS2.p2.2.1.m1.6.6.1.3.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.1"></times><ci id="S3.SS2.p2.2.1.m1.6.6.1.3.2.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.2">ğœ†</ci><apply id="S3.SS2.p2.2.1.m1.6.6.1.3.3.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.1.m1.6.6.1.3.3.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3">subscript</csymbol><apply id="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.2"><csymbol cd="latexml" id="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.1.1.cmml" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.2.2.1">norm</csymbol><ci id="S3.SS2.p2.2.1.m1.3.3.cmml" xref="S3.SS2.p2.2.1.m1.3.3">ğ›¾</ci></apply><cn id="S3.SS2.p2.2.1.m1.6.6.1.3.3.3.cmml" type="integer" xref="S3.SS2.p2.2.1.m1.6.6.1.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.1.m1.6c">\hat{\gamma},\hat{\textbf{D}}=argmin_{\gamma,\textbf{D}}\sum_{c\in C}||\textbf%
{S}^{c}-\textbf{D}^{c}\gamma||_{F}^{2}+\lambda||\gamma||_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.1.m1.6d">over^ start_ARG italic_Î³ end_ARG , over^ start_ARG D end_ARG = italic_a italic_r italic_g italic_m italic_i italic_n start_POSTSUBSCRIPT italic_Î³ , D end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_c âˆˆ italic_C end_POSTSUBSCRIPT | | S start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT - D start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT italic_Î³ | | start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Î» | | italic_Î³ | | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>,</p>
<p class="ltx_p" id="S3.SS2.p2.11">where <math alttext="C" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m1.1"><semantics id="S3.SS2.p2.3.m1.1a"><mi id="S3.SS2.p2.3.m1.1.1" xref="S3.SS2.p2.3.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m1.1b"><ci id="S3.SS2.p2.3.m1.1.1.cmml" xref="S3.SS2.p2.3.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m1.1d">italic_C</annotation></semantics></math> is the set of channels, <math alttext="\textbf{S}^{c}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m2.1"><semantics id="S3.SS2.p2.4.m2.1a"><msup id="S3.SS2.p2.4.m2.1.1" xref="S3.SS2.p2.4.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.4.m2.1.1.2" xref="S3.SS2.p2.4.m2.1.1.2a.cmml">S</mtext><mi id="S3.SS2.p2.4.m2.1.1.3" xref="S3.SS2.p2.4.m2.1.1.3.cmml">c</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m2.1b"><apply id="S3.SS2.p2.4.m2.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m2.1.1.1.cmml" xref="S3.SS2.p2.4.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m2.1.1.2a.cmml" xref="S3.SS2.p2.4.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.4.m2.1.1.2.cmml" xref="S3.SS2.p2.4.m2.1.1.2">S</mtext></ci><ci id="S3.SS2.p2.4.m2.1.1.3.cmml" xref="S3.SS2.p2.4.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m2.1c">\textbf{S}^{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m2.1d">S start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\textbf{D}^{c}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m3.1"><semantics id="S3.SS2.p2.5.m3.1a"><msup id="S3.SS2.p2.5.m3.1.1" xref="S3.SS2.p2.5.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.5.m3.1.1.2" xref="S3.SS2.p2.5.m3.1.1.2a.cmml">D</mtext><mi id="S3.SS2.p2.5.m3.1.1.3" xref="S3.SS2.p2.5.m3.1.1.3.cmml">c</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m3.1b"><apply id="S3.SS2.p2.5.m3.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m3.1.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.5.m3.1.1.2a.cmml" xref="S3.SS2.p2.5.m3.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS2.p2.5.m3.1.1.2.cmml" xref="S3.SS2.p2.5.m3.1.1.2">D</mtext></ci><ci id="S3.SS2.p2.5.m3.1.1.3.cmml" xref="S3.SS2.p2.5.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m3.1c">\textbf{D}^{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m3.1d">D start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> are matrix extracted on channel <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m4.1"><semantics id="S3.SS2.p2.6.m4.1a"><mi id="S3.SS2.p2.6.m4.1.1" xref="S3.SS2.p2.6.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m4.1b"><ci id="S3.SS2.p2.6.m4.1.1.cmml" xref="S3.SS2.p2.6.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m4.1d">italic_c</annotation></semantics></math> from <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.SS2.p2.11.1">S</span> and <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.SS2.p2.11.2">D</span>, <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m7.1"><semantics id="S3.SS2.p2.9.m7.1a"><mi id="S3.SS2.p2.9.m7.1.1" xref="S3.SS2.p2.9.m7.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m7.1b"><ci id="S3.SS2.p2.9.m7.1.1.cmml" xref="S3.SS2.p2.9.m7.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m7.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m7.1d">italic_Î³</annotation></semantics></math> is usersâ€™ <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.11.3">codes</span>, <math alttext="||.||_{F}" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.10.m8.1"><semantics id="S3.SS2.p2.10.m8.1a"><mrow id="S3.SS2.p2.10.m8.1b"><mo fence="false" id="S3.SS2.p2.10.m8.1.1" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S3.SS2.p2.10.m8.1.2" stretchy="false">|</mo><mo id="S3.SS2.p2.10.m8.1.3" lspace="0.167em" rspace="0.167em">.</mo><mo fence="false" id="S3.SS2.p2.10.m8.1.4" rspace="0.167em" stretchy="false">|</mo><msub id="S3.SS2.p2.10.m8.1.5"><mo fence="false" id="S3.SS2.p2.10.m8.1.5.2" stretchy="false">|</mo><mi id="S3.SS2.p2.10.m8.1.5.3">F</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m8.1c">||.||_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.10.m8.1d">| | . | | start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> is the Frobenius matrix norm, and <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p2.11.m9.1"><semantics id="S3.SS2.p2.11.m9.1a"><mi id="S3.SS2.p2.11.m9.1.1" xref="S3.SS2.p2.11.m9.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m9.1b"><ci id="S3.SS2.p2.11.m9.1.1.cmml" xref="S3.SS2.p2.11.m9.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m9.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.11.m9.1d">italic_Î»</annotation></semantics></math> is the regularization parameter.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.7">While detecting patterns in <span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.SS2.p3.7.1">D</span>, the approximate resolution of this optimization program extracts user embeddings <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_Î³</annotation></semantics></math>, which maps signals onto the dictionary under a sparsity constraint.
With the chosen normalization, all signals are left to be in <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.2"><semantics id="S3.SS2.p3.3.m3.2a"><mrow id="S3.SS2.p3.3.m3.2.2.1" xref="S3.SS2.p3.3.m3.2.2.2.cmml"><mo id="S3.SS2.p3.3.m3.2.2.1.2" stretchy="false" xref="S3.SS2.p3.3.m3.2.2.2.cmml">[</mo><mrow id="S3.SS2.p3.3.m3.2.2.1.1" xref="S3.SS2.p3.3.m3.2.2.1.1.cmml"><mo id="S3.SS2.p3.3.m3.2.2.1.1a" xref="S3.SS2.p3.3.m3.2.2.1.1.cmml">âˆ’</mo><mn id="S3.SS2.p3.3.m3.2.2.1.1.2" xref="S3.SS2.p3.3.m3.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS2.p3.3.m3.2.2.1.3" xref="S3.SS2.p3.3.m3.2.2.2.cmml">,</mo><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">1</mn><mo id="S3.SS2.p3.3.m3.2.2.1.4" stretchy="false" xref="S3.SS2.p3.3.m3.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.2b"><interval closure="closed" id="S3.SS2.p3.3.m3.2.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2.1"><apply id="S3.SS2.p3.3.m3.2.2.1.1.cmml" xref="S3.SS2.p3.3.m3.2.2.1.1"><minus id="S3.SS2.p3.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.p3.3.m3.2.2.1.1"></minus><cn id="S3.SS2.p3.3.m3.2.2.1.1.2.cmml" type="integer" xref="S3.SS2.p3.3.m3.2.2.1.1.2">1</cn></apply><cn id="S3.SS2.p3.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.2d">[ - 1 , 1 ]</annotation></semantics></math> interval. We therefore expect the atoms learned on these signals to capture intra-user variations rather than general inter-user tendencies, at least more strongly than with a <math alttext="L_{2}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">ğ¿</ci><cn id="S3.SS2.p3.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p3.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">L_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> normalization.
By recovering <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_Î³</annotation></semantics></math> after the optimization process, we obtain an association of the usersâ€™ consumption histories with the surfaced trends, thus we employ these associations as user embeddings, as depicted on FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S3.F2" title="Figure 2 â€£ 3.2. Detecting Listening Behavior Patterns â€£ 3. Methodology â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_tag">2</span></a>.
The sparsity constraint controls the <math alttext="L_{1}" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">L</mi><mn id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">ğ¿</ci><cn id="S3.SS2.p3.6.m6.1.1.3.cmml" type="integer" xref="S3.SS2.p3.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> norm of the embeddings, so a high <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">italic_Î»</annotation></semantics></math> coefficient will tend to specialize certain atoms for the reconstruction of particular user signals.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The choice of the number of atoms, the number of iterations of the resolution algorithm, and <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_Î»</annotation></semantics></math> value can depend on the task one wants to focus on, for example a trade-off can be made between reconstruction scores, atom understandability, and a task-related score on a validation set.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Creating a specific userâ€™s embedding.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S3.F2.g1" src="x2.png" width="373"/>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Framework evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Now that the PACE framework is in place, our attention turns to its evaluation in the context of activity-driven listening prediction. If the learned embeddings successfully capture recurrent consumption patterns, we can employ them to deduce the various recurring activities that influence listening sessions. We can do so as a result of a survey given to the Deezer users, asking them about the activities they engage in during listening. We pay particular attention to relating the results obtained to the <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">a priori</span> regularity and frequency of the activities performed.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In this section, the exact number of atoms is set by practical considerations. Indeed, in order to keep a large enough panel of detected behaviors, without losing ourselves in too many patterns to analyze, we compute 32 atoms.
We implement both PACE framework and the following evaluation part in Python and make code available online<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://github.com/deezer/modeling_activity_pace" title="">PACE source code : https://github.com/deezer/modeling_activity_pace</a></span></span></span>, where the framework parameters are detailed.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The dataset used is sourced from the research project RECORDS<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://records.huma-num.fr/" title="">RECORDS web page : https://records.huma-num.fr/</a></span></span></span>, funded by the French National Agency for Research, bringing together researchers from diverse backgrounds, studying practices on music streaming platforms.
Over a period of several weeks, surveys were sent to Deezer users by their email address, asking a wide range of questions about their cultural practices, musical tastes and socio-demographic variables.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">One of the asked questions was: â€œIn what contexts do you regularly listen to music?â€.
The users could then select multiple answers including: <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">Waking up</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">wake up</span>), <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">When playing sports</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.4">sports</span>), <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.5">On public transport (excluding driving)</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.6">transport.</span>), <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.7">When working (including studying)</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.8">work</span>), <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.9">Just before going to sleep</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.10">asleep</span>), and <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.11">When receiving friends</span> (<span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.12">friends</span>). Other activities are proposed, but we choose to focus on these 6 examples as they have a significant rate of positive answers, and are expected to be practiced with various regularities and frequencies, allowing us to test the effectiveness of our models.
Overall, there are around 10k respondents, which we reduce to 7k, removing the less active ones (less than 6 streams per day on average), for whom listening patterns are harder to detect.
Among the proposed answers, <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.13">wake up</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.14">asleep</span> are the less frequent activities (respectively 18% and 15%), while the others concern between one-third and one-half of the respondents (38% for <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.15">transport.</span>, 39% for <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.16">work</span>, 50% for <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.17">sports</span>, and 47% for <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.18">friends</span>). In comparison with the study carried out in <cite class="ltx_cite ltx_citemacro_citep">(Volokhin and Agichtein, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib22" title="">2018</a>)</cite>, respondents are far more frequent users of activity-driven listening.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Our evaluation task consists in predicting the answers of the survey from the user embeddings.
Thus, we frame the problem as 6 binary classifications, one per activity, each classification targeting whether the user has declared to practice the given activity or not.
The implemented classifiers are logistic regressions models, optimized with 5 splits GridSearchCV. We evaluate the performance by the ROC AUC score on a test set (<math alttext="33\%" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">33</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1">percent</csymbol><cn id="S4.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.2">33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">33\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">33 %</annotation></semantics></math> of the whole dataset, that is not involved in the learning of atoms).</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We note that the pool of respondents is mainly made up of French users, with no significant gender bias, with the mean age being 32 years old (SD = 14). RECORDS researchers aim to publishing both anonymized listening histories and survey answers in open access during 2024.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Classification Scores</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To the best of our knowledge, there are not many studies that attempt to predict activities from listening histories like ours.
Therefore, we build several baselines to serve as complexity assessment and to position our model.
The baseline models, based on the same modeling architecture but with training data capturing other types of information, are described hereafter: <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">(1)</span>Â <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Total Volume</span>: from the logs, we retain the total number of streams performed during the covered period (1-dimension vectors); <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">(2)</span>Â <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">Gender &amp; age</span>: we encode the age group and the gender of the respondent (respectively coded in five and three categories, leading to 2-dimensions vectors); <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.5">(3)</span>Â <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.6">Other Activities</span>: for each user, we form a binary vector from the <math alttext="6" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">6</annotation></semantics></math> activity answers, then removing the activity we aim to predict (5-dimensions vectors).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Against those baselines, we evaluate our embeddings <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">(a)</span>, that we enrich concatenating vectors with <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">Gender &amp; age</span> <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.3">(b)</span>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>ROC AUC test scores of trained models.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1" style="font-size:90%;">wake up</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1" style="font-size:90%;">transport</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1" style="font-size:90%;">work</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1" style="font-size:90%;">sports</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.6.1" style="font-size:90%;">friends</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.7.1" style="font-size:90%;">asleep</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1.1">(1)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">0.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.6">0.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.7">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1">(2)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">0.60</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.6">0.51</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.7">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.1.1">(3)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.2.1">0.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.3.1">0.78</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.4.1">0.74</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.5.1">0.78</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.6.1">0.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.7.1">0.78</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.1.1">(a)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.2">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.3">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.4">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.5">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.6">0.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.5.4.7">0.73</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.5.1.1">(b)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.2">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.3">0.71</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.4">0.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.5">0.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.6">0.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.6.5.7">0.74</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.2">As a reference, itâ€™s worth noting that a random variable following a <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.1">well-informed</span> binomial distribution (with parameter <math alttext="p" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_p</annotation></semantics></math> equals to the proportion of positive responses for the label to predict) yields a score of <math alttext="0.5" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn id="S4.SS2.p3.2.m2.1.1.cmml" type="float" xref="S4.SS2.p3.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">0.5</annotation></semantics></math>. Almost all the baselines are strictly above this value, showing their relevance to the task. However, in general, baselines <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.2">Total Volume</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.3">Gender &amp; age</span> show relatively low scores, while <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.4">Other Activities</span> baseline is particularly strong.
This high performance is not surprising, since there is some homogeneity in the user profiles that perform the same activities due to confounding factors such as age.
The training data thus form an activity-driven listening profile, and represent information very similar to the target.
Also, whether derived from collected (<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.5">Total Volume</span>), or declarative data (<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.6">Gender &amp; age</span>, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.2.7">Other Activities</span>), the performance of baselines remains quite similar across all labels.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">In general, PACE shows intermediate performance between <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">Total Volume</span> or <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.2">Gender &amp; age</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.3">Other Activities</span>, which demonstrates that our embeddings capture a significant proportion of the information contained in the activity profile.
Contrary to baseline models, ours reveal greater disparities in performance among the labels.
For instance, the most challenging label to predict from our embeddings is <span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.4">sports</span>.
This weakness can be understood given the regularity focus of PACE, as sports activities are typically less regular and frequent compared to activities such as falling asleep (most people do it daily).</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Incorporating the <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.1">Genre &amp; Age</span> variables into the embeddings typically leads to improved performance compared to the two models taken separately.
This demonstrates a certain complementarity between the sociological profile and the listening profile.
Specifically, the labels <span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.2">transport.</span>, and <span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.3">work</span> benefit the most from this association.
This can be explained as these activities are closely associated with the working population in urban areas who commute to work daily.
This user profile, which is likely prevalent in the dataset, may not be present in all age groups (e.g., among those under 18 who are still students, or retirees), emphasizing the importance of accounting for a sociological profile.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Model Interpretation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Logistic regression assigns coefficients to each label for every variable within the training vectors, i.e., each atom. The sign of these coefficients signifies whether the atomâ€™s impact is positive or negative, while their absolute values indicate the magnitude of their influence. Analyzing these coefficients enables us to gain insights into the predictive models. Specifically, linking these atoms to our understanding of preferred time slots for various activities can serve as a validity check.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Logistic Regression coefficients of the model based purely on PACE embeddings.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="123" id="S4.F3.g1" src="x3.png" width="390"/>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.3">To do so, we can look for atoms having a positive coefficient for a single or few labels.
Such atoms would identify the typical listening behaviors of the population performing the activity under consideration.
To this end, Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4.F3" title="Figure 3 â€£ 4.3. Model Interpretation â€£ 4. Framework evaluation â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_tag">3</span></a> suggests matching, among others, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">atom <math alttext="0" class="ltx_Math" display="inline" id="S4.SS3.p2.1.1.m1.1"><semantics id="S4.SS3.p2.1.1.m1.1a"><mn id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><cn id="S4.SS3.p2.1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p2.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></span> for <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.3.4">transport.</span>, <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.2.2">atom <math alttext="2" class="ltx_Math" display="inline" id="S4.SS3.p2.2.2.m1.1"><semantics id="S4.SS3.p2.2.2.m1.1a"><mn id="S4.SS3.p2.2.2.m1.1.1" xref="S4.SS3.p2.2.2.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.2.m1.1b"><cn id="S4.SS3.p2.2.2.m1.1.1.cmml" type="integer" xref="S4.SS3.p2.2.2.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.2.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.2.m1.1d">2</annotation></semantics></math></span> for <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.3.5">work</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.3.3">atom <math alttext="27" class="ltx_Math" display="inline" id="S4.SS3.p2.3.3.m1.1"><semantics id="S4.SS3.p2.3.3.m1.1a"><mn id="S4.SS3.p2.3.3.m1.1.1" xref="S4.SS3.p2.3.3.m1.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.3.m1.1b"><cn id="S4.SS3.p2.3.3.m1.1.1.cmml" type="integer" xref="S4.SS3.p2.3.3.m1.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.3.m1.1c">27</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.3.m1.1d">27</annotation></semantics></math></span> for <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.3.6">friends</span>.
We display those atoms in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#S4.F4" title="Figure 4 â€£ 4.3. Model Interpretation â€£ 4. Framework evaluation â€£ Modeling Activity-Driven Music Listening with PACE"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Examples of detected listening patterns: atoms 0, 2 and 27.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="118" id="S4.F4.g1" src="x4.png" width="390"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="118" id="S4.F4.g2" src="x5.png" width="390"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="118" id="S4.F4.g3" src="x6.png" width="390"/></div>
</div>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">Atom 0</span> shows peaks of activity on mornings and evenings of working days for <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.2">volume</span> channel, which seems consistent with listening while commuting. For <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.3">atom 2</span>, the listening slots are concentrated on working days, and fit well with conventional working hours.
In addition, there are small peaks in the mornings and evenings, suggesting also listening during commuting as in <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.4">atom 0</span>, which pairs well with weekly workersâ€™ patterns.
Furthermore, the <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.5">organicity</span> channel shows that listening at work is predominantly organic, whereas on weekend evenings, listening is much more algorithmic, revealing perhaps festive contexts with much less engagement. Lastly, <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.6">atom 27</span> is dominated by channel <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.7">volume</span>, with a lot of streams during the weekend and in particular on Friday and Saturday evenings, which coincides with typical festive times with friends.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Generally speaking, in the example above, the atoms highlighted by the logistic regression coefficients are in line with our knowledge regarding the privileged times for practicing the different activities. We were also able to confirm the consistency of the highlighted time slots with the listening data of Deezer playlists dedicated to specific activities (partying, work, sport).
This suggests that PACE successfully achieves its purpose, but also shows great promise for picturing more precisely the behaviors involved in soundtracking by taking advantage of the several information channels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduced PACE, a user embedding method based on regularity in music consumption. With the PACE framework, usersâ€™ consumption logs are represented by weekly time series, aggregating 4 signals, each representing a specific aspect of user behavior. With dictionary learning, understandable typical listening behaviors were extracted, forming a projection base of fixed size. The projections thus form user embeddings whose composition is easy to understand, since each coefficient represents the part taken by a particular atom in the reconstruction of the signal.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">To validate our approach, we focus on activity-driven listening.
Matching logs with declared activities allowed us to confirm the relevance of PACE, insofar as the most regular activities were the easiest to predict.
Furthermore, studying the surfaced atoms and their relationship with the predicted labels enables us to unveil valuable insights about the nature of music consumption.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In <cite class="ltx_cite ltx_citemacro_citep">(Yang and Capra, <a class="ltx_ref" href="https://arxiv.org/html/2405.01417v1#bib.bib25" title="">2023</a>)</cite> perspective, PACE takes advantage of MIR context, and proves its efficiency in predicting a situational context (activity).
As a way to specialize embeddings for capturing regularity information, it would be relevant to study PACE in relation to other areas of musical listening practices, such as the diversity of content listened to, or appetite for new music.
For example, future work should investigate how the PACE embeddings could be integrated to recommender systems to improve contextual recommendation.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">To progress on this task, a straightforward future work would be to integrate channels related to the listened content. This would facilitate the modeling of activities that are more connected to, say, a specific music genre. PACE is flexible enough to the possibility of integrating other information channels, which could provide new insights regarding regular listening behavior, but also be adapted to a multitude of downstream tasks where different information is needed and PACE might be of use.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This paper has been partially realized in the framework of the â€œRECORDSâ€ grant (ANR-2019-CE38-0013) funded by the ANR (French National Agency of Research)</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adomavicius and Tuzhilin (2010)</span>
<span class="ltx_bibblock">
Gediminas Adomavicius and Alexander Tuzhilin. 2010.

</span>
<span class="ltx_bibblock">Context-aware recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Recommender systems handbook</em>. Springer, 217â€“253.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ashton Anderson, Ravi Kumar, Andrew Tomkins, and Sergei Vassilvitskii. 2014.

</span>
<span class="ltx_bibblock">The dynamics of repeat consumption. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 23rd international conference on World wide web</em>. 419â€“430.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benson etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
AustinÂ R Benson, Ravi Kumar, and Andrew Tomkins. 2016.

</span>
<span class="ltx_bibblock">Modeling user consumption sequences. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 25th International Conference on World Wide Web</em>. 519â€“529.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braunhofer etÂ al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Matthias Braunhofer, Marius Kaminskas, and Francesco Ricci. 2013.

</span>
<span class="ltx_bibblock">Location-aware music recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">International Journal of Multimedia Information Retrieval</em> 2 (2013), 31â€“44.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng and Shen (2016)</span>
<span class="ltx_bibblock">
Zhiyong Cheng and Jialie Shen. 2016.

</span>
<span class="ltx_bibblock">On effective location-aware music recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Transactions on Information Systems (TOIS)</em> 34, 2 (2016), 1â€“32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Datta etÂ al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Hannes Datta, George Knox, and BartÂ J Bronnenberg. 2018.

</span>
<span class="ltx_bibblock">Changing their tune: How consumersâ€™ adoption of online streaming affects music consumption and discovery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Marketing Science</em> 37, 1 (2018), 5â€“21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dias etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ricardo Dias, ManuelÂ J Fonseca, and Ricardo Cunha. 2014.

</span>
<span class="ltx_bibblock">A User-centered Music Recommendation Approach for Daily Activities.. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">CBRecSys@ RecSys</em>. Citeseer, 26â€“33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DuprÃ©Â la Tour etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tom DuprÃ©Â la Tour, Thomas Moreau, Mainak Jas, and Alexandre Gramfort. 2018.

</span>
<span class="ltx_bibblock">Multivariate convolutional sparse coding for electromagnetic brain signals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Advances in Neural Information Processing Systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fuentes etÂ al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Christian Fuentes, Johan Hagberg, and Hans Kjellberg. 2019.

</span>
<span class="ltx_bibblock">Soundtracking: music listening practices in the digital age.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">European Journal of Marketing</em> 53, 3 (2019), 483â€“503.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginoux etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
ClÃ©ment Ginoux, Sandrine Isoard-Gautheur, and Philippe Sarrazin. 2020.

</span>
<span class="ltx_bibblock">â€œWhat did you do this weekend?â€ Relationships between weekend activities, recovery experiences and changes in work-related well-being.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen etÂ al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Casper Hansen, Christian Hansen, Lucas Maystre, Rishabh Mehrotra, Brian Brost, Federico Tomasi, and Mounia Lalmas. 2020.

</span>
<span class="ltx_bibblock">Contextual and sequential user embeddings for large-scale music recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 14th ACM Conference on Recommender Systems</em>. 53â€“62.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ibrahim etÂ al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
KarimÂ M Ibrahim, ElenaÂ V Epure, Geoffroy Peeters, and Gael Richard. 2022.

</span>
<span class="ltx_bibblock">Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv preprint arXiv:2211.07250</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ibrahim etÂ al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
KarimÂ M Ibrahim, Jimena Royo-Letelier, ElenaÂ V Epure, Geoffroy Peeters, and Gael Richard. 2020.

</span>
<span class="ltx_bibblock">Audio-based auto-tagging with contextual tags for music. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 16â€“20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mairal etÂ al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. 2009.

</span>
<span class="ltx_bibblock">Online dictionary learning for sparse coding. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 26th annual international conference on machine learning</em>. 689â€“696.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">North etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
AdrianÂ C North, DavidÂ J Hargreaves, and JonÂ J Hargreaves. 2004.

</span>
<span class="ltx_bibblock">Uses of music in everyday life.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Music perception</em> 22, 1 (2004), 41â€“77.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oh etÂ al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sejoon Oh, Ankur Bhardwaj, Jongseok Han, Sungchul Kim, RyanÂ A Rossi, and Srijan Kumar. 2022.

</span>
<span class="ltx_bibblock">Implicit session contexts for next-item recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 4364â€“4368.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sen and Larson (2015)</span>
<span class="ltx_bibblock">
Abhishek Sen and MarthaÂ A Larson. 2015.

</span>
<span class="ltx_bibblock">From Sensors to Songs: A Learning-Free Novel Music Recommendation System using Contextual Sensor Data.. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">LocalRec@ RecSys</em>. 40â€“43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sguerra etÂ al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bruno Sguerra, Viet-Anh Tran, and Romain Hennequin. 2022.

</span>
<span class="ltx_bibblock">Discovery Dynamics: Leveraging Repeated Exposure for User and Music Characterization. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 16th ACM Conference on Recommender Systems</em>. 556â€“561.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sguerra etÂ al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bruno Sguerra, Viet-Anh Tran, and Romain Hennequin. 2023.

</span>
<span class="ltx_bibblock">Ex2Vec: Characterizing Users and Items from the Mere Exposure Effect. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 971â€“977.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor and Letham (2018)</span>
<span class="ltx_bibblock">
SeanÂ J Taylor and Benjamin Letham. 2018.

</span>
<span class="ltx_bibblock">Forecasting at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">The American Statistician</em> 72, 1 (2018), 37â€“45.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Volokhin and Agichtein (2018)</span>
<span class="ltx_bibblock">
Sergey Volokhin and Eugene Agichtein. 2018.

</span>
<span class="ltx_bibblock">Understanding Music Listening Intents During Daily Activities with Implications for Contextual Music Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2018 Conference on Human Information Interaction&amp;Retrieval - CHIIR â€™18</em>. ACM Press, New Brunswick, NJ, USA, 313â€“316.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3176349.3176885" title="">https://doi.org/10.1145/3176349.3176885</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Chen-Ya Wang, Yu-Chi Wang, and Seng-ChoÂ T Chou. 2018.

</span>
<span class="ltx_bibblock">A context and emotion aware system for personalized music recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Journal of Internet Technology</em> 19, 3 (2018), 765â€“779.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2020)</span>
<span class="ltx_bibblock">
Justin Wang. 2020.

</span>
<span class="ltx_bibblock">Lofi hip-hop radio: Beats to relax/study to.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">The Word: Tha Stanford Journal of Student Hiphop Research</em> 1, 1 (2020), 10â€“23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Capra (2023)</span>
<span class="ltx_bibblock">
Yuyu Yang and Rob Capra. 2023.

</span>
<span class="ltx_bibblock">Nested Contexts of Music Information Retrieval: A Framework of Contextual Factors. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2023 Conference on Human Information Interaction and Retrieval</em>. 368â€“372.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu May  2 16:04:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
