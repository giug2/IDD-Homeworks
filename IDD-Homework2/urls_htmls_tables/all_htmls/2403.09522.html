<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation</title>
<!--Generated on Thu May  2 19:00:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.09522v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S1" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S2" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S2.SS0.SSS0.Px1" title="In 2 Background ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Large Language Model for Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S2.SS0.SSS0.Px2" title="In 2 Background ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Knowledge Distillation for Neural Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S2.SS0.SSS0.Px3" title="In 2 Background ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Large Language Model for Synthesizing Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3.SS1" title="In 3 Methodology ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Knowledge Selection via <span class="ltx_text ltx_font_italic">Feedbacker</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3.SS2" title="In 3 Methodology ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Knowledge Extension via <span class="ltx_text ltx_font_italic">Parallel Data Synthesizer and Word Analoger</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3.SS2.SSS0.Px1" title="In 3.2 Knowledge Extension via Parallel Data Synthesizer and Word Analoger ‣ 3 Methodology ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Parallel Data Synthesizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3.SS2.SSS0.Px2" title="In 3.2 Knowledge Extension via Parallel Data Synthesizer and Word Analoger ‣ 3 Methodology ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Word Analoger</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S3.SS3" title="In 3 Methodology ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Implementation of <span class="ltx_text ltx_font_smallcaps">MT-Patcher</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS1" title="In 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Student Translation Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Backbone LLM for <span class="ltx_text ltx_font_smallcaps">MT-Patcher</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Competitors</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS2" title="In 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results on General Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS2.SSS0.Px1" title="In 4.2 Results on General Machine Translation ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">MT-Patcher</span> can select more valuable examples.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS2.SSS0.Px2" title="In 4.2 Results on General Machine Translation ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Parallel data synthesizer and word analoger improve the effectiveness of <span class="ltx_text ltx_font_smallcaps">MT-Patcher</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS2.SSS0.Px3" title="In 4.2 Results on General Machine Translation ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"></span> ‣ <span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results on General Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS2.SSS0.Px4" title="In 4.2 Results on General Machine Translation ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">MT-Patcher</span> also works when the teacher is not very strong.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS3" title="In 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results on Specific Language Phenomena</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS3.SSS0.Px1" title="In 4.3 Results on Specific Language Phenomena ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Multiple contexts facilitate generalization on <span class="ltx_text ltx_font_italic">Unseen Context</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.SS3.SSS0.Px2" title="In 4.3 Results on Specific Language Phenomena ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Error Anticipation improves performances on <span class="ltx_text ltx_font_italic">Unseen Word</span>.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.SS1" title="In 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Impact of the number of synthesized contexts per word and analogous word</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.SS2" title="In 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Does asking for feedback better elicit LLMs’ translation knowledge?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.SS3" title="In 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>The Effectiveness of Iterative Feedback</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.SS4" title="In 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Transferability of <span class="ltx_text ltx_font_smallcaps">MT-Patcher</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S6" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A1" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompts for <span class="ltx_text ltx_font_smallcaps">MT-Patcher</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A2" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A3" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span><span class="ltx_text ltx_font_smallcaps">MT-Patcher</span> suffers less from catastrophic forgetting.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A4" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of datasets used for chemistry materials and Chinese idioms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5" title="In MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Prompts for Evaluation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id7.id1">MT-Patcher</span>: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiahuan Li<sup class="ltx_sup" id="id8.4.id1"><span class="ltx_text ltx_font_italic" id="id8.4.id1.1">♣∗</span></sup>, Shanbo Cheng<math alttext="{}^{\spadesuit}\dagger" class="ltx_math_unparsed" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mmultiscripts id="id2.2.m2.1.1"><mo id="id2.2.m2.1.1.2">†</mo><mprescripts id="id2.2.m2.1.1a"></mprescripts><mrow id="id2.2.m2.1.1b"></mrow><mi id="id2.2.m2.1.1.3" mathvariant="normal">♠</mi></mmultiscripts><annotation encoding="application/x-tex" id="id2.2.m2.1b">{}^{\spadesuit}\dagger</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1c">start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT †</annotation></semantics></math>, Shujian Huang<math alttext="{}^{\clubsuit}\dagger" class="ltx_math_unparsed" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mmultiscripts id="id3.3.m3.1.1"><mo id="id3.3.m3.1.1.2">†</mo><mprescripts id="id3.3.m3.1.1a"></mprescripts><mrow id="id3.3.m3.1.1b"></mrow><mi id="id3.3.m3.1.1.3" mathvariant="normal">♣</mi></mmultiscripts><annotation encoding="application/x-tex" id="id3.3.m3.1b">{}^{\clubsuit}\dagger</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1c">start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT †</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiajun Chen<sup class="ltx_sup" id="id9.4.id1">♣</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.5.id2">♣</sup> National Key Laboratory for Novel Software Technology, Nanjing University, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id11.6.id3">♠</sup> ByteDance Research 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id12.7.id4">lijh@smail.nju.edu.cn, {huangsj,chenjj}@nju.edu.cn</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id13.8.id5">chengshanbo@bytedance.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called <span class="ltx_text ltx_font_smallcaps" id="id14.id1.1">MT-Patcher</span>, which transfers knowledge from LLMs to existing MT models in a <span class="ltx_text ltx_font_italic" id="id14.id1.2">selective, comprehensive and proactive</span> manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the student MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.6">
<p class="ltx_p" id="p1.6.7"><span class="ltx_text ltx_font_smallcaps" id="p1.6.7.1">MT-Patcher</span><span class="ltx_text ltx_font_bold" id="p1.6.7.2">: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.6.6" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.6.6.6" style="width:0.0pt;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p1.6.6.6.6">
<span class="ltx_thead">
<span class="ltx_tr" id="p1.4.4.4.4.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">
Jiahuan Li<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.3.3.3.3.3.3.3.1.1">♣∗</span></sup>, Shanbo Cheng<math alttext="{}^{\spadesuit}\dagger" class="ltx_math_unparsed" display="inline" id="p1.2.2.2.2.2.2.2.m2.1"><semantics id="p1.2.2.2.2.2.2.2.m2.1a"><mmultiscripts id="p1.2.2.2.2.2.2.2.m2.1.1"><mo id="p1.2.2.2.2.2.2.2.m2.1.1.2">†</mo><mprescripts id="p1.2.2.2.2.2.2.2.m2.1.1a"></mprescripts><mrow id="p1.2.2.2.2.2.2.2.m2.1.1b"></mrow><mi id="p1.2.2.2.2.2.2.2.m2.1.1.3" mathvariant="normal">♠</mi></mmultiscripts><annotation encoding="application/x-tex" id="p1.2.2.2.2.2.2.2.m2.1b">{}^{\spadesuit}\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.2.2.2.2.2.2.2.m2.1c">start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT †</annotation></semantics></math>, Shujian Huang<math alttext="{}^{\clubsuit}\dagger" class="ltx_math_unparsed" display="inline" id="p1.3.3.3.3.3.3.3.m3.1"><semantics id="p1.3.3.3.3.3.3.3.m3.1a"><mmultiscripts id="p1.3.3.3.3.3.3.3.m3.1.1"><mo id="p1.3.3.3.3.3.3.3.m3.1.1.2">†</mo><mprescripts id="p1.3.3.3.3.3.3.3.m3.1.1a"></mprescripts><mrow id="p1.3.3.3.3.3.3.3.m3.1.1b"></mrow><mi id="p1.3.3.3.3.3.3.3.m3.1.1.3" mathvariant="normal">♣</mi></mmultiscripts><annotation encoding="application/x-tex" id="p1.3.3.3.3.3.3.3.m3.1b">{}^{\clubsuit}\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.3.3.3.3.3.3.3.m3.1c">start_FLOATSUPERSCRIPT ♣ end_FLOATSUPERSCRIPT †</annotation></semantics></math>  </span>and<span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4"> Jiajun Chen<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.1.1">♣</span></sup></span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.1"><sup class="ltx_sup" id="p1.5.5.5.5.5.1.1">♣</sup> National Key Laboratory for Novel Software Technology, Nanjing University, China</span></span>
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.1"><sup class="ltx_sup" id="p1.6.6.6.6.6.1.1">♠</sup> ByteDance Research</span></span>
<span class="ltx_tr" id="p1.6.6.6.6.7.1">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.7.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.6.6.6.6.7.1.1.1">lijh@smail.nju.edu.cn, {huangsj,chenjj}@nju.edu.cn</span></span></span>
<span class="ltx_tr" id="p1.6.6.6.6.8.2">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.8.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.6.6.6.6.8.2.1.1">chengshanbo@bytedance.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><math alttext="*" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><mo id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><times id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">∗</annotation></semantics></math> Work done while Jiahuan Li’s internship at ByteDance Research.</span></span></span><span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><math alttext="\dagger" class="ltx_Math" display="inline" id="footnote1a.m1.1"><semantics id="footnote1a.m1.1b"><mo id="footnote1a.m1.1.1" xref="footnote1a.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="footnote1a.m1.1c"><ci id="footnote1a.m1.1.1.cmml" xref="footnote1a.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1a.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="footnote1a.m1.1e">†</annotation></semantics></math> Corresponding authors.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLM) have shown their impressive capabilities across almost all natural language tasks <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib3" title="">2020</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib38" title="">2023</a>)</cite>. However, their ability strongly correlates with the model size. In the field of machine translation, competitive results can only be evidenced on larger LLMs, while medium-sized LLMs like Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib30" title="">2023</a>)</cite> and ParroT <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib13" title="">2023a</a>)</cite> still lag behind supervised NMT systems by a large margin <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib13" title="">2023a</a>; Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib40" title="">2023</a>)</cite>. How to efficiently transfer knowledge from larger LLMs to existing MT models that are affordable to deploy, is an important research direction.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The most common method for knowledge transferring is knowledge distillation (KD) <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib11" title="">2015</a>; Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib16" title="">2016</a>)</cite>, where given an unlabeled corpus, a student model is trained to mimic the output of a teacher model on the corpus. Although KD is a well-studied technique and has proven effective in many previous works <cite class="ltx_cite ltx_citemacro_citep">(Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib16" title="">2016</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib33" title="">2021</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib20" title="">2023</a>)</cite>, we argue that when transferring knowledge from giant LLMs to existing MT models, the traditional KD method does not take the capability of the student and teacher model into consideration, therefore leaving much room for improvement in terms of both efficiency and effectiveness.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Firstly, in contrast to student models in previous works <cite class="ltx_cite ltx_citemacro_citep">(Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib16" title="">2016</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib33" title="">2021</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib20" title="">2023</a>)</cite> that are randomly initialized, recent student MT models <cite class="ltx_cite ltx_citemacro_citep">(Hsieh et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib12" title="">2023</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib8" title="">2023</a>)</cite> already exhibit a reasonable level of language proficiency, i.e., they can already accurately translate most examples in the unlabeled corpus. This renders the fine-tuning of student models on <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">all</span> teacher outputs both redundant and inefficient.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Secondly, the efficacy of KD is significantly constrained by the coverage of the monolingual corpus, which impedes their performance when translating words in novel contexts or words unseen in the monolingual corpus. However, modern LLMs grasp strong translation and language knowledge, as well as the ability to follow human instructions. This enables the development of more efficient and effective strategies for addressing these problems.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we introduce <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">MT-Patcher</span>, a novel framework designed for the knowledge transfer from LLMs to existing MT models in a <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">selective, comprehensive, and proactive</span> manner. The design philosophy of <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">MT-Patcher</span> is inspired by effective teaching strategies observed in real-world scenarios. Rather than subjecting students to endless drills, an effective teacher would first assess the student’s current abilities, then design practice to reinforce areas of weakness and extend learning to new situations <cite class="ltx_cite ltx_citemacro_citep">(Lee Jr and Pruitt, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib17" title="">1979</a>; Epstein and Voorhis, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib6" title="">2001</a>)</cite>. Leveraging the strong language capabilities of LLMs, our method seeks to emulate these pedagogical strategies. Specifically, we gather instructional data from GPT-4, which demonstrates how to identify and correct errors in student model translations, anticipate additional potential errors that the student models may commit, and synthesize diverse contexts for relevant translation knowledge that aids the student model in rectifying these errors. We subsequently fine-tune an existing proficient LLM on these data to transform it into an <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">MT-Patcher</span> model.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.3">We conduct experiments on translating specific language phenomena (chemistry materials and Chinese idioms) and on general machine translation benchmarks (WMT22 Chinese <math alttext="\to" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" stretchy="false" xref="S1.p6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">→</annotation></semantics></math> English, English <math alttext="\to" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mo id="S1.p6.2.m2.1.1" stretchy="false" xref="S1.p6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><ci id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">→</annotation></semantics></math> German and English <math alttext="\to" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mo id="S1.p6.3.m3.1.1" stretchy="false" xref="S1.p6.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><ci id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">→</annotation></semantics></math> Japanese). Experimental results show that finetuning the student model on only 10% examples selected by <span class="ltx_text ltx_font_smallcaps" id="S1.p6.3.1">MT-Patcher</span> is equivalent to finetuning on all examples as in KD, and enlarging the finetuning corpus via the context synthesis and proactive error prediction technique further improves the translation performance.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Large Language Model for Machine Translation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Numerous studies have attempted to leverage LLMs for machine translation. Initial efforts <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib19" title="">2022</a>; Vilar et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib32" title="">2022</a>; Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib1" title="">2023</a>; Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib40" title="">2023</a>; Hendy et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib10" title="">2023</a>; Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib14" title="">2023b</a>)</cite> centered on in-context learning, which utilizes several translation examples to guide the translation behavior of LLMs. Subsequent research <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib13" title="">2023a</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib18" title="">2023</a>)</cite> shifted the focus to fine-tuning LLMs on existing parallel corpora to more effectively harness their translation capabilities. However, the translation performance of LLMs has not been as remarkable as their performance in other NLP tasks. Only state-of-the-art LLMs such as GPT-3 and GPT-4, which boast more than 100 billion parameters, can rival the performance of commercial translation systems <cite class="ltx_cite ltx_citemacro_citep">(Hendy et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib10" title="">2023</a>; Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib14" title="">2023b</a>)</cite>. Meanwhile, other medium-sized LLMs significantly trail behind supervised MT models <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib40" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib18" title="">2023</a>; Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib13" title="">2023a</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib18" title="">2023</a>)</cite> suggest that the primary barrier to enhancing LLMs’ performance is the lack of translation knowledge. Given that larger LLMs inherently possess more knowledge due to the scaling law <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib15" title="">2020</a>)</cite>, our work concentrates on transferring knowledge from these models to existing MT models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Knowledge Distillation for Neural Machine Translation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Knowledge distillation (KD), which improves smaller <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">student</span> models by learning on larger <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.2">teacher</span> models’ output, is widely used in machine translation. Two common KD methods are LogitKD <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib11" title="">2015</a>; Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib29" title="">2018</a>)</cite>, which optimizes the student model to match the teacher model’s predicted distribution, and Sequence KD (SeqKD) <cite class="ltx_cite ltx_citemacro_citep">(Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib16" title="">2016</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib33" title="">2021</a>; Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib9" title="">2018</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib39" title="">2019</a>)</cite>, where the student learns from the teacher-generated pseudo target sequence. As LogitKD requires access to the teacher’s logits, it is impractical for distilling from proprietary LLMs. Therefore, we base our method on SeqKD, where student refers the smaller MT model we would like to improve, and teacher refers to larger LLMs which possess more translation knowledge than student.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Selective KD has been proposed by <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib33" title="">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib20" title="">2023</a>)</cite>, but they all rely on comparing student models’ outputs to oracle references. Unlike these works, our method instructs the LLM to identify student translation errors directly.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Large Language Model for Synthesizing Datasets</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">With the growing generative capabilities of Large Language Models (LLMs), many works attempt to harness them for corpora generation. The generated corpora can serve as demonstrations for few-shot prompting <cite class="ltx_cite ltx_citemacro_citep">(Sahu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib27" title="">2022</a>)</cite>, fine-tuning corpora for existing models <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib34" title="">2021</a>)</cite>, or seed corpora for human refinement <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib36" title="">2021a</a>)</cite>. Studies such as <cite class="ltx_cite ltx_citemacro_citet">Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib5" title="">2023</a>); Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib35" title="">2023</a>)</cite> also explore ways to balance diversity, accuracy, and bias reduction in LLM-based dataset synthesis. However, these approaches often generate datasets from scratch, ignoring the capabilities of the models being optimized, resulting in less efficiency compared to our method.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S2.F1.g1" src="extracted/2403.09522v2/figs/arch.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The illustration of <span class="ltx_text ltx_font_smallcaps" id="S2.F1.2.1">MT-Patcher</span> framework. The correct translation for the source sentence should be ‘Methanol is a colorless transparent liquid.’.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">MT-Patcher</span> , a framework that distills knowledge from LLMs to existing MT systems more efficiently and effectively. The process of <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">MT-Patcher</span> undergoes two stages:</p>
</div>
<div class="ltx_para" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Knowledge Selection</span>: In this stage, the LLM acts as the <span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.2">feedbacker</span>, which provides natural language feedback to translations of student models. Based on the
feedback, we select source sentences with identified errors, which indicate knowledge deficiency of the student models, to the next stage.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Knowledge Extension</span>: In this stage, the LLM acts as the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">parallel data synthesizer</span> and <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.3">word analoger</span>, which help the student model learn words it makes mistakes on by extending to more diverse contexts and similar words.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S2.F1" title="Figure 1 ‣ Large Language Model for Synthesizing Datasets ‣ 2 Background ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates how <span class="ltx_text ltx_font_smallcaps" id="S3.p3.1.1">MT-Patcher</span> works.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Knowledge Selection via <span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">Feedbacker</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">When transferring knowledge from LLMs to existing MT models, traditional SeqKD would finetune the student model on <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">all</span> teacher’s output, ignoring the fact that the student model can already translate most of the examples well. Furthermore, several recent studies have unveiled emergent abilities in LLMs, such as <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">Self-Refinement</span> <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib21" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Self-Debug</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib4" title="">2024</a>)</cite>, suggesting that iterative refinement of an initial draft may be a more effective strategy to tap into the knowledge reserves of LLMs.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.9">To improve the efficiency of SeqKD and better elicit LLMs’ knowledge, we propose to finetune LLMs to be a <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.9.1">feedbacker</span>, which produces natural language feedback of the student models’ translation instead of directly generating its own translations.
Formally, given a source sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_X</annotation></semantics></math> and its corresponding translation <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_Y</annotation></semantics></math>, the goal of the feedbacker is to generate a comprehensive assessment <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_f</annotation></semantics></math>. This assessment comprises tuples of <math alttext="(c,\{(s_{i},e_{i},t_{i})\}_{i=1}^{N},p)" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.3"><semantics id="S3.SS1.p2.4.m4.3a"><mrow id="S3.SS1.p2.4.m4.3.3.1" xref="S3.SS1.p2.4.m4.3.3.2.cmml"><mo id="S3.SS1.p2.4.m4.3.3.1.2" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.2.cmml">(</mo><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">c</mi><mo id="S3.SS1.p2.4.m4.3.3.1.3" xref="S3.SS1.p2.4.m4.3.3.2.cmml">,</mo><msubsup id="S3.SS1.p2.4.m4.3.3.1.1" xref="S3.SS1.p2.4.m4.3.3.1.1.cmml"><mrow id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.2.cmml"><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.2.cmml">{</mo><mrow id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml"><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.4" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml">(</mo><msub id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.5" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.2.cmml">e</mi><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.6" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.2.cmml">t</mi><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.7" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml">)</mo></mrow><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.4.m4.3.3.1.1.1.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.3.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.3.1" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.4.m4.3.3.1.1.1.3.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.4.m4.3.3.1.1.3" xref="S3.SS1.p2.4.m4.3.3.1.1.3.cmml">N</mi></msubsup><mo id="S3.SS1.p2.4.m4.3.3.1.4" xref="S3.SS1.p2.4.m4.3.3.2.cmml">,</mo><mi id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml">p</mi><mo id="S3.SS1.p2.4.m4.3.3.1.5" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.3b"><vector id="S3.SS1.p2.4.m4.3.3.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑐</ci><apply id="S3.SS1.p2.4.m4.3.3.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1">superscript</csymbol><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1">subscript</csymbol><set id="S3.SS1.p2.4.m4.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1"><vector id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.4.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3"><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.2">𝑒</ci><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></vector></set><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3"><eq id="S3.SS1.p2.4.m4.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.1"></eq><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.2">𝑖</ci><cn id="S3.SS1.p2.4.m4.3.3.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.4.m4.3.3.1.1.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.3">𝑁</ci></apply><ci id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">𝑝</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.3c">(c,\{(s_{i},e_{i},t_{i})\}_{i=1}^{N},p)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.3d">( italic_c , { ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT , italic_p )</annotation></semantics></math>, where <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_c</annotation></semantics></math> describes whether <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">italic_Y</annotation></semantics></math> contains translation errors, <math alttext="s_{i},e_{i},t_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.3"><semantics id="S3.SS1.p2.7.m7.3a"><mrow id="S3.SS1.p2.7.m7.3.3.3" xref="S3.SS1.p2.7.m7.3.3.4.cmml"><msub id="S3.SS1.p2.7.m7.1.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.1.1.2" xref="S3.SS1.p2.7.m7.1.1.1.1.2.cmml">s</mi><mi id="S3.SS1.p2.7.m7.1.1.1.1.3" xref="S3.SS1.p2.7.m7.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.7.m7.3.3.3.4" xref="S3.SS1.p2.7.m7.3.3.4.cmml">,</mo><msub id="S3.SS1.p2.7.m7.2.2.2.2" xref="S3.SS1.p2.7.m7.2.2.2.2.cmml"><mi id="S3.SS1.p2.7.m7.2.2.2.2.2" xref="S3.SS1.p2.7.m7.2.2.2.2.2.cmml">e</mi><mi id="S3.SS1.p2.7.m7.2.2.2.2.3" xref="S3.SS1.p2.7.m7.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS1.p2.7.m7.3.3.3.5" xref="S3.SS1.p2.7.m7.3.3.4.cmml">,</mo><msub id="S3.SS1.p2.7.m7.3.3.3.3" xref="S3.SS1.p2.7.m7.3.3.3.3.cmml"><mi id="S3.SS1.p2.7.m7.3.3.3.3.2" xref="S3.SS1.p2.7.m7.3.3.3.3.2.cmml">t</mi><mi id="S3.SS1.p2.7.m7.3.3.3.3.3" xref="S3.SS1.p2.7.m7.3.3.3.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.3b"><list id="S3.SS1.p2.7.m7.3.3.4.cmml" xref="S3.SS1.p2.7.m7.3.3.3"><apply id="S3.SS1.p2.7.m7.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.2">𝑠</ci><ci id="S3.SS1.p2.7.m7.1.1.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.7.m7.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.2.2.2.2.1.cmml" xref="S3.SS1.p2.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.2.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2.2.2.2">𝑒</ci><ci id="S3.SS1.p2.7.m7.2.2.2.2.3.cmml" xref="S3.SS1.p2.7.m7.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS1.p2.7.m7.3.3.3.3.cmml" xref="S3.SS1.p2.7.m7.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.3.3.3.3.1.cmml" xref="S3.SS1.p2.7.m7.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.7.m7.3.3.3.3.2.cmml" xref="S3.SS1.p2.7.m7.3.3.3.3.2">𝑡</ci><ci id="S3.SS1.p2.7.m7.3.3.3.3.3.cmml" xref="S3.SS1.p2.7.m7.3.3.3.3.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.3c">s_{i},e_{i},t_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.3d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> corresponds to the source span, explanation and correction of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">italic_i</annotation></semantics></math>-th identified error, respectively, and <math alttext="p" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.1"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.1d">italic_p</annotation></semantics></math> is the final post-edited translation that incorporates all error corrections.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Knowledge Extension via <span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Parallel Data Synthesizer and Word Analoger</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Another limitation of SeqKD is that the knowledge it can transfer is strictly limited to the given monolingual corpus. This limitation can hinder its generalizability in two key ways. Firstly, the correct translation of mistranslated words or phrases can only be learned within the contexts present in the given monolingual corpus, potentially limiting its applicability to broader contexts. Secondly, SeqKD also lacks the capacity for knowledge extrapolation, which prevents it from transferring knowledge that does not occur in the monolingual corpus.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Inspired by the principle of knowledge extension when designing good practice in the educational process <cite class="ltx_cite ltx_citemacro_citep">(Lee Jr and Pruitt, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib17" title="">1979</a>; Epstein and Voorhis, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib6" title="">2001</a>)</cite>, we transform LLMs into two modules to mitigate above two problems, respectively: <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">parallel data synthesizer</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">word analoger</span>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Parallel Data Synthesizer</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.5">The goal of the parallel data synthesizer is to synthesize parallel sentences <math alttext="(X^{\prime},Y^{\prime})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.2"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.2a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.3.cmml"><mo id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.3.cmml">(</mo><msup id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml">X</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.3.cmml">,</mo><msup id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml">Y</mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml">′</mo></msup><mo id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.5" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.2">𝑋</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.3">′</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.2">𝑌</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.3">′</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.2c">(X^{\prime},Y^{\prime})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.2d">( italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT )</annotation></semantics></math> that contain a specific pair of phrases <math alttext="(s,c)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.2"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.2.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.2.m2.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">s</mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.2.cmml">c</mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.2.m2.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">𝑠</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.2.2">𝑐</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.2c">(s,c)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.2d">( italic_s , italic_c )</annotation></semantics></math> where the student model makes mistakes in the context <math alttext="(X,Y)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.2"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.2a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.2.3.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">X</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml">Y</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.3.m3.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">𝑋</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.2.2">𝑌</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.2c">(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.2d">( italic_X , italic_Y )</annotation></semantics></math>, in order to generalize the current translation knowledge to more contexts. Ideally, the synthesized parallel sentences should be semantically diverse yet still similar to the original context in other aspects. However, in the preliminary experiments, we find that even for powerful LLMs like GPT-4, when conditioning them on the original context <math alttext="(X,Y)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.2"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">X</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.cmml">Y</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">𝑋</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2">𝑌</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.2c">(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.4.m4.2d">( italic_X , italic_Y )</annotation></semantics></math>, the generated parallel data lacks diversity and mostly resembles <math alttext="(X,Y)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m5.2"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.2a"><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.2.3.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p1.5.m5.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml">X</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m5.2.3.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m5.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.cmml">Y</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m5.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.3.2"><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">𝑋</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2">𝑌</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.2c">(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.5.m5.2d">( italic_X , italic_Y )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p2.1">To tackle this problem, we introduce another module called sentence analyzer, which first extracts the information of <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p2.1.1">domain, topic</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p2.1.2">style</span> of the original context. We then instruct the LLMs to synthesize parallel sentences with the same attributes as well as containing the phrase pair <math alttext="(s,c)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.1.m1.2"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.2a"><mrow id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml"><mo id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">s</mi><mo id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS0.Px1.p2.1.m1.2.2" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.2.cmml">c</mi><mo id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.2b"><interval closure="open" id="S3.SS2.SSS0.Px1.p2.1.m1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.3.2"><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">𝑠</ci><ci id="S3.SS2.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.2.2">𝑐</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.2c">(s,c)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p2.1.m1.2d">( italic_s , italic_c )</annotation></semantics></math>. This process can be seen as an information bottleneck where we squeeze the semantic information yet keep other attributes.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Word Analoger</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We further introduce the word analoger to proactively predict potential errors the student model may commit. For example, if the student MT model incorrectly translates the term <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.1">methanol</span>, an educated guess is that it may struggle with translating words within the domain of chemistry, such as <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.2">benzene</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.3">ethanol</span>. By anticipating these potential errors, we can enhance the student model’s translation capability for words not present in the monolingual corpus.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.4">Practically, given a source sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.1.m1.1d">italic_X</annotation></semantics></math> and a word <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p2.2.m2.1a"><mi id="S3.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.2.m2.1d">italic_s</annotation></semantics></math> that the student MT model mistranslates, the word analoger aims to associate more words from two perspectives: (1) category, i.e., words belonging to the same category as <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p2.3.m3.1a"><mi id="S3.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.3.m3.1b"><ci id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.3.m3.1d">italic_s</annotation></semantics></math>, and (2) semantic, i.e., words that frequently co-occur with <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p2.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.4.m4.1d">italic_s</annotation></semantics></math>. We also require that the generated words should be rare and challenging in the prompt, ensuring that the student model will struggle to translate them accurately.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation of <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.1.1">MT-Patcher</span> </h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.11">Theoretically, state-of-the-art LLMs like GPT-4 can already serve as an <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.11.1">MT-Patcher</span> to transfer its knowledge to MT models. However, in practice, because we do not have unlimited access to GPT-4, we instead collect the demonstration data from GPT-4. Specifically, given a student model, we first use it to generate its translation on 20,000 monolingual sentences randomly selected from the monolingual corpus. We then leverage GPT-4 to execute the pipeline of <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.11.2">MT-Patcher</span> including (1) giving feedback <math alttext="f" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_f</annotation></semantics></math> given the source sentence and student’s translation <math alttext="(X,Y)" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.2"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.3.2" xref="S3.SS3.p1.2.m2.2.3.1.cmml"><mo id="S3.SS3.p1.2.m2.2.3.2.1" stretchy="false" xref="S3.SS3.p1.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">X</mi><mo id="S3.SS3.p1.2.m2.2.3.2.2" xref="S3.SS3.p1.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">Y</mi><mo id="S3.SS3.p1.2.m2.2.3.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><interval closure="open" id="S3.SS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.2"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑋</ci><ci id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">𝑌</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">(X,Y)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.2d">( italic_X , italic_Y )</annotation></semantics></math>, (2) analyzing the domain, topic and style <math alttext="(d,t,st)" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.3"><semantics id="S3.SS3.p1.3.m3.3a"><mrow id="S3.SS3.p1.3.m3.3.3.1" xref="S3.SS3.p1.3.m3.3.3.2.cmml"><mo id="S3.SS3.p1.3.m3.3.3.1.2" stretchy="false" xref="S3.SS3.p1.3.m3.3.3.2.cmml">(</mo><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">d</mi><mo id="S3.SS3.p1.3.m3.3.3.1.3" xref="S3.SS3.p1.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS3.p1.3.m3.2.2" xref="S3.SS3.p1.3.m3.2.2.cmml">t</mi><mo id="S3.SS3.p1.3.m3.3.3.1.4" xref="S3.SS3.p1.3.m3.3.3.2.cmml">,</mo><mrow id="S3.SS3.p1.3.m3.3.3.1.1" xref="S3.SS3.p1.3.m3.3.3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.3.3.1.1.2" xref="S3.SS3.p1.3.m3.3.3.1.1.2.cmml">s</mi><mo id="S3.SS3.p1.3.m3.3.3.1.1.1" xref="S3.SS3.p1.3.m3.3.3.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.3.m3.3.3.1.1.3" xref="S3.SS3.p1.3.m3.3.3.1.1.3.cmml">t</mi></mrow><mo id="S3.SS3.p1.3.m3.3.3.1.5" stretchy="false" xref="S3.SS3.p1.3.m3.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.3b"><vector id="S3.SS3.p1.3.m3.3.3.2.cmml" xref="S3.SS3.p1.3.m3.3.3.1"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑑</ci><ci id="S3.SS3.p1.3.m3.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2">𝑡</ci><apply id="S3.SS3.p1.3.m3.3.3.1.1.cmml" xref="S3.SS3.p1.3.m3.3.3.1.1"><times id="S3.SS3.p1.3.m3.3.3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.3.3.1.1.1"></times><ci id="S3.SS3.p1.3.m3.3.3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.3.3.1.1.2">𝑠</ci><ci id="S3.SS3.p1.3.m3.3.3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.3.3.1.1.3">𝑡</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.3c">(d,t,st)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.3d">( italic_d , italic_t , italic_s italic_t )</annotation></semantics></math> of the source sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_X</annotation></semantics></math> (3) making analogies <math alttext="(WA_{x},WA_{y})" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.2"><semantics id="S3.SS3.p1.5.m5.2a"><mrow id="S3.SS3.p1.5.m5.2.2.2" xref="S3.SS3.p1.5.m5.2.2.3.cmml"><mo id="S3.SS3.p1.5.m5.2.2.2.3" stretchy="false" xref="S3.SS3.p1.5.m5.2.2.3.cmml">(</mo><mrow id="S3.SS3.p1.5.m5.1.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.1.1.2" xref="S3.SS3.p1.5.m5.1.1.1.1.2.cmml">W</mi><mo id="S3.SS3.p1.5.m5.1.1.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.1.1.cmml">⁢</mo><msub id="S3.SS3.p1.5.m5.1.1.1.1.3" xref="S3.SS3.p1.5.m5.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.5.m5.1.1.1.1.3.2" xref="S3.SS3.p1.5.m5.1.1.1.1.3.2.cmml">A</mi><mi id="S3.SS3.p1.5.m5.1.1.1.1.3.3" xref="S3.SS3.p1.5.m5.1.1.1.1.3.3.cmml">x</mi></msub></mrow><mo id="S3.SS3.p1.5.m5.2.2.2.4" xref="S3.SS3.p1.5.m5.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.5.m5.2.2.2.2" xref="S3.SS3.p1.5.m5.2.2.2.2.cmml"><mi id="S3.SS3.p1.5.m5.2.2.2.2.2" xref="S3.SS3.p1.5.m5.2.2.2.2.2.cmml">W</mi><mo id="S3.SS3.p1.5.m5.2.2.2.2.1" xref="S3.SS3.p1.5.m5.2.2.2.2.1.cmml">⁢</mo><msub id="S3.SS3.p1.5.m5.2.2.2.2.3" xref="S3.SS3.p1.5.m5.2.2.2.2.3.cmml"><mi id="S3.SS3.p1.5.m5.2.2.2.2.3.2" xref="S3.SS3.p1.5.m5.2.2.2.2.3.2.cmml">A</mi><mi id="S3.SS3.p1.5.m5.2.2.2.2.3.3" xref="S3.SS3.p1.5.m5.2.2.2.2.3.3.cmml">y</mi></msub></mrow><mo id="S3.SS3.p1.5.m5.2.2.2.5" stretchy="false" xref="S3.SS3.p1.5.m5.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.2b"><interval closure="open" id="S3.SS3.p1.5.m5.2.2.3.cmml" xref="S3.SS3.p1.5.m5.2.2.2"><apply id="S3.SS3.p1.5.m5.1.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1"><times id="S3.SS3.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.1"></times><ci id="S3.SS3.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.2">𝑊</ci><apply id="S3.SS3.p1.5.m5.1.1.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.3.2">𝐴</ci><ci id="S3.SS3.p1.5.m5.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.5.m5.1.1.1.1.3.3">𝑥</ci></apply></apply><apply id="S3.SS3.p1.5.m5.2.2.2.2.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2"><times id="S3.SS3.p1.5.m5.2.2.2.2.1.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.1"></times><ci id="S3.SS3.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.2">𝑊</ci><apply id="S3.SS3.p1.5.m5.2.2.2.2.3.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.2.2.2.2.3.1.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.3">subscript</csymbol><ci id="S3.SS3.p1.5.m5.2.2.2.2.3.2.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.3.2">𝐴</ci><ci id="S3.SS3.p1.5.m5.2.2.2.2.3.3.cmml" xref="S3.SS3.p1.5.m5.2.2.2.2.3.3">𝑦</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.2c">(WA_{x},WA_{y})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.2d">( italic_W italic_A start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_W italic_A start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )</annotation></semantics></math> given the source sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_X</annotation></semantics></math> and a word <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><mi id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><ci id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_s</annotation></semantics></math> in <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.1"><semantics id="S3.SS3.p1.8.m8.1a"><mi id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><ci id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.1d">italic_X</annotation></semantics></math> (4) synthesizing parallel sentences containing error source words <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.1"><semantics id="S3.SS3.p1.9.m9.1a"><mi id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><ci id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.1d">italic_s</annotation></semantics></math> and their corrections <math alttext="c" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m10.1"><semantics id="S3.SS3.p1.10.m10.1a"><mi id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><ci id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m10.1d">italic_c</annotation></semantics></math> with the same domain, topic and style attribute <math alttext="(d,t,st)" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m11.3"><semantics id="S3.SS3.p1.11.m11.3a"><mrow id="S3.SS3.p1.11.m11.3.3.1" xref="S3.SS3.p1.11.m11.3.3.2.cmml"><mo id="S3.SS3.p1.11.m11.3.3.1.2" stretchy="false" xref="S3.SS3.p1.11.m11.3.3.2.cmml">(</mo><mi id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml">d</mi><mo id="S3.SS3.p1.11.m11.3.3.1.3" xref="S3.SS3.p1.11.m11.3.3.2.cmml">,</mo><mi id="S3.SS3.p1.11.m11.2.2" xref="S3.SS3.p1.11.m11.2.2.cmml">t</mi><mo id="S3.SS3.p1.11.m11.3.3.1.4" xref="S3.SS3.p1.11.m11.3.3.2.cmml">,</mo><mrow id="S3.SS3.p1.11.m11.3.3.1.1" xref="S3.SS3.p1.11.m11.3.3.1.1.cmml"><mi id="S3.SS3.p1.11.m11.3.3.1.1.2" xref="S3.SS3.p1.11.m11.3.3.1.1.2.cmml">s</mi><mo id="S3.SS3.p1.11.m11.3.3.1.1.1" xref="S3.SS3.p1.11.m11.3.3.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p1.11.m11.3.3.1.1.3" xref="S3.SS3.p1.11.m11.3.3.1.1.3.cmml">t</mi></mrow><mo id="S3.SS3.p1.11.m11.3.3.1.5" stretchy="false" xref="S3.SS3.p1.11.m11.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.3b"><vector id="S3.SS3.p1.11.m11.3.3.2.cmml" xref="S3.SS3.p1.11.m11.3.3.1"><ci id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">𝑑</ci><ci id="S3.SS3.p1.11.m11.2.2.cmml" xref="S3.SS3.p1.11.m11.2.2">𝑡</ci><apply id="S3.SS3.p1.11.m11.3.3.1.1.cmml" xref="S3.SS3.p1.11.m11.3.3.1.1"><times id="S3.SS3.p1.11.m11.3.3.1.1.1.cmml" xref="S3.SS3.p1.11.m11.3.3.1.1.1"></times><ci id="S3.SS3.p1.11.m11.3.3.1.1.2.cmml" xref="S3.SS3.p1.11.m11.3.3.1.1.2">𝑠</ci><ci id="S3.SS3.p1.11.m11.3.3.1.1.3.cmml" xref="S3.SS3.p1.11.m11.3.3.1.1.3">𝑡</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.3c">(d,t,st)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m11.3d">( italic_d , italic_t , italic_s italic_t )</annotation></semantics></math>. Finally, we finetune the teacher LLM on these data to transform it to an <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.11.3">MT-Patcher</span>. All prompts we use for building <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.11.4">MT-Patcher</span> can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A1" title="Appendix A Prompts for MT-Patcher ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">We evaluate our method on Chinese <math alttext="\to" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" stretchy="false" xref="S4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">→</annotation></semantics></math> English and English <math alttext="\to" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" stretchy="false" xref="S4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">→</annotation></semantics></math> German translation.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.2.2.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1">Chinese</span> <math alttext="\to" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2">English</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T1.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1">English</span> <math alttext="\to" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">→</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.2">German</span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="S4.T1.4.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.4.5.1.1.1">Teacher Model: Baichuan2 13B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="S4.T1.4.5.1.2"><span class="ltx_text ltx_font_italic" id="S4.T1.4.5.1.2.1">Teacher Model: Llama2 13B</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T1.4.4.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.1">|<math alttext="\mathcal{D}_{f}" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><msub id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T1.3.3.1.m1.1.1.2" xref="S4.T1.3.3.1.m1.1.1.2.cmml">𝒟</mi><mi id="S4.T1.3.3.1.m1.1.1.3" xref="S4.T1.3.3.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2">𝒟</ci><ci id="S4.T1.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\mathcal{D}_{f}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>|</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.5">BLEURT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.6">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.2">|<math alttext="\mathcal{D}_{f}" class="ltx_Math" display="inline" id="S4.T1.4.4.2.m1.1"><semantics id="S4.T1.4.4.2.m1.1a"><msub id="S4.T1.4.4.2.m1.1.1" xref="S4.T1.4.4.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T1.4.4.2.m1.1.1.2" xref="S4.T1.4.4.2.m1.1.1.2.cmml">𝒟</mi><mi id="S4.T1.4.4.2.m1.1.1.3" xref="S4.T1.4.4.2.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b"><apply id="S4.T1.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.2.m1.1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.2.m1.1.1.2.cmml" xref="S4.T1.4.4.2.m1.1.1.2">𝒟</ci><ci id="S4.T1.4.4.2.m1.1.1.3.cmml" xref="S4.T1.4.4.2.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">\mathcal{D}_{f}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.2.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>|</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.7">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.8">BLEURT</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.9">BLEU</th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.4.6.2.1">Teacher</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.2">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.3">80.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.4">67.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.5">23.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.6">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.7">81.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.8">72.9</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.6.2.9">26.0</th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="9" id="S4.T1.4.7.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.4.7.3.1.1">Student Model: ParroT-7B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.8.1.1">Student</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.3">75.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.4">60.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.5">18.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.7">80.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.8.1.8">69.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.8.1.9">23.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.9.2.1">SeqKD-Equal</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.2">119k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.3">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.4">61.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.5">21.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.6">107k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.7">80.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.9.2.8">70.8</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.9.2.9">24.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.10.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.10.3.1">SeqKD-Full</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.2">1M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.3">76.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.4">61.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.5">22.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.6">1M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.7">80.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.10.3.8">71.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.10.3.9">24.6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.11.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.11.4.1">
<span class="ltx_ERROR undefined" id="S4.T1.4.11.4.1.1">\hdashline</span><span class="ltx_text ltx_font_smallcaps" id="S4.T1.4.11.4.1.2">MT-Patcher</span>
</th>
<td class="ltx_td" id="S4.T1.4.11.4.2"></td>
<td class="ltx_td" id="S4.T1.4.11.4.3"></td>
<td class="ltx_td" id="S4.T1.4.11.4.4"></td>
<td class="ltx_td" id="S4.T1.4.11.4.5"></td>
<td class="ltx_td" id="S4.T1.4.11.4.6"></td>
<td class="ltx_td" id="S4.T1.4.11.4.7"></td>
<td class="ltx_td" id="S4.T1.4.11.4.8"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T1.4.11.4.9"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.12.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.4.12.5.1">+ PE</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.2">119k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.3">76.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.4">61.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.5">22.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.6">107k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.7">80.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.12.5.8">71.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.12.5.9">24.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.13.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.4.13.6.1">+ PE + PDS</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.2">595k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.3">77.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.4">62.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.5">23.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.6">535k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.7">81.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.13.6.8">72.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.13.6.9">25.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.14.7">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.4.14.7.1">+ PE + PDS + WA</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.2">1.07M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.3.1">78.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.4.1">63.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.5.1">23.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.6">963k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.7.1">81.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.14.7.8"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.8.1">72.6</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.14.7.9"><span class="ltx_text ltx_font_bold" id="S4.T1.4.14.7.9.1">26.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.15.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="9" id="S4.T1.4.15.8.1"><span class="ltx_text ltx_font_italic" id="S4.T1.4.15.8.1.1">Student Model: NLLB 3.3B</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.16.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.16.9.1">Student</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.3">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.4">63.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.5">20.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.7">86.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.16.9.8">76.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.16.9.9">34.3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.17.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.17.10.1">SeqKD-Equal</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.2">104k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.3">79.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.4">66.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.5">25.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.6">124k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.7">85.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.17.10.8">74.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.17.10.9">32.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.18.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.18.11.1">SeqKD-Full</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.2">1M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.3">79.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.4">66.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.18.11.5.1">25.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.6">1M</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.7">84.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.18.11.8">74.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.18.11.9">31.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.19.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.4.19.12.1">
<span class="ltx_ERROR undefined" id="S4.T1.4.19.12.1.1">\hdashline</span><span class="ltx_text ltx_font_smallcaps" id="S4.T1.4.19.12.1.2">MT-Patcher</span>
</th>
<td class="ltx_td" id="S4.T1.4.19.12.2"></td>
<td class="ltx_td" id="S4.T1.4.19.12.3"></td>
<td class="ltx_td" id="S4.T1.4.19.12.4"></td>
<td class="ltx_td" id="S4.T1.4.19.12.5"></td>
<td class="ltx_td" id="S4.T1.4.19.12.6"></td>
<td class="ltx_td" id="S4.T1.4.19.12.7"></td>
<td class="ltx_td" id="S4.T1.4.19.12.8"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T1.4.19.12.9"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.20.13">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.4.20.13.1">+ PE</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.2">104k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.3">79.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.4">67.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.5">24.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.6">87k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.7">86.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.20.13.8">76.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.20.13.9">34.5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.21.14">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.4.21.14.1">+ PE + PDS</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.2">520k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.3">79.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.4">67.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.5">24.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.6">435k</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.7">86.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.21.14.8">77.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.21.14.9">34.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.22.15">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T1.4.22.15.1">+ PE + PDS + WA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.2">936k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.22.15.3.1">80.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.22.15.4.1">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.5">25.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.6">783k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.22.15.7.1">87.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.8"><span class="ltx_text ltx_font_bold" id="S4.T1.4.22.15.8.1">77.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.4.22.15.9"><span class="ltx_text ltx_font_bold" id="S4.T1.4.22.15.9.1">35.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Translation performance of the proposed method and other baselines on the WMT22 Chinese<math alttext="\to" class="ltx_Math" display="inline" id="S4.T1.8.m1.1"><semantics id="S4.T1.8.m1.1b"><mo id="S4.T1.8.m1.1.1" stretchy="false" xref="S4.T1.8.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.m1.1c"><ci id="S4.T1.8.m1.1.1.cmml" xref="S4.T1.8.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.m1.1e">→</annotation></semantics></math>English and English<math alttext="\to" class="ltx_Math" display="inline" id="S4.T1.9.m2.1"><semantics id="S4.T1.9.m2.1b"><mo id="S4.T1.9.m2.1.1" stretchy="false" xref="S4.T1.9.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.m2.1c"><ci id="S4.T1.9.m2.1.1.cmml" xref="S4.T1.9.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.m2.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.m2.1e">→</annotation></semantics></math>German test sets. <math alttext="|D_{f}|" class="ltx_Math" display="inline" id="S4.T1.10.m3.1"><semantics id="S4.T1.10.m3.1b"><mrow id="S4.T1.10.m3.1.1.1" xref="S4.T1.10.m3.1.1.2.cmml"><mo id="S4.T1.10.m3.1.1.1.2" stretchy="false" xref="S4.T1.10.m3.1.1.2.1.cmml">|</mo><msub id="S4.T1.10.m3.1.1.1.1" xref="S4.T1.10.m3.1.1.1.1.cmml"><mi id="S4.T1.10.m3.1.1.1.1.2" xref="S4.T1.10.m3.1.1.1.1.2.cmml">D</mi><mi id="S4.T1.10.m3.1.1.1.1.3" xref="S4.T1.10.m3.1.1.1.1.3.cmml">f</mi></msub><mo id="S4.T1.10.m3.1.1.1.3" stretchy="false" xref="S4.T1.10.m3.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.m3.1c"><apply id="S4.T1.10.m3.1.1.2.cmml" xref="S4.T1.10.m3.1.1.1"><abs id="S4.T1.10.m3.1.1.2.1.cmml" xref="S4.T1.10.m3.1.1.1.2"></abs><apply id="S4.T1.10.m3.1.1.1.1.cmml" xref="S4.T1.10.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.T1.10.m3.1.1.1.1.1.cmml" xref="S4.T1.10.m3.1.1.1.1">subscript</csymbol><ci id="S4.T1.10.m3.1.1.1.1.2.cmml" xref="S4.T1.10.m3.1.1.1.1.2">𝐷</ci><ci id="S4.T1.10.m3.1.1.1.1.3.cmml" xref="S4.T1.10.m3.1.1.1.1.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.m3.1d">|D_{f}|</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.m3.1e">| italic_D start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT |</annotation></semantics></math> denotes the number of examples used to finetune the student model. SeqKD-Full refers to the student model finetunes on the full 1M pseudo parallel sentences, while SeqKD-Equal finetunes on random subsets of the teacher’s translations with equal size to that of <span class="ltx_text ltx_font_smallcaps" id="S4.T1.12.1">MT-Patcher</span>.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Student Translation Model</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">For student translation models, we consider NLLB-200 3.3B <cite class="ltx_cite ltx_citemacro_citep">(NLLB Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib22" title="">2022</a>)</cite>, a multilingual translation model pre-trained on 200 languages. Having been trained on massive parallel data, it can already translate reasonably well but falls short of language knowledge compared to LLMs, making it an ideal knowledge recipient for our experiment.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">Due to the increasing interest in adopting LLMs for MT, we also consider ParroT <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib13" title="">2023a</a>)</cite>, an LLM-based MT model finetuned on WMT validation sets from LLaMA-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib31" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Backbone LLM for <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS0.Px2.1.1">MT-Patcher</span> </h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">The backbone LLMs for building <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS0.Px2.p1.1.1">MT-Patcher</span> in this paper are LLaMA2-13B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib31" title="">2023</a>)</cite> and Baichuan-2-13B <cite class="ltx_cite ltx_citemacro_citep">(Baichuan Inc, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib2" title="">2023</a>)</cite>. LLaMA2-13B is an English LLM and used to build <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS0.Px2.p1.1.2">MT-Patcher</span> for English-German translation models. Baichuan-2-13B is trained on a mix of both Chinese and English corpus and demonstrates much stronger abilities in Chinese compared to LLaMA2. Therefore, we adopt it for building <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS0.Px2.p1.1.3">MT-Patcher</span> for Chinese-English translation models. For each language pair considered, we fully finetune the corresponding LLM on the collected data for 3 epochs. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A2" title="Appendix B Implementation details ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a> for more implementation details.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Competitors</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">We compare the translation performance of the following methods:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Student</span> is the translation model to be patched. In this paper, it refers to NLLB 3.3B or ParroT.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Teacher</span> is the model that is achieved by finetuning the larger LLM to perform translation directly. For a fair comparison, we finetune the LLM on GPT-4’s translation on the monolingual sentences.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">SeqKD</span> are models achieved by finetuning the Student model on the Teacher’s translations.
</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.I1.i4.p1.1.1">MT-Patcher<span class="ltx_text ltx_font_upright" id="S4.I1.i4.p1.1.1.1">  (PE)</span></span> is the variant of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i4.p1.1.2">MT-Patcher</span> , finetuning the Student model on the post-editing results in feedback.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.I1.i5.p1.1.1">MT-Patcher<span class="ltx_text ltx_font_upright" id="S4.I1.i5.p1.1.1.1">  (PE + PDS)</span></span> is the variant of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i5.p1.1.2">MT-Patcher</span> which finetunes the Student model on the post-editing results as well as additional synthesized parallel sentences generated by parallel data synthesizer containing (error, correction) pairs. Unless other stated, we set the number of pseudo-parallel sentences to be 4 in this paper.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.I1.i6.p1.1.1">MT-Patcher<span class="ltx_text ltx_font_upright" id="S4.I1.i6.p1.1.1.1">  (PE + PDS + WA)</span></span> is the variant of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i6.p1.1.2">MT-Patcher</span> which finetunes the Student model on the post-editing results and parallel sentences generated by parallel data synthesizer containing (error, correction) pairs and additional word pairs from word analoger. We generate 2 analogous words for each category and 1 context for each word.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Chemistry Materials</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Chinese Idioms</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.1.2.2.2"><span class="ltx_text ltx_font_italic" id="S4.T2.1.2.2.2.1">Unseen Context</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.1.2.2.3"><span class="ltx_text ltx_font_italic" id="S4.T2.1.2.2.3.1">Unseen Word</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.1.2.2.4"><span class="ltx_text ltx_font_italic" id="S4.T2.1.2.2.4.1">Unseen Context</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.1.2.2.5"><span class="ltx_text ltx_font_italic" id="S4.T2.1.2.2.5.1">Unseen Word</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.3.3.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.2">Accuracy</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.3">Rel. Perf.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.4">Accuracy</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.5">Rel. Perf.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.6">Score</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.7">Rel. Perf.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3.8">Score</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.3.3.9">Rel. Perf.</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.4.4.1">Student</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.2">6.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.3">22.4%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.4">6.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.5">23.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.6">1.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.7">39.8%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.8">1.16</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.1.4.4.9">37.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1">Teacher</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.2">26.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.3">97.4%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.4">25.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.5">97.4%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.6">2.78</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.7">92.3%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.8">2.82</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.5.5.9">91.0%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1">
<span class="ltx_ERROR undefined" id="S4.T2.1.6.6.1.1">\hdashline</span>Feedbacker</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.2">26.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3">100%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.4">26.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.5">100%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.6">3.01</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.7">100%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.8">3.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.6.6.9">100%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.7.1">
<span class="ltx_ERROR undefined" id="S4.T2.1.7.7.1.1">\hdashline</span>SeqKD-Full</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.2">15.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.3">58.1%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.4">10.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.5">40.0%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.6">1.65</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.7">54.8%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.8">1.62</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.7.7.9">52.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.1.8.8.1.1">MT-Patcher</span></th>
<td class="ltx_td" id="S4.T2.1.8.8.2"></td>
<td class="ltx_td" id="S4.T2.1.8.8.3"></td>
<td class="ltx_td" id="S4.T2.1.8.8.4"></td>
<td class="ltx_td" id="S4.T2.1.8.8.5"></td>
<td class="ltx_td" id="S4.T2.1.8.8.6"></td>
<td class="ltx_td" id="S4.T2.1.8.8.7"></td>
<td class="ltx_td" id="S4.T2.1.8.8.8"></td>
<td class="ltx_td ltx_nopad_r" id="S4.T2.1.8.8.9"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T2.1.9.9.1">+ PE</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.2">15.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.3">59.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.4">11.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.5">41.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.6">1.73</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.7">57.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.9.8">1.78</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.9.9.9">57.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T2.1.10.10.1">+ PE + PDS</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.2">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.3">80.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.4">11.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.5">42.3%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.6">2.04</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.7">67.8%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.8">1.81</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.10.10.9">58.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.11.11.1">+ PE + PDS + WA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.2">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.3">82.0%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.4">16.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.5">61.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.6">2.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.7">69.8%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.8">2.02</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.1.11.11.9">65.2%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of different models when translating chemistry materials (evaluated in accuracy) and Chinese Idioms (evaluated by scores given by GPT-4). Rel. Perf: the relative performances of models compared to feedbacker, which is the best extent we can elicit knowledge from LLMs in this table.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T3.3.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.1.1.2">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.1.1.3">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.3.1.1.4">BLEURT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.2.1.1">Student</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.2">15.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.3">85.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.1.4">58.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.3.2.1">SeqKD</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.2.2">16.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.2.3">85.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.2.4">61.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.3.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.3.4.3.1.1">MT-Patcher</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.4.3.2.1">16.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.4.3.3.1">86.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.3.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.4.3.4.1">62.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Effectiveness of <span class="ltx_text ltx_font_smallcaps" id="S4.T3.5.1">MT-Patcher</span> on WMT English <math alttext="\to" class="ltx_Math" display="inline" id="S4.T3.2.m1.1"><semantics id="S4.T3.2.m1.1b"><mo id="S4.T3.2.m1.1.1" stretchy="false" xref="S4.T3.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><ci id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.m1.1e">→</annotation></semantics></math> Japanese translation test sets. The student model is NLLB 3.3B.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results on General Machine Translation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.T1" title="Table 1 ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> presents experimental results on general machine translation benchmarks: WMT22 Chinese<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">→</annotation></semantics></math>English and English<math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" stretchy="false" xref="S4.SS2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">→</annotation></semantics></math>German translation. We randomly select 1,000,000 sentences from RefinedWeb <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib23" title="">2023</a>)</cite> and WuDao 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib37" title="">2021b</a>)</cite>, respectively, as English and Chinese monolingual corpus. Performance are evaluated in COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib25" title="">2020</a>)</cite>, BLEURT <cite class="ltx_cite ltx_citemacro_citep">(Sellam et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib28" title="">2020</a>)</cite> <span class="ltx_note ltx_role_footnote" id="footnote1b"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The model we used for COMET and BLEURT is wmt22-comet-da and BLEURT-20, respectively.</span></span></span> and sacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib24" title="">2018</a>)</cite>. We can see that:</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.1.1">MT-Patcher</span> can select more valuable examples.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.T1" title="Table 1 ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we can first see that the performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.1">MT-Patcher</span> (PE) is better SeqKD-Equal, and can be comparable to SeqKD-Full. This indicates the proposed method can select more valuable examples and discard useless examples. We also find our method suffers less from catastrophic forgetting compared to SeqKD-Full (See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A3" title="Appendix C MT-Patcher suffers less from catastrophic forgetting. ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">C</span></a> for more experimental results). This makes <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.2">MT-Patcher</span> an appealing method for real-world applications, considering the cost for finetuning the Student model is growing nowadays.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Parallel data synthesizer and word analoger improve the effectiveness of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px2.1.1">MT-Patcher</span>.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We can also see that applying the parallel data synthesizer and word analoger to generate more patch data can further improve the translation performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px2.p1.1.1">MT-Patcher</span>, highlighting the benefits of extending coverage of context and knowledge during the process of knowledge transferring.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph"></h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">It is worth noting that in the English <math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math> German direction, the teacher based on LLaMA-2-13B performs substantially worse than the student (NLLB 3.3B), which is consistent with previous findings <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib18" title="">2023</a>)</cite> that it is not trivial to adopt existing LLMs to outperform supervised translation models. As a result, SeqKD from this teacher leads to poor performance. However, based on the same backbone LLM, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.p1.1.1">MT-Patcher</span> can still improve the performance of the Student model. This can be attributed to the hypothesis that revising an initial draft is a better way to elicit the knowledge of LLMs than direct generation, which we provide a further analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.SS2" title="5.2 Does asking for feedback better elicit LLMs’ translation knowledge? ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px4.1.1">MT-Patcher</span> also works when the teacher is not very strong.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px4.p1.3">Although we mainly focus on settings where we have strong teachers (which is why we choose different teacher models for English <math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px4.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px4.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px4.p1.1.m1.1d">→</annotation></semantics></math> German and Chinese <math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px4.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px4.p1.2.m2.1.1" stretchy="false" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px4.p1.2.m2.1d">→</annotation></semantics></math> English translation), we also experiment with medium resource translation: WMT22 English <math alttext="\to" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px4.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px4.p1.3.m3.1a"><mo id="S4.SS2.SSS0.Px4.p1.3.m3.1.1" stretchy="false" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.3.m3.1b"><ci id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px4.p1.3.m3.1d">→</annotation></semantics></math> Japanese, using LLaMA2 as the teacher and NLLB 3.3B as the student. We present the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.T3" title="Table 3 ‣ Competitors ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. We find <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px4.p1.3.1">MT-Patcher</span> can still outperform SeqKD in this setting.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results on Specific Language Phenomena</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In order to understand how <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.1">MT-Patcher</span> can improve the effectiveness of knowledge transfer, we present experiments on the Chinese-to-English translation for two specific language phenomena: <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">chemistry materials</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.3">Chinese idioms</span>. We select them for two reasons: <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.4">(1)</span> Both belong to long-tailed knowledge that student MT models cannot grasp very well. <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.5">(2)</span> There are also distinctions between them: chemistry materials represent simple, context-free knowledge, while Chinese idioms represent more abstract and metaphorical knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Specifically, for each language phenomenon, we first collect a list of 6,000 of them and their corresponding translations from the web. We then split these word pairs into two categories: <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">Seen</span> and <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">Unseen</span>, and create a monolingual set as well as two test sets based on the split <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Details of the dataset and data split can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A4" title="Appendix D Details of datasets used for chemistry materials and Chinese idioms ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span>:</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Monolingual Set</span>. For each word pair in the <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.2">Seen</span> set, we ask GPT-4 to synthesize one sentence that contains the source word. This set is for SeqKD and <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.3">MT-Patcher</span> to leverage.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Test Set for Unseen Context.</span> For each word pair in the <span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.2">Seen</span> set, we also ask GPT-4 to synthesize one parallel sentence pair that contains the source and target word in the source and target sentence, respectively. This set is for testing models’ generalization ability when source words are seen yet contexts are novel.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Test Set for Unseen Word.</span> We collect the test set for Unseen Word in a similar way as Unseen Context using the word pairs in the <span class="ltx_text ltx_font_italic" id="S4.I2.i3.p1.1.2">Unseen</span> set. This set is for testing models’ generalization ability to novel words.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">We take the Baichuan-2-13B as the LLM and NLLB 3.3B as the student model, and present the experimental results in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.T2" title="Table 2 ‣ Competitors ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. The accuracy of translating chemistry materials represents the percentage of test examples where the correct translation of the source chemistry material is found in the translation. Regarding Chinese idioms, due to the difficulty of providing reference translations of them, we instead ask GPT-4 to assess the translation quality given the source sentence, target sentence and dictionary definition. We report the average score, which ranges from 0 to 5. For ease of comparison, we also report how different models perform relative to the feedbackers, for which we directly take its correction as the translation.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Multiple contexts facilitate generalization on <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px1.1.1">Unseen Context</span>.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">From Table  <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S4.T2" title="Table 2 ‣ Competitors ‣ 4.1 Experimental Settings ‣ 4 Experiments ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, we can see that despite that the Teacher model achieves significantly better performance than the Student model, the SeqKD-Full method can only narrow less than half of the gap. However, by synthesizing more contexts for each error, <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px1.p1.1.1">MT-Patcher</span> (+PE + PDG) improves the relative performance from 59.2% to 80.5% for chemistry materials, and 57.5% to 69.8% for Chinese Idioms, indicating the importance of translation knowledge in multiple contexts in order to generalize to novel contexts better.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Error Anticipation improves performances on <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px2.1.1">Unseen Word</span>.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">We can also observe that both SeqKD-Full and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.1">MT-Patcher</span> (+PE + PDG) cannot behave well on the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px2.p1.1.2">Unseen Word</span> set, which can be attributed to their inability to extrapolate from the observed errors to unseen errors. By generating analogous words to anticipate more errors, the translation performances on <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS0.Px2.p1.1.3">Unseen Word</span> are significantly improved, validating the effectiveness of the proposed error anticipation method.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We provide further analysis on how <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.1">MT-Patcher</span> works and its applicability to real-world scenarios. All experiments are conducted on the WMT22 Chinese-to-English translation datasets, and the student MT model is NLLB 3.3B.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="S5.F2.g1" src="extracted/2403.09522v2/figs/scaling.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Translation performance as the number of synthesized contexts per word and analogous word grows.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Impact of the number of synthesized contexts per word and analogous word</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.F2" title="Figure 2 ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, we plot how increasing the number of synthesized contexts per word and analogous words affects the translation performance of the student model. Note that we only synthesize one context for each analogous word. We can see increasing both numbers results in improved translation performance. For synthesized contexts, the gain plateau between 16 to 32 suggests this amount of different contexts is adequate for word or phrase learning. For analogous words, however, we observe the performance grows at a log-linear rate <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>It is worth noting that this does not mean <span class="ltx_text ltx_font_smallcaps" id="footnote3.1">MT-Patcher</span> can improve the translation performance endlessly, since it cannot generate an unlimited amount of valid analogous words. The performance will eventually plateau, although we have not scaled to the number due to the computational limitation.</span></span></span>.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S5.F3.g1" src="extracted/2403.09522v2/figs/dg_vs_feedback.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of translation quality on error words between the Teacher’s translation and the feedbacker’s correction.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Does asking for feedback better elicit LLMs’ translation knowledge?</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We conduct a head-to-head comparison between two ways to leverage the teacher LLM: ask the teacher to directly provide translation vs. ask <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.1">MT-Patcher</span> to give feedback on the student’s translation. Specifically, we randomly select 1000 examples and compare the correction provided by <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.2">MT-Patcher</span> to the translation provided by the teacher. The comparison is made by both human and GPT-4.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.F3" title="Figure 3 ‣ 5.1 Impact of the number of synthesized contexts per word and analogous word ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. It can be seen that <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p2.1.1">MT-Patcher</span>’s corrections are considered by both GPT-4 and human evaluators to be comparable or better than the teacher’s translation on more than 80% examples, demonstrating the benefits of eliciting LLM’s knowledge in the form of feedback.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>The Effectiveness of Iterative Feedback</h3>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="S5.F4.g1" src="extracted/2403.09522v2/figs/iterative_feedback.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Accuracy of corrections and percentage of remaining data after applying different epochs of iterative feedback.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.8.9.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T4.8.9.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.8.9.1.2">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.8.9.1.3">BLEURT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.8.9.1.4">BLEU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.1"><math alttext="k=1" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml"><mi id="S5.T4.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1"><eq id="S5.T4.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1.1"></eq><ci id="S5.T4.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.1.1.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">k=1</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1d">italic_k = 1</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.3">67.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.4">24.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.2.2.1"><math alttext="k=2" class="ltx_Math" display="inline" id="S5.T4.2.2.1.m1.1"><semantics id="S5.T4.2.2.1.m1.1a"><mrow id="S5.T4.2.2.1.m1.1.1" xref="S5.T4.2.2.1.m1.1.1.cmml"><mi id="S5.T4.2.2.1.m1.1.1.2" xref="S5.T4.2.2.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.2.2.1.m1.1.1.1" xref="S5.T4.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.2.2.1.m1.1.1.3" xref="S5.T4.2.2.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.1.m1.1b"><apply id="S5.T4.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1"><eq id="S5.T4.2.2.1.m1.1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1.1"></eq><ci id="S5.T4.2.2.1.m1.1.1.2.cmml" xref="S5.T4.2.2.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.2.2.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.1.m1.1c">k=2</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.1.m1.1d">italic_k = 2</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.2">79.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.3">67.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.2.2.4">24.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.3.3.1"><math alttext="k=3" class="ltx_Math" display="inline" id="S5.T4.3.3.1.m1.1"><semantics id="S5.T4.3.3.1.m1.1a"><mrow id="S5.T4.3.3.1.m1.1.1" xref="S5.T4.3.3.1.m1.1.1.cmml"><mi id="S5.T4.3.3.1.m1.1.1.2" xref="S5.T4.3.3.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.3.3.1.m1.1.1.1" xref="S5.T4.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.3.3.1.m1.1.1.3" xref="S5.T4.3.3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.1.m1.1b"><apply id="S5.T4.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.1.m1.1.1"><eq id="S5.T4.3.3.1.m1.1.1.1.cmml" xref="S5.T4.3.3.1.m1.1.1.1"></eq><ci id="S5.T4.3.3.1.m1.1.1.2.cmml" xref="S5.T4.3.3.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.3.3.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.3.3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.1.m1.1c">k=3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.1.m1.1d">italic_k = 3</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.2">80.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.1">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.4">24.9</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.4.4.1"><math alttext="k=4" class="ltx_Math" display="inline" id="S5.T4.4.4.1.m1.1"><semantics id="S5.T4.4.4.1.m1.1a"><mrow id="S5.T4.4.4.1.m1.1.1" xref="S5.T4.4.4.1.m1.1.1.cmml"><mi id="S5.T4.4.4.1.m1.1.1.2" xref="S5.T4.4.4.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.4.4.1.m1.1.1.1" xref="S5.T4.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.4.4.1.m1.1.1.3" xref="S5.T4.4.4.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.1.m1.1b"><apply id="S5.T4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.1.m1.1.1"><eq id="S5.T4.4.4.1.m1.1.1.1.cmml" xref="S5.T4.4.4.1.m1.1.1.1"></eq><ci id="S5.T4.4.4.1.m1.1.1.2.cmml" xref="S5.T4.4.4.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.4.4.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.4.4.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.1.m1.1c">k=4</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.1.m1.1d">italic_k = 4</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.2.1">80.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.3.1">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.4.4.4.1">25.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.5.5.1"><math alttext="k=5" class="ltx_Math" display="inline" id="S5.T4.5.5.1.m1.1"><semantics id="S5.T4.5.5.1.m1.1a"><mrow id="S5.T4.5.5.1.m1.1.1" xref="S5.T4.5.5.1.m1.1.1.cmml"><mi id="S5.T4.5.5.1.m1.1.1.2" xref="S5.T4.5.5.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.5.5.1.m1.1.1.1" xref="S5.T4.5.5.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.5.5.1.m1.1.1.3" xref="S5.T4.5.5.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.1.m1.1b"><apply id="S5.T4.5.5.1.m1.1.1.cmml" xref="S5.T4.5.5.1.m1.1.1"><eq id="S5.T4.5.5.1.m1.1.1.1.cmml" xref="S5.T4.5.5.1.m1.1.1.1"></eq><ci id="S5.T4.5.5.1.m1.1.1.2.cmml" xref="S5.T4.5.5.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.5.5.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.5.5.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.1.m1.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.1.m1.1d">italic_k = 5</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.2"><span class="ltx_text ltx_font_bold" id="S5.T4.5.5.2.1">80.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.3">67.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.5.5.4">25.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.6.6.1"><math alttext="k=6" class="ltx_Math" display="inline" id="S5.T4.6.6.1.m1.1"><semantics id="S5.T4.6.6.1.m1.1a"><mrow id="S5.T4.6.6.1.m1.1.1" xref="S5.T4.6.6.1.m1.1.1.cmml"><mi id="S5.T4.6.6.1.m1.1.1.2" xref="S5.T4.6.6.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.6.6.1.m1.1.1.1" xref="S5.T4.6.6.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.6.6.1.m1.1.1.3" xref="S5.T4.6.6.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.1.m1.1b"><apply id="S5.T4.6.6.1.m1.1.1.cmml" xref="S5.T4.6.6.1.m1.1.1"><eq id="S5.T4.6.6.1.m1.1.1.1.cmml" xref="S5.T4.6.6.1.m1.1.1.1"></eq><ci id="S5.T4.6.6.1.m1.1.1.2.cmml" xref="S5.T4.6.6.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.6.6.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.6.6.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.1.m1.1c">k=6</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.1.m1.1d">italic_k = 6</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.6.6.2">80.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T4.6.6.3.1">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.6.6.4">24.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.7.7.1"><math alttext="k=7" class="ltx_Math" display="inline" id="S5.T4.7.7.1.m1.1"><semantics id="S5.T4.7.7.1.m1.1a"><mrow id="S5.T4.7.7.1.m1.1.1" xref="S5.T4.7.7.1.m1.1.1.cmml"><mi id="S5.T4.7.7.1.m1.1.1.2" xref="S5.T4.7.7.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.7.7.1.m1.1.1.1" xref="S5.T4.7.7.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.7.7.1.m1.1.1.3" xref="S5.T4.7.7.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.1.m1.1b"><apply id="S5.T4.7.7.1.m1.1.1.cmml" xref="S5.T4.7.7.1.m1.1.1"><eq id="S5.T4.7.7.1.m1.1.1.1.cmml" xref="S5.T4.7.7.1.m1.1.1.1"></eq><ci id="S5.T4.7.7.1.m1.1.1.2.cmml" xref="S5.T4.7.7.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.7.7.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.7.7.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.1.m1.1c">k=7</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.1.m1.1d">italic_k = 7</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T4.7.7.2">79.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.7.7.3">67.4</td>
<td class="ltx_td ltx_align_center" id="S5.T4.7.7.4">24.9</td>
</tr>
<tr class="ltx_tr" id="S5.T4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.8.8.1"><math alttext="k=8" class="ltx_Math" display="inline" id="S5.T4.8.8.1.m1.1"><semantics id="S5.T4.8.8.1.m1.1a"><mrow id="S5.T4.8.8.1.m1.1.1" xref="S5.T4.8.8.1.m1.1.1.cmml"><mi id="S5.T4.8.8.1.m1.1.1.2" xref="S5.T4.8.8.1.m1.1.1.2.cmml">k</mi><mo id="S5.T4.8.8.1.m1.1.1.1" xref="S5.T4.8.8.1.m1.1.1.1.cmml">=</mo><mn id="S5.T4.8.8.1.m1.1.1.3" xref="S5.T4.8.8.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.1.m1.1b"><apply id="S5.T4.8.8.1.m1.1.1.cmml" xref="S5.T4.8.8.1.m1.1.1"><eq id="S5.T4.8.8.1.m1.1.1.1.cmml" xref="S5.T4.8.8.1.m1.1.1.1"></eq><ci id="S5.T4.8.8.1.m1.1.1.2.cmml" xref="S5.T4.8.8.1.m1.1.1.2">𝑘</ci><cn id="S5.T4.8.8.1.m1.1.1.3.cmml" type="integer" xref="S5.T4.8.8.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.1.m1.1c">k=8</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.1.m1.1d">italic_k = 8</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.8.8.2"><span class="ltx_text ltx_font_bold" id="S5.T4.8.8.2.1">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.8.8.3"><span class="ltx_text ltx_font_bold" id="S5.T4.8.8.3.1">67.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.8.8.4">24.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Translation performance of NLLB-3B model finetuned on post-editing data after <math alttext="k" class="ltx_Math" display="inline" id="S5.T4.10.m1.1"><semantics id="S5.T4.10.m1.1b"><mi id="S5.T4.10.m1.1.1" xref="S5.T4.10.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T4.10.m1.1c"><ci id="S5.T4.10.m1.1.1.cmml" xref="S5.T4.10.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.T4.10.m1.1e">italic_k</annotation></semantics></math> epochs of iterative feedback.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In this section, we explore whether the application of iterative feedback on post-edited translations can enhance the final translation quality, thereby yielding a better Student model. While iterative feedback may incur additional computational costs, it allows us to compare feedback across multiple iterations and assess the reliability of error identification and correction from the feedbacker. Intuitively, if an error span identified and rectified in the <math alttext="i" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_i</annotation></semantics></math>-th epoch is still deemed problematic in the subsequent epoch, it suggests an inconsistency in the feedbacker’s decision-making process. To prevent the introduction of incorrect knowledge during the knowledge transfer process, examples with such inconsistencies are discarded.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We randomly select 2000 instances of <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p2.1.1">MT-Patcher</span>’s feedback on NLLB-3B’s translation results and apply iterative feedback. We then ask GPT-4 to evaluate the feedback quality after each iterative feedback epoch. The results, depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.F4" title="Figure 4 ‣ 5.3 The Effectiveness of Iterative Feedback ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, indicate that iterative feedback can enhance the accuracy of corrections in remaining examples, converging to 90.4% after 4 epochs at the expense of filtering out approximately 20% of examples. To understand the quality-quantity trade-off of demonstration data, we further fine-tune the Student NLLB model on post-editing data after each iterative feedback epoch and display the translation performance in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.T4" title="Table 4 ‣ 5.3 The Effectiveness of Iterative Feedback ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>. Despite a decrease in the amount of fine-tuning data as the epoch increases, the translation performance of the fine-tuned model continues to improve, highlighting the significance of high-quality fine-tuning data.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.6.7.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T5.6.7.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.6.7.1.2">NLLB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.6.7.1.3">ParroT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T5.4.4.5"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.1.1.1.1">ZH<math alttext="\to" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.m1.1d">→</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.2"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.2.2.2.1">EN<math alttext="\to" class="ltx_Math" display="inline" id="S5.T5.2.2.2.1.m1.1"><semantics id="S5.T5.2.2.2.1.m1.1a"><mo id="S5.T5.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T5.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><ci id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.1.m1.1d">→</annotation></semantics></math>DE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.3"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.3.3.3.1">ZH<math alttext="\to" class="ltx_Math" display="inline" id="S5.T5.3.3.3.1.m1.1"><semantics id="S5.T5.3.3.3.1.m1.1a"><mo id="S5.T5.3.3.3.1.m1.1.1" stretchy="false" xref="S5.T5.3.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.1.m1.1b"><ci id="S5.T5.3.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.1.m1.1d">→</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.4.4.4"><span class="ltx_text ltx_font_smallcaps" id="S5.T5.4.4.4.1">EN<math alttext="\to" class="ltx_Math" display="inline" id="S5.T5.4.4.4.1.m1.1"><semantics id="S5.T5.4.4.4.1.m1.1a"><mo id="S5.T5.4.4.4.1.m1.1.1" stretchy="false" xref="S5.T5.4.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.1.m1.1b"><ci id="S5.T5.4.4.4.1.m1.1.1.cmml" xref="S5.T5.4.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.4.1.m1.1d">→</annotation></semantics></math>DE</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.6.8.1.1">Student</th>
<td class="ltx_td ltx_align_center" id="S5.T5.6.8.1.2">76.8</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.8.1.3">86.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.8.1.4">75.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.6.8.1.5">80.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.6.9.2.1">SeqKD-Full</th>
<td class="ltx_td ltx_align_center" id="S5.T5.6.9.2.2">79.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.9.2.3">84.8</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.9.2.4">76.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.6.9.2.5">80.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.5.5.1">
<span class="ltx_ERROR undefined" id="S5.T5.5.5.1.1">\hdashline</span>NLLB<sup class="ltx_sup" id="S5.T5.5.5.1.2">†</sup>
</th>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.2">80.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.3">87.2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.4">77.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T5.5.5.5">81.3</td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.6.6.1">ParroT<sup class="ltx_sup" id="S5.T5.6.6.1.1">†</sup>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.6.6.2">79.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.6.6.3">86.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.6.6.4">78.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T5.6.6.5">81.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Translation performances when applying <span class="ltx_text ltx_font_smallcaps" id="S5.T5.11.1">MT-Patcher</span> trained on one student model to another. Performances are evaluated by COMET score. Models with <math alttext="\dagger" class="ltx_Math" display="inline" id="S5.T5.8.m1.1"><semantics id="S5.T5.8.m1.1b"><mo id="S5.T5.8.m1.1.1" xref="S5.T5.8.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T5.8.m1.1c"><ci id="S5.T5.8.m1.1.1.cmml" xref="S5.T5.8.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.m1.1e">†</annotation></semantics></math> are <span class="ltx_text ltx_font_smallcaps" id="S5.T5.12.2">MT-Patcher</span> (+ PE + PDS + WA) trained for the corresponding MT model. For reference, we also list the performances of the original student model and SeqKD-Full baselines.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Transferability of <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.1.1">MT-Patcher</span> </h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The construction of <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.1">MT-Patcher</span> is model-dependent; that is given an MT model, LLMs are finetuned on the data from GPT-4 which demonstrates how to execute the <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.2">MT-Patcher</span> pipeline on the translation of the corresponding MT model. Considering the cost of data collection and model training, one may question whether <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.3">MT-Patcher</span> is transferable, i.e., a patcher model for one MT model can improve the performance of another MT model. We present such results in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#S5.T5" title="Table 5 ‣ 5.3 The Effectiveness of Iterative Feedback ‣ 5 Discussion ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>. Although the performance of applying <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.4">MT-Patcher</span> to its dedicated MT model is superior, the application of <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.5">MT-Patcher</span> trained on another model still significantly surpasses the baseline results, suggesting the potential for a robust <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.6">MT-Patcher</span> across various MT models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">MT-Patcher</span>, a framework designed to leverage capabilities of LLMs to enhance the efficiency and effectiveness of translation knowledge transfer from LLMs to existing MT models. Our approach involves a pipeline that initially generates feedback on translations produced by MT models, followed by the synthesis of potential errors and diverse contexts to systematically rectify these translation errors. Through experimentation on both general and narrow domain MT benchmarks, we demonstrate that <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">MT-Patcher</span> effectively improves student MT models’ performances compared to SeqKD baselines, and exhibits successful transferability across different models.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In the future, we plan to refine our method from two angles. Firstly, previous works <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib7" title="">2019</a>; Riley et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#bib.bib26" title="">2020</a>)</cite> have identified translationese as a significant issue, and training on pseudo data generated by LLMs can exacerbate this problem. A promising solution could involve retrieving target sentences containing correction words and back-translating them to the source side. Secondly, the feedback’s <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">reason</span> field contains a wealth of valuable information. We intend to explore more efficient strategies to harness this data.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our method focuses on transferring translation knowledge, especially long-tailed lexical knowledge from LLMs to existing MT models, which cannot solve all kinds of translation errors, such as misunderstanding the sentence structure, over/under-translation, etc.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">We leverage GPT-4 as evaluators in multiple experiments in this paper. Despite its evaluation has been shown to correlate with human beings well in many previous works, there is still knowledge deficiency in itself and cannot guarantee that the evaluation contains no errors.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We would like to thank the anonymous reviewers for their insightful comments. Shujian Huang and Shanbo Cheng are the corresponding authors. This work is supported by National Science Foundation of China (No. 62376116, 62176120), the Liaoning Provincial Research Foundation for Basic Research (No. 2022-KF-26-02).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2023)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.564" title="">In-context examples selection for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 8857–8873, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baichuan Inc (2023)</span>
<span class="ltx_bibblock">
Baichuan Inc. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf" title="">Baichuan 2: Open Large-scale Language Models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, 2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=KuPixIqPiq" title="">Teaching large language models to self-debug</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2023)</span>
<span class="ltx_bibblock">
John Chung, Ece Kamar, and Saleema Amershi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.34" title="">Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 575–593, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Epstein and Voorhis (2001)</span>
<span class="ltx_bibblock">
Joyce L. Epstein and Frances L. Van Voorhis. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1207/S15326985EP3603_4" title="">More than minutes: Teachers’ roles in designing homework</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Educational Psychologist</em>, 36(3):181–193.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2019)</span>
<span class="ltx_bibblock">
Markus Freitag, Isaac Caswell, and Scott Roy. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5204" title="">APE at scale and its implications on MT evaluation biases</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</em>, pages 34–44, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2023)</span>
<span class="ltx_bibblock">
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/fu23d.html" title="">Specializing smaller language models towards multi-step reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">Proceedings of Machine Learning Research</em>, pages 10421–10430. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2018)</span>
<span class="ltx_bibblock">
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=B1l8BtlCb" title="">Non-autoregressive neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock">How good are GPT models at machine translation? a comprehensive evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, cs.CL/2302.09210v1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al. (2015)</span>
<span class="ltx_bibblock">
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:7200347" title="">Distilling the knowledge in a neural network</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ArXiv</em>, abs/1503.02531.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et al. (2023)</span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.507" title="">Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 8003–8017, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. (2023a)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.1001" title="">ParroT: Translating during chat using large language models tuned with human translation and feedback</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 15009–15020, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. (2023b)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023b.

</span>
<span class="ltx_bibblock">Is ChatGPT a good translator? Yes with GPT-4 as the engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, cs.CL/2301.08745v3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, cs.LG/2001.08361v1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Rush (2016)</span>
<span class="ltx_bibblock">
Yoon Kim and Alexander M. Rush. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D16-1139" title="">Sequence-level knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee Jr and Pruitt (1979)</span>
<span class="ltx_bibblock">
Jackson F Lee Jr and K Wayne Pruitt. 1979.

</span>
<span class="ltx_bibblock">Homework assignments: Classroom games or teaching tools?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">The Clearing House</em>, 53(1):31–35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. 2023.

</span>
<span class="ltx_bibblock">Eliciting the translation ability of large language models via multilingual finetuning with translation instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, cs.CL/2305.15083.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.emnlp-main.616" title="">Few-shot learning with multilingual generative language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Min Liu, Yu Bao, Chengqi Zhao, and Shujian Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v37i11.26555" title="">Selective knowledge distillation for non-autoregressive neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence</em>, AAAI’23/IAAI’23/EAAI’23. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=S37hOerQLB" title="">Self-refine: Iterative refinement with self-feedback</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.01116" title="">The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2306.01116</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riley et al. (2020)</span>
<span class="ltx_bibblock">
Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.691" title="">Translationese as a language in “multilingual” NMT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7737–7746, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. (2022)</span>
<span class="ltx_bibblock">
Gaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida Atighehchian, David Vazquez, and Dzmitry Bahdanau. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.nlp4convai-1.5" title="">Data augmentation for intent classification with off-the-shelf large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 4th Workshop on NLP for Conversational AI</em>, pages 47–57, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.704" title="">BLEURT: Learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7881–7892, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2018)</span>
<span class="ltx_bibblock">
Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2018.

</span>
<span class="ltx_bibblock">Multilingual neural machine translation with knowledge distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/GitHub" title="">Stanford alpaca: An instruction-following llama model</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et al. (2022)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022.

</span>
<span class="ltx_bibblock">Prompting palm for translation: Assessing strategies and performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, 2211.09102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Fusheng Wang, Jianhao Yan, Fandong Meng, and Jie Zhou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.504" title="">Selective knowledge distillation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6456–6466, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al. (2021)</span>
<span class="ltx_bibblock">
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung Park. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.192" title="">GPT3Mix: Leveraging large-scale language models for text augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 2225–2239, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.15895" title="">Large language model as attributed training data generator: A tale of diversity and bias</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021a)</span>
<span class="ltx_bibblock">
Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. 2021a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Fkpr2RYDvI1" title="">Synthbio: A case study in faster curation of text datasets</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2021b)</span>
<span class="ltx_bibblock">
Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.aiopen.2021.06.001" title="">WuDaoCorpora: A super large-scale chinese corpora for pre-training language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">AI Open</em>, 2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.18223" title="">A survey of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2019)</span>
<span class="ltx_bibblock">
Chunting Zhou, Jiatao Gu, and Graham Neubig. 2019.

</span>
<span class="ltx_bibblock">Understanding knowledge distillation in non-autoregressive machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023.

</span>
<span class="ltx_bibblock">Multilingual machine translation with large language models: Empirical results and analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, cs.CL/2304.04675v2.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A0.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="A0.F5.g1" src="extracted/2403.09522v2/figs/variants.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of variants of <span class="ltx_text ltx_font_smallcaps" id="A0.F5.2.1">MT-Patcher</span>. PDS denotes the parallel data synthesizer, and WA denotes the word analoger.</figcaption>
</figure>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompts for <span class="ltx_text ltx_font_smallcaps" id="A1.1.1">MT-Patcher</span> </h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T7" title="Table 7 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T8" title="Table 8 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T9" title="Table 9 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T10" title="Table 10 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">10</span></a> shows the prompt we used for the feedbacker, sentence analysis, parallel data synthesis and word analogy task, respectively.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T6.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.5.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T6.5.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.5.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.5.1.1.2.1">COMET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.5.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.5.1.1.3.1">BLEURT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.5.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T6.5.1.1.4.1">BLEU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T6.5.2.1.1">Student</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.5.2.1.2">82.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.5.2.1.3">70.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.5.2.1.4">26.4</td>
</tr>
<tr class="ltx_tr" id="A1.T6.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.5.3.2.1">SeqKD-Full</th>
<td class="ltx_td ltx_align_center" id="A1.T6.5.3.2.2">75.9</td>
<td class="ltx_td ltx_align_center" id="A1.T6.5.3.2.3">62.8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.5.3.2.4">22.3</td>
</tr>
<tr class="ltx_tr" id="A1.T6.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T6.5.4.3.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T6.5.4.3.1.1">MT-Patcher</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.5.4.3.2">81.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.5.4.3.3">69.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.5.4.3.4">26.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Translation performance on WMT22 German <math alttext="\to" class="ltx_Math" display="inline" id="A1.T6.3.m1.1"><semantics id="A1.T6.3.m1.1b"><mo id="A1.T6.3.m1.1.1" stretchy="false" xref="A1.T6.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A1.T6.3.m1.1c"><ci id="A1.T6.3.m1.1.1.cmml" xref="A1.T6.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="A1.T6.3.m1.1e">→</annotation></semantics></math> English test set. SeqKD-Full and <span class="ltx_text ltx_font_smallcaps" id="A1.T6.7.1">MT-Patcher</span> are finetuned student models on pseudo Chinese <math alttext="\to" class="ltx_Math" display="inline" id="A1.T6.4.m2.1"><semantics id="A1.T6.4.m2.1b"><mo id="A1.T6.4.m2.1.1" stretchy="false" xref="A1.T6.4.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A1.T6.4.m2.1c"><ci id="A1.T6.4.m2.1.1.cmml" xref="A1.T6.4.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.4.m2.1d">\to</annotation><annotation encoding="application/x-llamapun" id="A1.T6.4.m2.1e">→</annotation></semantics></math> English parallel sentences.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We fully finetune LLMs on the collected demonstration data from GPT-4 for 3 epochs. The learning rate is set to <math alttext="1e-5" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mrow id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml"><mn id="A2.p1.1.m1.1.1.2.2" xref="A2.p1.1.m1.1.1.2.2.cmml">1</mn><mo id="A2.p1.1.m1.1.1.2.1" xref="A2.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.p1.1.m1.1.1.2.3" xref="A2.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">−</mo><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><minus id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></minus><apply id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2"><times id="A2.p1.1.m1.1.1.2.1.cmml" xref="A2.p1.1.m1.1.1.2.1"></times><cn id="A2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A2.p1.1.m1.1.1.2.2">1</cn><ci id="A2.p1.1.m1.1.1.2.3.cmml" xref="A2.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A2.p1.1.m1.1.1.3.cmml" type="integer" xref="A2.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">1e-5</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">1 italic_e - 5</annotation></semantics></math>, and the batch size is 64. During training, we only compute the next token prediction loss on the response tokens.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span><span class="ltx_text ltx_font_smallcaps" id="A3.1.1">MT-Patcher</span> suffers less from catastrophic forgetting.</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.2">We test the German<math alttext="\to" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mo id="A3.p1.1.m1.1.1" stretchy="false" xref="A3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">→</annotation></semantics></math>English performance of competitors in the Chinese<math alttext="\to" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mo id="A3.p1.2.m2.1.1" stretchy="false" xref="A3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">→</annotation></semantics></math>English setting, including the original student model (ParroT-7B), SeqKD-Full, and <span class="ltx_text ltx_font_smallcaps" id="A3.p1.2.1">MT-Patcher</span> (PE). We found SeqKD-Full experiences a significant decrease in performance, while MT-Patcher’s performance degradation is much less. This suggests that <span class="ltx_text ltx_font_smallcaps" id="A3.p1.2.2">MT-Patcher</span> is less prone to catastrophic forgetting, thereby demonstrating its potential for repeated application to a target MT system without detriment to its initial capabilities.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of datasets used for chemistry materials and Chinese idioms</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">For chemistry materials, the data is extracted from <span class="ltx_text ltx_font_italic" id="A4.p1.1.1">Inventory of Existing Chemical Substances in China</span>, released by Ministry of Ecology and Environment, China <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mee.gov.cn/gkml/hbb/bgg/201301/t20130131_245810.htm" title="">https://www.mee.gov.cn/gkml/hbb/bgg/201301/t20130131_245810.htm</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">For Chinese idioms, we use the crawled data from the Github repo <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pwxcoo/chinese-xinhua" title="">https://github.com/pwxcoo/chinese-xinhua</a></span></span></span>, and have manually checked the data quality (Of the randomly selected 50 examples, there are only 2 examples that have quality issues).</p>
</div>
<div class="ltx_para" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">We split each word set to two subsets with 5500 and 500 words, respectively, and use GPT-4 to synthesize contexts for them. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A4.F6" title="Figure 6 ‣ Appendix D Details of datasets used for chemistry materials and Chinese idioms ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the process of constructing the monolingual set and two test sets.</p>
</div>
<figure class="ltx_figure" id="A4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="422" id="A4.F6.g1" src="extracted/2403.09522v2/figs/testset_flowchart.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Illustration of the process how the monolingual set and two test sets are splitted from initial collected word sets.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Prompts for Evaluation</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T11" title="Table 11 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">11</span></a> shows the prompt we used for evaluating the translation quality of Chinese idioms. Table <a class="ltx_ref" href="https://arxiv.org/html/2403.09522v2#A5.T12" title="Table 12 ‣ Appendix E Prompts for Evaluation ‣ MT-Patcher: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">12</span></a> shows the prompt we used for translation comparison between direct generation and feedback.</p>
</div>
<figure class="ltx_figure" id="A5.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig1.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig1.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig1.2.1">Assuming you are a highly proficient translator skilled at providing detailed and comprehensive assessments of machine translations. I will give you a &lt;srclang&gt; sentence X and its &lt;tgtlang&gt; translation Y, and I would like you to help assess the translation. 
<br class="ltx_break"/>1. You should first provide an overall assessment. 
<br class="ltx_break"/>2. Following that, 
<br class="ltx_break"/>- If there are no errors, just say "No error." and do not provide an explanation. 
<br class="ltx_break"/>- If there are errors, please specify 
<br class="ltx_break"/>- the error type, 
<br class="ltx_break"/>- the corresponding segment in the &lt;srclang&gt; sentence X, 
<br class="ltx_break"/>- the corresponding segment in the translation Y, 
<br class="ltx_break"/>- the reason for the error, 
<br class="ltx_break"/>- and the correct translation for the segment 
<br class="ltx_break"/>- If there are errors, you should also provide a good translation at the end of the assessment. 
<br class="ltx_break"/>4. For multiple errors, you should address them separately. 
<br class="ltx_break"/>5. Try to pinpoint the smallest segments containing errors and explain them, avoiding cases where the error encompasses the entire sentence. 
<br class="ltx_break"/>6. Carefully read the original text and the translation to identify all translation errors. 
<br class="ltx_break"/>7. Your response should be in English. 
<br class="ltx_break"/>8. Be concise. 
<br class="ltx_break"/>
<br class="ltx_break"/>Now, please assess the following translation: 
<br class="ltx_break"/>
<br class="ltx_break"/>&lt;srclang&gt;: &lt;srctext&gt; 
<br class="ltx_break"/>&lt;tgtlang&gt;: &lt;tgttext&gt; 
<br class="ltx_break"/>
<br class="ltx_break"/>Assessment: 
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Prompt that we use for the feedbacker task.</figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="A5.fig2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig2.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig2.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig2.2.1">Suppose you are a language expert of &lt;srclang&gt; and &lt;tgtlang&gt;. Given a sentence X, please point out its topic, domain and style. 
<br class="ltx_break"/>Input: 
<br class="ltx_break"/>X: &lt;srctext&gt; 
<br class="ltx_break"/>Output:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Prompt that we use for the sentence analysis task.</figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="A5.fig3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig3.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig3.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig3.2.1">Suppose you are a language expert of &lt;srclang&gt; and &lt;tgtlang&gt;. Given a topic, a domain and a style, as well as a bilingual word pair, please generate a pair of parallel sentences that adhere to the given topic, domain and style. They should also contain the given word pair. 
<br class="ltx_break"/>Input: 
<br class="ltx_break"/>Domain: &lt;domain&gt; 
<br class="ltx_break"/>Topic: &lt;topic&gt; 
<br class="ltx_break"/>Style: &lt;style&gt; 
<br class="ltx_break"/>Word Pair: &lt;wordpair&gt; 
<br class="ltx_break"/>
<br class="ltx_break"/>Output:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Prompt that we use for the parallel data synthesizer task.</figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="A5.fig4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig4.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig4.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig4.2.1">Assume you are a &lt;srclang&gt; and &lt;tgtlang&gt; language expert with a wealth of knowledge and associative ability in both languages. I will give you a word/phrase P from an &lt;srclang&gt; sentence X. Please associate from the following aspects and generate three words similar to X for each aspect, and provide the &lt;tgtlang&gt; translation of these words. 
<br class="ltx_break"/>
<br class="ltx_break"/>Aspects of association: 
<br class="ltx_break"/>- Category. What kind of category does this word belong to? 
<br class="ltx_break"/>- Semantics. What words often appear in the same context as the given word? 
<br class="ltx_break"/>
<br class="ltx_break"/>NOTE, the associated words should be rare words, so that it is unlike for a machine translation system to translate it correctly. 
<br class="ltx_break"/>
<br class="ltx_break"/>Input: 
<br class="ltx_break"/>X: &lt;srctext&gt; 
<br class="ltx_break"/>P: &lt;errorword&gt; 
<br class="ltx_break"/>
<br class="ltx_break"/>Output:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Prompt that we use for the word analogy task.</figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="A5.fig5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig5.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig5.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig5.2.1">Assume you are a language expert in English and Chinese. I will give you a Chinese idiom S, a sentence X that contains S, and a machine-generated English translation Y of the source sentence X.
I will also give you the explanation/definition E of the idiom S. Your task is to first identify the translation of S in Y, and judge whether the translation of the idiom is correct. 
<br class="ltx_break"/>
<br class="ltx_break"/>Note:
<br class="ltx_break"/>1. The score range is 0/1/2/3/4/5, where 
<br class="ltx_break"/>- 0: Completely incorrect translation or no translation 
<br class="ltx_break"/>- 1: Literal translation of the original, without conveying any implied meaning, leaving non-Chinese background readers baffled 
<br class="ltx_break"/>- 2: Literal translation of the original, partially conveying the implied meaning, easy for non-Chinese background readers to understand 
<br class="ltx_break"/>- 3: Interpretative translation of the idiom, but only partially conveying the implied meaning 
<br class="ltx_break"/>- 4: Interpretative translation of the idiom, fully conveying the implied meaning 
<br class="ltx_break"/>- 5: The translation perfectly conveys the implied meaning of the idiom, is very easy for all readers to understand, and also maintains the aesthetic sense of the original 
<br class="ltx_break"/>
<br class="ltx_break"/>2. You should generate the explanation of your decision concisely. 
<br class="ltx_break"/>Now, please process the following inputs:
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Prompt that we use for evaluating the quality of translating Chinese idioms.</figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="A5.fig6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="A5.fig6.1">{mdframed}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="A5.fig6.2" style="width:433.6pt;"><span class="ltx_text ltx_font_typewriter" id="A5.fig6.2.1">Assume you are a language expert in Chinese and English.
I will give you a sentence X, the word P in that sentence, and two translations of the sentence X: A and B. Your task is to assess which translation contains the correct translation of the word P. 
<br class="ltx_break"/>
<br class="ltx_break"/>Requirements: 
<br class="ltx_break"/>(1) Ignore other differences between the two translations. Only compare the translation of the word P. 
<br class="ltx_break"/>(2) Your answer should first state the reason for your comparison, and then give your comparison. 
<br class="ltx_break"/>(3) Your comparison should be A, B, C and D. 
<br class="ltx_break"/>- A: the first translation of the word P is better. 
<br class="ltx_break"/>- B: the second translation of the word P is better. 
<br class="ltx_break"/>- C: Both are fine. 
<br class="ltx_break"/>- D: Both are bad. 
<br class="ltx_break"/>
<br class="ltx_break"/>Now, please process the following inputs:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="A5.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Prompt that we use for comparing translations from direction generation and feedback.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu May  2 19:00:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
