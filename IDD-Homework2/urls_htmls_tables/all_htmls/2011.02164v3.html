<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.02164] An Improved Attention for Visual Question Answering</title><meta property="og:description" content="We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question wit…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Improved Attention for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Improved Attention for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.02164">

<!--Generated on Tue Mar 19 04:40:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An Improved Attention for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tanzila Rahman<sup id="id8.6.id1" class="ltx_sup"><span id="id8.6.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>   Shih-Han Chou<sup id="id9.7.id2" class="ltx_sup"><span id="id9.7.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>   Leonid Sigal<sup id="id10.8.id3" class="ltx_sup"><span id="id10.8.id3.1" class="ltx_text ltx_font_italic">1,2,3</span></sup>   Giuseppe Carenini<sup id="id11.9.id4" class="ltx_sup"><span id="id11.9.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id12.10.id5" class="ltx_sup">1</sup>Department of Computer Science
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> University of British Columbia
<br class="ltx_break">Vancouver
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> BC
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Canada
<br class="ltx_break"><sup id="id13.3.id1" class="ltx_sup">2</sup>Vector Institute for AI   <sup id="id14.4.id2" class="ltx_sup">3</sup>Canada CIFAR AI Chair 
<br class="ltx_break"><span id="id15.5.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{trahman8, shchou75, lsigal, carenini}@cs.ubc.ca</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an <em id="id16.id1.1" class="ltx_emph ltx_font_italic">information vector</em> and an <em id="id16.id1.2" class="ltx_emph ltx_font_italic">attention gate</em> using attention results and current context; and then adds another attention to generate final <em id="id16.id1.3" class="ltx_emph ltx_font_italic">attended information</em> by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves better performance than the baseline method.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Different perceptual modalities can capture complementary information about aspects of an object, event or activity. As a result, multimodal representations are often shown to perform better in inference.
Multimodal learning is widely used in the computer vision and forms basis for many visuo-lingual tasks, including image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, image-text matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>).
Visual question answering (VQA) is perhaps the most challenging, requiring detailed and intricate image and textual understanding (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Moreover, questions can be free-form and open-ended which requires VQA system to perform, simultaneously, a large collection of artificial intelligence tasks (<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, fine-grained recognition, object detection, activity recognition and visual common sense reasoning) to predict an accurate answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The answer format can also take different forms: a word, a phrase, yes/no, multiple choice, or a fill in the blank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2011.02164/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="58" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of our proposed framework.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> Given an image and a query question, we first extract visual and language features respectively. Our proposed Modular Co-Attention on Attention Network (MCAoAN) takes the features as inputs and refines both features jointly. The multi-modal attention fusion fuses the refined visual and language features and then predicts final answer.</span></span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Inspired by the recent advantages of deep neural network, attention based approaches are widely used to solve many computer vision problems including VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. An attention based approach for VQA was first introduced by Shih <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and nowadays it has become an essential component in most of the architectures. Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> include co-attention architecture to generate simultaneous attention in both visual and textual modality which increases prediction accuracy.
The limitation of these, more <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">global</span>, co-attention methods, is their inability to model interactions and attention among individual image regions and segments of text (<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">e.g.</span>, at the word token level).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, dense co-attention networks (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, DCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>) have been proposed, where each image region is able to interact with any (and all) words in the question. As a result, the models can get better understanding and reason about the image-question relationships; this, in turn, results in improved VQA performance. However, the bottleneck of these dense co-attention networks is the lack of self-attention within each modality, <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">e.g.</span>, region-to-region relationships in the image and word-to-word relationships in the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To overcome this, Yu <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed a deep Modular Co-Attention Network (MCAN) which consists of cascaded Modular Co-Attention (MCA) layers. MCA layer is obtained by combining two general attention units: self-attention (SA) and guided attention (GA). SA is able to capture intra-modal interactions (<span id="S1.p4.1.2" class="ltx_text ltx_font_italic">e.g.</span>, region-to-region and word-to-word) while GA can capture cross-modal interactions (<span id="S1.p4.1.3" class="ltx_text ltx_font_italic">e.g.</span>, word-to-region and region-to-word) by using multi-head attention architecture.
While expressive and highly flexible, this form of attention still has a limitations.
Specifically, the result is <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">always</span> a weighted combination of value pairs among which the model is attending. This maybe problematic when there is no closely related context over which the model is attending (<span id="S1.p4.1.5" class="ltx_text ltx_font_italic">e.g.</span>, a word for which no context word or image region exists). In such a case attention would result in a noisy or, worse, distracting output vector that can negatively impact the performance.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Motivated by Huang <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, in this paper we leverage the idea of Attention on Attention (AoA) module to address the above mentioned limitation.
The AoA module is cascaded several times to form a novel Modular Co-Attention on Attention Network (MCAoAN) which is an improved extension to Modular Co-Attention Network (MCAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
The AoA module generates an information vector and an attention gate by using two separate linear transformations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which is similar to GLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Attention results and query context are concatenated together and through a linear transformation we can obtain an information vector. Similarly through another linear transformation followed by a sigmoid activation function we can obtain an attention gate. By applying element-wise multiplication, we finally obtain attended information which builds relation between multiple attention heads and keep only the most related one discarding all irrelevant attention results. As a result, the model is able to predict more accurate answer. We also propose a multi-modal fusion mechanism to dynamically modulate modality importance while combining image and language features.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Contributions.</span> Our contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce an Attention on Attention module to form a Modular Co-attention on Attention Network (MCAoAN). MCAoAN captures intra- and inter-modal attention within and among visual and language modalities as well as able to mitigate information flow from irrelevant context.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We also present a multimodal attention-based fusion mechanism to incorporate both image and question features. Our fusion network dynamically decides how to weight each modality to generate final feature representation to predict the correct answer.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments on the VQA-v2 benchmark dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> illustrate that the proposed method outperforms competitors, establishing significantly better performance than the baseline methods in visual question answering.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first briefly describe existing approaches for visual question answering and later review classical approaches to fuse image and question features.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Question Answering</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Antol <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> first introduced the task of visual question answering (VQA), by combining computer vision with natural language processing, to mimic human understanding about a particular visual environment. The model used a CNN for feature extraction and an LSTM for language processing. The features were combined using element-wise multiplication in service of classifying the answers.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Over the last few years, a large number of deep neural networks have been proposed to improve the performance on VQA. Moreover, attention-based approaches became widely used to solve various sequence learning tasks, including VQA. The goal of attention module is to identify the most relevant part of image or textual content. Yang <em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> introduced an attention network to support multi-step reasoning for the image question answering task. A combination of bottom-up and top-down attention mechanism was presented in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. A set of salient image regions were proposed by bottom-up attention mechanism using Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. On the other hand, task specific context was used to predict an attention distribution by top-down mechanism over the image regions. A model-agnostic framework is proposed by Shah <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">et al.</em>  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which relies on cycle consistency to learn VQA model. Their model not only answers the posed question, but also generates diverse and semantically similar variations of questions conditioned on the answer. They enforce network to match the predicted answer with the ground truth answer to the original question. Wu <em id="S2.SS1.p2.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> propose a differential networks (DN), a novel plug and play module where differences between pair-wise features are used to reduce noise and learn inter-dependency between features. To extract image and text feature, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> are used respectively. Both features are refined by a differential module and finally combined to predict the answers.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Recently, co-attention based approaches are becoming popular. The goal of co-attention model is to learn image and question attention simultaneously. Lu <em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> introduced a co-attention network that jointly reasons about image and question attention in a hierarchical fashion. Yu <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposed an architecture to reduce irrelevant features by applying self attention for question embedding and question conditioned attention for image embedding. Multi-modal attention is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to focus on images, questions or answers feature simultaneously. Recently, bilinear attention is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> to locate more accurate objects. A multi-step dual attention for multimodal reasoning and matching is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. One major limitation of these co-attention based approaches is lack of dense interactions between different modalities. To overcome this limitation, dense co-attention based methods are proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. But dense co-attention can generate irrelevant vector in scenarios where nothing is related to the query. To overcome the problem, motivated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, in this paper we combine Attention-on-Attention (AoA) module with Modular co-attention network to improve existing architecture. Our revised attention mechanism delivers significantly better performance in VQA.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fusion Strategies for VQA</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To combine multi-modal features, sophisticated fusion technique is required. Depending on the type of fusion, existing VQA models can be divided into two categories: linear and bilinear <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Linear models use simple fusion approaches to combine image and question features. Simple element-wise summation and element-wise multiplication are used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> respectively. On the other hand, bilinear model uses more fine-grained approache to fuse image and question features. Fukui <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> used outer product to fuse multi-modal features. A low-rank projection followed by an element-wise multiplication is used by Kim <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. A Multi-modal Factorized Bilinear (MFB) pooling approach with co-attention learning is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Wu <em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposed a Differential Networks (DN) based Fusion (DF) approach which first calculates differences between image and textual feature elements and then combines the differential representations to predict final answer.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In this paper, we propose an attention-based multi-modal fusion to combine image and question features by dynamically deciding how much weight to put on each modality; the weighted features are used to predict final answer.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Motivated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, in this paper we present Modular Co-Attention on Attention Network (MCAoAN) module which is an extension of Modular Co-Attention Network (MCAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
MCAoAN consists of Modular Co-Attention on Attention (MCAoA) layer which is a composition of two primary attention units: Self Attention on Attention (SAoA) and Guided Attention on Attention (GAoA) unit. In this section, we first discuss SAoA and GAoA units in Section <a href="#S3.SS1" title="3.1 SAoA and GAoA Units ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> followed by Modular Co-Attention on Attention (MCAoA) layer in Section <a href="#S3.SS2" title="3.2 MCAoA layers ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Lastly we present our MCAoAN with multimodal fusion mechanism in Section <a href="#S3.SS3" title="3.3 MCAoAN ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> and Section <a href="#S3.SS4" title="3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> respectively.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2011.02164/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="69" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Self Attention on Attention block</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2011.02164/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="72" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Guided Attention on Attention block</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of the two basic attention units.<span id="S3.F2.4.2.1" class="ltx_text ltx_font_medium"> (a) Self Attention on Attention block (SAoA), which takes input feature X and output attended feature Z for X; and (b) Guided Attention on Attention block (GAoA),which takes two input features X and Y and generate attended feature Z for the input X guided by Y feature. Here X and Y represents image and question features respectively.</span></span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>SAoA and GAoA Units</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our SAoA unit (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)) is an extension of multi-head self attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Multi-head attention consists of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation></semantics></math> parallel heads where each head can be represented as a scaled dot product attention function as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="{\operatorname{f}}_{\operatorname{att}}={\operatorname{f}}(Q,K,V)={\text{\tt Softmax}}\left({\frac{QK}{\sqrt{d}}}\right)V," display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.2" xref="S3.E1.m1.6.6.1.1.2.cmml"><mi mathvariant="normal" id="S3.E1.m1.6.6.1.1.2.2" xref="S3.E1.m1.6.6.1.1.2.2.cmml">f</mi><mi id="S3.E1.m1.6.6.1.1.2.3" xref="S3.E1.m1.6.6.1.1.2.3.cmml">att</mi></msub><mo id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.4.2" xref="S3.E1.m1.6.6.1.1.4.1.cmml"><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">f</mi><mo id="S3.E1.m1.6.6.1.1.4.2a" xref="S3.E1.m1.6.6.1.1.4.1.cmml">⁡</mo><mrow id="S3.E1.m1.6.6.1.1.4.2.1" xref="S3.E1.m1.6.6.1.1.4.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.1.1.4.2.1.1" xref="S3.E1.m1.6.6.1.1.4.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Q</mi><mo id="S3.E1.m1.6.6.1.1.4.2.1.2" xref="S3.E1.m1.6.6.1.1.4.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">K</mi><mo id="S3.E1.m1.6.6.1.1.4.2.1.3" xref="S3.E1.m1.6.6.1.1.4.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">V</mi><mo stretchy="false" id="S3.E1.m1.6.6.1.1.4.2.1.4" xref="S3.E1.m1.6.6.1.1.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.5" xref="S3.E1.m1.6.6.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.6" xref="S3.E1.m1.6.6.1.1.6.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.6.6.1.1.6.2" xref="S3.E1.m1.6.6.1.1.6.2a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.6.1" xref="S3.E1.m1.6.6.1.1.6.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.6.3.2" xref="S3.E1.m1.5.5.cmml"><mo id="S3.E1.m1.6.6.1.1.6.3.2.1" xref="S3.E1.m1.5.5.cmml">(</mo><mfrac id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml"><mi id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.1" xref="S3.E1.m1.5.5.2.1.cmml">​</mo><mi id="S3.E1.m1.5.5.2.3" xref="S3.E1.m1.5.5.2.3.cmml">K</mi></mrow><msqrt id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml"><mi id="S3.E1.m1.5.5.3.2" xref="S3.E1.m1.5.5.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.E1.m1.6.6.1.1.6.3.2.2" xref="S3.E1.m1.5.5.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.6.1a" xref="S3.E1.m1.6.6.1.1.6.1.cmml">​</mo><mi id="S3.E1.m1.6.6.1.1.6.4" xref="S3.E1.m1.6.6.1.1.6.4.cmml">V</mi></mrow></mrow><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1"><and id="S3.E1.m1.6.6.1.1a.cmml" xref="S3.E1.m1.6.6.1"></and><apply id="S3.E1.m1.6.6.1.1b.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3"></eq><apply id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.2.2.cmml" xref="S3.E1.m1.6.6.1.1.2.2">f</ci><ci id="S3.E1.m1.6.6.1.1.2.3.cmml" xref="S3.E1.m1.6.6.1.1.2.3">att</ci></apply><apply id="S3.E1.m1.6.6.1.1.4.1.cmml" xref="S3.E1.m1.6.6.1.1.4.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">f</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑄</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝐾</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑉</ci></apply></apply><apply id="S3.E1.m1.6.6.1.1c.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.5.cmml" xref="S3.E1.m1.6.6.1.1.5"></eq><share href="#S3.E1.m1.6.6.1.1.4.cmml" id="S3.E1.m1.6.6.1.1d.cmml" xref="S3.E1.m1.6.6.1"></share><apply id="S3.E1.m1.6.6.1.1.6.cmml" xref="S3.E1.m1.6.6.1.1.6"><times id="S3.E1.m1.6.6.1.1.6.1.cmml" xref="S3.E1.m1.6.6.1.1.6.1"></times><ci id="S3.E1.m1.6.6.1.1.6.2a.cmml" xref="S3.E1.m1.6.6.1.1.6.2"><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.6.6.1.1.6.2.cmml" xref="S3.E1.m1.6.6.1.1.6.2">Softmax</mtext></ci><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.6.6.1.1.6.3.2"><divide id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.6.6.1.1.6.3.2"></divide><apply id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"><times id="S3.E1.m1.5.5.2.1.cmml" xref="S3.E1.m1.5.5.2.1"></times><ci id="S3.E1.m1.5.5.2.2.cmml" xref="S3.E1.m1.5.5.2.2">𝑄</ci><ci id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.3">𝐾</ci></apply><apply id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"><root id="S3.E1.m1.5.5.3a.cmml" xref="S3.E1.m1.5.5.3"></root><ci id="S3.E1.m1.5.5.3.2.cmml" xref="S3.E1.m1.5.5.3.2">𝑑</ci></apply></apply><ci id="S3.E1.m1.6.6.1.1.6.4.cmml" xref="S3.E1.m1.6.6.1.1.6.4">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">{\operatorname{f}}_{\operatorname{att}}={\operatorname{f}}(Q,K,V)={\text{\tt Softmax}}\left({\frac{QK}{\sqrt{d}}}\right)V,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.7" class="ltx_p">where attention function <math id="S3.SS1.p1.2.m1.4" class="ltx_Math" alttext="\operatorname{f}(Q,K,V)" display="inline"><semantics id="S3.SS1.p1.2.m1.4a"><mrow id="S3.SS1.p1.2.m1.4.5.2" xref="S3.SS1.p1.2.m1.4.5.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.2.m1.1.1" xref="S3.SS1.p1.2.m1.1.1.cmml">f</mi><mo id="S3.SS1.p1.2.m1.4.5.2a" xref="S3.SS1.p1.2.m1.4.5.1.cmml">⁡</mo><mrow id="S3.SS1.p1.2.m1.4.5.2.1" xref="S3.SS1.p1.2.m1.4.5.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m1.4.5.2.1.1" xref="S3.SS1.p1.2.m1.4.5.1.cmml">(</mo><mi id="S3.SS1.p1.2.m1.2.2" xref="S3.SS1.p1.2.m1.2.2.cmml">Q</mi><mo id="S3.SS1.p1.2.m1.4.5.2.1.2" xref="S3.SS1.p1.2.m1.4.5.1.cmml">,</mo><mi id="S3.SS1.p1.2.m1.3.3" xref="S3.SS1.p1.2.m1.3.3.cmml">K</mi><mo id="S3.SS1.p1.2.m1.4.5.2.1.3" xref="S3.SS1.p1.2.m1.4.5.1.cmml">,</mo><mi id="S3.SS1.p1.2.m1.4.4" xref="S3.SS1.p1.2.m1.4.4.cmml">V</mi><mo stretchy="false" id="S3.SS1.p1.2.m1.4.5.2.1.4" xref="S3.SS1.p1.2.m1.4.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m1.4b"><apply id="S3.SS1.p1.2.m1.4.5.1.cmml" xref="S3.SS1.p1.2.m1.4.5.2"><ci id="S3.SS1.p1.2.m1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1">f</ci><ci id="S3.SS1.p1.2.m1.2.2.cmml" xref="S3.SS1.p1.2.m1.2.2">𝑄</ci><ci id="S3.SS1.p1.2.m1.3.3.cmml" xref="S3.SS1.p1.2.m1.3.3">𝐾</ci><ci id="S3.SS1.p1.2.m1.4.4.cmml" xref="S3.SS1.p1.2.m1.4.4">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m1.4c">\operatorname{f}(Q,K,V)</annotation></semantics></math> operates on Q, K and V corresponds to query, key and value respectively. The output of this attention function is the weighted average vector <math id="S3.SS1.p1.3.m2.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S3.SS1.p1.3.m2.1a"><msup id="S3.SS1.p1.3.m2.1.1" xref="S3.SS1.p1.3.m2.1.1.cmml"><mi id="S3.SS1.p1.3.m2.1.1.2" xref="S3.SS1.p1.3.m2.1.1.2.cmml">V</mi><mo id="S3.SS1.p1.3.m2.1.1.3" xref="S3.SS1.p1.3.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m2.1b"><apply id="S3.SS1.p1.3.m2.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m2.1.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m2.1.1.2.cmml" xref="S3.SS1.p1.3.m2.1.1.2">𝑉</ci><ci id="S3.SS1.p1.3.m2.1.1.3.cmml" xref="S3.SS1.p1.3.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m2.1c">V^{\prime}</annotation></semantics></math>. To do so, first we calculate the similarity scores between Q and K; and normalize the scores with <span id="S3.SS1.p1.7.1" class="ltx_text ltx_font_typewriter">Softmax</span>. The normalized scores are then used together with V to generate weighted average vector <math id="S3.SS1.p1.4.m3.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S3.SS1.p1.4.m3.1a"><msup id="S3.SS1.p1.4.m3.1.1" xref="S3.SS1.p1.4.m3.1.1.cmml"><mi id="S3.SS1.p1.4.m3.1.1.2" xref="S3.SS1.p1.4.m3.1.1.2.cmml">V</mi><mo id="S3.SS1.p1.4.m3.1.1.3" xref="S3.SS1.p1.4.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m3.1b"><apply id="S3.SS1.p1.4.m3.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m3.1.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.4.m3.1.1.2.cmml" xref="S3.SS1.p1.4.m3.1.1.2">𝑉</ci><ci id="S3.SS1.p1.4.m3.1.1.3.cmml" xref="S3.SS1.p1.4.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m3.1c">V^{\prime}</annotation></semantics></math>. Here, <math id="S3.SS1.p1.5.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p1.5.m4.1a"><mi id="S3.SS1.p1.5.m4.1.1" xref="S3.SS1.p1.5.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m4.1b"><ci id="S3.SS1.p1.5.m4.1.1.cmml" xref="S3.SS1.p1.5.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m4.1c">d</annotation></semantics></math> is the dimension of <math id="S3.SS1.p1.6.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p1.6.m5.1a"><mi id="S3.SS1.p1.6.m5.1.1" xref="S3.SS1.p1.6.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m5.1b"><ci id="S3.SS1.p1.6.m5.1.1.cmml" xref="S3.SS1.p1.6.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m5.1c">Q</annotation></semantics></math> and <math id="S3.SS1.p1.7.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p1.7.m6.1a"><mi id="S3.SS1.p1.7.m6.1.1" xref="S3.SS1.p1.7.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m6.1b"><ci id="S3.SS1.p1.7.m6.1.1.cmml" xref="S3.SS1.p1.7.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m6.1c">K</annotation></semantics></math>; both dimensions are the same.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">The multi-head attention module always generates weighted vector, no matter whether it finds any relation between Q and K/V or not. So this approach can easily mislead or generate wrong answer for VQA. Therefore, following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we incorporate another attention function over the multi-head attention module to measure the relation between attention results (<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msup id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">V</mi><mo id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑉</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">V^{\prime}</annotation></semantics></math>) and the query(<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">Q</annotation></semantics></math>). The final AoA block will generate an information vector (<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">I</annotation></semantics></math>) and attention gate (<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">G</annotation></semantics></math>) through two separate linear transformations which can be represented as follows:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="{\operatorname{I}}={\operatorname{W}}_{\operatorname{Q}}{\operatorname{Q}}+{\operatorname{W}}_{\operatorname{V}^{\prime}}{\operatorname{V}}^{\prime}+{\operatorname{b}}_{\operatorname{I}}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">I</mi><mo id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><msub id="S3.E2.m1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.3.2.1.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.2.1.2" xref="S3.E2.m1.1.1.1.1.3.2.1.2.cmml">W</mi><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.2.1.3" xref="S3.E2.m1.1.1.1.1.3.2.1.3.cmml">Q</mi></msub><mo lspace="0.167em" id="S3.E2.m1.1.1.1.1.3.2a" xref="S3.E2.m1.1.1.1.1.3.2.cmml">⁡</mo><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml">Q</mi></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><msub id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.3.1.2" xref="S3.E2.m1.1.1.1.1.3.3.1.2.cmml">W</mi><msup id="S3.E2.m1.1.1.1.1.3.3.1.3" xref="S3.E2.m1.1.1.1.1.3.3.1.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.3.1.3.2" xref="S3.E2.m1.1.1.1.1.3.3.1.3.2.cmml">V</mi><mo id="S3.E2.m1.1.1.1.1.3.3.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.1.3.3.cmml">′</mo></msup></msub><mo lspace="0.167em" id="S3.E2.m1.1.1.1.1.3.3a" xref="S3.E2.m1.1.1.1.1.3.3.cmml">⁡</mo><msup id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.1.1.3.3.2.2.cmml">V</mi><mo id="S3.E2.m1.1.1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.1.1.3.3.2.3.cmml">′</mo></msup></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.1.3.1a" xref="S3.E2.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E2.m1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.3.4.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.4.2" xref="S3.E2.m1.1.1.1.1.3.4.2.cmml">b</mi><mi mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.4.3" xref="S3.E2.m1.1.1.1.1.3.4.3.cmml">I</mi></msub></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2">I</ci><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><plus id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><apply id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1.2">W</ci><ci id="S3.E2.m1.1.1.1.1.3.2.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.1.3">Q</ci></apply><ci id="S3.E2.m1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.2">Q</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><apply id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.2">W</ci><apply id="S3.E2.m1.1.1.1.1.3.3.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3.2">V</ci><ci id="S3.E2.m1.1.1.1.1.3.3.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3.3">′</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.2">V</ci><ci id="S3.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.3">′</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.4.1.cmml" xref="S3.E2.m1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.4.2.cmml" xref="S3.E2.m1.1.1.1.1.3.4.2">b</ci><ci id="S3.E2.m1.1.1.1.1.3.4.3.cmml" xref="S3.E2.m1.1.1.1.1.3.4.3">I</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">{\operatorname{I}}={\operatorname{W}}_{\operatorname{Q}}{\operatorname{Q}}+{\operatorname{W}}_{\operatorname{V}^{\prime}}{\operatorname{V}}^{\prime}+{\operatorname{b}}_{\operatorname{I}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="{\operatorname{G}}=\sigma({\operatorname{W}}_{\operatorname{G}}{\operatorname{Q}}+{\operatorname{W}}_{\operatorname{G}^{\prime}}{\operatorname{V}}^{\prime}+{\operatorname{b}}_{\operatorname{G}})," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">G</mi><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.2.cmml">W</mi><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.3.cmml">G</mi></msub><mo lspace="0.167em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">Q</mi></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.2.cmml">W</mi><msup id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.2.cmml">G</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.3.cmml">′</mo></msup></msub><mo lspace="0.167em" id="S3.E3.m1.1.1.1.1.1.1.1.1.3a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">⁡</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">V</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">′</mo></msup></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml">b</mi><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml">G</mi></msub></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">G</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝜎</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.2">W</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.3">G</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2">Q</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.2">W</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.2">G</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.3.3">′</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2">V</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.3">′</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2">b</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3">G</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">{\operatorname{G}}=\sigma({\operatorname{W}}_{\operatorname{G}}{\operatorname{Q}}+{\operatorname{W}}_{\operatorname{G}^{\prime}}{\operatorname{V}}^{\prime}+{\operatorname{b}}_{\operatorname{G}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2011.02164/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="70" height="73" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of Modular Co-Attention on Attention (MCAoA) layer.<span id="S3.F3.4.2.1" class="ltx_text ltx_font_medium"> It consists of two attention units: Self Attention on Attention (SAoA) unit and Guided Attention on Attention (GAoA) unit where Y and X denotes question and image features respectively.</span></span></figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.17" class="ltx_p">Here, <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="{\operatorname{W}}_{\operatorname{Q}}" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><msub id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">W</mi><mi mathvariant="normal" id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">W</ci><ci id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">Q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">{\operatorname{W}}_{\operatorname{Q}}</annotation></semantics></math>, <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="{\operatorname{W}}_{\operatorname{V}^{\prime}}" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><msub id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">W</mi><msup id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS1.p5.2.m2.1.1.3.2" xref="S3.SS1.p5.2.m2.1.1.3.2.cmml">V</mi><mo id="S3.SS1.p5.2.m2.1.1.3.3" xref="S3.SS1.p5.2.m2.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">W</ci><apply id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.2.m2.1.1.3.1.cmml" xref="S3.SS1.p5.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p5.2.m2.1.1.3.2.cmml" xref="S3.SS1.p5.2.m2.1.1.3.2">V</ci><ci id="S3.SS1.p5.2.m2.1.1.3.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">{\operatorname{W}}_{\operatorname{V}^{\prime}}</annotation></semantics></math>, <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="{\operatorname{W}}_{\operatorname{G}}" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><msub id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">W</mi><mi mathvariant="normal" id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">W</ci><ci id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">G</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">{\operatorname{W}}_{\operatorname{G}}</annotation></semantics></math>, <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="{\operatorname{W}}_{\operatorname{G}^{\prime}}" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><msub id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.4.m4.1.1.2" xref="S3.SS1.p5.4.m4.1.1.2.cmml">W</mi><msup id="S3.SS1.p5.4.m4.1.1.3" xref="S3.SS1.p5.4.m4.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS1.p5.4.m4.1.1.3.2" xref="S3.SS1.p5.4.m4.1.1.3.2.cmml">G</mi><mo id="S3.SS1.p5.4.m4.1.1.3.3" xref="S3.SS1.p5.4.m4.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><apply id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.2.cmml" xref="S3.SS1.p5.4.m4.1.1.2">W</ci><apply id="S3.SS1.p5.4.m4.1.1.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p5.4.m4.1.1.3.1.cmml" xref="S3.SS1.p5.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p5.4.m4.1.1.3.2.cmml" xref="S3.SS1.p5.4.m4.1.1.3.2">G</ci><ci id="S3.SS1.p5.4.m4.1.1.3.3.cmml" xref="S3.SS1.p5.4.m4.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">{\operatorname{W}}_{\operatorname{G}^{\prime}}</annotation></semantics></math> <math id="S3.SS1.p5.5.m5.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.SS1.p5.5.m5.1a"><mo id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><in id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">\in</annotation></semantics></math> <math id="S3.SS1.p5.6.m6.1" class="ltx_Math" alttext="{\mathbb{R}}^{d\times d}" display="inline"><semantics id="S3.SS1.p5.6.m6.1a"><msup id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml"><mi id="S3.SS1.p5.6.m6.1.1.2" xref="S3.SS1.p5.6.m6.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.p5.6.m6.1.1.3" xref="S3.SS1.p5.6.m6.1.1.3.cmml"><mi id="S3.SS1.p5.6.m6.1.1.3.2" xref="S3.SS1.p5.6.m6.1.1.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.6.m6.1.1.3.1" xref="S3.SS1.p5.6.m6.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p5.6.m6.1.1.3.3" xref="S3.SS1.p5.6.m6.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><apply id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.6.m6.1.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p5.6.m6.1.1.2.cmml" xref="S3.SS1.p5.6.m6.1.1.2">ℝ</ci><apply id="S3.SS1.p5.6.m6.1.1.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3"><times id="S3.SS1.p5.6.m6.1.1.3.1.cmml" xref="S3.SS1.p5.6.m6.1.1.3.1"></times><ci id="S3.SS1.p5.6.m6.1.1.3.2.cmml" xref="S3.SS1.p5.6.m6.1.1.3.2">𝑑</ci><ci id="S3.SS1.p5.6.m6.1.1.3.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">{\mathbb{R}}^{d\times d}</annotation></semantics></math> and <math id="S3.SS1.p5.7.m7.1" class="ltx_Math" alttext="{\operatorname{b}}_{\operatorname{I}}" display="inline"><semantics id="S3.SS1.p5.7.m7.1a"><msub id="S3.SS1.p5.7.m7.1.1" xref="S3.SS1.p5.7.m7.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.7.m7.1.1.2" xref="S3.SS1.p5.7.m7.1.1.2.cmml">b</mi><mi mathvariant="normal" id="S3.SS1.p5.7.m7.1.1.3" xref="S3.SS1.p5.7.m7.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.7.m7.1b"><apply id="S3.SS1.p5.7.m7.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.7.m7.1.1.1.cmml" xref="S3.SS1.p5.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p5.7.m7.1.1.2.cmml" xref="S3.SS1.p5.7.m7.1.1.2">b</ci><ci id="S3.SS1.p5.7.m7.1.1.3.cmml" xref="S3.SS1.p5.7.m7.1.1.3">I</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.7.m7.1c">{\operatorname{b}}_{\operatorname{I}}</annotation></semantics></math>, <math id="S3.SS1.p5.8.m8.1" class="ltx_Math" alttext="{\operatorname{b}}_{\operatorname{G}}" display="inline"><semantics id="S3.SS1.p5.8.m8.1a"><msub id="S3.SS1.p5.8.m8.1.1" xref="S3.SS1.p5.8.m8.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.8.m8.1.1.2" xref="S3.SS1.p5.8.m8.1.1.2.cmml">b</mi><mi mathvariant="normal" id="S3.SS1.p5.8.m8.1.1.3" xref="S3.SS1.p5.8.m8.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.8.m8.1b"><apply id="S3.SS1.p5.8.m8.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.8.m8.1.1.1.cmml" xref="S3.SS1.p5.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p5.8.m8.1.1.2.cmml" xref="S3.SS1.p5.8.m8.1.1.2">b</ci><ci id="S3.SS1.p5.8.m8.1.1.3.cmml" xref="S3.SS1.p5.8.m8.1.1.3">G</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.8.m8.1c">{\operatorname{b}}_{\operatorname{G}}</annotation></semantics></math> <math id="S3.SS1.p5.9.m9.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S3.SS1.p5.9.m9.1a"><mo id="S3.SS1.p5.9.m9.1.1" xref="S3.SS1.p5.9.m9.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.9.m9.1b"><in id="S3.SS1.p5.9.m9.1.1.cmml" xref="S3.SS1.p5.9.m9.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.9.m9.1c">\in</annotation></semantics></math> <math id="S3.SS1.p5.10.m10.1" class="ltx_Math" alttext="{\mathbb{R}}^{d}" display="inline"><semantics id="S3.SS1.p5.10.m10.1a"><msup id="S3.SS1.p5.10.m10.1.1" xref="S3.SS1.p5.10.m10.1.1.cmml"><mi id="S3.SS1.p5.10.m10.1.1.2" xref="S3.SS1.p5.10.m10.1.1.2.cmml">ℝ</mi><mi id="S3.SS1.p5.10.m10.1.1.3" xref="S3.SS1.p5.10.m10.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.10.m10.1b"><apply id="S3.SS1.p5.10.m10.1.1.cmml" xref="S3.SS1.p5.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.10.m10.1.1.1.cmml" xref="S3.SS1.p5.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p5.10.m10.1.1.2.cmml" xref="S3.SS1.p5.10.m10.1.1.2">ℝ</ci><ci id="S3.SS1.p5.10.m10.1.1.3.cmml" xref="S3.SS1.p5.10.m10.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.10.m10.1c">{\mathbb{R}}^{d}</annotation></semantics></math>. <math id="S3.SS1.p5.11.m11.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p5.11.m11.1a"><mi id="S3.SS1.p5.11.m11.1.1" xref="S3.SS1.p5.11.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.11.m11.1b"><ci id="S3.SS1.p5.11.m11.1.1.cmml" xref="S3.SS1.p5.11.m11.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.11.m11.1c">d</annotation></semantics></math> is the dimension of <math id="S3.SS1.p5.12.m12.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p5.12.m12.1a"><mi id="S3.SS1.p5.12.m12.1.1" xref="S3.SS1.p5.12.m12.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.12.m12.1b"><ci id="S3.SS1.p5.12.m12.1.1.cmml" xref="S3.SS1.p5.12.m12.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.12.m12.1c">Q</annotation></semantics></math> and <math id="S3.SS1.p5.13.m13.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S3.SS1.p5.13.m13.1a"><msup id="S3.SS1.p5.13.m13.1.1" xref="S3.SS1.p5.13.m13.1.1.cmml"><mi id="S3.SS1.p5.13.m13.1.1.2" xref="S3.SS1.p5.13.m13.1.1.2.cmml">V</mi><mo id="S3.SS1.p5.13.m13.1.1.3" xref="S3.SS1.p5.13.m13.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.13.m13.1b"><apply id="S3.SS1.p5.13.m13.1.1.cmml" xref="S3.SS1.p5.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.13.m13.1.1.1.cmml" xref="S3.SS1.p5.13.m13.1.1">superscript</csymbol><ci id="S3.SS1.p5.13.m13.1.1.2.cmml" xref="S3.SS1.p5.13.m13.1.1.2">𝑉</ci><ci id="S3.SS1.p5.13.m13.1.1.3.cmml" xref="S3.SS1.p5.13.m13.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.13.m13.1c">V^{\prime}</annotation></semantics></math> where
<math id="S3.SS1.p5.14.m14.1" class="ltx_Math" alttext="V^{\prime}" display="inline"><semantics id="S3.SS1.p5.14.m14.1a"><msup id="S3.SS1.p5.14.m14.1.1" xref="S3.SS1.p5.14.m14.1.1.cmml"><mi id="S3.SS1.p5.14.m14.1.1.2" xref="S3.SS1.p5.14.m14.1.1.2.cmml">V</mi><mo id="S3.SS1.p5.14.m14.1.1.3" xref="S3.SS1.p5.14.m14.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.14.m14.1b"><apply id="S3.SS1.p5.14.m14.1.1.cmml" xref="S3.SS1.p5.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.14.m14.1.1.1.cmml" xref="S3.SS1.p5.14.m14.1.1">superscript</csymbol><ci id="S3.SS1.p5.14.m14.1.1.2.cmml" xref="S3.SS1.p5.14.m14.1.1.2">𝑉</ci><ci id="S3.SS1.p5.14.m14.1.1.3.cmml" xref="S3.SS1.p5.14.m14.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.14.m14.1c">V^{\prime}</annotation></semantics></math> = <math id="S3.SS1.p5.15.m15.1" class="ltx_Math" alttext="{\operatorname{f}}_{\operatorname{att}}" display="inline"><semantics id="S3.SS1.p5.15.m15.1a"><msub id="S3.SS1.p5.15.m15.1.1" xref="S3.SS1.p5.15.m15.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.15.m15.1.1.2" xref="S3.SS1.p5.15.m15.1.1.2.cmml">f</mi><mi id="S3.SS1.p5.15.m15.1.1.3" xref="S3.SS1.p5.15.m15.1.1.3.cmml">att</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.15.m15.1b"><apply id="S3.SS1.p5.15.m15.1.1.cmml" xref="S3.SS1.p5.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.15.m15.1.1.1.cmml" xref="S3.SS1.p5.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p5.15.m15.1.1.2.cmml" xref="S3.SS1.p5.15.m15.1.1.2">f</ci><ci id="S3.SS1.p5.15.m15.1.1.3.cmml" xref="S3.SS1.p5.15.m15.1.1.3">att</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.15.m15.1c">{\operatorname{f}}_{\operatorname{att}}</annotation></semantics></math> and <math id="S3.SS1.p5.16.m16.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p5.16.m16.1a"><mi id="S3.SS1.p5.16.m16.1.1" xref="S3.SS1.p5.16.m16.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.16.m16.1b"><ci id="S3.SS1.p5.16.m16.1.1.cmml" xref="S3.SS1.p5.16.m16.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.16.m16.1c">\sigma</annotation></semantics></math> denotes sigmoid function. AoA block adds another attention via element-wise multiplication between both information vector and attention gate. Moreover, SAoA uses a point-wise feed-forward layer after the AoA block, considering only input features <math id="S3.SS1.p5.17.m17.4" class="ltx_Math" alttext="X=[x_{1},x_{2},...,x_{m}]\in{\mathbb{R}}" display="inline"><semantics id="S3.SS1.p5.17.m17.4a"><mrow id="S3.SS1.p5.17.m17.4.4" xref="S3.SS1.p5.17.m17.4.4.cmml"><mi id="S3.SS1.p5.17.m17.4.4.5" xref="S3.SS1.p5.17.m17.4.4.5.cmml">X</mi><mo id="S3.SS1.p5.17.m17.4.4.6" xref="S3.SS1.p5.17.m17.4.4.6.cmml">=</mo><mrow id="S3.SS1.p5.17.m17.4.4.3.3" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p5.17.m17.4.4.3.3.4" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml">[</mo><msub id="S3.SS1.p5.17.m17.2.2.1.1.1" xref="S3.SS1.p5.17.m17.2.2.1.1.1.cmml"><mi id="S3.SS1.p5.17.m17.2.2.1.1.1.2" xref="S3.SS1.p5.17.m17.2.2.1.1.1.2.cmml">x</mi><mn id="S3.SS1.p5.17.m17.2.2.1.1.1.3" xref="S3.SS1.p5.17.m17.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p5.17.m17.4.4.3.3.5" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p5.17.m17.3.3.2.2.2" xref="S3.SS1.p5.17.m17.3.3.2.2.2.cmml"><mi id="S3.SS1.p5.17.m17.3.3.2.2.2.2" xref="S3.SS1.p5.17.m17.3.3.2.2.2.2.cmml">x</mi><mn id="S3.SS1.p5.17.m17.3.3.2.2.2.3" xref="S3.SS1.p5.17.m17.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p5.17.m17.4.4.3.3.6" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p5.17.m17.1.1" xref="S3.SS1.p5.17.m17.1.1.cmml">…</mi><mo id="S3.SS1.p5.17.m17.4.4.3.3.7" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p5.17.m17.4.4.3.3.3" xref="S3.SS1.p5.17.m17.4.4.3.3.3.cmml"><mi id="S3.SS1.p5.17.m17.4.4.3.3.3.2" xref="S3.SS1.p5.17.m17.4.4.3.3.3.2.cmml">x</mi><mi id="S3.SS1.p5.17.m17.4.4.3.3.3.3" xref="S3.SS1.p5.17.m17.4.4.3.3.3.3.cmml">m</mi></msub><mo stretchy="false" id="S3.SS1.p5.17.m17.4.4.3.3.8" xref="S3.SS1.p5.17.m17.4.4.3.4.cmml">]</mo></mrow><mo id="S3.SS1.p5.17.m17.4.4.7" xref="S3.SS1.p5.17.m17.4.4.7.cmml">∈</mo><mi id="S3.SS1.p5.17.m17.4.4.8" xref="S3.SS1.p5.17.m17.4.4.8.cmml">ℝ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.17.m17.4b"><apply id="S3.SS1.p5.17.m17.4.4.cmml" xref="S3.SS1.p5.17.m17.4.4"><and id="S3.SS1.p5.17.m17.4.4a.cmml" xref="S3.SS1.p5.17.m17.4.4"></and><apply id="S3.SS1.p5.17.m17.4.4b.cmml" xref="S3.SS1.p5.17.m17.4.4"><eq id="S3.SS1.p5.17.m17.4.4.6.cmml" xref="S3.SS1.p5.17.m17.4.4.6"></eq><ci id="S3.SS1.p5.17.m17.4.4.5.cmml" xref="S3.SS1.p5.17.m17.4.4.5">𝑋</ci><list id="S3.SS1.p5.17.m17.4.4.3.4.cmml" xref="S3.SS1.p5.17.m17.4.4.3.3"><apply id="S3.SS1.p5.17.m17.2.2.1.1.1.cmml" xref="S3.SS1.p5.17.m17.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.17.m17.2.2.1.1.1.1.cmml" xref="S3.SS1.p5.17.m17.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p5.17.m17.2.2.1.1.1.2.cmml" xref="S3.SS1.p5.17.m17.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS1.p5.17.m17.2.2.1.1.1.3.cmml" xref="S3.SS1.p5.17.m17.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p5.17.m17.3.3.2.2.2.cmml" xref="S3.SS1.p5.17.m17.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.17.m17.3.3.2.2.2.1.cmml" xref="S3.SS1.p5.17.m17.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p5.17.m17.3.3.2.2.2.2.cmml" xref="S3.SS1.p5.17.m17.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S3.SS1.p5.17.m17.3.3.2.2.2.3.cmml" xref="S3.SS1.p5.17.m17.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p5.17.m17.1.1.cmml" xref="S3.SS1.p5.17.m17.1.1">…</ci><apply id="S3.SS1.p5.17.m17.4.4.3.3.3.cmml" xref="S3.SS1.p5.17.m17.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p5.17.m17.4.4.3.3.3.1.cmml" xref="S3.SS1.p5.17.m17.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p5.17.m17.4.4.3.3.3.2.cmml" xref="S3.SS1.p5.17.m17.4.4.3.3.3.2">𝑥</ci><ci id="S3.SS1.p5.17.m17.4.4.3.3.3.3.cmml" xref="S3.SS1.p5.17.m17.4.4.3.3.3.3">𝑚</ci></apply></list></apply><apply id="S3.SS1.p5.17.m17.4.4c.cmml" xref="S3.SS1.p5.17.m17.4.4"><in id="S3.SS1.p5.17.m17.4.4.7.cmml" xref="S3.SS1.p5.17.m17.4.4.7"></in><share href="#S3.SS1.p5.17.m17.4.4.3.cmml" id="S3.SS1.p5.17.m17.4.4d.cmml" xref="S3.SS1.p5.17.m17.4.4"></share><ci id="S3.SS1.p5.17.m17.4.4.8.cmml" xref="S3.SS1.p5.17.m17.4.4.8">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.17.m17.4c">X=[x_{1},x_{2},...,x_{m}]\in{\mathbb{R}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.5" class="ltx_p">Following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, we also propose another attention unit called guided attention on attention (GAoA) unit (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b)). Unlike SAoA unit, GAoA uses AoA block and a point-wise feed-forward layer along with two input features <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mi id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><ci id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">X</annotation></semantics></math> and <math id="S3.SS1.p6.2.m2.4" class="ltx_Math" alttext="Y=[y_{1},y_{2},...,y_{n}]\in{\mathbb{R}}" display="inline"><semantics id="S3.SS1.p6.2.m2.4a"><mrow id="S3.SS1.p6.2.m2.4.4" xref="S3.SS1.p6.2.m2.4.4.cmml"><mi id="S3.SS1.p6.2.m2.4.4.5" xref="S3.SS1.p6.2.m2.4.4.5.cmml">Y</mi><mo id="S3.SS1.p6.2.m2.4.4.6" xref="S3.SS1.p6.2.m2.4.4.6.cmml">=</mo><mrow id="S3.SS1.p6.2.m2.4.4.3.3" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p6.2.m2.4.4.3.3.4" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml">[</mo><msub id="S3.SS1.p6.2.m2.2.2.1.1.1" xref="S3.SS1.p6.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p6.2.m2.2.2.1.1.1.2" xref="S3.SS1.p6.2.m2.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS1.p6.2.m2.2.2.1.1.1.3" xref="S3.SS1.p6.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p6.2.m2.4.4.3.3.5" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p6.2.m2.3.3.2.2.2" xref="S3.SS1.p6.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.p6.2.m2.3.3.2.2.2.2" xref="S3.SS1.p6.2.m2.3.3.2.2.2.2.cmml">y</mi><mn id="S3.SS1.p6.2.m2.3.3.2.2.2.3" xref="S3.SS1.p6.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p6.2.m2.4.4.3.3.6" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p6.2.m2.4.4.3.3.7" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p6.2.m2.4.4.3.3.3" xref="S3.SS1.p6.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS1.p6.2.m2.4.4.3.3.3.2" xref="S3.SS1.p6.2.m2.4.4.3.3.3.2.cmml">y</mi><mi id="S3.SS1.p6.2.m2.4.4.3.3.3.3" xref="S3.SS1.p6.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS1.p6.2.m2.4.4.3.3.8" xref="S3.SS1.p6.2.m2.4.4.3.4.cmml">]</mo></mrow><mo id="S3.SS1.p6.2.m2.4.4.7" xref="S3.SS1.p6.2.m2.4.4.7.cmml">∈</mo><mi id="S3.SS1.p6.2.m2.4.4.8" xref="S3.SS1.p6.2.m2.4.4.8.cmml">ℝ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.4b"><apply id="S3.SS1.p6.2.m2.4.4.cmml" xref="S3.SS1.p6.2.m2.4.4"><and id="S3.SS1.p6.2.m2.4.4a.cmml" xref="S3.SS1.p6.2.m2.4.4"></and><apply id="S3.SS1.p6.2.m2.4.4b.cmml" xref="S3.SS1.p6.2.m2.4.4"><eq id="S3.SS1.p6.2.m2.4.4.6.cmml" xref="S3.SS1.p6.2.m2.4.4.6"></eq><ci id="S3.SS1.p6.2.m2.4.4.5.cmml" xref="S3.SS1.p6.2.m2.4.4.5">𝑌</ci><list id="S3.SS1.p6.2.m2.4.4.3.4.cmml" xref="S3.SS1.p6.2.m2.4.4.3.3"><apply id="S3.SS1.p6.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p6.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p6.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p6.2.m2.2.2.1.1.1.2">𝑦</ci><cn type="integer" id="S3.SS1.p6.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p6.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p6.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p6.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.p6.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p6.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.p6.2.m2.3.3.2.2.2.2">𝑦</ci><cn type="integer" id="S3.SS1.p6.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS1.p6.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">…</ci><apply id="S3.SS1.p6.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p6.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS1.p6.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p6.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS1.p6.2.m2.4.4.3.3.3.2">𝑦</ci><ci id="S3.SS1.p6.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS1.p6.2.m2.4.4.3.3.3.3">𝑛</ci></apply></list></apply><apply id="S3.SS1.p6.2.m2.4.4c.cmml" xref="S3.SS1.p6.2.m2.4.4"><in id="S3.SS1.p6.2.m2.4.4.7.cmml" xref="S3.SS1.p6.2.m2.4.4.7"></in><share href="#S3.SS1.p6.2.m2.4.4.3.cmml" id="S3.SS1.p6.2.m2.4.4d.cmml" xref="S3.SS1.p6.2.m2.4.4"></share><ci id="S3.SS1.p6.2.m2.4.4.8.cmml" xref="S3.SS1.p6.2.m2.4.4.8">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.4c">Y=[y_{1},y_{2},...,y_{n}]\in{\mathbb{R}}</annotation></semantics></math> where <math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mi id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><ci id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">X</annotation></semantics></math> is guided by <math id="S3.SS1.p6.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS1.p6.4.m4.1a"><mi id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><ci id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.1c">Y</annotation></semantics></math>. In both attention unit, feed forward layer takes the output feature of AoA block and apply two fully connected layers along with ReLU and dropout function (i.e. <math id="S3.SS1.p6.5.m5.3" class="ltx_Math" alttext="\text{\tt FC}(4d)-\text{\tt ReLU}-\text{\tt dropout}(0.1)-\text{\tt FC}(d)" display="inline"><semantics id="S3.SS1.p6.5.m5.3a"><mrow id="S3.SS1.p6.5.m5.3.3" xref="S3.SS1.p6.5.m5.3.3.cmml"><mrow id="S3.SS1.p6.5.m5.3.3.1" xref="S3.SS1.p6.5.m5.3.3.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.1.3" xref="S3.SS1.p6.5.m5.3.3.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p6.5.m5.3.3.1.2" xref="S3.SS1.p6.5.m5.3.3.1.2.cmml">​</mo><mrow id="S3.SS1.p6.5.m5.3.3.1.1.1" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.1.1.1.2" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p6.5.m5.3.3.1.1.1.1" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.cmml"><mn id="S3.SS1.p6.5.m5.3.3.1.1.1.1.2" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p6.5.m5.3.3.1.1.1.1.1" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.5.m5.3.3.1.1.1.1.3" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.3.cmml">d</mi></mrow><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.1.1.1.3" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p6.5.m5.3.3.2" xref="S3.SS1.p6.5.m5.3.3.2.cmml">−</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.3" xref="S3.SS1.p6.5.m5.3.3.3a.cmml">ReLU</mtext><mo id="S3.SS1.p6.5.m5.3.3.2a" xref="S3.SS1.p6.5.m5.3.3.2.cmml">−</mo><mrow id="S3.SS1.p6.5.m5.3.3.4" xref="S3.SS1.p6.5.m5.3.3.4.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.4.2" xref="S3.SS1.p6.5.m5.3.3.4.2a.cmml">dropout</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p6.5.m5.3.3.4.1" xref="S3.SS1.p6.5.m5.3.3.4.1.cmml">​</mo><mrow id="S3.SS1.p6.5.m5.3.3.4.3.2" xref="S3.SS1.p6.5.m5.3.3.4.cmml"><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.4.3.2.1" xref="S3.SS1.p6.5.m5.3.3.4.cmml">(</mo><mn id="S3.SS1.p6.5.m5.1.1" xref="S3.SS1.p6.5.m5.1.1.cmml">0.1</mn><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.4.3.2.2" xref="S3.SS1.p6.5.m5.3.3.4.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p6.5.m5.3.3.2b" xref="S3.SS1.p6.5.m5.3.3.2.cmml">−</mo><mrow id="S3.SS1.p6.5.m5.3.3.5" xref="S3.SS1.p6.5.m5.3.3.5.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.5.2" xref="S3.SS1.p6.5.m5.3.3.5.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p6.5.m5.3.3.5.1" xref="S3.SS1.p6.5.m5.3.3.5.1.cmml">​</mo><mrow id="S3.SS1.p6.5.m5.3.3.5.3.2" xref="S3.SS1.p6.5.m5.3.3.5.cmml"><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.5.3.2.1" xref="S3.SS1.p6.5.m5.3.3.5.cmml">(</mo><mi id="S3.SS1.p6.5.m5.2.2" xref="S3.SS1.p6.5.m5.2.2.cmml">d</mi><mo stretchy="false" id="S3.SS1.p6.5.m5.3.3.5.3.2.2" xref="S3.SS1.p6.5.m5.3.3.5.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.5.m5.3b"><apply id="S3.SS1.p6.5.m5.3.3.cmml" xref="S3.SS1.p6.5.m5.3.3"><minus id="S3.SS1.p6.5.m5.3.3.2.cmml" xref="S3.SS1.p6.5.m5.3.3.2"></minus><apply id="S3.SS1.p6.5.m5.3.3.1.cmml" xref="S3.SS1.p6.5.m5.3.3.1"><times id="S3.SS1.p6.5.m5.3.3.1.2.cmml" xref="S3.SS1.p6.5.m5.3.3.1.2"></times><ci id="S3.SS1.p6.5.m5.3.3.1.3a.cmml" xref="S3.SS1.p6.5.m5.3.3.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.1.3.cmml" xref="S3.SS1.p6.5.m5.3.3.1.3">FC</mtext></ci><apply id="S3.SS1.p6.5.m5.3.3.1.1.1.1.cmml" xref="S3.SS1.p6.5.m5.3.3.1.1.1"><times id="S3.SS1.p6.5.m5.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.1"></times><cn type="integer" id="S3.SS1.p6.5.m5.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.2">4</cn><ci id="S3.SS1.p6.5.m5.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p6.5.m5.3.3.1.1.1.1.3">𝑑</ci></apply></apply><ci id="S3.SS1.p6.5.m5.3.3.3a.cmml" xref="S3.SS1.p6.5.m5.3.3.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.3.cmml" xref="S3.SS1.p6.5.m5.3.3.3">ReLU</mtext></ci><apply id="S3.SS1.p6.5.m5.3.3.4.cmml" xref="S3.SS1.p6.5.m5.3.3.4"><times id="S3.SS1.p6.5.m5.3.3.4.1.cmml" xref="S3.SS1.p6.5.m5.3.3.4.1"></times><ci id="S3.SS1.p6.5.m5.3.3.4.2a.cmml" xref="S3.SS1.p6.5.m5.3.3.4.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.4.2.cmml" xref="S3.SS1.p6.5.m5.3.3.4.2">dropout</mtext></ci><cn type="float" id="S3.SS1.p6.5.m5.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1">0.1</cn></apply><apply id="S3.SS1.p6.5.m5.3.3.5.cmml" xref="S3.SS1.p6.5.m5.3.3.5"><times id="S3.SS1.p6.5.m5.3.3.5.1.cmml" xref="S3.SS1.p6.5.m5.3.3.5.1"></times><ci id="S3.SS1.p6.5.m5.3.3.5.2a.cmml" xref="S3.SS1.p6.5.m5.3.3.5.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p6.5.m5.3.3.5.2.cmml" xref="S3.SS1.p6.5.m5.3.3.5.2">FC</mtext></ci><ci id="S3.SS1.p6.5.m5.2.2.cmml" xref="S3.SS1.p6.5.m5.2.2">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.5.m5.3c">\text{\tt FC}(4d)-\text{\tt ReLU}-\text{\tt dropout}(0.1)-\text{\tt FC}(d)</annotation></semantics></math>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MCAoA layers</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.6" class="ltx_p">Modular Co-Attention on Attention (MCAoA) layer (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 SAoA and GAoA Units ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) consists of two attention units discussed in Section <a href="#S3.SS1" title="3.1 SAoA and GAoA Units ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Here <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">X</annotation></semantics></math> and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">Y</annotation></semantics></math> represents image and question feature respectively. MCAoA layer is designed to handle multimodal interactions. We use cascaded MCAoA layers, <span id="S3.SS2.p1.6.1" class="ltx_text ltx_font_italic">i.e.</span>, output from previous MCAoA is fed as input to the next MCAoA layer. For both input features, MCAoA layer first uses two separate SAoA units to caption intra-modal interactions for <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">X</annotation></semantics></math> and <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">Y</annotation></semantics></math> separately and then uses GAoA unit to capture inter-modal relationships where <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">Y</annotation></semantics></math> guides <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">X</annotation></semantics></math> feature.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>MCAoAN</h3>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2011.02164/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.11.5.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of overall architecture of proposed Modular Co-Attention on Attention Network (MCAoAN).<span id="S3.F4.8.4.4" class="ltx_text ltx_font_medium"> The network takes image and question feature as inputs. Image features are the intermediate features extracted from a Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> model and each work from the question is transformed to a vector using 300-D GloVe word embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> followed by a LSTM unit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Both features are fed to an Encoder-Decoder module consists of cascaded MCAoA layers and generate <math id="S3.F4.5.1.1.m1.1" class="ltx_Math" alttext="X_{L}" display="inline"><semantics id="S3.F4.5.1.1.m1.1b"><msub id="S3.F4.5.1.1.m1.1.1" xref="S3.F4.5.1.1.m1.1.1.cmml"><mi id="S3.F4.5.1.1.m1.1.1.2" xref="S3.F4.5.1.1.m1.1.1.2.cmml">X</mi><mi id="S3.F4.5.1.1.m1.1.1.3" xref="S3.F4.5.1.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.5.1.1.m1.1c"><apply id="S3.F4.5.1.1.m1.1.1.cmml" xref="S3.F4.5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F4.5.1.1.m1.1.1.1.cmml" xref="S3.F4.5.1.1.m1.1.1">subscript</csymbol><ci id="S3.F4.5.1.1.m1.1.1.2.cmml" xref="S3.F4.5.1.1.m1.1.1.2">𝑋</ci><ci id="S3.F4.5.1.1.m1.1.1.3.cmml" xref="S3.F4.5.1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.1.1.m1.1d">X_{L}</annotation></semantics></math> and <math id="S3.F4.6.2.2.m2.1" class="ltx_Math" alttext="Y_{L}" display="inline"><semantics id="S3.F4.6.2.2.m2.1b"><msub id="S3.F4.6.2.2.m2.1.1" xref="S3.F4.6.2.2.m2.1.1.cmml"><mi id="S3.F4.6.2.2.m2.1.1.2" xref="S3.F4.6.2.2.m2.1.1.2.cmml">Y</mi><mi id="S3.F4.6.2.2.m2.1.1.3" xref="S3.F4.6.2.2.m2.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.6.2.2.m2.1c"><apply id="S3.F4.6.2.2.m2.1.1.cmml" xref="S3.F4.6.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F4.6.2.2.m2.1.1.1.cmml" xref="S3.F4.6.2.2.m2.1.1">subscript</csymbol><ci id="S3.F4.6.2.2.m2.1.1.2.cmml" xref="S3.F4.6.2.2.m2.1.1.2">𝑌</ci><ci id="S3.F4.6.2.2.m2.1.1.3.cmml" xref="S3.F4.6.2.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.2.2.m2.1d">Y_{L}</annotation></semantics></math> feature representations. <math id="S3.F4.7.3.3.m3.1" class="ltx_Math" alttext="X_{L}" display="inline"><semantics id="S3.F4.7.3.3.m3.1b"><msub id="S3.F4.7.3.3.m3.1.1" xref="S3.F4.7.3.3.m3.1.1.cmml"><mi id="S3.F4.7.3.3.m3.1.1.2" xref="S3.F4.7.3.3.m3.1.1.2.cmml">X</mi><mi id="S3.F4.7.3.3.m3.1.1.3" xref="S3.F4.7.3.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.7.3.3.m3.1c"><apply id="S3.F4.7.3.3.m3.1.1.cmml" xref="S3.F4.7.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F4.7.3.3.m3.1.1.1.cmml" xref="S3.F4.7.3.3.m3.1.1">subscript</csymbol><ci id="S3.F4.7.3.3.m3.1.1.2.cmml" xref="S3.F4.7.3.3.m3.1.1.2">𝑋</ci><ci id="S3.F4.7.3.3.m3.1.1.3.cmml" xref="S3.F4.7.3.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.7.3.3.m3.1d">X_{L}</annotation></semantics></math> and <math id="S3.F4.8.4.4.m4.1" class="ltx_Math" alttext="Y_{L}" display="inline"><semantics id="S3.F4.8.4.4.m4.1b"><msub id="S3.F4.8.4.4.m4.1.1" xref="S3.F4.8.4.4.m4.1.1.cmml"><mi id="S3.F4.8.4.4.m4.1.1.2" xref="S3.F4.8.4.4.m4.1.1.2.cmml">Y</mi><mi id="S3.F4.8.4.4.m4.1.1.3" xref="S3.F4.8.4.4.m4.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.8.4.4.m4.1c"><apply id="S3.F4.8.4.4.m4.1.1.cmml" xref="S3.F4.8.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F4.8.4.4.m4.1.1.1.cmml" xref="S3.F4.8.4.4.m4.1.1">subscript</csymbol><ci id="S3.F4.8.4.4.m4.1.1.2.cmml" xref="S3.F4.8.4.4.m4.1.1.2">𝑌</ci><ci id="S3.F4.8.4.4.m4.1.1.3.cmml" xref="S3.F4.8.4.4.m4.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.8.4.4.m4.1d">Y_{L}</annotation></semantics></math> denotes image and question feature respectively and combined together to generate desire answer by a multi-modal fusion module. </span></span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">In this section, we discuss our proposed modular co-attention on attention network (MCAoAN) (see Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 MCAoAN ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) which is motivated by  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. First we have to pre-process the inputs (<span id="S3.SS3.p1.2.1" class="ltx_text ltx_font_italic">i.e.</span>, image and query question) into appropriate feature representations. We use Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> with ResNet-101 as its backbone which is pretrained on Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to process input images. The intermediate feature of the detected object from Faster R-CNN is considered as visual feature representation. Following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, we also consider a threshold value to obtain dynamic number of detected objects, <span id="S3.SS3.p1.2.2" class="ltx_text ltx_font_italic">e.g.</span>, <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑥</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">x_{i}</annotation></semantics></math> is corresponds to i-th object feature. The final image feature is represented by a feature matrix <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">X</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">The input query question is first tokenized and later trimmed to maximum 14 words. The pre-trained GloVe embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is used to transformed each word into a vector representation. This results a final representation of size <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="n\times 300" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑛</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">n\times 300</annotation></semantics></math> for a sequence of words where <math id="S3.SS3.p2.2.m2.2" class="ltx_Math" alttext="n\in[1,14]" display="inline"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.3" xref="S3.SS3.p2.2.m2.2.3.cmml"><mi id="S3.SS3.p2.2.m2.2.3.2" xref="S3.SS3.p2.2.m2.2.3.2.cmml">n</mi><mo id="S3.SS3.p2.2.m2.2.3.1" xref="S3.SS3.p2.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS3.p2.2.m2.2.3.3.2" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.2.3.3.2.1" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">1</mn><mo id="S3.SS3.p2.2.m2.2.3.3.2.2" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">14</mn><mo stretchy="false" id="S3.SS3.p2.2.m2.2.3.3.2.3" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><apply id="S3.SS3.p2.2.m2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.3"><in id="S3.SS3.p2.2.m2.2.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.1"></in><ci id="S3.SS3.p2.2.m2.2.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.2">𝑛</ci><interval closure="closed" id="S3.SS3.p2.2.m2.2.3.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.3.2"><cn type="integer" id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">1</cn><cn type="integer" id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2">14</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">n\in[1,14]</annotation></semantics></math> denotes the number of word in the sequence. The word embedding is fed to a one layer LSTM network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and generate final query feature matrix <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">Y</annotation></semantics></math> by capturing the output features of all words.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.5" class="ltx_p">Both input features are passed to the encoder-decoder module which contain cascaded MCAoA layers. Similar to  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, encoder learns attention question features <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="Y_{L}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">Y</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑌</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">Y_{L}</annotation></semantics></math> by stacking <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">L</annotation></semantics></math> number of SAoA units. On the other hand, decoder learns attended image features <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="X_{L}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">X</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑋</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">X_{L}</annotation></semantics></math> by stacking <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">L</annotation></semantics></math> number of GAoA units by using query features <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="Y_{L}" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><msub id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">Y</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">𝑌</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">Y_{L}</annotation></semantics></math>.

<br class="ltx_break"></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-modal Fusion. </h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.7" class="ltx_p">The outputs (i.e image features <math id="S3.SS4.p1.1.m1.4" class="ltx_Math" alttext="X_{L}=[x_{1},x_{2},...,x_{m}]\in{\mathbb{R}}^{m\times d}" display="inline"><semantics id="S3.SS4.p1.1.m1.4a"><mrow id="S3.SS4.p1.1.m1.4.4" xref="S3.SS4.p1.1.m1.4.4.cmml"><msub id="S3.SS4.p1.1.m1.4.4.5" xref="S3.SS4.p1.1.m1.4.4.5.cmml"><mi id="S3.SS4.p1.1.m1.4.4.5.2" xref="S3.SS4.p1.1.m1.4.4.5.2.cmml">X</mi><mi id="S3.SS4.p1.1.m1.4.4.5.3" xref="S3.SS4.p1.1.m1.4.4.5.3.cmml">L</mi></msub><mo id="S3.SS4.p1.1.m1.4.4.6" xref="S3.SS4.p1.1.m1.4.4.6.cmml">=</mo><mrow id="S3.SS4.p1.1.m1.4.4.3.3" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS4.p1.1.m1.4.4.3.3.4" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml">[</mo><msub id="S3.SS4.p1.1.m1.2.2.1.1.1" xref="S3.SS4.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.2.2.1.1.1.2" xref="S3.SS4.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.SS4.p1.1.m1.2.2.1.1.1.3" xref="S3.SS4.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.p1.1.m1.4.4.3.3.5" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p1.1.m1.3.3.2.2.2" xref="S3.SS4.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS4.p1.1.m1.3.3.2.2.2.2" xref="S3.SS4.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.SS4.p1.1.m1.3.3.2.2.2.3" xref="S3.SS4.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.p1.1.m1.4.4.3.3.6" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS4.p1.1.m1.4.4.3.3.7" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p1.1.m1.4.4.3.3.3" xref="S3.SS4.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS4.p1.1.m1.4.4.3.3.3.2" xref="S3.SS4.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S3.SS4.p1.1.m1.4.4.3.3.3.3" xref="S3.SS4.p1.1.m1.4.4.3.3.3.3.cmml">m</mi></msub><mo stretchy="false" id="S3.SS4.p1.1.m1.4.4.3.3.8" xref="S3.SS4.p1.1.m1.4.4.3.4.cmml">]</mo></mrow><mo id="S3.SS4.p1.1.m1.4.4.7" xref="S3.SS4.p1.1.m1.4.4.7.cmml">∈</mo><msup id="S3.SS4.p1.1.m1.4.4.8" xref="S3.SS4.p1.1.m1.4.4.8.cmml"><mi id="S3.SS4.p1.1.m1.4.4.8.2" xref="S3.SS4.p1.1.m1.4.4.8.2.cmml">ℝ</mi><mrow id="S3.SS4.p1.1.m1.4.4.8.3" xref="S3.SS4.p1.1.m1.4.4.8.3.cmml"><mi id="S3.SS4.p1.1.m1.4.4.8.3.2" xref="S3.SS4.p1.1.m1.4.4.8.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.1.m1.4.4.8.3.1" xref="S3.SS4.p1.1.m1.4.4.8.3.1.cmml">×</mo><mi id="S3.SS4.p1.1.m1.4.4.8.3.3" xref="S3.SS4.p1.1.m1.4.4.8.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.4b"><apply id="S3.SS4.p1.1.m1.4.4.cmml" xref="S3.SS4.p1.1.m1.4.4"><and id="S3.SS4.p1.1.m1.4.4a.cmml" xref="S3.SS4.p1.1.m1.4.4"></and><apply id="S3.SS4.p1.1.m1.4.4b.cmml" xref="S3.SS4.p1.1.m1.4.4"><eq id="S3.SS4.p1.1.m1.4.4.6.cmml" xref="S3.SS4.p1.1.m1.4.4.6"></eq><apply id="S3.SS4.p1.1.m1.4.4.5.cmml" xref="S3.SS4.p1.1.m1.4.4.5"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.4.4.5.1.cmml" xref="S3.SS4.p1.1.m1.4.4.5">subscript</csymbol><ci id="S3.SS4.p1.1.m1.4.4.5.2.cmml" xref="S3.SS4.p1.1.m1.4.4.5.2">𝑋</ci><ci id="S3.SS4.p1.1.m1.4.4.5.3.cmml" xref="S3.SS4.p1.1.m1.4.4.5.3">𝐿</ci></apply><list id="S3.SS4.p1.1.m1.4.4.3.4.cmml" xref="S3.SS4.p1.1.m1.4.4.3.3"><apply id="S3.SS4.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS4.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS4.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS4.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS4.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS4.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S3.SS4.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">…</ci><apply id="S3.SS4.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS4.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS4.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS4.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS4.p1.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S3.SS4.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.4.4.3.3.3.3">𝑚</ci></apply></list></apply><apply id="S3.SS4.p1.1.m1.4.4c.cmml" xref="S3.SS4.p1.1.m1.4.4"><in id="S3.SS4.p1.1.m1.4.4.7.cmml" xref="S3.SS4.p1.1.m1.4.4.7"></in><share href="#S3.SS4.p1.1.m1.4.4.3.cmml" id="S3.SS4.p1.1.m1.4.4d.cmml" xref="S3.SS4.p1.1.m1.4.4"></share><apply id="S3.SS4.p1.1.m1.4.4.8.cmml" xref="S3.SS4.p1.1.m1.4.4.8"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.4.4.8.1.cmml" xref="S3.SS4.p1.1.m1.4.4.8">superscript</csymbol><ci id="S3.SS4.p1.1.m1.4.4.8.2.cmml" xref="S3.SS4.p1.1.m1.4.4.8.2">ℝ</ci><apply id="S3.SS4.p1.1.m1.4.4.8.3.cmml" xref="S3.SS4.p1.1.m1.4.4.8.3"><times id="S3.SS4.p1.1.m1.4.4.8.3.1.cmml" xref="S3.SS4.p1.1.m1.4.4.8.3.1"></times><ci id="S3.SS4.p1.1.m1.4.4.8.3.2.cmml" xref="S3.SS4.p1.1.m1.4.4.8.3.2">𝑚</ci><ci id="S3.SS4.p1.1.m1.4.4.8.3.3.cmml" xref="S3.SS4.p1.1.m1.4.4.8.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.4c">X_{L}=[x_{1},x_{2},...,x_{m}]\in{\mathbb{R}}^{m\times d}</annotation></semantics></math> and question features <math id="S3.SS4.p1.2.m2.1" class="ltx_math_unparsed" alttext="Y_{L}=[y_{1},y_{2},....,y_{n}]\in{\mathbb{R}}^{n\times d}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1b"><msub id="S3.SS4.p1.2.m2.1.2"><mi id="S3.SS4.p1.2.m2.1.2.2">Y</mi><mi id="S3.SS4.p1.2.m2.1.2.3">L</mi></msub><mo id="S3.SS4.p1.2.m2.1.3">=</mo><mrow id="S3.SS4.p1.2.m2.1.4"><mo stretchy="false" id="S3.SS4.p1.2.m2.1.4.1">[</mo><msub id="S3.SS4.p1.2.m2.1.4.2"><mi id="S3.SS4.p1.2.m2.1.4.2.2">y</mi><mn id="S3.SS4.p1.2.m2.1.4.2.3">1</mn></msub><mo id="S3.SS4.p1.2.m2.1.4.3">,</mo><msub id="S3.SS4.p1.2.m2.1.4.4"><mi id="S3.SS4.p1.2.m2.1.4.4.2">y</mi><mn id="S3.SS4.p1.2.m2.1.4.4.3">2</mn></msub><mo id="S3.SS4.p1.2.m2.1.4.5">,</mo><mi mathvariant="normal" id="S3.SS4.p1.2.m2.1.1">…</mi><mo lspace="0em" rspace="0.167em" id="S3.SS4.p1.2.m2.1.4.6">.</mo><mo id="S3.SS4.p1.2.m2.1.4.7">,</mo><msub id="S3.SS4.p1.2.m2.1.4.8"><mi id="S3.SS4.p1.2.m2.1.4.8.2">y</mi><mi id="S3.SS4.p1.2.m2.1.4.8.3">n</mi></msub><mo stretchy="false" id="S3.SS4.p1.2.m2.1.4.9">]</mo></mrow><mo id="S3.SS4.p1.2.m2.1.5">∈</mo><msup id="S3.SS4.p1.2.m2.1.6"><mi id="S3.SS4.p1.2.m2.1.6.2">ℝ</mi><mrow id="S3.SS4.p1.2.m2.1.6.3"><mi id="S3.SS4.p1.2.m2.1.6.3.2">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.2.m2.1.6.3.1">×</mo><mi id="S3.SS4.p1.2.m2.1.6.3.3">d</mi></mrow></msup></mrow><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">Y_{L}=[y_{1},y_{2},....,y_{n}]\in{\mathbb{R}}^{n\times d}</annotation></semantics></math>) from encoder-decoder contains attended information about image and query regions. Therefore, we need to apply an appropriate fusion mechanism to combine both feature representation. In this paper, we propose two kind of multi-modal fusion networks (see Figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) to aggregate features of both modality: (1) Multi-modal Attention Fusion and (2) Multi-modal Mutan Fusion. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, we first use two layers of MLP (i.e. <span id="S3.SS4.p1.7.1" class="ltx_text ltx_markedasmath">FC(d)- ReLU - Dropout(0.1) - FC(1)</span>) for both <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="X_{L}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">X</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝑋</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">X_{L}</annotation></semantics></math> and <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="Y_{L}" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><msub id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml">Y</mi><mi id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2">𝑌</ci><ci id="S3.SS4.p1.5.m5.1.1.3.cmml" xref="S3.SS4.p1.5.m5.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">Y_{L}</annotation></semantics></math>; and generate attended features <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="X^{\prime}" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><msup id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">X</mi><mo id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">𝑋</ci><ci id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">X^{\prime}</annotation></semantics></math> and <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="Y^{\prime}" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><msup id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml"><mi id="S3.SS4.p1.7.m7.1.1.2" xref="S3.SS4.p1.7.m7.1.1.2.cmml">Y</mi><mo id="S3.SS4.p1.7.m7.1.1.3" xref="S3.SS4.p1.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><apply id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m7.1.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.p1.7.m7.1.1.2.cmml" xref="S3.SS4.p1.7.m7.1.1.2">𝑌</ci><ci id="S3.SS4.p1.7.m7.1.1.3.cmml" xref="S3.SS4.p1.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">Y^{\prime}</annotation></semantics></math> as follows:</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\operatorname{X^{\prime}}=\sum_{i=1}^{m}\text{\tt Softmax}(\text{\tt MLP}(\operatorname{X_{L}}))\operatorname{x}_{i}," display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><msup id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml"><mi mathvariant="normal" id="S3.E4.m1.2.2.1.1.3.2" xref="S3.E4.m1.2.2.1.1.3.2.cmml">X</mi><mo id="S3.E4.m1.2.2.1.1.3.3" xref="S3.E4.m1.2.2.1.1.3.3.cmml">′</mo></msup><mo rspace="0.111em" id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><munderover id="S3.E4.m1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E4.m1.2.2.1.1.1.2.2.2" xref="S3.E4.m1.2.2.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E4.m1.2.2.1.1.1.2.2.3" xref="S3.E4.m1.2.2.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.2.2.3.2" xref="S3.E4.m1.2.2.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E4.m1.2.2.1.1.1.2.2.3.1" xref="S3.E4.m1.2.2.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.2.2.1.1.1.2.2.3.3" xref="S3.E4.m1.2.2.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.2.2.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">X</mi><mi mathvariant="normal" id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.2a" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">​</mo><msub id="S3.E4.m1.2.2.1.1.1.1.4" xref="S3.E4.m1.2.2.1.1.1.1.4.cmml"><mi mathvariant="normal" id="S3.E4.m1.2.2.1.1.1.1.4.2" xref="S3.E4.m1.2.2.1.1.1.1.4.2.cmml">x</mi><mi id="S3.E4.m1.2.2.1.1.1.1.4.3" xref="S3.E4.m1.2.2.1.1.1.1.4.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"></eq><apply id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.E4.m1.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.2">X</ci><ci id="S3.E4.m1.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.3.3">′</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"><apply id="S3.E4.m1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.2">subscript</csymbol><sum id="S3.E4.m1.2.2.1.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2.2.2"></sum><apply id="S3.E4.m1.2.2.1.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.2.2.3"><eq id="S3.E4.m1.2.2.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.2.2.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.2.2.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.2.3">𝑚</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2"></times><ci id="S3.E4.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">Softmax</mtext></ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2">MLP</mtext></ci><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">X</ci><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">L</ci></apply></apply><apply id="S3.E4.m1.2.2.1.1.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.4.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.4">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.4.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.4.2">x</ci><ci id="S3.E4.m1.2.2.1.1.1.1.4.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.4.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\operatorname{X^{\prime}}=\sum_{i=1}^{m}\text{\tt Softmax}(\text{\tt MLP}(\operatorname{X_{L}}))\operatorname{x}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.1" class="ltx_p">and</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\operatorname{Y^{\prime}}=\sum_{i=1}^{n}\text{\tt Softmax}(\text{\tt MLP}(\operatorname{Y_{L}}))\operatorname{y}_{i}," display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><msup id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml"><mi mathvariant="normal" id="S3.E5.m1.2.2.1.1.3.2" xref="S3.E5.m1.2.2.1.1.3.2.cmml">Y</mi><mo id="S3.E5.m1.2.2.1.1.3.3" xref="S3.E5.m1.2.2.1.1.3.3.cmml">′</mo></msup><mo rspace="0.111em" id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><munderover id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E5.m1.2.2.1.1.1.2.2.2" xref="S3.E5.m1.2.2.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E5.m1.2.2.1.1.1.2.2.3" xref="S3.E5.m1.2.2.1.1.1.2.2.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.2.2.3.2" xref="S3.E5.m1.2.2.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E5.m1.2.2.1.1.1.2.2.3.1" xref="S3.E5.m1.2.2.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E5.m1.2.2.1.1.1.2.2.3.3" xref="S3.E5.m1.2.2.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E5.m1.2.2.1.1.1.2.3" xref="S3.E5.m1.2.2.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.2.2.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S3.E5.m1.1.1.cmml">(</mo><msub id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">Y</mi><mi mathvariant="normal" id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.1.2a" xref="S3.E5.m1.2.2.1.1.1.1.2.cmml">​</mo><msub id="S3.E5.m1.2.2.1.1.1.1.4" xref="S3.E5.m1.2.2.1.1.1.1.4.cmml"><mi mathvariant="normal" id="S3.E5.m1.2.2.1.1.1.1.4.2" xref="S3.E5.m1.2.2.1.1.1.1.4.2.cmml">y</mi><mi id="S3.E5.m1.2.2.1.1.1.1.4.3" xref="S3.E5.m1.2.2.1.1.1.1.4.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><apply id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.E5.m1.2.2.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2">Y</ci><ci id="S3.E5.m1.2.2.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.3.3">′</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><apply id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.2">superscript</csymbol><apply id="S3.E5.m1.2.2.1.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.2.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.2">subscript</csymbol><sum id="S3.E5.m1.2.2.1.1.1.2.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2.2.2"></sum><apply id="S3.E5.m1.2.2.1.1.1.2.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.2.2.3"><eq id="S3.E5.m1.2.2.1.1.1.2.2.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.2.2.3.1"></eq><ci id="S3.E5.m1.2.2.1.1.1.2.2.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E5.m1.2.2.1.1.1.2.2.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E5.m1.2.2.1.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2"></times><ci id="S3.E5.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.2.2.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3">Softmax</mtext></ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2a.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2">MLP</mtext></ci><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">Y</ci><ci id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">L</ci></apply></apply><apply id="S3.E5.m1.2.2.1.1.1.1.4.cmml" xref="S3.E5.m1.2.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.4.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.4">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.4.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.4.2">y</ci><ci id="S3.E5.m1.2.2.1.1.1.1.4.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.4.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\operatorname{Y^{\prime}}=\sum_{i=1}^{n}\text{\tt Softmax}(\text{\tt MLP}(\operatorname{Y_{L}}))\operatorname{y}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2011.02164/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="382" height="67" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of proposed multimodal fusion network.<span id="S3.F5.4.2.1" class="ltx_text ltx_font_medium"> (a) Multi-modal attention fusion where we apply simple concatenation to combine initial attended features from both image and language modalities and apply series of fully connected layer to generate weighted features. The final weighted features represents how much importance should we give on each modality. (b) Multi-modal mutan fusion, another version of multi-modal fusion where we incorporate mutan fusion instead of concatenation keeping rest of the network similar to multi-modal attention fusion.</span></span></figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.11" class="ltx_p">Now we have rich attended features from both modality and at the same time each modality should use to generate attention with one another for better prediction. Therefore, we have to decide, how much information should use from each modality. Following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, in multi-modal attention fusion, we apply concatenation on <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\operatorname{X^{\prime}}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msup id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">X</mi><mo id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">X</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\operatorname{X^{\prime}}</annotation></semantics></math> and <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="\operatorname{Y^{\prime}}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><msup id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">Y</mi><mo id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">Y</ci><ci id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\operatorname{Y^{\prime}}</annotation></semantics></math> followed by a series of fully-connected layers (<span id="S3.SS4.p3.11.1" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S3.SS4.p3.3.m3.5" class="ltx_Math" alttext="\text{\tt FC}(1024)-\text{\tt Dropout}(0.2)-\text{\tt FC}(512)-\text{\tt Dropout}(0.2)-\text{\tt FC}(2)-\text{\tt Softmax}" display="inline"><semantics id="S3.SS4.p3.3.m3.5a"><mrow id="S3.SS4.p3.3.m3.5.6" xref="S3.SS4.p3.3.m3.5.6.cmml"><mrow id="S3.SS4.p3.3.m3.5.6.2" xref="S3.SS4.p3.3.m3.5.6.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.2.2" xref="S3.SS4.p3.3.m3.5.6.2.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.5.6.2.1" xref="S3.SS4.p3.3.m3.5.6.2.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.5.6.2.3.2" xref="S3.SS4.p3.3.m3.5.6.2.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.2.3.2.1" xref="S3.SS4.p3.3.m3.5.6.2.cmml">(</mo><mn id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">1024</mn><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.2.3.2.2" xref="S3.SS4.p3.3.m3.5.6.2.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.3.m3.5.6.1" xref="S3.SS4.p3.3.m3.5.6.1.cmml">−</mo><mrow id="S3.SS4.p3.3.m3.5.6.3" xref="S3.SS4.p3.3.m3.5.6.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.3.2" xref="S3.SS4.p3.3.m3.5.6.3.2a.cmml">Dropout</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.5.6.3.1" xref="S3.SS4.p3.3.m3.5.6.3.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.5.6.3.3.2" xref="S3.SS4.p3.3.m3.5.6.3.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.3.3.2.1" xref="S3.SS4.p3.3.m3.5.6.3.cmml">(</mo><mn id="S3.SS4.p3.3.m3.2.2" xref="S3.SS4.p3.3.m3.2.2.cmml">0.2</mn><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.3.3.2.2" xref="S3.SS4.p3.3.m3.5.6.3.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.3.m3.5.6.1a" xref="S3.SS4.p3.3.m3.5.6.1.cmml">−</mo><mrow id="S3.SS4.p3.3.m3.5.6.4" xref="S3.SS4.p3.3.m3.5.6.4.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.4.2" xref="S3.SS4.p3.3.m3.5.6.4.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.5.6.4.1" xref="S3.SS4.p3.3.m3.5.6.4.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.5.6.4.3.2" xref="S3.SS4.p3.3.m3.5.6.4.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.4.3.2.1" xref="S3.SS4.p3.3.m3.5.6.4.cmml">(</mo><mn id="S3.SS4.p3.3.m3.3.3" xref="S3.SS4.p3.3.m3.3.3.cmml">512</mn><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.4.3.2.2" xref="S3.SS4.p3.3.m3.5.6.4.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.3.m3.5.6.1b" xref="S3.SS4.p3.3.m3.5.6.1.cmml">−</mo><mrow id="S3.SS4.p3.3.m3.5.6.5" xref="S3.SS4.p3.3.m3.5.6.5.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.5.2" xref="S3.SS4.p3.3.m3.5.6.5.2a.cmml">Dropout</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.5.6.5.1" xref="S3.SS4.p3.3.m3.5.6.5.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.5.6.5.3.2" xref="S3.SS4.p3.3.m3.5.6.5.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.5.3.2.1" xref="S3.SS4.p3.3.m3.5.6.5.cmml">(</mo><mn id="S3.SS4.p3.3.m3.4.4" xref="S3.SS4.p3.3.m3.4.4.cmml">0.2</mn><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.5.3.2.2" xref="S3.SS4.p3.3.m3.5.6.5.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.3.m3.5.6.1c" xref="S3.SS4.p3.3.m3.5.6.1.cmml">−</mo><mrow id="S3.SS4.p3.3.m3.5.6.6" xref="S3.SS4.p3.3.m3.5.6.6.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.6.2" xref="S3.SS4.p3.3.m3.5.6.6.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.5.6.6.1" xref="S3.SS4.p3.3.m3.5.6.6.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.5.6.6.3.2" xref="S3.SS4.p3.3.m3.5.6.6.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.6.3.2.1" xref="S3.SS4.p3.3.m3.5.6.6.cmml">(</mo><mn id="S3.SS4.p3.3.m3.5.5" xref="S3.SS4.p3.3.m3.5.5.cmml">2</mn><mo stretchy="false" id="S3.SS4.p3.3.m3.5.6.6.3.2.2" xref="S3.SS4.p3.3.m3.5.6.6.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p3.3.m3.5.6.1d" xref="S3.SS4.p3.3.m3.5.6.1.cmml">−</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.7" xref="S3.SS4.p3.3.m3.5.6.7a.cmml">Softmax</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.5b"><apply id="S3.SS4.p3.3.m3.5.6.cmml" xref="S3.SS4.p3.3.m3.5.6"><minus id="S3.SS4.p3.3.m3.5.6.1.cmml" xref="S3.SS4.p3.3.m3.5.6.1"></minus><apply id="S3.SS4.p3.3.m3.5.6.2.cmml" xref="S3.SS4.p3.3.m3.5.6.2"><times id="S3.SS4.p3.3.m3.5.6.2.1.cmml" xref="S3.SS4.p3.3.m3.5.6.2.1"></times><ci id="S3.SS4.p3.3.m3.5.6.2.2a.cmml" xref="S3.SS4.p3.3.m3.5.6.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.2.2.cmml" xref="S3.SS4.p3.3.m3.5.6.2.2">FC</mtext></ci><cn type="integer" id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">1024</cn></apply><apply id="S3.SS4.p3.3.m3.5.6.3.cmml" xref="S3.SS4.p3.3.m3.5.6.3"><times id="S3.SS4.p3.3.m3.5.6.3.1.cmml" xref="S3.SS4.p3.3.m3.5.6.3.1"></times><ci id="S3.SS4.p3.3.m3.5.6.3.2a.cmml" xref="S3.SS4.p3.3.m3.5.6.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.3.2.cmml" xref="S3.SS4.p3.3.m3.5.6.3.2">Dropout</mtext></ci><cn type="float" id="S3.SS4.p3.3.m3.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2">0.2</cn></apply><apply id="S3.SS4.p3.3.m3.5.6.4.cmml" xref="S3.SS4.p3.3.m3.5.6.4"><times id="S3.SS4.p3.3.m3.5.6.4.1.cmml" xref="S3.SS4.p3.3.m3.5.6.4.1"></times><ci id="S3.SS4.p3.3.m3.5.6.4.2a.cmml" xref="S3.SS4.p3.3.m3.5.6.4.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.4.2.cmml" xref="S3.SS4.p3.3.m3.5.6.4.2">FC</mtext></ci><cn type="integer" id="S3.SS4.p3.3.m3.3.3.cmml" xref="S3.SS4.p3.3.m3.3.3">512</cn></apply><apply id="S3.SS4.p3.3.m3.5.6.5.cmml" xref="S3.SS4.p3.3.m3.5.6.5"><times id="S3.SS4.p3.3.m3.5.6.5.1.cmml" xref="S3.SS4.p3.3.m3.5.6.5.1"></times><ci id="S3.SS4.p3.3.m3.5.6.5.2a.cmml" xref="S3.SS4.p3.3.m3.5.6.5.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.5.2.cmml" xref="S3.SS4.p3.3.m3.5.6.5.2">Dropout</mtext></ci><cn type="float" id="S3.SS4.p3.3.m3.4.4.cmml" xref="S3.SS4.p3.3.m3.4.4">0.2</cn></apply><apply id="S3.SS4.p3.3.m3.5.6.6.cmml" xref="S3.SS4.p3.3.m3.5.6.6"><times id="S3.SS4.p3.3.m3.5.6.6.1.cmml" xref="S3.SS4.p3.3.m3.5.6.6.1"></times><ci id="S3.SS4.p3.3.m3.5.6.6.2a.cmml" xref="S3.SS4.p3.3.m3.5.6.6.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.6.2.cmml" xref="S3.SS4.p3.3.m3.5.6.6.2">FC</mtext></ci><cn type="integer" id="S3.SS4.p3.3.m3.5.5.cmml" xref="S3.SS4.p3.3.m3.5.5">2</cn></apply><ci id="S3.SS4.p3.3.m3.5.6.7a.cmml" xref="S3.SS4.p3.3.m3.5.6.7"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p3.3.m3.5.6.7.cmml" xref="S3.SS4.p3.3.m3.5.6.7">Softmax</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.5c">\text{\tt FC}(1024)-\text{\tt Dropout}(0.2)-\text{\tt FC}(512)-\text{\tt Dropout}(0.2)-\text{\tt FC}(2)-\text{\tt Softmax}</annotation></semantics></math>) (see Figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a)). The output of these operations is a 2-dimensional feature vector that corresponds to the importance of two modality for answer prediction. After that, we generate weighted average of attended feature (i.e. <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">A</annotation></semantics></math> and <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">B</annotation></semantics></math>) for each modality similar to eq. <a href="#S3.E4" title="In 3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and  <a href="#S3.E5" title="In 3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mi id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><ci id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">A</annotation></semantics></math> and <math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><mi id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><ci id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">B</annotation></semantics></math> is combined with attended visual and textual features <math id="S3.SS4.p3.8.m8.1" class="ltx_Math" alttext="X^{\prime}" display="inline"><semantics id="S3.SS4.p3.8.m8.1a"><msup id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml"><mi id="S3.SS4.p3.8.m8.1.1.2" xref="S3.SS4.p3.8.m8.1.1.2.cmml">X</mi><mo id="S3.SS4.p3.8.m8.1.1.3" xref="S3.SS4.p3.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.1b"><apply id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m8.1.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1">superscript</csymbol><ci id="S3.SS4.p3.8.m8.1.1.2.cmml" xref="S3.SS4.p3.8.m8.1.1.2">𝑋</ci><ci id="S3.SS4.p3.8.m8.1.1.3.cmml" xref="S3.SS4.p3.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.1c">X^{\prime}</annotation></semantics></math> and <math id="S3.SS4.p3.9.m9.1" class="ltx_Math" alttext="Y^{\prime}" display="inline"><semantics id="S3.SS4.p3.9.m9.1a"><msup id="S3.SS4.p3.9.m9.1.1" xref="S3.SS4.p3.9.m9.1.1.cmml"><mi id="S3.SS4.p3.9.m9.1.1.2" xref="S3.SS4.p3.9.m9.1.1.2.cmml">Y</mi><mo id="S3.SS4.p3.9.m9.1.1.3" xref="S3.SS4.p3.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m9.1b"><apply id="S3.SS4.p3.9.m9.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m9.1.1.1.cmml" xref="S3.SS4.p3.9.m9.1.1">superscript</csymbol><ci id="S3.SS4.p3.9.m9.1.1.2.cmml" xref="S3.SS4.p3.9.m9.1.1.2">𝑌</ci><ci id="S3.SS4.p3.9.m9.1.1.3.cmml" xref="S3.SS4.p3.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.9.m9.1c">Y^{\prime}</annotation></semantics></math>. Finally, fused feature is fed to a <span id="S3.SS4.p3.11.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">LayerNorm</span> to stabilize the training followed by a fully connected layer and sigmoid activation to generate predicted answer <math id="S3.SS4.p3.11.m11.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS4.p3.11.m11.1a"><mi id="S3.SS4.p3.11.m11.1.1" xref="S3.SS4.p3.11.m11.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.11.m11.1b"><ci id="S3.SS4.p3.11.m11.1.1.cmml" xref="S3.SS4.p3.11.m11.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.11.m11.1c">Z</annotation></semantics></math>. We use binary cross-entropy loss (BCE) to train the network.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">On the other hand, we also leverage a powerful fusion technique, MUTAN fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, to integrate image and question features (see figure <a href="#S3.F5" title="Figure 5 ‣ 3.4 Multi-modal Fusion. ‣ 3 Our Approach ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b)) in multi-modal mutan fusion. The network is similar to the above model but replacing the concatenation to MUTAN fusion with fully-connected layers (<span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S3.SS4.p4.1.m1.2" class="ltx_Math" alttext="\text{\tt Dropout}(0.2)-\text{\tt FC}(2)-\text{\tt Softmax}" display="inline"><semantics id="S3.SS4.p4.1.m1.2a"><mrow id="S3.SS4.p4.1.m1.2.3" xref="S3.SS4.p4.1.m1.2.3.cmml"><mrow id="S3.SS4.p4.1.m1.2.3.2" xref="S3.SS4.p4.1.m1.2.3.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.2.2" xref="S3.SS4.p4.1.m1.2.3.2.2a.cmml">Dropout</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p4.1.m1.2.3.2.1" xref="S3.SS4.p4.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.SS4.p4.1.m1.2.3.2.3.2" xref="S3.SS4.p4.1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.SS4.p4.1.m1.2.3.2.3.2.1" xref="S3.SS4.p4.1.m1.2.3.2.cmml">(</mo><mn id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">0.2</mn><mo stretchy="false" id="S3.SS4.p4.1.m1.2.3.2.3.2.2" xref="S3.SS4.p4.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.1.m1.2.3.1" xref="S3.SS4.p4.1.m1.2.3.1.cmml">−</mo><mrow id="S3.SS4.p4.1.m1.2.3.3" xref="S3.SS4.p4.1.m1.2.3.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.3.2" xref="S3.SS4.p4.1.m1.2.3.3.2a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p4.1.m1.2.3.3.1" xref="S3.SS4.p4.1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.SS4.p4.1.m1.2.3.3.3.2" xref="S3.SS4.p4.1.m1.2.3.3.cmml"><mo stretchy="false" id="S3.SS4.p4.1.m1.2.3.3.3.2.1" xref="S3.SS4.p4.1.m1.2.3.3.cmml">(</mo><mn id="S3.SS4.p4.1.m1.2.2" xref="S3.SS4.p4.1.m1.2.2.cmml">2</mn><mo stretchy="false" id="S3.SS4.p4.1.m1.2.3.3.3.2.2" xref="S3.SS4.p4.1.m1.2.3.3.cmml">)</mo></mrow></mrow><mo id="S3.SS4.p4.1.m1.2.3.1a" xref="S3.SS4.p4.1.m1.2.3.1.cmml">−</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.4" xref="S3.SS4.p4.1.m1.2.3.4a.cmml">Softmax</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.2b"><apply id="S3.SS4.p4.1.m1.2.3.cmml" xref="S3.SS4.p4.1.m1.2.3"><minus id="S3.SS4.p4.1.m1.2.3.1.cmml" xref="S3.SS4.p4.1.m1.2.3.1"></minus><apply id="S3.SS4.p4.1.m1.2.3.2.cmml" xref="S3.SS4.p4.1.m1.2.3.2"><times id="S3.SS4.p4.1.m1.2.3.2.1.cmml" xref="S3.SS4.p4.1.m1.2.3.2.1"></times><ci id="S3.SS4.p4.1.m1.2.3.2.2a.cmml" xref="S3.SS4.p4.1.m1.2.3.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.2.2.cmml" xref="S3.SS4.p4.1.m1.2.3.2.2">Dropout</mtext></ci><cn type="float" id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">0.2</cn></apply><apply id="S3.SS4.p4.1.m1.2.3.3.cmml" xref="S3.SS4.p4.1.m1.2.3.3"><times id="S3.SS4.p4.1.m1.2.3.3.1.cmml" xref="S3.SS4.p4.1.m1.2.3.3.1"></times><ci id="S3.SS4.p4.1.m1.2.3.3.2a.cmml" xref="S3.SS4.p4.1.m1.2.3.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.3.2.cmml" xref="S3.SS4.p4.1.m1.2.3.3.2">FC</mtext></ci><cn type="integer" id="S3.SS4.p4.1.m1.2.2.cmml" xref="S3.SS4.p4.1.m1.2.2">2</cn></apply><ci id="S3.SS4.p4.1.m1.2.3.4a.cmml" xref="S3.SS4.p4.1.m1.2.3.4"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.p4.1.m1.2.3.4.cmml" xref="S3.SS4.p4.1.m1.2.3.4">Softmax</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.2c">\text{\tt Dropout}(0.2)-\text{\tt FC}(2)-\text{\tt Softmax}</annotation></semantics></math>).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we first describe the dataset (see Section <a href="#S4.SS1" title="4.1 Datasets ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) used in our experiments. Then we present experimental setup and implementation details in Section <a href="#S4.SS2" title="4.2 Experiment and Implementation Details ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. In Section <a href="#S4.SS3" title="4.3 Ablation studies ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we include a number of ablations to show the effectiveness of our proposed model. Lastly, we discuss experimental results in Section <a href="#S4.SS4" title="4.4 Experimental Results ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.8.1.1" class="ltx_tr">
<th id="S4.T1.8.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.8.1.1.1.1" class="ltx_text ltx_font_bold">L</span></th>
<th id="S4.T1.8.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.8.1.1.2.1" class="ltx_text ltx_font_bold">All</span></th>
<th id="S4.T1.8.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.8.1.1.3.1" class="ltx_text ltx_font_bold">Other</span></th>
<th id="S4.T1.8.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.8.1.1.4.1" class="ltx_text ltx_font_bold">Y/N</span></th>
<th id="S4.T1.8.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.8.1.1.5.1" class="ltx_text ltx_font_bold">Num</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.8.2.1" class="ltx_tr">
<td id="S4.T1.8.2.1.1" class="ltx_td ltx_align_left ltx_border_t">L = 2</td>
<td id="S4.T1.8.2.1.2" class="ltx_td ltx_align_left ltx_border_t">81.88</td>
<td id="S4.T1.8.2.1.3" class="ltx_td ltx_align_left ltx_border_t">74.47</td>
<td id="S4.T1.8.2.1.4" class="ltx_td ltx_align_left ltx_border_t">96.11</td>
<td id="S4.T1.8.2.1.5" class="ltx_td ltx_align_left ltx_border_t">69.00</td>
</tr>
<tr id="S4.T1.8.3.2" class="ltx_tr">
<td id="S4.T1.8.3.2.1" class="ltx_td ltx_align_left">L = 4</td>
<td id="S4.T1.8.3.2.2" class="ltx_td ltx_align_left">83.34</td>
<td id="S4.T1.8.3.2.3" class="ltx_td ltx_align_left"><span id="S4.T1.8.3.2.3.1" class="ltx_text ltx_font_bold">76.48</span></td>
<td id="S4.T1.8.3.2.4" class="ltx_td ltx_align_left">96.65</td>
<td id="S4.T1.8.3.2.5" class="ltx_td ltx_align_left">71.00</td>
</tr>
<tr id="S4.T1.8.4.3" class="ltx_tr">
<td id="S4.T1.8.4.3.1" class="ltx_td ltx_align_left">L = 6</td>
<td id="S4.T1.8.4.3.2" class="ltx_td ltx_align_left"><span id="S4.T1.8.4.3.2.1" class="ltx_text ltx_font_bold">83.45</span></td>
<td id="S4.T1.8.4.3.3" class="ltx_td ltx_align_left">76.45</td>
<td id="S4.T1.8.4.3.4" class="ltx_td ltx_align_left"><span id="S4.T1.8.4.3.4.1" class="ltx_text ltx_font_bold">96.83</span></td>
<td id="S4.T1.8.4.3.5" class="ltx_td ltx_align_left"><span id="S4.T1.8.4.3.5.1" class="ltx_text ltx_font_bold">71.44</span></td>
</tr>
<tr id="S4.T1.8.5.4" class="ltx_tr">
<td id="S4.T1.8.5.4.1" class="ltx_td ltx_align_left ltx_border_b">L = 8</td>
<td id="S4.T1.8.5.4.2" class="ltx_td ltx_align_left ltx_border_b">82.20</td>
<td id="S4.T1.8.5.4.3" class="ltx_td ltx_align_left ltx_border_b">75.42</td>
<td id="S4.T1.8.5.4.4" class="ltx_td ltx_align_left ltx_border_b">95.87</td>
<td id="S4.T1.8.5.4.5" class="ltx_td ltx_align_left ltx_border_b">68.53</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.9.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.6.3" class="ltx_text" style="font-size:90%;"> <span id="S4.T1.4.1.1" class="ltx_text ltx_font_bold">Experimental results with different <math id="S4.T1.4.1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.T1.4.1.1.m1.1b"><mi id="S4.T1.4.1.1.m1.1.1" xref="S4.T1.4.1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.1.1.m1.1c"><ci id="S4.T1.4.1.1.m1.1.1.cmml" xref="S4.T1.4.1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.1.1.m1.1d">L</annotation></semantics></math>.</span> Here we use a range of values from 2 to 8 on validation set. Best performance is achieved with <math id="S4.T1.5.2.m1.1" class="ltx_Math" alttext="L=6" display="inline"><semantics id="S4.T1.5.2.m1.1b"><mrow id="S4.T1.5.2.m1.1.1" xref="S4.T1.5.2.m1.1.1.cmml"><mi id="S4.T1.5.2.m1.1.1.2" xref="S4.T1.5.2.m1.1.1.2.cmml">L</mi><mo id="S4.T1.5.2.m1.1.1.1" xref="S4.T1.5.2.m1.1.1.1.cmml">=</mo><mn id="S4.T1.5.2.m1.1.1.3" xref="S4.T1.5.2.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.2.m1.1c"><apply id="S4.T1.5.2.m1.1.1.cmml" xref="S4.T1.5.2.m1.1.1"><eq id="S4.T1.5.2.m1.1.1.1.cmml" xref="S4.T1.5.2.m1.1.1.1"></eq><ci id="S4.T1.5.2.m1.1.1.2.cmml" xref="S4.T1.5.2.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.T1.5.2.m1.1.1.3.cmml" xref="S4.T1.5.2.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.2.m1.1d">L=6</annotation></semantics></math>. Therefore, in this paper we choose <math id="S4.T1.6.3.m2.1" class="ltx_Math" alttext="L=6" display="inline"><semantics id="S4.T1.6.3.m2.1b"><mrow id="S4.T1.6.3.m2.1.1" xref="S4.T1.6.3.m2.1.1.cmml"><mi id="S4.T1.6.3.m2.1.1.2" xref="S4.T1.6.3.m2.1.1.2.cmml">L</mi><mo id="S4.T1.6.3.m2.1.1.1" xref="S4.T1.6.3.m2.1.1.1.cmml">=</mo><mn id="S4.T1.6.3.m2.1.1.3" xref="S4.T1.6.3.m2.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.3.m2.1c"><apply id="S4.T1.6.3.m2.1.1.cmml" xref="S4.T1.6.3.m2.1.1"><eq id="S4.T1.6.3.m2.1.1.1.cmml" xref="S4.T1.6.3.m2.1.1.1"></eq><ci id="S4.T1.6.3.m2.1.1.2.cmml" xref="S4.T1.6.3.m2.1.1.2">𝐿</ci><cn type="integer" id="S4.T1.6.3.m2.1.1.3.cmml" xref="S4.T1.6.3.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.3.m2.1d">L=6</annotation></semantics></math> for our work.</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">All</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">Other</span></th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.2.1.1.4.1" class="ltx_text ltx_font_bold">Y/N</span></th>
<th id="S4.T2.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.2.1.1.5.1" class="ltx_text ltx_font_bold">Num</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<td id="S4.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">81.20</td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_align_left ltx_border_t">73.73</td>
<td id="S4.T2.2.2.1.4" class="ltx_td ltx_align_left ltx_border_t">95.86</td>
<td id="S4.T2.2.2.1.5" class="ltx_td ltx_align_left ltx_border_t">67.30</td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<td id="S4.T2.2.3.2.1" class="ltx_td ltx_align_left">Ours (MCAoAN)</td>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_left">82.91</td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_align_left">75.92</td>
<td id="S4.T2.2.3.2.4" class="ltx_td ltx_align_left">96.47</td>
<td id="S4.T2.2.3.2.5" class="ltx_td ltx_align_left">70.38</td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<td id="S4.T2.2.4.3.1" class="ltx_td ltx_align_left">Ours (MCAoAN + Mutan)</td>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_left">83.00</td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_align_left">76.13</td>
<td id="S4.T2.2.4.3.4" class="ltx_td ltx_align_left">96.36</td>
<td id="S4.T2.2.4.3.5" class="ltx_td ltx_align_left"><span id="S4.T2.2.4.3.5.1" class="ltx_text ltx_font_bold">70.42</span></td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<td id="S4.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_border_b">Ours (MCAoAN + Multi-modal Attention Fusion)</td>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T2.2.5.4.2.1" class="ltx_text ltx_font_bold">83.25</span></td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T2.2.5.4.3.1" class="ltx_text ltx_font_bold">76.51</span></td>
<td id="S4.T2.2.5.4.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T2.2.5.4.4.1" class="ltx_text ltx_font_bold">96.58</span></td>
<td id="S4.T2.2.5.4.5" class="ltx_td ltx_align_left ltx_border_b">70.40</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;"> <span id="S4.T2.5.2.1" class="ltx_text ltx_font_bold">Visual Question Answering results using VQA-v2 dataset</span>. Comparison of our proposed approach with state-of-the-art method on validation set. Here we also show each component in our proposed method contribute to increase the performance of VQA system. </span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T3.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">All</span></th>
<th id="S4.T3.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.2.1.1.3.1" class="ltx_text ltx_font_bold">Other</span></th>
<th id="S4.T3.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.2.1.1.4.1" class="ltx_text ltx_font_bold">Y/N</span></th>
<th id="S4.T3.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.2.1.1.5.1" class="ltx_text ltx_font_bold">Num</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.1" class="ltx_tr">
<td id="S4.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Bottom-up <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T3.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">65.32</td>
<td id="S4.T3.2.2.1.3" class="ltx_td ltx_align_left ltx_border_t">56.05</td>
<td id="S4.T3.2.2.1.4" class="ltx_td ltx_align_left ltx_border_t">81.82</td>
<td id="S4.T3.2.2.1.5" class="ltx_td ltx_align_left ltx_border_t">44.21</td>
</tr>
<tr id="S4.T3.2.3.2" class="ltx_tr">
<td id="S4.T3.2.3.2.1" class="ltx_td ltx_align_left">MFH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T3.2.3.2.2" class="ltx_td ltx_align_left">68.76</td>
<td id="S4.T3.2.3.2.3" class="ltx_td ltx_align_left">59.89</td>
<td id="S4.T3.2.3.2.4" class="ltx_td ltx_align_left">84.27</td>
<td id="S4.T3.2.3.2.5" class="ltx_td ltx_align_left">49.56</td>
</tr>
<tr id="S4.T3.2.4.3" class="ltx_tr">
<td id="S4.T3.2.4.3.1" class="ltx_td ltx_align_left">BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S4.T3.2.4.3.2" class="ltx_td ltx_align_left">69.52</td>
<td id="S4.T3.2.4.3.3" class="ltx_td ltx_align_left">60.26</td>
<td id="S4.T3.2.4.3.4" class="ltx_td ltx_align_left">85.31</td>
<td id="S4.T3.2.4.3.5" class="ltx_td ltx_align_left">50.93</td>
</tr>
<tr id="S4.T3.2.5.4" class="ltx_tr">
<td id="S4.T3.2.5.4.1" class="ltx_td ltx_align_left">BAN+Counter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S4.T3.2.5.4.2" class="ltx_td ltx_align_left">70.04</td>
<td id="S4.T3.2.5.4.3" class="ltx_td ltx_align_left">60.52</td>
<td id="S4.T3.2.5.4.4" class="ltx_td ltx_align_left">85.42</td>
<td id="S4.T3.2.5.4.5" class="ltx_td ltx_align_left">54.04</td>
</tr>
<tr id="S4.T3.2.6.5" class="ltx_tr">
<td id="S4.T3.2.6.5.1" class="ltx_td ltx_align_left">MuRel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T3.2.6.5.2" class="ltx_td ltx_align_left">68.03</td>
<td id="S4.T3.2.6.5.3" class="ltx_td ltx_align_left">57.85</td>
<td id="S4.T3.2.6.5.4" class="ltx_td ltx_align_left">84.77</td>
<td id="S4.T3.2.6.5.5" class="ltx_td ltx_align_left">49.84</td>
</tr>
<tr id="S4.T3.2.7.6" class="ltx_tr">
<td id="S4.T3.2.7.6.1" class="ltx_td ltx_align_left">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T3.2.7.6.2" class="ltx_td ltx_align_left">70.63</td>
<td id="S4.T3.2.7.6.3" class="ltx_td ltx_align_left">60.72</td>
<td id="S4.T3.2.7.6.4" class="ltx_td ltx_align_left">86.82</td>
<td id="S4.T3.2.7.6.5" class="ltx_td ltx_align_left">53.26</td>
</tr>
<tr id="S4.T3.2.8.7" class="ltx_tr">
<td id="S4.T3.2.8.7.1" class="ltx_td ltx_align_left ltx_border_b">Ours (MCAoA)</td>
<td id="S4.T3.2.8.7.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.2.8.7.2.1" class="ltx_text ltx_font_bold">70.90</span></td>
<td id="S4.T3.2.8.7.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.2.8.7.3.1" class="ltx_text ltx_font_bold">60.97</span></td>
<td id="S4.T3.2.8.7.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.2.8.7.4.1" class="ltx_text ltx_font_bold">87.05</span></td>
<td id="S4.T3.2.8.7.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.2.8.7.5.1" class="ltx_text ltx_font_bold">53.81</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;"> <span id="S4.T3.5.2.1" class="ltx_text ltx_font_bold">Experimental results with other state-of-the-art models on Test-dev.</span> </span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.2.1" class="ltx_text ltx_font_bold">All</span></th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.3.1" class="ltx_text ltx_font_bold">Other</span></th>
<th id="S4.T4.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.4.1" class="ltx_text ltx_font_bold">Y/N</span></th>
<th id="S4.T4.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.5.1" class="ltx_text ltx_font_bold">Num</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.1" class="ltx_tr">
<td id="S4.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Bottom-up <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T4.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">65.67</td>
<td id="S4.T4.2.2.1.3" class="ltx_td ltx_align_left ltx_border_t">56.26</td>
<td id="S4.T4.2.2.1.4" class="ltx_td ltx_align_left ltx_border_t">82.20</td>
<td id="S4.T4.2.2.1.5" class="ltx_td ltx_align_left ltx_border_t">43.90</td>
</tr>
<tr id="S4.T4.2.3.2" class="ltx_tr">
<td id="S4.T4.2.3.2.1" class="ltx_td ltx_align_left">BAN+Counter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S4.T4.2.3.2.2" class="ltx_td ltx_align_left">70.35</td>
<td id="S4.T4.2.3.2.3" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.3.2.4" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.3.2.5" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T4.2.4.3" class="ltx_tr">
<td id="S4.T4.2.4.3.1" class="ltx_td ltx_align_left">MuRel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T4.2.4.3.2" class="ltx_td ltx_align_left">68.41</td>
<td id="S4.T4.2.4.3.3" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.4.3.4" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.4.3.5" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T4.2.5.4" class="ltx_tr">
<td id="S4.T4.2.5.4.1" class="ltx_td ltx_align_left">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T4.2.5.4.2" class="ltx_td ltx_align_left">70.90</td>
<td id="S4.T4.2.5.4.3" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.5.4.4" class="ltx_td ltx_align_left">-</td>
<td id="S4.T4.2.5.4.5" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T4.2.6.5" class="ltx_tr">
<td id="S4.T4.2.6.5.1" class="ltx_td ltx_align_left ltx_border_b">Ours (MCAoA)</td>
<td id="S4.T4.2.6.5.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T4.2.6.5.2.1" class="ltx_text ltx_font_bold">71.14</span></td>
<td id="S4.T4.2.6.5.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T4.2.6.5.3.1" class="ltx_text ltx_font_bold">61.18</span></td>
<td id="S4.T4.2.6.5.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T4.2.6.5.4.1" class="ltx_text ltx_font_bold">87.25</span></td>
<td id="S4.T4.2.6.5.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T4.2.6.5.5.1" class="ltx_text ltx_font_bold">53.36</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.5.2" class="ltx_text" style="font-size:90%;"> <span id="S4.T4.5.2.1" class="ltx_text ltx_font_bold">Experimental results with other state-of-the-art models on Test-std.</span> </span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate our method, in this paper we use VQA-v2 benchmark dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which consists of images from MS-COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> with human annotated question-answer pairs. There are 3 questions for each image and 10 answers per questions. The dataset has three parts: train set (80k images with 444k QA pairs), validation set (40k images with 214k QA pairs) and test set (80k images with 448k QA pairs). Moreover, test set is splited into two subsets: test-dev and test-standard where both are used for online evaluation performance. For measuring the overall accuracy, three types of answer are considered: Number, Yes/No and other.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment and Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To evaluate our method, we follow the experimental protocol proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The number of head in multi-head attention is 8. The latent dimension for both multi-head and AoA block is 512. Therefore, the dimension of each head is <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="512/8=64" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mrow id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">512</mn><mo id="S4.SS2.p1.1.m1.1.1.2.1" xref="S4.SS2.p1.1.m1.1.1.2.1.cmml">/</mo><mn id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">8</mn></mrow><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><divide id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1.2.1"></divide><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">512</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">8</cn></apply><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">512/8=64</annotation></semantics></math>. The size of the answer vocabulary is 3129.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.7" class="ltx_p">To train the MCAoA network we use Adam solver with <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><msub id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2.2" xref="S4.SS2.p2.1.m1.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p2.1.m1.1.1.2.3" xref="S4.SS2.p2.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><apply id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.p2.1.m1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\beta_{1}=0.9</annotation></semantics></math> and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\beta_{2}=0.98" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><msub id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p2.2.m2.1.1.2.3" xref="S4.SS2.p2.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\beta_{2}=0.98</annotation></semantics></math>. We train our network up to 13 epoch with batch size 64 which takes around 24hrs to complete the training. The learning rate set to <math id="S4.SS2.p2.3.m3.2" class="ltx_Math" alttext="min(2.5Te^{-5},1e^{-4})" display="inline"><semantics id="S4.SS2.p2.3.m3.2a"><mrow id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml"><mi id="S4.SS2.p2.3.m3.2.2.4" xref="S4.SS2.p2.3.m3.2.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.2.2.3" xref="S4.SS2.p2.3.m3.2.2.3.cmml">​</mo><mi id="S4.SS2.p2.3.m3.2.2.5" xref="S4.SS2.p2.3.m3.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.2.2.3a" xref="S4.SS2.p2.3.m3.2.2.3.cmml">​</mo><mi id="S4.SS2.p2.3.m3.2.2.6" xref="S4.SS2.p2.3.m3.2.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.2.2.3b" xref="S4.SS2.p2.3.m3.2.2.3.cmml">​</mo><mrow id="S4.SS2.p2.3.m3.2.2.2.2" xref="S4.SS2.p2.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="S4.SS2.p2.3.m3.2.2.2.2.3" xref="S4.SS2.p2.3.m3.2.2.2.3.cmml">(</mo><mrow id="S4.SS2.p2.3.m3.1.1.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.1.1.1.2" xref="S4.SS2.p2.3.m3.1.1.1.1.1.2.cmml">2.5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.1.1.1.3" xref="S4.SS2.p2.3.m3.1.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1.1.1.1a" xref="S4.SS2.p2.3.m3.1.1.1.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.3.m3.1.1.1.1.1.4" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.cmml"><mi id="S4.SS2.p2.3.m3.1.1.1.1.1.4.2" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.2.cmml">e</mi><mrow id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.cmml"><mo id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3a" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.cmml">−</mo><mn id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.2" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.2.cmml">5</mn></mrow></msup></mrow><mo id="S4.SS2.p2.3.m3.2.2.2.2.4" xref="S4.SS2.p2.3.m3.2.2.2.3.cmml">,</mo><mrow id="S4.SS2.p2.3.m3.2.2.2.2.2" xref="S4.SS2.p2.3.m3.2.2.2.2.2.cmml"><mn id="S4.SS2.p2.3.m3.2.2.2.2.2.2" xref="S4.SS2.p2.3.m3.2.2.2.2.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.2.2.2.2.2.1" xref="S4.SS2.p2.3.m3.2.2.2.2.2.1.cmml">​</mo><msup id="S4.SS2.p2.3.m3.2.2.2.2.2.3" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.cmml"><mi id="S4.SS2.p2.3.m3.2.2.2.2.2.3.2" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.2.cmml">e</mi><mrow id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.cmml"><mo id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3a" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.cmml">−</mo><mn id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.2" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.2.cmml">4</mn></mrow></msup></mrow><mo stretchy="false" id="S4.SS2.p2.3.m3.2.2.2.2.5" xref="S4.SS2.p2.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.2b"><apply id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2"><times id="S4.SS2.p2.3.m3.2.2.3.cmml" xref="S4.SS2.p2.3.m3.2.2.3"></times><ci id="S4.SS2.p2.3.m3.2.2.4.cmml" xref="S4.SS2.p2.3.m3.2.2.4">𝑚</ci><ci id="S4.SS2.p2.3.m3.2.2.5.cmml" xref="S4.SS2.p2.3.m3.2.2.5">𝑖</ci><ci id="S4.SS2.p2.3.m3.2.2.6.cmml" xref="S4.SS2.p2.3.m3.2.2.6">𝑛</ci><interval closure="open" id="S4.SS2.p2.3.m3.2.2.2.3.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2"><apply id="S4.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.1"></times><cn type="float" id="S4.SS2.p2.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.2">2.5</cn><ci id="S4.SS2.p2.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.3">𝑇</ci><apply id="S4.SS2.p2.3.m3.1.1.1.1.1.4.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.1.1.4.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.1.1.1.4.2.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.2">𝑒</ci><apply id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3"><minus id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3"></minus><cn type="integer" id="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1.1.4.3.2">5</cn></apply></apply></apply><apply id="S4.SS2.p2.3.m3.2.2.2.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2"><times id="S4.SS2.p2.3.m3.2.2.2.2.2.1.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.2.2.2.2.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.2">1</cn><apply id="S4.SS2.p2.3.m3.2.2.2.2.2.3.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.2.2.2.2.2.3.1.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3">superscript</csymbol><ci id="S4.SS2.p2.3.m3.2.2.2.2.2.3.2.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.2">𝑒</ci><apply id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3"><minus id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.1.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3"></minus><cn type="integer" id="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.2.cmml" xref="S4.SS2.p2.3.m3.2.2.2.2.2.3.3.2">4</cn></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.2c">min(2.5Te^{-5},1e^{-4})</annotation></semantics></math> where <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">T</annotation></semantics></math> represents current epoch. Learning rate starts to decay by <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="1/5" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mn id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">1</mn><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">/</mo><mn id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><divide id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></divide><cn type="integer" id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">1</cn><cn type="integer" id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">1/5</annotation></semantics></math> every <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mn id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><cn type="integer" id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">2</annotation></semantics></math> epochs when <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="10\leq T" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mrow id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mn id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">10</mn><mo id="S4.SS2.p2.7.m7.1.1.1" xref="S4.SS2.p2.7.m7.1.1.1.cmml">≤</mo><mi id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><leq id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1.1"></leq><cn type="integer" id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">10</cn><ci id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">10\leq T</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation studies</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We run a number of experiments to show the effectiveness of our proposed method and results of these experiments are presented in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.5" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Number of Cascaded Layer (<math id="S4.SS3.p2.1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS3.p2.1.1.m1.1a"><mi id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><ci id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.1c">L</annotation></semantics></math>):</span> MCAoA layers consist of <math id="S4.SS3.p2.2.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS3.p2.2.m1.1a"><mi id="S4.SS3.p2.2.m1.1.1" xref="S4.SS3.p2.2.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m1.1b"><ci id="S4.SS3.p2.2.m1.1.1.cmml" xref="S4.SS3.p2.2.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m1.1c">L</annotation></semantics></math> number of stacked SAoA and GAoA units. From Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see, initially, with the increasing value of <math id="S4.SS3.p2.3.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS3.p2.3.m2.1a"><mi id="S4.SS3.p2.3.m2.1.1" xref="S4.SS3.p2.3.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m2.1b"><ci id="S4.SS3.p2.3.m2.1.1.cmml" xref="S4.SS3.p2.3.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m2.1c">L</annotation></semantics></math>, performance of the model is also increasing – up to <math id="S4.SS3.p2.4.m3.1" class="ltx_Math" alttext="L=6" display="inline"><semantics id="S4.SS3.p2.4.m3.1a"><mrow id="S4.SS3.p2.4.m3.1.1" xref="S4.SS3.p2.4.m3.1.1.cmml"><mi id="S4.SS3.p2.4.m3.1.1.2" xref="S4.SS3.p2.4.m3.1.1.2.cmml">L</mi><mo id="S4.SS3.p2.4.m3.1.1.1" xref="S4.SS3.p2.4.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.4.m3.1.1.3" xref="S4.SS3.p2.4.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m3.1b"><apply id="S4.SS3.p2.4.m3.1.1.cmml" xref="S4.SS3.p2.4.m3.1.1"><eq id="S4.SS3.p2.4.m3.1.1.1.cmml" xref="S4.SS3.p2.4.m3.1.1.1"></eq><ci id="S4.SS3.p2.4.m3.1.1.2.cmml" xref="S4.SS3.p2.4.m3.1.1.2">𝐿</ci><cn type="integer" id="S4.SS3.p2.4.m3.1.1.3.cmml" xref="S4.SS3.p2.4.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m3.1c">L=6</annotation></semantics></math>. After that the performance is saturated. We use <math id="S4.SS3.p2.5.m4.1" class="ltx_Math" alttext="L=6" display="inline"><semantics id="S4.SS3.p2.5.m4.1a"><mrow id="S4.SS3.p2.5.m4.1.1" xref="S4.SS3.p2.5.m4.1.1.cmml"><mi id="S4.SS3.p2.5.m4.1.1.2" xref="S4.SS3.p2.5.m4.1.1.2.cmml">L</mi><mo id="S4.SS3.p2.5.m4.1.1.1" xref="S4.SS3.p2.5.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.5.m4.1.1.3" xref="S4.SS3.p2.5.m4.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m4.1b"><apply id="S4.SS3.p2.5.m4.1.1.cmml" xref="S4.SS3.p2.5.m4.1.1"><eq id="S4.SS3.p2.5.m4.1.1.1.cmml" xref="S4.SS3.p2.5.m4.1.1.1"></eq><ci id="S4.SS3.p2.5.m4.1.1.2.cmml" xref="S4.SS3.p2.5.m4.1.1.2">𝐿</ci><cn type="integer" id="S4.SS3.p2.5.m4.1.1.3.cmml" xref="S4.SS3.p2.5.m4.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m4.1c">L=6</annotation></semantics></math> in our final model. We use <em id="S4.SS3.p2.5.2" class="ltx_emph ltx_font_italic">validation set</em> for this experiment with the default hyperparameters of  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2011.02164/assets/x7.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of some qualitative results from validation set using MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and our method.<span id="S4.F6.4.2.1" class="ltx_text ltx_font_medium"> First we present ground-truth (GT) annotations followed by the predicted answers of state-of-the-art method and our proposed method. Here Q and A represents query question and generated answer respectively. Moreover, red text indicates predicted wrong answer for the corresponding question.</span></span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2011.02164/assets/x8.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="385" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative results with multi-modal fusion.<span id="S4.F7.4.2.1" class="ltx_text ltx_font_medium"> The first row is the input images, questions and ground truth answers. The second row is the baseline model MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. The third row is the proposed model, MCAoAN w/ multi-modal fusion. The probabilities on the image and in front of the question represent the weight from each modality. We also show the attention across bounding boxes and words. In the image, the brighter area with green bbox has higher weight. For questions, the darker color of the word, the higher attention score.</span></span></figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2011.02164/assets/x9.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="378" height="89" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Illustration of some failure cases using our method.<span id="S4.F8.4.2.1" class="ltx_text ltx_font_medium"> Here Q and A represents query question and predicted wrong answer (mark as red) respectively.</span></span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Effectiveness of Each Individual Component: </span> In this paper, our improved architecture has two important components: (1) MCAoAN network which consists of SAoA module and GAoA module and (2) Multi-modal fusion to incorporate image and language features. Here, we describe two different fusion mechanism : Mutan fusion and Multi-modal attention fusion. Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows experimental results of these individual components and compare with existing MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> on  <em id="S4.SS3.p3.1.2" class="ltx_emph ltx_font_italic">validation set</em>. From the table, we see that incorporating SAoA and GAoA module with MCAN improves the performance of VQA system.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Moreover, we argue that a sophisticated way to aggregate language and visual features to support multi-modal reasoning is essential to further boost the performance. Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> also shows the comparison of different fusions with the MCAoA only where the former achieves better performance. More specifically, our proposed MCAoAN with both multi-modal fusion modules outperforms the baseline about <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="2\%" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">2\%</annotation></semantics></math> accuracy on the whole <em id="S4.SS3.p4.1.1" class="ltx_emph ltx_font_italic">validation set</em>. This shows that the fusion module is important to combine vision and language representations. The proposed both fusion modules are suitable for VQA tasks. Among them multi-modal attention fusion performs the best. Beside that, Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> also shows that each individual component within our proposed method is important to increase the performance of VQA system.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We evaluate our model on VQA-v2 dataset and compare with other state-of-the-art methods. We re-run the PyTorch implementation provided by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/MILVLG/mcan-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/mcan-vqa</a></span></span></span> and compare the results with our proposed method. Table <a href="#S4.T3" title="Table 3 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4.T4" title="Table 4 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows experimental results using test-dev and test-std respectively using online evaluation <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://evalai.cloudcv.org/web/challenges/challenge-page/163/overview" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://evalai.cloudcv.org/web/challenges/challenge-page/163/overview</a></span></span></span>. Offline evaluation only supports on validation split (see table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Figure <a href="#S4.F6" title="Figure 6 ‣ 4.3 Ablation studies ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, shows some qualitative results using our method on validation set. From the experimental results, we can see that our proposed method outperforms other baseline methods on VQA. In Figure <a href="#S4.F7" title="Figure 7 ‣ 4.3 Ablation studies ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we also visualize multi-modal fusion to compare how correctly MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and our proposed multi-modal attention fusion can able to focus on image and question elements. The brighter bounding-box along with green color within the image and darker color in question represents higher attention score. We can see that our proposed method is able to focus more on correct answer. Beside that, Figure <a href="#S4.F8" title="Figure 8 ‣ 4.3 Ablation studies ‣ 4 Experiments ‣ An Improved Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows typical failure cases using our method.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose an improved end-to-end attention based architecture for visual question answering. Our proposed method includes modular co-attention on attention module with multi-modal fusion architecture. In this paper, we propose two version of multi-modal fusion : multi-modal attention fusion and multi-modal mutan fusion. Experimental results show that each component within our model improve the performance of VQA system. Moreover, The final network achieves significant performance on VQA-v2 dataset.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 6077–6086, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 2612–2620, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Murel: Multimodal relational reasoning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1989–1998, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Empirical evaluation of gated recurrent neural networks on sequence
modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.3555</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Language modeling with gated convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 933–941. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01847</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 6904–6913, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 9(8):1735–1780, 1997.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Attention on attention for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4634–4643, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages
1564–1574, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and
Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Hadamard product for low-rank bilinear pooling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1610.04325</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Stacked cross attention for image-text matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 201–216, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Ruiyu Li and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Visual question answering with question representation update (qru).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages
4655–4663, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Hierarchical question-image co-attention for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages
289–297, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Co-attending free-form regions and detections with multi-modal
multiplicative feature embedding for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.06794</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Oier Mees, Andreas Eitel, and Wolfram Burgard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Choosing smartly: Adaptive multimodal fusion for object detection in
changing environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 151–156. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Dual attention networks for multimodal reasoning and matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 299–307, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Duy-Kien Nguyen and Takayuki Okatani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Improved fusion of visual and language representations by dense
symmetric co-attention for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 6087–6096, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 1532–1543, 2014.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Tanzila Rahman, Bicheng Xu, and Leonid Sigal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Watch, listen and tell: Multi-modal weakly supervised dense event
captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 8908–8917, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages
91–99, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Idan Schwartz, Alexander Schwing, and Tamir Hazan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">High-order attention models for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages
3664–3674, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Cycle-consistency for robust visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 6649–6658, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Kevin J Shih, Saurabh Singh, and Derek Hoiem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Where to look: Focus regions for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 4613–4621, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, and Snehasis Mukherjee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Visual question answering using deep learning: A survey and
performance analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.01860</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Peter Anderson, Xiaodong He, and Anton Van Den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Tips and tricks for visual question answering: Learnings from the
2017 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 4223–4232, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning two-branch neural networks for image-text matching tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">,
41(2):394–407, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Chenfei Wu, Jinlai Liu, Xiaojie Wang, and Ruifan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Differential networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, volume 33, pages 8997–9004, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Show, attend and tell: Neural image caption generation with visual
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages
2048–2057, 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 21–29, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 6281–6290, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 1821–1830, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Beyond bilinear: Generalized multimodal factorized high-order pooling
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on neural networks and learning systems</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">,
29(12):5947–5959, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.02163" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.02164" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.02164">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.02164" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.02165" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 04:40:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
