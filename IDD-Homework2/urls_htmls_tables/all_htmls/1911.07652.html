<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1911.07652] Information-Theoretic Perspective of Federated Learning</title><meta property="og:description" content="An approach to distributed machine learning is to train models on local datasets and aggregate these models into a single, stronger model. A popular instance of this form of parallelization is federated learning, where…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Information-Theoretic Perspective of Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Information-Theoretic Perspective of Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1911.07652">

<!--Generated on Sun Mar 17 12:49:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Information-Theoretic Perspective of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linara Adilova 
<br class="ltx_break">Fraunhofer Center for Machine Learning 
<br class="ltx_break">Fraunhofer IAIS, Germany 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">linara.adilova@iais.fraunhofer.de</span>
&amp;Julia Rosenzweig 
<br class="ltx_break">Fraunhofer IAIS, Germany 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">julia.rosenzweig@iais.fraunhofer.de</span>
&amp;Michael Kamp
<br class="ltx_break">Monash University, Australia 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">michael.kamp@monash.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">An approach to distributed machine learning is to train models on local datasets and aggregate these models into a single, stronger model. A popular instance of this form of parallelization is federated learning, where the nodes periodically send their local models to a coordinator that aggregates them and redistributes the aggregation back to continue training with it. The most frequently used form of aggregation is averaging the model parameters, e.g., the weights of a neural network. However, due to the non-convexity of the loss surface of neural networks, averaging can lead to detrimental effects and it remains an open question under which conditions averaging is beneficial. In this paper, we study this problem from the perspective of information theory: We measure the mutual information between representation and inputs as well as representation and labels in local models and compare it to the respective information contained in the representation of the averaged model. Our empirical results confirm previous observations about the practical usefulness of averaging for neural networks, even if local dataset distributions vary strongly. Furthermore, we obtain more insights about the impact of the aggregation frequency on the information flow and thus on the success of distributed learning. These insights will be helpful both in improving the current synchronization process and in further understanding the effects of model aggregation.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">In distributed machine learning with decentralized data and communication constraints, federated learning has become a popular approach, particularly for training neural networks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The idea is to train models locally and periodically average their parameters — assuming that all local networks have the same architecture. Local nodes then continue training with the average. This approach is well-understood in the convex case <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, but can be arbitrarily bad in the non-convex case of training neural networks. So far, the approach has been shown to work in practice in several scenarios, given that the local models are initialized similarly. The existing research is approaching this problem via studying the geometry of loss surfaces <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, but it is yet hard to apply it directly to the distributed case.
Information theory is a statistical basement of data science, but the fact that many expressions we encounter in this context are analytically intractable is a limiting factor for its widespread application. <cite class="ltx_cite ltx_citemacro_citet">Tishby and Zaslavsky [<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> were the first to apply information theory to deep learning. A neural network is seen as a Markov chain and the information about input data propagated through layers is decreasing with the deepness of the layer. The goal of the training is maximizing the information about the label contained in the representations in the network and compressing the information about the input as much as possible to still obtain sufficient statistics. Many others followed on this line of research, including <cite class="ltx_cite ltx_citemacro_citet">Shwartz-Ziv and Tishby [<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Saxe et al. [<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.6" class="ltx_p">We apply information theory to understand the aggregation process of deep neural networks in a federated learning setup. For that, we analyse the development of mutual information in the local and global models with averaging as aggregation method. Mutual information is a quantity that measures how much can be learned about one random variable from another. It is defined in terms of Kullback-Leibler divergence, i.e., <math id="S1.p2.1.m1.6" class="ltx_math_unparsed" alttext="MI(X,Y)\coloneqq KL(p(x,y)||p(x)p(y))" display="inline"><semantics id="S1.p2.1.m1.6a"><mrow id="S1.p2.1.m1.6b"><mi id="S1.p2.1.m1.6.7">M</mi><mi id="S1.p2.1.m1.6.8">I</mi><mrow id="S1.p2.1.m1.6.9"><mo stretchy="false" id="S1.p2.1.m1.6.9.1">(</mo><mi id="S1.p2.1.m1.1.1">X</mi><mo id="S1.p2.1.m1.6.9.2">,</mo><mi id="S1.p2.1.m1.2.2">Y</mi><mo stretchy="false" id="S1.p2.1.m1.6.9.3">)</mo></mrow><mo id="S1.p2.1.m1.6.10">≔</mo><mi id="S1.p2.1.m1.6.11">K</mi><mi id="S1.p2.1.m1.6.12">L</mi><mrow id="S1.p2.1.m1.6.13"><mo stretchy="false" id="S1.p2.1.m1.6.13.1">(</mo><mi id="S1.p2.1.m1.6.13.2">p</mi><mrow id="S1.p2.1.m1.6.13.3"><mo stretchy="false" id="S1.p2.1.m1.6.13.3.1">(</mo><mi id="S1.p2.1.m1.3.3">x</mi><mo id="S1.p2.1.m1.6.13.3.2">,</mo><mi id="S1.p2.1.m1.4.4">y</mi><mo stretchy="false" id="S1.p2.1.m1.6.13.3.3">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false" id="S1.p2.1.m1.6.13.4">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S1.p2.1.m1.6.13.5">|</mo><mi id="S1.p2.1.m1.6.13.6">p</mi><mrow id="S1.p2.1.m1.6.13.7"><mo stretchy="false" id="S1.p2.1.m1.6.13.7.1">(</mo><mi id="S1.p2.1.m1.5.5">x</mi><mo stretchy="false" id="S1.p2.1.m1.6.13.7.2">)</mo></mrow><mi id="S1.p2.1.m1.6.13.8">p</mi><mrow id="S1.p2.1.m1.6.13.9"><mo stretchy="false" id="S1.p2.1.m1.6.13.9.1">(</mo><mi id="S1.p2.1.m1.6.6">y</mi><mo stretchy="false" id="S1.p2.1.m1.6.13.9.2">)</mo></mrow><mo stretchy="false" id="S1.p2.1.m1.6.13.10">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S1.p2.1.m1.6c">MI(X,Y)\coloneqq KL(p(x,y)||p(x)p(y))</annotation></semantics></math> where <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="p(x)" display="inline"><semantics id="S1.p2.2.m2.1a"><mrow id="S1.p2.2.m2.1.2" xref="S1.p2.2.m2.1.2.cmml"><mi id="S1.p2.2.m2.1.2.2" xref="S1.p2.2.m2.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S1.p2.2.m2.1.2.1" xref="S1.p2.2.m2.1.2.1.cmml">​</mo><mrow id="S1.p2.2.m2.1.2.3.2" xref="S1.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S1.p2.2.m2.1.2.3.2.1" xref="S1.p2.2.m2.1.2.cmml">(</mo><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S1.p2.2.m2.1.2.3.2.2" xref="S1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.2.cmml" xref="S1.p2.2.m2.1.2"><times id="S1.p2.2.m2.1.2.1.cmml" xref="S1.p2.2.m2.1.2.1"></times><ci id="S1.p2.2.m2.1.2.2.cmml" xref="S1.p2.2.m2.1.2.2">𝑝</ci><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">p(x)</annotation></semantics></math> and <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="p(y)" display="inline"><semantics id="S1.p2.3.m3.1a"><mrow id="S1.p2.3.m3.1.2" xref="S1.p2.3.m3.1.2.cmml"><mi id="S1.p2.3.m3.1.2.2" xref="S1.p2.3.m3.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S1.p2.3.m3.1.2.1" xref="S1.p2.3.m3.1.2.1.cmml">​</mo><mrow id="S1.p2.3.m3.1.2.3.2" xref="S1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S1.p2.3.m3.1.2.3.2.1" xref="S1.p2.3.m3.1.2.cmml">(</mo><mi id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">y</mi><mo stretchy="false" id="S1.p2.3.m3.1.2.3.2.2" xref="S1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><apply id="S1.p2.3.m3.1.2.cmml" xref="S1.p2.3.m3.1.2"><times id="S1.p2.3.m3.1.2.1.cmml" xref="S1.p2.3.m3.1.2.1"></times><ci id="S1.p2.3.m3.1.2.2.cmml" xref="S1.p2.3.m3.1.2.2">𝑝</ci><ci id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">p(y)</annotation></semantics></math> are marginal probability distributions of <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S1.p2.4.m4.1a"><mi id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">X</annotation></semantics></math> and <math id="S1.p2.5.m5.1" class="ltx_Math" alttext="Y," display="inline"><semantics id="S1.p2.5.m5.1a"><mrow id="S1.p2.5.m5.1.2.2"><mi id="S1.p2.5.m5.1.1" xref="S1.p2.5.m5.1.1.cmml">Y</mi><mo id="S1.p2.5.m5.1.2.2.1">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.5.m5.1b"><ci id="S1.p2.5.m5.1.1.cmml" xref="S1.p2.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.5.m5.1c">Y,</annotation></semantics></math> respectively, and <math id="S1.p2.6.m6.2" class="ltx_Math" alttext="p(x,y)" display="inline"><semantics id="S1.p2.6.m6.2a"><mrow id="S1.p2.6.m6.2.3" xref="S1.p2.6.m6.2.3.cmml"><mi id="S1.p2.6.m6.2.3.2" xref="S1.p2.6.m6.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S1.p2.6.m6.2.3.1" xref="S1.p2.6.m6.2.3.1.cmml">​</mo><mrow id="S1.p2.6.m6.2.3.3.2" xref="S1.p2.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S1.p2.6.m6.2.3.3.2.1" xref="S1.p2.6.m6.2.3.3.1.cmml">(</mo><mi id="S1.p2.6.m6.1.1" xref="S1.p2.6.m6.1.1.cmml">x</mi><mo id="S1.p2.6.m6.2.3.3.2.2" xref="S1.p2.6.m6.2.3.3.1.cmml">,</mo><mi id="S1.p2.6.m6.2.2" xref="S1.p2.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S1.p2.6.m6.2.3.3.2.3" xref="S1.p2.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.6.m6.2b"><apply id="S1.p2.6.m6.2.3.cmml" xref="S1.p2.6.m6.2.3"><times id="S1.p2.6.m6.2.3.1.cmml" xref="S1.p2.6.m6.2.3.1"></times><ci id="S1.p2.6.m6.2.3.2.cmml" xref="S1.p2.6.m6.2.3.2">𝑝</ci><interval closure="open" id="S1.p2.6.m6.2.3.3.1.cmml" xref="S1.p2.6.m6.2.3.3.2"><ci id="S1.p2.6.m6.1.1.cmml" xref="S1.p2.6.m6.1.1">𝑥</ci><ci id="S1.p2.6.m6.2.2.cmml" xref="S1.p2.6.m6.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.6.m6.2c">p(x,y)</annotation></semantics></math> is the joint probability distribution.
There exist known conditions for the beneficial application of averaging, e.g., frequent aggregation into a global model and iid distribution of the local datasets <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We analyse the effect of the fulfillment of such conditions on the results of training.
To the best of our knowledge, there are no other attempts yet to study federated learning from an information-theoretic perspective.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Experiments</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">For our experiments we choose the image classification task CIFAR10 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and apply the convolutional network LeNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to learn it. For the distributed setup we take the case with two local models for more tractable analysis. Mutual Information is estimated at each aggregation with the EDGE technique <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for the local networks as well as for the global one. In the first setup we evaluate the impact of averaging once at the end of the training process. We analyze this with respect to the length of the training process and find that, the longer the training process, the less successful averaging is. The second setup is periodic averaging with redistributing the averaged model. We perform our experiments for the case of iid and non-iid local datasets, because according to the literature iid data results in better model quality <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and with non-iid datasets we can more clearly see how the information that is existent only in one of the local nodes propagates with the help of averaging.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.11" class="ltx_p">The aggregation is performed every <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.p2.1.m1.1a"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><cn type="integer" id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">100</annotation></semantics></math> local batches. Averaging on the networks is performed weight-vise. The training setup chosen is mini-batch stochastic gradient descent with batch size <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S2.p2.2.m2.1a"><mn id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><cn type="integer" id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">32</annotation></semantics></math>. Every local network is trained for <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S2.p2.3.m3.1a"><mn id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><cn type="integer" id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">20</annotation></semantics></math> epochs on its local dataset (of size <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="25000" display="inline"><semantics id="S2.p2.4.m4.1a"><mn id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">25000</mn><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><cn type="integer" id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">25000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">25000</annotation></semantics></math>).
We measure mutual information of the representation <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.p2.5.m5.1a"><mi id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">Z</annotation></semantics></math> with input <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.p2.6.m6.1a"><mi id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><ci id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">X</annotation></semantics></math> and of the representation <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.p2.7.m7.1a"><mi id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">Z</annotation></semantics></math> with label <math id="S2.p2.8.m8.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.p2.8.m8.1a"><mi id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><ci id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">Y</annotation></semantics></math>. We consider as representation <math id="S2.p2.9.m9.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.p2.9.m9.1a"><mi id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.1b"><ci id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.1c">Z</annotation></semantics></math> the output of the last hidden fully connected layer of the network. We estimate information separately for each of the local datasets in all the cases.
The non-iid case was emulated via separating <math id="S2.p2.10.m10.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.p2.10.m10.1a"><mn id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.1b"><cn type="integer" id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.1c">10</annotation></semantics></math> of the CIFAR10 classes into non intersecting groups of <math id="S2.p2.11.m11.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S2.p2.11.m11.1a"><mn id="S2.p2.11.m11.1.1" xref="S2.p2.11.m11.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.1b"><cn type="integer" id="S2.p2.11.m11.1.1.cmml" xref="S2.p2.11.m11.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.1c">5</annotation></semantics></math> classes each and presenting each local node examples only from one of these groups.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F1.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/iid/nosync/MI_input_paper.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F1.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/iid/nosync/MI_label_paper.png" id="S2.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Mutual information measured for the iid case without sending back the aggregated model. The estimated amount of information is shown for input <math id="S2.F1.5.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.F1.5.m1.1b"><mi id="S2.F1.5.m1.1.1" xref="S2.F1.5.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.F1.5.m1.1c"><ci id="S2.F1.5.m1.1.1.cmml" xref="S2.F1.5.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.5.m1.1d">X</annotation></semantics></math> (left) and label <math id="S2.F1.6.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.F1.6.m2.1b"><mi id="S2.F1.6.m2.1.1" xref="S2.F1.6.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.F1.6.m2.1c"><ci id="S2.F1.6.m2.1.1.cmml" xref="S2.F1.6.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.6.m2.1d">Y</annotation></semantics></math> (right) for each aggregation point.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/iid/sync/MI_input_paper.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/iid/sync/MI_label_paper.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mutual information measured for the iid case with aggregation process and continuing learning from the averaged point. The estimated amount of information is shown for input <math id="S2.F2.5.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.F2.5.m1.1b"><mi id="S2.F2.5.m1.1.1" xref="S2.F2.5.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.F2.5.m1.1c"><ci id="S2.F2.5.m1.1.1.cmml" xref="S2.F2.5.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m1.1d">X</annotation></semantics></math> (left) and label <math id="S2.F2.6.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.F2.6.m2.1b"><mi id="S2.F2.6.m2.1.1" xref="S2.F2.6.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.F2.6.m2.1c"><ci id="S2.F2.6.m2.1.1.cmml" xref="S2.F2.6.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m2.1d">Y</annotation></semantics></math> (right) for each aggregation point.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.6" class="ltx_p">In Figures <a href="#S2.F1" title="Figure 1 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S2.F2" title="Figure 2 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> mutual information was measured in the setup with iid datasets.
After approximately <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.p3.1.m1.1a"><mn id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><cn type="integer" id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">50</annotation></semantics></math> aggregations without back-propagating the aggregated model, the information in the averaged model falls drastically. While each of the nodes eventually achieves more than <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mn id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">75</mn><mo id="S2.p3.2.m2.1.1.1" xref="S2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="latexml" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">75\%</annotation></semantics></math> training accuracy on the corresponding local dataset, the final averaged model has only <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="36\%" display="inline"><semantics id="S2.p3.3.m3.1a"><mrow id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml"><mn id="S2.p3.3.m3.1.1.2" xref="S2.p3.3.m3.1.1.2.cmml">36</mn><mo id="S2.p3.3.m3.1.1.1" xref="S2.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"><csymbol cd="latexml" id="S2.p3.3.m3.1.1.1.cmml" xref="S2.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.2">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">36\%</annotation></semantics></math> of accuracy on each of the local datasets.
Since Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that local models fail to be successfully averaged after the <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.p3.4.m4.1a"><mn id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><cn type="integer" id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">10</annotation></semantics></math>-th aggregation, we perform an experiment with periodic synchronization having the period of synchronization set exactly to <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="10*100=1000" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mrow id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml"><mn id="S2.p3.5.m5.1.1.2.2" xref="S2.p3.5.m5.1.1.2.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S2.p3.5.m5.1.1.2.1" xref="S2.p3.5.m5.1.1.2.1.cmml">∗</mo><mn id="S2.p3.5.m5.1.1.2.3" xref="S2.p3.5.m5.1.1.2.3.cmml">100</mn></mrow><mo id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">=</mo><mn id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><eq id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1"></eq><apply id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2"><times id="S2.p3.5.m5.1.1.2.1.cmml" xref="S2.p3.5.m5.1.1.2.1"></times><cn type="integer" id="S2.p3.5.m5.1.1.2.2.cmml" xref="S2.p3.5.m5.1.1.2.2">10</cn><cn type="integer" id="S2.p3.5.m5.1.1.2.3.cmml" xref="S2.p3.5.m5.1.1.2.3">100</cn></apply><cn type="integer" id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">10*100=1000</annotation></semantics></math> local batches for training. The resulting measurements in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show that in this case mutual information with input behaves in approximately the same way, while mutual information with label does not drop anymore and even grows significantly. This also results in higher training accuracy of the averaged model – more than <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="73\%" display="inline"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mn id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">73</mn><mo id="S2.p3.6.m6.1.1.1" xref="S2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><csymbol cd="latexml" id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">73</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">73\%</annotation></semantics></math> on each of the local datasets in the end of the training.
Interesting to note, that such a big period of synchronization is usually not used in the federated setup, nevertheless the experiments show that it can be beneficial. Thus, an analysis of information flow in the averaged model can help to identify a period for synchronization that is large enough and thus saving communication costs, but at the same moment allows for successful distributed training.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/noniid/nosync/MI_input_paper.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/noniid/nosync/MI_label_paper.png" id="S2.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Mutual information measured for the non-iid case without sending back the aggregated model. The estimated amount of information is shown for input <math id="S2.F3.5.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.F3.5.m1.1b"><mi id="S2.F3.5.m1.1.1" xref="S2.F3.5.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.F3.5.m1.1c"><ci id="S2.F3.5.m1.1.1.cmml" xref="S2.F3.5.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.5.m1.1d">X</annotation></semantics></math> (left) and label <math id="S2.F3.6.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.F3.6.m2.1b"><mi id="S2.F3.6.m2.1.1" xref="S2.F3.6.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.F3.6.m2.1c"><ci id="S2.F3.6.m2.1.1.cmml" xref="S2.F3.6.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.6.m2.1d">Y</annotation></semantics></math> (right) for each aggregation point.</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/noniid/sync/MI_input_paper.png" id="S2.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F4.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/1911.07652/assets/images/noniid/sync/MI_label_paper.png" id="S2.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="281" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mutual information measured for the non-iid case with aggregation process and continuing learning from the averaged point. The estimated amount of information is shown for input <math id="S2.F4.5.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.F4.5.m1.1b"><mi id="S2.F4.5.m1.1.1" xref="S2.F4.5.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.F4.5.m1.1c"><ci id="S2.F4.5.m1.1.1.cmml" xref="S2.F4.5.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.5.m1.1d">X</annotation></semantics></math> (left) and label <math id="S2.F4.6.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.F4.6.m2.1b"><mi id="S2.F4.6.m2.1.1" xref="S2.F4.6.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.F4.6.m2.1c"><ci id="S2.F4.6.m2.1.1.cmml" xref="S2.F4.6.m2.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.6.m2.1d">Y</annotation></semantics></math> (right) for each aggregation point.</figcaption>
</figure>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.3" class="ltx_p">We now repeat the experiments with non-iid local datasets. While with iid datasets the averaged model ceases to be able to combine information about local labels when the local models become more fit, with non-iid datasets this is happening almost instantly. In Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> on the right the mutual information of the representation and the label estimated for the averaged model decreases from the very beginning of the training process and goes down to <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.p4.1.m1.1a"><mn id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><cn type="integer" id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1">0</cn></annotation-xml></semantics></math>.
Figure <a href="#S2.F4" title="Figure 4 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the development of mutual information for non-iid datasets and periodic synchronization. Here we have chosen the period of synchronization <math id="S2.p4.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S2.p4.2.m2.1a"><mn id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><cn type="integer" id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">100</annotation></semantics></math> local batches that corresponds to the very first aggregation in Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We do not observe anymore the drop of the mutual information with the labels in the averaged network as in Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Experiments ‣ Information-Theoretic Perspective of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Moreover, the mutual information in the averaged model with each of the local datasets stays close to the mutual information estimated for the local nodes. Interestingly enough, it seems that information propagated through averaging from the second node to the first one hinders it from training better. But overall here the final averaged model achieves considerably high accuracy – approximately <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S2.p4.3.m3.1a"><mrow id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml"><mn id="S2.p4.3.m3.1.1.2" xref="S2.p4.3.m3.1.1.2.cmml">60</mn><mo id="S2.p4.3.m3.1.1.1" xref="S2.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><apply id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1"><csymbol cd="latexml" id="S2.p4.3.m3.1.1.1.cmml" xref="S2.p4.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S2.p4.3.m3.1.1.2.cmml" xref="S2.p4.3.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">60\%</annotation></semantics></math> on each of the local datasets.
The analysis of the non-iid case indicates that the very frequent aggregation can be beneficial, but can also deteriorate the local training process.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">An inspection of the mutual information with the representation of the aggregated model helps to identify a beneficial synchronization period in terms of training quality and communication costs and also sheds light on the dynamics of the training process with non-iid datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Future Work</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">The paper leaves as an open question the theoretical analysis of mutual information development, that can be used to answer the questions of when and how to aggregate local models. The first aspect is closely linked to achieving communication benefits in the sense of reduced communication costs due to an improved timing of aggregation in the federated setup. Further examining how different aggregation methods behave in terms of the development of mutual information can help answering the question of how to optimally aggregate the local models to improve the quality of the resulting global model.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Acknowledgments</h3>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">This research has been partly funded by the German Federal Ministry of Education and Research, ML2R - Förderkennzeichen 01S18038B.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamp et al. [2014]</span>
<span class="ltx_bibblock">
M. Kamp, M. Boley, D. Keren, A. Schuster, and I. Sharfman.

</span>
<span class="ltx_bibblock">Communication-efficient distributed online prediction by dynamic
model synchronization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Discovery in Databases</em>,
pages 623–639, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamp et al. [2018]</span>
<span class="ltx_bibblock">
M. Kamp, L. Adilova, J. Sicking, F. Hüger, P. Schlicht, T. Wirtz, and
S. Wrobel.

</span>
<span class="ltx_bibblock">Efficient decentralized deep learning by dynamic model averaging.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and Knowledge
Discovery in Databases</em>, pages 393–409. Springer, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keskar et al. [2016]</span>
<span class="ltx_bibblock">
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang.

</span>
<span class="ltx_bibblock">On large-batch training for deep learning: Generalization gap and
sharp minima.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2014]</span>
<span class="ltx_bibblock">
A. Krizhevsky, V. Nair, and G. Hinton.

</span>
<span class="ltx_bibblock">The cifar-10 dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">online: http://www. cs. toronto. edu/kriz/cifar. html</em>, 55,
2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al. [1998]</span>
<span class="ltx_bibblock">
Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, et al.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, 86(11):2278–2324, 1998.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, pages 1273–1282,
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Hein [2017]</span>
<span class="ltx_bibblock">
Q. Nguyen and M. Hein.

</span>
<span class="ltx_bibblock">The loss surface of deep and wide neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em>, pages 2603–2612. JMLR. org, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noshad et al. [2019]</span>
<span class="ltx_bibblock">
M. Noshad, Y. Zeng, and A. O. Hero.

</span>
<span class="ltx_bibblock">Scalable mutual information estimation using dependence graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 2962–2966. IEEE, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxe et al. [2018]</span>
<span class="ltx_bibblock">
A. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. Tracey, and
D. Cox.

</span>
<span class="ltx_bibblock">On the information bottleneck theory of deep learning.

</span>
<span class="ltx_bibblock">02 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shwartz-Ziv and Tishby [2017]</span>
<span class="ltx_bibblock">
R. Shwartz-Ziv and N. Tishby.

</span>
<span class="ltx_bibblock">Opening the black box of deep neural networks via information.

</span>
<span class="ltx_bibblock">03 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tishby and Zaslavsky [2015]</span>
<span class="ltx_bibblock">
N. Tishby and N. Zaslavsky.

</span>
<span class="ltx_bibblock">Deep learning and the information bottleneck principle.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Information Theory Workshop (ITW)</em>, pages 1–5.
IEEE, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1911.07651" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1911.07652" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1911.07652">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1911.07652" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1911.07653" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 12:49:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
