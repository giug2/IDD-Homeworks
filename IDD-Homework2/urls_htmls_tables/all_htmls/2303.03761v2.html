<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.03761] Graph Neural Networks in Vision-Language Image Understanding: A Survey</title><meta property="og:description" content="2D image understanding is a complex problem within Computer Vision, but it holds the key to providing human level scene comprehension. It goes further than identifying the objects in an image, and instead it attempts t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Graph Neural Networks in Vision-Language Image Understanding: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Graph Neural Networks in Vision-Language Image Understanding: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.03761">

<!--Generated on Thu Feb 29 21:30:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Graph Neural Networks,  Image Captioning,  Visual Question Answering,  Image Retrieval
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Graph Neural Networks in Vision-Language Image Understanding: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henry Senior, 
Gregory Slabaugh, 
Shanxin Yuan, 
and Luca Rossi
</span><span class="ltx_author_notes">H. Senior, G. Slabaugh, and S. Yuan are with the Digital Environment Research Institute, Queen Mary University of London, United Kingdon. e-mail: h.senior@qmul.ac.ukL. Rossi is with The Hong Kong Polytechnic University, Hong Kong.Manuscript received April 19, 2005; revised August 26, 2015.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">2D image understanding is a complex problem within Computer Vision, but it holds the key to providing human level scene comprehension. It goes further than identifying the objects in an image, and instead it attempts to <span id="id1.id1.1" class="ltx_text ltx_font_italic">understand</span> the scene. Solutions to this problem form the underpinning of a range of tasks, including image captioning, Visual Question Answering (VQA), and image retrieval. Graphs provide a natural way to represent the relational arrangement between objects in an image, and thus in recent years Graph Neural Networks (GNNs) have become a standard component of many 2D image understanding pipelines, becoming a core architectural component especially in the VQA group of tasks. In this survey, we review this rapidly evolving field and we provide a taxonomy of graph types used in 2D image understanding approaches, a comprehensive list of the GNN models used in this domain, and a roadmap of future potential developments. To the best of our knowledge, this is the first comprehensive survey that covers image captioning, visual question answering, and image retrieval techniques that focus on using GNNs as the main part of their architecture.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Graph Neural Networks, Image Captioning, Visual Question Answering, Image Retrieval

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent years have seen an explosion of research into Graph Neural Networks (GNNs), with a flurry of new architectures being presented in top-tier machine learning conferences and journals every year <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The ability of GNNs to learn in non-Euclidean domains makes them powerful tools to analyse data where structure plays an important role, from chemoinformatics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to network analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Indeed, these models can also be applied to problems not traditionally associated with graphs such as 3D object detection in LiDAR point clouds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and shape analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">GNN-based approaches have gained increasing popularity for solving 2D image understanding vision-language tasks, similar to other domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Whilst advances in this domain are discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, it is a wide ranging survey. Our work focuses specifically on vision-language and therefore covers these topics more extensively.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We view 2D image understanding as the high level challenge of making a computer understand a two-dimensional image to a level equal to or greater than a human. Models that enable this should be able to reason about the image in order to describe it (image captioning), explain aspects of it (Visual Question Answering (VQA), or find similar images (image retrieval). These are all tasks that humans can do with relative ease, however, they are incredibly difficult for deep learning models and require a large amount of data. These tasks also fall under the category of vision-language problems, as they require the model to have an understanding of both the image pixels and a language (typically English) in which the models can express their understanding. Whilst there is a plethora of techniques that have been applied to these problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, this survey focuses on graph-based approaches. There are a range of graphs that are applicable, but the most widely used and understood is the semantic scene graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. This graph is constructed of nodes representing visual objects and edges representing the semantic relationships between them. The semantic graph as well as further graph types are discussed in Section <a href="#S2.SS3" title="II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Alongside a taxonomy of the graph types used across 2D image understanding tasks, this paper contributes a much needed overview of these approaches. Covering the three main tasks, we also include an overview of popular GNN techniques as well as insights on the direction of future GNN work. In the discussion section of this paper we argue that the increasingly popular Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is actually a special case GNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We expand upon this argument to suggest that GNNs should not be overlooked as they may offer better inductive biases for a range of tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our main contributions are: 1) a taxonomy of the graph types used in 2D image understanding tasks; 2) a comprehensive survey of GNN-based approaches to common 2D image understanding tasks; 3) a roadmap of potential future developments for the community to explore.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The remainder of this paper is organised as follows: Section <a href="#S2" title="II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> gives a taxonomy of the tasks discussed and their corresponding datasets, as well as an overview of the different graph types used throughout. Section <a href="#S3" title="III An Overview of Graph Neural Networks ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> gives an overview of the common GNN architectures used. It also briefly mentions current and future research directions for GNNs and signposts appropriate surveys. The main body of the paper is formed of Sections <a href="#S4" title="IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, <a href="#S5" title="V Visual Question Answering ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, and <a href="#S6" title="VI Image Retrieval ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, which detail GNN-based approaches to image captioning, VQA, and image retrieval, respectively. We then conclude the paper with a three part discussion, with Section <a href="#S7.SS1" title="VII-A Why GNNs When We Have Transformers? ‣ VII Discussion and Conclusion ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-A</span></span></a> covering the advantages that GNNs still offer despite the rapid adoption of the Transformer architecture. This is followed by Section <a href="#S7.SS2" title="VII-B Latent Diffusion and the Future of Image Captioning ‣ VII Discussion and Conclusion ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-B</span></span></a> which links the emerging field of latent diffusion and image generation to image captioning. Finally, Section <a href="#S7.SS3" title="VII-C Final Notes ‣ VII Discussion and Conclusion ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VII-C</span></span></a> concludes the paper and provides potential directions for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Definitions</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we outline the background required to view this survey in context. We first briefly define a generic graph before outlining the taxonomy of the field. Finally, we give an overview of the various graph types.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">2D vision-language Tasks Taxonomies</span>
</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2303.03761/assets/figures/2d-scene-understanding-taxonomy.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="936" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>2D vision-language task taxonomy.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This paper follows the taxonomies of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and joins them together for a more complete overview of 2D vision-language tasks (see Figure <a href="#S2.F1" title="Figure 1 ‣ II-A 2D vision-language Tasks Taxonomies ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This section gives a brief overview of the existing taxonomies and highlights the sections of them this survey focuses on. It also highlights the main datasets used for various tasks discussed in the paper, these are summarised in Table <a href="#S2.T1" title="TABLE I ‣ II-A 2D vision-language Tasks Taxonomies ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>A table showing a summary of common datasets</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:100.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.4pt,31.1pt) scale(0.615584132234441,0.615584132234441) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Dataset</th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Main Task</th>
<th id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Features</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S2.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Mutli-Task</td>
<td id="S2.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">108,000 images; 5.4 million region descriptions; 1.7 million question-answer pairs; scene graphs</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S2.T1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Image Captioning</td>
<td id="S2.T1.1.1.3.2.3" class="ltx_td ltx_align_left">330,000 images with 5 human generated reference captions for training and validation sets</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">Flickr30K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T1.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Image Captioning</td>
<td id="S2.T1.1.1.4.3.3" class="ltx_td ltx_align_left">31,000 images each with 5 human generated reference captions</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S2.T1.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">VQA</td>
<td id="S2.T1.1.1.5.4.3" class="ltx_td ltx_align_left">265,000 images; Average of 5.4 questions per image each with 10 ground truth answers</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S2.T1.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">VQA</td>
<td id="S2.T1.1.1.6.5.3" class="ltx_td ltx_align_left">2,190 images; 5,826 questions; Knowledge base of 4,216 facts</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">OKVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S2.T1.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">VQA</td>
<td id="S2.T1.1.1.7.6.3" class="ltx_td ltx_align_left">14,000 questions, each with five ground truth answers, with knowledge extracted from wikipedia</td>
</tr>
<tr id="S2.T1.1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r">TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S2.T1.1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r">VQA</td>
<td id="S2.T1.1.1.8.7.3" class="ltx_td ltx_align_left">28,000 images; 45,000 questions each with 10 ground truth answers</td>
</tr>
<tr id="S2.T1.1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r">Text-KVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S2.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">VQA</td>
<td id="S2.T1.1.1.9.8.3" class="ltx_td ltx_align_left">257,000 images; 1.3 million QA pairs; Inclusion of a knowledge base</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">Whilst individual vision-language tasks have their own unique datasets, they are unified by the Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, an expansive dataset that provides ground truths for a range of vision-language tasks. As the most generic dataset, it has <math id="S2.SS1.p2.1.m1.2" class="ltx_Math" alttext="33,877" display="inline"><semantics id="S2.SS1.p2.1.m1.2a"><mrow id="S2.SS1.p2.1.m1.2.3.2" xref="S2.SS1.p2.1.m1.2.3.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">33</mn><mo id="S2.SS1.p2.1.m1.2.3.2.1" xref="S2.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml">877</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.2b"><list id="S2.SS1.p2.1.m1.2.3.1.cmml" xref="S2.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">33</cn><cn type="integer" id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2">877</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.2c">33,877</annotation></semantics></math> object categories and <math id="S2.SS1.p2.2.m2.2" class="ltx_Math" alttext="68,111" display="inline"><semantics id="S2.SS1.p2.2.m2.2a"><mrow id="S2.SS1.p2.2.m2.2.3.2" xref="S2.SS1.p2.2.m2.2.3.1.cmml"><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">68</mn><mo id="S2.SS1.p2.2.m2.2.3.2.1" xref="S2.SS1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS1.p2.2.m2.2.2" xref="S2.SS1.p2.2.m2.2.2.cmml">111</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.2b"><list id="S2.SS1.p2.2.m2.2.3.1.cmml" xref="S2.SS1.p2.2.m2.2.3.2"><cn type="integer" id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">68</cn><cn type="integer" id="S2.SS1.p2.2.m2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2">111</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.2c">68,111</annotation></semantics></math> attribute categories. At the time of its publication this was the largest and most dense dataset containing image descriptions, objects, attributes, relationships, and question answer pairs. Additionally, the Visual Genome also contains region graphs, scene graphs, and question-answer pairs. This results in it being a very wide ranging dataset with lots of applications in visual cognition tasks such as scene graph generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">For image captioning, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> who identify three main approaches: 1) retrieval-based captioning, 2) template-based captioning, and 3) deep learning-based captioning. Retrieval-based captioning is built on the assumption that for every image, a caption exists, and needs to be retrieved from a bank of existing captions. It was the foundation of early image captioning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and yielded good results without the need for deep learning. However, not all images may have appropriate captions. If the captions are generic, they will only be able to describe aspects of an image and may omit its most important feature. In contrast, template-based captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> uses a pre-defined caption format and uses object detection to fill in the blanks. This approach is good for generating consistent captions, but can result in captions that are unnatural and clearly generated by a machine. Contemporary approaches to the task of image captioning are based on deep learning models. Early work focused on a CNN encoder feeding an RNN-based decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, however more recent deep learning approaches have developed to incorporate a wide variety of techniques including GNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. In this survey, we focus specifically on deep learning approaches to image captioning, and focus on graph-based approaches. Deep learning approaches are typically trained on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> or Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> which contain a set of images accompanied by five human generated captions.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Taxonomies of VQA are usually defined through the lens of the datasets used by the various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Here we focus on 1) the standard VQA task of answering a question about an image, 2) the fact-based VQA (FVQA) task of answering questions that require external knowledge to answer, and 3) text-VQA, the task of answering questions that require the model to read text in the scene and combine it with visual data. Each of the various VQA tasks have their own set of speicalised datasets. The original VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and the subsequently updated VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> dataset address the original task of answering questions based on the visual information in the image. The FVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is built using images from ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> alongside facts from DBPedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and WebChild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. The images have three forms of visual concepts extracted from them using a range of models. These visual concepts include objects (items identified in the image), scene (scene level features such as room label), and actions. Question-answer pairs were generated by human annotators who selected a visual concept and an accompanying fact triplet which they used to generate a question. Finally, the text-KVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> was built by compiling images from a Kaggle movie poster challenge<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.kaggle.com/datasets/neha1703/movie-genre-from-its-poster</span></span></span>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, and Google Image search results from combining brand names with postfixes such as “store” or “building.” This collection of images was then given to human annotators who removed images that did not contain text of brand names. The result is a dataset of 257K images with three groupings: book, movie, and scene. Accompanying these images are 1.3 million question-answer pairs. Each image grouping gets its own triplet-based knowledge base from a relevant source: WikiData <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, IMBd, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> respectively.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Image retrieval spans multiple tasks, all of which make use of deep learning in contemporary approaches. We follow the taxonomy of Alexander <span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and address the following sub tasks: text-based image retrieval, content-based image retrieval, sketch-based retrieval, semantic-based retrieval, and annotation-based retrieval. The number of datasets used for image retrieval are vast and the community has not solidified around a single dataset in the way image captioning has around COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. This presents a challenge when making accurate comparisons between systems as the challenge presented by different datasets varies complicating direct comparisons across datasets. Whilst image retrieval specific datasets exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, there are papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> that make use of image captioning datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, showing the wide range of varied datasets that exist for image retrieval.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Fundamental Graph Theoretical Concepts</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.11" class="ltx_p"><span id="S2.SS2.p1.11.1" class="ltx_text ltx_font_bold">Undirected Graph.</span> We define an undirected graph <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">G</annotation></semantics></math> to be a tuple of sets <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">V</annotation></semantics></math> and <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">E</annotation></semantics></math>, i.e., <math id="S2.SS2.p1.4.m4.2" class="ltx_Math" alttext="G=(V,E)" display="inline"><semantics id="S2.SS2.p1.4.m4.2a"><mrow id="S2.SS2.p1.4.m4.2.3" xref="S2.SS2.p1.4.m4.2.3.cmml"><mi id="S2.SS2.p1.4.m4.2.3.2" xref="S2.SS2.p1.4.m4.2.3.2.cmml">G</mi><mo id="S2.SS2.p1.4.m4.2.3.1" xref="S2.SS2.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.SS2.p1.4.m4.2.3.3.2" xref="S2.SS2.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.p1.4.m4.2.3.3.2.1" xref="S2.SS2.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">V</mi><mo id="S2.SS2.p1.4.m4.2.3.3.2.2" xref="S2.SS2.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS2.p1.4.m4.2.2" xref="S2.SS2.p1.4.m4.2.2.cmml">E</mi><mo stretchy="false" id="S2.SS2.p1.4.m4.2.3.3.2.3" xref="S2.SS2.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.2b"><apply id="S2.SS2.p1.4.m4.2.3.cmml" xref="S2.SS2.p1.4.m4.2.3"><eq id="S2.SS2.p1.4.m4.2.3.1.cmml" xref="S2.SS2.p1.4.m4.2.3.1"></eq><ci id="S2.SS2.p1.4.m4.2.3.2.cmml" xref="S2.SS2.p1.4.m4.2.3.2">𝐺</ci><interval closure="open" id="S2.SS2.p1.4.m4.2.3.3.1.cmml" xref="S2.SS2.p1.4.m4.2.3.3.2"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">𝑉</ci><ci id="S2.SS2.p1.4.m4.2.2.cmml" xref="S2.SS2.p1.4.m4.2.2">𝐸</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.2c">G=(V,E)</annotation></semantics></math>. The set <math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><mi id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">V</annotation></semantics></math> contains <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><mi id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">n</annotation></semantics></math> vertices (sometimes referred to as nodes) that are connected by the edges in the set <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">E</annotation></semantics></math>, i.e., if <math id="S2.SS2.p1.8.m8.1" class="ltx_Math" alttext="v\in V" display="inline"><semantics id="S2.SS2.p1.8.m8.1a"><mrow id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml"><mi id="S2.SS2.p1.8.m8.1.1.2" xref="S2.SS2.p1.8.m8.1.1.2.cmml">v</mi><mo id="S2.SS2.p1.8.m8.1.1.1" xref="S2.SS2.p1.8.m8.1.1.1.cmml">∈</mo><mi id="S2.SS2.p1.8.m8.1.1.3" xref="S2.SS2.p1.8.m8.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><apply id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1"><in id="S2.SS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1.1"></in><ci id="S2.SS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2">𝑣</ci><ci id="S2.SS2.p1.8.m8.1.1.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">v\in V</annotation></semantics></math> and <math id="S2.SS2.p1.9.m9.1" class="ltx_Math" alttext="u\in V" display="inline"><semantics id="S2.SS2.p1.9.m9.1a"><mrow id="S2.SS2.p1.9.m9.1.1" xref="S2.SS2.p1.9.m9.1.1.cmml"><mi id="S2.SS2.p1.9.m9.1.1.2" xref="S2.SS2.p1.9.m9.1.1.2.cmml">u</mi><mo id="S2.SS2.p1.9.m9.1.1.1" xref="S2.SS2.p1.9.m9.1.1.1.cmml">∈</mo><mi id="S2.SS2.p1.9.m9.1.1.3" xref="S2.SS2.p1.9.m9.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.1b"><apply id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1"><in id="S2.SS2.p1.9.m9.1.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1.1"></in><ci id="S2.SS2.p1.9.m9.1.1.2.cmml" xref="S2.SS2.p1.9.m9.1.1.2">𝑢</ci><ci id="S2.SS2.p1.9.m9.1.1.3.cmml" xref="S2.SS2.p1.9.m9.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.1c">u\in V</annotation></semantics></math> are connected by an edge then <math id="S2.SS2.p1.10.m10.2" class="ltx_Math" alttext="e_{v,u}\in E" display="inline"><semantics id="S2.SS2.p1.10.m10.2a"><mrow id="S2.SS2.p1.10.m10.2.3" xref="S2.SS2.p1.10.m10.2.3.cmml"><msub id="S2.SS2.p1.10.m10.2.3.2" xref="S2.SS2.p1.10.m10.2.3.2.cmml"><mi id="S2.SS2.p1.10.m10.2.3.2.2" xref="S2.SS2.p1.10.m10.2.3.2.2.cmml">e</mi><mrow id="S2.SS2.p1.10.m10.2.2.2.4" xref="S2.SS2.p1.10.m10.2.2.2.3.cmml"><mi id="S2.SS2.p1.10.m10.1.1.1.1" xref="S2.SS2.p1.10.m10.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p1.10.m10.2.2.2.4.1" xref="S2.SS2.p1.10.m10.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p1.10.m10.2.2.2.2" xref="S2.SS2.p1.10.m10.2.2.2.2.cmml">u</mi></mrow></msub><mo id="S2.SS2.p1.10.m10.2.3.1" xref="S2.SS2.p1.10.m10.2.3.1.cmml">∈</mo><mi id="S2.SS2.p1.10.m10.2.3.3" xref="S2.SS2.p1.10.m10.2.3.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m10.2b"><apply id="S2.SS2.p1.10.m10.2.3.cmml" xref="S2.SS2.p1.10.m10.2.3"><in id="S2.SS2.p1.10.m10.2.3.1.cmml" xref="S2.SS2.p1.10.m10.2.3.1"></in><apply id="S2.SS2.p1.10.m10.2.3.2.cmml" xref="S2.SS2.p1.10.m10.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m10.2.3.2.1.cmml" xref="S2.SS2.p1.10.m10.2.3.2">subscript</csymbol><ci id="S2.SS2.p1.10.m10.2.3.2.2.cmml" xref="S2.SS2.p1.10.m10.2.3.2.2">𝑒</ci><list id="S2.SS2.p1.10.m10.2.2.2.3.cmml" xref="S2.SS2.p1.10.m10.2.2.2.4"><ci id="S2.SS2.p1.10.m10.1.1.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1.1.1">𝑣</ci><ci id="S2.SS2.p1.10.m10.2.2.2.2.cmml" xref="S2.SS2.p1.10.m10.2.2.2.2">𝑢</ci></list></apply><ci id="S2.SS2.p1.10.m10.2.3.3.cmml" xref="S2.SS2.p1.10.m10.2.3.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m10.2c">e_{v,u}\in E</annotation></semantics></math>. For an undirected graph we have that <math id="S2.SS2.p1.11.m11.4" class="ltx_Math" alttext="e_{v,u}=e_{u,v}" display="inline"><semantics id="S2.SS2.p1.11.m11.4a"><mrow id="S2.SS2.p1.11.m11.4.5" xref="S2.SS2.p1.11.m11.4.5.cmml"><msub id="S2.SS2.p1.11.m11.4.5.2" xref="S2.SS2.p1.11.m11.4.5.2.cmml"><mi id="S2.SS2.p1.11.m11.4.5.2.2" xref="S2.SS2.p1.11.m11.4.5.2.2.cmml">e</mi><mrow id="S2.SS2.p1.11.m11.2.2.2.4" xref="S2.SS2.p1.11.m11.2.2.2.3.cmml"><mi id="S2.SS2.p1.11.m11.1.1.1.1" xref="S2.SS2.p1.11.m11.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p1.11.m11.2.2.2.4.1" xref="S2.SS2.p1.11.m11.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p1.11.m11.2.2.2.2" xref="S2.SS2.p1.11.m11.2.2.2.2.cmml">u</mi></mrow></msub><mo id="S2.SS2.p1.11.m11.4.5.1" xref="S2.SS2.p1.11.m11.4.5.1.cmml">=</mo><msub id="S2.SS2.p1.11.m11.4.5.3" xref="S2.SS2.p1.11.m11.4.5.3.cmml"><mi id="S2.SS2.p1.11.m11.4.5.3.2" xref="S2.SS2.p1.11.m11.4.5.3.2.cmml">e</mi><mrow id="S2.SS2.p1.11.m11.4.4.2.4" xref="S2.SS2.p1.11.m11.4.4.2.3.cmml"><mi id="S2.SS2.p1.11.m11.3.3.1.1" xref="S2.SS2.p1.11.m11.3.3.1.1.cmml">u</mi><mo id="S2.SS2.p1.11.m11.4.4.2.4.1" xref="S2.SS2.p1.11.m11.4.4.2.3.cmml">,</mo><mi id="S2.SS2.p1.11.m11.4.4.2.2" xref="S2.SS2.p1.11.m11.4.4.2.2.cmml">v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.11.m11.4b"><apply id="S2.SS2.p1.11.m11.4.5.cmml" xref="S2.SS2.p1.11.m11.4.5"><eq id="S2.SS2.p1.11.m11.4.5.1.cmml" xref="S2.SS2.p1.11.m11.4.5.1"></eq><apply id="S2.SS2.p1.11.m11.4.5.2.cmml" xref="S2.SS2.p1.11.m11.4.5.2"><csymbol cd="ambiguous" id="S2.SS2.p1.11.m11.4.5.2.1.cmml" xref="S2.SS2.p1.11.m11.4.5.2">subscript</csymbol><ci id="S2.SS2.p1.11.m11.4.5.2.2.cmml" xref="S2.SS2.p1.11.m11.4.5.2.2">𝑒</ci><list id="S2.SS2.p1.11.m11.2.2.2.3.cmml" xref="S2.SS2.p1.11.m11.2.2.2.4"><ci id="S2.SS2.p1.11.m11.1.1.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1.1.1">𝑣</ci><ci id="S2.SS2.p1.11.m11.2.2.2.2.cmml" xref="S2.SS2.p1.11.m11.2.2.2.2">𝑢</ci></list></apply><apply id="S2.SS2.p1.11.m11.4.5.3.cmml" xref="S2.SS2.p1.11.m11.4.5.3"><csymbol cd="ambiguous" id="S2.SS2.p1.11.m11.4.5.3.1.cmml" xref="S2.SS2.p1.11.m11.4.5.3">subscript</csymbol><ci id="S2.SS2.p1.11.m11.4.5.3.2.cmml" xref="S2.SS2.p1.11.m11.4.5.3.2">𝑒</ci><list id="S2.SS2.p1.11.m11.4.4.2.3.cmml" xref="S2.SS2.p1.11.m11.4.4.2.4"><ci id="S2.SS2.p1.11.m11.3.3.1.1.cmml" xref="S2.SS2.p1.11.m11.3.3.1.1">𝑢</ci><ci id="S2.SS2.p1.11.m11.4.4.2.2.cmml" xref="S2.SS2.p1.11.m11.4.4.2.2">𝑣</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.11.m11.4c">e_{v,u}=e_{u,v}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.11" class="ltx_p"><span id="S2.SS2.p2.11.1" class="ltx_text ltx_font_bold">Directed Graph.</span> A directed graph is a graph where the existence of <math id="S2.SS2.p2.1.m1.2" class="ltx_Math" alttext="e_{v,u}" display="inline"><semantics id="S2.SS2.p2.1.m1.2a"><msub id="S2.SS2.p2.1.m1.2.3" xref="S2.SS2.p2.1.m1.2.3.cmml"><mi id="S2.SS2.p2.1.m1.2.3.2" xref="S2.SS2.p2.1.m1.2.3.2.cmml">e</mi><mrow id="S2.SS2.p2.1.m1.2.2.2.4" xref="S2.SS2.p2.1.m1.2.2.2.3.cmml"><mi id="S2.SS2.p2.1.m1.1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p2.1.m1.2.2.2.4.1" xref="S2.SS2.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p2.1.m1.2.2.2.2" xref="S2.SS2.p2.1.m1.2.2.2.2.cmml">u</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.2b"><apply id="S2.SS2.p2.1.m1.2.3.cmml" xref="S2.SS2.p2.1.m1.2.3"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.2.3.1.cmml" xref="S2.SS2.p2.1.m1.2.3">subscript</csymbol><ci id="S2.SS2.p2.1.m1.2.3.2.cmml" xref="S2.SS2.p2.1.m1.2.3.2">𝑒</ci><list id="S2.SS2.p2.1.m1.2.2.2.3.cmml" xref="S2.SS2.p2.1.m1.2.2.2.4"><ci id="S2.SS2.p2.1.m1.1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1.1">𝑣</ci><ci id="S2.SS2.p2.1.m1.2.2.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2.2.2">𝑢</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.2c">e_{v,u}</annotation></semantics></math> does not imply the existence of <math id="S2.SS2.p2.2.m2.2" class="ltx_Math" alttext="e_{u,v}" display="inline"><semantics id="S2.SS2.p2.2.m2.2a"><msub id="S2.SS2.p2.2.m2.2.3" xref="S2.SS2.p2.2.m2.2.3.cmml"><mi id="S2.SS2.p2.2.m2.2.3.2" xref="S2.SS2.p2.2.m2.2.3.2.cmml">e</mi><mrow id="S2.SS2.p2.2.m2.2.2.2.4" xref="S2.SS2.p2.2.m2.2.2.2.3.cmml"><mi id="S2.SS2.p2.2.m2.1.1.1.1" xref="S2.SS2.p2.2.m2.1.1.1.1.cmml">u</mi><mo id="S2.SS2.p2.2.m2.2.2.2.4.1" xref="S2.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p2.2.m2.2.2.2.2" xref="S2.SS2.p2.2.m2.2.2.2.2.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.2b"><apply id="S2.SS2.p2.2.m2.2.3.cmml" xref="S2.SS2.p2.2.m2.2.3"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.2.3.1.cmml" xref="S2.SS2.p2.2.m2.2.3">subscript</csymbol><ci id="S2.SS2.p2.2.m2.2.3.2.cmml" xref="S2.SS2.p2.2.m2.2.3.2">𝑒</ci><list id="S2.SS2.p2.2.m2.2.2.2.3.cmml" xref="S2.SS2.p2.2.m2.2.2.2.4"><ci id="S2.SS2.p2.2.m2.1.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1.1.1">𝑢</ci><ci id="S2.SS2.p2.2.m2.2.2.2.2.cmml" xref="S2.SS2.p2.2.m2.2.2.2.2">𝑣</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.2c">e_{u,v}</annotation></semantics></math> as well. Let <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">A</annotation></semantics></math> be the <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="n\times n" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mrow id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml"><mi id="S2.SS2.p2.4.m4.1.1.2" xref="S2.SS2.p2.4.m4.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p2.4.m4.1.1.1" xref="S2.SS2.p2.4.m4.1.1.1.cmml">×</mo><mi id="S2.SS2.p2.4.m4.1.1.3" xref="S2.SS2.p2.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><apply id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1"><times id="S2.SS2.p2.4.m4.1.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1.1"></times><ci id="S2.SS2.p2.4.m4.1.1.2.cmml" xref="S2.SS2.p2.4.m4.1.1.2">𝑛</ci><ci id="S2.SS2.p2.4.m4.1.1.3.cmml" xref="S2.SS2.p2.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">n\times n</annotation></semantics></math> binary adjacency matrix such that <math id="S2.SS2.p2.5.m5.2" class="ltx_Math" alttext="A_{v,u}=1" display="inline"><semantics id="S2.SS2.p2.5.m5.2a"><mrow id="S2.SS2.p2.5.m5.2.3" xref="S2.SS2.p2.5.m5.2.3.cmml"><msub id="S2.SS2.p2.5.m5.2.3.2" xref="S2.SS2.p2.5.m5.2.3.2.cmml"><mi id="S2.SS2.p2.5.m5.2.3.2.2" xref="S2.SS2.p2.5.m5.2.3.2.2.cmml">A</mi><mrow id="S2.SS2.p2.5.m5.2.2.2.4" xref="S2.SS2.p2.5.m5.2.2.2.3.cmml"><mi id="S2.SS2.p2.5.m5.1.1.1.1" xref="S2.SS2.p2.5.m5.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p2.5.m5.2.2.2.4.1" xref="S2.SS2.p2.5.m5.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p2.5.m5.2.2.2.2" xref="S2.SS2.p2.5.m5.2.2.2.2.cmml">u</mi></mrow></msub><mo id="S2.SS2.p2.5.m5.2.3.1" xref="S2.SS2.p2.5.m5.2.3.1.cmml">=</mo><mn id="S2.SS2.p2.5.m5.2.3.3" xref="S2.SS2.p2.5.m5.2.3.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.2b"><apply id="S2.SS2.p2.5.m5.2.3.cmml" xref="S2.SS2.p2.5.m5.2.3"><eq id="S2.SS2.p2.5.m5.2.3.1.cmml" xref="S2.SS2.p2.5.m5.2.3.1"></eq><apply id="S2.SS2.p2.5.m5.2.3.2.cmml" xref="S2.SS2.p2.5.m5.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m5.2.3.2.1.cmml" xref="S2.SS2.p2.5.m5.2.3.2">subscript</csymbol><ci id="S2.SS2.p2.5.m5.2.3.2.2.cmml" xref="S2.SS2.p2.5.m5.2.3.2.2">𝐴</ci><list id="S2.SS2.p2.5.m5.2.2.2.3.cmml" xref="S2.SS2.p2.5.m5.2.2.2.4"><ci id="S2.SS2.p2.5.m5.1.1.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1.1.1">𝑣</ci><ci id="S2.SS2.p2.5.m5.2.2.2.2.cmml" xref="S2.SS2.p2.5.m5.2.2.2.2">𝑢</ci></list></apply><cn type="integer" id="S2.SS2.p2.5.m5.2.3.3.cmml" xref="S2.SS2.p2.5.m5.2.3.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.2c">A_{v,u}=1</annotation></semantics></math> if <math id="S2.SS2.p2.6.m6.2" class="ltx_Math" alttext="e_{v,u}\in E" display="inline"><semantics id="S2.SS2.p2.6.m6.2a"><mrow id="S2.SS2.p2.6.m6.2.3" xref="S2.SS2.p2.6.m6.2.3.cmml"><msub id="S2.SS2.p2.6.m6.2.3.2" xref="S2.SS2.p2.6.m6.2.3.2.cmml"><mi id="S2.SS2.p2.6.m6.2.3.2.2" xref="S2.SS2.p2.6.m6.2.3.2.2.cmml">e</mi><mrow id="S2.SS2.p2.6.m6.2.2.2.4" xref="S2.SS2.p2.6.m6.2.2.2.3.cmml"><mi id="S2.SS2.p2.6.m6.1.1.1.1" xref="S2.SS2.p2.6.m6.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p2.6.m6.2.2.2.4.1" xref="S2.SS2.p2.6.m6.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p2.6.m6.2.2.2.2" xref="S2.SS2.p2.6.m6.2.2.2.2.cmml">u</mi></mrow></msub><mo id="S2.SS2.p2.6.m6.2.3.1" xref="S2.SS2.p2.6.m6.2.3.1.cmml">∈</mo><mi id="S2.SS2.p2.6.m6.2.3.3" xref="S2.SS2.p2.6.m6.2.3.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.2b"><apply id="S2.SS2.p2.6.m6.2.3.cmml" xref="S2.SS2.p2.6.m6.2.3"><in id="S2.SS2.p2.6.m6.2.3.1.cmml" xref="S2.SS2.p2.6.m6.2.3.1"></in><apply id="S2.SS2.p2.6.m6.2.3.2.cmml" xref="S2.SS2.p2.6.m6.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m6.2.3.2.1.cmml" xref="S2.SS2.p2.6.m6.2.3.2">subscript</csymbol><ci id="S2.SS2.p2.6.m6.2.3.2.2.cmml" xref="S2.SS2.p2.6.m6.2.3.2.2">𝑒</ci><list id="S2.SS2.p2.6.m6.2.2.2.3.cmml" xref="S2.SS2.p2.6.m6.2.2.2.4"><ci id="S2.SS2.p2.6.m6.1.1.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1.1.1">𝑣</ci><ci id="S2.SS2.p2.6.m6.2.2.2.2.cmml" xref="S2.SS2.p2.6.m6.2.2.2.2">𝑢</ci></list></apply><ci id="S2.SS2.p2.6.m6.2.3.3.cmml" xref="S2.SS2.p2.6.m6.2.3.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.2c">e_{v,u}\in E</annotation></semantics></math>. Then it follows that <math id="S2.SS2.p2.7.m7.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS2.p2.7.m7.1a"><mi id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><ci id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">A</annotation></semantics></math> is asymmetric (symmetric) for directed (undirected) graphs. More in general, <math id="S2.SS2.p2.8.m8.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS2.p2.8.m8.1a"><mi id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b"><ci id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">A</annotation></semantics></math> can be a real-valued matrix, where the value of <math id="S2.SS2.p2.9.m9.2" class="ltx_Math" alttext="A_{v,u}" display="inline"><semantics id="S2.SS2.p2.9.m9.2a"><msub id="S2.SS2.p2.9.m9.2.3" xref="S2.SS2.p2.9.m9.2.3.cmml"><mi id="S2.SS2.p2.9.m9.2.3.2" xref="S2.SS2.p2.9.m9.2.3.2.cmml">A</mi><mrow id="S2.SS2.p2.9.m9.2.2.2.4" xref="S2.SS2.p2.9.m9.2.2.2.3.cmml"><mi id="S2.SS2.p2.9.m9.1.1.1.1" xref="S2.SS2.p2.9.m9.1.1.1.1.cmml">v</mi><mo id="S2.SS2.p2.9.m9.2.2.2.4.1" xref="S2.SS2.p2.9.m9.2.2.2.3.cmml">,</mo><mi id="S2.SS2.p2.9.m9.2.2.2.2" xref="S2.SS2.p2.9.m9.2.2.2.2.cmml">u</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m9.2b"><apply id="S2.SS2.p2.9.m9.2.3.cmml" xref="S2.SS2.p2.9.m9.2.3"><csymbol cd="ambiguous" id="S2.SS2.p2.9.m9.2.3.1.cmml" xref="S2.SS2.p2.9.m9.2.3">subscript</csymbol><ci id="S2.SS2.p2.9.m9.2.3.2.cmml" xref="S2.SS2.p2.9.m9.2.3.2">𝐴</ci><list id="S2.SS2.p2.9.m9.2.2.2.3.cmml" xref="S2.SS2.p2.9.m9.2.2.2.4"><ci id="S2.SS2.p2.9.m9.1.1.1.1.cmml" xref="S2.SS2.p2.9.m9.1.1.1.1">𝑣</ci><ci id="S2.SS2.p2.9.m9.2.2.2.2.cmml" xref="S2.SS2.p2.9.m9.2.2.2.2">𝑢</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m9.2c">A_{v,u}</annotation></semantics></math> can be interpreted as the strength of the connection between <math id="S2.SS2.p2.10.m10.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS2.p2.10.m10.1a"><mi id="S2.SS2.p2.10.m10.1.1" xref="S2.SS2.p2.10.m10.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m10.1b"><ci id="S2.SS2.p2.10.m10.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m10.1c">v</annotation></semantics></math> and <math id="S2.SS2.p2.11.m11.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S2.SS2.p2.11.m11.1a"><mi id="S2.SS2.p2.11.m11.1.1" xref="S2.SS2.p2.11.m11.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.11.m11.1b"><ci id="S2.SS2.p2.11.m11.1.1.cmml" xref="S2.SS2.p2.11.m11.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.11.m11.1c">u</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.11" class="ltx_p"><span id="S2.SS2.p3.11.1" class="ltx_text ltx_font_bold">Neighbourhood.</span> The neighbourhood <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{N}(v)" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.2" xref="S2.SS2.p3.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p3.1.m1.1.2.2" xref="S2.SS2.p3.1.m1.1.2.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.1.2.1" xref="S2.SS2.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS2.p3.1.m1.1.2.3.2" xref="S2.SS2.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p3.1.m1.1.2.3.2.1" xref="S2.SS2.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">v</mi><mo stretchy="false" id="S2.SS2.p3.1.m1.1.2.3.2.2" xref="S2.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.2"><times id="S2.SS2.p3.1.m1.1.2.1.cmml" xref="S2.SS2.p3.1.m1.1.2.1"></times><ci id="S2.SS2.p3.1.m1.1.2.2.cmml" xref="S2.SS2.p3.1.m1.1.2.2">𝒩</ci><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\mathcal{N}(v)</annotation></semantics></math> of a vertex <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="v\in V" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">v</mi><mo id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">∈</mo><mi id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><in id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1"></in><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">𝑣</ci><ci id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">v\in V</annotation></semantics></math> is the subset of nodes in <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mi id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><ci id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">V</annotation></semantics></math> that are connected to <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">v</annotation></semantics></math>. The neighbour <math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mi id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><ci id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">u</annotation></semantics></math> can be either directly connected to <math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mi id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><ci id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">v</annotation></semantics></math>, i.e., <math id="S2.SS2.p3.7.m7.2" class="ltx_Math" alttext="(v,u)\in E" display="inline"><semantics id="S2.SS2.p3.7.m7.2a"><mrow id="S2.SS2.p3.7.m7.2.3" xref="S2.SS2.p3.7.m7.2.3.cmml"><mrow id="S2.SS2.p3.7.m7.2.3.2.2" xref="S2.SS2.p3.7.m7.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS2.p3.7.m7.2.3.2.2.1" xref="S2.SS2.p3.7.m7.2.3.2.1.cmml">(</mo><mi id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml">v</mi><mo id="S2.SS2.p3.7.m7.2.3.2.2.2" xref="S2.SS2.p3.7.m7.2.3.2.1.cmml">,</mo><mi id="S2.SS2.p3.7.m7.2.2" xref="S2.SS2.p3.7.m7.2.2.cmml">u</mi><mo stretchy="false" id="S2.SS2.p3.7.m7.2.3.2.2.3" xref="S2.SS2.p3.7.m7.2.3.2.1.cmml">)</mo></mrow><mo id="S2.SS2.p3.7.m7.2.3.1" xref="S2.SS2.p3.7.m7.2.3.1.cmml">∈</mo><mi id="S2.SS2.p3.7.m7.2.3.3" xref="S2.SS2.p3.7.m7.2.3.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.2b"><apply id="S2.SS2.p3.7.m7.2.3.cmml" xref="S2.SS2.p3.7.m7.2.3"><in id="S2.SS2.p3.7.m7.2.3.1.cmml" xref="S2.SS2.p3.7.m7.2.3.1"></in><interval closure="open" id="S2.SS2.p3.7.m7.2.3.2.1.cmml" xref="S2.SS2.p3.7.m7.2.3.2.2"><ci id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1">𝑣</ci><ci id="S2.SS2.p3.7.m7.2.2.cmml" xref="S2.SS2.p3.7.m7.2.2">𝑢</ci></interval><ci id="S2.SS2.p3.7.m7.2.3.3.cmml" xref="S2.SS2.p3.7.m7.2.3.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.2c">(v,u)\in E</annotation></semantics></math>, or it can be indirectly connected by traversing <math id="S2.SS2.p3.8.m8.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.SS2.p3.8.m8.1a"><mi id="S2.SS2.p3.8.m8.1.1" xref="S2.SS2.p3.8.m8.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.8.m8.1b"><ci id="S2.SS2.p3.8.m8.1.1.cmml" xref="S2.SS2.p3.8.m8.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.8.m8.1c">r</annotation></semantics></math> edges from <math id="S2.SS2.p3.9.m9.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS2.p3.9.m9.1a"><mi id="S2.SS2.p3.9.m9.1.1" xref="S2.SS2.p3.9.m9.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.9.m9.1b"><ci id="S2.SS2.p3.9.m9.1.1.cmml" xref="S2.SS2.p3.9.m9.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.9.m9.1c">v</annotation></semantics></math> to <math id="S2.SS2.p3.10.m10.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S2.SS2.p3.10.m10.1a"><mi id="S2.SS2.p3.10.m10.1.1" xref="S2.SS2.p3.10.m10.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.10.m10.1b"><ci id="S2.SS2.p3.10.m10.1.1.cmml" xref="S2.SS2.p3.10.m10.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.10.m10.1c">u</annotation></semantics></math>. Note that some definitions include <math id="S2.SS2.p3.11.m11.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS2.p3.11.m11.1a"><mi id="S2.SS2.p3.11.m11.1.1" xref="S2.SS2.p3.11.m11.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.11.m11.1b"><ci id="S2.SS2.p3.11.m11.1.1.cmml" xref="S2.SS2.p3.11.m11.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.11.m11.1c">v</annotation></semantics></math> itself as part of the neighbourhood.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Complete Graph.</span> A complete graph is one (directed or undirected) where for each vertex, there is an edge connecting it to every other vertex in the set <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">V</annotation></semantics></math>. A complete graph is therefore a graph with the maximum number of edges for a given number of nodes.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.2" class="ltx_p"><span id="S2.SS2.p5.2.1" class="ltx_text ltx_font_bold">Multipartite Graph.</span> A multipartite graph (also known as <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mi id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><ci id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">K</annotation></semantics></math>-partite graph) is a graph where the nodes can be separated into <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mi id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><ci id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">K</annotation></semantics></math> different sets. For scene understanding tasks, this allows for a graph representation where one set of nodes represent objects and another represents relationship between objects.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p"><span id="S2.SS2.p6.1.1" class="ltx_text ltx_font_bold">Multimodal Graph.</span> A multimodal graph is one with nodes that have features from different modalities. This approach is commonly used in VQA where the image and text modalities are mixed. Multimodal graphs enable visual features to co-exist in a graph with word embeddings.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Common Graph Types in 2D vision-language Tasks</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">This section organises the various graph types used across all three tasks discussed in the survey. Some graphs, such as the semantic and spatial graphs, are used across all tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, while others are more domain specific, like the knowledge graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Figure <a href="#S2.F2" title="Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a sample image from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> together with various types of graphs that can be used to describe it. This section, alongside the figure, is organised so that graph that represent a single image and graphs that represent portions the dataset are grouped together.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Semantic Graph.</span> Sometimes referred to as a scene graph, a semantic graph (shown in Figure <a href="#S2.F2.sf3" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2c</span></a>) is a one that encapsulates the semantic relationships between visual objects within a scene. Across the literature, the terms ‘semantic graph’ and ‘scene graph’ are used somewhat interchangeably, depending on the paper. However, in this survey we use the term ‘semantic graph’ because there are many ways to describe a visual scene as a graph, whereas the ‘semantic graph’ label is more precise about what the graph represents. Semantic graphs come in different flavours. One approach is to define a directed graph with nodes representing visual objects extracted by an object detector such as Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and edges representing semantic relationships between them. This is the approach of Yao <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, where, using a dataset such as Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, a model predicts the semantic relationships to form edges in the graph. Alternatively, the semantic graph can be seen as a multipartite graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> (shown in Figure <a href="#S2.F2.sf4" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2d</span></a>), where attribute nodes describe the object nodes they are linked to. They also change the way relationships are represented by using nodes rather than edge features. This yields a semantic graph with three node types: visual object, object attribute, and inter-object relationship. This definition follows that of the ‘scene graph’ defined by Johnson <span id="S2.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Finally, another form of semantic graph exists, the textual semantic graph<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> (shown in figure <a href="#S2.F2.sf6" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2f</span></a>). Unlike visual semantic graphs, textual ones are not generated from the image itself but rather its caption. Specifically, the caption is parsed through the Stanford Dependency Parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, a widely used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> probabilistic sentence parser. Given a caption, the parser will return its grammatical structure, identifying components such nouns, verbs, and adjectives and marking the relationship between them. This is then modified from a tree into a graph, following the techniques outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Spatial Graph.</span> Yao <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> define a spatial graph (Figure <a href="#S2.F2.sf7" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2g</span></a>) as one representing the spatial relationship between objects. Visual objects detected by an object detector form nodes, and the edges between the nodes represent one of 11 pre-defined spatial relationships that may occur between the two objects. These include inside (labelled ‘1’), cover (labelled ‘2’), overlap (labelled ‘3’), and eight positional relationships (labelled ‘4’-‘11’) based on the angle between the centroid of the two objects. These graphs are directional but will not always be complete as there are cases where two objects have a weak spatial relationship and are therefore not connected by an edge in the spatial graph. Guo <span id="S2.SS3.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> define a graph of a similar nature known as a geometry graph. It is defined as an undirected graph that encodes relative spatial positions between objects with an overlap and relative distance that meet certain thresholds.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.5" class="ltx_p"><span id="S2.SS3.p4.5.1" class="ltx_text ltx_font_bold">Hierarchical Spatial.</span> These graphs build on from the spatial graph but the relationships between nodes focus on the hierarchical nature of the spatial relationship between the detected objects within an image. Yao <span id="S2.SS3.p4.5.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> propose to use a tree (i.e., a graph where each pair of nodes is connected by a single path) to define a hierarchical image representation. An image (<math id="S2.SS3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.SS3.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><ci id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">\mathcal{I}</annotation></semantics></math>) is first divided into regions using Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> (<math id="S2.SS3.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{R}=\{r_{i}\}^{K}_{i=1}" display="inline"><semantics id="S2.SS3.p4.2.m2.1a"><mrow id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.2.m2.1.1.3" xref="S2.SS3.p4.2.m2.1.1.3.cmml">ℛ</mi><mo id="S2.SS3.p4.2.m2.1.1.2" xref="S2.SS3.p4.2.m2.1.1.2.cmml">=</mo><msubsup id="S2.SS3.p4.2.m2.1.1.1" xref="S2.SS3.p4.2.m2.1.1.1.cmml"><mrow id="S2.SS3.p4.2.m2.1.1.1.1.1.1" xref="S2.SS3.p4.2.m2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS3.p4.2.m2.1.1.1.1.1.1.2" xref="S2.SS3.p4.2.m2.1.1.1.1.1.2.cmml">{</mo><msub id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.2" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.3" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS3.p4.2.m2.1.1.1.1.1.1.3" xref="S2.SS3.p4.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.SS3.p4.2.m2.1.1.1.3" xref="S2.SS3.p4.2.m2.1.1.1.3.cmml"><mi id="S2.SS3.p4.2.m2.1.1.1.3.2" xref="S2.SS3.p4.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S2.SS3.p4.2.m2.1.1.1.3.1" xref="S2.SS3.p4.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S2.SS3.p4.2.m2.1.1.1.3.3" xref="S2.SS3.p4.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.SS3.p4.2.m2.1.1.1.1.3" xref="S2.SS3.p4.2.m2.1.1.1.1.3.cmml">K</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><apply id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1"><eq id="S2.SS3.p4.2.m2.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.2"></eq><ci id="S2.SS3.p4.2.m2.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.3">ℛ</ci><apply id="S2.SS3.p4.2.m2.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.2.m2.1.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.1">subscript</csymbol><apply id="S2.SS3.p4.2.m2.1.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.2.m2.1.1.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.1">superscript</csymbol><set id="S2.SS3.p4.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1"><apply id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.2">𝑟</ci><ci id="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="S2.SS3.p4.2.m2.1.1.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.1.1.3">𝐾</ci></apply><apply id="S2.SS3.p4.2.m2.1.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.1.3"><eq id="S2.SS3.p4.2.m2.1.1.1.3.1.cmml" xref="S2.SS3.p4.2.m2.1.1.1.3.1"></eq><ci id="S2.SS3.p4.2.m2.1.1.1.3.2.cmml" xref="S2.SS3.p4.2.m2.1.1.1.3.2">𝑖</ci><cn type="integer" id="S2.SS3.p4.2.m2.1.1.1.3.3.cmml" xref="S2.SS3.p4.2.m2.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">\mathcal{R}=\{r_{i}\}^{K}_{i=1}</annotation></semantics></math>) with each region being further divided into instance segmentations (<math id="S2.SS3.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{M}=\{m_{i}\}^{K}_{i=1}" display="inline"><semantics id="S2.SS3.p4.3.m3.1a"><mrow id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.3.m3.1.1.3" xref="S2.SS3.p4.3.m3.1.1.3.cmml">ℳ</mi><mo id="S2.SS3.p4.3.m3.1.1.2" xref="S2.SS3.p4.3.m3.1.1.2.cmml">=</mo><msubsup id="S2.SS3.p4.3.m3.1.1.1" xref="S2.SS3.p4.3.m3.1.1.1.cmml"><mrow id="S2.SS3.p4.3.m3.1.1.1.1.1.1" xref="S2.SS3.p4.3.m3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS3.p4.3.m3.1.1.1.1.1.1.2" xref="S2.SS3.p4.3.m3.1.1.1.1.1.2.cmml">{</mo><msub id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.cmml"><mi id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.2" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.2.cmml">m</mi><mi id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.3" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS3.p4.3.m3.1.1.1.1.1.1.3" xref="S2.SS3.p4.3.m3.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.SS3.p4.3.m3.1.1.1.3" xref="S2.SS3.p4.3.m3.1.1.1.3.cmml"><mi id="S2.SS3.p4.3.m3.1.1.1.3.2" xref="S2.SS3.p4.3.m3.1.1.1.3.2.cmml">i</mi><mo id="S2.SS3.p4.3.m3.1.1.1.3.1" xref="S2.SS3.p4.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S2.SS3.p4.3.m3.1.1.1.3.3" xref="S2.SS3.p4.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.SS3.p4.3.m3.1.1.1.1.3" xref="S2.SS3.p4.3.m3.1.1.1.1.3.cmml">K</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><apply id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1"><eq id="S2.SS3.p4.3.m3.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.2"></eq><ci id="S2.SS3.p4.3.m3.1.1.3.cmml" xref="S2.SS3.p4.3.m3.1.1.3">ℳ</ci><apply id="S2.SS3.p4.3.m3.1.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.3.m3.1.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.1">subscript</csymbol><apply id="S2.SS3.p4.3.m3.1.1.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.3.m3.1.1.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.1">superscript</csymbol><set id="S2.SS3.p4.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1"><apply id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.2">𝑚</ci><ci id="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="S2.SS3.p4.3.m3.1.1.1.1.3.cmml" xref="S2.SS3.p4.3.m3.1.1.1.1.3">𝐾</ci></apply><apply id="S2.SS3.p4.3.m3.1.1.1.3.cmml" xref="S2.SS3.p4.3.m3.1.1.1.3"><eq id="S2.SS3.p4.3.m3.1.1.1.3.1.cmml" xref="S2.SS3.p4.3.m3.1.1.1.3.1"></eq><ci id="S2.SS3.p4.3.m3.1.1.1.3.2.cmml" xref="S2.SS3.p4.3.m3.1.1.1.3.2">𝑖</ci><cn type="integer" id="S2.SS3.p4.3.m3.1.1.1.3.3.cmml" xref="S2.SS3.p4.3.m3.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">\mathcal{M}=\{m_{i}\}^{K}_{i=1}</annotation></semantics></math>). This gives a three-layer tree structure (<math id="S2.SS3.p4.4.m4.4" class="ltx_Math" alttext="\mathcal{T}=(\mathcal{I},\mathcal{R},\mathcal{M},\mathcal{E}_{tree})" display="inline"><semantics id="S2.SS3.p4.4.m4.4a"><mrow id="S2.SS3.p4.4.m4.4.4" xref="S2.SS3.p4.4.m4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.4.m4.4.4.3" xref="S2.SS3.p4.4.m4.4.4.3.cmml">𝒯</mi><mo id="S2.SS3.p4.4.m4.4.4.2" xref="S2.SS3.p4.4.m4.4.4.2.cmml">=</mo><mrow id="S2.SS3.p4.4.m4.4.4.1.1" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml"><mo stretchy="false" id="S2.SS3.p4.4.m4.4.4.1.1.2" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml">ℐ</mi><mo id="S2.SS3.p4.4.m4.4.4.1.1.3" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.4.m4.2.2" xref="S2.SS3.p4.4.m4.2.2.cmml">ℛ</mi><mo id="S2.SS3.p4.4.m4.4.4.1.1.4" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.4.m4.3.3" xref="S2.SS3.p4.4.m4.3.3.cmml">ℳ</mi><mo id="S2.SS3.p4.4.m4.4.4.1.1.5" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml">,</mo><msub id="S2.SS3.p4.4.m4.4.4.1.1.1" xref="S2.SS3.p4.4.m4.4.4.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.4.m4.4.4.1.1.1.2" xref="S2.SS3.p4.4.m4.4.4.1.1.1.2.cmml">ℰ</mi><mrow id="S2.SS3.p4.4.m4.4.4.1.1.1.3" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.cmml"><mi id="S2.SS3.p4.4.m4.4.4.1.1.1.3.2" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.4.m4.4.4.1.1.1.3.1" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.4.m4.4.4.1.1.1.3.3" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.4.m4.4.4.1.1.1.3.1a" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.4.m4.4.4.1.1.1.3.4" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.4.m4.4.4.1.1.1.3.1b" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.4.m4.4.4.1.1.1.3.5" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.5.cmml">e</mi></mrow></msub><mo stretchy="false" id="S2.SS3.p4.4.m4.4.4.1.1.6" xref="S2.SS3.p4.4.m4.4.4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.4b"><apply id="S2.SS3.p4.4.m4.4.4.cmml" xref="S2.SS3.p4.4.m4.4.4"><eq id="S2.SS3.p4.4.m4.4.4.2.cmml" xref="S2.SS3.p4.4.m4.4.4.2"></eq><ci id="S2.SS3.p4.4.m4.4.4.3.cmml" xref="S2.SS3.p4.4.m4.4.4.3">𝒯</ci><vector id="S2.SS3.p4.4.m4.4.4.1.2.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1"><ci id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1">ℐ</ci><ci id="S2.SS3.p4.4.m4.2.2.cmml" xref="S2.SS3.p4.4.m4.2.2">ℛ</ci><ci id="S2.SS3.p4.4.m4.3.3.cmml" xref="S2.SS3.p4.4.m4.3.3">ℳ</ci><apply id="S2.SS3.p4.4.m4.4.4.1.1.1.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.4.m4.4.4.1.1.1.1.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1">subscript</csymbol><ci id="S2.SS3.p4.4.m4.4.4.1.1.1.2.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.2">ℰ</ci><apply id="S2.SS3.p4.4.m4.4.4.1.1.1.3.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3"><times id="S2.SS3.p4.4.m4.4.4.1.1.1.3.1.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.1"></times><ci id="S2.SS3.p4.4.m4.4.4.1.1.1.3.2.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.2">𝑡</ci><ci id="S2.SS3.p4.4.m4.4.4.1.1.1.3.3.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.3">𝑟</ci><ci id="S2.SS3.p4.4.m4.4.4.1.1.1.3.4.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.4">𝑒</ci><ci id="S2.SS3.p4.4.m4.4.4.1.1.1.3.5.cmml" xref="S2.SS3.p4.4.m4.4.4.1.1.1.3.5">𝑒</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.4c">\mathcal{T}=(\mathcal{I},\mathcal{R},\mathcal{M},\mathcal{E}_{tree})</annotation></semantics></math>, where <math id="S2.SS3.p4.5.m5.1" class="ltx_Math" alttext="\mathcal{E}_{tree}" display="inline"><semantics id="S2.SS3.p4.5.m5.1a"><msub id="S2.SS3.p4.5.m5.1.1" xref="S2.SS3.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p4.5.m5.1.1.2" xref="S2.SS3.p4.5.m5.1.1.2.cmml">ℰ</mi><mrow id="S2.SS3.p4.5.m5.1.1.3" xref="S2.SS3.p4.5.m5.1.1.3.cmml"><mi id="S2.SS3.p4.5.m5.1.1.3.2" xref="S2.SS3.p4.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.5.m5.1.1.3.1" xref="S2.SS3.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.5.m5.1.1.3.3" xref="S2.SS3.p4.5.m5.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.5.m5.1.1.3.1a" xref="S2.SS3.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.5.m5.1.1.3.4" xref="S2.SS3.p4.5.m5.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.5.m5.1.1.3.1b" xref="S2.SS3.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS3.p4.5.m5.1.1.3.5" xref="S2.SS3.p4.5.m5.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.5.m5.1b"><apply id="S2.SS3.p4.5.m5.1.1.cmml" xref="S2.SS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.5.m5.1.1.1.cmml" xref="S2.SS3.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS3.p4.5.m5.1.1.2.cmml" xref="S2.SS3.p4.5.m5.1.1.2">ℰ</ci><apply id="S2.SS3.p4.5.m5.1.1.3.cmml" xref="S2.SS3.p4.5.m5.1.1.3"><times id="S2.SS3.p4.5.m5.1.1.3.1.cmml" xref="S2.SS3.p4.5.m5.1.1.3.1"></times><ci id="S2.SS3.p4.5.m5.1.1.3.2.cmml" xref="S2.SS3.p4.5.m5.1.1.3.2">𝑡</ci><ci id="S2.SS3.p4.5.m5.1.1.3.3.cmml" xref="S2.SS3.p4.5.m5.1.1.3.3">𝑟</ci><ci id="S2.SS3.p4.5.m5.1.1.3.4.cmml" xref="S2.SS3.p4.5.m5.1.1.3.4">𝑒</ci><ci id="S2.SS3.p4.5.m5.1.1.3.5.cmml" xref="S2.SS3.p4.5.m5.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.5.m5.1c">\mathcal{E}_{tree}</annotation></semantics></math> is the set of connecting edges) to represent the image, as shown in Figure <a href="#S2.F2.sf5" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2e</span></a>. He <span id="S2.SS3.p4.5.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> use a hierarchical spatial graph, with relationships representing ‘parent’, ‘child’, and ‘neighbour’ relationships depending on the intersection over union of the bounding boxes.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.9" class="ltx_p"><span id="S2.SS3.p5.9.1" class="ltx_text ltx_font_bold">Similarity Graph.</span> The similarity graph (Figure <a href="#S2.F2.sf8" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2h</span></a>) proposed by Kan <span id="S2.SS3.p5.9.2" class="ltx_text ltx_font_italic">et al</span>.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> (referred to as a semantic graph by the authors) is generated by computing the dot product between two visual features extracted by Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. The dot products are then used to form the values of an adjacency matrix <math id="S2.SS3.p5.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS3.p5.1.m1.1a"><mi id="S2.SS3.p5.1.m1.1.1" xref="S2.SS3.p5.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.1.m1.1b"><ci id="S2.SS3.p5.1.m1.1.1.cmml" xref="S2.SS3.p5.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.1.m1.1c">A</annotation></semantics></math> as the operation captures the similarity between two vectors, the higher the dot product, the closer the two vectors are. Faster-RCNN extracts a set of <math id="S2.SS3.p5.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS3.p5.2.m2.1a"><mi id="S2.SS3.p5.2.m2.1.1" xref="S2.SS3.p5.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.2.m2.1b"><ci id="S2.SS3.p5.2.m2.1.1.cmml" xref="S2.SS3.p5.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.2.m2.1c">n</annotation></semantics></math> visual features, where each feature <math id="S2.SS3.p5.3.m3.1" class="ltx_Math" alttext="x(v)" display="inline"><semantics id="S2.SS3.p5.3.m3.1a"><mrow id="S2.SS3.p5.3.m3.1.2" xref="S2.SS3.p5.3.m3.1.2.cmml"><mi id="S2.SS3.p5.3.m3.1.2.2" xref="S2.SS3.p5.3.m3.1.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.3.m3.1.2.1" xref="S2.SS3.p5.3.m3.1.2.1.cmml">​</mo><mrow id="S2.SS3.p5.3.m3.1.2.3.2" xref="S2.SS3.p5.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS3.p5.3.m3.1.2.3.2.1" xref="S2.SS3.p5.3.m3.1.2.cmml">(</mo><mi id="S2.SS3.p5.3.m3.1.1" xref="S2.SS3.p5.3.m3.1.1.cmml">v</mi><mo stretchy="false" id="S2.SS3.p5.3.m3.1.2.3.2.2" xref="S2.SS3.p5.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.3.m3.1b"><apply id="S2.SS3.p5.3.m3.1.2.cmml" xref="S2.SS3.p5.3.m3.1.2"><times id="S2.SS3.p5.3.m3.1.2.1.cmml" xref="S2.SS3.p5.3.m3.1.2.1"></times><ci id="S2.SS3.p5.3.m3.1.2.2.cmml" xref="S2.SS3.p5.3.m3.1.2.2">𝑥</ci><ci id="S2.SS3.p5.3.m3.1.1.cmml" xref="S2.SS3.p5.3.m3.1.1">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.3.m3.1c">x(v)</annotation></semantics></math> is associated to a node <math id="S2.SS3.p5.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS3.p5.4.m4.1a"><mi id="S2.SS3.p5.4.m4.1.1" xref="S2.SS3.p5.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.4.m4.1b"><ci id="S2.SS3.p5.4.m4.1.1.cmml" xref="S2.SS3.p5.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.4.m4.1c">v</annotation></semantics></math> and the value of the edge between two nodes <math id="S2.SS3.p5.5.m5.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S2.SS3.p5.5.m5.1a"><mi id="S2.SS3.p5.5.m5.1.1" xref="S2.SS3.p5.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.5.m5.1b"><ci id="S2.SS3.p5.5.m5.1.1.cmml" xref="S2.SS3.p5.5.m5.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.5.m5.1c">v</annotation></semantics></math> and <math id="S2.SS3.p5.6.m6.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S2.SS3.p5.6.m6.1a"><mi id="S2.SS3.p5.6.m6.1.1" xref="S2.SS3.p5.6.m6.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.6.m6.1b"><ci id="S2.SS3.p5.6.m6.1.1.cmml" xref="S2.SS3.p5.6.m6.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.6.m6.1c">u</annotation></semantics></math> is given by <math id="S2.SS3.p5.7.m7.5" class="ltx_Math" alttext="A_{u,v}=\sigma\left(x(v)^{T}Mx(u)\right)" display="inline"><semantics id="S2.SS3.p5.7.m7.5a"><mrow id="S2.SS3.p5.7.m7.5.5" xref="S2.SS3.p5.7.m7.5.5.cmml"><msub id="S2.SS3.p5.7.m7.5.5.3" xref="S2.SS3.p5.7.m7.5.5.3.cmml"><mi id="S2.SS3.p5.7.m7.5.5.3.2" xref="S2.SS3.p5.7.m7.5.5.3.2.cmml">A</mi><mrow id="S2.SS3.p5.7.m7.2.2.2.4" xref="S2.SS3.p5.7.m7.2.2.2.3.cmml"><mi id="S2.SS3.p5.7.m7.1.1.1.1" xref="S2.SS3.p5.7.m7.1.1.1.1.cmml">u</mi><mo id="S2.SS3.p5.7.m7.2.2.2.4.1" xref="S2.SS3.p5.7.m7.2.2.2.3.cmml">,</mo><mi id="S2.SS3.p5.7.m7.2.2.2.2" xref="S2.SS3.p5.7.m7.2.2.2.2.cmml">v</mi></mrow></msub><mo id="S2.SS3.p5.7.m7.5.5.2" xref="S2.SS3.p5.7.m7.5.5.2.cmml">=</mo><mrow id="S2.SS3.p5.7.m7.5.5.1" xref="S2.SS3.p5.7.m7.5.5.1.cmml"><mi id="S2.SS3.p5.7.m7.5.5.1.3" xref="S2.SS3.p5.7.m7.5.5.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.7.m7.5.5.1.2" xref="S2.SS3.p5.7.m7.5.5.1.2.cmml">​</mo><mrow id="S2.SS3.p5.7.m7.5.5.1.1.1" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml"><mo id="S2.SS3.p5.7.m7.5.5.1.1.1.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p5.7.m7.5.5.1.1.1.1" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml"><mi id="S2.SS3.p5.7.m7.5.5.1.1.1.1.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.1" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.1.cmml">​</mo><msup id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.cmml"><mrow id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.2.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.2.2.1" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.cmml">(</mo><mi id="S2.SS3.p5.7.m7.3.3" xref="S2.SS3.p5.7.m7.3.3.cmml">v</mi><mo stretchy="false" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.2.2.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.cmml">)</mo></mrow><mi id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.3" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.1a" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.1.cmml">​</mo><mi id="S2.SS3.p5.7.m7.5.5.1.1.1.1.4" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.1b" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.1.cmml">​</mo><mi id="S2.SS3.p5.7.m7.5.5.1.1.1.1.5" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.1c" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.1.cmml">​</mo><mrow id="S2.SS3.p5.7.m7.5.5.1.1.1.1.6.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.6.2.1" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml">(</mo><mi id="S2.SS3.p5.7.m7.4.4" xref="S2.SS3.p5.7.m7.4.4.cmml">u</mi><mo stretchy="false" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.6.2.2" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS3.p5.7.m7.5.5.1.1.1.3" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.7.m7.5b"><apply id="S2.SS3.p5.7.m7.5.5.cmml" xref="S2.SS3.p5.7.m7.5.5"><eq id="S2.SS3.p5.7.m7.5.5.2.cmml" xref="S2.SS3.p5.7.m7.5.5.2"></eq><apply id="S2.SS3.p5.7.m7.5.5.3.cmml" xref="S2.SS3.p5.7.m7.5.5.3"><csymbol cd="ambiguous" id="S2.SS3.p5.7.m7.5.5.3.1.cmml" xref="S2.SS3.p5.7.m7.5.5.3">subscript</csymbol><ci id="S2.SS3.p5.7.m7.5.5.3.2.cmml" xref="S2.SS3.p5.7.m7.5.5.3.2">𝐴</ci><list id="S2.SS3.p5.7.m7.2.2.2.3.cmml" xref="S2.SS3.p5.7.m7.2.2.2.4"><ci id="S2.SS3.p5.7.m7.1.1.1.1.cmml" xref="S2.SS3.p5.7.m7.1.1.1.1">𝑢</ci><ci id="S2.SS3.p5.7.m7.2.2.2.2.cmml" xref="S2.SS3.p5.7.m7.2.2.2.2">𝑣</ci></list></apply><apply id="S2.SS3.p5.7.m7.5.5.1.cmml" xref="S2.SS3.p5.7.m7.5.5.1"><times id="S2.SS3.p5.7.m7.5.5.1.2.cmml" xref="S2.SS3.p5.7.m7.5.5.1.2"></times><ci id="S2.SS3.p5.7.m7.5.5.1.3.cmml" xref="S2.SS3.p5.7.m7.5.5.1.3">𝜎</ci><apply id="S2.SS3.p5.7.m7.5.5.1.1.1.1.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1"><times id="S2.SS3.p5.7.m7.5.5.1.1.1.1.1.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.1"></times><ci id="S2.SS3.p5.7.m7.5.5.1.1.1.1.2.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.2">𝑥</ci><apply id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.1.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3">superscript</csymbol><ci id="S2.SS3.p5.7.m7.3.3.cmml" xref="S2.SS3.p5.7.m7.3.3">𝑣</ci><ci id="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.3.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.3.3">𝑇</ci></apply><ci id="S2.SS3.p5.7.m7.5.5.1.1.1.1.4.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.4">𝑀</ci><ci id="S2.SS3.p5.7.m7.5.5.1.1.1.1.5.cmml" xref="S2.SS3.p5.7.m7.5.5.1.1.1.1.5">𝑥</ci><ci id="S2.SS3.p5.7.m7.4.4.cmml" xref="S2.SS3.p5.7.m7.4.4">𝑢</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.7.m7.5c">A_{u,v}=\sigma\left(x(v)^{T}Mx(u)\right)</annotation></semantics></math>, where <math id="S2.SS3.p5.8.m8.1" class="ltx_Math" alttext="\sigma(\cdot)" display="inline"><semantics id="S2.SS3.p5.8.m8.1a"><mrow id="S2.SS3.p5.8.m8.1.2" xref="S2.SS3.p5.8.m8.1.2.cmml"><mi id="S2.SS3.p5.8.m8.1.2.2" xref="S2.SS3.p5.8.m8.1.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p5.8.m8.1.2.1" xref="S2.SS3.p5.8.m8.1.2.1.cmml">​</mo><mrow id="S2.SS3.p5.8.m8.1.2.3.2" xref="S2.SS3.p5.8.m8.1.2.cmml"><mo stretchy="false" id="S2.SS3.p5.8.m8.1.2.3.2.1" xref="S2.SS3.p5.8.m8.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS3.p5.8.m8.1.1" xref="S2.SS3.p5.8.m8.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS3.p5.8.m8.1.2.3.2.2" xref="S2.SS3.p5.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.8.m8.1b"><apply id="S2.SS3.p5.8.m8.1.2.cmml" xref="S2.SS3.p5.8.m8.1.2"><times id="S2.SS3.p5.8.m8.1.2.1.cmml" xref="S2.SS3.p5.8.m8.1.2.1"></times><ci id="S2.SS3.p5.8.m8.1.2.2.cmml" xref="S2.SS3.p5.8.m8.1.2.2">𝜎</ci><ci id="S2.SS3.p5.8.m8.1.1.cmml" xref="S2.SS3.p5.8.m8.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.8.m8.1c">\sigma(\cdot)</annotation></semantics></math> is a non-linear function and <math id="S2.SS3.p5.9.m9.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS3.p5.9.m9.1a"><mi id="S2.SS3.p5.9.m9.1.1" xref="S2.SS3.p5.9.m9.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.9.m9.1b"><ci id="S2.SS3.p5.9.m9.1.1.cmml" xref="S2.SS3.p5.9.m9.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.9.m9.1c">M</annotation></semantics></math> is a learnt weight matrix. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> suggest that generating the graph this way allows for relationships between objects to be discovered in a data-driven manner, rather than relying on a model trained on a dataset such as the Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.4" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold">Image Graphs/<math id="S2.SS3.p6.1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS3.p6.1.1.m1.1a"><mi id="S2.SS3.p6.1.1.m1.1.1" xref="S2.SS3.p6.1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.1.1.m1.1b"><ci id="S2.SS3.p6.1.1.m1.1.1.cmml" xref="S2.SS3.p6.1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.1.1.m1.1c">K</annotation></semantics></math>-Nearest Neighbour Graph.</span> In their 2021 image captioning work, Dong <span id="S2.SS3.p6.4.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> construct an image graph by converting images into a latent feature space by averaging the object vectors output by feeding the image into Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. The <math id="S2.SS3.p6.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS3.p6.2.m1.1a"><mi id="S2.SS3.p6.2.m1.1.1" xref="S2.SS3.p6.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.2.m1.1b"><ci id="S2.SS3.p6.2.m1.1.1.cmml" xref="S2.SS3.p6.2.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.2.m1.1c">K</annotation></semantics></math> closest images from the training data or search space in terms of <math id="S2.SS3.p6.3.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S2.SS3.p6.3.m2.1a"><msub id="S2.SS3.p6.3.m2.1.1" xref="S2.SS3.p6.3.m2.1.1.cmml"><mi id="S2.SS3.p6.3.m2.1.1.2" xref="S2.SS3.p6.3.m2.1.1.2.cmml">l</mi><mn id="S2.SS3.p6.3.m2.1.1.3" xref="S2.SS3.p6.3.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.3.m2.1b"><apply id="S2.SS3.p6.3.m2.1.1.cmml" xref="S2.SS3.p6.3.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p6.3.m2.1.1.1.cmml" xref="S2.SS3.p6.3.m2.1.1">subscript</csymbol><ci id="S2.SS3.p6.3.m2.1.1.2.cmml" xref="S2.SS3.p6.3.m2.1.1.2">𝑙</ci><cn type="integer" id="S2.SS3.p6.3.m2.1.1.3.cmml" xref="S2.SS3.p6.3.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.3.m2.1c">l_{2}</annotation></semantics></math> distance are then turned into an undirected complete graph, shown in Figure <a href="#S2.F2.sf9" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2i</span></a>. This is a similar approach used by Liu <span id="S2.SS3.p6.4.3" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> with their <math id="S2.SS3.p6.4.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS3.p6.4.m3.1a"><mi id="S2.SS3.p6.4.m3.1.1" xref="S2.SS3.p6.4.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.4.m3.1b"><ci id="S2.SS3.p6.4.m3.1.1.cmml" xref="S2.SS3.p6.4.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.4.m3.1c">K</annotation></semantics></math>-nearest neighbour graph.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p"><span id="S2.SS3.p7.1.1" class="ltx_text ltx_font_bold">Topic Graph.</span> Proposed by Kan <span id="S2.SS3.p7.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, the topic graph is an undirected graph of nodes representing topics extracted by GPU-DMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. Topics are latent features representing shared knowledge across the entire caption set. Modelling them as a graph, as shown in Figure <a href="#S2.F2.sf10" title="In Figure 2 ‣ II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2j</span></a>, with edges computed by taking the dot product of the two nodes, allows the modelling of knowledge represented in the captions.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p"><span id="S2.SS3.p8.1.1" class="ltx_text ltx_font_bold">Region Adjacency Graph.</span> Defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, a Region Adjacency Graph uses a superpixel segmentation. Superpixels form the nodes of the graph and edges are added to connect adjacent region pairs. Edges are then weighted to represent how compatible the two adjacent regions are.</p>
</div>
<div id="S2.SS3.p9" class="ltx_para">
<p id="S2.SS3.p9.1" class="ltx_p"><span id="S2.SS3.p9.1.1" class="ltx_text ltx_font_bold">Knowledge Graph. </span> A knowledge graph, or fact graph, is a graph-based representation of information. Whilst there is no agreed structure of these graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, they typically take the form of triplets. They are used in a wide variety of tasks to provide the information needed to ”reason”. Hence, knowledge graphs enable the FVQA task.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/391895-cropped.jpg" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_portrait" width="120" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>COCO training image 391895 (Cropped) </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/object-detection.jpg" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_portrait" width="120" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Image with object detection labels </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/semantic-graph.jpg" id="S2.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>A semantic graph </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/multipartite-semantic-graph.jpg" id="S2.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="162" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>A multipartite semantic graph </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/tree.jpg" id="S2.F2.sf5.g1" class="ltx_graphics ltx_img_landscape" width="240" height="160" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>An image tree representation </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/textual-semantic-graph.jpg" id="S2.F2.sf6.g1" class="ltx_graphics ltx_img_landscape" width="180" height="29" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>A textual semantic graph representation of the caption <span id="S2.F2.sf6.2.1" class="ltx_text ltx_font_italic">“A man riding on the back of a motorcycle”</span> </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/spatial-graph.jpg" id="S2.F2.sf7.g1" class="ltx_graphics ltx_img_landscape" width="180" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>A spatial graph representation </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/similarity-graph.jpg" id="S2.F2.sf8.g1" class="ltx_graphics ltx_img_landscape" width="180" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>A similarity graph representation </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/image-graph.png" id="S2.F2.sf9.g1" class="ltx_graphics ltx_img_landscape" width="180" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>An image graph representation </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf10" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.03761/assets/figures/graph-types/topic-graph.jpg" id="S2.F2.sf10.g1" class="ltx_graphics ltx_img_landscape" width="180" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(j) </span>A topic graph representation </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A visual comparison of the various graph types used across vision-language tasks. Best viewed in colour.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">An Overview of Graph Neural Networks</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Over the past years a large number of GNN architectures have been introduced in the literature. Wu <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> proposed a taxonomy containing four distinct groups: recurrent GNNs, convolutional GNNs, autoencoder GNNs, and spatial-temporal GNNs. The applications discussed in this paper mostly utilise convolutional GNNs, for a comprehensive overview of other architectures readers are directed to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. GNNs, especially traditional architectures such as Graph Convolutional Network, have a deep grounding in relational inductive biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. They are built on the assumption of homophily, i.e. that connected nodes are similar.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Graph Convolutional Networks (GCNs)</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">One common convolutional GNN architecture is the Message Passing Neural Networks (MPNNs) proposed by Gilmer <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> Although this architecture has been shown to be limited <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, it forms a good abstraction of GNNs.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Gilmer <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. describe MPNNs as being comprised of a message function, update function, and readout function. These functions will vary depending on the application of the network, but are learnable, differentiable, and permutation invariant. The message and update functions will run for a number of time steps <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">T</annotation></semantics></math>, passing messages between connected nodes of the graph. These are used to update the hidden feature vectors of the nodes, which are then used to update the node feature vector, which in turn is used in the readout function.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.8" class="ltx_p">The messages are defined as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\bar{m}^{(t+1)}_{v}=\sum_{u\in\mathcal{N}(v)}M_{t}(\bar{h}^{(t)}_{v},\bar{h}^{(t)}_{u},\bar{e}_{v,u})\,," display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><msubsup id="S3.E1.m1.7.7.1.1.5" xref="S3.E1.m1.7.7.1.1.5.cmml"><mover accent="true" id="S3.E1.m1.7.7.1.1.5.2.2" xref="S3.E1.m1.7.7.1.1.5.2.2.cmml"><mi id="S3.E1.m1.7.7.1.1.5.2.2.2" xref="S3.E1.m1.7.7.1.1.5.2.2.2.cmml">m</mi><mo id="S3.E1.m1.7.7.1.1.5.2.2.1" xref="S3.E1.m1.7.7.1.1.5.2.2.1.cmml">¯</mo></mover><mi id="S3.E1.m1.7.7.1.1.5.3" xref="S3.E1.m1.7.7.1.1.5.3.cmml">v</mi><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo rspace="0.111em" id="S3.E1.m1.7.7.1.1.4" xref="S3.E1.m1.7.7.1.1.4.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><munder id="S3.E1.m1.7.7.1.1.3.4" xref="S3.E1.m1.7.7.1.1.3.4.cmml"><mo movablelimits="false" id="S3.E1.m1.7.7.1.1.3.4.2" xref="S3.E1.m1.7.7.1.1.3.4.2.cmml">∑</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mi id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml">u</mi><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">∈</mo><mrow id="S3.E1.m1.2.2.1.4" xref="S3.E1.m1.2.2.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.4.2" xref="S3.E1.m1.2.2.1.4.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.4.1" xref="S3.E1.m1.2.2.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.4.3.2" xref="S3.E1.m1.2.2.1.4.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.4.3.2.1" xref="S3.E1.m1.2.2.1.4.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">v</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.4.3.2.2" xref="S3.E1.m1.2.2.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S3.E1.m1.7.7.1.1.3.3" xref="S3.E1.m1.7.7.1.1.3.3.cmml"><msub id="S3.E1.m1.7.7.1.1.3.3.5" xref="S3.E1.m1.7.7.1.1.3.3.5.cmml"><mi id="S3.E1.m1.7.7.1.1.3.3.5.2" xref="S3.E1.m1.7.7.1.1.3.3.5.2.cmml">M</mi><mi id="S3.E1.m1.7.7.1.1.3.3.5.3" xref="S3.E1.m1.7.7.1.1.3.3.5.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.3.3.4" xref="S3.E1.m1.7.7.1.1.3.3.4.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.3.3.3.3" xref="S3.E1.m1.7.7.1.1.3.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.3.3.3.3.4" xref="S3.E1.m1.7.7.1.1.3.3.3.4.cmml">(</mo><msubsup id="S3.E1.m1.7.7.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.2.cmml">h</mi><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.1.cmml">¯</mo></mover><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.3.cmml">v</mi><mrow id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.3.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">t</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.3.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.7.7.1.1.3.3.3.3.5" xref="S3.E1.m1.7.7.1.1.3.3.3.4.cmml">,</mo><msubsup id="S3.E1.m1.7.7.1.1.2.2.2.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.cmml"><mover accent="true" id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.2.cmml">h</mi><mo id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.1" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.1.cmml">¯</mo></mover><mi id="S3.E1.m1.7.7.1.1.2.2.2.2.2.3" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.3.cmml">u</mi><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.3.1" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml">t</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.7.7.1.1.3.3.3.3.6" xref="S3.E1.m1.7.7.1.1.3.3.3.4.cmml">,</mo><msub id="S3.E1.m1.7.7.1.1.3.3.3.3.3" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.cmml"><mover accent="true" id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.cmml"><mi id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.2" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.2.cmml">e</mi><mo id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.1" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.6.6.2.4" xref="S3.E1.m1.6.6.2.3.cmml"><mi id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml">v</mi><mo id="S3.E1.m1.6.6.2.4.1" xref="S3.E1.m1.6.6.2.3.cmml">,</mo><mi id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.2.cmml">u</mi></mrow></msub><mo rspace="0.170em" stretchy="false" id="S3.E1.m1.7.7.1.1.3.3.3.3.7" xref="S3.E1.m1.7.7.1.1.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1"><eq id="S3.E1.m1.7.7.1.1.4.cmml" xref="S3.E1.m1.7.7.1.1.4"></eq><apply id="S3.E1.m1.7.7.1.1.5.cmml" xref="S3.E1.m1.7.7.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.5.1.cmml" xref="S3.E1.m1.7.7.1.1.5">subscript</csymbol><apply id="S3.E1.m1.7.7.1.1.5.2.cmml" xref="S3.E1.m1.7.7.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.5.2.1.cmml" xref="S3.E1.m1.7.7.1.1.5">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.5.2.2.cmml" xref="S3.E1.m1.7.7.1.1.5.2.2"><ci id="S3.E1.m1.7.7.1.1.5.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.5.2.2.1">¯</ci><ci id="S3.E1.m1.7.7.1.1.5.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.5.2.2.2">𝑚</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.E1.m1.7.7.1.1.5.3.cmml" xref="S3.E1.m1.7.7.1.1.5.3">𝑣</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><apply id="S3.E1.m1.7.7.1.1.3.4.cmml" xref="S3.E1.m1.7.7.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.4.1.cmml" xref="S3.E1.m1.7.7.1.1.3.4">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.3.4.2.cmml" xref="S3.E1.m1.7.7.1.1.3.4.2"></sum><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><in id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></in><ci id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3">𝑢</ci><apply id="S3.E1.m1.2.2.1.4.cmml" xref="S3.E1.m1.2.2.1.4"><times id="S3.E1.m1.2.2.1.4.1.cmml" xref="S3.E1.m1.2.2.1.4.1"></times><ci id="S3.E1.m1.2.2.1.4.2.cmml" xref="S3.E1.m1.2.2.1.4.2">𝒩</ci><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">𝑣</ci></apply></apply></apply><apply id="S3.E1.m1.7.7.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.3"><times id="S3.E1.m1.7.7.1.1.3.3.4.cmml" xref="S3.E1.m1.7.7.1.1.3.3.4"></times><apply id="S3.E1.m1.7.7.1.1.3.3.5.cmml" xref="S3.E1.m1.7.7.1.1.3.3.5"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.3.5.1.cmml" xref="S3.E1.m1.7.7.1.1.3.3.5">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.3.5.2.cmml" xref="S3.E1.m1.7.7.1.1.3.3.5.2">𝑀</ci><ci id="S3.E1.m1.7.7.1.1.3.3.5.3.cmml" xref="S3.E1.m1.7.7.1.1.3.3.5.3">𝑡</ci></apply><vector id="S3.E1.m1.7.7.1.1.3.3.3.4.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3"><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2"><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.1">¯</ci><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.2.2.2">ℎ</ci></apply><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">𝑡</ci></apply><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.E1.m1.7.7.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2"><ci id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.1">¯</ci><ci id="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.2.2.2">ℎ</ci></apply><ci id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1">𝑡</ci></apply><ci id="S3.E1.m1.7.7.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.7.7.1.1.2.2.2.2.2.3">𝑢</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.3.3.3.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3">subscript</csymbol><apply id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2"><ci id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.1">¯</ci><ci id="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.3.3.3.3.2.2">𝑒</ci></apply><list id="S3.E1.m1.6.6.2.3.cmml" xref="S3.E1.m1.6.6.2.4"><ci id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1">𝑣</ci><ci id="S3.E1.m1.6.6.2.2.cmml" xref="S3.E1.m1.6.6.2.2">𝑢</ci></list></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\bar{m}^{(t+1)}_{v}=\sum_{u\in\mathcal{N}(v)}M_{t}(\bar{h}^{(t)}_{v},\bar{h}^{(t)}_{u},\bar{e}_{v,u})\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.7" class="ltx_p">where a message for a node at the next time step <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bar{m}^{(t+1)}_{v}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msubsup id="S3.SS1.p3.1.m1.1.2" xref="S3.SS1.p3.1.m1.1.2.cmml"><mover accent="true" id="S3.SS1.p3.1.m1.1.2.2.2" xref="S3.SS1.p3.1.m1.1.2.2.2.cmml"><mi id="S3.SS1.p3.1.m1.1.2.2.2.2" xref="S3.SS1.p3.1.m1.1.2.2.2.2.cmml">m</mi><mo id="S3.SS1.p3.1.m1.1.2.2.2.1" xref="S3.SS1.p3.1.m1.1.2.2.2.1.cmml">¯</mo></mover><mi id="S3.SS1.p3.1.m1.1.2.3" xref="S3.SS1.p3.1.m1.1.2.3.cmml">v</mi><mrow id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml">t</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2">subscript</csymbol><apply id="S3.SS1.p3.1.m1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.2.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2">superscript</csymbol><apply id="S3.SS1.p3.1.m1.1.2.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2.2.2"><ci id="S3.SS1.p3.1.m1.1.2.2.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2.2.2.1">¯</ci><ci id="S3.SS1.p3.1.m1.1.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2.2.2.2">𝑚</ci></apply><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><plus id="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.SS1.p3.1.m1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.2.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\bar{m}^{(t+1)}_{v}</annotation></semantics></math> is given by combining its current hidden state <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\bar{h}^{(t)}_{v}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msubsup id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mover accent="true" id="S3.SS1.p3.2.m2.1.2.2.2" xref="S3.SS1.p3.2.m2.1.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.1.2.2.2.2" xref="S3.SS1.p3.2.m2.1.2.2.2.2.cmml">h</mi><mo id="S3.SS1.p3.2.m2.1.2.2.2.1" xref="S3.SS1.p3.2.m2.1.2.2.2.1.cmml">¯</mo></mover><mi id="S3.SS1.p3.2.m2.1.2.3" xref="S3.SS1.p3.2.m2.1.2.3.cmml">v</mi><mrow id="S3.SS1.p3.2.m2.1.1.1.3" xref="S3.SS1.p3.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.3.1" xref="S3.SS1.p3.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S3.SS1.p3.2.m2.1.1.1.3.2" xref="S3.SS1.p3.2.m2.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">subscript</csymbol><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">superscript</csymbol><apply id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2"><ci id="S3.SS1.p3.2.m2.1.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2.1">¯</ci><ci id="S3.SS1.p3.2.m2.1.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2.2">ℎ</ci></apply><ci id="S3.SS1.p3.2.m2.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1">𝑡</ci></apply><ci id="S3.SS1.p3.2.m2.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bar{h}^{(t)}_{v}</annotation></semantics></math> with that of its neighbour <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\bar{h}^{(t)}_{u}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msubsup id="S3.SS1.p3.3.m3.1.2" xref="S3.SS1.p3.3.m3.1.2.cmml"><mover accent="true" id="S3.SS1.p3.3.m3.1.2.2.2" xref="S3.SS1.p3.3.m3.1.2.2.2.cmml"><mi id="S3.SS1.p3.3.m3.1.2.2.2.2" xref="S3.SS1.p3.3.m3.1.2.2.2.2.cmml">h</mi><mo id="S3.SS1.p3.3.m3.1.2.2.2.1" xref="S3.SS1.p3.3.m3.1.2.2.2.1.cmml">¯</mo></mover><mi id="S3.SS1.p3.3.m3.1.2.3" xref="S3.SS1.p3.3.m3.1.2.3.cmml">u</mi><mrow id="S3.SS1.p3.3.m3.1.1.1.3" xref="S3.SS1.p3.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.3.m3.1.1.1.3.1" xref="S3.SS1.p3.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.p3.3.m3.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S3.SS1.p3.3.m3.1.1.1.3.2" xref="S3.SS1.p3.3.m3.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.2.cmml" xref="S3.SS1.p3.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.2.1.cmml" xref="S3.SS1.p3.3.m3.1.2">subscript</csymbol><apply id="S3.SS1.p3.3.m3.1.2.2.cmml" xref="S3.SS1.p3.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.2.2.1.cmml" xref="S3.SS1.p3.3.m3.1.2">superscript</csymbol><apply id="S3.SS1.p3.3.m3.1.2.2.2.cmml" xref="S3.SS1.p3.3.m3.1.2.2.2"><ci id="S3.SS1.p3.3.m3.1.2.2.2.1.cmml" xref="S3.SS1.p3.3.m3.1.2.2.2.1">¯</ci><ci id="S3.SS1.p3.3.m3.1.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.1.2.2.2.2">ℎ</ci></apply><ci id="S3.SS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1">𝑡</ci></apply><ci id="S3.SS1.p3.3.m3.1.2.3.cmml" xref="S3.SS1.p3.3.m3.1.2.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\bar{h}^{(t)}_{u}</annotation></semantics></math> and any edge feature <math id="S3.SS1.p3.4.m4.2" class="ltx_Math" alttext="\bar{e}_{v,u}" display="inline"><semantics id="S3.SS1.p3.4.m4.2a"><msub id="S3.SS1.p3.4.m4.2.3" xref="S3.SS1.p3.4.m4.2.3.cmml"><mover accent="true" id="S3.SS1.p3.4.m4.2.3.2" xref="S3.SS1.p3.4.m4.2.3.2.cmml"><mi id="S3.SS1.p3.4.m4.2.3.2.2" xref="S3.SS1.p3.4.m4.2.3.2.2.cmml">e</mi><mo id="S3.SS1.p3.4.m4.2.3.2.1" xref="S3.SS1.p3.4.m4.2.3.2.1.cmml">¯</mo></mover><mrow id="S3.SS1.p3.4.m4.2.2.2.4" xref="S3.SS1.p3.4.m4.2.2.2.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.cmml">v</mi><mo id="S3.SS1.p3.4.m4.2.2.2.4.1" xref="S3.SS1.p3.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p3.4.m4.2.2.2.2" xref="S3.SS1.p3.4.m4.2.2.2.2.cmml">u</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.2b"><apply id="S3.SS1.p3.4.m4.2.3.cmml" xref="S3.SS1.p3.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.2.3.1.cmml" xref="S3.SS1.p3.4.m4.2.3">subscript</csymbol><apply id="S3.SS1.p3.4.m4.2.3.2.cmml" xref="S3.SS1.p3.4.m4.2.3.2"><ci id="S3.SS1.p3.4.m4.2.3.2.1.cmml" xref="S3.SS1.p3.4.m4.2.3.2.1">¯</ci><ci id="S3.SS1.p3.4.m4.2.3.2.2.cmml" xref="S3.SS1.p3.4.m4.2.3.2.2">𝑒</ci></apply><list id="S3.SS1.p3.4.m4.2.2.2.3.cmml" xref="S3.SS1.p3.4.m4.2.2.2.4"><ci id="S3.SS1.p3.4.m4.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1">𝑣</ci><ci id="S3.SS1.p3.4.m4.2.2.2.2.cmml" xref="S3.SS1.p3.4.m4.2.2.2.2">𝑢</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.2c">\bar{e}_{v,u}</annotation></semantics></math> in a multilayer perceptron (MLP) <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="M_{t}(\cdot)" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mrow id="S3.SS1.p3.5.m5.1.2" xref="S3.SS1.p3.5.m5.1.2.cmml"><msub id="S3.SS1.p3.5.m5.1.2.2" xref="S3.SS1.p3.5.m5.1.2.2.cmml"><mi id="S3.SS1.p3.5.m5.1.2.2.2" xref="S3.SS1.p3.5.m5.1.2.2.2.cmml">M</mi><mi id="S3.SS1.p3.5.m5.1.2.2.3" xref="S3.SS1.p3.5.m5.1.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m5.1.2.1" xref="S3.SS1.p3.5.m5.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.5.m5.1.2.3.2" xref="S3.SS1.p3.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.5.m5.1.2.3.2.1" xref="S3.SS1.p3.5.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.5.m5.1.2.3.2.2" xref="S3.SS1.p3.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.2.cmml" xref="S3.SS1.p3.5.m5.1.2"><times id="S3.SS1.p3.5.m5.1.2.1.cmml" xref="S3.SS1.p3.5.m5.1.2.1"></times><apply id="S3.SS1.p3.5.m5.1.2.2.cmml" xref="S3.SS1.p3.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.2.2.1.cmml" xref="S3.SS1.p3.5.m5.1.2.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.2.2.2.cmml" xref="S3.SS1.p3.5.m5.1.2.2.2">𝑀</ci><ci id="S3.SS1.p3.5.m5.1.2.2.3.cmml" xref="S3.SS1.p3.5.m5.1.2.2.3">𝑡</ci></apply><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">M_{t}(\cdot)</annotation></semantics></math>. Given that a message is an aggregation of all the connected nodes, the summation acts over the nodes connected to the node <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="u\in\mathcal{N}(v)" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.2" xref="S3.SS1.p3.6.m6.1.2.cmml"><mi id="S3.SS1.p3.6.m6.1.2.2" xref="S3.SS1.p3.6.m6.1.2.2.cmml">u</mi><mo id="S3.SS1.p3.6.m6.1.2.1" xref="S3.SS1.p3.6.m6.1.2.1.cmml">∈</mo><mrow id="S3.SS1.p3.6.m6.1.2.3" xref="S3.SS1.p3.6.m6.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.6.m6.1.2.3.2" xref="S3.SS1.p3.6.m6.1.2.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.m6.1.2.3.1" xref="S3.SS1.p3.6.m6.1.2.3.1.cmml">​</mo><mrow id="S3.SS1.p3.6.m6.1.2.3.3.2" xref="S3.SS1.p3.6.m6.1.2.3.cmml"><mo stretchy="false" id="S3.SS1.p3.6.m6.1.2.3.3.2.1" xref="S3.SS1.p3.6.m6.1.2.3.cmml">(</mo><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">v</mi><mo stretchy="false" id="S3.SS1.p3.6.m6.1.2.3.3.2.2" xref="S3.SS1.p3.6.m6.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.2.cmml" xref="S3.SS1.p3.6.m6.1.2"><in id="S3.SS1.p3.6.m6.1.2.1.cmml" xref="S3.SS1.p3.6.m6.1.2.1"></in><ci id="S3.SS1.p3.6.m6.1.2.2.cmml" xref="S3.SS1.p3.6.m6.1.2.2">𝑢</ci><apply id="S3.SS1.p3.6.m6.1.2.3.cmml" xref="S3.SS1.p3.6.m6.1.2.3"><times id="S3.SS1.p3.6.m6.1.2.3.1.cmml" xref="S3.SS1.p3.6.m6.1.2.3.1"></times><ci id="S3.SS1.p3.6.m6.1.2.3.2.cmml" xref="S3.SS1.p3.6.m6.1.2.3.2">𝒩</ci><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">u\in\mathcal{N}(v)</annotation></semantics></math>, i.e., the neighbourhood of <math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mi id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><ci id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">v</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">These messages are then used to update the hidden vectors by combining the node current state with the message in an MLP <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="U_{t}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">U</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝑈</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">U_{t}</annotation></semantics></math>.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\bar{h}^{(t+1)}_{v}=U_{t}(\bar{h}^{t}_{v},\bar{m}^{(t+1)}_{v})" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msubsup id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mover accent="true" id="S3.E2.m1.4.4.4.2.2" xref="S3.E2.m1.4.4.4.2.2.cmml"><mi id="S3.E2.m1.4.4.4.2.2.2" xref="S3.E2.m1.4.4.4.2.2.2.cmml">h</mi><mo id="S3.E2.m1.4.4.4.2.2.1" xref="S3.E2.m1.4.4.4.2.2.1.cmml">¯</mo></mover><mi id="S3.E2.m1.4.4.4.3" xref="S3.E2.m1.4.4.4.3.cmml">v</mi><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">t</mi><mo id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><msub id="S3.E2.m1.4.4.2.4" xref="S3.E2.m1.4.4.2.4.cmml"><mi id="S3.E2.m1.4.4.2.4.2" xref="S3.E2.m1.4.4.2.4.2.cmml">U</mi><mi id="S3.E2.m1.4.4.2.4.3" xref="S3.E2.m1.4.4.2.4.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.3" xref="S3.E2.m1.4.4.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.3.3.1.1.1.1.2.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml">h</mi><mo id="S3.E2.m1.3.3.1.1.1.1.2.2.1" xref="S3.E2.m1.3.3.1.1.1.1.2.2.1.cmml">¯</mo></mover><mi id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">v</mi><mi id="S3.E2.m1.3.3.1.1.1.1.2.3" xref="S3.E2.m1.3.3.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.E2.m1.4.4.2.2.2.4" xref="S3.E2.m1.4.4.2.2.3.cmml">,</mo><msubsup id="S3.E2.m1.4.4.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.cmml"><mover accent="true" id="S3.E2.m1.4.4.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.4.4.2.2.2.2.2.2.2" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2.cmml">m</mi><mo id="S3.E2.m1.4.4.2.2.2.2.2.2.1" xref="S3.E2.m1.4.4.2.2.2.2.2.2.1.cmml">¯</mo></mover><mi id="S3.E2.m1.4.4.2.2.2.2.3" xref="S3.E2.m1.4.4.2.2.2.2.3.cmml">v</mi><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">t</mi><mo id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml">+</mo><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.E2.m1.4.4.2.2.2.5" xref="S3.E2.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"></eq><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4">subscript</csymbol><apply id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.1.cmml" xref="S3.E2.m1.4.4.4">superscript</csymbol><apply id="S3.E2.m1.4.4.4.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2"><ci id="S3.E2.m1.4.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.4.2.2.1">¯</ci><ci id="S3.E2.m1.4.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2">ℎ</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></plus><ci id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.E2.m1.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.3">𝑣</ci></apply><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><times id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"></times><apply id="S3.E2.m1.4.4.2.4.cmml" xref="S3.E2.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.4.1.cmml" xref="S3.E2.m1.4.4.2.4">subscript</csymbol><ci id="S3.E2.m1.4.4.2.4.2.cmml" xref="S3.E2.m1.4.4.2.4.2">𝑈</ci><ci id="S3.E2.m1.4.4.2.4.3.cmml" xref="S3.E2.m1.4.4.2.4.3">𝑡</ci></apply><interval closure="open" id="S3.E2.m1.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2"><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.1">¯</ci><ci id="S3.E2.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.2.2">ℎ</ci></apply><ci id="S3.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.E2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.3">𝑣</ci></apply><apply id="S3.E2.m1.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2">subscript</csymbol><apply id="S3.E2.m1.4.4.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2"><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.1">¯</ci><ci id="S3.E2.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.2.2.2.2.2.2.2">𝑚</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><plus id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"></plus><ci id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2">𝑡</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">1</cn></apply></apply><ci id="S3.E2.m1.4.4.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.2.2.2.2.3">𝑣</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\bar{h}^{(t+1)}_{v}=U_{t}(\bar{h}^{t}_{v},\bar{m}^{(t+1)}_{v})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p4.3" class="ltx_p">Once the message passing phase has run for <math id="S3.SS1.p4.2.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p4.2.m1.1a"><mi id="S3.SS1.p4.2.m1.1.1" xref="S3.SS1.p4.2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m1.1b"><ci id="S3.SS1.p4.2.m1.1.1.cmml" xref="S3.SS1.p4.2.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m1.1c">T</annotation></semantics></math> time steps, a readout phase is then conducted using a readout function, <math id="S3.SS1.p4.3.m2.1" class="ltx_Math" alttext="R(\cdot)" display="inline"><semantics id="S3.SS1.p4.3.m2.1a"><mrow id="S3.SS1.p4.3.m2.1.2" xref="S3.SS1.p4.3.m2.1.2.cmml"><mi id="S3.SS1.p4.3.m2.1.2.2" xref="S3.SS1.p4.3.m2.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m2.1.2.1" xref="S3.SS1.p4.3.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.p4.3.m2.1.2.3.2" xref="S3.SS1.p4.3.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m2.1.2.3.2.1" xref="S3.SS1.p4.3.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m2.1.1" xref="S3.SS1.p4.3.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p4.3.m2.1.2.3.2.2" xref="S3.SS1.p4.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m2.1b"><apply id="S3.SS1.p4.3.m2.1.2.cmml" xref="S3.SS1.p4.3.m2.1.2"><times id="S3.SS1.p4.3.m2.1.2.1.cmml" xref="S3.SS1.p4.3.m2.1.2.1"></times><ci id="S3.SS1.p4.3.m2.1.2.2.cmml" xref="S3.SS1.p4.3.m2.1.2.2">𝑅</ci><ci id="S3.SS1.p4.3.m2.1.1.cmml" xref="S3.SS1.p4.3.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m2.1c">R(\cdot)</annotation></semantics></math>. This stage makes use of an MLP that considers the updated feature vectors of nodes on the graph to produce a prediction and is defined as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\hat{y}=R(\{\bar{h}^{T}_{v}|\bar{v}\in G\})" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">y</mi><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">^</mo></mover><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">{</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">h</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">¯</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">v</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">|</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2.2.cmml">v</mi><mo id="S3.E3.m1.1.1.1.1.1.1.2.2.2.1" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2.1.cmml">¯</mo></mover><mo id="S3.E3.m1.1.1.1.1.1.1.2.2.1" xref="S3.E3.m1.1.1.1.1.1.1.2.2.1.cmml">∈</mo><mi id="S3.E3.m1.1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.2.2.3.cmml">G</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.2.5" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">}</mo></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><ci id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1">^</ci><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝑦</ci></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝑅</ci><apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.3">conditional-set</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1">¯</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2">ℎ</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2"><in id="S3.E3.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2.1"></in><apply id="S3.E3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2"><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2.1">¯</ci><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2.2.2">𝑣</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2.2.3">𝐺</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\hat{y}=R(\{\bar{h}^{T}_{v}|\bar{v}\in G\})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">In order to make the GCN architecture scale to large graphs, the GraphSAGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> architecture changes the message function. Rather than taking messages from the entire neighbourhood of a node, a random sample is used. This reduces the number of messages that require processing, resulting in an architecture that works well on large graphs.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Gated Graph Neural Networks</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The core idea behind the Gated Graph Neural Network (GGNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> is to replace the update function from the message passing architecture (Equation <a href="#S3.E2" title="In III-A Graph Convolutional Networks (GCNs) ‣ III An Overview of Graph Neural Networks ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) with a Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. The GRU is a recurrent neural network with a update and reset gates that controls which data can flow through the network (and be retained) and which data cannot (and therefore be forgotten).</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.5" class="ltx_Math" alttext="\bar{h}^{(t+1)}_{v}=GRU(\bar{h}^{(t)}_{v},\sum_{w\in\mathcal{N}(v)}\mathbf{W}\bar{h}^{(t)}_{w})\,." display="block"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.5.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1" xref="S3.E4.m1.5.5.1.1.cmml"><msubsup id="S3.E4.m1.5.5.1.1.4" xref="S3.E4.m1.5.5.1.1.4.cmml"><mover accent="true" id="S3.E4.m1.5.5.1.1.4.2.2" xref="S3.E4.m1.5.5.1.1.4.2.2.cmml"><mi id="S3.E4.m1.5.5.1.1.4.2.2.2" xref="S3.E4.m1.5.5.1.1.4.2.2.2.cmml">h</mi><mo id="S3.E4.m1.5.5.1.1.4.2.2.1" xref="S3.E4.m1.5.5.1.1.4.2.2.1.cmml">¯</mo></mover><mi id="S3.E4.m1.5.5.1.1.4.3" xref="S3.E4.m1.5.5.1.1.4.3.cmml">v</mi><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml">t</mi><mo id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E4.m1.5.5.1.1.3" xref="S3.E4.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.5.5.1.1.2" xref="S3.E4.m1.5.5.1.1.2.cmml"><mi id="S3.E4.m1.5.5.1.1.2.4" xref="S3.E4.m1.5.5.1.1.2.4.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.3" xref="S3.E4.m1.5.5.1.1.2.3.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.2.5" xref="S3.E4.m1.5.5.1.1.2.5.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.3a" xref="S3.E4.m1.5.5.1.1.2.3.cmml">​</mo><mi id="S3.E4.m1.5.5.1.1.2.6" xref="S3.E4.m1.5.5.1.1.2.6.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.3b" xref="S3.E4.m1.5.5.1.1.2.3.cmml">​</mo><mrow id="S3.E4.m1.5.5.1.1.2.2.2" xref="S3.E4.m1.5.5.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.2.2.2.3" xref="S3.E4.m1.5.5.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E4.m1.5.5.1.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.cmml">h</mi><mo id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.1.cmml">¯</mo></mover><mi id="S3.E4.m1.5.5.1.1.1.1.1.1.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml">v</mi><mrow id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.3.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml">t</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo rspace="0em" id="S3.E4.m1.5.5.1.1.2.2.2.4" xref="S3.E4.m1.5.5.1.1.2.2.3.cmml">,</mo><mrow id="S3.E4.m1.5.5.1.1.2.2.2.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.cmml"><munder id="S3.E4.m1.5.5.1.1.2.2.2.2.1" xref="S3.E4.m1.5.5.1.1.2.2.2.2.1.cmml"><mo movablelimits="false" id="S3.E4.m1.5.5.1.1.2.2.2.2.1.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.1.2.cmml">∑</mo><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.cmml"><mi id="S3.E4.m1.3.3.1.3" xref="S3.E4.m1.3.3.1.3.cmml">w</mi><mo id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.2.cmml">∈</mo><mrow id="S3.E4.m1.3.3.1.4" xref="S3.E4.m1.3.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.3.3.1.4.2" xref="S3.E4.m1.3.3.1.4.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.4.1" xref="S3.E4.m1.3.3.1.4.1.cmml">​</mo><mrow id="S3.E4.m1.3.3.1.4.3.2" xref="S3.E4.m1.3.3.1.4.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.4.3.2.1" xref="S3.E4.m1.3.3.1.4.cmml">(</mo><mi id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml">v</mi><mo stretchy="false" id="S3.E4.m1.3.3.1.4.3.2.2" xref="S3.E4.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S3.E4.m1.5.5.1.1.2.2.2.2.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.cmml"><mi id="S3.E4.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.2.cmml">𝐖</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.2.2.2.2.2.1" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.1.cmml">​</mo><msubsup id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.cmml"><mover accent="true" id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.cmml"><mi id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.2.cmml">h</mi><mo id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.1" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.1.cmml">¯</mo></mover><mi id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.3" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.3.cmml">w</mi><mrow id="S3.E4.m1.4.4.1.3" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.1.3.1" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.cmml">(</mo><mi id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml">t</mi><mo stretchy="false" id="S3.E4.m1.4.4.1.3.2" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow><mo stretchy="false" id="S3.E4.m1.5.5.1.1.2.2.2.5" xref="S3.E4.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0.170em" id="S3.E4.m1.5.5.1.2" xref="S3.E4.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.5.5.1"><eq id="S3.E4.m1.5.5.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.3"></eq><apply id="S3.E4.m1.5.5.1.1.4.cmml" xref="S3.E4.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.4.1.cmml" xref="S3.E4.m1.5.5.1.1.4">subscript</csymbol><apply id="S3.E4.m1.5.5.1.1.4.2.cmml" xref="S3.E4.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.4.2.1.cmml" xref="S3.E4.m1.5.5.1.1.4">superscript</csymbol><apply id="S3.E4.m1.5.5.1.1.4.2.2.cmml" xref="S3.E4.m1.5.5.1.1.4.2.2"><ci id="S3.E4.m1.5.5.1.1.4.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.4.2.2.1">¯</ci><ci id="S3.E4.m1.5.5.1.1.4.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.4.2.2.2">ℎ</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><plus id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"></plus><ci id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.E4.m1.5.5.1.1.4.3.cmml" xref="S3.E4.m1.5.5.1.1.4.3">𝑣</ci></apply><apply id="S3.E4.m1.5.5.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.2"><times id="S3.E4.m1.5.5.1.1.2.3.cmml" xref="S3.E4.m1.5.5.1.1.2.3"></times><ci id="S3.E4.m1.5.5.1.1.2.4.cmml" xref="S3.E4.m1.5.5.1.1.2.4">𝐺</ci><ci id="S3.E4.m1.5.5.1.1.2.5.cmml" xref="S3.E4.m1.5.5.1.1.2.5">𝑅</ci><ci id="S3.E4.m1.5.5.1.1.2.6.cmml" xref="S3.E4.m1.5.5.1.1.2.6">𝑈</ci><interval closure="open" id="S3.E4.m1.5.5.1.1.2.2.3.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2"><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2"><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.1">¯</ci><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.2.2.2">ℎ</ci></apply><ci id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1">𝑡</ci></apply><ci id="S3.E4.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2"><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.1">subscript</csymbol><sum id="S3.E4.m1.5.5.1.1.2.2.2.2.1.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.1.2"></sum><apply id="S3.E4.m1.3.3.1.cmml" xref="S3.E4.m1.3.3.1"><in id="S3.E4.m1.3.3.1.2.cmml" xref="S3.E4.m1.3.3.1.2"></in><ci id="S3.E4.m1.3.3.1.3.cmml" xref="S3.E4.m1.3.3.1.3">𝑤</ci><apply id="S3.E4.m1.3.3.1.4.cmml" xref="S3.E4.m1.3.3.1.4"><times id="S3.E4.m1.3.3.1.4.1.cmml" xref="S3.E4.m1.3.3.1.4.1"></times><ci id="S3.E4.m1.3.3.1.4.2.cmml" xref="S3.E4.m1.3.3.1.4.2">𝒩</ci><ci id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1">𝑣</ci></apply></apply></apply><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2"><times id="S3.E4.m1.5.5.1.1.2.2.2.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.1"></times><ci id="S3.E4.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.2">𝐖</ci><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3">subscript</csymbol><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3">superscript</csymbol><apply id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2"><ci id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.1.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.1">¯</ci><ci id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.2.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.2.2.2">ℎ</ci></apply><ci id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1.1">𝑡</ci></apply><ci id="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.5.5.1.1.2.2.2.2.2.3.3">𝑤</ci></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">\bar{h}^{(t+1)}_{v}=GRU(\bar{h}^{(t)}_{v},\sum_{w\in\mathcal{N}(v)}\mathbf{W}\bar{h}^{(t)}_{w})\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.2" class="ltx_p">The GGNN also replaces the message function from Equation <a href="#S3.E1" title="In III-A Graph Convolutional Networks (GCNs) ‣ III An Overview of Graph Neural Networks ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with a learnable weight matrix. Using the GRU alongside back-propagation through time enables the GGNN to operate on series data. However, due to the recurrent nature of the architecture, it can become unfeasible in terms of memory to run the GGNN on large graphs.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Graph Attention Networks (GATs)</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Following on from the multi-head attention mechanism of the popular Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, Graph Attention Networks (GATs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> extend the common GCN to include this attention attribute. Using an attention function, typically modelled by an MLP, the architecture calculates an attention weighting between two nodes. This process is repeated <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">K</annotation></semantics></math> times using <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">K</annotation></semantics></math> attention heads in parallel. The attention scores are then averaged to give the final weights.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.4" class="ltx_p">The self-attention is computed by a function <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="a(h^{t}_{v},h^{t}_{w})" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.4" xref="S3.SS3.p2.1.m1.2.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.2.2.3" xref="S3.SS3.p2.1.m1.2.2.3.cmml">​</mo><mrow id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">(</mo><msubsup id="S3.SS3.p2.1.m1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.2.cmml">h</mi><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml">v</mi><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S3.SS3.p2.1.m1.2.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.p2.1.m1.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.2.cmml">h</mi><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml">w</mi><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.3.cmml">t</mi></msubsup><mo stretchy="false" id="S3.SS3.p2.1.m1.2.2.2.2.5" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2"><times id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.3"></times><ci id="S3.SS3.p2.1.m1.2.2.4.cmml" xref="S3.SS3.p2.1.m1.2.2.4">𝑎</ci><interval closure="open" id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.2">ℎ</ci><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2">subscript</csymbol><apply id="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.2">ℎ</ci><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.3">𝑡</ci></apply><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3">𝑤</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">a(h^{t}_{v},h^{t}_{w})</annotation></semantics></math> (typically an MLP) that attends to a node and one of its neighbours. Once every node pairing in the graph has their attention computed, the scores are passed through a softmax function to give a normalised attention coefficient. This process is then extended to multi-head attention by repeating the process across <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">K</annotation></semantics></math> different attention heads, each with different initialisation weights. The final node representation is achieved by concatenating or averaging (represented as <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\|" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mo id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\|</annotation></semantics></math>) the <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">K</annotation></semantics></math> attention heads together.</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.6" class="ltx_math_unparsed" alttext="\bar{h}_{v}^{(t+1)}=\Bigg{\|}^{K}_{k=1}\sigma(\sum_{w\in\mathcal{N}(v)}\alpha_{v,w}^{(k)}\mathbf{W}^{(k)}\bar{h}_{w})" display="block"><semantics id="S3.E5.m1.6a"><mrow id="S3.E5.m1.6b"><msubsup id="S3.E5.m1.6.7"><mover accent="true" id="S3.E5.m1.6.7.2.2"><mi id="S3.E5.m1.6.7.2.2.2">h</mi><mo id="S3.E5.m1.6.7.2.2.1">¯</mo></mover><mi id="S3.E5.m1.6.7.2.3">v</mi><mrow id="S3.E5.m1.1.1.1.1"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.2">(</mo><mrow id="S3.E5.m1.1.1.1.1.1"><mi id="S3.E5.m1.1.1.1.1.1.2">t</mi><mo id="S3.E5.m1.1.1.1.1.1.1">+</mo><mn id="S3.E5.m1.1.1.1.1.1.3">1</mn></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.3">)</mo></mrow></msubsup><mo rspace="0em" id="S3.E5.m1.6.8">=</mo><msubsup id="S3.E5.m1.6.9"><mo lspace="0em" mathsize="260%" rspace="0.167em" id="S3.E5.m1.6.9.2.2">∥</mo><mrow id="S3.E5.m1.6.9.3"><mi id="S3.E5.m1.6.9.3.2">k</mi><mo id="S3.E5.m1.6.9.3.1">=</mo><mn id="S3.E5.m1.6.9.3.3">1</mn></mrow><mi id="S3.E5.m1.6.9.2.3">K</mi></msubsup><mi id="S3.E5.m1.6.10">σ</mi><mrow id="S3.E5.m1.6.11"><mo stretchy="false" id="S3.E5.m1.6.11.1">(</mo><munder id="S3.E5.m1.6.11.2"><mo lspace="0em" movablelimits="false" id="S3.E5.m1.6.11.2.2">∑</mo><mrow id="S3.E5.m1.2.2.1"><mi id="S3.E5.m1.2.2.1.3">w</mi><mo id="S3.E5.m1.2.2.1.2">∈</mo><mrow id="S3.E5.m1.2.2.1.4"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.2.2.1.4.2">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.4.1">​</mo><mrow id="S3.E5.m1.2.2.1.4.3.2"><mo stretchy="false" id="S3.E5.m1.2.2.1.4.3.2.1">(</mo><mi id="S3.E5.m1.2.2.1.1">v</mi><mo stretchy="false" id="S3.E5.m1.2.2.1.4.3.2.2">)</mo></mrow></mrow></mrow></munder><msubsup id="S3.E5.m1.6.11.3"><mi id="S3.E5.m1.6.11.3.2.2">α</mi><mrow id="S3.E5.m1.4.4.2.4"><mi id="S3.E5.m1.3.3.1.1">v</mi><mo id="S3.E5.m1.4.4.2.4.1">,</mo><mi id="S3.E5.m1.4.4.2.2">w</mi></mrow><mrow id="S3.E5.m1.5.5.1.3"><mo stretchy="false" id="S3.E5.m1.5.5.1.3.1">(</mo><mi id="S3.E5.m1.5.5.1.1">k</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.3.2">)</mo></mrow></msubsup><msup id="S3.E5.m1.6.11.4"><mi id="S3.E5.m1.6.11.4.2">𝐖</mi><mrow id="S3.E5.m1.6.6.1.3"><mo stretchy="false" id="S3.E5.m1.6.6.1.3.1">(</mo><mi id="S3.E5.m1.6.6.1.1">k</mi><mo stretchy="false" id="S3.E5.m1.6.6.1.3.2">)</mo></mrow></msup><msub id="S3.E5.m1.6.11.5"><mover accent="true" id="S3.E5.m1.6.11.5.2"><mi id="S3.E5.m1.6.11.5.2.2">h</mi><mo id="S3.E5.m1.6.11.5.2.1">¯</mo></mover><mi id="S3.E5.m1.6.11.5.3">w</mi></msub><mo stretchy="false" id="S3.E5.m1.6.11.6">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E5.m1.6c">\bar{h}_{v}^{(t+1)}=\Bigg{\|}^{K}_{k=1}\sigma(\sum_{w\in\mathcal{N}(v)}\alpha_{v,w}^{(k)}\mathbf{W}^{(k)}\bar{h}_{w})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Graph Memory Networks</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Recent years have seen the development of Graph Memory Networks, which can conceptually be thought of as models with an internal and external memory. When there are multiple graphs overlapping the same spatial information, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, the use of some form of external memory can allow for an aggregation of node updates and the graph undergoes message passing. This essentially allows for features from multiple graphs to be combined in some way that goes beyond a more simplistic pooling operation. In the case of Khademi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, two graphs are constructed across the same image but may have different nodes. These graphs are updated using a GGNN. An external spatial memory is constructed to aggregate information from across the graphs as they are updated, using a neural network with an attention mechanism. The final state of the spatial memory is used to perform the final task.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Modern Graph Neural Network Architectures</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In recent years, the limits of message passing GNNs have become increasingly evident, from their tendency to oversmooth the input features as the depth of the network increases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> to their unsatisfactory performance in heterophilic settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, i.e., when neighbouring nodes in the input graphs are dissimilar. Furthermore, the expressive power of GNNs based on the message passing mechanism has been shown to be bounded by that of the well-known Weisfeiler-Lehman isomorphism test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, meaning that there are inherent limits to their ability to generate different representations for structurally different input graphs.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Motivated by the desire to overcome these issues, researchers have now started looking at alternative models that move away from standard message passing architectures. Efforts in this direction include, among many others, higher-order message passing architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, cell complexes networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, networks based on diffusion processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. To the best of our knowledge, the application of these architectures to the 2D image understanding tasks discussed in this paper has not been explored yet. As such, we refer the readers to the referenced papers for detailed information on the respective architectures.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Image Captioning</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Image captioning is the challenging task of producing a natural language description of an image. Outside of being an interesting technical challenge, it presents an opportunity to develop accessibility technologies for severely sight impaired (formally ‘blind’) and sight impaired users (formally ‘visually impaired’ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The UK Department of Health and Social Care adopted the more inclusive phrasing around 2017</span></span></span>). Additionally, it has applications in problems ranging from image indexing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> to surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. There are three forms of image captioning techniques: 1) retrieval-based captioning, where a caption is retrieved from a set of existing captions, 2) template-based captioning, where a pre-existing template is filled in using information extracted from the image, 3) and Deep Learning-based image captioning, where a neural network is tasked with generating a caption from an input image. We propose to refine this taxonomy to differentiate between GNN-based approaches and more traditional Deep Learning powered image captioning. The following section details the GNN-based approaches to image captioning, of which there have been a number of in recent years. Figure <a href="#S4.F3" title="Figure 3 ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the structure of a generic GNN-based image captioning architecture.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2303.03761/assets/figures/abstract-captioning-architecture.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An abstract overview of GNN-based image captioning architectures discussed in this section. Most architectures extract image features and use them to construct at least one graph to represent the image. Some papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> build higher level graphs at an image level rather than an object level. A GNN is then applied to these graphs and the resulting features are fed into a language generator that creates an appropriate caption for the image. Traditionally this was an LSTM, but more recently the trend is to use Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Best viewed in colour.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">GNN-based approaches to image captioning all follow the traditional Encoder-Decoder-based approach common in Deep Learning image captioning techniques. Images first undergo object detection, the output of which is used to create an encoding. These encodings are then decoded, traditionally with a long short-term memory network (LSTM), into a caption. Through incorporating GNNs, researchers have been able to enhance the encoded image representation by incorporating spatial and semantic information into the embeddings.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">As the task of image captioning has developed over time, so have the evaluation metrics used to assess the performance of proposed architectures. Originally, image captioning relied heavily on machine translation evaluation techniques such as BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, and METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> as no image captioning specific metric existed. However, this changed with the introduction of both CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> and SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. The performance metrics are detailed in Table <a href="#S4.T2" title="TABLE II ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>A table detailing the different image captioning performance metrics</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:242.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.5pt,31.0pt) scale(0.796306277128636,0.796306277128636) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left">Metric</td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_left">Original Field</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_left">Based On</td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.2.1.4.1.1" class="ltx_p" style="width:227.6pt;">Description</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">BLEU</td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">Machine translation</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">Precision</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.3.2.4.1.1" class="ltx_p" style="width:227.6pt;">Based on a modified n-gram precision where the reference word is exhausted after a matching candidate word is identified. BLEU favours captions that are a similar in length to the reference caption.</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t">ROUGE</td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">Machine text summarisation</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">Recall</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.4.3.4.1.1" class="ltx_p" style="width:227.6pt;">Built with four components: an n-gram recall between the candidate and reference set, a comparison of the longest common sub-sequence, a comparison of the weighted longest common sub-sequence, and finally, the skip-bigram co-occurrence statistic.</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">METEOR</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">Machine translation</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="F_{mean}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">F</mi><mrow id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1a" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.4" xref="S4.T2.1.1.1.1.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1b" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.5" xref="S4.T2.1.1.1.1.m1.1.1.3.5.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝐹</ci><apply id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.2">𝑚</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3">𝑒</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.4.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.4">𝑎</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.5.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.5">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">F_{mean}</annotation></semantics></math></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.4.1.1" class="ltx_p" style="width:227.6pt;">Uses the harmonic mean of the precision and recall between candidate caption and reference captions.</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_t">CIDEr</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t">Image Captioning</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_t">n-grams</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.5.4.4.1.1" class="ltx_p" style="width:227.6pt;">The metric is based on a number of intuitions. Firstly, that if an n-gram is not present in the reference captions it should not appear in the candidate caption. Secondly, that it should encode how often n-grams present in the candidate captions are present. And finally, n-grams that occur across all the reference captions should be assigned a lower weighting as they will be things like articles and have little to no important information.</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">SPICE</td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">Image Captioning</td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">Textual Semantic Tree</td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S4.T2.1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.6.5.4.1.1" class="ltx_p" style="width:227.6pt;">Reference and candidate captions are converted into textual scene graphs to compare the semantic makeup of the captions.</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The first architecture to use a GNN to improve image captioning was by Yao <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. In their work, they propose the use of a GCN to improve the feature embeddings of objects in an image. They first start by applying a Faster R-CNN object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> to the image in order to extract feature vectors representing objects. These feature vectors are then used to create two graphs: a bidirectional spatial graph encoding spatial relationships between objects and a directed semantic graph which encodes the semantic relationships between objects. A GCN is then applied to both graphs before the enhanced features of the graphs undergo mean pooling. They are then decoded by an LSTM into a caption. As the whole graphs are used to inform the caption generation, it may lead to scenarios where dense graphs lead to redundant or low value information being included in the caption.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Zhong <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> focus solely on a semantic scene graph and address the problem of which nodes and edges to include in the final caption. This is challenging for scenes containing a lot of detected objects as the semantic scene graphs can become relatively large. The problem is addressed by decomposing the semantic graph into various subgraphs that cover various parts of the image. They are then scored using a function trained to determine how closely the subgraph resembles the ground truth caption. This enables the selection of subgraphs from the main scene graph that will go on to generate useful captions. The starting semantic graph is generated by MotifNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> (a common off-the-shelf semantic graph generator). Zhong <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> make use of a GCN to aggregate neighbourhood information of the proposed sub-graph. Unlike Yao <span id="S4.p5.1.3" class="ltx_text ltx_font_italic">et al.</span>, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> use only a semantic graph. They focus on the link between the language and semantic graph and do not make use of spatial information.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Another work that makes use of the semantic graph is that of Song <span id="S4.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. They investigate how both implicit and explicit features can be utilised to generate accurate and high quality image captions. The authors define implicit features as representing global interactions between objects and explicit features as those defined on a semantic graph. For the latter, rather than using multiple graphs, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> only uses a single semantic graph. However, rather than predicting the graph directly via MotifNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> as in other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, its construction starts with a spatial graph. After object detection, a fully connected directed graph is generated between the objects (with nodes being represented by the object feature vector). The edges of this graph are then whittled away in a two step process. Firstly, edges between objects that have zero overlap (measured as intersection over union) and an <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S4.p6.1.m1.1a"><msub id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">l</mi><mn id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1">subscript</csymbol><ci id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">l_{2}</annotation></semantics></math> distance less than the longest side of either objects bounding box are removed. The remaining edges are used to determine which object pairs have their relationship detected by MotifNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. Those relationships with a high enough probability are kept whilst the others are removed. This results in a semantic graph that indirectly contains spatial information, going beyond the semantic graph of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. The final graph is then processed by a GGNN, the output of which is a representation of the explicit features. The implicit features are generated by a Transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The entire image, alongside the regions within the detected object bounding boxes are encoded. These features are then used alongside those of the explicit features as input to an LSTM language decoder that is used to generate the final caption. The work demonstrates the successes possible when using GNNs alongside Transformers, using their different inductive biases to best model different interactions (see Table <a href="#S4.T3" title="TABLE III ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>). However, both the implicit and explicit relationships remain local to a single image. Further work could consider how often certain relationships occur over the entire dataset.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Guo <span id="S4.p7.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> took a very similar approach to Yao <span id="S4.p7.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> with their work, utilising a dual graph architecture containing a semantic and spatial graph. However, they make the observation that images can be represented by a collection of Visual Semantic Unit (VSU) vectors, which represent an object, its attributes, and its relationships. These VSUs are combined into a semantic graph that models relationships as nodes rather than edge features and adds attribute nodes conencted to objects, thus making it multipartite. Doing so gives the graph a closer resemblance to the captions it will go on to generate as objects map to nouns, relationships to verbs and prepositions, and finally attributes to adjectives. The authors argue that this approach allows the model to explicitly learn relationships and model them directly. As argued in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, a scene graph of an image has a close mapping to the image caption. Nodes representing objects map directly to nouns, edge features (in the case of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>) or nodes (in the case of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>) that encode relationships map clearly to prepositions, and nodes representing attributes map to adjectives. This strong relationship between the graph structure generated by the encoder and the final sentence outputted by the decoder further supports the use of the image-graph-sentence architecture used by many image captioning systems.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">Zhou <span id="S4.p8.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> use an LSTM alongside a Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> based image feature extractor, with the addition of a visual self-attention mechanism. The authors make use of a multipartite semantic scene graph, following the style of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. Specifically, they propose to use three GCNs to create context aware feature vectors for each of the object, attribute, and relationship nodes. The resulting context aware nodes undergo fusion with the self attention maps, enabling the model to control the granularity of captions. Finally, the authors test two methods of training an LSTM-based language generator, the first being a traditional supervised approach with cross entropy loss, the second being a reinforcement learning-based approach that uses CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> as the reward function. By utilising context dependent GCNs in their architecture, to specifically account for the object, attribute, and relationship nodes, SASG is able to achieve competitive results when compared with similar models, as shown in Table <a href="#S4.T3" title="TABLE III ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">SGAE (Scene Graph Auto-Encoder) is another paper to make use of a multipartite semantic graph. In the paper, Yang <span id="S4.p9.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> take a caption and convert it into a multipartite textual semantic graph using a similar process to that of the SPICE metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> (detailed further in Table <a href="#S4.T2" title="TABLE II ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>). The nodes of the graph are converted to word embeddings which are then converted into feature embeddings by way of a GCN, with each node type being given its own GCN with independent parameters. These feature embeddings are then combined with a dictionary to enable them to be re-encoded before they are used to generate a sentence. The dictionary weights are updated via back-propagating the cross entropy loss from the sentence regeneration. By including a dictionary, the authors are able to learn inductive biases from the captions. This allows generated captions to go from “man on motorcycle” to “man riding motorcycle”. When given an image, SGAE generates a multipartite visual semantic graph, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, using Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and MotifNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. These visual features are then combined with their word embeddings through a multi-modal GCN and then re-encoded using the previously learnt dictionary. These features are then used to generate the final sentence.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">Rather than utilising multiple graphs, Wang <span id="S4.p10.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> instead use a single fully connected spatial graph with an attention mechanism to learn the relationships between different regions. This graph is formed of nodes that represent the spatial information of regions within the image. Once formed, it is passed through a GGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> to learn the weights associated with the edges. Once learnt, these edge weights correspond to the probability of a relationship existing between the two nodes.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">The work of Yao <span id="S4.p11.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, following on from their GCN-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, presents an image encoder that makes use of a novel HIerarchy Parsing (HIP) architecture. Rather than encoding the image in a traditional scene graph structure like most contemporary image captioning papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, Yao <span id="S4.p11.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> take the novel approach of using a tree structure (discussed in Section <a href="#S2.SS3" title="II-C Common Graph Types in 2D vision-language Tasks ‣ II Background and Definitions ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>), exploiting the hierarchical nature of objects in images. Unlike their previous work which focused on the semantic and spatial relationships, this work is about the hierarchical structure within an image. This hierarchical relationship can be viewed as a combination of both semantic and spatial information - therefore merging the two graphs used previously. The feature vectors representing the vertices on the tree are then improved through the use of Tree-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>. As trees are a special case graph, the authors also demonstrate that their previous work GCN-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> can be used to to create enriched embeddings from the tree before decoding it with an LSTM. They demonstrate that the inclusion of the hierarchy passing improves scores on all benchmarks when compared with GCN-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, which does not use hierarchical relationships.</p>
</div>
<div id="S4.p12" class="ltx_para">
<p id="S4.p12.4" class="ltx_p">The work of He <span id="S4.p12.4.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> build on the idea of a hierarchical spatial relationships proposed by Yao <span id="S4.p12.4.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p12.4.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib68" title="" class="ltx_ref">68</a><span id="S4.p12.4.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span>. However, rather than use a tree to represent these relationships, they use a graph with three relationship types: parent, neighbour, and child. They then propose a modification to the popular Transformer layer to better adapt it to the task of image processing. After detecting objects using Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, a hierarchical spatial relationship graph is constructed. Three adjacency matrices are then built from this graph to model the three relationship types (<math id="S4.p12.1.m1.3" class="ltx_Math" alttext="\Omega_{p},\Omega_{n},\Omega_{c}" display="inline"><semantics id="S4.p12.1.m1.3a"><mrow id="S4.p12.1.m1.3.3.3" xref="S4.p12.1.m1.3.3.4.cmml"><msub id="S4.p12.1.m1.1.1.1.1" xref="S4.p12.1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S4.p12.1.m1.1.1.1.1.2" xref="S4.p12.1.m1.1.1.1.1.2.cmml">Ω</mi><mi id="S4.p12.1.m1.1.1.1.1.3" xref="S4.p12.1.m1.1.1.1.1.3.cmml">p</mi></msub><mo id="S4.p12.1.m1.3.3.3.4" xref="S4.p12.1.m1.3.3.4.cmml">,</mo><msub id="S4.p12.1.m1.2.2.2.2" xref="S4.p12.1.m1.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.p12.1.m1.2.2.2.2.2" xref="S4.p12.1.m1.2.2.2.2.2.cmml">Ω</mi><mi id="S4.p12.1.m1.2.2.2.2.3" xref="S4.p12.1.m1.2.2.2.2.3.cmml">n</mi></msub><mo id="S4.p12.1.m1.3.3.3.5" xref="S4.p12.1.m1.3.3.4.cmml">,</mo><msub id="S4.p12.1.m1.3.3.3.3" xref="S4.p12.1.m1.3.3.3.3.cmml"><mi mathvariant="normal" id="S4.p12.1.m1.3.3.3.3.2" xref="S4.p12.1.m1.3.3.3.3.2.cmml">Ω</mi><mi id="S4.p12.1.m1.3.3.3.3.3" xref="S4.p12.1.m1.3.3.3.3.3.cmml">c</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p12.1.m1.3b"><list id="S4.p12.1.m1.3.3.4.cmml" xref="S4.p12.1.m1.3.3.3"><apply id="S4.p12.1.m1.1.1.1.1.cmml" xref="S4.p12.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p12.1.m1.1.1.1.1.1.cmml" xref="S4.p12.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.p12.1.m1.1.1.1.1.2.cmml" xref="S4.p12.1.m1.1.1.1.1.2">Ω</ci><ci id="S4.p12.1.m1.1.1.1.1.3.cmml" xref="S4.p12.1.m1.1.1.1.1.3">𝑝</ci></apply><apply id="S4.p12.1.m1.2.2.2.2.cmml" xref="S4.p12.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.p12.1.m1.2.2.2.2.1.cmml" xref="S4.p12.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.p12.1.m1.2.2.2.2.2.cmml" xref="S4.p12.1.m1.2.2.2.2.2">Ω</ci><ci id="S4.p12.1.m1.2.2.2.2.3.cmml" xref="S4.p12.1.m1.2.2.2.2.3">𝑛</ci></apply><apply id="S4.p12.1.m1.3.3.3.3.cmml" xref="S4.p12.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S4.p12.1.m1.3.3.3.3.1.cmml" xref="S4.p12.1.m1.3.3.3.3">subscript</csymbol><ci id="S4.p12.1.m1.3.3.3.3.2.cmml" xref="S4.p12.1.m1.3.3.3.3.2">Ω</ci><ci id="S4.p12.1.m1.3.3.3.3.3.cmml" xref="S4.p12.1.m1.3.3.3.3.3">𝑐</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.1.m1.3c">\Omega_{p},\Omega_{n},\Omega_{c}</annotation></semantics></math> respectively). The authors modify the Transformer layer so that rather compute self-attention across the whole spatial graph, there is a sub-layer for each relationship type. Each sub-layer processes the query <math id="S4.p12.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p12.2.m2.1a"><mi id="S4.p12.2.m2.1.1" xref="S4.p12.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p12.2.m2.1b"><ci id="S4.p12.2.m2.1.1.cmml" xref="S4.p12.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.2.m2.1c">Q</annotation></semantics></math> with its own key <math id="S4.p12.3.m3.1" class="ltx_Math" alttext="K_{i}" display="inline"><semantics id="S4.p12.3.m3.1a"><msub id="S4.p12.3.m3.1.1" xref="S4.p12.3.m3.1.1.cmml"><mi id="S4.p12.3.m3.1.1.2" xref="S4.p12.3.m3.1.1.2.cmml">K</mi><mi id="S4.p12.3.m3.1.1.3" xref="S4.p12.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p12.3.m3.1b"><apply id="S4.p12.3.m3.1.1.cmml" xref="S4.p12.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p12.3.m3.1.1.1.cmml" xref="S4.p12.3.m3.1.1">subscript</csymbol><ci id="S4.p12.3.m3.1.1.2.cmml" xref="S4.p12.3.m3.1.1.2">𝐾</ci><ci id="S4.p12.3.m3.1.1.3.cmml" xref="S4.p12.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.3.m3.1c">K_{i}</annotation></semantics></math> and value <math id="S4.p12.4.m4.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S4.p12.4.m4.1a"><msub id="S4.p12.4.m4.1.1" xref="S4.p12.4.m4.1.1.cmml"><mi id="S4.p12.4.m4.1.1.2" xref="S4.p12.4.m4.1.1.2.cmml">V</mi><mi id="S4.p12.4.m4.1.1.3" xref="S4.p12.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p12.4.m4.1b"><apply id="S4.p12.4.m4.1.1.cmml" xref="S4.p12.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p12.4.m4.1.1.1.cmml" xref="S4.p12.4.m4.1.1">subscript</csymbol><ci id="S4.p12.4.m4.1.1.2.cmml" xref="S4.p12.4.m4.1.1.2">𝑉</ci><ci id="S4.p12.4.m4.1.1.3.cmml" xref="S4.p12.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.4.m4.1c">V_{i}</annotation></semantics></math> with the modified attention mechanism:</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.4" class="ltx_Math" alttext="Attention(Q,K_{i},V_{i})=\Omega_{i}\odot Softmax\left(\frac{QK_{i}^{T}}{\sqrt{d}}\right)V_{i}" display="block"><semantics id="S4.E6.m1.4a"><mrow id="S4.E6.m1.4.4" xref="S4.E6.m1.4.4.cmml"><mrow id="S4.E6.m1.4.4.2" xref="S4.E6.m1.4.4.2.cmml"><mi id="S4.E6.m1.4.4.2.4" xref="S4.E6.m1.4.4.2.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.5" xref="S4.E6.m1.4.4.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3a" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.6" xref="S4.E6.m1.4.4.2.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3b" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.7" xref="S4.E6.m1.4.4.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3c" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.8" xref="S4.E6.m1.4.4.2.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3d" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.9" xref="S4.E6.m1.4.4.2.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3e" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.10" xref="S4.E6.m1.4.4.2.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3f" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.11" xref="S4.E6.m1.4.4.2.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3g" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.12" xref="S4.E6.m1.4.4.2.12.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3h" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mrow id="S4.E6.m1.4.4.2.2.2" xref="S4.E6.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.3" xref="S4.E6.m1.4.4.2.2.3.cmml">(</mo><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">Q</mi><mo id="S4.E6.m1.4.4.2.2.2.4" xref="S4.E6.m1.4.4.2.2.3.cmml">,</mo><msub id="S4.E6.m1.3.3.1.1.1.1" xref="S4.E6.m1.3.3.1.1.1.1.cmml"><mi id="S4.E6.m1.3.3.1.1.1.1.2" xref="S4.E6.m1.3.3.1.1.1.1.2.cmml">K</mi><mi id="S4.E6.m1.3.3.1.1.1.1.3" xref="S4.E6.m1.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E6.m1.4.4.2.2.2.5" xref="S4.E6.m1.4.4.2.2.3.cmml">,</mo><msub id="S4.E6.m1.4.4.2.2.2.2" xref="S4.E6.m1.4.4.2.2.2.2.cmml"><mi id="S4.E6.m1.4.4.2.2.2.2.2" xref="S4.E6.m1.4.4.2.2.2.2.2.cmml">V</mi><mi id="S4.E6.m1.4.4.2.2.2.2.3" xref="S4.E6.m1.4.4.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.6" xref="S4.E6.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.4.4.3" xref="S4.E6.m1.4.4.3.cmml">=</mo><mrow id="S4.E6.m1.4.4.4" xref="S4.E6.m1.4.4.4.cmml"><mrow id="S4.E6.m1.4.4.4.2" xref="S4.E6.m1.4.4.4.2.cmml"><msub id="S4.E6.m1.4.4.4.2.2" xref="S4.E6.m1.4.4.4.2.2.cmml"><mi mathvariant="normal" id="S4.E6.m1.4.4.4.2.2.2" xref="S4.E6.m1.4.4.4.2.2.2.cmml">Ω</mi><mi id="S4.E6.m1.4.4.4.2.2.3" xref="S4.E6.m1.4.4.4.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E6.m1.4.4.4.2.1" xref="S4.E6.m1.4.4.4.2.1.cmml">⊙</mo><mi id="S4.E6.m1.4.4.4.2.3" xref="S4.E6.m1.4.4.4.2.3.cmml">S</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.3" xref="S4.E6.m1.4.4.4.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1a" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.4" xref="S4.E6.m1.4.4.4.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1b" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.5" xref="S4.E6.m1.4.4.4.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1c" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.6" xref="S4.E6.m1.4.4.4.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1d" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.7" xref="S4.E6.m1.4.4.4.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1e" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mi id="S4.E6.m1.4.4.4.8" xref="S4.E6.m1.4.4.4.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1f" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><mrow id="S4.E6.m1.4.4.4.9.2" xref="S4.E6.m1.2.2.cmml"><mo id="S4.E6.m1.4.4.4.9.2.1" xref="S4.E6.m1.2.2.cmml">(</mo><mfrac id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml"><mrow id="S4.E6.m1.2.2.2" xref="S4.E6.m1.2.2.2.cmml"><mi id="S4.E6.m1.2.2.2.2" xref="S4.E6.m1.2.2.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.1" xref="S4.E6.m1.2.2.2.1.cmml">​</mo><msubsup id="S4.E6.m1.2.2.2.3" xref="S4.E6.m1.2.2.2.3.cmml"><mi id="S4.E6.m1.2.2.2.3.2.2" xref="S4.E6.m1.2.2.2.3.2.2.cmml">K</mi><mi id="S4.E6.m1.2.2.2.3.2.3" xref="S4.E6.m1.2.2.2.3.2.3.cmml">i</mi><mi id="S4.E6.m1.2.2.2.3.3" xref="S4.E6.m1.2.2.2.3.3.cmml">T</mi></msubsup></mrow><msqrt id="S4.E6.m1.2.2.3" xref="S4.E6.m1.2.2.3.cmml"><mi id="S4.E6.m1.2.2.3.2" xref="S4.E6.m1.2.2.3.2.cmml">d</mi></msqrt></mfrac><mo id="S4.E6.m1.4.4.4.9.2.2" xref="S4.E6.m1.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.1g" xref="S4.E6.m1.4.4.4.1.cmml">​</mo><msub id="S4.E6.m1.4.4.4.10" xref="S4.E6.m1.4.4.4.10.cmml"><mi id="S4.E6.m1.4.4.4.10.2" xref="S4.E6.m1.4.4.4.10.2.cmml">V</mi><mi id="S4.E6.m1.4.4.4.10.3" xref="S4.E6.m1.4.4.4.10.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.4b"><apply id="S4.E6.m1.4.4.cmml" xref="S4.E6.m1.4.4"><eq id="S4.E6.m1.4.4.3.cmml" xref="S4.E6.m1.4.4.3"></eq><apply id="S4.E6.m1.4.4.2.cmml" xref="S4.E6.m1.4.4.2"><times id="S4.E6.m1.4.4.2.3.cmml" xref="S4.E6.m1.4.4.2.3"></times><ci id="S4.E6.m1.4.4.2.4.cmml" xref="S4.E6.m1.4.4.2.4">𝐴</ci><ci id="S4.E6.m1.4.4.2.5.cmml" xref="S4.E6.m1.4.4.2.5">𝑡</ci><ci id="S4.E6.m1.4.4.2.6.cmml" xref="S4.E6.m1.4.4.2.6">𝑡</ci><ci id="S4.E6.m1.4.4.2.7.cmml" xref="S4.E6.m1.4.4.2.7">𝑒</ci><ci id="S4.E6.m1.4.4.2.8.cmml" xref="S4.E6.m1.4.4.2.8">𝑛</ci><ci id="S4.E6.m1.4.4.2.9.cmml" xref="S4.E6.m1.4.4.2.9">𝑡</ci><ci id="S4.E6.m1.4.4.2.10.cmml" xref="S4.E6.m1.4.4.2.10">𝑖</ci><ci id="S4.E6.m1.4.4.2.11.cmml" xref="S4.E6.m1.4.4.2.11">𝑜</ci><ci id="S4.E6.m1.4.4.2.12.cmml" xref="S4.E6.m1.4.4.2.12">𝑛</ci><vector id="S4.E6.m1.4.4.2.2.3.cmml" xref="S4.E6.m1.4.4.2.2.2"><ci id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">𝑄</ci><apply id="S4.E6.m1.3.3.1.1.1.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.3.3.1.1.1.1.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S4.E6.m1.3.3.1.1.1.1.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.2">𝐾</ci><ci id="S4.E6.m1.3.3.1.1.1.1.3.cmml" xref="S4.E6.m1.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S4.E6.m1.4.4.2.2.2.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.2.2.2.2.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S4.E6.m1.4.4.2.2.2.2.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.2">𝑉</ci><ci id="S4.E6.m1.4.4.2.2.2.2.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.3">𝑖</ci></apply></vector></apply><apply id="S4.E6.m1.4.4.4.cmml" xref="S4.E6.m1.4.4.4"><times id="S4.E6.m1.4.4.4.1.cmml" xref="S4.E6.m1.4.4.4.1"></times><apply id="S4.E6.m1.4.4.4.2.cmml" xref="S4.E6.m1.4.4.4.2"><csymbol cd="latexml" id="S4.E6.m1.4.4.4.2.1.cmml" xref="S4.E6.m1.4.4.4.2.1">direct-product</csymbol><apply id="S4.E6.m1.4.4.4.2.2.cmml" xref="S4.E6.m1.4.4.4.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.2.1.cmml" xref="S4.E6.m1.4.4.4.2.2">subscript</csymbol><ci id="S4.E6.m1.4.4.4.2.2.2.cmml" xref="S4.E6.m1.4.4.4.2.2.2">Ω</ci><ci id="S4.E6.m1.4.4.4.2.2.3.cmml" xref="S4.E6.m1.4.4.4.2.2.3">𝑖</ci></apply><ci id="S4.E6.m1.4.4.4.2.3.cmml" xref="S4.E6.m1.4.4.4.2.3">𝑆</ci></apply><ci id="S4.E6.m1.4.4.4.3.cmml" xref="S4.E6.m1.4.4.4.3">𝑜</ci><ci id="S4.E6.m1.4.4.4.4.cmml" xref="S4.E6.m1.4.4.4.4">𝑓</ci><ci id="S4.E6.m1.4.4.4.5.cmml" xref="S4.E6.m1.4.4.4.5">𝑡</ci><ci id="S4.E6.m1.4.4.4.6.cmml" xref="S4.E6.m1.4.4.4.6">𝑚</ci><ci id="S4.E6.m1.4.4.4.7.cmml" xref="S4.E6.m1.4.4.4.7">𝑎</ci><ci id="S4.E6.m1.4.4.4.8.cmml" xref="S4.E6.m1.4.4.4.8">𝑥</ci><apply id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.4.4.4.9.2"><divide id="S4.E6.m1.2.2.1.cmml" xref="S4.E6.m1.4.4.4.9.2"></divide><apply id="S4.E6.m1.2.2.2.cmml" xref="S4.E6.m1.2.2.2"><times id="S4.E6.m1.2.2.2.1.cmml" xref="S4.E6.m1.2.2.2.1"></times><ci id="S4.E6.m1.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2">𝑄</ci><apply id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.3.1.cmml" xref="S4.E6.m1.2.2.2.3">superscript</csymbol><apply id="S4.E6.m1.2.2.2.3.2.cmml" xref="S4.E6.m1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.3.2.1.cmml" xref="S4.E6.m1.2.2.2.3">subscript</csymbol><ci id="S4.E6.m1.2.2.2.3.2.2.cmml" xref="S4.E6.m1.2.2.2.3.2.2">𝐾</ci><ci id="S4.E6.m1.2.2.2.3.2.3.cmml" xref="S4.E6.m1.2.2.2.3.2.3">𝑖</ci></apply><ci id="S4.E6.m1.2.2.2.3.3.cmml" xref="S4.E6.m1.2.2.2.3.3">𝑇</ci></apply></apply><apply id="S4.E6.m1.2.2.3.cmml" xref="S4.E6.m1.2.2.3"><root id="S4.E6.m1.2.2.3a.cmml" xref="S4.E6.m1.2.2.3"></root><ci id="S4.E6.m1.2.2.3.2.cmml" xref="S4.E6.m1.2.2.3.2">𝑑</ci></apply></apply><apply id="S4.E6.m1.4.4.4.10.cmml" xref="S4.E6.m1.4.4.4.10"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.10.1.cmml" xref="S4.E6.m1.4.4.4.10">subscript</csymbol><ci id="S4.E6.m1.4.4.4.10.2.cmml" xref="S4.E6.m1.4.4.4.10.2">𝑉</ci><ci id="S4.E6.m1.4.4.4.10.3.cmml" xref="S4.E6.m1.4.4.4.10.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.4c">Attention(Q,K_{i},V_{i})=\Omega_{i}\odot Softmax\left(\frac{QK_{i}^{T}}{\sqrt{d}}\right)V_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S4.p12.7" class="ltx_p">Where <math id="S4.p12.5.m1.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S4.p12.5.m1.1a"><mo id="S4.p12.5.m1.1.1" xref="S4.p12.5.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S4.p12.5.m1.1b"><csymbol cd="latexml" id="S4.p12.5.m1.1.1.cmml" xref="S4.p12.5.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.5.m1.1c">\odot</annotation></semantics></math> is the Hadamard product and <math id="S4.p12.6.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p12.6.m2.1a"><mi id="S4.p12.6.m2.1.1" xref="S4.p12.6.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p12.6.m2.1b"><ci id="S4.p12.6.m2.1.1.cmml" xref="S4.p12.6.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.6.m2.1c">i</annotation></semantics></math> refers to the relationship type <math id="S4.p12.7.m3.3" class="ltx_Math" alttext="i\in\{parent,neighbour,child\}" display="inline"><semantics id="S4.p12.7.m3.3a"><mrow id="S4.p12.7.m3.3.3" xref="S4.p12.7.m3.3.3.cmml"><mi id="S4.p12.7.m3.3.3.5" xref="S4.p12.7.m3.3.3.5.cmml">i</mi><mo id="S4.p12.7.m3.3.3.4" xref="S4.p12.7.m3.3.3.4.cmml">∈</mo><mrow id="S4.p12.7.m3.3.3.3.3" xref="S4.p12.7.m3.3.3.3.4.cmml"><mo stretchy="false" id="S4.p12.7.m3.3.3.3.3.4" xref="S4.p12.7.m3.3.3.3.4.cmml">{</mo><mrow id="S4.p12.7.m3.1.1.1.1.1" xref="S4.p12.7.m3.1.1.1.1.1.cmml"><mi id="S4.p12.7.m3.1.1.1.1.1.2" xref="S4.p12.7.m3.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.1.1.1.1.1.1" xref="S4.p12.7.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p12.7.m3.1.1.1.1.1.3" xref="S4.p12.7.m3.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.1.1.1.1.1.1a" xref="S4.p12.7.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p12.7.m3.1.1.1.1.1.4" xref="S4.p12.7.m3.1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.1.1.1.1.1.1b" xref="S4.p12.7.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p12.7.m3.1.1.1.1.1.5" xref="S4.p12.7.m3.1.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.1.1.1.1.1.1c" xref="S4.p12.7.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p12.7.m3.1.1.1.1.1.6" xref="S4.p12.7.m3.1.1.1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.1.1.1.1.1.1d" xref="S4.p12.7.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p12.7.m3.1.1.1.1.1.7" xref="S4.p12.7.m3.1.1.1.1.1.7.cmml">t</mi></mrow><mo id="S4.p12.7.m3.3.3.3.3.5" xref="S4.p12.7.m3.3.3.3.4.cmml">,</mo><mrow id="S4.p12.7.m3.2.2.2.2.2" xref="S4.p12.7.m3.2.2.2.2.2.cmml"><mi id="S4.p12.7.m3.2.2.2.2.2.2" xref="S4.p12.7.m3.2.2.2.2.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.3" xref="S4.p12.7.m3.2.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1a" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.4" xref="S4.p12.7.m3.2.2.2.2.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1b" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.5" xref="S4.p12.7.m3.2.2.2.2.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1c" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.6" xref="S4.p12.7.m3.2.2.2.2.2.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1d" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.7" xref="S4.p12.7.m3.2.2.2.2.2.7.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1e" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.8" xref="S4.p12.7.m3.2.2.2.2.2.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1f" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.9" xref="S4.p12.7.m3.2.2.2.2.2.9.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.2.2.2.2.2.1g" xref="S4.p12.7.m3.2.2.2.2.2.1.cmml">​</mo><mi id="S4.p12.7.m3.2.2.2.2.2.10" xref="S4.p12.7.m3.2.2.2.2.2.10.cmml">r</mi></mrow><mo id="S4.p12.7.m3.3.3.3.3.6" xref="S4.p12.7.m3.3.3.3.4.cmml">,</mo><mrow id="S4.p12.7.m3.3.3.3.3.3" xref="S4.p12.7.m3.3.3.3.3.3.cmml"><mi id="S4.p12.7.m3.3.3.3.3.3.2" xref="S4.p12.7.m3.3.3.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.3.3.3.3.3.1" xref="S4.p12.7.m3.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p12.7.m3.3.3.3.3.3.3" xref="S4.p12.7.m3.3.3.3.3.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.3.3.3.3.3.1a" xref="S4.p12.7.m3.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p12.7.m3.3.3.3.3.3.4" xref="S4.p12.7.m3.3.3.3.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.3.3.3.3.3.1b" xref="S4.p12.7.m3.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p12.7.m3.3.3.3.3.3.5" xref="S4.p12.7.m3.3.3.3.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.p12.7.m3.3.3.3.3.3.1c" xref="S4.p12.7.m3.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p12.7.m3.3.3.3.3.3.6" xref="S4.p12.7.m3.3.3.3.3.3.6.cmml">d</mi></mrow><mo stretchy="false" id="S4.p12.7.m3.3.3.3.3.7" xref="S4.p12.7.m3.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p12.7.m3.3b"><apply id="S4.p12.7.m3.3.3.cmml" xref="S4.p12.7.m3.3.3"><in id="S4.p12.7.m3.3.3.4.cmml" xref="S4.p12.7.m3.3.3.4"></in><ci id="S4.p12.7.m3.3.3.5.cmml" xref="S4.p12.7.m3.3.3.5">𝑖</ci><set id="S4.p12.7.m3.3.3.3.4.cmml" xref="S4.p12.7.m3.3.3.3.3"><apply id="S4.p12.7.m3.1.1.1.1.1.cmml" xref="S4.p12.7.m3.1.1.1.1.1"><times id="S4.p12.7.m3.1.1.1.1.1.1.cmml" xref="S4.p12.7.m3.1.1.1.1.1.1"></times><ci id="S4.p12.7.m3.1.1.1.1.1.2.cmml" xref="S4.p12.7.m3.1.1.1.1.1.2">𝑝</ci><ci id="S4.p12.7.m3.1.1.1.1.1.3.cmml" xref="S4.p12.7.m3.1.1.1.1.1.3">𝑎</ci><ci id="S4.p12.7.m3.1.1.1.1.1.4.cmml" xref="S4.p12.7.m3.1.1.1.1.1.4">𝑟</ci><ci id="S4.p12.7.m3.1.1.1.1.1.5.cmml" xref="S4.p12.7.m3.1.1.1.1.1.5">𝑒</ci><ci id="S4.p12.7.m3.1.1.1.1.1.6.cmml" xref="S4.p12.7.m3.1.1.1.1.1.6">𝑛</ci><ci id="S4.p12.7.m3.1.1.1.1.1.7.cmml" xref="S4.p12.7.m3.1.1.1.1.1.7">𝑡</ci></apply><apply id="S4.p12.7.m3.2.2.2.2.2.cmml" xref="S4.p12.7.m3.2.2.2.2.2"><times id="S4.p12.7.m3.2.2.2.2.2.1.cmml" xref="S4.p12.7.m3.2.2.2.2.2.1"></times><ci id="S4.p12.7.m3.2.2.2.2.2.2.cmml" xref="S4.p12.7.m3.2.2.2.2.2.2">𝑛</ci><ci id="S4.p12.7.m3.2.2.2.2.2.3.cmml" xref="S4.p12.7.m3.2.2.2.2.2.3">𝑒</ci><ci id="S4.p12.7.m3.2.2.2.2.2.4.cmml" xref="S4.p12.7.m3.2.2.2.2.2.4">𝑖</ci><ci id="S4.p12.7.m3.2.2.2.2.2.5.cmml" xref="S4.p12.7.m3.2.2.2.2.2.5">𝑔</ci><ci id="S4.p12.7.m3.2.2.2.2.2.6.cmml" xref="S4.p12.7.m3.2.2.2.2.2.6">ℎ</ci><ci id="S4.p12.7.m3.2.2.2.2.2.7.cmml" xref="S4.p12.7.m3.2.2.2.2.2.7">𝑏</ci><ci id="S4.p12.7.m3.2.2.2.2.2.8.cmml" xref="S4.p12.7.m3.2.2.2.2.2.8">𝑜</ci><ci id="S4.p12.7.m3.2.2.2.2.2.9.cmml" xref="S4.p12.7.m3.2.2.2.2.2.9">𝑢</ci><ci id="S4.p12.7.m3.2.2.2.2.2.10.cmml" xref="S4.p12.7.m3.2.2.2.2.2.10">𝑟</ci></apply><apply id="S4.p12.7.m3.3.3.3.3.3.cmml" xref="S4.p12.7.m3.3.3.3.3.3"><times id="S4.p12.7.m3.3.3.3.3.3.1.cmml" xref="S4.p12.7.m3.3.3.3.3.3.1"></times><ci id="S4.p12.7.m3.3.3.3.3.3.2.cmml" xref="S4.p12.7.m3.3.3.3.3.3.2">𝑐</ci><ci id="S4.p12.7.m3.3.3.3.3.3.3.cmml" xref="S4.p12.7.m3.3.3.3.3.3.3">ℎ</ci><ci id="S4.p12.7.m3.3.3.3.3.3.4.cmml" xref="S4.p12.7.m3.3.3.3.3.3.4">𝑖</ci><ci id="S4.p12.7.m3.3.3.3.3.3.5.cmml" xref="S4.p12.7.m3.3.3.3.3.3.5">𝑙</ci><ci id="S4.p12.7.m3.3.3.3.3.3.6.cmml" xref="S4.p12.7.m3.3.3.3.3.3.6">𝑑</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p12.7.m3.3c">i\in\{parent,neighbour,child\}</annotation></semantics></math>. Using the Hadamard product essentially zeroes out the attention between regions whose relationship is not being processed by that sub-layer. The resulting encodings are decoded by an LSTM to produce captions.</p>
</div>
<div id="S4.p13" class="ltx_para">
<p id="S4.p13.2" class="ltx_p">Like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the <math id="S4.p13.1.m1.1" class="ltx_Math" alttext="\mathcal{M}2" display="inline"><semantics id="S4.p13.1.m1.1a"><mrow id="S4.p13.1.m1.1.1" xref="S4.p13.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p13.1.m1.1.1.2" xref="S4.p13.1.m1.1.1.2.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S4.p13.1.m1.1.1.1" xref="S4.p13.1.m1.1.1.1.cmml">​</mo><mn id="S4.p13.1.m1.1.1.3" xref="S4.p13.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p13.1.m1.1b"><apply id="S4.p13.1.m1.1.1.cmml" xref="S4.p13.1.m1.1.1"><times id="S4.p13.1.m1.1.1.1.cmml" xref="S4.p13.1.m1.1.1.1"></times><ci id="S4.p13.1.m1.1.1.2.cmml" xref="S4.p13.1.m1.1.1.2">ℳ</ci><cn type="integer" id="S4.p13.1.m1.1.1.3.cmml" xref="S4.p13.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p13.1.m1.1c">\mathcal{M}2</annotation></semantics></math> meshed memory Transformer proposed by Cornia <span id="S4.p13.2.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> also makes use of the increasingly popular Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Unlike other papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> which make use of some predefined structure on extracted image features (spatial graph, semantic graph, etc), <math id="S4.p13.2.m2.1" class="ltx_Math" alttext="\mathcal{M}2" display="inline"><semantics id="S4.p13.2.m2.1a"><mrow id="S4.p13.2.m2.1.1" xref="S4.p13.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p13.2.m2.1.1.2" xref="S4.p13.2.m2.1.1.2.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S4.p13.2.m2.1.1.1" xref="S4.p13.2.m2.1.1.1.cmml">​</mo><mn id="S4.p13.2.m2.1.1.3" xref="S4.p13.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p13.2.m2.1b"><apply id="S4.p13.2.m2.1.1.cmml" xref="S4.p13.2.m2.1.1"><times id="S4.p13.2.m2.1.1.1.cmml" xref="S4.p13.2.m2.1.1.1"></times><ci id="S4.p13.2.m2.1.1.2.cmml" xref="S4.p13.2.m2.1.1.2">ℳ</ci><cn type="integer" id="S4.p13.2.m2.1.1.3.cmml" xref="S4.p13.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p13.2.m2.1c">\mathcal{M}2</annotation></semantics></math> uses stacks of self-attention layers across the set of all the image regions. The standard key and values from the Transformer are edited to include the concatenation of learnable persistent memory vectors. These allow the architecture to encode a-priori knowledge such as ‘eggs’ and ‘toast’ make up the concept ‘breakfast’. When decoding the output of the encoder, a stack of self-attention layers is also used. Each decoder layer is connected via a gated cross attention mechanism to each of the encoder layers, giving way to the “meshed” concept of the paper. The output of the decoder block is used to generate the final output caption.</p>
</div>
<div id="S4.p14" class="ltx_para">
<p id="S4.p14.3" class="ltx_p">The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> propose using a novel similarity (referred to as a semantic in the paper) and topic graphs. Built on dot product similarity, the graphs are produced without the requirement of graph extraction models such as MotifNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. Rather, a set of vertices <math id="S4.p14.1.m1.1" class="ltx_Math" alttext="V=\{v_{i}\in\mathbb{R}^{d_{obj}}\}^{n_{obj}}_{i=1}" display="inline"><semantics id="S4.p14.1.m1.1a"><mrow id="S4.p14.1.m1.1.1" xref="S4.p14.1.m1.1.1.cmml"><mi id="S4.p14.1.m1.1.1.3" xref="S4.p14.1.m1.1.1.3.cmml">V</mi><mo id="S4.p14.1.m1.1.1.2" xref="S4.p14.1.m1.1.1.2.cmml">=</mo><msubsup id="S4.p14.1.m1.1.1.1" xref="S4.p14.1.m1.1.1.1.cmml"><mrow id="S4.p14.1.m1.1.1.1.1.1.1" xref="S4.p14.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p14.1.m1.1.1.1.1.1.1.2" xref="S4.p14.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S4.p14.1.m1.1.1.1.1.1.1.1" xref="S4.p14.1.m1.1.1.1.1.1.1.1.cmml"><msub id="S4.p14.1.m1.1.1.1.1.1.1.1.2" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.2.2" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.2.3" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.p14.1.m1.1.1.1.1.1.1.1.1" xref="S4.p14.1.m1.1.1.1.1.1.1.1.1.cmml">∈</mo><msup id="S4.p14.1.m1.1.1.1.1.1.1.1.3" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.3.2" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.2.cmml">ℝ</mi><msub id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.2" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.2.cmml">d</mi><mrow id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.2" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.3" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1a" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.4" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.4.cmml">j</mi></mrow></msub></msup></mrow><mo stretchy="false" id="S4.p14.1.m1.1.1.1.1.1.1.3" xref="S4.p14.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.p14.1.m1.1.1.1.3" xref="S4.p14.1.m1.1.1.1.3.cmml"><mi id="S4.p14.1.m1.1.1.1.3.2" xref="S4.p14.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S4.p14.1.m1.1.1.1.3.1" xref="S4.p14.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S4.p14.1.m1.1.1.1.3.3" xref="S4.p14.1.m1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S4.p14.1.m1.1.1.1.1.3" xref="S4.p14.1.m1.1.1.1.1.3.cmml"><mi id="S4.p14.1.m1.1.1.1.1.3.2" xref="S4.p14.1.m1.1.1.1.1.3.2.cmml">n</mi><mrow id="S4.p14.1.m1.1.1.1.1.3.3" xref="S4.p14.1.m1.1.1.1.1.3.3.cmml"><mi id="S4.p14.1.m1.1.1.1.1.3.3.2" xref="S4.p14.1.m1.1.1.1.1.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p14.1.m1.1.1.1.1.3.3.1" xref="S4.p14.1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p14.1.m1.1.1.1.1.3.3.3" xref="S4.p14.1.m1.1.1.1.1.3.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.p14.1.m1.1.1.1.1.3.3.1a" xref="S4.p14.1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p14.1.m1.1.1.1.1.3.3.4" xref="S4.p14.1.m1.1.1.1.1.3.3.4.cmml">j</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p14.1.m1.1b"><apply id="S4.p14.1.m1.1.1.cmml" xref="S4.p14.1.m1.1.1"><eq id="S4.p14.1.m1.1.1.2.cmml" xref="S4.p14.1.m1.1.1.2"></eq><ci id="S4.p14.1.m1.1.1.3.cmml" xref="S4.p14.1.m1.1.1.3">𝑉</ci><apply id="S4.p14.1.m1.1.1.1.cmml" xref="S4.p14.1.m1.1.1.1"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.2.cmml" xref="S4.p14.1.m1.1.1.1">subscript</csymbol><apply id="S4.p14.1.m1.1.1.1.1.cmml" xref="S4.p14.1.m1.1.1.1"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.1.2.cmml" xref="S4.p14.1.m1.1.1.1">superscript</csymbol><set id="S4.p14.1.m1.1.1.1.1.1.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1"><apply id="S4.p14.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1"><in id="S4.p14.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.1"></in><apply id="S4.p14.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.p14.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.2">ℝ</ci><apply id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.2">𝑑</ci><apply id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3"><times id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.2">𝑜</ci><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.3">𝑏</ci><ci id="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.4.cmml" xref="S4.p14.1.m1.1.1.1.1.1.1.1.3.3.3.4">𝑗</ci></apply></apply></apply></apply></set><apply id="S4.p14.1.m1.1.1.1.1.3.cmml" xref="S4.p14.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p14.1.m1.1.1.1.1.3.1.cmml" xref="S4.p14.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.p14.1.m1.1.1.1.1.3.2.cmml" xref="S4.p14.1.m1.1.1.1.1.3.2">𝑛</ci><apply id="S4.p14.1.m1.1.1.1.1.3.3.cmml" xref="S4.p14.1.m1.1.1.1.1.3.3"><times id="S4.p14.1.m1.1.1.1.1.3.3.1.cmml" xref="S4.p14.1.m1.1.1.1.1.3.3.1"></times><ci id="S4.p14.1.m1.1.1.1.1.3.3.2.cmml" xref="S4.p14.1.m1.1.1.1.1.3.3.2">𝑜</ci><ci id="S4.p14.1.m1.1.1.1.1.3.3.3.cmml" xref="S4.p14.1.m1.1.1.1.1.3.3.3">𝑏</ci><ci id="S4.p14.1.m1.1.1.1.1.3.3.4.cmml" xref="S4.p14.1.m1.1.1.1.1.3.3.4">𝑗</ci></apply></apply></apply><apply id="S4.p14.1.m1.1.1.1.3.cmml" xref="S4.p14.1.m1.1.1.1.3"><eq id="S4.p14.1.m1.1.1.1.3.1.cmml" xref="S4.p14.1.m1.1.1.1.3.1"></eq><ci id="S4.p14.1.m1.1.1.1.3.2.cmml" xref="S4.p14.1.m1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.p14.1.m1.1.1.1.3.3.cmml" xref="S4.p14.1.m1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p14.1.m1.1c">V=\{v_{i}\in\mathbb{R}^{d_{obj}}\}^{n_{obj}}_{i=1}</annotation></semantics></math> are extracted as ResNet features from a Faster-RCNN object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Edges in the adjacency matrix are then populated using the dot product between the feature vectors in <math id="S4.p14.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S4.p14.2.m2.1a"><mi id="S4.p14.2.m2.1.1" xref="S4.p14.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.p14.2.m2.1b"><ci id="S4.p14.2.m2.1.1.cmml" xref="S4.p14.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p14.2.m2.1c">V</annotation></semantics></math> with <math id="S4.p14.3.m3.1" class="ltx_Math" alttext="a_{ij}=\sigma(v_{i}^{T}Mv_{j})" display="inline"><semantics id="S4.p14.3.m3.1a"><mrow id="S4.p14.3.m3.1.1" xref="S4.p14.3.m3.1.1.cmml"><msub id="S4.p14.3.m3.1.1.3" xref="S4.p14.3.m3.1.1.3.cmml"><mi id="S4.p14.3.m3.1.1.3.2" xref="S4.p14.3.m3.1.1.3.2.cmml">a</mi><mrow id="S4.p14.3.m3.1.1.3.3" xref="S4.p14.3.m3.1.1.3.3.cmml"><mi id="S4.p14.3.m3.1.1.3.3.2" xref="S4.p14.3.m3.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p14.3.m3.1.1.3.3.1" xref="S4.p14.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S4.p14.3.m3.1.1.3.3.3" xref="S4.p14.3.m3.1.1.3.3.3.cmml">j</mi></mrow></msub><mo id="S4.p14.3.m3.1.1.2" xref="S4.p14.3.m3.1.1.2.cmml">=</mo><mrow id="S4.p14.3.m3.1.1.1" xref="S4.p14.3.m3.1.1.1.cmml"><mi id="S4.p14.3.m3.1.1.1.3" xref="S4.p14.3.m3.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.p14.3.m3.1.1.1.2" xref="S4.p14.3.m3.1.1.1.2.cmml">​</mo><mrow id="S4.p14.3.m3.1.1.1.1.1" xref="S4.p14.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p14.3.m3.1.1.1.1.1.2" xref="S4.p14.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p14.3.m3.1.1.1.1.1.1" xref="S4.p14.3.m3.1.1.1.1.1.1.cmml"><msubsup id="S4.p14.3.m3.1.1.1.1.1.1.2" xref="S4.p14.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S4.p14.3.m3.1.1.1.1.1.1.2.2.2" xref="S4.p14.3.m3.1.1.1.1.1.1.2.2.2.cmml">v</mi><mi id="S4.p14.3.m3.1.1.1.1.1.1.2.2.3" xref="S4.p14.3.m3.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S4.p14.3.m3.1.1.1.1.1.1.2.3" xref="S4.p14.3.m3.1.1.1.1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.p14.3.m3.1.1.1.1.1.1.1" xref="S4.p14.3.m3.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.p14.3.m3.1.1.1.1.1.1.3" xref="S4.p14.3.m3.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p14.3.m3.1.1.1.1.1.1.1a" xref="S4.p14.3.m3.1.1.1.1.1.1.1.cmml">​</mo><msub id="S4.p14.3.m3.1.1.1.1.1.1.4" xref="S4.p14.3.m3.1.1.1.1.1.1.4.cmml"><mi id="S4.p14.3.m3.1.1.1.1.1.1.4.2" xref="S4.p14.3.m3.1.1.1.1.1.1.4.2.cmml">v</mi><mi id="S4.p14.3.m3.1.1.1.1.1.1.4.3" xref="S4.p14.3.m3.1.1.1.1.1.1.4.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.p14.3.m3.1.1.1.1.1.3" xref="S4.p14.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p14.3.m3.1b"><apply id="S4.p14.3.m3.1.1.cmml" xref="S4.p14.3.m3.1.1"><eq id="S4.p14.3.m3.1.1.2.cmml" xref="S4.p14.3.m3.1.1.2"></eq><apply id="S4.p14.3.m3.1.1.3.cmml" xref="S4.p14.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.p14.3.m3.1.1.3.1.cmml" xref="S4.p14.3.m3.1.1.3">subscript</csymbol><ci id="S4.p14.3.m3.1.1.3.2.cmml" xref="S4.p14.3.m3.1.1.3.2">𝑎</ci><apply id="S4.p14.3.m3.1.1.3.3.cmml" xref="S4.p14.3.m3.1.1.3.3"><times id="S4.p14.3.m3.1.1.3.3.1.cmml" xref="S4.p14.3.m3.1.1.3.3.1"></times><ci id="S4.p14.3.m3.1.1.3.3.2.cmml" xref="S4.p14.3.m3.1.1.3.3.2">𝑖</ci><ci id="S4.p14.3.m3.1.1.3.3.3.cmml" xref="S4.p14.3.m3.1.1.3.3.3">𝑗</ci></apply></apply><apply id="S4.p14.3.m3.1.1.1.cmml" xref="S4.p14.3.m3.1.1.1"><times id="S4.p14.3.m3.1.1.1.2.cmml" xref="S4.p14.3.m3.1.1.1.2"></times><ci id="S4.p14.3.m3.1.1.1.3.cmml" xref="S4.p14.3.m3.1.1.1.3">𝜎</ci><apply id="S4.p14.3.m3.1.1.1.1.1.1.cmml" xref="S4.p14.3.m3.1.1.1.1.1"><times id="S4.p14.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.1"></times><apply id="S4.p14.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p14.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.p14.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p14.3.m3.1.1.1.1.1.1.2.2.1.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.p14.3.m3.1.1.1.1.1.1.2.2.2.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2.2.2">𝑣</ci><ci id="S4.p14.3.m3.1.1.1.1.1.1.2.2.3.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S4.p14.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="S4.p14.3.m3.1.1.1.1.1.1.3.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.3">𝑀</ci><apply id="S4.p14.3.m3.1.1.1.1.1.1.4.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.p14.3.m3.1.1.1.1.1.1.4.1.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.p14.3.m3.1.1.1.1.1.1.4.2.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.4.2">𝑣</ci><ci id="S4.p14.3.m3.1.1.1.1.1.1.4.3.cmml" xref="S4.p14.3.m3.1.1.1.1.1.1.4.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p14.3.m3.1c">a_{ij}=\sigma(v_{i}^{T}Mv_{j})</annotation></semantics></math>. Once both graphs have been constructed, a GCN is applied to both in order to enrich the nodes with local context. A graph self-attention mechanism is then applied to ensure nodes are not just accounting for their immediate neighbours. The improved graphs are then decoded via an LSTM to generate captions.</p>
</div>
<div id="S4.p15" class="ltx_para">
<p id="S4.p15.2" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Dong <span id="S4.p15.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> use a spatial graph to show a directed relationship between detected objects within the input image. Locally, object features are extracted by a CNN to associate a vector to each vertex of the spatial graph. This process is completed for each image in the dataset. In addition to this graph, the authors introduce an image level graph. Specifically, each image is represented by a feature vector that is the average of its associated set of object feature vectors. The image graph for a corresponding image is formed as a fully connected undirected graph of the <math id="S4.p15.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.p15.1.m1.1a"><mi id="S4.p15.1.m1.1.1" xref="S4.p15.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.p15.1.m1.1b"><ci id="S4.p15.1.m1.1.1.cmml" xref="S4.p15.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p15.1.m1.1c">K</annotation></semantics></math> images whose <math id="S4.p15.2.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S4.p15.2.m2.1a"><msub id="S4.p15.2.m2.1.1" xref="S4.p15.2.m2.1.1.cmml"><mi id="S4.p15.2.m2.1.1.2" xref="S4.p15.2.m2.1.1.2.cmml">l</mi><mn id="S4.p15.2.m2.1.1.3" xref="S4.p15.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p15.2.m2.1b"><apply id="S4.p15.2.m2.1.1.cmml" xref="S4.p15.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p15.2.m2.1.1.1.cmml" xref="S4.p15.2.m2.1.1">subscript</csymbol><ci id="S4.p15.2.m2.1.1.2.cmml" xref="S4.p15.2.m2.1.1.2">𝑙</ci><cn type="integer" id="S4.p15.2.m2.1.1.3.cmml" xref="S4.p15.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p15.2.m2.1c">l_{2}</annotation></semantics></math> distance is the closest to the input image. Both the local spatial graph and the more global image level graph are processed by GCNs to create richer embeddings that can be used for caption generation. This approach is shown to work extremely well, with Dual-GCN achieving outperforming comparable models in the BLEU, METEOR, and ROGUE metrics (see Table <a href="#S4.T3" title="TABLE III ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>).</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>A table showing the model details and various benchmark results of selected image captioning models trained on the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> dataset using the Karpathy split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. <span id="S4.T3.3.1" class="ltx_text ltx_font_bold">Bold</span>: Best score.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:113.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-231.0pt,60.4pt) scale(0.484107636357888,0.484107636357888) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Model</th>
<th id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Graph Types</th>
<th id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Architecture(s)</th>
<th id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Language Generator</th>
<th id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">BLEU-1</th>
<th id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column">BLEU-4</th>
<th id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column">METEOR</th>
<th id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column">ROGUE</th>
<th id="S4.T3.1.1.2.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column">CIDEr</th>
<th id="S4.T3.1.1.2.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column">SPICE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GCN-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_t">Spatial, Semantic</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t">GCN</td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">LSTM</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_left ltx_border_t">80.9</td>
<td id="S4.T3.1.1.3.1.6" class="ltx_td ltx_align_left ltx_border_t">38.3</td>
<td id="S4.T3.1.1.3.1.7" class="ltx_td ltx_align_left ltx_border_t">28.6</td>
<td id="S4.T3.1.1.3.1.8" class="ltx_td ltx_align_left ltx_border_t">58.5</td>
<td id="S4.T3.1.1.3.1.9" class="ltx_td ltx_align_left ltx_border_t">128.7</td>
<td id="S4.T3.1.1.3.1.10" class="ltx_td ltx_align_left ltx_border_t">22.1</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_left">Hierarchical Spatial (Tree)</td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_left">GCN</td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T3.1.1.4.2.6" class="ltx_td ltx_align_left">39.1</td>
<td id="S4.T3.1.1.4.2.7" class="ltx_td ltx_align_left">28.9</td>
<td id="S4.T3.1.1.4.2.8" class="ltx_td ltx_align_left">59.2</td>
<td id="S4.T3.1.1.4.2.9" class="ltx_td ltx_align_left">130.6</td>
<td id="S4.T3.1.1.4.2.10" class="ltx_td ltx_align_left">22.3</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SGAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_left">Multipartite Textual, Multipartite Semantic</td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_left">GCN, multi-modal GNN</td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_left">80.8</td>
<td id="S4.T3.1.1.5.3.6" class="ltx_td ltx_align_left">38.4</td>
<td id="S4.T3.1.1.5.3.7" class="ltx_td ltx_align_left">28.4</td>
<td id="S4.T3.1.1.5.3.8" class="ltx_td ltx_align_left">58.6</td>
<td id="S4.T3.1.1.5.3.9" class="ltx_td ltx_align_left">127.8</td>
<td id="S4.T3.1.1.5.3.10" class="ltx_td ltx_align_left">22.1</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<th id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VSUA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</th>
<td id="S4.T3.1.1.6.4.2" class="ltx_td ltx_align_left">Multipartite Semantic, Spatial</td>
<td id="S4.T3.1.1.6.4.3" class="ltx_td ltx_align_left">GCN</td>
<td id="S4.T3.1.1.6.4.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.6.4.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T3.1.1.6.4.6" class="ltx_td ltx_align_left">38.4</td>
<td id="S4.T3.1.1.6.4.7" class="ltx_td ltx_align_left">28.5</td>
<td id="S4.T3.1.1.6.4.8" class="ltx_td ltx_align_left">58.4</td>
<td id="S4.T3.1.1.6.4.9" class="ltx_td ltx_align_left">128.6</td>
<td id="S4.T3.1.1.6.4.10" class="ltx_td ltx_align_left">22.0</td>
</tr>
<tr id="S4.T3.1.1.7.5" class="ltx_tr">
<th id="S4.T3.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ARL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</th>
<td id="S4.T3.1.1.7.5.2" class="ltx_td ltx_align_left">Spatial</td>
<td id="S4.T3.1.1.7.5.3" class="ltx_td ltx_align_left">GGNN</td>
<td id="S4.T3.1.1.7.5.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.7.5.5" class="ltx_td ltx_align_left">75.9</td>
<td id="S4.T3.1.1.7.5.6" class="ltx_td ltx_align_left">35.8</td>
<td id="S4.T3.1.1.7.5.7" class="ltx_td ltx_align_left">27.8</td>
<td id="S4.T3.1.1.7.5.8" class="ltx_td ltx_align_left">56.4</td>
<td id="S4.T3.1.1.7.5.9" class="ltx_td ltx_align_left">111.3</td>
<td id="S4.T3.1.1.7.5.10" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T3.1.1.8.6" class="ltx_tr">
<th id="S4.T3.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SUB-GC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</th>
<td id="S4.T3.1.1.8.6.2" class="ltx_td ltx_align_left">Semantic</td>
<td id="S4.T3.1.1.8.6.3" class="ltx_td ltx_align_left">GCN</td>
<td id="S4.T3.1.1.8.6.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.8.6.5" class="ltx_td ltx_align_left">76.8</td>
<td id="S4.T3.1.1.8.6.6" class="ltx_td ltx_align_left">36.2</td>
<td id="S4.T3.1.1.8.6.7" class="ltx_td ltx_align_left">27.7</td>
<td id="S4.T3.1.1.8.6.8" class="ltx_td ltx_align_left">56.6</td>
<td id="S4.T3.1.1.8.6.9" class="ltx_td ltx_align_left">115.3</td>
<td id="S4.T3.1.1.8.6.10" class="ltx_td ltx_align_left">20.7</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}^{2}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">ℳ</mi><mn id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">ℳ</ci><cn type="integer" id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\mathcal{M}^{2}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left">-</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left">Transformer</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r">Transformer</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_left">80.8</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_left">39.1</td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_left">29.2</td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_left">58.6</td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_left">131.2</td>
<td id="S4.T3.1.1.1.10" class="ltx_td ltx_align_left">22.6</td>
</tr>
<tr id="S4.T3.1.1.9.7" class="ltx_tr">
<th id="S4.T3.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Image Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S4.T3.1.1.9.7.2" class="ltx_td ltx_align_left">Hierarchical Spatial</td>
<td id="S4.T3.1.1.9.7.3" class="ltx_td ltx_align_left">Transformer</td>
<td id="S4.T3.1.1.9.7.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.9.7.5" class="ltx_td ltx_align_left">80.8</td>
<td id="S4.T3.1.1.9.7.6" class="ltx_td ltx_align_left">39.5</td>
<td id="S4.T3.1.1.9.7.7" class="ltx_td ltx_align_left">29.1</td>
<td id="S4.T3.1.1.9.7.8" class="ltx_td ltx_align_left">59.0</td>
<td id="S4.T3.1.1.9.7.9" class="ltx_td ltx_align_left">130.8</td>
<td id="S4.T3.1.1.9.7.10" class="ltx_td ltx_align_left">22.8</td>
</tr>
<tr id="S4.T3.1.1.10.8" class="ltx_tr">
<th id="S4.T3.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Topic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</th>
<td id="S4.T3.1.1.10.8.2" class="ltx_td ltx_align_left">Similarity</td>
<td id="S4.T3.1.1.10.8.3" class="ltx_td ltx_align_left">GCN</td>
<td id="S4.T3.1.1.10.8.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.10.8.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T3.1.1.10.8.6" class="ltx_td ltx_align_left">39.2</td>
<td id="S4.T3.1.1.10.8.7" class="ltx_td ltx_align_left">29.1</td>
<td id="S4.T3.1.1.10.8.8" class="ltx_td ltx_align_left">59.0</td>
<td id="S4.T3.1.1.10.8.9" class="ltx_td ltx_align_left">129.5</td>
<td id="S4.T3.1.1.10.8.10" class="ltx_td ltx_align_left">22.6</td>
</tr>
<tr id="S4.T3.1.1.11.9" class="ltx_tr">
<th id="S4.T3.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Dual-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</th>
<td id="S4.T3.1.1.11.9.2" class="ltx_td ltx_align_left">Spatial, Image</td>
<td id="S4.T3.1.1.11.9.3" class="ltx_td ltx_align_left">(Dual) GCN</td>
<td id="S4.T3.1.1.11.9.4" class="ltx_td ltx_align_left ltx_border_r">Transformer</td>
<td id="S4.T3.1.1.11.9.5" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.11.9.5.1" class="ltx_text ltx_font_bold">82.2</span></td>
<td id="S4.T3.1.1.11.9.6" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.11.9.6.1" class="ltx_text ltx_font_bold">39.7</span></td>
<td id="S4.T3.1.1.11.9.7" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.11.9.7.1" class="ltx_text ltx_font_bold">29.7</span></td>
<td id="S4.T3.1.1.11.9.8" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.11.9.8.1" class="ltx_text ltx_font_bold">59.7</span></td>
<td id="S4.T3.1.1.11.9.9" class="ltx_td ltx_align_left">129.2</td>
<td id="S4.T3.1.1.11.9.10" class="ltx_td ltx_align_left">-</td>
</tr>
<tr id="S4.T3.1.1.12.10" class="ltx_tr">
<th id="S4.T3.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">EIVRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</th>
<td id="S4.T3.1.1.12.10.2" class="ltx_td ltx_align_left">Spatial, Semantic</td>
<td id="S4.T3.1.1.12.10.3" class="ltx_td ltx_align_left">GGNN</td>
<td id="S4.T3.1.1.12.10.4" class="ltx_td ltx_align_left ltx_border_r">Transformer</td>
<td id="S4.T3.1.1.12.10.5" class="ltx_td ltx_align_left">-</td>
<td id="S4.T3.1.1.12.10.6" class="ltx_td ltx_align_left">39.4</td>
<td id="S4.T3.1.1.12.10.7" class="ltx_td ltx_align_left">29.3</td>
<td id="S4.T3.1.1.12.10.8" class="ltx_td ltx_align_left">59.1</td>
<td id="S4.T3.1.1.12.10.9" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.12.10.9.1" class="ltx_text ltx_font_bold">131.9</span></td>
<td id="S4.T3.1.1.12.10.10" class="ltx_td ltx_align_left">22.8</td>
</tr>
<tr id="S4.T3.1.1.13.11" class="ltx_tr">
<th id="S4.T3.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SASG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</th>
<td id="S4.T3.1.1.13.11.2" class="ltx_td ltx_align_left">Birpartite Semantic</td>
<td id="S4.T3.1.1.13.11.3" class="ltx_td ltx_align_left">GCN</td>
<td id="S4.T3.1.1.13.11.4" class="ltx_td ltx_align_left ltx_border_r">LSTM</td>
<td id="S4.T3.1.1.13.11.5" class="ltx_td ltx_align_left">81.8</td>
<td id="S4.T3.1.1.13.11.6" class="ltx_td ltx_align_left">38.9</td>
<td id="S4.T3.1.1.13.11.7" class="ltx_td ltx_align_left">29.2</td>
<td id="S4.T3.1.1.13.11.8" class="ltx_td ltx_align_left">59.4</td>
<td id="S4.T3.1.1.13.11.9" class="ltx_td ltx_align_left">128.9</td>
<td id="S4.T3.1.1.13.11.10" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.13.11.10.1" class="ltx_text ltx_font_bold">25.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Visual Question Answering</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">VQA is the challenging task of designing and implementing models that are able to answer natural language questions about a given image. These answers can range from simple yes/no to more natural, longer form answers. Questions can also vary in complexity. As the field has developed, more specific VQA tasks have emerged. The first to emerge was FVQA, sometimes known as Knowledge Visual Question Answering (KVQA), where external knowledge sources are required to answer the questions. Another task that has emerged is Textual VQA, where the models must understand the text within the scene in order to generate answers. All three tasks have their own datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and have an active community developing solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">VQA</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Originally proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, VQA has developed beyond simple ‘yes’ or ‘no’ answers to richer natural language answers. A common thread of work is to leverage the multi-modal aspect of VQA and utilise both visual features from the input image and textual features from the question <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">One of the first works in VQA to make use of GNNs was that of Teney <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. Their work is based on the clip art focused dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Their model takes a visual scene graph as input alongside a question. The question is then parsed into a textual scene graph using the Stanford Dependency Parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. These scene graphs are then processed independently using a GGNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> modified to incorporate an attention mechanism. The original feature vectors are then combined using an attention mechanism that reflects how relevant two nodes from the scene graphs are to one another.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Khademi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> takes a multimodal approach to VQA by using dense region captions alongside extracted visual features. Given a query and input image, the model will first extract visual regions using a Faster-RCNN object detector and generated a set of features using ResNet and encoding the bounding box information into these features. An off-the-shelf dense region captioning model is also used to create a set of captions and associated bounding boxes. The captions and bounding box information are encoded using a GRU. Each set of features is turned into a graph (visual and textual respectively) with outgoing and incoming edges existing between features if the Euclidean distance between the centre of the normalised bounding boxes is less than <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\gamma=0.5" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">γ</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><eq id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></eq><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝛾</ci><cn type="float" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\gamma=0.5</annotation></semantics></math>. Both graphs are processed by a GGNN with updated features being used to update an external spatial memory unit - thus making the network a Graph Memory Network (described in Section <a href="#S3.SS4" title="III-D Graph Memory Networks ‣ III An Overview of Graph Neural Networks ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>). After propagating the node features, the final state of the external spatial memory network is turned into a complete graph using each location as a node. This final graph is processed by a GGNN to produce the final answer. The multimodal approach presented in this paper is shown to be highly effective when compared to similar VQA methods. This approach is shown to work extremely well in benchmarks, with the proposed MN-GMN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> performing favourably with comparable models (Table <a href="#S5.T4" title="TABLE IV ‣ V-A VQA ‣ V Visual Question Answering ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>).</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">MORN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> is another work that focuses on capturing the complex multi-modal relationships between the question and image. Like many recent works in Deep Learning, it adopts the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> architecture. Built with three main components, the model first creates a visual graph of the image starting from a fully connected graph of detected objects and a GCN is used to aggregate the visual features. The second part of the model creates a textual scene graph from the input question. Both graphs are merged together by the final component of the model, a relational multi-modal Transformer, which is used to align the representations.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.2" class="ltx_p">Sharma <span id="S5.SS1.p5.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> follow the vision-language multi-modal approach but diverge from the use of a textual semantic graph and instead opt to use word embeddings. The authors utilise a novel GGNN-based architecture that processes an undirected complete graph of nodes representing visual features. Nodes are weighted with the probability that a relationship occurs between them. In line with other VQA work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, the question is capped to <math id="S5.SS1.p5.1.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.SS1.p5.1.m1.1a"><mn id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><cn type="integer" id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">14</annotation></semantics></math> words, with each one being converted into GloVe embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Questions with fewer than <math id="S5.SS1.p5.2.m2.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.SS1.p5.2.m2.1a"><mn id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><cn type="integer" id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">14</annotation></semantics></math> words are padded with zero-vectors. A question embedding is then generated using a GRU applied to the word embeddings. An LSTM-based attention mechanism considers both the question vector and the visual representations making up the nodes of the scene graph. This module considers previously attended areas when exploring new visual features. Finally, an LSTM-based language generator is used to generate the final answer. Another work to forgo using a textual scene graph, Zhang <span id="S5.SS1.p5.2.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> make use of word vectors to embed information about the image into a semantic graph. Using a GNN, they are able to create enriched feature vectors representing the nodes, edges, and an image feature vector representing the global state. They include the question into the image feature by averaging the word vectors, which enables the GNN to reason about the image. Whilst both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> yield good results, by only using word or sentence level embeddings and not using a textual scene graph, they fail to model relationships in the textual domain. This therefore removes the ability for the models to reason in that domain alone.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">Both Li <span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> and Nuthalapati <span id="S5.SS1.p6.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> take a different route to the established multi-modal approach and instead use different forms of visual information. Li <span id="S5.SS1.p6.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> take inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and make use of both semantic and spatial graphs to represent the image. In addition to these explicit graphs, they also introduce an implicit graph, i.e., a fully connected graph between the detected objects with edge weights set by a GAT. The relation-aware visual features are then combined with the question vector using multi-modal fusion. The fused output is then used to predict an answer via an MLP.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">Nuthalapati <span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> use a dual scene graph approach, using both visual and semantic graphs. These graphs are merged into a single graph embedding using a novel GAT architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> that is able to attend to edges as well as nodes. The graphs are enriched with negative entities that appear in the question but not the graph. Pruning then takes place to remove nodes and edges that are <math id="S5.SS1.p7.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS1.p7.1.m1.1a"><mi id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.1b"><ci id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.1c">K</annotation></semantics></math> hops away from features mentioned in the question. A decoder is then used to produce an answer to the inputted question.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>A table showing the model details and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> Test-Dev results of selected VQA models. <span id="S5.T4.2.1" class="ltx_text ltx_font_bold">Bold</span>: Best score.</figcaption>
<div id="S5.T4.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:64.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-147.6pt,21.9pt) scale(0.594949798287044,0.594949798287044) ;">
<table id="S5.T4.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.3.1.1.1" class="ltx_tr">
<th id="S5.T4.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Model</th>
<th id="S5.T4.3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Graphs Used</th>
<th id="S5.T4.3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Architecture</th>
<th id="S5.T4.3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">Overall</th>
<th id="S5.T4.3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">Y/N</th>
<th id="S5.T4.3.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column">Number</th>
<th id="S5.T4.3.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Other</th>
<th id="S5.T4.3.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column">Test-Std</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.3.1.2.1" class="ltx_tr">
<td id="S5.T4.3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GraphVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> (Abstract Scenes only)</td>
<td id="S5.T4.3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Visual and Textual Semantic</td>
<td id="S5.T4.3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GCN</td>
<td id="S5.T4.3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">70.42</td>
<td id="S5.T4.3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">81.26</td>
<td id="S5.T4.3.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T4.3.1.2.1.6.1" class="ltx_text ltx_font_bold">76.47</span></td>
<td id="S5.T4.3.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">56.28</td>
<td id="S5.T4.3.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T4.3.1.3.2" class="ltx_tr">
<td id="S5.T4.3.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">ReGAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S5.T4.3.1.3.2.2" class="ltx_td ltx_align_left">Semantic, Spatial</td>
<td id="S5.T4.3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">GAT</td>
<td id="S5.T4.3.1.3.2.4" class="ltx_td ltx_align_left">70.27</td>
<td id="S5.T4.3.1.3.2.5" class="ltx_td ltx_align_left">86.08</td>
<td id="S5.T4.3.1.3.2.6" class="ltx_td ltx_align_left">54.42</td>
<td id="S5.T4.3.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r">60.33</td>
<td id="S5.T4.3.1.3.2.8" class="ltx_td ltx_align_left">70.59</td>
</tr>
<tr id="S5.T4.3.1.4.3" class="ltx_tr">
<td id="S5.T4.3.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">MN-GMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S5.T4.3.1.4.3.2" class="ltx_td ltx_align_left">Visual and Textual Semantic</td>
<td id="S5.T4.3.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">Graph Memory Network</td>
<td id="S5.T4.3.1.4.3.4" class="ltx_td ltx_align_left"><span id="S5.T4.3.1.4.3.4.1" class="ltx_text ltx_font_bold">73.2</span></td>
<td id="S5.T4.3.1.4.3.5" class="ltx_td ltx_align_left"><span id="S5.T4.3.1.4.3.5.1" class="ltx_text ltx_font_bold">88.2</span></td>
<td id="S5.T4.3.1.4.3.6" class="ltx_td ltx_align_left">56</td>
<td id="S5.T4.3.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T4.3.1.4.3.7.1" class="ltx_text ltx_font_bold">64.2</span></td>
<td id="S5.T4.3.1.4.3.8" class="ltx_td ltx_align_left"><span id="S5.T4.3.1.4.3.8.1" class="ltx_text ltx_font_bold">73.5</span></td>
</tr>
<tr id="S5.T4.3.1.5.4" class="ltx_tr">
<td id="S5.T4.3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">Sharma et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
<td id="S5.T4.3.1.5.4.2" class="ltx_td ltx_align_left">Semantic</td>
<td id="S5.T4.3.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">GGNN</td>
<td id="S5.T4.3.1.5.4.4" class="ltx_td ltx_align_left">67.96</td>
<td id="S5.T4.3.1.5.4.5" class="ltx_td ltx_align_left">84.12</td>
<td id="S5.T4.3.1.5.4.6" class="ltx_td ltx_align_left">46.12</td>
<td id="S5.T4.3.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r">58.13</td>
<td id="S5.T4.3.1.5.4.8" class="ltx_td ltx_align_left">67.98</td>
</tr>
<tr id="S5.T4.3.1.6.5" class="ltx_tr">
<td id="S5.T4.3.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">MORN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S5.T4.3.1.6.5.2" class="ltx_td ltx_align_left">Visual and Textual Semantic</td>
<td id="S5.T4.3.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">GCN</td>
<td id="S5.T4.3.1.6.5.4" class="ltx_td ltx_align_left">71.21</td>
<td id="S5.T4.3.1.6.5.5" class="ltx_td ltx_align_left">87.15</td>
<td id="S5.T4.3.1.6.5.6" class="ltx_td ltx_align_left">55.22</td>
<td id="S5.T4.3.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r">61.19</td>
<td id="S5.T4.3.1.6.5.8" class="ltx_td ltx_align_left">71.53</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Knowledge/Fact-Based VQA</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Knowledge or Fact-Based VQA is the challenging task of making use of external knowledge given in knowledge graphs such as WikiData <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to answer questions about an image. The major challenge of this task is to create a model that can make use of all three mediums (image, question, and fact) to generate an appropriate answer. The MUCKO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> architectural diagram shown in Figure <a href="#S5.F4" title="Figure 4 ‣ V-B Knowledge/Fact-Based VQA ‣ V Visual Question Answering ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (reused with permission), is shown as a representative example of models that approach FVQA.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2303.03761/assets/figures/MUCKO-figure.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="371" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The MUCKO architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> (reused with permission). Best viewed in colour.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.8" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>, the authors present a novel GCN-based architecture for FVQA. Alongside the question and answer sets, a knowledge base of facts is also included, <math id="S5.SS2.p2.1.m1.5" class="ltx_Math" alttext="KB=\{f_{1},f_{2},...,f_{|KB|}\}" display="inline"><semantics id="S5.SS2.p2.1.m1.5a"><mrow id="S5.SS2.p2.1.m1.5.5" xref="S5.SS2.p2.1.m1.5.5.cmml"><mrow id="S5.SS2.p2.1.m1.5.5.5" xref="S5.SS2.p2.1.m1.5.5.5.cmml"><mi id="S5.SS2.p2.1.m1.5.5.5.2" xref="S5.SS2.p2.1.m1.5.5.5.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.5.5.5.1" xref="S5.SS2.p2.1.m1.5.5.5.1.cmml">​</mo><mi id="S5.SS2.p2.1.m1.5.5.5.3" xref="S5.SS2.p2.1.m1.5.5.5.3.cmml">B</mi></mrow><mo id="S5.SS2.p2.1.m1.5.5.4" xref="S5.SS2.p2.1.m1.5.5.4.cmml">=</mo><mrow id="S5.SS2.p2.1.m1.5.5.3.3" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml"><mo stretchy="false" id="S5.SS2.p2.1.m1.5.5.3.3.4" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml">{</mo><msub id="S5.SS2.p2.1.m1.3.3.1.1.1" xref="S5.SS2.p2.1.m1.3.3.1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.3.3.1.1.1.2" xref="S5.SS2.p2.1.m1.3.3.1.1.1.2.cmml">f</mi><mn id="S5.SS2.p2.1.m1.3.3.1.1.1.3" xref="S5.SS2.p2.1.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS2.p2.1.m1.5.5.3.3.5" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml">,</mo><msub id="S5.SS2.p2.1.m1.4.4.2.2.2" xref="S5.SS2.p2.1.m1.4.4.2.2.2.cmml"><mi id="S5.SS2.p2.1.m1.4.4.2.2.2.2" xref="S5.SS2.p2.1.m1.4.4.2.2.2.2.cmml">f</mi><mn id="S5.SS2.p2.1.m1.4.4.2.2.2.3" xref="S5.SS2.p2.1.m1.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S5.SS2.p2.1.m1.5.5.3.3.6" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S5.SS2.p2.1.m1.2.2" xref="S5.SS2.p2.1.m1.2.2.cmml">…</mi><mo id="S5.SS2.p2.1.m1.5.5.3.3.7" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml">,</mo><msub id="S5.SS2.p2.1.m1.5.5.3.3.3" xref="S5.SS2.p2.1.m1.5.5.3.3.3.cmml"><mi id="S5.SS2.p2.1.m1.5.5.3.3.3.2" xref="S5.SS2.p2.1.m1.5.5.3.3.3.2.cmml">f</mi><mrow id="S5.SS2.p2.1.m1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.1.m1.1.1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S5.SS2.p2.1.m1.1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS2.p2.1.m1.1.1.1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.1.1.1.3.cmml">B</mi></mrow><mo stretchy="false" id="S5.SS2.p2.1.m1.1.1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.1.2.1.cmml">|</mo></mrow></msub><mo stretchy="false" id="S5.SS2.p2.1.m1.5.5.3.3.8" xref="S5.SS2.p2.1.m1.5.5.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.5b"><apply id="S5.SS2.p2.1.m1.5.5.cmml" xref="S5.SS2.p2.1.m1.5.5"><eq id="S5.SS2.p2.1.m1.5.5.4.cmml" xref="S5.SS2.p2.1.m1.5.5.4"></eq><apply id="S5.SS2.p2.1.m1.5.5.5.cmml" xref="S5.SS2.p2.1.m1.5.5.5"><times id="S5.SS2.p2.1.m1.5.5.5.1.cmml" xref="S5.SS2.p2.1.m1.5.5.5.1"></times><ci id="S5.SS2.p2.1.m1.5.5.5.2.cmml" xref="S5.SS2.p2.1.m1.5.5.5.2">𝐾</ci><ci id="S5.SS2.p2.1.m1.5.5.5.3.cmml" xref="S5.SS2.p2.1.m1.5.5.5.3">𝐵</ci></apply><set id="S5.SS2.p2.1.m1.5.5.3.4.cmml" xref="S5.SS2.p2.1.m1.5.5.3.3"><apply id="S5.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S5.SS2.p2.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.3.3.1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.3.3.1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.3.3.1.1.1.2">𝑓</ci><cn type="integer" id="S5.SS2.p2.1.m1.3.3.1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.3.3.1.1.1.3">1</cn></apply><apply id="S5.SS2.p2.1.m1.4.4.2.2.2.cmml" xref="S5.SS2.p2.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.4.4.2.2.2.1.cmml" xref="S5.SS2.p2.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S5.SS2.p2.1.m1.4.4.2.2.2.2.cmml" xref="S5.SS2.p2.1.m1.4.4.2.2.2.2">𝑓</ci><cn type="integer" id="S5.SS2.p2.1.m1.4.4.2.2.2.3.cmml" xref="S5.SS2.p2.1.m1.4.4.2.2.2.3">2</cn></apply><ci id="S5.SS2.p2.1.m1.2.2.cmml" xref="S5.SS2.p2.1.m1.2.2">…</ci><apply id="S5.SS2.p2.1.m1.5.5.3.3.3.cmml" xref="S5.SS2.p2.1.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.5.5.3.3.3.1.cmml" xref="S5.SS2.p2.1.m1.5.5.3.3.3">subscript</csymbol><ci id="S5.SS2.p2.1.m1.5.5.3.3.3.2.cmml" xref="S5.SS2.p2.1.m1.5.5.3.3.3.2">𝑓</ci><apply id="S5.SS2.p2.1.m1.1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1"><abs id="S5.SS2.p2.1.m1.1.1.1.2.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.2"></abs><apply id="S5.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1"><times id="S5.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.1"></times><ci id="S5.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.2">𝐾</ci><ci id="S5.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.1.1.1.3">𝐵</ci></apply></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.5c">KB=\{f_{1},f_{2},...,f_{|KB|}\}</annotation></semantics></math>. Each fact <math id="S5.SS2.p2.2.m2.3" class="ltx_Math" alttext="f=(x,r,y)" display="inline"><semantics id="S5.SS2.p2.2.m2.3a"><mrow id="S5.SS2.p2.2.m2.3.4" xref="S5.SS2.p2.2.m2.3.4.cmml"><mi id="S5.SS2.p2.2.m2.3.4.2" xref="S5.SS2.p2.2.m2.3.4.2.cmml">f</mi><mo id="S5.SS2.p2.2.m2.3.4.1" xref="S5.SS2.p2.2.m2.3.4.1.cmml">=</mo><mrow id="S5.SS2.p2.2.m2.3.4.3.2" xref="S5.SS2.p2.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S5.SS2.p2.2.m2.3.4.3.2.1" xref="S5.SS2.p2.2.m2.3.4.3.1.cmml">(</mo><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">x</mi><mo id="S5.SS2.p2.2.m2.3.4.3.2.2" xref="S5.SS2.p2.2.m2.3.4.3.1.cmml">,</mo><mi id="S5.SS2.p2.2.m2.2.2" xref="S5.SS2.p2.2.m2.2.2.cmml">r</mi><mo id="S5.SS2.p2.2.m2.3.4.3.2.3" xref="S5.SS2.p2.2.m2.3.4.3.1.cmml">,</mo><mi id="S5.SS2.p2.2.m2.3.3" xref="S5.SS2.p2.2.m2.3.3.cmml">y</mi><mo stretchy="false" id="S5.SS2.p2.2.m2.3.4.3.2.4" xref="S5.SS2.p2.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.3b"><apply id="S5.SS2.p2.2.m2.3.4.cmml" xref="S5.SS2.p2.2.m2.3.4"><eq id="S5.SS2.p2.2.m2.3.4.1.cmml" xref="S5.SS2.p2.2.m2.3.4.1"></eq><ci id="S5.SS2.p2.2.m2.3.4.2.cmml" xref="S5.SS2.p2.2.m2.3.4.2">𝑓</ci><vector id="S5.SS2.p2.2.m2.3.4.3.1.cmml" xref="S5.SS2.p2.2.m2.3.4.3.2"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝑥</ci><ci id="S5.SS2.p2.2.m2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2">𝑟</ci><ci id="S5.SS2.p2.2.m2.3.3.cmml" xref="S5.SS2.p2.2.m2.3.3">𝑦</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.3c">f=(x,r,y)</annotation></semantics></math> is formed of a visual concept grounded in the image (<math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">x</annotation></semantics></math>), an attribute or phrase (<math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">y</annotation></semantics></math>), and a relation linking the two <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><ci id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">r</annotation></semantics></math>. Relationships exist in a predefined set of 13 different ways a concept and attribute can be related. Their work first reduces the search space to the 100 facts most likely to contain the correct answer by using GloVe embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> of words in the question and facts before further reducing it to the most relevant facts <math id="S5.SS2.p2.6.m6.1" class="ltx_Math" alttext="f_{rel}" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><msub id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mi id="S5.SS2.p2.6.m6.1.1.2" xref="S5.SS2.p2.6.m6.1.1.2.cmml">f</mi><mrow id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml"><mi id="S5.SS2.p2.6.m6.1.1.3.2" xref="S5.SS2.p2.6.m6.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.1.1.3.1" xref="S5.SS2.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.6.m6.1.1.3.3" xref="S5.SS2.p2.6.m6.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.1.1.3.1a" xref="S5.SS2.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.6.m6.1.1.3.4" xref="S5.SS2.p2.6.m6.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.2">𝑓</ci><apply id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3"><times id="S5.SS2.p2.6.m6.1.1.3.1.cmml" xref="S5.SS2.p2.6.m6.1.1.3.1"></times><ci id="S5.SS2.p2.6.m6.1.1.3.2.cmml" xref="S5.SS2.p2.6.m6.1.1.3.2">𝑟</ci><ci id="S5.SS2.p2.6.m6.1.1.3.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3.3">𝑒</ci><ci id="S5.SS2.p2.6.m6.1.1.3.4.cmml" xref="S5.SS2.p2.6.m6.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">f_{rel}</annotation></semantics></math>. These most relevant facts are turned into a graph where all the visual concepts and attributes from <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="f_{rel}" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><msub id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml"><mi id="S5.SS2.p2.7.m7.1.1.2" xref="S5.SS2.p2.7.m7.1.1.2.cmml">f</mi><mrow id="S5.SS2.p2.7.m7.1.1.3" xref="S5.SS2.p2.7.m7.1.1.3.cmml"><mi id="S5.SS2.p2.7.m7.1.1.3.2" xref="S5.SS2.p2.7.m7.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.7.m7.1.1.3.1" xref="S5.SS2.p2.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.7.m7.1.1.3.3" xref="S5.SS2.p2.7.m7.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.7.m7.1.1.3.1a" xref="S5.SS2.p2.7.m7.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.7.m7.1.1.3.4" xref="S5.SS2.p2.7.m7.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><apply id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.7.m7.1.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S5.SS2.p2.7.m7.1.1.2.cmml" xref="S5.SS2.p2.7.m7.1.1.2">𝑓</ci><apply id="S5.SS2.p2.7.m7.1.1.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3"><times id="S5.SS2.p2.7.m7.1.1.3.1.cmml" xref="S5.SS2.p2.7.m7.1.1.3.1"></times><ci id="S5.SS2.p2.7.m7.1.1.3.2.cmml" xref="S5.SS2.p2.7.m7.1.1.3.2">𝑟</ci><ci id="S5.SS2.p2.7.m7.1.1.3.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3.3">𝑒</ci><ci id="S5.SS2.p2.7.m7.1.1.3.4.cmml" xref="S5.SS2.p2.7.m7.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">f_{rel}</annotation></semantics></math> form the nodes. An edge joins two nodes if they are related by a fact in <math id="S5.SS2.p2.8.m8.1" class="ltx_Math" alttext="f_{rel}" display="inline"><semantics id="S5.SS2.p2.8.m8.1a"><msub id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml"><mi id="S5.SS2.p2.8.m8.1.1.2" xref="S5.SS2.p2.8.m8.1.1.2.cmml">f</mi><mrow id="S5.SS2.p2.8.m8.1.1.3" xref="S5.SS2.p2.8.m8.1.1.3.cmml"><mi id="S5.SS2.p2.8.m8.1.1.3.2" xref="S5.SS2.p2.8.m8.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.8.m8.1.1.3.1" xref="S5.SS2.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.8.m8.1.1.3.3" xref="S5.SS2.p2.8.m8.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.8.m8.1.1.3.1a" xref="S5.SS2.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.8.m8.1.1.3.4" xref="S5.SS2.p2.8.m8.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><apply id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.8.m8.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S5.SS2.p2.8.m8.1.1.2.cmml" xref="S5.SS2.p2.8.m8.1.1.2">𝑓</ci><apply id="S5.SS2.p2.8.m8.1.1.3.cmml" xref="S5.SS2.p2.8.m8.1.1.3"><times id="S5.SS2.p2.8.m8.1.1.3.1.cmml" xref="S5.SS2.p2.8.m8.1.1.3.1"></times><ci id="S5.SS2.p2.8.m8.1.1.3.2.cmml" xref="S5.SS2.p2.8.m8.1.1.3.2">𝑟</ci><ci id="S5.SS2.p2.8.m8.1.1.3.3.cmml" xref="S5.SS2.p2.8.m8.1.1.3.3">𝑒</ci><ci id="S5.SS2.p2.8.m8.1.1.3.4.cmml" xref="S5.SS2.p2.8.m8.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">f_{rel}</annotation></semantics></math>. A GCN is then used to ‘reason’ over the graph to predict the final answer. Using a message passing architecture, the authors are able to update the feature representations of the nodes which can then be fed into an MLP which predicts a binary label corresponding to whether or not the entity contains the answer.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Zhu <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> use a multi-modal graph approach to representing images with a visual, semantic, and knowledge graph. After graph construction, GCNs are applied to each modality to create richer feature embeddings. These embeddings are then processed in a cross-modal manner. Visual-Fact aggregation and Semantic-Fact aggregation operations produce complimentary information which is then used with a Fact-Fact convolutional layer. This final layer takes into account all three modalities and produces an answer that considers the global context. The authors continue their work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> by changing the cross-modal mechanism for a novel GRUC (Graph-based Read, Update, and Control) mechanism. The GRUC operates in a parallel pipeline. One pipeline starts with a concept from the knowledge graph and recurrently incorporates knowledge from the visual graph. Another starts with the same knowledge graph concept but incorporates semantic knowledge. At the end of the recurrent operations, the outputs of the two pipelines are fused together with the question and original fact node. This fused feature is then used to predict the final answer. The change made to the cross-modal attention mechanism yields significant improvements in the F-VQA benchmark when compared with MUCKO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Liu <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> also adopt a multi-modal approach, but use only the semantic and knowledge modalities. They propose a dual process system for FVQA that is based on the Dual-Process Theory from Cognitive Science <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>. Their approach utilises a BERT encoder to represent the input question and a Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> based feature extractor to represent the image features. The first of the two systems, based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, joins these two representations into a single multi-modal representation. The second system then develops a semantic graph by turning dense region captions into textual scene graphs (using SPICE), as well as a knowledge graph generated using the question input. A message passing GNN is then used to identify the important nodes and aggregate information between them using an attention weighting. A joint representation for each knowledge graph node is then learned by combining the whole semantic graph with the node with relation to an attention weighting. This joint representation is then used to predict the final answer.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Moving away from the multi-modal approach, SGEITL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> makes a semantic graph of the image and then follows Yang <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and introduces skip edges to the graph, essentially making it a complete graph. This graph then goes through a multi-hop graph Transformer, which masks the attention between nodes based on their distance, ensuring that only close by nodes are attended to. Through their work, they demonstrate that structural information is useful when approaching the complex VQA task.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.3" class="ltx_p">With their TRiG model, Gao <span id="S5.SS2.p6.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> advocate taking an alternative approach to FVQA and rather than generating the answer in some multi-modal space, they propose to use the textual space. They argue that this prevents further fusion with additional outside knowledge, and that as most of this data are in textual form, it makes sense to work in that domain. TRiG therefore has three components. It first converts the image into a caption using an off-the-shelf image captioning tool. The model then finds the top <math id="S5.SS2.p6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p6.1.m1.1a"><mi id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><ci id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">K</annotation></semantics></math> relevant facts from a knowledge base of Wikipedia articles before using a T5 backboned Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> to fuse and decode the <math id="S5.SS2.p6.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.SS2.p6.2.m2.1a"><mo id="S5.SS2.p6.2.m2.1.1" xref="S5.SS2.p6.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.2.m2.1b"><lt id="S5.SS2.p6.2.m2.1.1.cmml" xref="S5.SS2.p6.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.2.m2.1c">&lt;</annotation></semantics></math>question, visual context, knowledge<math id="S5.SS2.p6.3.m3.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.SS2.p6.3.m3.1a"><mo id="S5.SS2.p6.3.m3.1.1" xref="S5.SS2.p6.3.m3.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.3.m3.1b"><gt id="S5.SS2.p6.3.m3.1.1.cmml" xref="S5.SS2.p6.3.m3.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.3.m3.1c">&gt;</annotation></semantics></math> triplet into an answer.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>A table showing the model details and results of selected models trained and tested against the OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and F-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> datasets. <span id="S5.T5.2.1" class="ltx_text ltx_font_bold">Bold</span>: Best score.</figcaption>
<div id="S5.T5.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:44.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-217.8pt,22.5pt) scale(0.498898858381482,0.498898858381482) ;">
<table id="S5.T5.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.3.1.1.1" class="ltx_tr">
<th id="S5.T5.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Model</th>
<th id="S5.T5.3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Graphs Used</th>
<th id="S5.T5.3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">GNN Architecture</th>
<th id="S5.T5.3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">OK-VQA Top-1 Results</th>
<th id="S5.T5.3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">OK-VQA Top-3 Results</th>
<th id="S5.T5.3.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column">F-VQA Top-1 Results</th>
<th id="S5.T5.3.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column">F-VQA Top-3 Results</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.3.1.2.1" class="ltx_tr">
<td id="S5.T5.3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Out of the box <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S5.T5.3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Knowledge</td>
<td id="S5.T5.3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GCN</td>
<td id="S5.T5.3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S5.T5.3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S5.T5.3.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">69.35</td>
<td id="S5.T5.3.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">80.25</td>
</tr>
<tr id="S5.T5.3.1.3.2" class="ltx_tr">
<td id="S5.T5.3.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">Mucko <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S5.T5.3.1.3.2.2" class="ltx_td ltx_align_left">Visual, Semantic, Knowledge</td>
<td id="S5.T5.3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">GCN</td>
<td id="S5.T5.3.1.3.2.4" class="ltx_td ltx_align_left">-</td>
<td id="S5.T5.3.1.3.2.5" class="ltx_td ltx_align_left">-</td>
<td id="S5.T5.3.1.3.2.6" class="ltx_td ltx_align_left">73.06</td>
<td id="S5.T5.3.1.3.2.7" class="ltx_td ltx_align_left">85.94</td>
</tr>
<tr id="S5.T5.3.1.4.3" class="ltx_tr">
<td id="S5.T5.3.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">GRUC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S5.T5.3.1.4.3.2" class="ltx_td ltx_align_left">Visual, Semantic, Knowledge</td>
<td id="S5.T5.3.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">GCN</td>
<td id="S5.T5.3.1.4.3.4" class="ltx_td ltx_align_left"><span id="S5.T5.3.1.4.3.4.1" class="ltx_text ltx_font_bold">29.87</span></td>
<td id="S5.T5.3.1.4.3.5" class="ltx_td ltx_align_left">32.65</td>
<td id="S5.T5.3.1.4.3.6" class="ltx_td ltx_align_left"><span id="S5.T5.3.1.4.3.6.1" class="ltx_text ltx_font_bold">79.63</span></td>
<td id="S5.T5.3.1.4.3.7" class="ltx_td ltx_align_left"><span id="S5.T5.3.1.4.3.7.1" class="ltx_text ltx_font_bold">91.20</span></td>
</tr>
<tr id="S5.T5.3.1.5.4" class="ltx_tr">
<td id="S5.T5.3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">Dual Process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>
</td>
<td id="S5.T5.3.1.5.4.2" class="ltx_td ltx_align_left">Semantic, Knowledge</td>
<td id="S5.T5.3.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">GCN</td>
<td id="S5.T5.3.1.5.4.4" class="ltx_td ltx_align_left">29.43</td>
<td id="S5.T5.3.1.5.4.5" class="ltx_td ltx_align_left"><span id="S5.T5.3.1.5.4.5.1" class="ltx_text ltx_font_bold">32.83</span></td>
<td id="S5.T5.3.1.5.4.6" class="ltx_td ltx_align_left">63.57</td>
<td id="S5.T5.3.1.5.4.7" class="ltx_td ltx_align_left">76.47</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Text VQA</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">TextVQA is the sub-task of VQA where the answers require the model to be able to read text that appears in images. Typically this involves tasks like reading brand names from buildings or the title of book covers. This information can then be combined with an external knowledge base, enabling the models to answer questions such as “Is the shop an American brand?” by reading the shop name and searching it in a knowledge base.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Gao <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> focus on the in-image text and how it can be better leveraged to improve VQA. They use a novel multi-modal graph made up of fully connected visual, semantic, and numeric subgraphs. Each subgraph represents a unique modality that can be found in an image: visual entities (represented by image feature extractors), semantic meaning of discovered text (initially discovered by OCR), along with numeric values and their semantic meaning. The paper proposed a model that aggregates information across modalities together using a relevance score. Once the three modalities have been aggregated, an attention mechanism is deployed to help predict the final answer. The focus on different modalities proves a useful approach, with the model performing favourably in benchmarks (see Table <a href="#S5.T6" title="TABLE VI ‣ V-C Text VQA ‣ V Visual Question Answering ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>).</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Another work that makes use of multi-modal graphs is Liang <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. Their work uses both image features and scene text features (extracted by OCR) to generate a spatial relationship graph similar to that of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The graph undergoes multi-head attention before being processed by a GNN that makes use of the attention weights. Multi-modal fusion is then used to join the node features with the question embedding and positional features. The output of this fusion operation is then used to predict a final answer.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>A table showing the model details and TextVQA-Val Accuracy results of selected Text-VQA models. <span id="S5.T6.2.1" class="ltx_text ltx_font_bold">Bold</span>: Best score.</figcaption>
<div id="S5.T6.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:32.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-147.7pt,10.9pt) scale(0.594859084118148,0.594859084118148) ;">
<table id="S5.T6.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.3.1.1.1" class="ltx_tr">
<th id="S5.T6.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Model</th>
<th id="S5.T6.3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Graphs Used</th>
<th id="S5.T6.3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">GNN Architecture</th>
<th id="S5.T6.3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">TextVQA-Val Accuracy (%)</th>
<th id="S5.T6.3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column">TextVQA-Test Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.3.1.2.1" class="ltx_tr">
<th id="S5.T6.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MM-GNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
</th>
<td id="S5.T6.3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Visual, Textual Semantic, Numeric</td>
<td id="S5.T6.3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Multi-modal GNN</td>
<td id="S5.T6.3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T6.3.1.2.1.4.1" class="ltx_text ltx_font_bold">31.44</span></td>
<td id="S5.T6.3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T6.3.1.2.1.5.1" class="ltx_text ltx_font_bold">31.10</span></td>
</tr>
<tr id="S5.T6.3.1.3.2" class="ltx_tr">
<th id="S5.T6.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MCG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</th>
<td id="S5.T6.3.1.3.2.2" class="ltx_td ltx_align_left">Spatial</td>
<td id="S5.T6.3.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r">Multi-modal (contextual) GNN</td>
<td id="S5.T6.3.1.3.2.4" class="ltx_td ltx_align_left">29.40</td>
<td id="S5.T6.3.1.3.2.5" class="ltx_td ltx_align_left">29.61</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Image Retrieval</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Image retrieval is the task of finding images from a database given some query. These queries can take many forms, including a similar image, a natural language query, or even a sketch. A common approach is to represent the database images as being in some space, where similar images are those with a minimal distance to the query. When this space is represented using graphs, GNNs become valuable for sharing features and acquiring more global context for the features.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Johnson <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> show that a scene graph can be used as the input of the image retrieval system. By allowing end users to create a scene graph where nodes represent objects, attributes, and relationships, they are able to return appropriate images via a scene graph grounding process. This involves matching each scene graph object node with a bounding box predicted by an object detector, and is represented probabilistically using a conditional random field (CRF). The advantage of using scene graphs as search queries over natural language is that they scale well in terms of complexity. Once a basic scene graph has been constructed, it is straightforward for it to be extended and made more complex by adding additional nodes. Another advantage is that it reduces the operations required to map the search query to the image.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Following on from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Yoon <span id="S6.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> propose IRSGS (Image Retrieval with Scene Graph Similarity) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, which makes use of a semantic graph, referred to as a scene graph in the paper. Given a query image, the model will generate a semantic graph and compare its similarity with graphs of images in the database. This graph comparison is achieved by taking the inner product of graph embeddings generated by a GNN (either GCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> or GIN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>). One key contribution of the paper is the concept of Surrogate Relevance, which is the similarity between the captions of the images being compared. Surrogate Relevance is calculated using the inner product between Sentence-BERT embeddings of the captions. This measure is used as the training signal of the model to hone the feature embeddings generated by the GNN. The graph-to-graph comparison behind the model allows this work to better scale to large image databases when compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The use of Surrogate Relevance allows the work to be potentially expanded to match against user queries if they are in the style of the captions used to power the relevance measure.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Using a <math id="S6.p4.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.p4.1.m1.1a"><mi id="S6.p4.1.m1.1.1" xref="S6.p4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.p4.1.m1.1b"><ci id="S6.p4.1.m1.1.1.cmml" xref="S6.p4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p4.1.m1.1c">K</annotation></semantics></math>-nearest neighbour graph of images represented as feature embeddings, Liu <span id="S6.p4.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> propose using a GCN alongside a novel loss function based on image similarity. The feature embeddings are enhanced to account for a global context across the whole image database using a GCN. Similarity between images is calculated by taking the inner product of the feature embeddings. The higher the similarity, the better the retrieval candidate. The author’s novel loss function is designed to move similar images closer together in the embedding space and dissimilar images further apart. Compared with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, by using the inner product, the similarity measure is far more deterministic. However, unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, it cannot be expanded to work alongside text-based image retrieval with a user query.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Zhang <span id="S6.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> also use a <math id="S6.p5.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.p5.1.m1.1a"><mi id="S6.p5.1.m1.1.1" xref="S6.p5.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.p5.1.m1.1b"><ci id="S6.p5.1.m1.1.1.cmml" xref="S6.p5.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p5.1.m1.1c">K</annotation></semantics></math>-nearest neighbour graph, but focus on improving the re-ranking process in content based image retrieval. A GNN is applied to aggregate features created from a modified adjacency matrix. Using a GNN allows the re-ranking process to de-emphasise nodes with a low confidence score.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.5" class="ltx_p">Rather than use a pure <math id="S6.p6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.p6.1.m1.1a"><mi id="S6.p6.1.m1.1.1" xref="S6.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.p6.1.m1.1b"><ci id="S6.p6.1.m1.1.1.cmml" xref="S6.p6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.1.m1.1c">K</annotation></semantics></math>-nearest neighbour graph, the DGCQ model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> is based on vector quantisation, a process from Information Theory for reducing the cardinality of a vector space. It can essentially be thought of as a many-to-one clustering technique where vectors in one space <math id="S6.p6.2.m2.1" class="ltx_Math" alttext="x\in\mathbb{R}^{d}" display="inline"><semantics id="S6.p6.2.m2.1a"><mrow id="S6.p6.2.m2.1.1" xref="S6.p6.2.m2.1.1.cmml"><mi id="S6.p6.2.m2.1.1.2" xref="S6.p6.2.m2.1.1.2.cmml">x</mi><mo id="S6.p6.2.m2.1.1.1" xref="S6.p6.2.m2.1.1.1.cmml">∈</mo><msup id="S6.p6.2.m2.1.1.3" xref="S6.p6.2.m2.1.1.3.cmml"><mi id="S6.p6.2.m2.1.1.3.2" xref="S6.p6.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S6.p6.2.m2.1.1.3.3" xref="S6.p6.2.m2.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.2.m2.1b"><apply id="S6.p6.2.m2.1.1.cmml" xref="S6.p6.2.m2.1.1"><in id="S6.p6.2.m2.1.1.1.cmml" xref="S6.p6.2.m2.1.1.1"></in><ci id="S6.p6.2.m2.1.1.2.cmml" xref="S6.p6.2.m2.1.1.2">𝑥</ci><apply id="S6.p6.2.m2.1.1.3.cmml" xref="S6.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S6.p6.2.m2.1.1.3.1.cmml" xref="S6.p6.2.m2.1.1.3">superscript</csymbol><ci id="S6.p6.2.m2.1.1.3.2.cmml" xref="S6.p6.2.m2.1.1.3.2">ℝ</ci><ci id="S6.p6.2.m2.1.1.3.3.cmml" xref="S6.p6.2.m2.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.2.m2.1c">x\in\mathbb{R}^{d}</annotation></semantics></math> are mapped to a set of code words (<math id="S6.p6.3.m3.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S6.p6.3.m3.1a"><msub id="S6.p6.3.m3.1.1" xref="S6.p6.3.m3.1.1.cmml"><mi id="S6.p6.3.m3.1.1.2" xref="S6.p6.3.m3.1.1.2.cmml">c</mi><mi id="S6.p6.3.m3.1.1.3" xref="S6.p6.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p6.3.m3.1b"><apply id="S6.p6.3.m3.1.1.cmml" xref="S6.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S6.p6.3.m3.1.1.1.cmml" xref="S6.p6.3.m3.1.1">subscript</csymbol><ci id="S6.p6.3.m3.1.1.2.cmml" xref="S6.p6.3.m3.1.1.2">𝑐</ci><ci id="S6.p6.3.m3.1.1.3.cmml" xref="S6.p6.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.3.m3.1c">c_{i}</annotation></semantics></math>) that make up a code book <math id="S6.p6.4.m4.3" class="ltx_Math" alttext="q(x)\in\mathcal{C}=\{c_{i};i\in\mathcal{I}\}" display="inline"><semantics id="S6.p6.4.m4.3a"><mrow id="S6.p6.4.m4.3.3" xref="S6.p6.4.m4.3.3.cmml"><mrow id="S6.p6.4.m4.3.3.3" xref="S6.p6.4.m4.3.3.3.cmml"><mi id="S6.p6.4.m4.3.3.3.2" xref="S6.p6.4.m4.3.3.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S6.p6.4.m4.3.3.3.1" xref="S6.p6.4.m4.3.3.3.1.cmml">​</mo><mrow id="S6.p6.4.m4.3.3.3.3.2" xref="S6.p6.4.m4.3.3.3.cmml"><mo stretchy="false" id="S6.p6.4.m4.3.3.3.3.2.1" xref="S6.p6.4.m4.3.3.3.cmml">(</mo><mi id="S6.p6.4.m4.1.1" xref="S6.p6.4.m4.1.1.cmml">x</mi><mo stretchy="false" id="S6.p6.4.m4.3.3.3.3.2.2" xref="S6.p6.4.m4.3.3.3.cmml">)</mo></mrow></mrow><mo id="S6.p6.4.m4.3.3.4" xref="S6.p6.4.m4.3.3.4.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S6.p6.4.m4.3.3.5" xref="S6.p6.4.m4.3.3.5.cmml">𝒞</mi><mo id="S6.p6.4.m4.3.3.6" xref="S6.p6.4.m4.3.3.6.cmml">=</mo><mrow id="S6.p6.4.m4.3.3.1.1" xref="S6.p6.4.m4.3.3.1.2.cmml"><mo stretchy="false" id="S6.p6.4.m4.3.3.1.1.2" xref="S6.p6.4.m4.3.3.1.2.cmml">{</mo><mrow id="S6.p6.4.m4.3.3.1.1.1" xref="S6.p6.4.m4.3.3.1.1.1.cmml"><mrow id="S6.p6.4.m4.3.3.1.1.1.1.1" xref="S6.p6.4.m4.3.3.1.1.1.1.2.cmml"><msub id="S6.p6.4.m4.3.3.1.1.1.1.1.1" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1.cmml"><mi id="S6.p6.4.m4.3.3.1.1.1.1.1.1.2" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1.2.cmml">c</mi><mi id="S6.p6.4.m4.3.3.1.1.1.1.1.1.3" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.p6.4.m4.3.3.1.1.1.1.1.2" xref="S6.p6.4.m4.3.3.1.1.1.1.2.cmml">;</mo><mi id="S6.p6.4.m4.2.2" xref="S6.p6.4.m4.2.2.cmml">i</mi></mrow><mo id="S6.p6.4.m4.3.3.1.1.1.2" xref="S6.p6.4.m4.3.3.1.1.1.2.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S6.p6.4.m4.3.3.1.1.1.3" xref="S6.p6.4.m4.3.3.1.1.1.3.cmml">ℐ</mi></mrow><mo stretchy="false" id="S6.p6.4.m4.3.3.1.1.3" xref="S6.p6.4.m4.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.4.m4.3b"><apply id="S6.p6.4.m4.3.3.cmml" xref="S6.p6.4.m4.3.3"><and id="S6.p6.4.m4.3.3a.cmml" xref="S6.p6.4.m4.3.3"></and><apply id="S6.p6.4.m4.3.3b.cmml" xref="S6.p6.4.m4.3.3"><in id="S6.p6.4.m4.3.3.4.cmml" xref="S6.p6.4.m4.3.3.4"></in><apply id="S6.p6.4.m4.3.3.3.cmml" xref="S6.p6.4.m4.3.3.3"><times id="S6.p6.4.m4.3.3.3.1.cmml" xref="S6.p6.4.m4.3.3.3.1"></times><ci id="S6.p6.4.m4.3.3.3.2.cmml" xref="S6.p6.4.m4.3.3.3.2">𝑞</ci><ci id="S6.p6.4.m4.1.1.cmml" xref="S6.p6.4.m4.1.1">𝑥</ci></apply><ci id="S6.p6.4.m4.3.3.5.cmml" xref="S6.p6.4.m4.3.3.5">𝒞</ci></apply><apply id="S6.p6.4.m4.3.3c.cmml" xref="S6.p6.4.m4.3.3"><eq id="S6.p6.4.m4.3.3.6.cmml" xref="S6.p6.4.m4.3.3.6"></eq><share href="#S6.p6.4.m4.3.3.5.cmml" id="S6.p6.4.m4.3.3d.cmml" xref="S6.p6.4.m4.3.3"></share><set id="S6.p6.4.m4.3.3.1.2.cmml" xref="S6.p6.4.m4.3.3.1.1"><apply id="S6.p6.4.m4.3.3.1.1.1.cmml" xref="S6.p6.4.m4.3.3.1.1.1"><in id="S6.p6.4.m4.3.3.1.1.1.2.cmml" xref="S6.p6.4.m4.3.3.1.1.1.2"></in><list id="S6.p6.4.m4.3.3.1.1.1.1.2.cmml" xref="S6.p6.4.m4.3.3.1.1.1.1.1"><apply id="S6.p6.4.m4.3.3.1.1.1.1.1.1.cmml" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.p6.4.m4.3.3.1.1.1.1.1.1.1.cmml" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S6.p6.4.m4.3.3.1.1.1.1.1.1.2.cmml" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1.2">𝑐</ci><ci id="S6.p6.4.m4.3.3.1.1.1.1.1.1.3.cmml" xref="S6.p6.4.m4.3.3.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S6.p6.4.m4.2.2.cmml" xref="S6.p6.4.m4.2.2">𝑖</ci></list><ci id="S6.p6.4.m4.3.3.1.1.1.3.cmml" xref="S6.p6.4.m4.3.3.1.1.1.3">ℐ</ci></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.4.m4.3c">q(x)\in\mathcal{C}=\{c_{i};i\in\mathcal{I}\}</annotation></semantics></math>. Where <math id="S6.p6.5.m5.1" class="ltx_Math" alttext="\mathcal{I}=1...(k-1)" display="inline"><semantics id="S6.p6.5.m5.1a"><mrow id="S6.p6.5.m5.1.1" xref="S6.p6.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.p6.5.m5.1.1.3" xref="S6.p6.5.m5.1.1.3.cmml">ℐ</mi><mo id="S6.p6.5.m5.1.1.2" xref="S6.p6.5.m5.1.1.2.cmml">=</mo><mrow id="S6.p6.5.m5.1.1.1" xref="S6.p6.5.m5.1.1.1.cmml"><mn id="S6.p6.5.m5.1.1.1.3" xref="S6.p6.5.m5.1.1.1.3.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.p6.5.m5.1.1.1.2" xref="S6.p6.5.m5.1.1.1.2.cmml">​</mo><mi mathvariant="normal" id="S6.p6.5.m5.1.1.1.4" xref="S6.p6.5.m5.1.1.1.4.cmml">…</mi><mo lspace="0em" rspace="0em" id="S6.p6.5.m5.1.1.1.2a" xref="S6.p6.5.m5.1.1.1.2.cmml">​</mo><mrow id="S6.p6.5.m5.1.1.1.1.1" xref="S6.p6.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S6.p6.5.m5.1.1.1.1.1.2" xref="S6.p6.5.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.p6.5.m5.1.1.1.1.1.1" xref="S6.p6.5.m5.1.1.1.1.1.1.cmml"><mi id="S6.p6.5.m5.1.1.1.1.1.1.2" xref="S6.p6.5.m5.1.1.1.1.1.1.2.cmml">k</mi><mo id="S6.p6.5.m5.1.1.1.1.1.1.1" xref="S6.p6.5.m5.1.1.1.1.1.1.1.cmml">−</mo><mn id="S6.p6.5.m5.1.1.1.1.1.1.3" xref="S6.p6.5.m5.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S6.p6.5.m5.1.1.1.1.1.3" xref="S6.p6.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.p6.5.m5.1b"><apply id="S6.p6.5.m5.1.1.cmml" xref="S6.p6.5.m5.1.1"><eq id="S6.p6.5.m5.1.1.2.cmml" xref="S6.p6.5.m5.1.1.2"></eq><ci id="S6.p6.5.m5.1.1.3.cmml" xref="S6.p6.5.m5.1.1.3">ℐ</ci><apply id="S6.p6.5.m5.1.1.1.cmml" xref="S6.p6.5.m5.1.1.1"><times id="S6.p6.5.m5.1.1.1.2.cmml" xref="S6.p6.5.m5.1.1.1.2"></times><cn type="integer" id="S6.p6.5.m5.1.1.1.3.cmml" xref="S6.p6.5.m5.1.1.1.3">1</cn><ci id="S6.p6.5.m5.1.1.1.4.cmml" xref="S6.p6.5.m5.1.1.1.4">…</ci><apply id="S6.p6.5.m5.1.1.1.1.1.1.cmml" xref="S6.p6.5.m5.1.1.1.1.1"><minus id="S6.p6.5.m5.1.1.1.1.1.1.1.cmml" xref="S6.p6.5.m5.1.1.1.1.1.1.1"></minus><ci id="S6.p6.5.m5.1.1.1.1.1.1.2.cmml" xref="S6.p6.5.m5.1.1.1.1.1.1.2">𝑘</ci><cn type="integer" id="S6.p6.5.m5.1.1.1.1.1.1.3.cmml" xref="S6.p6.5.m5.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p6.5.m5.1c">\mathcal{I}=1...(k-1)</annotation></semantics></math>. By using vector quantisation, the model learns code words that can be combined with image features to form landmark graph. This graph is based on the similarity graph except it also has nodes learned through the quantisation process. Once the landmark graph has been constructed, a GCN is use to propagate features with the objective of moving similar images closer together in the feature space. The use of vector quantisation allows for the landmark graph to exist in a lower dimensional space, reducing computation when computing which images from the graph to return as candidates.</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p">The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> move to adopt a multi-modal approach. They use GraphSAGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> to effectively learn multi-modal node embeddings containing visual and conceptual information from the connections in the graph. The distance between connected nodes are reduced, whilst the distance between disconnected nodes is increased. By using graph nodes that represent images as well as nodes representing metadata tags, their model is able to provide content-based image retrieval as well as tag prediction. At inference time, images shown to the model can be attached to the graph through their <math id="S6.p7.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.p7.1.m1.1a"><mi id="S6.p7.1.m1.1.1" xref="S6.p7.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.p7.1.m1.1b"><ci id="S6.p7.1.m1.1.1.cmml" xref="S6.p7.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p7.1.m1.1c">K</annotation></semantics></math> nearest images, attached to relevant tags, or both. Unlike previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Misraa <span id="S6.p7.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> make use of multi-modal embeddings in the graph nodes.</p>
</div>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">Schuster <span id="S6.p8.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> continue the work of Johnson <span id="S6.p8.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, by creating a natural language parser that converts a query into a scene graph that can be processed by their work. This allows them to go beyond content-based image retrieval and move into text-based image retrieval. Their parser works by creating a dependency tree using the Stanford Dependency Parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and then modifying the tree. They first execute a quantification modifier that ensures nouns are the head of the phrase. This is followed by pronoun resolution to make the relationship between two objects more explicit. Finally, plural nouns are processed. This involves copying noun instances when numeric modifiers are given. This textual scene graph is then mapped to images following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S6.p9" class="ltx_para">
<p id="S6.p9.1" class="ltx_p">Cui <span id="S6.p9.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> also tackle text-based image retrieval. They present work that makes use of a GCN to provide cross-modal reasoning on visual and textual information. Input features are split into channels which form a complete graph and undergo graph convolution. Once the textual and visual features are projected into a common space, they have their distances measured using the cosine similarity. These similarity scores are then stored in a matrix representing the similarities between visual and textual inputs.</p>
</div>
<div id="S6.p10" class="ltx_para">
<p id="S6.p10.1" class="ltx_p">Zhang <span id="S6.p10.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> tackle the challenging task of Composing Text and Image to Image Retrieval, where given a reference image and modification query the image retrieval system must find an image similar to the reference that contains the modifications outlined in the query. The principle challenge of this emerging task is its cross-modality nature. The authors tackle this challenge by first generating a spatial graph of the reference image and a textual feature of the modification query. These features are then concatenated before the graph is processed by a GAT whose attention mechanism has been altered to account for the directionality of the graph and the spatial data it encodes. A collection of GRUs that form a Global Semantic Reasoning (GSR) unit are then used to create the final embedding for the reference image. The same process is used on the target image but without the concatenation of the textual feature. A cross-modal loss function and adversarial loss function are combined to ensure that the features outputted by the GSR of the same category are moved closer together.</p>
</div>
<div id="S6.p11" class="ltx_para">
<p id="S6.p11.1" class="ltx_p">Chaudhuri <span id="S6.p11.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> adopt a Siamese-based network architecture where two similar inputs go into two separate networks that share weights. This network architecture typically uses contrastive loss or triplet loss to ensure the outputs of these networks are similar. The authors use a novel Siamese-GCN on a region adjacency graph that is formed by connecting adjacent segmented regions and weighting the edge accounting for the distance and angle between centroids of the regions. They apply their technique to high resolution remote sensing images for content-based image retrieval. By using a Siamese-GCN with contrastive loss, the authors are able to learn an embedding that brings similar images together and forces dissimilar images apart. This work is then followed up by the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>, where they add a range of attention mechanisms. They implement both node-level and edge-level attention mechanisms (in a similar style to GAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>). These attention mechanisms are then incorporated into the Siamese-GCN to yield improvements over their previous work.</p>
</div>
<div id="S6.p12" class="ltx_para">
<p id="S6.p12.1" class="ltx_p">Another work to incorporate a siamese network design was Zhang <span id="S6.p12.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>. They use a three part network design to perform zero-shot sketch-based image retrieval with a Siamese-based encoding network which creates features of the image and associated sketch using ResNet50. These features are the concatenated together to create node features. The similarity between nodes is calculated using a metric function modelled by an MLP, and this operation is used to populate the adjacency matrix of a similarity graph. A GCN is then applied to the similarity graph to create fusion embeddings of sketch-image pairs. Rather than use an MLP to reconstruct the semantic information from the GCN embeddings, the authors chose to use a Conditional Variational Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. Doing so enables the model to generate semantic information for sketches of unseen classes, aiding the zero-shot component of the model.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we draw upon the views of Battaglia <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and discuss how the popular Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> can be viewed through the lens of GNNs. We then discuss how its dependence on consistent structure may pose challenges should image generation techniques be applied to create new training data for image captioning. The section concludes with a final summary of the paper and an overview of the challenges and future research directions that lie ahead for graph-based 2D image understanding.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.4.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.5.2" class="ltx_text ltx_font_italic">Why GNNs When We Have Transformers?</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Recent years have seen the rapid rise in popularity of the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Originally proposed in the Natural Language Processing domain, it was quickly applied as a generalised encoder in computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> Further work then expanded the architecture so that it can process images directly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>, allowing it to operate as a backbone for common vision tasks. The wide range of applications the architecture can be applied to has led to it dominating much of deep learning in recent years.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">There has been some effort by the community to unify the attention-based approach with GNNs. Battaglia <span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposes a more generic Graph Network which both Transformers and GNNs fall into. They present a viewpoint where Transformers can be viewed as a neural architecture operating on a complete graph.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">Viewing GNNs and Transformers as Graph Networks shows that they share a number of similarities. Both architectures take a set of values and decide how much different values should be considered when transforming them to update the values, with GNNs ignoring nodes that are not connected and Transformers scaling the importance of an input. It is worth noting that if the graph being processed by a GNN is a complete graph, the graph network will allow all nodes to have their messages propagated to one being updated. Therefore, it is possible to view the Transformer as a special case GNN operating on a complete graph. While GNNs use the read module to take advantage of an underlying structure, the Transformer learns one based on the task.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p">By applying a Transformer to a task, a graph structure is being learnt from scratch. Meanwhile, there are plenty of graph structures that appear naturally within vision-language tasks. This multitude of graph types allow for different structures to be taken for the image, from the semantic structure of an image to the hierarchical structure of the image with regards to the entire training set. Graphs appear naturally in the language component of the tasks as well, with sentence dependency trees being closely aligned to semantic scene graphs (when the scene graph is made multipartite as in the case of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>). When clear graph representations of data exist, they should be utilised rather than ignored, rather than learning a graph structure using a more general purpose architecture. Utilising existing graph structures enables a Graph Network with the appropriate inductive biases to be deployed. It also results in fewer computations as messages are not being passed between all possible node connections.</p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.1" class="ltx_p">When it is possible to utilise multiple graphs, it is advantageous to do so when compared to using a single graph. As shown with image captioning (Table <a href="#S4.T3" title="TABLE III ‣ IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>), architectures that only use a single graph type perform sub-optimally compared to their multigraph counterparts. ARL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, Sub-GC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, and Topic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> all use a single graph (spatial, semantic, similarity respectively) and all three suffer in benchmarks. Whilst Topic performs well in BLEU, METEOR, and ROGUE, when evaluated using metrics designed specifically for image captioning (SPICE and CIDEr) its performance falters against comparable models. This theme of multigraph approaches performing more favourably is also found across the VQA, FVQA, and text-VQA tasks, with multigraph approaches outperforming their single graph counterparts.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.4.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.5.2" class="ltx_text ltx_font_italic">Latent Diffusion and the Future of Image Captioning</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Currently, image captioning techniques are constrained by their training data. As popular as COCO is within the Computer Vision community for its wide ranging scenes and generalisability to the real world, it has its shortcomings. Captioning systems trained on it alone will never understand particular art styles, or objects outside of the 80 categories covered by the COCO dataset. The advent of image generation techniques such as DALLE<math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mo id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><ci id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">\cdot</annotation></semantics></math>2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> present an opportunity for image captioning systems to go well beyond an 80 category limit and start understanding various stylistic elements of images. Work in this area is in its infancy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>, but previous non-generative unsupervised approaches to image captioning are very promising <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.2" class="ltx_p">We speculate that latent diffusion-based captioning may be a promising avenue of research. However, for this approach to work effectively, image generation techniques will need to develop further. Currently DALLE<math id="S7.SS2.p2.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S7.SS2.p2.1.m1.1a"><mo id="S7.SS2.p2.1.m1.1.1" xref="S7.SS2.p2.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.1.m1.1b"><ci id="S7.SS2.p2.1.m1.1.1.cmml" xref="S7.SS2.p2.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.1.m1.1c">\cdot</annotation></semantics></math>2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> and similar systems do not understand structure as deeply as would be required for them to be able to replace the training data of a captioning system. As impressive as they are, they can struggle to assemble images correctly when the prompt asks for something that is unlikely in real life. When asked to generate an image of “A monkey riding on the back of a polar bear”, DALLE<math id="S7.SS2.p2.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S7.SS2.p2.2.m2.1a"><mo id="S7.SS2.p2.2.m2.1.1" xref="S7.SS2.p2.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.2.m2.1b"><ci id="S7.SS2.p2.2.m2.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.2.m2.1c">\cdot</annotation></semantics></math>2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> can sometimes struggle to <span id="S7.SS2.p2.2.1" class="ltx_text ltx_font_italic">understand</span> the requested spatial relation between the two animals, resulting in the sample result shown in Figure <a href="#S7.F5" title="Figure 5 ‣ VII-B Latent Diffusion and the Future of Image Captioning ‣ VII Discussion and Conclusion ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S7.F5" class="ltx_figure"><img src="/html/2303.03761/assets/figures/DALLE-a-monkey-riding-on-the-back-of-a-polar-bear.png" id="S7.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="419" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>One of the images generated by OpenAI’s DALLE<math id="S7.F5.2.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S7.F5.2.m1.1b"><mo id="S7.F5.2.m1.1.1" xref="S7.F5.2.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S7.F5.2.m1.1c"><ci id="S7.F5.2.m1.1.1.cmml" xref="S7.F5.2.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.F5.2.m1.1d">\cdot</annotation></semantics></math>2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> given the prompt <span id="S7.F5.4.1" class="ltx_text ltx_font_italic">‘A monkey riding on the back of a polar bear’</span>. Note the inverted relationship in the generated image. Best viewed in colour.</figcaption>
</figure>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">Discovering examples of incorrect relationships in images is not just a case of dreaming up relationships between objects that are unlikely to exist in training data. Conwell and Ullman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> conducted a participant study where they asked <math id="S7.SS2.p3.1.m1.1" class="ltx_Math" alttext="169" display="inline"><semantics id="S7.SS2.p3.1.m1.1a"><mn id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml">169</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><cn type="integer" id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1">169</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">169</annotation></semantics></math> people to select generated images that they felt well matched a given prompt. They found that across the generated images in their study, only 22% matched the original prompt. The authors conclude that <span id="S7.SS2.p3.1.1" class="ltx_text ltx_font_italic">“current image generation models do not yet have a grasp of even basic relations involving simple objects and agents”</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>. Whilst latent diffusion methods may play a role in the future of image captioning, they have a long way to go understanding structure before this is possible. In order for Graph Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to be applicable to diffusion generated training data, the structure within the image and the caption/prompt will need to be consistent. Supervised learning approaches require large amounts of very clean training data in order to work well, so Graph Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> may struggle if the underlying structure in the image data is not as expected.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS3.4.1.1" class="ltx_text">VII-C</span> </span><span id="S7.SS3.5.2" class="ltx_text ltx_font_italic">Final Notes</span>
</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Vision-language tasks such as image captioning and VQA pose significant opportunities for accessibility technology to be developed for those with sight impairment or severe sight impairment. Having widespread automatic alt-text generation on websites and applications enabling queries about images shared online, there is substantial impact that research in these fields can have. However, models trained on current datasets are prone to the biases of sighted humans. The questions asked in VQA datasets, and the captions given in image captioning datasets do not necessarily cater to the needs of possible end users of this technology. A lot is said in the field of the technology being applied to aid those with various levels of sight impairment, but little action is actually taken. Whilst the release of trained models is promising, making these models available outside of the research community would be beneficial. Another direction the community could take towards using this research to aid those with forms of sight impairment would be to curate a dataset of images with questions posed by those we seek to help, i.e., people with sight impairment. This dataset could also include captions that focus on aspects of an image deemed to be important to those with sight impairment. The inclusion of these captions would yield models that generate captions that prioritise the information required by someone with sight impairment rather than trying to mirror the style of captions generated by sighted humans as is the case with models trained on existing image captioning datasets COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> or Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">The state of the art (SOTA) in vision-language tasks is currently dominated by large Transformer-based models developed by industrial labs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>. This makes comparing these models to those discussed in this paper difficult given the model size and compute power used for training. However, there are a few take home points.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">In the case of image captioning, the Transformer-based model <math id="S7.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{M}2" display="inline"><semantics id="S7.SS3.p3.1.m1.1a"><mrow id="S7.SS3.p3.1.m1.1.1" xref="S7.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.SS3.p3.1.m1.1.1.2" xref="S7.SS3.p3.1.m1.1.1.2.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S7.SS3.p3.1.m1.1.1.1" xref="S7.SS3.p3.1.m1.1.1.1.cmml">​</mo><mn id="S7.SS3.p3.1.m1.1.1.3" xref="S7.SS3.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.1.m1.1b"><apply id="S7.SS3.p3.1.m1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1"><times id="S7.SS3.p3.1.m1.1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1.1"></times><ci id="S7.SS3.p3.1.m1.1.1.2.cmml" xref="S7.SS3.p3.1.m1.1.1.2">ℳ</ci><cn type="integer" id="S7.SS3.p3.1.m1.1.1.3.cmml" xref="S7.SS3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.1.m1.1c">\mathcal{M}2</annotation></semantics></math> is outperformed by GNN-based architectures, namely Dual-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. This leads the authors to posit that there is a strong inductive bias in using imposed graph structures rather than allowing all relationships between detected objects to be processed using self-attention. The use of a global context graph (taking into account the whole dataset) alongside a local context graph (image level relationships) by Dual-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> is shown to work extremely well and this dual graph approach could be the seed for future works.</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p">It could be that given the scale of the models currently achieving SOTA that there are some emergent properties that develop in these models when they achieve such as scale. Future work should consider scaling graph-based architectures, such as those discussed in this survey, to the scale of the large models being produced by industry labs.</p>
</div>
<div id="S7.SS3.p5" class="ltx_para">
<p id="S7.SS3.p5.1" class="ltx_p">For FVQA and image retrieval, the graph-based approaches have stronger inductive biases for the reasoning stages of the tasks. Both tasks require the processing of graph data (in the case of a knowledge graph in FVQ or some graph representation of the search space in image retrieval). It is well documented that Transformers do not perform well on sparse graphs (such as knowledge graphs) or large graphs (such as those used in image retrieval).</p>
</div>
<div id="S7.SS3.p6" class="ltx_para">
<p id="S7.SS3.p6.1" class="ltx_p">The adoption of GNN-based image captioning techniques has proven promising. Given that this approach is relatively new, there is ample opportunity for further research to be carried out in this field. As shown in Section <a href="#S4" title="IV Image Captioning ‣ Graph Neural Networks in Vision-Language Image Understanding: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> the majority of image captioning techniques make use of either GCN or GGNN architectures. As GNNs develop and newer more expressive techniques are approached, the community should move to adopt these over traditional message passing style networks. Models such as GAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> may provide advantages over the techniques being used as they incorporate self-attention mechanisms into the architecture, a technique proven to yield impressive results given the popularity of the Transformer.</p>
</div>
<div id="S7.SS3.p7" class="ltx_para">
<p id="S7.SS3.p7.1" class="ltx_p">All the GNNs being used in the vision-language tasks discussed in the survey are built on the concept of homophily, i.e., similar nodes are connected by an edge. This is not always the case though given that a semantic graph connects dissimilar objects that are semantically related. Some of the graphs detailed are homophilic (e.g., image graph), but many others are not. This leads us to speculate that there are ample research opportunities for applying GNN architectures that respect the amount of homophily or heterophily of the graph being processed.</p>
</div>
<div id="S7.SS3.p8" class="ltx_para">
<p id="S7.SS3.p8.1" class="ltx_p">Another direction of research would be investigating combinations of different graph representations (both at the image level and dataset level) to identify combinations that work well together. Using different graph representations will allow for better utilisation of both local and global features.</p>
</div>
<div id="S7.SS3.p9" class="ltx_para">
<p id="S7.SS3.p9.1" class="ltx_p">The incorporation of outside knowledge into image captioning could provide an interesting research direction. It is often pointed out that image captioning is a useful accessibility technology for those with sight impairment. However, this assumes the user is an adult with a developed understanding of the world. Image captioning systems may struggle to be applied in a paediatric accessibility setting. Having the model explain the world in greater detail may be of use.</p>
</div>
<div id="S7.SS3.p10" class="ltx_para">
<p id="S7.SS3.p10.1" class="ltx_p">Another potential future research direction would be the unification of the three tasks discussed in this paper. Developing a single unified model that could perform competently in all three would hail an important breakthrough. In order to perform this, a model would have to have a common intermediary space for which it could map between the text and image spaces. We posit that this space would most likely be graph-based due to their expressive nature. However, a textual representation may also be performant as Gao <span id="S7.SS3.p10.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> showed reasoning in the text space improved performance over graph-based reasoning in VQA.</p>
</div>
<div id="S7.SS3.p11" class="ltx_para">
<p id="S7.SS3.p11.1" class="ltx_p">In summary, vision-language tasks such as those discussed in this paper are set to have a fruitful future, with many opportunities for various graph structures to be exploited.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank colleagues in DERI for their thoughtful and insightful feedback as we developed some of the ideas presented in this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. P. Chamberlain, S. Shirobokov, E. Rossi, F. Frasca, T. Markovich,
N. Hammerla, M. M. Bronstein, and M. Hansmire, “Graph neural networks for
link prediction with subgraph sketching,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F. Barbero, C. Bodnar, H. S. de Ocáriz Borde, M. Bronstein,
P. Veličković, and P. Liò, “Sheaf neural networks with
connection laplacians,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Topological, Algebraic and Geometric
Learning Workshops 2022</em>.   PMLR, 2022,
pp. 28–36.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Frasca, B. Bevilacqua, M. M. Bronstein, and H. Maron, “Understanding and
extending subgraph gnns by rethinking their symmetries,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Peng, R. Hu, F. Kong, J. Gan, Y. Mo, X. Shi, and X. Zhu, “Reverse graph
learning for graph neural network,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on neural networks and
learning systems</em>, pp. 1–12, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
X. Ai, C. Sun, Z. Zhang, and E. R. Hancock, “Two-level graph neural network,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on neural networks and learning systems</em>, pp. 1–14, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Yang, W. Dai, C. Li, J. Zou, and H. Xiong, “Ncgnn: Node-level capsule graph
neural network for semisupervised classification,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on
neural networks and learning systems</em>, pp. 1–15, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Wang, M. Liu, Y. Luo, Z. Xu, Y. Xie, L. Wang, L. Cai, Q. Qi, Z. Yuan,
T. Yang <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advanced graph and sequence neural networks for
molecular property prediction and drug discovery,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Bioinformatics</em>,
vol. 38, no. 9, pp. 2579–2586, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. J. Clipman, S. H. Mehta, S. Mohapatra, A. K. Srikrishnan, K. J. Zook,
P. Duggal, S. Saravanan, P. Nandagopal, M. S. Kumar, G. M. Lucas
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deep learning and social network analysis elucidate drivers
of hiv transmission in a high-incidence cohort of people who inject drugs,”
<em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Science Advances</em>, vol. 8, no. 42, p. eabf0158, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. Shi and R. Rajkumar, “Point-gnn: Graph neural network for 3d object
detection in a point cloud,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp. 1711–1719.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L. Cosmo, G. Minello, M. Bronstein, E. Rodolà, L. Rossi, and A. Torsello,
“3d shape analysis through a quantum lens: the average mixing kernel
signature,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, pp. 1–20, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun,
“Graph neural networks: A review of methods and applications,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">AI
Open</em>, vol. 1, pp. 57–81, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H.-C. Yi, Z.-H. You, D.-S. Huang, and C. K. Kwoh, “Graph representation
learning in bioinformatics: trends, methods and applications,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Briefings in Bioinformatics</em>, vol. 23, no. 1, p. bbab340, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. J. Thomas, T. H. N. Tran, G. P. Lechuga, and B. Belaton, “Convolutional
graph neural networks: a review and applications of graph autoencoder in
chemoinformatics,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Deep learning techniques and optimization
strategies in big data analytics</em>, pp. 107–123, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
W. Liu, Y. Zhang, J. Wang, Y. He, J. Caverlee, P. P. K. Chan, D. S. Yeung, and
P.-A. Heng, “Item relationship graph neural networks for e-commerce,”
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on neural networks and learning systems</em>, vol. 33, no. 9,
pp. 4785–4799, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
W. Lin and B. Li, “Status-aware signed heterogeneous network embedding with
graph neural networks,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on neural networks and learning
systems</em>, pp. 1–13, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Chen, Y. Wu, Q. Dai, H.-Y. Zhou, M. Xu, S. Yang, X. Han, and Y. Yu, “A
survey on graph neural networks and graph transformers in computer vision: A
task-oriented perspective,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier,
and D. Forsyth, “Every picture tells a story: Generating sentences from
images,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.   Springer,
2010, pp. 15–29.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
I. Laina, C. Rupprecht, and N. Navab, “Towards unsupervised image captioning
with shared multimodal embeddings,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 7414–7424.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. Li, L. Zhu, P. Liu, and Y. Yang, “Entangled transformer for image
captioning,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 8928–8937.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Shao, J. Han, D. Marnerides, and K. Debattista, “Region-object
relation-aware dense captioning via transformer,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on
neural networks and learning systems</em>, pp. 1–12, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Liu, X. Zhang, F. Huang, L. Cheng, and Z. Li, “Adversarial learning with
multi-modal attention for visual question answering,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on
neural networks and learning systems</em>, vol. 32, no. 9, pp. 3894–3908, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Gu, K. Vyas, M. Shen, J. Yang, and G.-Z. Yang, “Deep graph-based multimodal
feature embedding for endomicroscopy image retrieval,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on
neural networks and learning systems</em>, vol. 32, no. 2, pp. 481–492, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. Staszewski, M. Jaworski, J. Cao, and L. Rutkowski, “A new approach to
descriptors generation for image retrieval by analyzing activations of deep
neural network layers,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and
Learning Systems</em>, vol. 33, no. 12, pp. 7913–7920, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. Shamma, M. Bernstein, and
L. Fei-Fei, “Image retrieval using scene graphs,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015, pp.
3668–3678.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. Yao, Y. Pan, Y. Li, and T. Mei, “Exploring visual relationship for image
captioning,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2018, pp. 684–699.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi,
M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Relational inductive biases, deep learning, and graph
networks,” <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A comprehensive
survey of deep learning for image captioning,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys
(CsUR)</em>, vol. 51, no. 6, pp. 1–36, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y. Zou and Q. Xie, “A survey on vqa: Datasets and approaches,” in
<em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ITCA</em>.   IEEE, 2020, pp. 289–297.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. A. Yusuf, F. Chong, and M. Xianling, “An analysis of graph convolutional
networks and recent datasets for visual question answering,”
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Review</em>, pp. 1–24, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. Alexander and S. Gunasekaran, “A survey on image retrieval methods,”
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, 2014. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://web-archive.southampton.ac.uk/cogprints.org/9815/1/SurveyonImageRetrievalMethods.pdf</span>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Visual genome:
Connecting language and vision using crowdsourced dense image annotations,”
<em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 123, no. 1, pp. 32–73, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.   Springer,
2014, pp. 740–755.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to
visual denotations: New similarity metrics for semantic inference over event
descriptions,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Transactions of the ACL</em>, vol. 2, pp. 67–78, 2014.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “Vqa: Visual question answering,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015, pp.
2425–2433.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
P. Wang, Q. Wu, C. Shen, A. Dick, and A. Van Den Hengel, “Fvqa: Fact-based
visual question answering,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">PAMI</em>, vol. 40, no. 10, pp. 2413–2427,
2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, “Ok-vqa: A visual
question answering benchmark requiring external knowledge,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and
M. Rohrbach, “Towards vqa models that can read,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp.
8317–8326.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. K. Singh, A. Mishra, S. Shekhar, and A. Chakraborty, “From strings to
things: Knowledge-enabled vqa model that can read and reason,” in
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 4602–4612.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh, “Graph r-cnn for scene graph
generation,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, September 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
C. Zhang, W.-L. Chao, and D. Xuan, “An empirical study on leveraging scene
graphs for visual question answering,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">BMVC</em>, K. Sidorov and
Y. Hicks, Eds.   BMVA Press, September
2019, pp. 151.1–151.14. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://dx.doi.org/10.5244/C.33.151</span>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Wu, J. Wieland, O. Farivar, and J. Schiller, “Automatic alt-text:
Computer-generated image descriptions for blind users on a social network
service,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ACM Conference on Computer Supported Cooperative Work and
Social Computing</em>, 2017, pp. 1180–1192.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
image descriptions,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015, pp. 3128–3137.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
X. Yang, K. Tang, H. Zhang, and J. Cai, “Auto-encoding scene graphs for image
captioning,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 10 685–10 694.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshed-memory
transformer for image captioning,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp.
10 578–10 587.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. He, W. Liao, H. R. Tavakoli, M. Yang, B. Rosenhahn, and N. Pugeault, “Image
captioning through image transformer,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ACCV</em>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the V
in VQA matter: Elevating the role of image understanding in Visual
Question Answering,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives,
“Dbpedia: A nucleus for a web of open data,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">The semantic
web</em>.   Springer, 2007, pp. 722–735.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
H. Liu and P. Singh, “Conceptnet—a practical commonsense reasoning
tool-kit,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">BT technology journal</em>, vol. 22, no. 4, pp. 211–226, 2004.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
N. Tandon, G. Melo, and G. Weikum, “Acquiring comparative commonsense
knowledge from the web,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 28, no. 1, 2014.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
B. K. Iwana, S. T. R. Rizvi, S. Ahmed, A. Dengel, and S. Uchida, “Judging a
book by its cover,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2016.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
D. Vrandečić and M. Krötzsch, “Wikidata: a free collaborative
knowledgebase,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol. 57, no. 10, pp.
78–85, 2014.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
X. Han, Z. Wu, P. X. Huang, X. Zhang, M. Zhu, Y. Li, Y. Zhao, and L. S. Davis,
“Automatic spatially-aware fashion concept discovery,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">ICCV</em>,
2017, pp. 1463–1471.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Cui, Y. Hu, Y. Sun, J. Gao, and B. Yin, “Cross-modal alignment with graph
reasoning for image-text retrieval,” <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and
Applications</em>, pp. 1–18, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
S. Yoon, W. Y. Kang, S. Jeon, S. Lee, C. Han, J. Park, and E.-S. Kim,
“Image-to-image retrieval by learning similarity between scene graphs,” in
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 35, no. 12, 2021, pp. 10 718–10 726.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A. K. Misraa, A. Kale, P. Aggarwal, and A. Aminian, “Multi-modal retrieval
using graph neural networks,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Yu, Z. Zhu, Y. Wang, W. Zhang, Y. Hu, and J. Tan, “Cross-modal knowledge
reasoning for knowledge-based visual question answering,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Pattern
Recognition</em>, vol. 108, p. 107563, 2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 28,
2015.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Y. Zhong, L. Wang, J. Chen, D. Yu, and Y. Li, “Comprehensive image captioning
via scene graph decomposition,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ECCV</em>.   Springer, 2020, pp. 211–229.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
L. Guo, J. Liu, J. Tang, J. Li, W. Luo, and H. Lu, “Aligning linguistic words
and visual semantic units for image captioning,” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">ACM International
Conference on Multimedia</em>, 2019, pp. 765–773.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Zhou, J. Yang, C. Zhang, and Y. Tang, “Joint scence network and
attention-guided for image captioning,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">ICDM</em>.   IEEE, 2021, pp. 1535–1540.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning, “Generating
semantically precise scene graphs from textual descriptions for improved
image retrieval,” in <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the fourth workshop on vision and
language</em>, 2015, pp. 70–80.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
M.-C. De Marneffe and C. D. Manning, “The stanford typed dependencies
representation,” in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Coling 2008: proceedings of the workshop on
cross-framework and cross-domain parser evaluation</em>, 2008, pp. 1–8.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
D. Teney, L. Liu, and A. van Den Hengel, “Graph-structured representations for
visual question answering,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 1–9.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
H. Pan and J. Huang, “Multimodal high-order relational network for
vision-and-language tasks,” <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 492, pp. 62–75,
2022.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
P. Anderson, B. Fernando, M. Johnson, and S. Gould, “Spice: Semantic
propositional image caption evaluation,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2016, pp. 382–398.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
T. Yao, Y. Pan, Y. Li, and T. Mei, “Hierarchy parsing for image captioning,”
in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 2621–2629.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
J. Kan, K. Hu, Z. Wang, Q. Wu, M. Hagenbuchner, and A. C. Tsoi, “Topic-guided
local-global graph neural network for image captioning,” in
<em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">ICME</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
X. Dong, C. Long, W. Xu, and C. Xiao, “Dual graph convolutional networks with
transformer and curriculum learning for image captioning,” in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">ICME</em>,
2021, pp. 2615–2624.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
C. Liu, G. Yu, M. Volkovs, C. Chang, H. Rai, J. Ma, and S. K. Gorti, “Guided
similarity separation for image retrieval,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
C. Li, H. Wang, Z. Zhang, A. Sun, and Z. Ma, “Topic modeling for short texts
with auxiliary word embeddings,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">ACM SIGIR conference on Research
and Development in Information Retrieval</em>, 2016, pp. 165–174.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
U. Chaudhuri, B. Banerjee, and A. Bhattacharya, “Siamese graph convolutional
network for content based remote sensing image retrieval,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Computer
vision and image understanding</em>, vol. 184, pp. 22–30, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
A. Hogan, E. Blomqvist, M. Cochez, C. d’Amato, G. d. Melo, C. Gutierrez,
S. Kirrane, J. E. L. Gayo, R. Navigli, S. Neumaier <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Knowledge
graphs,” <em id="bib.bib74.2.2" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, vol. 54, no. 4, pp. 1–37,
2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive
survey on graph neural networks,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on neural networks and
learning systems</em>, vol. 32, no. 1, pp. 4–24, 2020.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and
M. Grohe, “Weisfeiler and leman go neural: Higher-order graph neural
networks,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 33, no. 01, 2019, pp. 4602–4609.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
W. L. Hamilton, R. Ying, and J. Leskovec, “Inductive representation learning
on large graphs,” in <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2017, pp. 1025–1035.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Y. Li, D. Tarlow, M. Brockschmidt, and R. S. Zemel, “Gated graph sequence
neural networks,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1511.05493, 2015.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
K. Cho, B. van Merrienboer, Çaglar Gülçehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio, “Learning phrase representations using rnn
encoder–decoder for statistical machine translation,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Conference
on Empirical Methods in Natural Language Processing</em>, 2014.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò,
and Y. Bengio, “Graph Attention Networks,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2018, accepted as
poster. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=rJXMpikCZ</span>

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
M. Khademi, “Multimodal neural graph memory networks for visual question
answering,” in <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the ACL</em>,
2020, pp. 7177–7188.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, “Measuring and relieving
the over-smoothing problem for graph neural networks from the topological
view,” in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 34, no. 04, 2020, pp. 3438–3445.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
C. Bodnar, F. D. Giovanni, B. P. Chamberlain, P. Liò, and M. M. Bronstein,
“Neural sheaf diffusion: A topological perspective on heterophily and
oversmoothing in GNNs,” in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, A. H. Oh, A. Agarwal,
D. Belgrave, and K. Cho, Eds., 2022. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=vbPsD-BhOZ</span>

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
S. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Lerman,
H. Harutyunyan, G. Ver Steeg, and A. Galstyan, “Mixhop: Higher-order graph
convolutional architectures via sparsified neighborhood mixing,” in
<em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">ICML</em>.   PMLR, 2019, pp. 21–29.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
C. Bodnar, F. Frasca, N. Otter, Y. Wang, P. Lio, G. F. Montufar, and
M. Bronstein, “Weisfeiler and lehman go cellular: Cw networks,”
<em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 34, pp. 2625–2640, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
B. Chamberlain, J. Rowbottom, M. I. Gorinova, M. Bronstein, S. Webb, and
E. Rossi, “Grand: Graph neural diffusion,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">ICML</em>.   PMLR, 2021, pp. 1407–1418.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
D. S. Lakshminarasimhan Srinivasan and A. Amutha, “Image captioning-a deep
learning approach,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">International Journal of Applied Engineering
Research</em>, vol. 13, no. 9, pp. 7239–7242, 2018.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
evaluation of machine translation,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual
meeting of the ACL</em>, 2002, pp. 311–318.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in
<em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Text summarization branches out</em>, 2004, pp. 74–81.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation with
improved correlation with human judgments,” in <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">ACL workshop on
intrinsic and extrinsic evaluation measures for machine translation and/or
summarization</em>, 2005, pp. 65–72.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-based image
description evaluation,” in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2015, pp. 4566–4575.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
F. Monti, K. Otness, and M. M. Bronstein, “Motifnet: a motif-based graph
convolutional network for directed graphs,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Data Science
Workshop (DSW)</em>.   IEEE, 2018, pp.
225–228.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Z. Song and X. Zhou, “Exploring explicit and implicit visual relationships for
image captioning,” in <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">ICME</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Wang, W. Wang, L. Wang, Z. Wang, D. D. Feng, and T. Tan, “Learning visual
relationship and context-aware attention for image captioning,”
<em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 98, p. 107075, 2020.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations
from tree-structured long short-term memory networks,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint</em>, 2015.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
H. Sharma and A. S. Jalal, “Visual question answering model based on graph
neural network and contextual attention,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>,
vol. 110, p. 104165, 2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word
representation,” in <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
L. Li, Z. Gan, Y. Cheng, and J. Liu, “Relation-aware graph attention network
for visual question answering,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 10 313–10 322.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
S. V. Nuthalapati, R. Chandradevan, E. Giunchiglia, B. Li, M. Kayser,
T. Lukasiewicz, and C. Yang, “Lightweight visual question answering using
scene graphs,” in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">ACM International Conference on Information &amp;
Knowledge Management</em>, 2021, pp. 3353–3357.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Z. Zhu, J. Yu, Y. Sun, Y. Hu, Y. Wang, and Q. Wu, “Mucko: Multi-layer
cross-modal knowledge reasoning for fact-based visual question answering,”
in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, 2020.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
M. Narasimhan, S. Lazebnik, and A. Schwing, “Out of the box: Reasoning with
graph convolution nets for factual visual question answering,”
<em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 31, 2018.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
L. Liu, M. Wang, X. He, L. Qing, and H. Chen, “Fact-based visual question
answering via dual-process system,” <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, vol.
237, p. 107650, 2022.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
K. E. Stanovich and R. F. West, “24. individual differences in reasoning:
Implications for the rationality debate?” <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Behavioural and Brain
Science</em>, vol. 23, no. 5, pp. 665–726, 2000.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Z. Wang, H. You, L. H. Li, A. Zareian, S. Park, Y. Liang, K.-W. Chang, and
S.-F. Chang, “Sgeitl: Scene graph enhanced image-text learning for visual
commonsense reasoning,” in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 36, no. 5, 2022, pp.
5914–5922.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
F. Gao, Q. Ping, G. Thattai, A. Reganti, Y. N. Wu, and P. Natarajan, “A
thousand words are worth more than a picture: Natural language-centric
outside-knowledge visual question answering,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, P. J. Liu <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Exploring the limits of transfer learning
with a unified text-to-text transformer.” <em id="bib.bib106.2.2" class="ltx_emph ltx_font_italic">JMLR</em>, vol. 21, no. 140, pp.
1–67, 2020.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
D. Gao, K. Li, R. Wang, S. Shan, and X. Chen, “Multi-modal graph neural
network for joint reasoning on vision and scene text,” in <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020,
pp. 12 746–12 756.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Y. Liang, X. Wang, X. Duan, and W. Zhu, “Multi-modal contextual graph neural
network for text visual question answering,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">ICPR</em>, 2021, pp.
3491–3498.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2017. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=SJU4ayYgl</span>

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
networks?” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2018.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
X. Zhang, M. Jiang, Z. Zheng, X. Tan, E. Ding, and Y. Yang, “Understanding
image retrieval re-ranking: a graph neural network perspective,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint</em>, 2020.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
M. Wang, W. Zhou, Q. Tian, and H. Li, “Deep graph convolutional quantization
networks for image retrieval,” <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">IEEE trans. on Multimedia</em>, 2022.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
F. Zhang, M. Xu, Q. Mao, and C. Xu, “Joint attribute manipulation and modality
alignment learning for composing text and image to image retrieval,” in
<em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">ACM International Conference on Multimedia</em>, 2020, pp. 3367–3376.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
U. Chaudhuri, B. Banerjee, A. Bhattacharya, and M. Datcu, “Attention-driven
graph convolution network for remote sensing image retrieval,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">IEEE
Geoscience and Remote Sensing Letters</em>, vol. 19, pp. 1–5, 2021.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Z. Zhang, Y. Zhang, R. Feng, T. Zhang, and W. Fan, “Zero-shot sketch-based
image retrieval via graph convolution network,” in <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, vol. 34,
no. 07, 2020, pp. 12 943–12 950.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang, “Variational neural machine
translation,” in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</em>.   Austin, Texas: ACL, Nov.
2016, pp. 521–530. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://aclanthology.org/D16-1050</span>

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <em id="bib.bib117.2.2" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2020.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
<em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical
text-conditional image generation with clip latents,” <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>,
2022.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
S. Xu, “Clip-diffusion-lm: Apply diffusion model on image captioning,”
<em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
H. Li, J. Gu, R. Koner, S. Sharifzadeh, and V. Tresp, “Do dall-e and flamingo
understand each other?” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
C. Conwell and T. Ullman, “Testing relational understanding in text-guided
image generation,” <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz,
S. Goodman, A. Grycner, B. Mustafa, L. Beyer <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Pali: A
jointly-scaled multilingual language-image model,” <em id="bib.bib123.2.2" class="ltx_emph ltx_font_italic">arXiv preprint</em>,
2022.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Y. Zeng, X. Zhang, H. Li, J. Wang, J. Zhang, and W. Zhou, “X<sup id="bib.bib124.2.1" class="ltx_sup">2</sup>-vlm:
All-in-one pre-trained model for vision-language tasks,” <em id="bib.bib124.3.2" class="ltx_emph ltx_font_italic">arXiv
preprint</em>, 2022.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao
<em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “mplug: Effective and efficient vision-language learning by
cross-modal skip-connections,” <em id="bib.bib125.2.2" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2022.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Henry Senior</span>  is a PhD student at Queen Mary, University of London in the Digital Environment Research Institute (DERI). His research focuses on the development and application of Graph Neural Networks to Vision-Language problems such as image captioning. Before joining DERI, Henry completed a BSc (Hons) in Computer Science with Professional Experience at the University of Salford and an MSc in Advanced Computer Science at the University of Liverpool.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Gregory Slabaugh</span>  is Professor of Computer Vision and AI and Director of the Digital Environment Research Institute (DERI) at Queen Mary University of London. Previously, he was Chief Scientist in Computer Vision (EU) for Huawei Technologies R&amp;D. His research interests include computational photography, medical image computing, and applications of deep learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Shanxin Yuan</span>  is a Lecturer in the Digital Environment Research Institute (DERI) at Queen Mary University of London. He received his PhD degree from Imperial College London, where he researched on 3D hand pose estimation. His research interests are machine learning and computer vision, particularly 3D digital humans and computational photography. He regularly reviews for major computer vision conferences (CVPR, ICCV, ECCV, and NeurIPS) and related journals (TPAMI, IJCV and TIP).</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td">
<span id="tab4.1.1.1.1" class="ltx_inline-block">
<span id="tab4.1.1.1.1.1" class="ltx_p"><span id="tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Luca Rossi</span>  received his PhD degree in computer science from Ca’ Foscari University of Venice, Italy, in 2013. He is currently an assistant professor with the Hong Kong Polytechnic University, having held various positions with Queen Mary University of London, U.K., the University of Birmingham, U.K., Aston University, U.K., and the Southern University of Science and Technology, China. He has published more than 50 papers in international journals and conferences. His research interests include the areas of pattern recognition, data mining, and network science. He is currently a member of the editorial board of the journal Pattern Recognition.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.03760" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.03761" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.03761">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.03761" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.03762" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 21:30:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
