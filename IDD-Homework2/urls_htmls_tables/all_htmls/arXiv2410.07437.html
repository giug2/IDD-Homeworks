<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Robust infrared small target detection using self-supervised and a contrario paradigms</title>
<!--Generated on Wed Oct  9 21:08:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Infrared small target detection,  " lang="en" name="keywords"/>
<base href="/html/2410.07437v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S1" title="In Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S2" title="In Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S2.SS1" title="In II Related works ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">A contrario paradigm</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S2.SS2" title="In II Related works ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Self-supervised learning and object detection</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S3" title="In Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S3.SS1" title="In III Proposed method ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Integrating an a contrario criterion into YOLO</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S3.SS2" title="In III Proposed method ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Choosing appropriate SSL initialisation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4" title="In Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS1" title="In IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Experimental set-up</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS1.SSS1" title="In IV-A Experimental set-up ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Datasets and evaluation metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS1.SSS2" title="In IV-A Experimental set-up ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS2" title="In IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Mixing both SSL and a contrario paradigms improve<span class="ltx_text" style="color:#000000;">s</span> the baselines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS3" title="In IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">What about frugal setting?</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.SS4" title="In IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Comparison with SOTA segmentation networks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S5" title="In Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Robust infrared small target detection using self-supervised and <span class="ltx_text ltx_font_italic" id="id1.id1">a contrario</span> paradigms</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alina Ciocarlan,
Sylvie Le Hégarat-Mascle,
Sidonie Lefebvre
and Arnaud Woiselle
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Detecting small targets in infrared images poses significant challenges in defense applications due to the presence of complex backgrounds and the small size of the targets. Traditional object detection methods often struggle to balance high detection rates with low false alarm rates, especially when dealing with small objects. In this paper, we introduce a novel approach that combines <span class="ltx_text ltx_font_italic" id="id2.id1.1">a contrario</span> paradigm with Self-Supervised Learning (SSL) to improve Infrared Small Target Detection (IRSTD). On the one hand, the integration of an <span class="ltx_text ltx_font_italic" id="id2.id1.2">a contrario</span> criterion into a YOLO detection head enhances feature map responses for small and <span class="ltx_text ltx_font_italic" id="id2.id1.3">unexpected</span> objects while effectively controlling false alarms. On the other hand, we explore SSL techniques to overcome the challenges of limited annotated data, common in IRSTD tasks. Specifically, we benchmark several representative SSL strategies for their effectiveness in improving small object detection performance. Our findings show that instance discrimination methods outperform masked image modeling strategies when applied to YOLO-based small object detection. Moreover, the combination of the <span class="ltx_text ltx_font_italic" id="id2.id1.4">a contrario</span> and SSL paradigms leads to significant performance improvements, narrowing the gap with state-of-the-art segmentation methods and even outperforming them in frugal settings. This <span class="ltx_text" id="id2.id1.5" style="color:#000000;">two</span>-pronged approach offers a robust solution for improving IRSTD performance, particularly under challenging conditions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Infrared small target detection, <span class="ltx_text ltx_font_italic" id="id3.id1">a contrario</span> paradigm, self-supervised learning, YOLO

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Infrared Small Target Detection (IRSTD) is a highly challenging but critical task in defense. The main difficulties arise from (i) the extremely small size of the targets, often occupying less than 20 pixels, (ii) the presence of complex and highly textured backgrounds <span class="ltx_text" id="S1.p1.1.1" style="color:#000000;">that</span> can result in numerous false alarms, and (iii) the challenging learning conditions. These conditions include training on small and heavily class-imbalanced datasets.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the literature for IRSTD, several methods have been proposed to address some of the above issues. The increasing availability of infrared small target detection datasets has <span class="ltx_text" id="S1.p2.1.1" style="color:#000000;">spurred</span> the development of deep learning-based methods <span class="ltx_text" id="S1.p2.1.2" style="color:#000000;">that</span> have demonstrated their effectiveness in extracting non-linear features from large annotated datasets. For example, dense nested U-shaped architectures (e.g., DNANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib1" title="">1</a>]</cite>) and specific multi-scale fusion modules (e.g., LSPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib2" title="">2</a>]</cite>) have been introduced, which effectively limit the information loss <span class="ltx_text" id="S1.p2.1.3" style="color:#000000;">on</span> small targets and lead to good performance on several IRSTD benchmarks. Some methods also include local and large-scale attention mechanisms in order to limit the confusion between targets and background elements (e.g., AGPCNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib3" title="">3</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Note however that State-Of-The-Art (SOTA) IRSTD methods all rely on segmentation networks, and a major issue <span class="ltx_text" id="S1.p3.1.1" style="color:#000000;">with</span> relying on such neural networks for object detection is that object fragmentation can occur when the segmentation map <span class="ltx_text" id="S1.p3.1.2" style="color:#000000;">is binarized</span>. This issue often leads to numerous undesired false alarms. Object detection algorithms <span class="ltx_text" id="S1.p3.1.3" style="color:#000000;">such as</span> Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib4" title="">4</a>]</cite> or YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib5" title="">5</a>]</cite> mitigate this risk by explicitly localizing objects through bounding box regression.
However, traditional object detection methods struggle to balance a high detection rate with a low false alarm rate. While some existing approaches have improved feature map responses for small targets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib8" title="">8</a>]</cite>, they <span class="ltx_text" id="S1.p3.1.4" style="color:#000000;">often</span> fail to manage false alarms caused by background elements, and no rigorous comparison with SOTA IRSTD methods <span class="ltx_text" id="S1.p3.1.5" style="color:#000000;">has been made</span>. Moreover, these methods typically do not leverage the <span class="ltx_text ltx_font_italic" id="S1.p3.1.6">unexpected</span> nature of small objects relative to the background, which could be addressed using an anomaly detection approach. Such a criterion helps in distinguishing small objects as unexpected patterns against the background, effectively reducing the Number of False Alarms (NFA) induced by background noise. As demonstrated in recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite>, this approach can achieve a better balance between precision and detection rate. Lastly, traditional object detectors face difficulties when dealing with class-imbalanced or limited datasets, which is a common and challenging condition in IRSTD.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the following, we propose to combine both <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">a contrario</span> and self-supervised learning paradigms to improve IRSTD. For this purpose, we propose a new YOLO detection head that integrates an <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">a contrario</span> criterion to perform the detection of small targets. <span class="ltx_text" id="S1.p4.1.3" style="color:#000000;">Inspired by</span> perception theory and anomaly detection methods, this criterion allows us to introduce an <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">a priori</span> on small targets – specifically, that they are <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">unexpected</span> – and to control the number of false alarms. Then, we explore the benefits of using self-supervised learning (SSL) to initialize the backbone of a modified version of YOLO. The aim of this unsupervised pre-training paradigm is to help the network learning features or invariances that are relevant <span class="ltx_text" id="S1.p4.1.6" style="color:#000000;">to</span> the downstream task.
One of the motivations <span class="ltx_text" id="S1.p4.1.7" style="color:#000000;">stems</span> from the fact that SSL methods have been shown in the literature to improve SOTA performance for many use cases. More specifically, SSL allows the network to learn general features from large unlabeled datasets which, when transferred to a final task, improve<span class="ltx_text" id="S1.p4.1.8" style="color:#000000;">s</span> performance despite difficult fine-tuning conditions (e.g., little annotated data or few computational resources). However, the use cases considered in the literature mainly concern classification or the detection of objects of medium to large size. This raises the following question: do these conclusions transfer well to the detection of small targets?</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this study, we <span class="ltx_text" id="S1.p5.1.1" style="color:#000000;">show</span> that SSL is indeed beneficial for IRSTD, particularly when combined with the <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">a contrario</span> detection head, leading to more robust results in challenging conditions (e.g., frugal setting). Our contributions are three-fold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">YOLO + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.1.1.m1.1"><semantics id="S1.I1.i1.p1.1.1.m1.1a"><msub id="S1.I1.i1.p1.1.1.m1.1.1" xref="S1.I1.i1.p1.1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S1.I1.i1.p1.1.1.m1.1.1.2" xref="S1.I1.i1.p1.1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S1.I1.i1.p1.1.1.m1.1.1.3" xref="S1.I1.i1.p1.1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.1.m1.1b"><apply id="S1.I1.i1.p1.1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.I1.i1.p1.1.1.m1.1.1.1.cmml" xref="S1.I1.i1.p1.1.1.m1.1.1">subscript</csymbol><ci id="S1.I1.i1.p1.1.1.m1.1.1.2a.cmml" xref="S1.I1.i1.p1.1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S1.I1.i1.p1.1.1.m1.1.1.2.cmml" xref="S1.I1.i1.p1.1.1.m1.1.1.2">NFA</mtext></ci><ci id="S1.I1.i1.p1.1.1.m1.1.1.3.cmml" xref="S1.I1.i1.p1.1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.p1.1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detection head</span>: We introduce a new YOLO detection head that integrates a pixel-level <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.2.2">a contrario</span> criterion, creating a network we call YOLO + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.2.m1.1"><semantics id="S1.I1.i1.p1.2.m1.1a"><msub id="S1.I1.i1.p1.2.m1.1.1" xref="S1.I1.i1.p1.2.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S1.I1.i1.p1.2.m1.1.1.2" xref="S1.I1.i1.p1.2.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S1.I1.i1.p1.2.m1.1.1.3" xref="S1.I1.i1.p1.2.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.2.m1.1b"><apply id="S1.I1.i1.p1.2.m1.1.1.cmml" xref="S1.I1.i1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S1.I1.i1.p1.2.m1.1.1.1.cmml" xref="S1.I1.i1.p1.2.m1.1.1">subscript</csymbol><ci id="S1.I1.i1.p1.2.m1.1.1.2a.cmml" xref="S1.I1.i1.p1.2.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S1.I1.i1.p1.2.m1.1.1.2.cmml" xref="S1.I1.i1.p1.2.m1.1.1.2">NFA</mtext></ci><ci id="S1.I1.i1.p1.2.m1.1.1.3.cmml" xref="S1.I1.i1.p1.2.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.2.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.p1.2.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>. This detection head can be integrated into any YOLO detector and is more robust to challenging conditions, such as frugal settings or complex backgrounds.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">SSL pre-training strategies and IRSTD</span>: We analyze the impact of different SSL pre-training strategies for IRSTD and show that instance discrimination methods are more effective than Masked Image Modeling (MIM) methods when used “as is” <span class="ltx_text" id="S1.I1.i2.p1.1.2" style="color:#000000;">to initialize</span> a convolution-based backbone.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">New robust and SOTA results for IRSTD</span>: Last but not least, we show that <span class="ltx_text" id="S1.I1.i3.p1.1.2" style="color:#000000;">the combination of</span> both SSL and <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.3">a contrario</span> paradigms not only narrows the performance gap with SOTA segmentation methods on very challenging IRSTD datasets, but can also outperform them <span class="ltx_text" id="S1.I1.i3.p1.1.4" style="color:#000000;">by</span> a large margin, especially in a frugal setting.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related works</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">A contrario paradigm</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In order to explicitly take advantage of the amount of information available on the background to discriminate small targets, it is interesting to consider reasoning used in anomaly detection, such as <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">a contrario</span> reasoning. <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">A contrario</span> paradigm, introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib10" title="">10</a>]</cite>, consists in the rejection of a naive model that describes a destruct<span class="ltx_text" id="S2.SS1.p1.1.3" style="color:#000000;">u</span>red background. Such reasoning originates from theories of perception, in particular <span class="ltx_text" id="S2.SS1.p1.1.4" style="color:#000000;">from</span> Gestalt theory. The threshold used to reject the background hypothesis <span class="ltx_text" id="S2.SS1.p1.1.5" style="color:#000000;">allows us</span> to control the Number of False Alarms (NFA). The latter can be defined as the product between the total number of tested <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.6">objects</span> and the tail distribution of the law followed by the chosen naive model. Depending on the type of object we consider, several <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.7">a contrario</span> formulation<span class="ltx_text" id="S2.SS1.p1.1.8" style="color:#000000;">s</span> can be <span class="ltx_text" id="S2.SS1.p1.1.9" style="color:#000000;">considered</span>.
For example, the most commonly used and straightforward naive model for gray-level feature maps is the Gaussian distribution of the pixel gray-level values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib12" title="">12</a>]</cite>. In this case, high intensity pixels (compared to the global statistics of the image) are more likely to belong to the target class. Such a criterion has been integrated into segmentation neural networks in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite> and ha<span class="ltx_text" id="S2.SS1.p1.1.10" style="color:#000000;">s</span> led to great performance <span class="ltx_text" id="S2.SS1.p1.1.11" style="color:#000000;">in</span> tiny object detection. When dealing with binary images, it is common to use the uniform spatial distribution of “true” pixels in the image grid as a naive model. <span class="ltx_text" id="S2.SS1.p1.1.12" style="color:#000000;">Then, it boils down to assuming a binomial distribution for</span> the number of pixels falling into a parametric shape describing our object of interest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib14" title="">14</a>]</cite>. The advantage of<span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.13"> a contrario</span> methods over other statistical tests (e.g., the family-wise error rate control) is that, in contrast to the latter, there is no control of the probability, but rather of the number of false alarms. Consequently, an increase in the size of the image has no effect on the total number of false alarms in the image. In addition, the explicit use of background information to discriminate targets alleviates the constraint of requiring a large number of target samples, and therefore a large number of data, in order to achieve high performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Self-supervised learning and object detection</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">SSL is a SOTA approach for performing unsupervised pre-training on large unlabeled datasets, and is a particularly active area of research. It relies on a pretext training task <span class="ltx_text" id="S2.SS2.p1.1.1" style="color:#000000;">able to generate its own ground truth (e.g., pseudo-labels)</span>, and such <span class="ltx_text" id="S2.SS2.p1.1.2" style="color:#000000;">a</span> strategy helps the network to learn invariances and latent patterns in the data. Several pretext tasks have been proposed in the literature, which we can divide into two main categories: instance discrimination methods, and Masked Image Modeling (MIM) methods.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Instance discrimination methods aim at modeling the decision <span class="ltx_text" id="S2.SS2.p2.1.1" style="color:#000000;">boundaries</span> between sub-sets of data represented in the latent space. Fundamental methods such as MoCov2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib15" title="">15</a>]</cite> or DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib16" title="">16</a>]</cite> consider images as instances, and perform inter-image discrimination. Concretely, given two transformed images, if these images are augmented views (called positive samples or pairs) of the same anchor image, <span class="ltx_text" id="S2.SS2.p2.1.2" style="color:#000000;">then</span> their features are forced to be represented similarly in the latent space. Teaching a network to identify whether two images come from the same anchor image forces the encoder to learn general and representative features of a given image while being invariant to the augmentations used to create <span class="ltx_text" id="S2.SS2.p2.1.3" style="color:#000000;">the</span> augmented views. Common data augmentations include random rotation, color jitter or <span class="ltx_text" id="S2.SS2.p2.1.4" style="color:#000000;">G</span>aussian noise and <span class="ltx_text" id="S2.SS2.p2.1.5" style="color:#000000;">G</span>aussian blur. Data augmentations should be <span class="ltx_text" id="S2.SS2.p2.1.6" style="color:#000000;">carefully</span> chosen according to the desired invariances (e.g., invariance to color, illumination, or occlusion) for the downstream task. Note however that most instance discrimination methods assume that the images are semantically consistent, which may impair the performance for dense prediction tasks.
For this purpose, some local instance discrimination methods have been proposed in the literature by, for example, designing <span class="ltx_text" id="S2.SS2.p2.1.7" style="color:#000000;">data-augmentations at the</span> object or region-level <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib17" title="">17</a>]</cite>, or applying instance discrimination loss at a local level (e.g., per pixel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib18" title="">18</a>]</cite>). In contrast to global instance discrimination techniques, which treat an image as a single instance, local instance discrimination methods appear more suitable for object detection, as they allow for the extraction of more local features.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text" id="S2.SS2.p3.1.1" style="color:#000000;">Unlike</span> instance discrimination methods, the objective of MIM methods is to recover corrupted images. The underlying hypothesis is that, if a network is able to reconstruct severely masked information, then it “understands” the semantics in the image. One strong invariance learned by such methods is occlusion invariance. Moreover, by reconstructing the corrupted data, image modeling methods better exploit local features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib19" title="">19</a>]</cite>. Famous MIM methods include Masked AutoEncoders (MAE,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib20" title="">20</a>]</cite>) and SimMIM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib21" title="">21</a>]</cite>, which have shown impressive performance on several object detection benchmarks, especially when combined with Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib22" title="">22</a>]</cite> encoders.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Integrating an a contrario criterion into YOLO</span>
</h3>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S3.F1.g1" src="x1.png" width="871"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.4.2.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.2.1" style="font-size:90%;"> Integration of our pixel-level criterion into a YOLO framework, through the <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S3.F1.2.1.m1.1"><semantics id="S3.F1.2.1.m1.1b"><msub id="S3.F1.2.1.m1.1.1" xref="S3.F1.2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.F1.2.1.m1.1.1.2" xref="S3.F1.2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.F1.2.1.m1.1.1.3" xref="S3.F1.2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F1.2.1.m1.1c"><apply id="S3.F1.2.1.m1.1.1.cmml" xref="S3.F1.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F1.2.1.m1.1.1.1.cmml" xref="S3.F1.2.1.m1.1.1">subscript</csymbol><ci id="S3.F1.2.1.m1.1.1.2a.cmml" xref="S3.F1.2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.F1.2.1.m1.1.1.2.cmml" xref="S3.F1.2.1.m1.1.1.2">NFA</mtext></ci><ci id="S3.F1.2.1.m1.1.1.3.cmml" xref="S3.F1.2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.2.1.m1.1d">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.F1.2.1.m1.1e">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detection head. This module can be added on top of any YOLO. </span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this section, we describe our method for integrating an <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">a contrario</span> criterion into a YOLO detection head. Specifically, the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">a contrario</span> test is <span class="ltx_text" id="S3.SS1.p1.1.3" style="color:#000000;">used</span> to re-estimate the objectness scores predicted by the YOLO detection head for each bounding box.
Traditionally,
<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">a contrario</span> criteria have been used to detect <span class="ltx_text" id="S3.SS1.p1.1.5" style="color:#000000;">objects based on a specific feature such as alignment</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib10" title="">10</a>]</cite>, <span class="ltx_text" id="S3.SS1.p1.1.6" style="color:#000000;">that can be easily measured, i.e., by</span> counting the number of points within the parametric shape <span class="ltx_text" id="S3.SS1.p1.1.7" style="color:#000000;">(strip for alignment) that characterizes the object’s geometry</span>.
Similarly, one might consider counting the points within the bounding box predicted by YOLO.
However, incorporating such a counting operation into the training loop of a neural network in a differentiable manner is challenging. This process typically requires thresholding to discretize the feature map, which breaks the continuity of the forward pass and can lead to an unstable training process. To address this issue, we propose to rely on a simpler pixel-level <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.8">a contrario</span> formulation. Instead of counting points within the bounding box, we estimate the objectness score for each bounding box using <span class="ltx_text" id="S3.SS1.p1.1.9" style="color:#000000;">only</span> its center, thereby ignoring its spatial extent and density. While this may seem limiting, we argue that this approach is actually efficient, particularly in the context of small object detection where the spatial extent of the bounding box is very small. Indeed, we will show that this strategy works <span class="ltx_text" id="S3.SS1.p1.1.10" style="color:#000000;">very</span> well in practice.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Concretely, <span class="ltx_text" id="S3.SS1.p2.3.1" style="color:#000000;">our</span> naive assumption <math alttext="H_{0}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">H</mi><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝐻</ci><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">H_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" id="S3.SS1.p2.3.2" style="color:#000000;">is</span> that the background noise <span class="ltx_text" id="S3.SS1.p2.3.3" style="color:#000000;">follows a Gaussian distribution</span> and <span class="ltx_text" id="S3.SS1.p2.3.4" style="color:#000000;">we</span> rely on the multi-channel NFA criterion introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite> for segmentation networks, which is defined for a centered input <math alttext="X_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">X</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑋</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">X_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_K</annotation></semantics></math> channels as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textsc{NFA}_{\mathcal{N}}(x_{i})=\frac{\eta_{test}}{\Gamma(K/2)}\Gamma(\frac{%
K}{2},\frac{1}{2}||\Sigma^{-1/2}x_{i}||^{2}_{2})," class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.3.cmml"><mtext class="ltx_font_smallcaps" id="S3.E1.m1.3.3.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.3.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.3.3.cmml">𝒩</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">η</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">t</mi><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">t</mi></mrow></msub><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" mathvariant="normal" xref="S3.E1.m1.1.1.1.3.cmml">Γ</mi><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">K</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">/</mo><mn id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml">⁢</mo><mi id="S3.E1.m1.3.3.1.1.2.3" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.2.3.cmml">Γ</mi><mo id="S3.E1.m1.3.3.1.1.2.2a" xref="S3.E1.m1.3.3.1.1.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1" xref="S3.E1.m1.3.3.1.1.2.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.2.cmml">(</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">K</mi><mn id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">2</mn></mfrac><mo id="S3.E1.m1.3.3.1.1.2.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.2.cmml">,</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.cmml"><mfrac id="S3.E1.m1.3.3.1.1.2.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3.cmml"><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3.2.cmml">1</mn><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3.3.cmml">2</mn></mfrac><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.2.cmml">⁢</mo><msubsup id="S3.E1.m1.3.3.1.1.2.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.2" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.2.cmml">Σ</mi><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3a" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.cmml">−</mo><mrow id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.cmml"><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.2.cmml">1</mn><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.1.cmml">/</mo><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.3.cmml">2</mn></mrow></mrow></msup><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow><mo id="S3.E1.m1.3.3.1.1.2.1.1.4" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.3.2a.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2"><mtext class="ltx_font_smallcaps" id="S3.E1.m1.3.3.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2">NFA</mtext></ci><ci id="S3.E1.m1.3.3.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3.3">𝒩</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><times id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"></times><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1"></divide><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝜂</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">𝑡</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">Γ</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><divide id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></divide><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝐾</ci><cn id="S3.E1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.3">2</cn></apply></apply></apply><ci id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3">Γ</ci><interval closure="open" id="S3.E1.m1.3.3.1.1.2.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2"></divide><ci id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2">𝐾</ci><cn id="S3.E1.m1.2.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.3">2</cn></apply><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3"><divide id="S3.E1.m1.3.3.1.1.2.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3"></divide><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.3.2.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3.2">1</cn><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.3.3">2</cn></apply><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.2">Σ</ci><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3"></minus><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2"><divide id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.1"></divide><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.2.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.2">1</cn><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.2.3.2.3">2</cn></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.1.3">2</cn></apply><cn id="S3.E1.m1.3.3.1.1.2.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.1.1.1.1.3">2</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\textsc{NFA}_{\mathcal{N}}(x_{i})=\frac{\eta_{test}}{\Gamma(K/2)}\Gamma(\frac{%
K}{2},\frac{1}{2}||\Sigma^{-1/2}x_{i}||^{2}_{2}),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_η start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT end_ARG start_ARG roman_Γ ( italic_K / 2 ) end_ARG roman_Γ ( divide start_ARG italic_K end_ARG start_ARG 2 end_ARG , divide start_ARG 1 end_ARG start_ARG 2 end_ARG | | roman_Σ start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.8">where <math alttext="\Gamma(.)" class="ltx_math_unparsed" display="inline" id="S3.SS1.p2.4.m1.1"><semantics id="S3.SS1.p2.4.m1.1a"><mrow id="S3.SS1.p2.4.m1.1b"><mi id="S3.SS1.p2.4.m1.1.1" mathvariant="normal">Γ</mi><mrow id="S3.SS1.p2.4.m1.1.2"><mo id="S3.SS1.p2.4.m1.1.2.1" stretchy="false">(</mo><mo id="S3.SS1.p2.4.m1.1.2.2" lspace="0em" rspace="0.167em">.</mo><mo id="S3.SS1.p2.4.m1.1.2.3" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m1.1c">\Gamma(.)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m1.1d">roman_Γ ( . )</annotation></semantics></math> and <math alttext="\Gamma(.,.)" class="ltx_math_unparsed" display="inline" id="S3.SS1.p2.5.m2.1"><semantics id="S3.SS1.p2.5.m2.1a"><mrow id="S3.SS1.p2.5.m2.1b"><mi id="S3.SS1.p2.5.m2.1.1" mathvariant="normal">Γ</mi><mrow id="S3.SS1.p2.5.m2.1.2"><mo id="S3.SS1.p2.5.m2.1.2.1" stretchy="false">(</mo><mo id="S3.SS1.p2.5.m2.1.2.2" lspace="0em" rspace="0.167em">.</mo><mo id="S3.SS1.p2.5.m2.1.2.3">,</mo><mo id="S3.SS1.p2.5.m2.1.2.4" lspace="0em" rspace="0.167em">.</mo><mo id="S3.SS1.p2.5.m2.1.2.5" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m2.1c">\Gamma(.,.)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m2.1d">roman_Γ ( . , . )</annotation></semantics></math> are the Gamma and upper incomplete Gamma functions respectively, <math alttext="\Sigma" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m3.1"><semantics id="S3.SS1.p2.6.m3.1a"><mi id="S3.SS1.p2.6.m3.1.1" mathvariant="normal" xref="S3.SS1.p2.6.m3.1.1.cmml">Σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m3.1b"><ci id="S3.SS1.p2.6.m3.1.1.cmml" xref="S3.SS1.p2.6.m3.1.1">Σ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m3.1c">\Sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m3.1d">roman_Σ</annotation></semantics></math> represents the covariance matrix of the centered variable <math alttext="X_{i}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m4.1"><semantics id="S3.SS1.p2.7.m4.1a"><msub id="S3.SS1.p2.7.m4.1.1" xref="S3.SS1.p2.7.m4.1.1.cmml"><mi id="S3.SS1.p2.7.m4.1.1.2" xref="S3.SS1.p2.7.m4.1.1.2.cmml">X</mi><mi id="S3.SS1.p2.7.m4.1.1.3" xref="S3.SS1.p2.7.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m4.1b"><apply id="S3.SS1.p2.7.m4.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m4.1.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m4.1.1.2.cmml" xref="S3.SS1.p2.7.m4.1.1.2">𝑋</ci><ci id="S3.SS1.p2.7.m4.1.1.3.cmml" xref="S3.SS1.p2.7.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m4.1c">X_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m4.1d">italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\eta_{test}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m5.1"><semantics id="S3.SS1.p2.8.m5.1a"><msub id="S3.SS1.p2.8.m5.1.1" xref="S3.SS1.p2.8.m5.1.1.cmml"><mi id="S3.SS1.p2.8.m5.1.1.2" xref="S3.SS1.p2.8.m5.1.1.2.cmml">η</mi><mrow id="S3.SS1.p2.8.m5.1.1.3" xref="S3.SS1.p2.8.m5.1.1.3.cmml"><mi id="S3.SS1.p2.8.m5.1.1.3.2" xref="S3.SS1.p2.8.m5.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p2.8.m5.1.1.3.1" xref="S3.SS1.p2.8.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m5.1.1.3.3" xref="S3.SS1.p2.8.m5.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p2.8.m5.1.1.3.1a" xref="S3.SS1.p2.8.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m5.1.1.3.4" xref="S3.SS1.p2.8.m5.1.1.3.4.cmml">s</mi><mo id="S3.SS1.p2.8.m5.1.1.3.1b" xref="S3.SS1.p2.8.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m5.1.1.3.5" xref="S3.SS1.p2.8.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m5.1b"><apply id="S3.SS1.p2.8.m5.1.1.cmml" xref="S3.SS1.p2.8.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m5.1.1.1.cmml" xref="S3.SS1.p2.8.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m5.1.1.2.cmml" xref="S3.SS1.p2.8.m5.1.1.2">𝜂</ci><apply id="S3.SS1.p2.8.m5.1.1.3.cmml" xref="S3.SS1.p2.8.m5.1.1.3"><times id="S3.SS1.p2.8.m5.1.1.3.1.cmml" xref="S3.SS1.p2.8.m5.1.1.3.1"></times><ci id="S3.SS1.p2.8.m5.1.1.3.2.cmml" xref="S3.SS1.p2.8.m5.1.1.3.2">𝑡</ci><ci id="S3.SS1.p2.8.m5.1.1.3.3.cmml" xref="S3.SS1.p2.8.m5.1.1.3.3">𝑒</ci><ci id="S3.SS1.p2.8.m5.1.1.3.4.cmml" xref="S3.SS1.p2.8.m5.1.1.3.4">𝑠</ci><ci id="S3.SS1.p2.8.m5.1.1.3.5.cmml" xref="S3.SS1.p2.8.m5.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m5.1c">\eta_{test}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m5.1d">italic_η start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the number of tested pixels (i.e., the total number of pixels within an image).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.6">The integration of this <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2a.cmml" xref="S3.SS1.p3.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">NFA</mtext></ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> criterion is illustrated on Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S3.F1" title="Figure 1 ‣ III-A Integrating an a contrario criterion into YOLO ‣ III Proposed method ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">1</span></a>. To do so, we first modify the YOLO detection head by separately predicting the bounding box coordinates, the classification scores and the objectness scores. <span class="ltx_text" id="S3.SS1.p3.6.2" style="color:#000000;">Then</span>, after the original convolution step for predicting the objectness score, <span class="ltx_text" id="S3.SS1.p3.6.3" style="color:#000000;">we add</span> our NFA detection head. The latter consists in computing the <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.4">significance</span> for each box, defined as <math alttext="-\log(\textsc{NFA}_{\mathcal{N}})" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.2"><semantics id="S3.SS1.p3.2.m2.2a"><mrow id="S3.SS1.p3.2.m2.2.2" xref="S3.SS1.p3.2.m2.2.2.cmml"><mo id="S3.SS1.p3.2.m2.2.2a" rspace="0.167em" xref="S3.SS1.p3.2.m2.2.2.cmml">−</mo><mrow id="S3.SS1.p3.2.m2.2.2.1.1" xref="S3.SS1.p3.2.m2.2.2.1.2.cmml"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">log</mi><mo id="S3.SS1.p3.2.m2.2.2.1.1a" xref="S3.SS1.p3.2.m2.2.2.1.2.cmml">⁡</mo><mrow id="S3.SS1.p3.2.m2.2.2.1.1.1" xref="S3.SS1.p3.2.m2.2.2.1.2.cmml"><mo id="S3.SS1.p3.2.m2.2.2.1.1.1.2" stretchy="false" xref="S3.SS1.p3.2.m2.2.2.1.2.cmml">(</mo><msub id="S3.SS1.p3.2.m2.2.2.1.1.1.1" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.2.m2.2.2.1.1.1.1.2" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.2.2.1.1.1.1.3" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.3.cmml">𝒩</mi></msub><mo id="S3.SS1.p3.2.m2.2.2.1.1.1.3" stretchy="false" xref="S3.SS1.p3.2.m2.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.2b"><apply id="S3.SS1.p3.2.m2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2"><minus id="S3.SS1.p3.2.m2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2"></minus><apply id="S3.SS1.p3.2.m2.2.2.1.2.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1"><log id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"></log><apply id="S3.SS1.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.1.2a.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.2.m2.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.2">NFA</mtext></ci><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.1.3">𝒩</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.2c">-\log(\textsc{NFA}_{\mathcal{N}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.2d">- roman_log ( NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT )</annotation></semantics></math>. Finally, we apply the <math alttext="\textsc{Sigm}_{\alpha}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2a.cmml">Sigm</mtext><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">α</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2a.cmml" xref="S3.SS1.p3.3.m3.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">Sigm</mtext></ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\textsc{Sigm}_{\alpha}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">Sigm start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT</annotation></semantics></math> activation function defined in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite> to obtain an objectness score that ranges between <math alttext="0" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mn id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><cn id="S3.SS1.p3.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p3.4.m4.1.1">0</cn></annotation-xml></semantics></math> and <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><mn id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><cn id="S3.SS1.p3.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">1</annotation></semantics></math>. Note that since the YOLO version we consider is multi-scale, we <span class="ltx_text" id="S3.SS1.p3.6.5" style="color:#000000;">get multiple</span> <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.6.6">significance</span> maps (one for each scale). However, <span class="ltx_text" id="S3.SS1.p3.6.1" style="color:#000000;">since we define a constant value for <math alttext="\eta_{test}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.1.m1.1"><semantics id="S3.SS1.p3.6.1.m1.1a"><msub id="S3.SS1.p3.6.1.m1.1.1" xref="S3.SS1.p3.6.1.m1.1.1.cmml"><mi id="S3.SS1.p3.6.1.m1.1.1.2" mathcolor="#000000" xref="S3.SS1.p3.6.1.m1.1.1.2.cmml">η</mi><mrow id="S3.SS1.p3.6.1.m1.1.1.3" xref="S3.SS1.p3.6.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.6.1.m1.1.1.3.2" mathcolor="#000000" xref="S3.SS1.p3.6.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p3.6.1.m1.1.1.3.1" xref="S3.SS1.p3.6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.6.1.m1.1.1.3.3" mathcolor="#000000" xref="S3.SS1.p3.6.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p3.6.1.m1.1.1.3.1a" xref="S3.SS1.p3.6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.6.1.m1.1.1.3.4" mathcolor="#000000" xref="S3.SS1.p3.6.1.m1.1.1.3.4.cmml">s</mi><mo id="S3.SS1.p3.6.1.m1.1.1.3.1b" xref="S3.SS1.p3.6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.6.1.m1.1.1.3.5" mathcolor="#000000" xref="S3.SS1.p3.6.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.1.m1.1b"><apply id="S3.SS1.p3.6.1.m1.1.1.cmml" xref="S3.SS1.p3.6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.1.m1.1.1.1.cmml" xref="S3.SS1.p3.6.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.1.m1.1.1.2.cmml" xref="S3.SS1.p3.6.1.m1.1.1.2">𝜂</ci><apply id="S3.SS1.p3.6.1.m1.1.1.3.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3"><times id="S3.SS1.p3.6.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3.1"></times><ci id="S3.SS1.p3.6.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS1.p3.6.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS1.p3.6.1.m1.1.1.3.4.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3.4">𝑠</ci><ci id="S3.SS1.p3.6.1.m1.1.1.3.5.cmml" xref="S3.SS1.p3.6.1.m1.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.1.m1.1c">\eta_{test}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.1.m1.1d">italic_η start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>,</span> each scale has the same weight in the decision. This strategy may be sub-optimal in some cases, where many large objects need to be detected. For this purpose, we introduce some weighting coefficient<span class="ltx_text" id="S3.SS1.p3.6.7" style="color:#000000;">s</span>, which are obtained using an attention layer, namely the ECA <span class="ltx_text" id="S3.SS1.p3.6.8" style="color:#000000;">(Efficient Channel Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib23" title="">23</a>]</cite>)</span> layer. The integration of <span class="ltx_text" id="S3.SS1.p3.6.9" style="color:#000000;">this</span> layer is omitted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S3.F1" title="Figure 1 ‣ III-A Integrating an a contrario criterion into YOLO ‣ III Proposed method ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">1</span></a> for simplicity.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text" id="S3.SS1.p4.1.1" style="color:#000000;">Finally, note that w</span>e train the YOLO<math alttext="+\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mo id="S3.SS1.p4.1.m1.1.1a" xref="S3.SS1.p4.1.m1.1.1.cmml">+</mo><msub id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.1.m1.1.1.2.2" xref="S3.SS1.p4.1.m1.1.1.2.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.1.1.2.3" xref="S3.SS1.p4.1.m1.1.1.2.3.cmml">𝒩</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><plus id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"></plus><apply id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.2.1.cmml" xref="S3.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.2a.cmml" xref="S3.SS1.p4.1.m1.1.1.2.2"><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.1.m1.1.1.2.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2.2">NFA</mtext></ci><ci id="S3.SS1.p4.1.m1.1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.1.2.3">𝒩</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">+\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">+ NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> in an end-to-end manner using the Mean Squared Error loss instead of <span class="ltx_text" id="S3.SS1.p4.1.2" style="color:#000000;">the</span> Binary Cross Entropy loss for the objectness scores as it has shown to lead to better performance in our experiments.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Choosing appropriate SSL initialisation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this work, we evaluate the benefits of SSL pre-training for IRSTD, with a particular focus on its effectiveness when combined with our YOLO + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2a.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">NFA</mtext></ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detector. To this end, we select a range of SSL strategies, including methods from MIM, global instance discrimination, and local instance discrimination approaches. Although local methods are expected to work better for our task, we choose to test several methods representative of different SSL strategies, including global instance discrimination methods. This allows us to assess the relevance of the different SSL strategies for our use case.
It is important to note that, in the literature, pre-trained weights are available only for ResNet-50 when dealing with convolutional backbones. Therefore, we adapt the YOLOv7-tiny architecture by replacing its original backbone with a ResNet-50. The SSL methods we consider for evaluation are as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">DINO</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib16" title="">16</a>]</cite> – This method belongs to the global instance discrimination category, and originates from BYOL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib24" title="">24</a>]</cite>, a pioneering SSL method based on self-distillation. Concretely, it consists in distilling the knowledge from a teacher branch to a student one (i.e.<span class="ltx_text" id="S3.I1.i1.p1.1.2" style="color:#000000;">,</span> training the student network to predict the representations learned by the teacher). To prevent the network from collapsing, the teacher<span class="ltx_text" id="S3.I1.i1.p1.1.3" style="color:#000000;">’s</span> weights are not shared with the student branch (i.e.<span class="ltx_text" id="S3.I1.i1.p1.1.4" style="color:#000000;">,</span> there is no backpropagation): instead, the teacher<span class="ltx_text" id="S3.I1.i1.p1.1.5" style="color:#000000;">’</span>s weights are progressively updated through an exponential moving average of the student<span class="ltx_text" id="S3.I1.i1.p1.1.6" style="color:#000000;">’s</span> weights.
DINO further improves BYOL by smoothly discretizing the representations. <span class="ltx_text" id="S3.I1.i1.p1.1.7" style="color:#000000;">The a</span>uthors also show that multi-crop augmentations are essential for improving the fine-tuning performance. <span class="ltx_text" id="S3.I1.i1.p1.1.8" style="color:#000000;">Specifically, local-to-global correspondences are learned by providing large crops to the teacher and small crops to the student that thus is trained to interpolate context from a small crop.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">ReSim</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib17" title="">17</a>]</cite> – This pretext task belongs to local instance discrimination category, where region-level augmentations are considered. Specifically, a sliding window extracts, in each branch <span class="ltx_text" id="S3.I1.i2.p1.1.2" style="color:#000000;">of a siamese network</span>, local features within the overlapping area between the two augmented views of the anchor sample. This creates local positive pairs that represent exactly the same spatial region in the original image (we say that the patches are geometrically aligned). The similarity between the pairs of local patches is <span class="ltx_text" id="S3.I1.i2.p1.1.3" style="color:#000000;">thus</span> enforced. ReSim can be built on several SSL frameworks, including SimSiam and MoCov2. In our experiments, we choose the MoCov2 framework, as it has led to better results in the original paper. In this case, negative samples can be sampled either from non-positive regions of the same image, or from crops from other images in the dataset.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">SparK</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib25" title="">25</a>]</cite> – This pretext task belongs to MIM methods, and is an adaptation of the famous MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib20" title="">20</a>]</cite> method to convolutional encoders. <span class="ltx_text" id="S3.I1.i3.p1.1.2" style="color:#000000;">U</span>nlike ViT architectures that analyze each patch independently, CNN-based encoders perform convolutions by sliding a window, and thus the receptive field of the convolution can overlap with both masked and unmasked areas. This leads to several issues such as masked pattern vanishing or the disturbance of <span class="ltx_text" id="S3.I1.i3.p1.1.3" style="color:#000000;">the</span> distribution <span class="ltx_text" id="S3.I1.i3.p1.1.4" style="color:#000000;">of</span> pixel values, as explained in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib25" title="">25</a>]</cite>. SparK introduces the use of partial or sparse convolution<span class="ltx_text" id="S3.I1.i3.p1.1.5" style="color:#000000;">s, i.e. convolutions that provide output only when the kernel center covers an active (unmasked) input site</span>. <span class="ltx_text" id="S3.I1.i3.p1.1.6" style="color:#000000;">The</span> authors of <span class="ltx_text" id="S3.I1.i3.p1.1.7" style="color:#000000;">the</span> SparK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib25" title="">25</a>]</cite> paper show that MAE pre-training with a CNN-based encoder can outperform ViT-based MAE pre-training when using sparse convolution and a modern CNN-based encoder, namely ConvX-B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We use weights pre-trained on the ImageNet dataset to initialize the encoder of our YOLO + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2a.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">NFA</mtext></ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detector. In the literature, encoders pre-trained with SSL methods are evaluated on the downstream tasks after fine-tuning the entire classification, detection or segmentation network. However, as explained in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib27" title="">27</a>]</cite>, such a fine-tuning strategy may not be suitable for dense prediction tasks<span class="ltx_text" id="S3.SS2.p2.1.1" style="color:#000000;">, i.e. tasks that output an image or a region set, such as small target detection</span>. This can be explained by the fact that a complex, randomly initialized detection or segmentation head has to be added on top of the encoder, and the backpropagation of these random weights can “break” the knowledge learned during the SSL pre-training of the encoder. To further improve the transfer learning performance on IRSTD and to investigate the benefits of each SSL method without the effects of fine-tuning, we propose to freeze the backbone layers (i.e., the ResNet layers in YOLO-R50) and to fine-tune <span class="ltx_text" id="S3.SS2.p2.1.2" style="color:#000000;">only</span> the rest of the neural network (i.e., the YOLOv7-tiny detection head), as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Experimental set-up</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.5.1.1">IV-A</span>1 </span>Datasets and evaluation metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">To assess our methods, we consider two IRSTD datasets, namely SIRST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib28" title="">28</a>]</cite> and IRSTD-1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib29" title="">29</a>]</cite> datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.5">SIRST is one of the first <span class="ltx_text" id="S4.SS1.SSS1.p2.5.1" style="color:#000000;">publicly released</span> real-image infrared small target datasets, and it is widely used in the literature as a reference dataset for IRSTD. This dataset contains <math alttext="427" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.1"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mn id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">427</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><cn id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p2.1.m1.1.1">427</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">427</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.1d">427</annotation></semantics></math> real mono-spectral infrared images (in NIR, SWIR or MWIR domains) with resolution <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.2.m2.1"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><mrow id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.SSS1.p2.2.m2.1.1.2" xref="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml">256</mn><mo id="S4.SS1.SSS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p2.2.m2.1.1.3" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><apply id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1"><times id="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.1"></times><cn id="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p2.2.m2.1.1.2">256</cn><cn id="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.2.m2.1d">256 × 256</annotation></semantics></math>. Moreover, <math alttext="90\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.3.m3.1"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><mrow id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml">90</mn><mo id="S4.SS1.SSS1.p2.3.m3.1.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">90\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.3.m3.1d">90 %</annotation></semantics></math> of the images contain a single target, and most targets follow the definition of a small target proposed by <span class="ltx_text" id="S4.SS1.SSS1.p2.5.2" style="color:#000000;">the</span> SPIE <span class="ltx_text" id="S4.SS1.SSS1.p2.5.3" style="color:#000000;">(Society of Photo-Optical Instrumentation Engineers)</span>, i.e. objects having a total spatial extent of less than <math alttext="80" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.4.m4.1"><semantics id="S4.SS1.SSS1.p2.4.m4.1a"><mn id="S4.SS1.SSS1.p2.4.m4.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.4.m4.1b"><cn id="S4.SS1.SSS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p2.4.m4.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.4.m4.1c">80</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.4.m4.1d">80</annotation></semantics></math> pixels (<math alttext="9\times 9" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.5.m5.1"><semantics id="S4.SS1.SSS1.p2.5.m5.1a"><mrow id="S4.SS1.SSS1.p2.5.m5.1.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.cmml"><mn id="S4.SS1.SSS1.p2.5.m5.1.1.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml">9</mn><mo id="S4.SS1.SSS1.p2.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p2.5.m5.1.1.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.5.m5.1b"><apply id="S4.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"><times id="S4.SS1.SSS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.1"></times><cn id="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p2.5.m5.1.1.2">9</cn><cn id="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p2.5.m5.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.5.m5.1c">9\times 9</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.5.m5.1d">9 × 9</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.6">IRSTD-1k is a recently published dataset; it is larger than SIRST (<math alttext="1000" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.1.m1.1"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mn id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><cn id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.1.m1.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">1000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.1.m1.1d">1000</annotation></semantics></math> images of resolution <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.2.m2.1"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mrow id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml"><mn id="S4.SS1.SSS1.p3.2.m2.1.1.2" xref="S4.SS1.SSS1.p3.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS1.SSS1.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS1.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p3.2.m2.1.1.3" xref="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><apply id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1"><times id="S4.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1.1"></times><cn id="S4.SS1.SSS1.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.2.m2.1.1.2">512</cn><cn id="S4.SS1.SSS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p3.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.2.m2.1d">512 × 512</annotation></semantics></math>) and contains more challenging scenes. It also contains some relatively large objects. Since our work focus<span class="ltx_text" id="S4.SS1.SSS1.p3.6.1" style="color:#000000;">es</span> on developing and evaluating methods for <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p3.6.2">small</span> target detection, we follow what is done in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite> and decide to remove <span class="ltx_text" id="S4.SS1.SSS1.p3.6.3" style="color:#000000;">the</span> images that contain targets having a spatial extent larger than <math alttext="90" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.3.m3.1"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><mn id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><cn id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.3.m3.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">90</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.3.m3.1d">90</annotation></semantics></math> pixels (this represents <math alttext="15\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.4.m4.1"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><mrow id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.SSS1.p3.4.m4.1.1.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.2.cmml">15</mn><mo id="S4.SS1.SSS1.p3.4.m4.1.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><apply id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p3.4.m4.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.4.m4.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.4.m4.1d">15 %</annotation></semantics></math> of the dataset). This filtered version of IRSTD-1k dataset is referred to as “IRSTD-850”. As YOLO networks are designed to take large image <span class="ltx_text" id="S4.SS1.SSS1.p3.6.4" style="color:#000000;">as</span> inputs, we upsample all images to the size <math alttext="640\times 640" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.5.m5.1"><semantics id="S4.SS1.SSS1.p3.5.m5.1a"><mrow id="S4.SS1.SSS1.p3.5.m5.1.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.cmml"><mn id="S4.SS1.SSS1.p3.5.m5.1.1.2" xref="S4.SS1.SSS1.p3.5.m5.1.1.2.cmml">640</mn><mo id="S4.SS1.SSS1.p3.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS1.p3.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS1.p3.5.m5.1.1.3" xref="S4.SS1.SSS1.p3.5.m5.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.5.m5.1b"><apply id="S4.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1"><times id="S4.SS1.SSS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.1"></times><cn id="S4.SS1.SSS1.p3.5.m5.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.5.m5.1.1.2">640</cn><cn id="S4.SS1.SSS1.p3.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.p3.5.m5.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.5.m5.1c">640\times 640</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.5.m5.1d">640 × 640</annotation></semantics></math> using bi-cubic interpolation. Both datasets are split into training, validation and test sets using a ratio of <math alttext="60:20:20" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.6.m6.1"><semantics id="S4.SS1.SSS1.p3.6.m6.1a"><mrow id="S4.SS1.SSS1.p3.6.m6.1.1" xref="S4.SS1.SSS1.p3.6.m6.1.1.cmml"><mn id="S4.SS1.SSS1.p3.6.m6.1.1.2" xref="S4.SS1.SSS1.p3.6.m6.1.1.2.cmml">60</mn><mo id="S4.SS1.SSS1.p3.6.m6.1.1.3" lspace="0.278em" rspace="0.278em" xref="S4.SS1.SSS1.p3.6.m6.1.1.3.cmml">:</mo><mn id="S4.SS1.SSS1.p3.6.m6.1.1.4" xref="S4.SS1.SSS1.p3.6.m6.1.1.4.cmml">20</mn><mo id="S4.SS1.SSS1.p3.6.m6.1.1.5" lspace="0.278em" rspace="0.278em" xref="S4.SS1.SSS1.p3.6.m6.1.1.5.cmml">:</mo><mn id="S4.SS1.SSS1.p3.6.m6.1.1.6" xref="S4.SS1.SSS1.p3.6.m6.1.1.6.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.6.m6.1b"><apply id="S4.SS1.SSS1.p3.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"><and id="S4.SS1.SSS1.p3.6.m6.1.1a.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"></and><apply id="S4.SS1.SSS1.p3.6.m6.1.1b.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"><ci id="S4.SS1.SSS1.p3.6.m6.1.1.3.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1.3">:</ci><cn id="S4.SS1.SSS1.p3.6.m6.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.6.m6.1.1.2">60</cn><cn id="S4.SS1.SSS1.p3.6.m6.1.1.4.cmml" type="integer" xref="S4.SS1.SSS1.p3.6.m6.1.1.4">20</cn></apply><apply id="S4.SS1.SSS1.p3.6.m6.1.1c.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"><ci id="S4.SS1.SSS1.p3.6.m6.1.1.5.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1.5">:</ci><share href="https://arxiv.org/html/2410.07437v1#S4.SS1.SSS1.p3.6.m6.1.1.4.cmml" id="S4.SS1.SSS1.p3.6.m6.1.1d.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"></share><cn id="S4.SS1.SSS1.p3.6.m6.1.1.6.cmml" type="integer" xref="S4.SS1.SSS1.p3.6.m6.1.1.6">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.6.m6.1c">60:20:20</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.6.m6.1d">60 : 20 : 20</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">For the evaluation, we focus on conventional object-level metrics, namely the F1 score and the Average Precision (AP), which computes the area under the object-level Precision-Recall curve. A detected object is counted as a true positive (TP) if it has an Intersection over Union (IoU) of at least <math alttext="5\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p4.1.m1.1"><semantics id="S4.SS1.SSS1.p4.1.m1.1a"><mrow id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.SSS1.p4.1.m1.1.1.2" xref="S4.SS1.SSS1.p4.1.m1.1.1.2.cmml">5</mn><mo id="S4.SS1.SSS1.p4.1.m1.1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.1b"><apply id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p4.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.1c">5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p4.1.m1.1d">5 %</annotation></semantics></math> with the ground truth. This low-constrained condition is due to the fact that <span class="ltx_text" id="S4.SS1.SSS1.p4.1.1" style="color:#000000;">for small targets,</span> a small shift in the number of predicted pixels leads to a large deviation in the IoU, as illustrated in <span class="ltx_text" id="S4.SS1.SSS1.p4.1.2" style="color:#000000;">Fig.1 of</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.5.1.1">IV-A</span>2 </span>Baselines</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.3">We compare our methods to several baselines. These include SOTA segmentation baselines such as DNANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib1" title="">1</a>]</cite> and DNIM+NFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite>. DNANet consists of several nested UNets and a multiscale fusion module that<span class="ltx_text" id="S4.SS1.SSS2.p1.3.1" style="color:#000000;">, together,</span> enable the segmentation of small objects variable size. DNIM+NFA consists in introducing an <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.3.2">a contrario</span> criterion on top of DNIM backbone (which is the backbone of DNANet network). We also consider YOLOv7-tiny and YOLO-R50 (YOLOv7-tiny with a ResNet-50 backbone) as YOLO baselines. All YOLO-based networks are trained from scratch on Nvidia RTX6000 GPU for <math alttext="600" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">600</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">600</annotation></semantics></math> epochs, with Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib32" title="">32</a>]</cite>, a batch size <span class="ltx_text" id="S4.SS1.SSS2.p1.3.3" style="color:#000000;">equal to</span> <math alttext="16" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mn id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><cn id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">16</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">16</annotation></semantics></math> and a learning rate <span class="ltx_text" id="S4.SS1.SSS2.p1.3.4" style="color:#000000;">equal to</span> <math alttext="0.001" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mn id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><cn id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" type="float" xref="S4.SS1.SSS2.p1.3.m3.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.3.m3.1d">0.001</annotation></semantics></math>. We use the same data-augmentation functions as those proposed by default in YOLOv7-tiny implementation. For DNANet and DNIM+NFA, we use the results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a>]</cite>, since our experimental setup is identical. The results presented in the <span class="ltx_text" id="S4.SS1.SSS2.p1.3.5" style="color:#000000;">t</span>ables are averaged across three different runs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Mixing both SSL and a contrario paradigms improve<span class="ltx_text" id="S4.SS2.6.2.1" style="color:#000000;">s</span> the baselines</span>
</h3>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.22">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.22.23.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T1.22.23.1.1" rowspan="2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.22.23.1.1.1" style="font-size:90%;">Backbone</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.22.23.1.2" rowspan="2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.22.23.1.2.1" style="font-size:90%;">Init.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.22.23.1.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.22.23.1.3.1" style="font-size:90%;">SIRST</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.22.23.1.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.22.23.1.4.1" style="font-size:90%;">IRSTD-850</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.T1.1.1.1.1" style="font-size:90%;">AP</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.T1.2.2.2.1" style="font-size:90%;">AP</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.24.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T1.22.24.2.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.1.1" style="font-size:90%;">YOLO-R50</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.22.24.2.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.2.1" style="font-size:90%;">Scratch</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.22.24.2.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.3.1" style="font-size:90%;">97.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.22.24.2.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.4.1" style="font-size:90%;">98.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.22.24.2.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.5.1" style="font-size:90%;">82.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.22.24.2.6" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.24.2.6.1" style="font-size:90%;">84.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.3.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T1.3.3.1.1" style="font-size:90%;">+</span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><msub id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T1.3.3.1.m1.1.1.2" mathsize="90%" xref="S4.T1.3.3.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T1.3.3.1.m1.1.1.3" mathsize="90%" xref="S4.T1.3.3.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.1.m1.1.1.2a.cmml" xref="S4.T1.3.3.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T1.3.3.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T1.3.3.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T1.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.7.7.6" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.7.7.6.1" style="font-size:90%;">Scratch</span></th>
<td class="ltx_td ltx_align_left" id="S4.T1.4.4.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="96.9" class="ltx_Math" display="inline" id="S4.T1.4.4.2.m1.1"><semantics id="S4.T1.4.4.2.m1.1a"><mn id="S4.T1.4.4.2.m1.1.1" mathsize="90%" xref="S4.T1.4.4.2.m1.1.1.cmml">96.9</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b"><cn id="S4.T1.4.4.2.m1.1.1.cmml" type="float" xref="S4.T1.4.4.2.m1.1.1">96.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">96.9</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.2.m1.1d">96.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T1.5.5.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="98.3" class="ltx_Math" display="inline" id="S4.T1.5.5.3.m1.1"><semantics id="S4.T1.5.5.3.m1.1a"><mn id="S4.T1.5.5.3.m1.1.1" mathsize="90%" xref="S4.T1.5.5.3.m1.1.1.cmml">98.3</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.3.m1.1b"><cn id="S4.T1.5.5.3.m1.1.1.cmml" type="float" xref="S4.T1.5.5.3.m1.1.1">98.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.3.m1.1c">98.3</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.3.m1.1d">98.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T1.6.6.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="87.0" class="ltx_Math" display="inline" id="S4.T1.6.6.4.m1.1"><semantics id="S4.T1.6.6.4.m1.1a"><mn id="S4.T1.6.6.4.m1.1.1" mathsize="90%" xref="S4.T1.6.6.4.m1.1.1.cmml">87.0</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.4.m1.1b"><cn id="S4.T1.6.6.4.m1.1.1.cmml" type="float" xref="S4.T1.6.6.4.m1.1.1">87.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.4.m1.1c">87.0</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.4.m1.1d">87.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T1.7.7.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="88.4" class="ltx_Math" display="inline" id="S4.T1.7.7.5.m1.1"><semantics id="S4.T1.7.7.5.m1.1a"><mn id="S4.T1.7.7.5.m1.1.1" mathsize="90%" xref="S4.T1.7.7.5.m1.1.1.cmml">88.4</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.5.m1.1b"><cn id="S4.T1.7.7.5.m1.1.1.cmml" type="float" xref="S4.T1.7.7.5.m1.1.1">88.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.5.m1.1c">88.4</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.5.m1.1d">88.4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.25.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T1.22.25.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.22.25.3.1.1" style="font-size:90%;color:#404040;">Instance discrimination methods</span></th>
<td class="ltx_td ltx_border_t" id="S4.T1.22.25.3.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.22.25.3.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.8.8.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T1.8.8.1.1" style="font-size:90%;">+</span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T1.8.8.1.m1.1"><semantics id="S4.T1.8.8.1.m1.1a"><msub id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T1.8.8.1.m1.1.1.2" mathsize="90%" xref="S4.T1.8.8.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T1.8.8.1.m1.1.1.3" mathsize="90%" xref="S4.T1.8.8.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><apply id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.8.8.1.m1.1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1">subscript</csymbol><ci id="S4.T1.8.8.1.m1.1.1.2a.cmml" xref="S4.T1.8.8.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T1.8.8.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T1.8.8.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T1.8.8.1.m1.1.1.3.cmml" xref="S4.T1.8.8.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.12.12.6" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.12.12.6.1" style="font-size:90%;">DINO</span></th>
<td class="ltx_td ltx_align_left" id="S4.T1.9.9.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="97.5" class="ltx_Math" display="inline" id="S4.T1.9.9.2.m1.1"><semantics id="S4.T1.9.9.2.m1.1a"><mn id="S4.T1.9.9.2.m1.1.1" mathsize="90%" xref="S4.T1.9.9.2.m1.1.1.cmml">97.5</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.2.m1.1b"><cn id="S4.T1.9.9.2.m1.1.1.cmml" type="float" xref="S4.T1.9.9.2.m1.1.1">97.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.2.m1.1c">97.5</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.2.m1.1d">97.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T1.10.10.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T1.10.10.3.1" style="font-size:90%;">98.6</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.11.11.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T1.11.11.4.1" style="font-size:90%;">88.4</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.12.12.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T1.12.12.5.1" style="font-size:90%;">90.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.17.17">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T1.13.13.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T1.13.13.1.1" style="font-size:90%;">+</span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T1.13.13.1.m1.1"><semantics id="S4.T1.13.13.1.m1.1a"><msub id="S4.T1.13.13.1.m1.1.1" xref="S4.T1.13.13.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T1.13.13.1.m1.1.1.2" mathsize="90%" xref="S4.T1.13.13.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T1.13.13.1.m1.1.1.3" mathsize="90%" xref="S4.T1.13.13.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><apply id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.13.13.1.m1.1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1">subscript</csymbol><ci id="S4.T1.13.13.1.m1.1.1.2a.cmml" xref="S4.T1.13.13.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T1.13.13.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T1.13.13.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T1.13.13.1.m1.1.1.3.cmml" xref="S4.T1.13.13.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.17.17.6" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.17.17.6.1" style="font-size:90%;">ReSim</span></th>
<td class="ltx_td ltx_align_left" id="S4.T1.14.14.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T1.14.14.2.1" style="font-size:90%;">99.1</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.15.15.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T1.15.15.3.1" style="font-size:90%;">98.6</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.16.16.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{87.6}" class="ltx_Math" display="inline" id="S4.T1.16.16.4.m1.1"><semantics id="S4.T1.16.16.4.m1.1a"><munder accentunder="true" id="S4.T1.16.16.4.m1.1.1" xref="S4.T1.16.16.4.m1.1.1.cmml"><mn id="S4.T1.16.16.4.m1.1.1.2" mathsize="90%" xref="S4.T1.16.16.4.m1.1.1.2.cmml">87.6</mn><mo id="S4.T1.16.16.4.m1.1.1.1" mathsize="90%" xref="S4.T1.16.16.4.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.4.m1.1b"><apply id="S4.T1.16.16.4.m1.1.1.cmml" xref="S4.T1.16.16.4.m1.1.1"><ci id="S4.T1.16.16.4.m1.1.1.1.cmml" xref="S4.T1.16.16.4.m1.1.1.1">¯</ci><cn id="S4.T1.16.16.4.m1.1.1.2.cmml" type="float" xref="S4.T1.16.16.4.m1.1.1.2">87.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.4.m1.1c">\underline{87.6}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.16.4.m1.1d">under¯ start_ARG 87.6 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T1.17.17.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{90.0}" class="ltx_Math" display="inline" id="S4.T1.17.17.5.m1.1"><semantics id="S4.T1.17.17.5.m1.1a"><munder accentunder="true" id="S4.T1.17.17.5.m1.1.1" xref="S4.T1.17.17.5.m1.1.1.cmml"><mn id="S4.T1.17.17.5.m1.1.1.2" mathsize="90%" xref="S4.T1.17.17.5.m1.1.1.2.cmml">90.0</mn><mo id="S4.T1.17.17.5.m1.1.1.1" mathsize="90%" xref="S4.T1.17.17.5.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.5.m1.1b"><apply id="S4.T1.17.17.5.m1.1.1.cmml" xref="S4.T1.17.17.5.m1.1.1"><ci id="S4.T1.17.17.5.m1.1.1.1.cmml" xref="S4.T1.17.17.5.m1.1.1.1">¯</ci><cn id="S4.T1.17.17.5.m1.1.1.2.cmml" type="float" xref="S4.T1.17.17.5.m1.1.1.2">90.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.5.m1.1c">\underline{90.0}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.17.17.5.m1.1d">under¯ start_ARG 90.0 end_ARG</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.26.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T1.22.26.4.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.22.26.4.1.1" style="font-size:90%;color:#404040;">MIM methods</span></th>
<td class="ltx_td ltx_border_t" id="S4.T1.22.26.4.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.22.26.4.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.22.22">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b" id="S4.T1.18.18.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T1.18.18.1.1" style="font-size:90%;">+</span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T1.18.18.1.m1.1"><semantics id="S4.T1.18.18.1.m1.1a"><msub id="S4.T1.18.18.1.m1.1.1" xref="S4.T1.18.18.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T1.18.18.1.m1.1.1.2" mathsize="90%" xref="S4.T1.18.18.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T1.18.18.1.m1.1.1.3" mathsize="90%" xref="S4.T1.18.18.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.1.m1.1b"><apply id="S4.T1.18.18.1.m1.1.1.cmml" xref="S4.T1.18.18.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.18.18.1.m1.1.1.1.cmml" xref="S4.T1.18.18.1.m1.1.1">subscript</csymbol><ci id="S4.T1.18.18.1.m1.1.1.2a.cmml" xref="S4.T1.18.18.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T1.18.18.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T1.18.18.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T1.18.18.1.m1.1.1.3.cmml" xref="S4.T1.18.18.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.18.18.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.22.22.6" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T1.22.22.6.1" style="font-size:90%;">SparK</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.19.19.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{97.8}" class="ltx_Math" display="inline" id="S4.T1.19.19.2.m1.1"><semantics id="S4.T1.19.19.2.m1.1a"><munder accentunder="true" id="S4.T1.19.19.2.m1.1.1" xref="S4.T1.19.19.2.m1.1.1.cmml"><mn id="S4.T1.19.19.2.m1.1.1.2" mathsize="90%" xref="S4.T1.19.19.2.m1.1.1.2.cmml">97.8</mn><mo id="S4.T1.19.19.2.m1.1.1.1" mathsize="90%" xref="S4.T1.19.19.2.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.2.m1.1b"><apply id="S4.T1.19.19.2.m1.1.1.cmml" xref="S4.T1.19.19.2.m1.1.1"><ci id="S4.T1.19.19.2.m1.1.1.1.cmml" xref="S4.T1.19.19.2.m1.1.1.1">¯</ci><cn id="S4.T1.19.19.2.m1.1.1.2.cmml" type="float" xref="S4.T1.19.19.2.m1.1.1.2">97.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.2.m1.1c">\underline{97.8}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.19.19.2.m1.1d">under¯ start_ARG 97.8 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.20.20.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{98.5}" class="ltx_Math" display="inline" id="S4.T1.20.20.3.m1.1"><semantics id="S4.T1.20.20.3.m1.1a"><munder accentunder="true" id="S4.T1.20.20.3.m1.1.1" xref="S4.T1.20.20.3.m1.1.1.cmml"><mn id="S4.T1.20.20.3.m1.1.1.2" mathsize="90%" xref="S4.T1.20.20.3.m1.1.1.2.cmml">98.5</mn><mo id="S4.T1.20.20.3.m1.1.1.1" mathsize="90%" xref="S4.T1.20.20.3.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.3.m1.1b"><apply id="S4.T1.20.20.3.m1.1.1.cmml" xref="S4.T1.20.20.3.m1.1.1"><ci id="S4.T1.20.20.3.m1.1.1.1.cmml" xref="S4.T1.20.20.3.m1.1.1.1">¯</ci><cn id="S4.T1.20.20.3.m1.1.1.2.cmml" type="float" xref="S4.T1.20.20.3.m1.1.1.2">98.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.3.m1.1c">\underline{98.5}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.20.20.3.m1.1d">under¯ start_ARG 98.5 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.21.21.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="86.9" class="ltx_Math" display="inline" id="S4.T1.21.21.4.m1.1"><semantics id="S4.T1.21.21.4.m1.1a"><mn id="S4.T1.21.21.4.m1.1.1" mathsize="90%" xref="S4.T1.21.21.4.m1.1.1.cmml">86.9</mn><annotation-xml encoding="MathML-Content" id="S4.T1.21.21.4.m1.1b"><cn id="S4.T1.21.21.4.m1.1.1.cmml" type="float" xref="S4.T1.21.21.4.m1.1.1">86.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.21.4.m1.1c">86.9</annotation><annotation encoding="application/x-llamapun" id="S4.T1.21.21.4.m1.1d">86.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.22.22.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="89.1" class="ltx_Math" display="inline" id="S4.T1.22.22.5.m1.1"><semantics id="S4.T1.22.22.5.m1.1a"><mn id="S4.T1.22.22.5.m1.1.1" mathsize="90%" xref="S4.T1.22.22.5.m1.1.1.cmml">89.1</mn><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.5.m1.1b"><cn id="S4.T1.22.22.5.m1.1.1.cmml" type="float" xref="S4.T1.22.22.5.m1.1.1">89.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.5.m1.1c">89.1</annotation><annotation encoding="application/x-llamapun" id="S4.T1.22.22.5.m1.1d">89.1</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results obtained on SIRST and IRSTD-850 datasets using a YOLO-R50 or YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T1.24.m1.1"><semantics id="S4.T1.24.m1.1b"><msub id="S4.T1.24.m1.1.1" xref="S4.T1.24.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T1.24.m1.1.1.2" xref="S4.T1.24.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T1.24.m1.1.1.3" xref="S4.T1.24.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.24.m1.1c"><apply id="S4.T1.24.m1.1.1.cmml" xref="S4.T1.24.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.24.m1.1.1.1.cmml" xref="S4.T1.24.m1.1.1">subscript</csymbol><ci id="S4.T1.24.m1.1.1.2a.cmml" xref="S4.T1.24.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T1.24.m1.1.1.2.cmml" xref="S4.T1.24.m1.1.1.2">NFA</mtext></ci><ci id="S4.T1.24.m1.1.1.3.cmml" xref="S4.T1.24.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.m1.1d">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.24.m1.1e">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> with different backbone initializations (Scratch, DINO, ReSim or SparK). <span class="ltx_text" id="S4.T1.33.1" style="color:#000000;">The b</span>est results are in bold, and <span class="ltx_text" id="S4.T1.34.2" style="color:#000000;">the</span> second best results are underlined. </figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.6">In this section, we evaluate the benefits of combining our <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2a.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detection head with the different SSL initialization, namely DINO, ReSim and SparK. The results obtained on SIRST and IRSTD-850 datasets are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.T1" title="TABLE I ‣ IV-B Mixing both SSL and a contrario paradigms improves the baselines ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">I</span></a><span class="ltx_text" id="S4.SS2.p1.6.1" style="color:#000000;">. T</span>hey have been averaged over three distinct training sessions.
First, we can notice that integrating an <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.6.2">a contrario</span> criterion into the training loop of YOLO-R50 leads to more robust and precise results as shown by the AP metric. Moreover, YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2a.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">NFA</mtext></ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" id="S4.SS2.p1.6.3" style="color:#000000;">significantly</span> improves the F1 score on the challenging IRSTD-850 dataset (<math alttext="+4.7\%" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mo id="S4.SS2.p1.3.m3.1.1a" xref="S4.SS2.p1.3.m3.1.1.cmml">+</mo><mrow id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.2.2.cmml">4.7</mn><mo id="S4.SS2.p1.3.m3.1.1.2.1" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><plus id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"></plus><apply id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.2.1">percent</csymbol><cn id="S4.SS2.p1.3.m3.1.1.2.2.cmml" type="float" xref="S4.SS2.p1.3.m3.1.1.2.2">4.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">+4.7\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">+ 4.7 %</annotation></semantics></math>).
Second, combining SSL with YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><msub id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p1.4.m4.1.1.2a.cmml" xref="S4.SS2.p1.4.m4.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">NFA</mtext></ci><ci id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> further improve<span class="ltx_text" id="S4.SS2.p1.6.4" style="color:#000000;">s</span> the results. More specifically, YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><msub id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2a.cmml" xref="S4.SS2.p1.5.m5.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">NFA</mtext></ci><ci id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights achieves more than <math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mn id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">99</mn><mo id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1">percent</csymbol><cn id="S4.SS2.p1.6.m6.1.1.2.cmml" type="integer" xref="S4.SS2.p1.6.m6.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">99 %</annotation></semantics></math> of F1 score, outperforming <span class="ltx_text" id="S4.SS2.p1.6.5" style="color:#000000;">all</span> other method<span class="ltx_text" id="S4.SS2.p1.6.6" style="color:#000000;">s</span> <span class="ltx_text" id="S4.SS2.p1.6.7" style="color:#000000;">by</span> a <span class="ltx_text" id="S4.SS2.p1.6.8" style="color:#000000;">wide</span> margin. It is worth noting that instance discrimination methods, whether local or global, appear to be more effective for IRSTD <span class="ltx_text" id="S4.SS2.p1.6.9" style="color:#000000;">than</span> MIM methods. This might seem counter-intuitive, as one would expect that extracting local features would be more beneficial for detecting small objects. However, this can be explained by the fact that higher-level feature extraction complements the locality already introduced by <span class="ltx_text" id="S4.SS2.p1.6.10" style="color:#000000;">the</span> convolutional layers. Additionally, the performance of MIM methods, which are more sensitive to image statistics due to their strong bias towards local details <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib33" title="">33</a>]</cite>, may be limited by the domain gap between RGB <span class="ltx_text" id="S4.SS2.p1.6.11" style="color:#000000;">training domain for the backbone layers, then frozen</span> and IR <span class="ltx_text" id="S4.SS2.p1.6.12" style="color:#000000;">(application domain)</span>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.F2" title="Figure 2 ‣ IV-B Mixing both SSL and a contrario paradigms improves the baselines ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">2</span></a> provides some examples of predictions on challenging scenes from the SIRST and IRSTD-850 datasets. Our YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2a.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> model initialized with ReSim weights leads to fewer missed detections compared to the baseline, particularly for very small targets, as seen in images a), b), and d). Additionally, the model shows greater robustness against false alarms, as illustrated in image e), where it effectively handles noisy backgrounds.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="950" id="S4.F2.g1" src="x2.png" width="490"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.4.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.2.1" style="font-size:90%;">Qualitative results obtained with YOLO-R50 and YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.F2.2.1.m1.1"><semantics id="S4.F2.2.1.m1.1b"><msub id="S4.F2.2.1.m1.1.1" xref="S4.F2.2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.F2.2.1.m1.1.1.2" xref="S4.F2.2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.F2.2.1.m1.1.1.3" xref="S4.F2.2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F2.2.1.m1.1c"><apply id="S4.F2.2.1.m1.1.1.cmml" xref="S4.F2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F2.2.1.m1.1.1.1.cmml" xref="S4.F2.2.1.m1.1.1">subscript</csymbol><ci id="S4.F2.2.1.m1.1.1.2a.cmml" xref="S4.F2.2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.F2.2.1.m1.1.1.2.cmml" xref="S4.F2.2.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.F2.2.1.m1.1.1.3.cmml" xref="S4.F2.2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.1.m1.1d">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.1.m1.1e">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights on challenging scenes from SIRST and IRSTD-850 datasets. True positives, false positives and missed detections are circled in green, red and yellow lines, respectively.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">What about frugal setting?</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.3"><span class="ltx_text" id="S4.SS3.p1.1.1" style="color:#000000;">For this section, we focus on the most promising method according to Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.T1" title="TABLE I ‣ IV-B Mixing both SSL and a contrario paradigms improves the baselines ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">I</span></a>, namely YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.1.m1.1"><semantics id="S4.SS3.p1.1.1.m1.1a"><msub id="S4.SS3.p1.1.1.m1.1.1" xref="S4.SS3.p1.1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.1.1.m1.1.1.2" mathcolor="#000000" xref="S4.SS3.p1.1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p1.1.1.m1.1.1.3" mathcolor="#000000" xref="S4.SS3.p1.1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.1.m1.1b"><apply id="S4.SS3.p1.1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.1.1.m1.1.1.2.cmml" mathcolor="#000000" xref="S4.SS3.p1.1.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS3.p1.1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> with ReSim initialization.</span> <span class="ltx_text" id="S4.SS3.p1.3.2" style="color:#000000;">We aim to</span> evaluate <span class="ltx_text" id="S4.SS3.p1.3.3" style="color:#000000;">it under more</span> challenging conditions, namely <math alttext="25" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m1.1"><semantics id="S4.SS3.p1.2.m1.1a"><mn id="S4.SS3.p1.2.m1.1.1" xref="S4.SS3.p1.2.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m1.1b"><cn id="S4.SS3.p1.2.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.2.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m1.1c">25</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m1.1d">25</annotation></semantics></math>-shot training on SIRST. Specifically, we compare YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m2.1"><semantics id="S4.SS3.p1.3.m2.1a"><msub id="S4.SS3.p1.3.m2.1.1" xref="S4.SS3.p1.3.m2.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.3.m2.1.1.2" xref="S4.SS3.p1.3.m2.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p1.3.m2.1.1.3" xref="S4.SS3.p1.3.m2.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m2.1b"><apply id="S4.SS3.p1.3.m2.1.1.cmml" xref="S4.SS3.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m2.1.1.1.cmml" xref="S4.SS3.p1.3.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.3.m2.1.1.2a.cmml" xref="S4.SS3.p1.3.m2.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS3.p1.3.m2.1.1.2.cmml" xref="S4.SS3.p1.3.m2.1.1.2">NFA</mtext></ci><ci id="S4.SS3.p1.3.m2.1.1.3.cmml" xref="S4.SS3.p1.3.m2.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m2.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m2.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> (using either randomly initialized weights or ReSim initialization) with several baselines: 1) SOTA segmentation networks for IRSTD and 2) YOLO baselines (some with ReSim initialization). The results<span class="ltx_text" id="S4.SS3.p1.3.4" style="color:#000000;">, averaged over three distinct training sessions,</span> are presented on Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.T2" title="TABLE II ‣ IV-C What about frugal setting? ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.8"><span class="ltx_text" id="S4.SS3.p2.8.1" style="color:#000000;">The first thing to note is that while the</span> YOLO baselines perform particularly poorly in a frugal setting compared to SOTA segmentation networks, the integration of our <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2a.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detection head, as well as its combination with SSL pre-training, leads to very impressive results. Indeed, YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2a.cmml" xref="S4.SS3.p2.2.m2.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">NFA</mtext></ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights <span class="ltx_text" id="S4.SS3.p2.8.2" style="color:#000000;">achieves</span> a F1 score of <math alttext="95.4\%" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">95.4</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p2.3.m3.1.1.2.cmml" type="float" xref="S4.SS3.p2.3.m3.1.1.2">95.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">95.4\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">95.4 %</annotation></semantics></math> with only <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">10</mn><mo id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1">percent</csymbol><cn id="S4.SS3.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">10 %</annotation></semantics></math> of the SIRST dataset, outperforming DNIM+NFA <span class="ltx_text" id="S4.SS3.p2.8.3" style="color:#000000;">by</span> a wide margin (<math alttext="+4.5\%" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mo id="S4.SS3.p2.5.m5.1.1a" xref="S4.SS3.p2.5.m5.1.1.cmml">+</mo><mrow id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml"><mn id="S4.SS3.p2.5.m5.1.1.2.2" xref="S4.SS3.p2.5.m5.1.1.2.2.cmml">4.5</mn><mo id="S4.SS3.p2.5.m5.1.1.2.1" xref="S4.SS3.p2.5.m5.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><plus id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"></plus><apply id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.5.m5.1.1.2.1.cmml" xref="S4.SS3.p2.5.m5.1.1.2.1">percent</csymbol><cn id="S4.SS3.p2.5.m5.1.1.2.2.cmml" type="float" xref="S4.SS3.p2.5.m5.1.1.2.2">4.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">+4.5\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">+ 4.5 %</annotation></semantics></math>). Notably, such a strategy allows us to achieve a F1 score that is <span class="ltx_text" id="S4.SS3.p2.8.4" style="color:#000000;">almost</span> as high as that obtained with the full SIRST dataset. It is worth noting that the NFA detection head contributes the most to <span class="ltx_text" id="S4.SS3.p2.8.5" style="color:#000000;">the</span> good performance in frugal setting. Indeed, YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><msub id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.6.m6.1.1.2" xref="S4.SS3.p2.6.m6.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p2.6.m6.1.1.3" xref="S4.SS3.p2.6.m6.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><apply id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.6.m6.1.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.p2.6.m6.1.1.2a.cmml" xref="S4.SS3.p2.6.m6.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.6.m6.1.1.2.cmml" xref="S4.SS3.p2.6.m6.1.1.2">NFA</mtext></ci><ci id="S4.SS3.p2.6.m6.1.1.3.cmml" xref="S4.SS3.p2.6.m6.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" id="S4.SS3.p2.8.6" style="color:#000000;">achieves</span> a F1 score of <math alttext="91.9\%" class="ltx_Math" display="inline" id="S4.SS3.p2.7.m7.1"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml"><mn id="S4.SS3.p2.7.m7.1.1.2" xref="S4.SS3.p2.7.m7.1.1.2.cmml">91.9</mn><mo id="S4.SS3.p2.7.m7.1.1.1" xref="S4.SS3.p2.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1"><csymbol cd="latexml" id="S4.SS3.p2.7.m7.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1.1">percent</csymbol><cn id="S4.SS3.p2.7.m7.1.1.2.cmml" type="float" xref="S4.SS3.p2.7.m7.1.1.2">91.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">91.9\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.7.m7.1d">91.9 %</annotation></semantics></math>, while YOLO-R50 with the best backbone initialization leads to a F1 score of <span class="ltx_text" id="S4.SS3.p2.8.7" style="color:#000000;">only</span> <math alttext="43.6\%" class="ltx_Math" display="inline" id="S4.SS3.p2.8.m8.1"><semantics id="S4.SS3.p2.8.m8.1a"><mrow id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml"><mn id="S4.SS3.p2.8.m8.1.1.2" xref="S4.SS3.p2.8.m8.1.1.2.cmml">43.6</mn><mo id="S4.SS3.p2.8.m8.1.1.1" xref="S4.SS3.p2.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><apply id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1"><csymbol cd="latexml" id="S4.SS3.p2.8.m8.1.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1.1">percent</csymbol><cn id="S4.SS3.p2.8.m8.1.1.2.cmml" type="float" xref="S4.SS3.p2.8.m8.1.1.2">43.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">43.6\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.8.m8.1d">43.6 %</annotation></semantics></math>. This highlights the robustness of our method in difficult training scenarios.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.3.4.1.1" rowspan="2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.1.1" style="font-size:90%;">Method</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="S4.T2.3.4.1.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.2.1" style="font-size:90%;">25-shots</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.5.2.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.1.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.3.5.2.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.2.2.1" style="font-size:90%;">AP</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="3" id="S4.T2.3.6.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.6.3.1.1" style="font-size:90%;color:#404040;">SOTA segmentation baselines for IRSTD</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.7.4.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T2.3.7.4.1.1" style="font-size:90%;">DNANet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.7.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib1" title="">1</a><span class="ltx_text" id="S4.T2.3.7.4.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_left" id="S4.T2.3.7.4.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.7.4.2.1" style="font-size:90%;">73.1</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.7.4.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.7.4.3.1" style="font-size:90%;">63.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:90%;">DNIM+</span><span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.T2.1.1.1.2" style="font-size:90%;">NFA</span><span class="ltx_text" id="S4.T2.1.1.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a><span class="ltx_text" id="S4.T2.1.1.1.5.2" style="font-size:90%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.1.1.2.1" style="font-size:90%;">90.9</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.1.1.3.1" style="font-size:90%;">93.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3" id="S4.T2.3.8.5.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.8.5.1.1" style="font-size:90%;color:#404040;">YOLO baselines</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.9.6.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.9.6.1.1" style="font-size:90%;">YOLOv7-tiny</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.3.9.6.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.9.6.2.1" style="font-size:90%;">21.8</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.9.6.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.9.6.3.1" style="font-size:90%;">15.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.10.7.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.10.7.1.1" style="font-size:90%;">YOLO-R50</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.3.10.7.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.10.7.2.1" style="font-size:90%;">26.1</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.10.7.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.10.7.3.1" style="font-size:90%;">23.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.11.8.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.11.8.1.1" style="font-size:90%;">YOLO-R50 + ReSim</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.3.11.8.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.11.8.2.1" style="font-size:90%;">43.6</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.3.11.8.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T2.3.11.8.3.1" style="font-size:90%;">43.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3" id="S4.T2.3.12.9.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.12.9.1.1" style="font-size:90%;color:#404040;">Our methods</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T2.2.2.1.1" style="font-size:90%;">YOLO-R50 + </span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><msub id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T2.2.2.1.m1.1.1.2" mathsize="90%" xref="S4.T2.2.2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T2.2.2.1.m1.1.1.3" mathsize="90%" xref="S4.T2.2.2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><apply id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.1.m1.1.1.2a.cmml" xref="S4.T2.2.2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T2.2.2.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T2.2.2.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.2.2.1" style="font-size:90%;">91.9</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.2.2.3.1" style="font-size:90%;">94.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.3.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T2.3.3.1.1" style="font-size:90%;">YOLO-R50 + </span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><msub id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T2.3.3.1.m1.1.1.2" mathsize="90%" xref="S4.T2.3.3.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T2.3.3.1.m1.1.1.3" mathsize="90%" xref="S4.T2.3.3.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><apply id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T2.3.3.1.m1.1.1.2a.cmml" xref="S4.T2.3.3.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T2.3.3.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T2.3.3.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T2.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T2.3.3.1.2" style="font-size:90%;"> + ReSim</span>
</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.3.3.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.2.1" style="font-size:90%;">95.4</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.3.3.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1" style="font-size:90%;">96.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results achieved in a 25-shot setting on SIRST dataset. The best results are in bold, and the second best results are underlined. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Comparison with SOTA segmentation networks</span>
</h3>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.12.13.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.12.13.1.1" rowspan="2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.12.13.1.1.1" style="font-size:90%;">Backbone init.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T3.12.13.1.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.12.13.1.2.1" style="font-size:90%;">SIRST</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T3.12.13.1.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.12.13.1.3.1" style="font-size:90%;">IRSTD-850</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.1.1.1.1" style="font-size:90%;">AP</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.2.2.2.1" style="font-size:90%;">AP</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.14.2">
<td class="ltx_td ltx_align_left ltx_border_tt" colspan="5" id="S4.T3.12.14.2.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.12.14.2.1.1" style="font-size:90%;color:#404040;">SOTA segmentation baselines for IRSTD</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.15.3">
<td class="ltx_td ltx_align_left" id="S4.T3.12.15.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T3.12.15.3.1.1" style="font-size:90%;">DNANet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.12.15.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib1" title="">1</a><span class="ltx_text" id="S4.T3.12.15.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.15.3.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.15.3.2.1" style="font-size:90%;">97.1</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.15.3.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.12.15.3.3.1" style="font-size:90%;">98.4</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.15.3.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.12.15.3.4.1" style="font-size:90%;">91.4</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.15.3.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.12.15.3.5.1" style="font-size:90%;">92.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.7">
<td class="ltx_td ltx_align_left" id="S4.T3.3.3.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T3.3.3.1.1" style="font-size:90%;">DNIM+</span><span class="ltx_text ltx_markedasmath ltx_font_smallcaps" id="S4.T3.3.3.1.2" style="font-size:90%;">NFA</span><span class="ltx_text" id="S4.T3.3.3.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.3.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#bib.bib9" title="">9</a><span class="ltx_text" id="S4.T3.3.3.1.5.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="S4.T3.4.4.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{97.6}" class="ltx_Math" display="inline" id="S4.T3.4.4.2.m1.1"><semantics id="S4.T3.4.4.2.m1.1a"><munder accentunder="true" id="S4.T3.4.4.2.m1.1.1" xref="S4.T3.4.4.2.m1.1.1.cmml"><mn id="S4.T3.4.4.2.m1.1.1.2" mathsize="90%" xref="S4.T3.4.4.2.m1.1.1.2.cmml">97.6</mn><mo id="S4.T3.4.4.2.m1.1.1.1" mathsize="90%" xref="S4.T3.4.4.2.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.2.m1.1b"><apply id="S4.T3.4.4.2.m1.1.1.cmml" xref="S4.T3.4.4.2.m1.1.1"><ci id="S4.T3.4.4.2.m1.1.1.1.cmml" xref="S4.T3.4.4.2.m1.1.1.1">¯</ci><cn id="S4.T3.4.4.2.m1.1.1.2.cmml" type="float" xref="S4.T3.4.4.2.m1.1.1.2">97.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.2.m1.1c">\underline{97.6}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.2.m1.1d">under¯ start_ARG 97.6 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{98.4}" class="ltx_Math" display="inline" id="S4.T3.5.5.3.m1.1"><semantics id="S4.T3.5.5.3.m1.1a"><munder accentunder="true" id="S4.T3.5.5.3.m1.1.1" xref="S4.T3.5.5.3.m1.1.1.cmml"><mn id="S4.T3.5.5.3.m1.1.1.2" mathsize="90%" xref="S4.T3.5.5.3.m1.1.1.2.cmml">98.4</mn><mo id="S4.T3.5.5.3.m1.1.1.1" mathsize="90%" xref="S4.T3.5.5.3.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.3.m1.1b"><apply id="S4.T3.5.5.3.m1.1.1.cmml" xref="S4.T3.5.5.3.m1.1.1"><ci id="S4.T3.5.5.3.m1.1.1.1.cmml" xref="S4.T3.5.5.3.m1.1.1.1">¯</ci><cn id="S4.T3.5.5.3.m1.1.1.2.cmml" type="float" xref="S4.T3.5.5.3.m1.1.1.2">98.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.3.m1.1c">\underline{98.4}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.3.m1.1d">under¯ start_ARG 98.4 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="\underline{91.3}" class="ltx_Math" display="inline" id="S4.T3.6.6.4.m1.1"><semantics id="S4.T3.6.6.4.m1.1a"><munder accentunder="true" id="S4.T3.6.6.4.m1.1.1" xref="S4.T3.6.6.4.m1.1.1.cmml"><mn id="S4.T3.6.6.4.m1.1.1.2" mathsize="90%" xref="S4.T3.6.6.4.m1.1.1.2.cmml">91.3</mn><mo id="S4.T3.6.6.4.m1.1.1.1" mathsize="90%" xref="S4.T3.6.6.4.m1.1.1.1.cmml">¯</mo></munder><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.4.m1.1b"><apply id="S4.T3.6.6.4.m1.1.1.cmml" xref="S4.T3.6.6.4.m1.1.1"><ci id="S4.T3.6.6.4.m1.1.1.1.cmml" xref="S4.T3.6.6.4.m1.1.1.1">¯</ci><cn id="S4.T3.6.6.4.m1.1.1.2.cmml" type="float" xref="S4.T3.6.6.4.m1.1.1.2">91.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.4.m1.1c">\underline{91.3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.4.m1.1d">under¯ start_ARG 91.3 end_ARG</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="S4.T3.7.7.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.7.7.5.1" style="font-size:90%;">94.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.16.4">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="5" id="S4.T3.12.16.4.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.12.16.4.1.1" style="font-size:90%;color:#404040;">YOLO baselines</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.17.5">
<td class="ltx_td ltx_align_left" id="S4.T3.12.17.5.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.17.5.1.1" style="font-size:90%;">YOLOv7-tiny</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.17.5.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.17.5.2.1" style="font-size:90%;">96.5</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.17.5.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.17.5.3.1" style="font-size:90%;">97.8</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.17.5.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.17.5.4.1" style="font-size:90%;">82.2</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.17.5.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.17.5.5.1" style="font-size:90%;">85.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.18.6">
<td class="ltx_td ltx_align_left" id="S4.T3.12.18.6.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.18.6.1.1" style="font-size:90%;">YOLO-R50</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.18.6.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.18.6.2.1" style="font-size:90%;">97.5</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.18.6.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.18.6.3.1" style="font-size:90%;">98.1</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.18.6.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.18.6.4.1" style="font-size:90%;">82.3</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.18.6.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text" id="S4.T3.12.18.6.5.1" style="font-size:90%;">84.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.19.7">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="5" id="S4.T3.12.19.7.1" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_font_italic" id="S4.T3.12.19.7.1.1" style="font-size:90%;color:#404040;">Our method</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.12">
<td class="ltx_td ltx_align_right ltx_border_b" id="S4.T3.8.8.1" style="padding-top:0.65pt;padding-bottom:0.65pt;">
<span class="ltx_text" id="S4.T3.8.8.1.1" style="font-size:90%;">+ </span><math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.T3.8.8.1.m1.1"><semantics id="S4.T3.8.8.1.m1.1a"><msub id="S4.T3.8.8.1.m1.1.1" xref="S4.T3.8.8.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.T3.8.8.1.m1.1.1.2" mathsize="90%" xref="S4.T3.8.8.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.T3.8.8.1.m1.1.1.3" mathsize="90%" xref="S4.T3.8.8.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.1.m1.1b"><apply id="S4.T3.8.8.1.m1.1.1.cmml" xref="S4.T3.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.8.8.1.m1.1.1.1.cmml" xref="S4.T3.8.8.1.m1.1.1">subscript</csymbol><ci id="S4.T3.8.8.1.m1.1.1.2a.cmml" xref="S4.T3.8.8.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.T3.8.8.1.m1.1.1.2.cmml" mathsize="90%" xref="S4.T3.8.8.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.T3.8.8.1.m1.1.1.3.cmml" xref="S4.T3.8.8.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T3.8.8.1.2" style="font-size:90%;"> + ReSim</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.9.9.2" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.9.9.2.1" style="font-size:90%;">99.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.10.10.3" style="padding-top:0.65pt;padding-bottom:0.65pt;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.10.10.3.1" style="font-size:90%;">98.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.11.11.4" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="87.6" class="ltx_Math" display="inline" id="S4.T3.11.11.4.m1.1"><semantics id="S4.T3.11.11.4.m1.1a"><mn id="S4.T3.11.11.4.m1.1.1" mathsize="90%" xref="S4.T3.11.11.4.m1.1.1.cmml">87.6</mn><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.4.m1.1b"><cn id="S4.T3.11.11.4.m1.1.1.cmml" type="float" xref="S4.T3.11.11.4.m1.1.1">87.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.4.m1.1c">87.6</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.11.4.m1.1d">87.6</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.12.12.5" style="padding-top:0.65pt;padding-bottom:0.65pt;"><math alttext="90.0" class="ltx_Math" display="inline" id="S4.T3.12.12.5.m1.1"><semantics id="S4.T3.12.12.5.m1.1a"><mn id="S4.T3.12.12.5.m1.1.1" mathsize="90%" xref="S4.T3.12.12.5.m1.1.1.cmml">90.0</mn><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.5.m1.1b"><cn id="S4.T3.12.12.5.m1.1.1.cmml" type="float" xref="S4.T3.12.12.5.m1.1.1">90.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.5.m1.1c">90.0</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.12.5.m1.1d">90.0</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Overview of the performance obtained by SOTA IRSTD methods, YOLO baselines as well as our methods on SIRST and IRSTD-850 datasets. The best performance is given in bold, and the second best results are underlined.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.3">In the previous sections, we have seen that both <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.3.1">a contrario</span> and self-supervised paradigms have led to impressive results for IRSTD. Specifically, combining both paradigms greatly improves the YOLO baselines, especially in a frugal context, where the SOTA segmentation methods are outperformed <span class="ltx_text" id="S4.SS4.p1.3.2" style="color:#000000;">by</span> a <span class="ltx_text" id="S4.SS4.p1.3.3" style="color:#000000;">wide</span> margin.
In this section, we compare our best <span class="ltx_text" id="S4.SS4.p1.3.4" style="color:#000000;">candidate</span> method, namely YOLO-R50+<math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2a.cmml" xref="S4.SS4.p1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights, to SOTA segmentation baselines in a data-sufficient context on SIRST a<span class="ltx_text" id="S4.SS4.p1.3.5" style="color:#000000;">n</span>d IRSTD-85 datasets. The results, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07437v1#S4.T3" title="TABLE III ‣ IV-D Comparison with SOTA segmentation networks ‣ IV Experiments ‣ Robust infrared small target detection using self-supervised and a contrario paradigms"><span class="ltx_text ltx_ref_tag">III</span></a>, indicate that our approach sets new SOTA results on the SIRST dataset: YOLO-R50 + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2a.cmml" xref="S4.SS4.p1.2.m2.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">NFA</mtext></ci><ci id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights outperforms DNANet and DNIM+NFA <span class="ltx_text" id="S4.SS4.p1.3.6" style="color:#000000;">by</span> a <span class="ltx_text" id="S4.SS4.p1.3.7" style="color:#000000;">wide</span> margin, achieving over <math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mn id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">99</mn><mo id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1">percent</csymbol><cn id="S4.SS4.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">99 %</annotation></semantics></math> F1 score. This is particularly noteworthy because detection networks typically struggle with small objects compared to segmentation networks.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The results on the IRSTD-850 dataset, however, are less encouraging, and incorporating this criterion into any YOLO backbone does not yield competitive performance. This is because both <span class="ltx_text" id="S4.SS4.p2.1.1" style="color:#000000;">considered</span> YOLO baselines perform poorly on IRSTD-850 dataset. While adding the <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><msub id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2a.cmml" xref="S4.SS4.p2.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">NFA</mtext></ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> detection head to a YOLO backbone significantly improves its baseline performance, it still does not close the performance gap between YOLO-based networks and SOTA segmentation baselines. This can be explained by the fact that, unlike SIRST dataset, IRSTD-850 dataset presents more complex and textured backgrounds, making it particularly <span class="ltx_text" id="S4.SS4.p2.1.2" style="color:#000000;">difficult</span> to distinguish the targets from the noisy background. Therefore, there is still room for improvement in object detectors for tiny object detection in complex environments. For instance, improvements could be made to the YOLO backbone to reduce information loss on small objects, or the SSL pre-training could be <span class="ltx_text" id="S4.SS4.p2.1.3" style="color:#000000;">performed</span> on an in-domain dataset for better adaptation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce an original approach for IRSTD that combines self-supervised learning with the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">a contrario</span> paradigm. Integrated within a YOLO detection framework, our approach significantly reduces the performance gap between traditional object detectors and SOTA segmentation methods for IRSTD. Furthermore, our YOLO + <math alttext="\textsc{NFA}_{\mathcal{N}}" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><msub id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mtext class="ltx_font_smallcaps" id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2a.cmml">NFA</mtext><mi class="ltx_font_mathcaligraphic" id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">𝒩</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2a.cmml" xref="S5.p1.1.m1.1.1.2"><mtext class="ltx_font_smallcaps" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">NFA</mtext></ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝒩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\textsc{NFA}_{\mathcal{N}}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">NFA start_POSTSUBSCRIPT caligraphic_N end_POSTSUBSCRIPT</annotation></semantics></math> initialized with ReSim weights achieves new state-of-the-art results on the SIRST dataset, as well as impressive performance in a few-shot setting, demonstrating its effectiveness and potential for improving small target detection in challenging IR data. To further enhance the detection performance, it is recommended to consider adapting a Vision Transformer for small target detection. This approach has demonstrated effectiveness in modeling complex scenes, particularly when combined with SSL pre-training. Ultimately, the design of a more suitable transfer learning strategy should be explored.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Li, C. Xiao, L. Wang, Y. Wang, Z. Lin, M. Li, W. An, and Y. Guo, “Dense nested attention network for infrared small target detection,” <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">IEEE Transactions on Image Processing</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Huang, S. Dai, T. Huang, X. Huang, and H. Wang, “Infrared small target segmentation with multiscale feature representation,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Infrared Physics &amp; Technology</span>, vol. 116, p. 103755, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. Zhang, L. Li, S. Cao, T. Pu, and Z. Peng, “Attention-guided pyramid context networks for detecting infrared small target under complex background,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Aerospace and Electronic Systems</span>, vol. 59, no. 4, pp. 4250–4261, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</span>, vol. 28, pp. 91–99, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pp. 779–788, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Mou, S. Lei, and X. Zhou, “YOLO-FR: A YOLOv5 infrared small target detection algorithm based on feature reassembly sampling method,” <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Sensors</span>, vol. 23, no. 5, p. 2710, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Li and Y. Shen, “YOLOSR-IST: A deep learning method for small target detection in infrared remote sensing images based on super-resolution and YOLO,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Signal Processing</span>, vol. 208, p. 108962, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Yang, X. Zhang, J. Zhang, J. Luo, M. Zhou, and Y. Pi, “Eflnet: Enhancing feature learning network for infrared small target detection,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Geoscience and Remote Sensing</span>, vol. 62, pp. 1–11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Ciocarlan, S. Le Hegarat-Mascle, S. Lefebvre, A. Woiselle, and C. Barbanson, “A contrario paradigm for YOLO-based infrared small target detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 5630–5634, IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Desolneux, L. Moisan, and J.-M. Morel, <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">From gestalt theory to image analysis: a probabilistic approach</span>, vol. 34.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Ehret, A. Davy, M. Delbracio, and J.-M. Morel, “How to reduce anomaly detection in images to anomaly detection in noise,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Image Processing On Line</span>, vol. 9, pp. 391–412, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
V. Vidal, M. Limbert, T. Ceillier, and L. Moisan, “Aggregated primary detectors for generic change detection in satellite images,” in <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</span>, pp. 59–62, IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Desolneux, L. Moisan, and J.-M. Morel, “A grouping principle and four applications,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, vol. 25, no. 4, pp. 508–513, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. L. Hégarat-Mascle, E. Aldea, and J. Vandoni, “Efficient evaluation of the number of false alarm criterion,” <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">EURASIP J. Image Video Process.</span>, vol. 2019, p. 35, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representation learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 9729–9738, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, “Emerging properties in self-supervised vision transformers,” in <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pp. 9650–9660, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell, “Region similarity representation learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp. 10539–10548, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li, “Dense contrastive learning for self-supervised visual pre-training,” in <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 3024–3033, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Z. Xie, Z. Geng, J. Hu, Z. Zhang, H. Hu, and Y. Cao, “Revealing the dark secrets of masked image modeling,” in <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 14475–14485, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders are scalable vision learners,” in <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 16000–16009, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, “Simmim: A simple framework for masked image modeling,” in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 9653–9663, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">et al.</span>, “An image is worth 16x16 words: Transformers for image recognition at scale,” in <span class="ltx_text ltx_font_italic" id="bib.bib22.2.2">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “Eca-net: Efficient channel attention for deep convolutional neural networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pp. 11531–11539, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">et al.</span>, “Bootstrap your own latent-a new approach to self-supervised learning,” <span class="ltx_text ltx_font_italic" id="bib.bib24.2.2">Advances in neural information processing systems</span>, vol. 33, pp. 21271–21284, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. Tian, Y. Jiang, C. Lin, L. Wang, Z. Yuan, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">et al.</span>, “Designing bert for convolutional networks: Sparse and hierarchical masked modeling,” in <span class="ltx_text ltx_font_italic" id="bib.bib25.2.2">The Eleventh International Conference on Learning Representations</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet for the 2020s,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 11976–11986, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Vasconcelos, V. Birodkar, and V. Dumoulin, “Proper reuse of image classification features improves object detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp. 13628–13637, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Dai, Y. Wu, F. Zhou, and K. Barnard, “Asymmetric contextual modulation for infrared small target detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span>, pp. 950–959, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Zhang, R. Zhang, Y. Yang, H. Bai, J. Zhang, and J. Guo, “Isnet: Shape matters for infrared small target detection,” in <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp. 877–886, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
W. Zhang, M. Cong, and L. Wang, “Algorithms for optical weak small targets detection and tracking: review,” in <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003</span>, vol. 1, pp. 643–647 Vol.1, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, X. Xie, and J. Han, “Towards large-scale small object detection: Survey and benchmarks,” <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1412.6980</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Park, W. Kim, B. Heo, T. Kim, and S. Yun, “What do self-supervised vision transformers learn?,” in <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">The Eleventh International Conference on Learning Representations</span>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  9 21:08:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
