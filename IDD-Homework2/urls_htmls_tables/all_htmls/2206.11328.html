<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.11328] Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks</title><meta property="og:description" content="Radio access network (RAN) slicing is a key element in enabling current 5G networks and next-generation networks to meet the requirements of different services in various verticals. However, the heterogeneous nature of‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.11328">

<!--Generated on Tue Feb 27 03:43:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Deep Reinforcement Learning,  Open RAN,  RAN Slicing,  Federated Learning,  Reinforcement Learning,  B5G,  6G.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amine Abouaomar,¬† Afaf Taik,¬†, Abderrahime Filali,¬† and Soumaya Cherkaoui
</span><span class="ltx_author_notes">Amine Abouaomar, Abderrahime Filali and Soumaya Cherkaoui are with the Department of Computer and Software Engineering, Polytechnique Montreal (e-mails: amine.abouaomar@polymtl.ca abderrahime.filali@polymtl.ca, soumaya.cherkaoui@polymtl.ca)Afaf Ta√Øk is with Universit√© de Sherbrooke (e-mails: afaf.taik@usherbrooke.ca)</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Radio access network (RAN) slicing is a key element in enabling current 5G networks and next-generation networks to meet the requirements of different services in various verticals. However, the heterogeneous nature of these services‚Äô requirements, along with the limited RAN resources, makes the RAN slicing very complex. Indeed, the challenge that mobile virtual network operators (MVNOs) face is to rapidly adapt their RAN slicing strategies to the frequent changes of the environment constraints and service requirements. Machine learning techniques, such as deep reinforcement learning (DRL), are increasingly considered a key enabler for automating the management and orchestration of RAN slicing operations. Nerveless, the ability to generalize DRL models to multiple RAN slicing environments may be limited, due to their strong dependence on the environment data on which they are trained. Federated learning enables MVNOs to leverage more diverse training inputs for DRL without the high cost of collecting this data from different RANs. In this paper, we propose a federated deep reinforcement learning approach for RAN slicing. In this approach, MVNOs collaborate to improve the performance of their DRL-based RAN slicing models. Each MVNO trains a DRL model and sends it for aggregation. The aggregated model is then sent back to each MVNO for immediate use and further training. The simulation results show the effectiveness of the proposed DRL approach.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Deep Reinforcement Learning, Open RAN, RAN Slicing, Federated Learning, Reinforcement Learning, B5G, 6G.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Beyond 5G and 6G are expected to support the explosive growth of data traffic generated by a huge number of connected devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. These devices enable advanced services to multiple vertical industries with diversified quality of service (QoS) requirements. Therefore, the traffic volume in these next-generation networks will grow exponentially carrying a huge amount of data. This will increase the demand for bandwidth to provide the connectivity required to transfer this data efficiently and rapidly. Accordingly, these next-generation networks will entail more stringent requirements than current 5G networks. To support all these services, mobile network operators (MNOs) are required to provide an adequate network infrastructure. To achieve this in a high-performance and cost-effective manner, MNOs rely on several cutting-edge technologies, such as network slicing (NS). NS allows the design of several logically independent networks, so-called network slices, which operate on a common physical infrastructure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In particular, radio access network (RAN) slicing consists in partitioning the RAN resources to create various RAN slices, each tailored and dedicated to meet the requirements of a specific service. An MNO owns the physical RAN resources, including radio resources, and leases them to mobile virtual network operators (MVNOs) to deploy RAN slices based on their offered services.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The allocation of radio resources to users is an extremely intricate operation for MVNOs. This is mainly due to the radio resources‚Äô scarcity and the heterogeneous QoS requirements of their services <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. To overcome these challenges, several efforts have been devoted to formulating such a RAN slicing problem using optimization techniques and solving it with heuristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. However, these approaches may not align with zero-touch network perspectives since they may be unable to quickly adapt to dynamic and permanent changes in the RAN environment. Machine learning (ML) techniques, specifically deep reinforcement learning (DRL) algorithms, can address this issue by bringing more automation to the management of RAN slicing operations. The stochastic RAN environment factors, such as the density of users, user requirements, and wireless channel transmission conditions have a major impact on the accuracy of the DRL models, which decreases the performance of radio resource allocation to the users. Indeed, when an MVNO builds its DRL resource allocation model using training datasets related only to its users‚Äô behavior and its surrounding environment, the accuracy of the DRL model may be limited. To benefit from a diversified dataset, MVNOs could collaborate by sharing their data with each other to provide a diverse and high-quality dataset. However, MVNOs are often competing entities and are unlikely to be willing to share their data for privacy issues. Federated learning (FL) has emerged as a promising alternative to data sharing, as it promotes privacy-preserving collaborative training through sharing model updates instead <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">FL is a cooperative learning approach in which multiple collaborators, MVNOs in our case, train an ML model using their private datasets, and then send their trained models to an aggregation entity to build a global model. The aggregation entity returns this model to all collaborators for utilization or further training. Thus, FL enables MVNOs to build a robust ML resource allocation model while maintaining data privacy since only trained models are shared. The shared experience will enable the RAN-slicing model to learn from varying scenarios, which makes it more adaptive to the environment changes. In fact, due to the unbalanced and non-independent, and identical distributions (non-i.i.d) of the users across MVNOs, alongside their varying numbers and requirements, FL becomes an attractive solution to build robust models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To implement a FL-based RAN slicing mechanism with a high level of automation and flexibility, Open RAN (for which the O-RAN alliance has built the O-RAN architecture as the foundation for virtualized RAN on open hardware and cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>) emerges as a revolutionary solution. O-RAN relies on openness and intelligence to provide flexible and cost-effective RAN services. Openness refers to the disaggregation of the RAN into several independent functions. Then, the use of open, standardized, and interoperable interfaces to ensure communication between these RAN functions. Thus, the need to use proprietary equipment will be reduced, which leads to more competition among vendors and further innovation. Also, O-RAN promotes the use of more intelligence and automation in controlling the RAN operation through RAN intelligent controllers (RICs). The RICs leverage the capabilities of ML to manage the RAN resources and components such that it becomes a zero-touch network system.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The motivation behind this work is to build reliable DRL-based radio resource allocation models for MVNOs by leveraging their heterogeneous and diverse datasets while preserving data privacy and security. Accordingly, we propose an FL-based cooperative radio resource allocation mechanism for MVNOs. In this mechanism, each MVNO trains an DRL radio resource allocation model according to its users‚Äô requirements and sends the trained models to the RIC for aggregation. Then, the RIC sends back the global DRL model to each MVNO to update its local DRL models. The proposed RAN slicing approach is consistent with the zero-touch network framework since it aims to autonomously manage the allocation of radio resources to users by adapting the allocation policy to the constraints of the environment and the requirements of users. Indeed, the use of DRL enables the implementation of a closed-loop control process of the RAN slicing operations. Furthermore, our proposed RAN slicing mechanism is implemented in an O-RAN architecture that favors using more intelligence and automation in monitoring RAN operations.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The main contributions of this paper are summarized as follows: we

<span id="S1.I1" class="ltx_inline-enumerate">
<span id="S1.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span id="S1.I1.i1.1" class="ltx_text">discuss DRL-based RAN slicing efforts and associated challenges in a multi-MVNOs environment,
</span></span>
<span id="S1.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span id="S1.I1.i2.1" class="ltx_text">design a federated DRL (FDRL) mechanism on an O-RAN architecture to improve the radio resource allocation operation of MVNOs, and
</span></span>
<span id="S1.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">(iii)</span> <span id="S1.I1.i3.1" class="ltx_text">illustrate, through extensive simulations, that the proposed RAN slicing mechanism enables a better allocation of the needed radio resources to satisfy users‚Äô QoS requirements in terms of delay and data rate.
</span></span>
</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps"> DRL AS A RAN SLICING ENABLER.</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">DRL-based RAN Slicing efforts</span>
</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2206.11328/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">An overview of the system model.</span></figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">DRL has been widely used as an efficient tool to perform RAN slicing. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the DRL is used to determine the configuration parameters of the RAN slices. It defines for each RAN slice the transmit power, the bandwidth, the shared bandwidth between the slices and the numerology. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> designs a DRL-based RAN slicing mechanism used by base stations to allocate radio resources to their associated users. When the resources of a base station are not sufficient, this mechanism consists in requesting additional resources from other base stations to satisfy the QoS requirements of its users.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To improve the RAN slicing operation in a multi-MVNOs scenario, various DLR-based efforts have been presented recently. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposes a two-level slicing resource allocation scheme using DRL. Based on the resources received from the MNO, each MVNO allocates to its users the required radio resource to satisfy their QoS. The MVNO uses a DRL algorithm to perform resource allocation to its users in order to maximize their satisfaction rate, which is defined by the number of packets successfully received from the network. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> presents a DRL-based resource allocation algorithm to maximize the utility of MVNOs while meeting QoS requirements of RAN slices. The utility of an MVNO is defined as the gain from allocating the resource to the slice minus the cost of leasing the resource from the MNO.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In all these approaches DRL has been used as an enabler for orchestrating RAN slicing. However, the ability to generalize DRL models to multiple RAN slicing environments may be limited, due to their strong dependence on the environment data on which they are trained. The proposed FDRL-based RAN slicing mechanism overcomes the robustness issue that can arise in approaches based only on DRL. FDRL enables MVNOs to leverage more diverse training inputs for DRL while avoiding to share the data of their RAN environments.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">RAN Slicing Challenges</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Although many approaches have shown that DRL enables efficient RAN slicing, several challenges remain open and deserve further investigation.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Robustness</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The main drawback of policy-learning methods, such as DRL, is their dependence on data. Training robust DRL models requires huge amounts of data. Note that the RAN environment has a high uncertainty related to the various user requirements, dynamic wireless transmission conditions, and traffic permanent fluctuations. Accordingly, the training in such environments can suffer from the scarcity of data. Thus, there is a need for providing heterogeneous scenarios to build robust DRL-based RAN slicing models.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>SLA satisfaction</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">In RAN slicing, SLA presents the minimum QoS requirements that should be guaranteed on a provided service. In beyond 5G/6G networks, RAN slicing is expected to deal with more fine-grained SLA. In addition, resources in the RAN environment are limited, leading to resource scarcity issues. Therefore, network operators need to design DRL algorithms that optimize resource allocation to meet SLAs and thus provide maximum service.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>RAN slice isolation</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">: DRL-based RAN slicing mechanisms can ensure flexible management of RAN slices. They can rapidly instantiate or reconfigure RAN slices to meet the requirements of users. However, a fast configuration of RAN slices can be achieved at the expense of their isolation. Therefore, DRL mechanisms must consider the tradeoff between fast management and accurate isolation of RAN slices.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS4.4.1.1" class="ltx_text">II-B</span>4 </span>Privacy and competition</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">While encouraging MVNOs to collaborate to build robust slicing models is desirable, several privacy issues arise. Indeed, MVNOs have no incentives to share data related to the QoS of their users with competitors. Instead, new techniques that promote both collaboration and privacy are needed to guarantee high SLA satisfaction.</p>
</div>
<div id="S2.SS2.SSS4.p2" class="ltx_para">
<p id="S2.SS2.SSS4.p2.1" class="ltx_p">In this work, we design a FDRL mechanism to allocate radio resources in a multi-MVNOs environment. We use FL to build a robust and private DRL-based RAN slicing mechanism that satisfies MVNOs‚Äô users in terms of SLA. The robustness of the proposed mechanism is achieved since the MVNOs collaborate the train a global DRL model. The MVNOs share their models locally trained based on their scenarios to build a robust global RAN slicing model.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">FDRL-Based Cooperative RAN Slicing Between MVNOs</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">System Model</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We consider a RIC-enabled RAN architecture with a single base station, <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> gNodeB, owned by an MNO. The BS is operating on a total bandwidth B. The MNO is responsible to serve a set of MVNOs by renting to each of them a fraction of the total bandwidth. Each MVNO has a set of users that upload their packets to the network. We consider two types of users, namely enhanced mobile broadband (eMBB) and ultra-reliable low-latency communication (URLLC).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We assume that bandwidth allocation to MVNOs has already been performed by the MNO. An MVNO allocates to each of its users a fraction of the bandwidth pre-allocated by the MNO to satisfy its QoS requirements when uploading packets. Indeed, the goal for each MVNO is to efficiently allocate the bandwidth to ensure a high data rate for eMBB users and a low transmission delay for URLLC users.
In this paper, we propose a FDRL-based RAN slicing mechanism for MVNOs, which collaborate to improve the performance of allocating their communication resources to their users.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Deep Deterministic Policy Gradient (DDPG)</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The RAN environment is highly dynamic, where traffic fluctuations are permanent and wireless transmission conditions change frequently. Thus, it is challenging for an MVNO to perform the radio resource slicing operation in a such an environment. To overcome this challenge, ML techniques such as DRL can be leveraged.
To solve the RAN slicing problem of an MVNO based on the DRL algorithm, we need to model this problem as a Markov decision process (MDP).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">MDP Formulation:</span>
For each MVNO, the bandwidth allocation problem is modeled as a single-agent MDP as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">The State Space:</span> The state observed by the agent includes the type of its users and their channel gains. The type of users, <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> eMBB and URLLC, is necessary to define the requirements of each user. The channel gain between a user and the gNodeB is calculated using the state channel information collected by the gNodeB. Since each MVNO might serve a different number of users each time, it raises a problem related to the size of the input and output of the trained model. To overcome this problem, we set the input size and output size according to the maximum number of users that an MVNO can serve at one time. In case the observed number of users is lower than this maximum user threshold, we use zero-padding. This allows us to adapt the varying number of users of each MVNO and to unify the trained model.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">The Action Space:</span> An agent has to decide which bandwidth fraction should be allocated to each of its users. The action chosen for a user is a real value between 0 and a threshold that defines the maximum bandwidth faction that a user can have.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">The Reward Function:</span> In this work, the main objective is the ensure a high data rate for the eMBB users and a low transmission delay for the URLLC users. The reward depends on the QoS provided to each of its users in terms of data rate and delay. Accordingly, the reward obtained by an agent is the total sum of the data rates achieved by the eMBB users and the inverse of the transmission delays experienced by the URLLC users. If the achieved data rate of an eMBB user is less than a minimum threshold, or if the latency experienced by an URLLC user is greater than a maximum threshold, this action will be punished by -0.1. Theses punishments will be added to the reward obtained by the agent. Moreover, if the total sum of the bandwidth fractions is greater than 1, the global action is considered as invalid, and it will be penalized by -0.05. Furthermore, in order to avoid the case where a fraction of the bandwidth is allocated to a user that does not exist, we associate this action to a punishment equal to a negative value that we add to the reward (-0.1).</p>
</div>
</li>
</ul>
<p id="S3.SS2.p2.2" class="ltx_p">DRL allows to create agents that learn from high-dimensional states, where the policy is represented as a deep neural network. DRL was first introduced through Deep-Q Networks (DQN), and was fast adopted by the research community to solve many practical decision making problems. Nonetheless, DQN is off-policy and may not perform well in environments that have high uncertainties such as wireless networks. While value-based RL algorithms like Q-learning optimize value function first then derive optimal policies, the policy-based methods directly optimize an objective function based on the rewards, which makes them suitable for large or infinite action spaces. Yet, policy-based RL might have noisy and unstable gradients. As a result, we propose to use an actor-critic based algorithm. In fact, actor-critic approaches combine strong points from both value-based and policy-based RL algorithms. Furthermore, since the fraction values are continuous, we use the deep deterministic policy gradient (DDPG) algorithm, which concurrently learns a Q-function and a policy and performs actions from a continuous space.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">DDPG algorithm:</span> It uses 4 neural networks: the actor network, the critic network, the actor target network, and the critic target network. For a given observed environment state, the actor chooses an action, and the critic uses a state-action Q-function to evaluate it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. To reduce the correlation between the training samples, DDPG uses the experience replay memory, which stabilizes its behavior. An experience is defined by the current state of the environment, the chosen action, the reward obtained, and the next state of the environment. The agent stores its experiences and then samples random mini-batches from them to train its DQN networks. The exploration policy of DDPG is performed by adding noise to the actions during the training process. The added noise enables the DDPG agent to efficiently explore its environment. We used the Ornstein‚ÄìUhlenbeck (OU) process to generate the noise values.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">FDRL-based RAN Slicing mechanism</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The disparity of users distributions across different geographical areas (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> suburbs and city center), for instance, makes using the same model across all the covered areas inadequate. Moreover, the amount of data collected by each MVNO in certain areas (<span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> rural areas) is fairly limited. Since it is beneficial for each MVNO to enhance its bandwidth allocation model, FL has created the opportunity for multiple MVNOs to leverage data from a broader set of users while avoiding sharing it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In this setting, each MVNO trains the bandwidth allocation model using the interaction with its users. Then, the MVNO uploads its locally trained model for the current round to the non-RT RIC. The non-RT RIC performs the aggregation of the models using weighted sum based on each MVNO‚Äôs number of users.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The FDRL-based mechanism consists of several communication rounds. Each communication round consists of several local episodes of training, after which the MVNOs update their local models and send the updates to the non-RT RIC.
In each step of the episode, each MVNO observes their local environment, selects an action and receives the corresponding reward. Each transition between two states, <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">i.e.,</span> experience, is stored in the replay buffer. When a predefined number of experiences is stored, MVNOs sample random mini-batches from the replay buffer, and update the different DDPG networks. The actor network is updated through policy gradient and the critic network is updated through loss function minimization. Subsequently, the target networks are updated as well. At the end of the predetermined number of local episodes, each MVNO sends its local updated model to the non-RT RIC for aggregation. The latter collects all the local updates from the MVNOs and generate the global model using a weighted sum.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The O-RAN Alliance Working Group 2 defines in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> three scenarios for the deployment of ML model learning and ML model inference. In this work, we opted for the scenario where the offline training process is hosted in the non-RT RIC and the online inference process is hosted in the near-RT RIC. This choice is related to the proposed FDRL-based RAN slicing mechanism. Indeed, we used a DDPG algorithm, which requires a huge amount of data to perform offline training of RAN slicing models. This offline training requires a resourceful unit with high computing and storage capabilities. Therefore, the non-RT RIC seems better suited to host the offline training of RAN slicing models. The near-RT RIC was chosen as the model inference host since 1) the radio resource allocation operation to users should be performed in a short time scale, and 2) the data needed at the execution time of the RAN slicing operation is available through the E2 interface, which connects the near-RT RIC to the RAN nodes.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Numerical results</span>
</h2>

<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x2.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="346" height="245" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x3.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="346" height="257" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Training performance.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Experiment parameters and scenarios</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We consider a RIC-enabled RAN architecture with a single base station. The simulated users are randomly scattered in an area of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="500m\times 500m" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.2.2.1" xref="S4.SS1.p1.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S4.SS1.p1.1.m1.1.1.2.2.3" xref="S4.SS1.p1.1.m1.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.2.1" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">√ó</mo><mn id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">500</mn></mrow><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><times id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2.1"></times><apply id="S4.SS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2"><times id="S4.SS1.p1.1.m1.1.1.2.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2.2">500</cn><ci id="S4.SS1.p1.1.m1.1.1.2.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2.3">ùëö</ci></apply><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.3">500</cn></apply><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">500m\times 500m</annotation></semantics></math> around the BS, and served by 3 MVNOs.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The MVNOs collectively train a DDPG model. The four networks of the model have two hidden fully connected layers with 400 and 300 neurons, respectively. We consider that each MVNO has a maximum number of users equal to 5. Since the maximum number of users is 5, the size of the input layer is 10 and the output layer is 5.
We used the Rectified linear unit as an activation function since it avoids vanishing gradients in backpropagation, especially since the action space is limited to small values. This is due to one of the conditions stating that the allocated fraction should be less than <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="float" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">0.3</annotation></semantics></math> to avoid squandering the bandwidth over a small handful of the users.
We used the Adam optimizer with two different learning rates for the actor and critic.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We implemented the proposed architecture using the Python programming language. We used the PyTorch framework to implement the DDPG algorithm. The simulations were conducted on a laptop with a 2.6 GHz Intel i7 Processor, 16GB of RAM, and NVIDIA GeForce RTX 2070 graphic card.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.3" class="ltx_p">For the FDRL mechanism, the training takes place over a total of 5 communication rounds. In each round, the model is trained by each MVNO for 500 episodes before sending the model to the RIC for aggregation. Each episode is composed of 50 steps, where the channel gains values are reset in each step, and the users‚Äô locations are reset every 25 episodes. In order to generate non-i.i.d distributions for user requirements, we set different probabilities of URLLC and eMBB users for each MVNO. The set of probabilities of URLLC users are <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">25</mn><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">25\%</annotation></semantics></math>, <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mn id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">50</mn><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">50\%</annotation></semantics></math>, and <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mn id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">75</mn><mo id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">75\%</annotation></semantics></math> for MVNOs 1, 2, and 3, respectively.
To further test our proposed solution, we generated an unequal distribution of users. Specifically, we considered a case where MVNOs 1, 2, and 3 have 5, 4, and 3 users, respectively. In this case, the fraction of the bandwidth allocated to each MVNO is proportional to its number of users.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">In the following, we study two scenarios: non-i.i.d with equal number of users, and non-i.i.d with unequal numbers of users. To evaluate the performance of FDRL, we compare the global model, <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_italic">i.e.,</span> our model trained with FDRL, with the local models that were trained by each MVNO without collaboration. We first show the evolution of the training rewards, then we stress test the resulting models under different distribution shift scenarios to evaluate their robustness.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">FDRL training results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The first considered scenario is non-i.i.d with equal number of users. The total number of users is 15, with 5 users being served by each MVNO. Fig. <a href="#S4.F2.sf1" title="In Figure 2 ‚Ä£ IV Numerical results ‚Ä£ Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> shows the evolution of the average reward of the local models and the global model across 5 experiments. While the global model improves through the shared experience, even surpassing the local models average in later rounds, the local models have degrading performances throughout the training likely due to overfitting. In fact, since in later rounds, the exploration induced by the OU noise is reduced, the local models allocate less bandwidth to the users, which degrades the values of the rewards. In contrast, the global model learns slower to generalize, but achieves more robust training overall by leveraging the shared experience.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x4.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">URLLC (75%,25%,50%)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x5.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">eMBB (75%,25%,50%)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x6.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="348" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">URLLC (50%, 75%,25%)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x7.png" id="S4.F3.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F3.sf4.3.2" class="ltx_text" style="font-size:90%;">eMBB (50%, 75%,25%)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Evaluation under varying user distributions.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The second considered scenario is non-i.i.d with unequal number of users. The total number of users is 12, where 5, 4, and 3 users are served by the first, second, and third MVNO, respectively. Fig. <a href="#S4.F2.sf2" title="In Figure 2 ‚Ä£ IV Numerical results ‚Ä£ Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> shows the evolution of the average reward of the local models and the global model across 5 experiments. Our first observation is that the cumulative rewards for both models are less than what was achieved in the case of equal numbers of users. This is mainly due to the punishment related to allocating bandwidth to non-existent users. Furthermore, similarly to the previous experiments, the global model improves slowly throughout the communication rounds, while the local models do not improve.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">FDRL performance evaluation</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To evaluate the performance of the proposed FDRL mechanism, we compared the number of invalid actions of the global model against each local model. Note that an action is considered invalid if it does not meet the user‚Äôs SLA requirements. We use the resulting local models and global models, and test them in different environments by varying the underlying user types‚Äô distributions of each MVNO, then varying the number of users served by each MVNO.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.4.1.1" class="ltx_text">IV-C</span>1 </span>Varying user types‚Äô distributions</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.3" class="ltx_p">The first considered scenario is non-i.i.d with equal number of users. The models are trained with a total of 15 users, where 5 users are served by each MVNO. To evaluate the robustness of the models in the case of changing users‚Äô requirements, we varied the underlying distributions of the users for each MVNO.
The probabilities of URLLC users in the trained models are <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">25</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">25\%</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mrow id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p1.2.m2.1.1.2" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.cmml">50</mn><mo id="S4.SS3.SSS1.p1.2.m2.1.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><apply id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">50\%</annotation></semantics></math>, and <math id="S4.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><mrow id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml"><mn id="S4.SS3.SSS1.p1.3.m3.1.1.2" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.cmml">75</mn><mo id="S4.SS3.SSS1.p1.3.m3.1.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><apply id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">75\%</annotation></semantics></math>, for the first, second, and third MVNO, respectively.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.6" class="ltx_p">In a first experiment, we changed the URLLC probabilities in test phase to <math id="S4.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mrow id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p2.1.m1.1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml">75</mn><mo id="S4.SS3.SSS1.p2.1.m1.1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">75\%</annotation></semantics></math>, <math id="S4.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS3.SSS1.p2.2.m2.1a"><mrow id="S4.SS3.SSS1.p2.2.m2.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS1.p2.2.m2.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml">25</mn><mo id="S4.SS3.SSS1.p2.2.m2.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.2.m2.1b"><apply id="S4.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.2.m2.1c">25\%</annotation></semantics></math> ,<math id="S4.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS3.SSS1.p2.3.m3.1a"><mrow id="S4.SS3.SSS1.p2.3.m3.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.cmml"><mn id="S4.SS3.SSS1.p2.3.m3.1.1.2" xref="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml">50</mn><mo id="S4.SS3.SSS1.p2.3.m3.1.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.3.m3.1b"><apply id="S4.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.3.m3.1c">50\%</annotation></semantics></math> for the first, second and third MVNOs, respectively. In a second experiment, we changed these probabilities to <math id="S4.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS3.SSS1.p2.4.m4.1a"><mrow id="S4.SS3.SSS1.p2.4.m4.1.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.cmml"><mn id="S4.SS3.SSS1.p2.4.m4.1.1.2" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.cmml">50</mn><mo id="S4.SS3.SSS1.p2.4.m4.1.1.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.4.m4.1b"><apply id="S4.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.4.m4.1.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.4.m4.1.1.2.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.4.m4.1c">50\%</annotation></semantics></math>, <math id="S4.SS3.SSS1.p2.5.m5.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S4.SS3.SSS1.p2.5.m5.1a"><mrow id="S4.SS3.SSS1.p2.5.m5.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.cmml"><mn id="S4.SS3.SSS1.p2.5.m5.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml">75</mn><mo id="S4.SS3.SSS1.p2.5.m5.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.5.m5.1b"><apply id="S4.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.5.m5.1c">75\%</annotation></semantics></math>, <math id="S4.SS3.SSS1.p2.6.m6.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS3.SSS1.p2.6.m6.1a"><mrow id="S4.SS3.SSS1.p2.6.m6.1.1" xref="S4.SS3.SSS1.p2.6.m6.1.1.cmml"><mn id="S4.SS3.SSS1.p2.6.m6.1.1.2" xref="S4.SS3.SSS1.p2.6.m6.1.1.2.cmml">25</mn><mo id="S4.SS3.SSS1.p2.6.m6.1.1.1" xref="S4.SS3.SSS1.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.6.m6.1b"><apply id="S4.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.6.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p2.6.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.6.m6.1c">25\%</annotation></semantics></math>.
Fig. <a href="#S4.F3" title="Figure 3 ‚Ä£ IV-B FDRL training results ‚Ä£ IV Numerical results ‚Ä£ Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the cumulative number of times where users‚Äô SLA requirements were not satisfied by the local models of the MVNOs and by the global model, while observing the same environments for a total of 20000 observations. We noticed that, overall, the global model‚Äôs actions are less prone to violate the SLA requirements for eMBB and URLLC users compared to the individually trained models. Additionally, as we attributed larger weights to the URLLC users, the global model prioritizes this type of users and is less likely to violate their required delay.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.4.1.1" class="ltx_text">IV-C</span>2 </span>Varying number of users</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">The second considered scenario is non-i.i.d with unequal number of users. The models are first trained with a total number of users of 12, where 5, 4, and 3 users are served by the first, second, and third MVNO, respectively. We seek to evaluate the robustness of the models in the case of changing number of users.
First, we changed the number of users in test time to 4, 3 ,5 for the first, second and third MVNOs, respectively. In a second experiment, we changed these numbers to 3, 5, 4.
Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ IV-C2 Varying number of users ‚Ä£ IV-C FDRL performance evaluation ‚Ä£ IV Numerical results ‚Ä£ Federated Deep Reinforcement Learning for Open RAN Slicing in 6G Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the number of times where users‚Äô SLA were not satisfied by the local models of the MVNOs and the global model, while observing the same environments for a total of 20000 observations.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Similarly to the previous experiments, the global model‚Äôs actions are less likely to violate the SLA requirements for eMBB and URLLC users compared to the individually trained models. Moreover, the third MVNO, being trained with mostly URLLC users, it has high satisfaction rate for this type, but it performs poorly for the eMBB users. In general, the enhancement in the QoS for both types of users using the global model makes it worthwhile for the MVNOs to collaborate.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x8.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">URLLC (4,3,5)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x9.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">eMBB (4,3,5)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x10.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">URLLC (3,5,4)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2206.11328/assets/x11.png" id="S4.F4.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">eMBB (3,5,4)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;"> Evaluation under different number of users.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this article, we proposed a FDRL RAN slicing mechanism in a multi-MVNOs environment to allocate radio resources to URLLC and eMBB users. We leverage FL to design a RAN slicing mechanism in which MVNOs collaborate to improve their policies for allocating radio resources to their users. We formulated the resource allocation problem for users of each MVNO as a single-agent MDP and used the DDPG algorithm to solve it. Then, FL is used to improve MVNOs‚Äô radio resource allocation models. The proposed FDRL mechanism has proven to be robust in meeting the diverse requirements of URLLC and eMMB users in different scenarios and more resilient to the environment changes.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Although the proposed FDRL-based RAN slicing mechanism allows a better robustness of radio resource allocation models, different interesting research directions deserve to be investigated. It is worth studying how to detect and track any deterioration in the performance of DRL models to proactively trigger the retraining process. Another interesting issue to investigate is how to further enhance privacy beyond what federated learning inherently promotes. Indeed, while FL guarantees data privacy to a certain extent, it is still possible for adversaries to infer data from the model updates. To mitigate this, integrating other privacy preserving techniques such as differential privacy could be further explored in the case of MNVOs cooperating in FDRL schemes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
W.¬†Jiang, B.¬†Han, M.¬†A. Habibi, and H.¬†D. Schotten, ‚ÄúThe Road Towards 6G: A
Comprehensive Survey,‚Äù <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Open Journal of the Communications
Society</span>, vol.¬†2, pp.¬†334‚Äì366, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.¬†Filali and other, ‚ÄúMulti-Access Edge Computing: A Survey,‚Äù <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE
Access</span>, vol.¬†8, pp.¬†197017‚Äì197046, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H.¬†Song, L.¬†Liu, J.¬†Ashdown, and Y.¬†Yi, ‚ÄúA Deep Reinforcement Learning
Framework for Spectrum Management in Dynamic Spectrum Access,‚Äù <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE
Internet Things J.</span>, vol.¬†8, no.¬†14, pp.¬†11208‚Äì11218, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M.¬†K. Motalleb, V.¬†Shah-Mansouri, S.¬†Parsaeefard, and O.¬†L.¬†A. L√≥pez,
‚ÄúResource Allocation in an Open RAN System using Network Slicing,‚Äù <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Netw. Service Manag.</span>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D.¬†A. Ravi, V.¬†K. Shah, C.¬†Li, Y.¬†T. Hou, and J.¬†H. Reed, ‚ÄúRAN Slicing in
Multi-MVNO Environment under Dynamic Channel Conditions,‚Äù <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE
Internet Things J.</span>, vol.¬†9, no.¬†6, pp.¬†4748‚Äì4757, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.¬†Koneƒçn·ª≥, H.¬†B. McMahan, F.¬†X. Yu, P.¬†Richt√°rik, A.¬†T. Suresh,
and D.¬†Bacon, ‚ÄúFederated learning: Strategies for improving communication
efficiency,‚Äù <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.¬†Kairouz <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">et¬†al.</span>, ‚ÄúAdvances and Open Problems in Federated
Learning,‚Äù <span id="bib.bib7.2.2" class="ltx_text ltx_font_italic">Fdn. and Trends in Machine Learning</span>, vol.¬†14, no.¬†1‚Äì2,
pp.¬†1‚Äì210, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
O.-R. Alliance, ‚ÄúO-RAN: Towards an Open and Smart RAN,‚Äù tech. rep., ,
Oct. 2018.

</span>
<span class="ltx_bibblock">White Paper.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M.¬†Setayesh, S.¬†Bahrami, and V.¬†W. Wong, ‚ÄúResource Slicing for eMBB and URLLC
Services in Radio Access Network Using Hierarchical Deep Learning,‚Äù <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Wireless Commun.</span>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.¬†Filali <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">et¬†al.</span>, ‚ÄúDynamic SDN-based Radio Access Network Slicing with
Deep Reinforcement Learning for URLLC and eMBB Services,‚Äù <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">IEEE
Trans. Netw. Sci. Eng.</span>, pp.¬†1‚Äì1, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
G.¬†Chen <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et¬†al.</span>, ‚ÄúTwo Tier Slicing Resource Allocation Algorithm Based
on Deep Reinforcement Learning and Joint Bidding in Wireless Access
Networks,‚Äù <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">Sensors</span>, vol.¬†22, no.¬†9, p.¬†3495, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Z.¬†Wang <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">et¬†al.</span>, ‚ÄúUtility Optimization for Resource Allocation in Edge
Network Slicing Using DRL,‚Äù in <span id="bib.bib12.2.2" class="ltx_text ltx_font_italic">GLOBECOM 2020-2020 IEEE Global
Communications Conference</span>, pp.¬†1‚Äì6, IEEE, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T.¬†P. Lillicrap, J.¬†J. Hunt, A.¬†Pritzel, N.¬†Heess, T.¬†Erez, Y.¬†Tassa,
D.¬†Silver, and D.¬†Wierstra, ‚ÄúContinuous Control with Deep Reinforcement
Learning,‚Äù <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1509.02971</span>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.¬†Wang, S.¬†Chen, Y.¬†Zhu, A.¬†Tang, and X.¬†Wang, ‚ÄúLinkSlice: Fine-grained
Network Slice Enforcement Based on Deep Reinforcement Learning,‚Äù <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE J. Sel. Areas Commun.</span>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O-RAN Alliance, ‚ÄúO-RAN WG 2 AI/ML Workflow Description and Requirements,‚Äù
tech. rep., , 2020.

</span>
<span class="ltx_bibblock">Tech. Spec. v1.02.02.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td"></td>
<td id="tab1.1.1.2" class="ltx_td">
<span id="tab1.1.1.2.1" class="ltx_inline-block">
<span id="tab1.1.1.2.1.1" class="ltx_p"><span id="tab1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Amine Abouaomar</span> 
[M] is currently an assistant professor of computer science at Al Akhawayn University, Morocco. He was a Post-Doctoral Research Fellow at Polytechnique Montr√©al. He received his Ph.D. in electrical engineering from Universit√© of Sherbrooke, Canada, and his Ph.D. in Computer Science from ENSIAS, Mohammed V University of Rabat, Morocco in 2021. He is an active IEEE member since 2017 and he is a technical chair program member of many international conferences. His research interest includes beyond 5G network, federated learning, and green machine learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td"></td>
<td id="tab2.1.1.2" class="ltx_td">
<span id="tab2.1.1.2.1" class="ltx_inline-block">
<span id="tab2.1.1.2.1.1" class="ltx_p"><span id="tab2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Afaf Ta√Øk</span> 
[M] received her Ph.D. degree in Electrical Engineering from Universit√© de Sherbrooke, Canada in 2022. She holds a B.Eng. degree in software engineering from ENSIAS, Rabat, Morocco (2018), and a DESS in applied sciences from the Universit√© de Sherbrooke, Canada (2018). She is the recipient of the best paper award from IEEE LCN 2021. Her research interests include edge computing and machine learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td"></td>
<td id="tab3.1.1.2" class="ltx_td">
<span id="tab3.1.1.2.1" class="ltx_inline-block">
<span id="tab3.1.1.2.1.1" class="ltx_p"><span id="tab3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Abderrahime Filali</span> 
[M] is currently working as a Post-Doctoral Research Fellow at the Department of Computer and Software Engineering at Polytechnique Montreal. He received his Ph.D. degree in electrical engineering from the Universit√© de Sherbrooke, Canada in 2021. He served as a reviewer for several international conferences and journals. His current research interests include resource management in next-generation networks.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab4" class="ltx_float biography">
<table id="tab4.1" class="ltx_tabular">
<tr id="tab4.1.1" class="ltx_tr">
<td id="tab4.1.1.1" class="ltx_td"></td>
<td id="tab4.1.1.2" class="ltx_td">
<span id="tab4.1.1.2.1" class="ltx_inline-block">
<span id="tab4.1.1.2.1.1" class="ltx_p"><span id="tab4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Soumaya Cherkaoui</span>  [SM] is a Full Professor at the Department of Computer and Software Engineering at Polytechnique Montreal. She has been on the editorial board of several IEEE journals. She authored numerous conference and journal papers. Her work was awarded with recognitions including several best paper awards at IEEE conferences. She is an IEEE ComSoc Distinguished Lecturer, a Professional Engineer in Canada, and has served as Chair of the IEEE ComSoc IoT-Ad Hoc and Sensor Networks Technical Committee.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.11327" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.11328" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.11328">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.11328" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.11329" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 03:43:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
