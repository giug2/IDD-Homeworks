<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models</title>
<!--Generated on Fri Oct 11 17:20:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Foundation models,  FM4SE,  SE4FM,  LLM-as-a-judge,  industry trends,  LLM
" lang="en" name="keywords"/>
<base href="/html/2410.09012v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S1" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S2" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background and Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S2.SS1" title="In II Background and Related Work ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">LLM-as-a-judge</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S2.SS2" title="In II Background and Related Work ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Related work</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Using an FM/LLM Jury for Labelling Blog Posts</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3.SS1" title="In III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Constructing the prompt</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3.SS2" title="In III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Merging the FM outputs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3.SS3" title="In III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Human-in-the-loop</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3.SS4" title="In III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Selected FMs</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4.SS1" title="In IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Identifying blogs related to SE and FM from technology companies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4.SS2" title="In IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Identifying the FM4SE and SE4FM area, activities and discussed tasks in the blog posts</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">RQ1: Which FM4SE activities are discussed in industry blog posts?</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.SS1" title="In V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Software development</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.SS2" title="In V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Software quality assurance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.SS3" title="In V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Software maintenance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.SS4" title="In V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Software management</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.SS5" title="In V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Comparison with Academic Research</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">RQ2: Which SE4FM activities are discussed in industry blog posts?</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS1" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Model deployment &amp; operation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS2" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">System architecture &amp; orchestration</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS3" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Data management</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS4" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Model customization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS5" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-E</span> </span><span class="ltx_text ltx_font_italic">Evaluation &amp; quality assurance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.SS6" title="In VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-F</span> </span><span class="ltx_text ltx_font_italic">Prompt construction</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S7" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Discussion of Future Research Directions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S7.SS1" title="In VII Discussion of Future Research Directions ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Research Directions for FM4SE</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S7.SS2" title="In VII Discussion of Future Research Directions ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Research Directions for SE4FM</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S8" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Threats to Validity</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S9" title="In Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IX </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Software Engineering and Foundation Models:
Insights from Industry Blogs Using a Jury of Foundation Models
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Queen’s University</span>
<br class="ltx_break"/>Kingston, Canada 
<br class="ltx_break"/>hao.li@queensu.ca
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cor-Paul Bezemer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">University of Alberta</span>
<br class="ltx_break"/>Edmonton, Canada 
<br class="ltx_break"/>bezemer@ualberta.ca
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ahmed E. Hassan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Queen’s University</span>
<br class="ltx_break"/>Kingston, Canada 
<br class="ltx_break"/>ahmed@cs.queensu.ca
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Foundation models (FMs) such as large language models (LLMs) have significantly impacted many fields, including software engineering (SE). The interaction between SE and FMs has led to the integration of FMs into SE practices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While several literature surveys exist on academic contributions to these trends, we are the first to provide a practitioner’s view. We analyze 155 FM4SE and 997 SE4FM blog posts from leading technology companies, leveraging an FM-powered surveying approach to systematically label and summarize the discussed activities and tasks.
We observed that while code generation is the most prominent FM4SE task, FMs are leveraged for many other SE activities such as code understanding, summarization, and API recommendation. The majority of blog posts on SE4FM are about model deployment &amp; operation, and system architecture &amp; orchestration. Although the emphasis is on cloud deployments, there is a growing interest in compressing FMs and deploying them on smaller devices such as edge or mobile devices.
We outline eight future research directions inspired by our gained insights, aiming to bridge the gap between academic findings and real-world applications. Our study not only enriches the body of knowledge on practical applications of FM4SE and SE4FM but also demonstrates the utility of FMs as a powerful and efficient approach in conducting literature surveys within technical and grey literature domains. Our dataset, results, code and used prompts can be found in our online replication package at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SAILResearch/fmse-blogs" title="">https://github.com/SAILResearch/fmse-blogs</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Foundation models, FM4SE, SE4FM, LLM-as-a-judge, industry trends, LLM

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the rapid advancements in machine learning (ML) have fundamentally transformed various fields, including software engineering (SE). Among these developments, foundation models (FMs) such as large language models (LLMs) have emerged as a major force, reshaping how software is developed, tested, and maintained <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite>. The interaction between SE and FMs has led to the emergence of two key trends: (1) FMs for SE (FM4SE), where FMs are leveraged to automate or enhance various SE tasks, such as code generation and testing, and (2) SE for FMs (SE4FM), where SE practices are adapted to the development and deployment of FMs.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Academic research has made significant strides in exploring these trends, but literature surveys have only focused on published, peer-reviewed literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>]</cite>, mostly leaving out the perspectives and experiences of industry practitioners. The input of practitioners, who work at the intersection of SE and FMs in real-world settings, is a crucial yet underexplored source of insights. While the research community has recognized the value of user-generated contents such as Q&amp;A websites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib2" title="">2</a>]</cite> and issue reports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib32" title="">32</a>]</cite>, less attention has been paid to grey literature such as technical blog posts from industry leaders. Tech companies publish blog posts for several reasons, including positioning themselves as innovation leaders and establishing thought leadership <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib4" title="">4</a>]</cite>. As a result, these blog posts often provide in-depth discussions on cutting-edge challenges and solutions in SE and FM integration.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of our FM/LLM Jury approach for labelling blog posts. A blog post is labelled by every FM in the jury, and the final label is selected through a majority vote (using the normalized confidence score as a tie-breaker).</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To bridge the gap between academic findings and industry practices, we analyze blog posts from leading technology companies, focusing on how practitioners discuss the challenges and approaches related to FM4SE and SE4FM. By systematically labelling and examining these blog posts, we seek to provide a clearer picture of how FMs are being integrated into the SE domain (i.e., FM4SE), and how SE principles are being applied to FMs (i.e., SE4FM) in industry. This study stands out by offering a synthesized industry voice, derived directly from real-world, practitioner-driven insights. We employ an ensemble of FMs as judges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib50" title="">50</a>]</cite> into an FM/LLM Jury <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib42" title="">42</a>]</cite> (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">1</span></a>) to assist with the labelling and synthesis of knowledge within 155 FM4SE and 997 SE4FM blog posts. Our study focuses on these research questions (RQs):</p>
</div>
<div class="ltx_para" id="S1.p4">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.1.1.1">RQ1.</span></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Which FM4SE activities are discussed in industry blog posts?</span>
Software development tasks, particularly code generation, are the most frequently discussed across FM4SE blogs. FMs are increasingly integrated as code assistants, providing developers with multifunctional tools to boost productivity. Vulnerability detection is the dominant software quality assurance task, while software maintenance activities primarily focus on refactoring and transforming existing codebases.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.1.1.1">RQ2.</span></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Which SE4FM activities are discussed in industry blog posts?</span>
The most discussed activities in SE4FM blog posts are model deployment &amp; operation, with a focus on cloud hosting and model serving &amp; scaling. Other trends include prompt chaining, workflow orchestration, and building AI agents. Data management activities focus on RAG and vector databases to support unstructured data and information retrieval. Model customization relies on fine-tuning methods such as full fine-tuning, LoRA, and RLHF.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of this paper are:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">The first study of industry blog posts on FM4SE and SE4FM to provide the practitioner’s view on these emerging and crucial topics in today’s software industry.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">A dataset of 1,152 blog posts from top technology companies related to FM4SE or SE4FM.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">A list of eight research directions that are driven by the findings of our survey on blog posts.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i4.p1">
<p class="ltx_p" id="S1.I2.i4.p1.1">A demonstration of an efficient approach that leverages a jury of FMs to assist with grey literature surveys on SE-related topics.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Paper Organization.</span> The rest of this paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S2" title="II Background and Related Work ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">II</span></a> presents background information and related work. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3" title="III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a> details the proposed FM/LLM Jury framework. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4" title="IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IV</span></a> presents our methodology. Sections <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5" title="V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">V</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6" title="VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">VI</span></a> present the findings of our research questions. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S7" title="VII Discussion of Future Research Directions ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">VII</span></a> discusses promising future research directions that follow from our survey. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S8" title="VIII Threats to Validity ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">VIII</span></a> discusses the threats to the validity our study. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S9" title="IX Conclusion ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IX</span></a> concludes this paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">LLM-as-a-judge</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Leveraging FMs/LLMs as evaluators, or LLM-as-a-judge, has emerged as a scalable alternative to traditional human evaluations to assess the quality of outputs from LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib21" title="">21</a>]</cite>. This assessment is not straightforward, as LLMs generate natural language, which needs to be compared with a ground truth semantically. LLM-as-a-judge builds on the idea that state-of-the-art models, especially those trained with Reinforcement Learning from Human Feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib36" title="">36</a>]</cite> (e.g., GPT-4) are well-aligned with human judgments, making them promising substitutes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib50" title="">50</a>]</cite>. While the use of FMs like GPT-4 as evaluators has become more common, these models often exhibit biases, such as favouring their own outputs over those from other models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib38" title="">38</a>]</cite>. To mitigate these biases, researchers have proposed the use of a panel of FM evaluators instead of relying on a single model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib42" title="">42</a>]</cite>. Instead of using a single FM/LLM to evaluate FM/LLM outputs, in this paper, we propose using a jury of FMs/LLMs to assist with the labelling and summarization of industry blog posts.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Related work</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The work that is closest related to our work consists of other literature surveys on FM4SE and SE4FM.
Recent comprehensive surveys on FM4SE have examined the rapidly growing field of FMs/LLMs applied to SE activities and tasks. Hou et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite> conducted a systematic survey of 395 studies covering the application of LLMs to 84 specific SE tasks across 6 SE activities. In addition, Wang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib44" title="">44</a>]</cite> surveyed 102 studies that have used LLMs for software testing. These applications have shown promise, but several challenges remain. For example, Fan et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib12" title="">12</a>]</cite> emphasized technical challenges like hallucinations when applying LLMs for SE, and highlighted the importance of hybrid techniques that combine traditional SE with LLMs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Other surveys (on SE4FM) focus on how established SE practices can be adapted to support building, testing, deploying and maintaining FMs. Chang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib5" title="">5</a>]</cite> reviewed evaluation methods and benchmarks for LLMs in different areas such as education and social sciences, highlighting the importance of robust benchmarks to assess the performance of LLMs. Most prior surveys on SE for models have focused on SE4ML rather than SE4FM (i.e., SE for machine learning models that are not foundation models). Martínez-Fernández et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>]</cite> reviewed 248 studies and classified them based on the Software Engineering Body of Knowledge (SWEBOK), identifying gaps in areas like maintenance and data handling. Villamizar et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib43" title="">43</a>]</cite> discussed gaps in requirements engineering for ML, while Masuda et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib29" title="">29</a>]</cite> surveyed quality assurance approaches, highlighting the need for specialized testing techniques in verifying an ML system’s output.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">All prior surveys on FM4SE and SE4FM focused on academic efforts. Our work is the first to provide an overview of FM4SE and SE4FM activities in top technology companies.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="366" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our methodology.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Using an FM/LLM Jury for Labelling Blog Posts</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Labelling blog posts using a single frontier FM (e.g., GPT-4o) in the LLM-as-a-judge approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib50" title="">50</a>]</cite> can be both expensive and potentially biased. To address these limitations, we propose FM/LLM Jury, a methodology that leverages multiple FMs to collaboratively label blog posts. In this framework, each model provides a label along with a confidence score, and these outputs are merged using a majority vote to determine a final label. This framework is inspired by Verga et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Constructing the prompt</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The prompt construction process is iterative and consists of the following key steps:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Step 1 – Create the golden dataset.</span>
To evaluate the performance of the prompts that we send to the FM and the quality of our FM/LLM Jury, we begin by constructing a golden dataset. We randomly sample a subset of blog posts and manually label them to serve as ground truth.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Step 2 – Design the prompt.</span>
We design prompts following best practices in prompt engineering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib35" title="">35</a>]</cite> and techniques outlined by Liu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib26" title="">26</a>]</cite>. The prompt should instruct the FMs on how to label the blog posts, specifying the labelling criteria and providing the necessary context. To improve labelling accuracy, we incorporate advanced techniques such as Chain-of-Thought prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib46" title="">46</a>]</cite> and few-shot in-context learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib3" title="">3</a>]</cite>. We used a predefined set of labels to ensure a common vocabulary for the classification. Because FMs generate natural language text, a common vocabulary is necessary to (1) group many different blog posts that may use different terms to describe the same aspects and (2) facilitate a comparison with prior work which also uses that vocabulary.
We also ask FMs to provide new labels if the predefined ones do not fit the content. Our replication package <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib23" title="">23</a>]</cite> includes all used prompts. We encourage researchers to refine and rerun the process with new blog posts to ensure relevance in this rapidly moving field.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Step 3 – Run the prompt on the golden dataset.</span>
Each FM in the jury is prompted to label the blog posts in the golden dataset. For each blog post, the FM outputs both a label and an associated confidence score. As FMs may exhibit overconfidence, directly using the raw confidence scores is likely to introduce bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib25" title="">25</a>]</cite>. While calibration methods exist to address this issue, they typically require access to the model’s internal information or fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib31" title="">31</a>]</cite>, which is not feasible with closed-source FMs. Therefore, we apply a z-score standardization based on the confidence score distribution across the dataset to normalize the confidence values.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.3.1">Step 4 – Compare FM labels with human labels.</span>
The produced labels are compared with the human-provided labels from the golden dataset. We assess inter-rater reliability using Cohen’s <math alttext="\kappa" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">italic_κ</annotation></semantics></math> coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib11" title="">11</a>]</cite>, which measures the degree of agreement between the FM-generated labels and human labels. We set a threshold of <math alttext="\kappa&gt;0.78" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mi id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">κ</mi><mo id="S3.SS1.p5.2.m2.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.cmml">&gt;</mo><mn id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">0.78</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><gt id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1"></gt><ci id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">𝜅</ci><cn id="S3.SS1.p5.2.m2.1.1.3.cmml" type="float" xref="S3.SS1.p5.2.m2.1.1.3">0.78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\kappa&gt;0.78</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">italic_κ &gt; 0.78</annotation></semantics></math> (indicating excellent agreement) for at least one FM in the jury. Additionally, all FMs must achieve <math alttext="\kappa\geq 0.63" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mrow id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">κ</mi><mo id="S3.SS1.p5.3.m3.1.1.1" xref="S3.SS1.p5.3.m3.1.1.1.cmml">≥</mo><mn id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">0.63</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><geq id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1.1"></geq><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">𝜅</ci><cn id="S3.SS1.p5.3.m3.1.1.3.cmml" type="float" xref="S3.SS1.p5.3.m3.1.1.3">0.63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">\kappa\geq 0.63</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">italic_κ ≥ 0.63</annotation></semantics></math> (indicating substantial agreement). If these thresholds are not met, we return to Step 2 and refine the prompt iteratively. The refinement process involves adding clarifications, improving instructions, or reordering prompt components to resolve ambiguities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib14" title="">14</a>]</cite>. The iterative loop continues until the desired agreement is reached.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Step 5 – Freeze the prompt for full dataset labelling.</span>
Once the prompt achieves the required level of agreement in Step 4, it is finalized and used to label the entire dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Merging the FM outputs</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">After the individual FMs in the jury provide their labels, we aggregate the results using a majority vote, where the final label is determined by the label that receives the most votes from the individual FMs. In case of a tie, we use the normalized confidence scores to break the tie.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Human-in-the-loop</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">A human-in-the-loop process was employed to decide whether to accept or reject new labels proposed by FMs. However, the process is only required when the FM/LLM Jury cannot resolve the final label. In our study, the FM/LLM Jury successfully handled all cases without the need for human involvement in the labelling process.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Selected FMs</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The jury FMs are selected based on their performance and ability to follow complex instructions. We used the LLM Arena Leaderboard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib8" title="">8</a>]</cite> to identify strong candidates. We selected the open-source Qwen2-72B-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib47" title="">47</a>]</cite> model, and two closed-source models, GPT-4o-mini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib34" title="">34</a>]</cite> and Gemini-1.5-Flash <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib41" title="">41</a>]</cite>. These models were selected due to the balance between cost and performance. We did not select the top-performing models, as they are around ten times as expensive.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S2.F2" title="Figure 2 ‣ II-B Related work ‣ II Background and Related Work ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of our methodology. In this section, we discuss every step.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Identifying blogs related to SE and FM from technology companies</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We employed a systematic approach using search queries based on keywords related to FM4SE and SE4FM to gather blog posts. The data collection process consists of these steps:</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Collected blogs with at least one relevant post.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Company</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Blog</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">AMD</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://community.amd.com/t5/ai/" title="">https://community.amd.com/t5/ai/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">Adobe</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.developer.adobe.com/" title="">https://blog.developer.adobe.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.4.3.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.3.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.adobe.com/" title="">https://blog.adobe.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">Alibaba</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.4.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.alibabacloud.com/blog/" title="">https://www.alibabacloud.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">Amazon</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.5.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amazon.science/blog" title="">https://www.amazon.science/blog</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.7.6.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.6.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/blogs/" title="">https://aws.amazon.com/blogs/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">Cisco</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.7.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.cisco.com/" title="">https://blogs.cisco.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.9.8.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.8.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.talosintelligence.com/" title="">https://blog.talosintelligence.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.10.9.1">Google</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.9.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developers.googleblog.com/en/" title="">https://developers.googleblog.com/en/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.11.10.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.10.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/" title="">https://blog.google/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.12.11.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.12.11.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/blog/" title="">https://cloud.google.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.13.12.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.13.12.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.google/blog/" title="">https://research.google/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14.13">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.14.13.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.14.13.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://deepmind.google/discover/blog/" title="">https://deepmind.google/discover/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.15.14.1">IBM</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.15.14.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.ibm.com/blog" title="">https://research.ibm.com/blog</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16.15">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.16.15.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.16.15.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.ibm.com/blogs/" title="">https://developer.ibm.com/blogs/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.17.16">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.17.16.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.17.16.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ibm.com/blog/" title="">https://www.ibm.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.18.17.1">Meta</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.18.17.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://tech.facebook.com/engineering/" title="">https://tech.facebook.com/engineering/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.19.18">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.19.18.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.19.18.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.facebook.com/blog/" title="">https://research.facebook.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.20.19">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.20.19.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.20.19.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engineering.fb.com/" title="">https://engineering.fb.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.21.20.1">Microsoft</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.21.20.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/blog/" title="">https://www.microsoft.com/en-us/research/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.22.21">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.22.21.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.22.21.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.microsoft.com/" title="">https://blogs.microsoft.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.23.22">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.23.22.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.23.22.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://devblogs.microsoft.com/" title="">https://devblogs.microsoft.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.24.23">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.24.23.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.24.23.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://techcommunity.microsoft.com/t5/" title="">https://techcommunity.microsoft.com/t5/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.25.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.25.24.1">Nvidia</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.25.24.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.nvidia.com/" title="">https://blogs.nvidia.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.26.25">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.26.25.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.26.25.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/" title="">https://developer.nvidia.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.27.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.27.26.1">Oracle</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.27.26.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.oracle.com/" title="">https://blogs.oracle.com/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.28.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.28.27.1">Qualcomm</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.28.27.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.qualcomm.com/developer/blog" title="">https://www.qualcomm.com/developer/blog</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.29.28">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.29.28.1">SAP</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.29.28.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://community.sap.com/t5/technology-blogs-by-sap" title="">https://community.sap.com/t5/technology-blogs-by-sap</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.30.29">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.30.29.1">Salesforce</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.30.29.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.salesforce.com/blog/" title="">https://www.salesforce.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.31.30">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.1.31.30.1"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.31.30.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engineering.salesforce.com/blog/" title="">https://engineering.salesforce.com/blog/</a></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.32.31">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.32.31.1"></th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.32.31.2"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.salesforceairesearch.com/" title="">https://blog.salesforceairesearch.com/</a></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Step 1 – Collect a list of blogs from technology companies.</span> We began by collecting a list of companies from the <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p2.1.2">“Technology”</span> sector with a market capitalization greater than $200 billion (<span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p2.1.3">“Mega”</span>) based on data from NASDAQ.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nasdaq.com" title="">https://www.nasdaq.com</a></span></span></span> In addition, we manually went through the top 100 companies from Forbes’ Global 2,000 list <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib33" title="">33</a>]</cite> and included companies that are categorized under the <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p2.1.4">“IT Software &amp; Services”</span> industry. In total, we included 20 companies. For each of these companies, we searched for relevant blogs using this query:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S4.SS1.p3.1.1" style="width:426.8pt;">
<span class="ltx_p" id="S4.SS1.p3.1.1.1">blog AND (<span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p3.1.1.1.1">“software”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p3.1.1.1.2">“research”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p3.1.1.1.3">“engineer”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p3.1.1.1.4">“engineering”</span>)</span>
</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We identified 35 blogs from 17 companies (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4.T1" title="TABLE I ‣ IV-A Identifying blogs related to SE and FM from technology companies ‣ IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">I</span></a>). This list is easily updated to include new companies/blogs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Step 2 – Search for keywords inside blogs using the Google Search API.</span> We adapted the keywords used in prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib48" title="">48</a>]</cite> to suit the FM4SE and SE4FM contexts:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S4.SS1.p6.1.1" style="width:426.8pt;">
<span class="ltx_p" id="S4.SS1.p6.1.1.1"><span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.1">“software”</span> AND (<span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.2">“large language model”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.3">“large language models”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.4">“LLM”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.5">“LLMs”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.6">“foundation model”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.7">“foundation models”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.8">“FM”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.9">“FMs”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.10">“generative AI”</span> OR <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p6.1.1.1.11">“GenAI”</span>) site:{url}</span>
</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Using these keywords, we queried the Google Search API across all the blogs identified in Step 1, limiting the date range from August 10, 2023, to August 10, 2024. This process yielded 7,120 search results, including information about each result’s URL, title, and snippet. We assigned a unique identifier (ID) to each search result, ranging from 0 to 7,120.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p8.1.1">Step 3 – Filter out non-blog posts and non-English results.</span>
The search results might contain unrelated pages such as index pages and author information pages. We filtered out URLs that contain strings such as <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p8.1.2">“/index/”</span> and <span class="ltx_text ltx_inline-quote ltx_outerquote" id="S4.SS1.p8.1.3">“/author/”</span> to eliminate non-blog posts. We used the <span class="ltx_text ltx_font_italic" id="S4.SS1.p8.1.4">langdetect</span> library<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Mimino666/langdetect" title="">https://github.com/Mimino666/langdetect</a></span></span></span> to remove results where the title or snippet was not in English.</p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p9.1.1">Step 4 – Download the contents of the blog posts.</span>
We downloaded all the content of all valid links. To further remove potential noise, we applied an outlier filter based on the interquartile range (IQR) to exclude content that was either too short or too long. After filtering, we retained 4,463 blog posts, each referenced by a unique ID from Step 2. Throughout this paper, individual blog posts are cited using their corresponding IDs (e.g., [<span class="ltx_text ltx_font_typewriter" id="S4.SS1.p9.1.2">122</span>] for blog post 122), allowing for direct reference to specific entries in the dataset. The full list of blog posts can be found in our online replication package <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Cohen’s Kappa between LLMs and human labels on the golden set for SE-FM area, SE4FM activity, and FM4SE activity classification tasks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T2.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.2.1.1.1">SE-FM</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.2.1.2.1">Area</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T2.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.3.1.1.1">SE4FM</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.3.1.2.1">Activities</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T2.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.4.1.1.1">FM4SE</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.1.4.1.2.1">Activities</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">Gemini-1.5-Flash-002</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.2">0.65</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.3">0.81</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.4">0.66</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1">GPT-4o-mini-2024-07-18</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.2">0.70</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.3">0.81</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.4">0.74</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1">Qwen2-72B-Instruct</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.2">0.85</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.3">0.76</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.4">0.79</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.1.1">LLM Jury</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.2.1">0.95</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.3.1">0.91</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T2.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.4.1">0.87</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Identifying the FM4SE and SE4FM area, activities and discussed tasks in the blog posts</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We first label blog posts as FM4SE or SE4FM-related. Second, we label the FM4SE or SE4FM activities discussed in the blog posts. Finally, we summarize the activity-specific tasks that are discussed, to facilitate our manual review of the posts. Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.T3" title="TABLE III ‣ V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.T4" title="TABLE IV ‣ VI-A Model deployment &amp; operation ‣ VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IV</span></a> show the identified activities and tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Step 1 – Label blog posts as FM4SE or SE4FM with FM/LLM Jury.</span>
We used the FM/LLM Jury framework (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3" title="III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a>) to classify blog posts as SE4FM, FM4SE, or unrelated. We created a golden dataset of 100 randomly selected blog posts, manually labelled by the first author, to evaluate the FM/LLM Jury for this classification. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4.T2" title="TABLE II ‣ IV-A Identifying blogs related to SE and FM from technology companies ‣ IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">II</span></a>, shows that the FM/LLM Jury achieved an excellent agreement (<math alttext="\kappa=0.95" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">κ</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝜅</ci><cn id="S4.SS2.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p2.1.m1.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\kappa=0.95</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_κ = 0.95</annotation></semantics></math>) with human labels on the test set. We then applied the FM/LLM Jury to the entire dataset of 4,463 blogs, classifying 3,126 as unrelated, 156 as FM4SE and 1,122 as SE4FM.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Step 2 – Label the FM4SE activities in the blog posts.</span>
We used the FM4SE activity labels from prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite>. A golden set of 30 FM4SE blogs was randomly sampled and manually labelled by the first author. Following our iterative prompt construction process (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S3" title="III Using an FM/LLM Jury for Labelling Blog Posts ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a>), we reached an excellent agreement (<math alttext="\kappa=0.91" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">κ</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">0.91</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><eq id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></eq><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝜅</ci><cn id="S4.SS2.p3.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p3.1.m1.1.1.3">0.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\kappa=0.91</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_κ = 0.91</annotation></semantics></math>) between the FM/LLM Jury and the human labels (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4.T2" title="TABLE II ‣ IV-A Identifying blogs related to SE and FM from technology companies ‣ IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">II</span></a>). We used the best-performing prompt to label all FM4SE activities in the 156 FM4SE blog posts. One blog post was labelled as ‘Other’ as the FM/LLM Jury could not identify any specific FM4SE activities, ending up with 155 FM4SE blog posts in total.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Step 3 – Label the SE4FM activities in the blog posts.</span>
Following a similar process, we labelled the SE4FM activities in the 1,122 SE4FM blogs. We created a list of SE4FM activity labels based on prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib15" title="">15</a>]</cite>. We randomly sampled a golden set of 30 SE4FM blog posts and manually labelled the SE4FM activities. Following our prompt construction process, the FM/LLM Jury demonstrated strong agreement (<math alttext="\kappa=0.87" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">κ</mi><mo id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">0.87</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><eq id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></eq><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">𝜅</ci><cn id="S4.SS2.p4.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p4.1.m1.1.1.3">0.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\kappa=0.87</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_κ = 0.87</annotation></semantics></math>) with the human labels. We used the best-performing prompt to label the SE4FM activities in all 1,122 SE4FM blog posts. 125 blog posts were labelled as ‘Other’ as the FM/LLM Jury could not identify any SE4FM activities, ending up with 997 FM4SE blog posts in total.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Step 4 – Identify the activity-specific tasks in the blog posts.</span> To facilitate our manual review of the blog posts, we prompted the FMs to take an additional step to identify the activity-specific tasks in the blog posts.
For FM4SE blogs, we prompted the FMs to identify relevant tasks from a curated set of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib24" title="">24</a>]</cite>. Similarly, for SE4FM blogs, the FMs identified SE4FM-related tasks from a curated set of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib15" title="">15</a>]</cite>. If no predefined tasks were relevant, we asked the FMs to generate new tags based on the blog content. In cases where the FMs proposed new task tags, we employed a human-in-the-loop process to determine whether to reclassify or ignore these tags. However, no human intervention was required for FM4SE posts, and eight new SE4FM task tags identified by the FM model Gemini-1.5-Flash were resolved by the FM/LLM Jury.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">RQ1: Which FM4SE activities are discussed in industry blog posts?</span>
</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Motivation. </span>Industry practitioners are at the forefront of applying FMs to SE, sharing practical insights and real-world experiences through blogs. While academic research has explored many aspects of FM4SE, the industry’s perspective remains underexplored. This study analyzes industry blogs to uncover key FM4SE activities and tasks discussed by practitioners, providing insights into real-world applications of FMs in SE.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_italic" id="S5.p2.1.1">Approach. </span>We used the FM/LLM Jury to label the FM4SE activities and tasks in the 155 FM4SE blog posts. To avoid overrepresentation, we counted each activity and task uniquely per company, even if they were mentioned in multiple blog posts from the same company. To gain insights into how these activities and tasks are discussed, we manually reviewed the selected blog posts. We also compared our findings with those reported by Hou et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>FM4SE activities and tasks that are discussed in industry blog posts. The Comp. column indicates the number of companies publishing blog posts about the task.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Activity</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Posts</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Task</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.4.1">Comp.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.1" rowspan="7"><span class="ltx_text" id="S5.T3.1.2.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.1.2.2.1.1.1">
<span class="ltx_tr" id="S5.T3.1.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.2.2.1.1.1.1.1">Software</span></span>
<span class="ltx_tr" id="S5.T3.1.2.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.2.2.1.1.1.2.1">development</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.2">100</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.2.2.3">Code generation</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.2.2.4">11</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<td class="ltx_td ltx_align_right" id="S5.T3.1.3.3.1">54</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.3.3.2">Code completion</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.3.3.3">8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.1">12</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.4.4.2">Code assistant</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3">5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.1">10</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.5.5.2">Code understanding</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.3">5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.1">6</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.6.6.2">Code summarization</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.6.6.3">4</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.1">3</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.7.7.2">Code optimization</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.3">3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.1">1</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.8.8.2">API recommendation</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.8.8.3">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.9.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.9.2.1">121</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.9.9.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.9.9.4.1">11</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.10.10.1" rowspan="3"><span class="ltx_text" id="S5.T3.1.10.10.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.1.10.10.1.1.1">
<span class="ltx_tr" id="S5.T3.1.10.10.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.10.10.1.1.1.1.1">Software</span></span>
<span class="ltx_tr" id="S5.T3.1.10.10.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.10.10.1.1.1.2.1">quality</span></span>
<span class="ltx_tr" id="S5.T3.1.10.10.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.10.10.1.1.1.3.1">assurance</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.10.10.2">10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.10.10.3">Vulnerability detection</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.10.10.4">3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.11.11">
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.1">5</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.11.11.2">Debugging</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.11.11.3">3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.12.12">
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.1">5</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.12.12.2">Test generation/automation</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.12.12.3">2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.13.13.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.13.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.13.2.1">17</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.13.13.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.13.13.4.1">4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.14.14.1" rowspan="6"><span class="ltx_text" id="S5.T3.1.14.14.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.1.14.14.1.1.1">
<span class="ltx_tr" id="S5.T3.1.14.14.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.14.14.1.1.1.1.1">Software</span></span>
<span class="ltx_tr" id="S5.T3.1.14.14.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.14.14.1.1.1.2.1">maintenance</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.14.14.2">4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.14.14.3">Code refactoring or revision</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.14.14.4">3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.15.15">
<td class="ltx_td ltx_align_right" id="S5.T3.1.15.15.1">3</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.15.15.2">Code translation</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.15.15.3">2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.16.16">
<td class="ltx_td ltx_align_right" id="S5.T3.1.16.16.1">7</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.16.16.2">Code transformation or modernization</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.16.16.3">2</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.17.17">
<td class="ltx_td ltx_align_right" id="S5.T3.1.17.17.1">1</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.17.17.2">Program repair</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.17.17.3">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.18.18">
<td class="ltx_td ltx_align_right" id="S5.T3.1.18.18.1">2</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.18.18.2">Code review</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.18.18.3">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.19.19">
<td class="ltx_td ltx_align_right" id="S5.T3.1.19.19.1">1</td>
<td class="ltx_td ltx_align_left" id="S5.T3.1.19.19.2">Log analysis</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.19.19.3">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.20.20">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.20.20.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.20.20.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.20.20.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.20.20.2.1">15</span></td>
<td class="ltx_td ltx_border_t" id="S5.T3.1.20.20.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.20.20.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.20.20.4.1">4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.21.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.21.21.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.21.21.1.1">
<tr class="ltx_tr" id="S5.T3.1.21.21.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.21.21.1.1.1.1">Software</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.21.21.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.21.21.1.1.2.1">management</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.21.21.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.21.21.2.1">1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.21.21.3">Software tool configuration</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.21.21.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.21.21.4.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.22.22">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.22.22.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.22.22.1.1">
<tr class="ltx_tr" id="S5.T3.1.22.22.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.22.22.1.1.1.1">Requirement</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.22.22.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.22.22.1.1.2.1">engineering</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.22.22.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.22.22.2.1">1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.22.22.3">Requirements analysis</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.22.22.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.22.22.4.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.23.23">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.23.23.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.1.23.23.1.1">
<tr class="ltx_tr" id="S5.T3.1.23.23.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.23.23.1.1.1.1">Software</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.23.23.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T3.1.23.23.1.1.2.1">design</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.23.23.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.23.23.2.1">0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.1.23.23.3">–</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T3.1.23.23.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.23.23.4.1">0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.24.24">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T3.1.24.24.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.24.24.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T3.1.24.24.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.24.24.2.1">155</span></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S5.T3.1.24.24.3"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S5.T3.1.24.24.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.24.24.4.1">11</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Software development</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Although code generation is the most prominent task, FMs are used for many other tasks in the software development process.</span> As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.T3" title="TABLE III ‣ V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a>, code generation emerges as the most prominent task. Practitioners report leveraging FMs to generate code in modern languages [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.2">B140</span>] such as Python and Java, but there is also growing attention for legacy systems, with FMs being used to generate COBOL code [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.3">B830</span>]. Additionally, FMs are applied to specialized domains such as SQL query generation [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.4">B183</span>], and domain-specific languages (DSLs) tailored to industry-specific needs such as semiconductor design [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p1.1.5">B8</span>]. The flexibility of FMs to adapt across various programming languages and domains highlights their versatility in software development.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">A topic that is closely related to, and often discussed together with code generation is <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">code completion</span>. A notable technique is <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">fill-in-the-middle</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib13" title="">13</a>]</cite> for code completion, which allows models to complete code based on partial inputs [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p2.1.3">B157</span>], further expanding the usability of FMs in practical coding environments.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Beyond code generation and completion, FMs are increasingly being integrated into software development as <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1.1">code assistants</span>, offering a range of functionalities.</span> These assistants not only generate and complete code but also help with <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">code understanding</span>, <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.3">code summarization</span>, <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.4">code optimization</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.5">API recommendations</span>. For instance, code assistants can understand code and explain it to developers [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.6">B5668</span>], summarize code changes for reviews [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.7">B322</span>], optimize code for performance [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.8">B3357</span>], or recommend APIs based on both public and private code repositories [<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p3.1.9">B3177</span>]. This multifaceted support streamlines the development process, boosting developer productivity and efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Software quality assurance</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Vulnerability detection is the most frequently discussed software quality assurance (QA) task.</span> Practitioners employ FMs to automate <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">common vulnerabilities and exposures (CVE) detection</span> and analysis [<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.3">B101</span>]. Other tasks under this category include <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.4">test generation and automation</span>, where FMs are used to generate test cases based on the functionality of a given code. For instance, the FM can suggest test cases for invalid inputs, edge cases, and error handling [<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.5">B5518</span>]. For <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.6">debugging</span> tasks, FMs can suggest where to insert logging and exception handling to track code execution and errors, and FMs can also be used to detect anomalies and fix common issues such as syntax and logical errors [<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.1.7">B5465</span>].</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Software maintenance</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">The use of FMs in software maintenance focuses on the refactoring, translation, and transformation of existing codebases.</span> Practitioners often discuss these tasks in the context of modernizing legacy systems. For instance, FMs are employed to <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.2">refactor</span> and <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.3">translate</span> legacy COBOL code into Java [<span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.4">B769</span>]. In addition, upgrading Java applications to newer Long-Term Support (LTS) versions [<span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.5">B369</span>] or migrating Java codebases to cloud-based infrastructures [<span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.6">B6378</span>] are tasks frequently associated with <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.7">code transformation</span>. Using FMs for these tasks helps industries transition from older systems to modern architectures more efficiently.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Software management</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.1">Software management receives the least attention in FM4SE blogs, and no discussions were found regarding requirements engineering or software design.</span> Practitioners use FMs to generate <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.2">software tool configurations</span> for managing cloud infrastructure components [<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.3">B5625</span>]. We did not find discussions on <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.4">requirements engineering</span> or <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.5">software design</span> in FM4SE blogs. One blog, which was initially misclassified as covering requirements analysis [<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.6">B1182</span>], was actually about using FMs to extract developer intent from code comments or function documentation for generating formal postconditions. Overall, FM-based requirements engineering and software design remain an underreported area in the industry.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.6.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.7.2">Comparison with Academic Research</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Software development is the most discussed activity in both industry blog posts and SE research papers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite>. Both practitioners and researchers frequently highlight code generation as a key task. However, there are some notable differences. Regarding software maintenance, for example, industry blogs focus more on code refactoring and revision, while academic research papers focus more on program repair. Additionally, while Hou et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite> reported that 4.3% of surveyed papers (17 out of 395) cover requirements engineering, we found no substantial discussion on this topic in the industry blogs we analyzed (except the one misclassified).</p>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="S5.SS5.1.p1">
<p class="ltx_p" id="S5.SS5.1.p1.1"><span class="ltx_text" id="S5.SS5.1.p1.1.1" style="border:2px solid black;border-radius:5px;;">
<span class="ltx_inline-block ltx_minipage ltx_align_middle" id="S5.SS5.1.p1.1.1.1" style="width:216.8pt;">
<span class="ltx_p" id="S5.SS5.1.p1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.1.p1.1.1.1.1.1">RQ1 Summary:</span>
Software development tasks, particularly code generation, are the most frequently discussed across FM4SE blogs. FMs are increasingly integrated as code assistants, providing developers with multifunctional tools to boost productivity. In the domain of software quality assurance, vulnerability detection is the dominant task, while software maintenance activities are primarily focused on refactoring and transforming existing codebases. Software management, requirements engineering, and software design receive less attention in these industry blogs.</span>
</span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">RQ2: Which SE4FM activities are discussed in industry blog posts?</span>
</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Motivation. </span>As FMs are integrated into production systems, applying SE principles to the development cycle of FM-based systems becomes increasingly important. While academic research studied SE for AI-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>]</cite>, FM-based systems present unique challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib15" title="">15</a>]</cite>, such as the resource-intensive nature of FMs, and the complexities involved in fine-tuning, deployment, and monitoring at scale. Industry blog posts offer valuable insights into how practitioners adapt SE principles for developing, deploying, managing, and scaling FMs in real-world settings. This study gives an overview of key SE4FM activities and tasks discussed in these blog posts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_italic" id="S6.p2.1.1">Approach. </span>We used the FM/LLM Jury to label the SE4FM activities and tasks in the 997 SE4FM blog posts. Similar to Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5" title="V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">V</span></a>, we counted each activity and task uniquely per company and manually reviewed selected blog posts for each task. We did not compare the discussion frequency with previous research like we did in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5" title="V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">V</span></a>, because there exists no prior survey on SE4FM, and SE4FM activities and tasks are quite different from those for SE4ML <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Model deployment &amp; operation</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.2"><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.2.1">Model deployment &amp; operation is the most frequently discussed activity in SE4FM industry blog posts.</span> <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.2.2">Model deployment on cloud</span> is the dominant task (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.T4" title="TABLE IV ‣ VI-A Model deployment &amp; operation ‣ VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IV</span></a>), reflecting the industry’s reliance on cloud environments for hosting foundation models. Foundation models are very resource intensive. For example, a large model such as Meta Llama 3 (with 405 billion parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib9" title="">9</a>]</cite>) [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p1.2.3">B1749,B5439,B6691</span>] requires <math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><mo id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">∼</annotation></semantics></math>810GB of GPU VRAM for inference, <math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mo id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">∼</annotation></semantics></math>3.25TB for fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib40" title="">40</a>]</cite> and much more for training from scratch. The cloud facilitates using such large models on-demand without the need for buying very expensive hardware.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">For <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.1">model serving &amp; scaling</span> [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.2">B111,B627,B6028</span>], techniques such as speculative decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib7" title="">7</a>]</cite> accelerate model inference by using draft models for faster response times [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.3">B5308</span>]. In addition, automatic model scaling ensures that resources automatically adjust to workload needs [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.4">B701,B4518</span>]. <span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.5">Model monitoring</span> [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.6">B6659</span>] is another key task, involving tracking token usage [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.7">B406,B920</span>] and monitoring system metrics such as memory and GPU load [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p2.1.8">B5192</span>].</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">While cloud deployment dominates, there is increasing interest in <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.1.1">model deployment on local</span> devices such as edge or mobile devices, and PCs.</span> For example, practitioners deploy FM-based medical chatbots for healthcare applications on edge devices, to facilitate data privacy [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.2">B139</span>]. Another reason to deploy FMs on smaller devices is to overcome the GPU supply-and-demand problem, which makes it hard for many companies to integrate FMs into their products [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.3">B1355</span>].
To enable running the resource-intensive FMs on relatively small devices, companies frequently use <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.4">model compression</span> techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib51" title="">51</a>]</cite> to reduce the required resources [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.5">B1744,B1993,B5463,B5714</span>]. For example, quantization techniques (e.g., 4-bit precision) enable running models on CPUs, avoiding the need for GPUs [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.6">B669</span>]. Compression techniques are not limited to text models: model quantization is also applied to image generative models such as Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib39" title="">39</a>]</cite> [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.7">B2254</span>]. Several libraries are used by practitioners that assist with running FMs on CPU, such as <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.8">LLaMA.cpp<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote3.1.1.1">3</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a></span></span></span></span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.9">ExLlama<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote4.1.1.1">4</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright" href="https://github.com/turboderp/exllama" title="">https://github.com/turboderp/exllama</a></span></span></span></span> [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.10">B1355</span>]. Also, several practitioners describe how the use of Neural Processing Units (NPUs) can facilitate running FMs locally [<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.11">B914,B1750</span>].</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>SE4FM activities and tasks that are discussed in industry blog posts. The Comp. column indicates the number of companies publishing blog posts about the task.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.1">Activity</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.2.1">Posts</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.3.1">Task</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.4.1">Comp.</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.2.1" rowspan="5"><span class="ltx_text" id="S6.T4.1.2.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.2.2.1.1.1">
<span class="ltx_tr" id="S6.T4.1.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.2.2.1.1.1.1.1">Model</span></span>
<span class="ltx_tr" id="S6.T4.1.2.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.2.2.1.1.1.2.1">deployment</span></span>
<span class="ltx_tr" id="S6.T4.1.2.2.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.2.2.1.1.1.3.1">&amp; operation</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.2.2.2">237</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.2.2.3">Model deployment on cloud</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.2.2.4">13</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.3.3">
<td class="ltx_td ltx_align_right" id="S6.T4.1.3.3.1">168</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.3.3.2">Model serving &amp; scaling</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.3.3.3">12</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.4.4">
<td class="ltx_td ltx_align_right" id="S6.T4.1.4.4.1">30</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.4.4.2">Model monitoring</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.4.4.3">8</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.5.5">
<td class="ltx_td ltx_align_right" id="S6.T4.1.5.5.1">31</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.5.5.2">Model compression</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.5.5.3">7</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.6.6">
<td class="ltx_td ltx_align_right" id="S6.T4.1.6.6.1">39</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.6.6.2">Model deployment on local</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.6.6.3">6</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.7.7.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.7.7.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.7.7.2.1">373</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.7.7.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.7.7.4.1">13</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.8.8.1" rowspan="5"><span class="ltx_text" id="S6.T4.1.8.8.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.8.8.1.1.1">
<span class="ltx_tr" id="S6.T4.1.8.8.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.8.8.1.1.1.1.1">System</span></span>
<span class="ltx_tr" id="S6.T4.1.8.8.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.8.8.1.1.1.2.1">architecture</span></span>
<span class="ltx_tr" id="S6.T4.1.8.8.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.8.8.1.1.1.3.1">&amp; orchestration</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.8.8.2">94</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.8.8.3">Model &amp; prompt chaining</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.8.8.4">11</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.9.9">
<td class="ltx_td ltx_align_right" id="S6.T4.1.9.9.1">108</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.9.9.2">Workflow orchestration</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.9.9.3">10</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.10.10">
<td class="ltx_td ltx_align_right" id="S6.T4.1.10.10.1">83</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.10.10.2">Building AI agents</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.10.10.3">10</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.11.11">
<td class="ltx_td ltx_align_right" id="S6.T4.1.11.11.1">99</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.11.11.2">Development platform &amp; studio</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.11.11.3">9</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.12.12">
<td class="ltx_td ltx_align_right" id="S6.T4.1.12.12.1">6</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.12.12.2">Implementing guardrails</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.12.12.3">3</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.13.13.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.13.13.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.13.13.2.1">287</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.13.13.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.13.13.4.1">12</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.14.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.14.14.1" rowspan="6"><span class="ltx_text" id="S6.T4.1.14.14.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.14.14.1.1.1">
<span class="ltx_tr" id="S6.T4.1.14.14.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.14.14.1.1.1.1.1">Data</span></span>
<span class="ltx_tr" id="S6.T4.1.14.14.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.14.14.1.1.1.2.1">management</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.14.14.2">90</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.14.14.3">RAG integration</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.14.14.4">11</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.15.15">
<td class="ltx_td ltx_align_right" id="S6.T4.1.15.15.1">48</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.15.15.2">Specialized databases</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.15.15.3">8</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.16.16">
<td class="ltx_td ltx_align_right" id="S6.T4.1.16.16.1">40</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.16.16.2">Dataset cleaning &amp; preparation</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.16.16.3">8</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.17.17">
<td class="ltx_td ltx_align_right" id="S6.T4.1.17.17.1">43</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.17.17.2">Dataset collection</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.17.17.3">6</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.18.18">
<td class="ltx_td ltx_align_right" id="S6.T4.1.18.18.1">7</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.18.18.2">Feature engineering</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.18.18.3">5</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.19.19">
<td class="ltx_td ltx_align_right" id="S6.T4.1.19.19.1">3</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.19.19.2">Dataset labelling</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.19.19.3">2</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.20.20">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.20.20.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.20.20.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.20.20.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.20.20.2.1">182</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.20.20.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.20.20.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.20.20.4.1">11</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.21.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.21.21.1" rowspan="3"><span class="ltx_text" id="S6.T4.1.21.21.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.21.21.1.1.1">
<span class="ltx_tr" id="S6.T4.1.21.21.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.21.21.1.1.1.1.1">Model</span></span>
<span class="ltx_tr" id="S6.T4.1.21.21.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.21.21.1.1.1.2.1">customization</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.21.21.2">85</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.21.21.3">General fine-tuning</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.21.21.4">10</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.22.22">
<td class="ltx_td ltx_align_right" id="S6.T4.1.22.22.1">20</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.22.22.2">LoRA</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.22.22.3">4</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.23.23">
<td class="ltx_td ltx_align_right" id="S6.T4.1.23.23.1">10</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.23.23.2">RLHF</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.23.23.3">4</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.24.24">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.24.24.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.24.24.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.24.24.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.24.24.2.1">104</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.24.24.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.24.24.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.24.24.4.1">11</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.25.25">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.25.25.1" rowspan="6"><span class="ltx_text" id="S6.T4.1.25.25.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.25.25.1.1.1">
<span class="ltx_tr" id="S6.T4.1.25.25.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.25.25.1.1.1.1.1">Evaluation</span></span>
<span class="ltx_tr" id="S6.T4.1.25.25.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.25.25.1.1.1.2.1">&amp; quality</span></span>
<span class="ltx_tr" id="S6.T4.1.25.25.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.25.25.1.1.1.3.1">assurance</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.25.25.2">17</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.25.25.3">Model evaluation</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.25.25.4">6</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.26.26">
<td class="ltx_td ltx_align_right" id="S6.T4.1.26.26.1">15</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.26.26.2">Model safety &amp; compliance</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.26.26.3">6</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.27.27">
<td class="ltx_td ltx_align_right" id="S6.T4.1.27.27.1">5</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.27.27.2">Testing strategies</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.27.27.3">4</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.28.28">
<td class="ltx_td ltx_align_right" id="S6.T4.1.28.28.1">6</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.28.28.2">Model risk &amp; trust</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.28.28.3">4</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.29.29">
<td class="ltx_td ltx_align_right" id="S6.T4.1.29.29.1">2</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.29.29.2">Model fairness &amp; bias</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.29.29.3">2</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.30.30">
<td class="ltx_td ltx_align_right" id="S6.T4.1.30.30.1">1</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.30.30.2">Model explainability</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.30.30.3">1</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.31.31">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.31.31.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.31.31.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.31.31.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.31.31.2.1">40</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.31.31.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.31.31.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.31.31.4.1">7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.32.32">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.32.32.1" rowspan="2"><span class="ltx_text" id="S6.T4.1.32.32.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.1.32.32.1.1.1">
<span class="ltx_tr" id="S6.T4.1.32.32.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.32.32.1.1.1.1.1">Prompt</span></span>
<span class="ltx_tr" id="S6.T4.1.32.32.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.32.32.1.1.1.2.1">construction</span></span>
</span></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.32.32.2">10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.32.32.3">Prompt engineering</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.32.32.4">5</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.33.33">
<td class="ltx_td ltx_align_right" id="S6.T4.1.33.33.1">4</td>
<td class="ltx_td ltx_align_left" id="S6.T4.1.33.33.2">Automated prompt generation</td>
<td class="ltx_td ltx_align_right" id="S6.T4.1.33.33.3">3</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.34.34">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.34.34.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.34.34.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.34.34.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.34.34.2.1">11</span></td>
<td class="ltx_td ltx_border_t" id="S6.T4.1.34.34.3"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.34.34.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.34.34.4.1">6</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.35.35">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.35.35.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T4.1.35.35.1.1">
<tr class="ltx_tr" id="S6.T4.1.35.35.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.35.35.1.1.1.1">Requirements</td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.35.35.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T4.1.35.35.1.1.2.1">engineering</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.35.35.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.35.35.2.1">0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.1.35.35.3">–</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.1.35.35.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.35.35.4.1">0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.1.36.36">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S6.T4.1.36.36.1"><span class="ltx_text ltx_font_bold" id="S6.T4.1.36.36.1.1"># Total</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S6.T4.1.36.36.2"><span class="ltx_text ltx_font_bold" id="S6.T4.1.36.36.2.1">997</span></td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S6.T4.1.36.36.3"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S6.T4.1.36.36.4"><span class="ltx_text ltx_font_bold" id="S6.T4.1.36.36.4.1">14</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">System architecture &amp; orchestration</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">System architecture &amp; orchestration activities, including building AI agents and model &amp; prompt chaining, have become a popular topic in SE4FM industry blog posts.</span> One of the most frequently discussed tasks is <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">model &amp; prompt chaining</span>, which is used to manage complex workflows by breaking them into smaller, more manageable steps, each handled by different models or prompts [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p1.1.3">B881</span>]. For example, a task such as responding to customer reviews might be divided into steps like filtering harmful content, performing sentiment analysis, and generating an appropriate response [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p1.1.4">B4881</span>]. Tools and frameworks like <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.5">LangChain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib6" title="">6</a>]</cite> and <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.6">PromptFlow</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib30" title="">30</a>]</cite> are commonly mentioned as practical solutions for implementing these chained workflows [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p1.1.7">B1357</span>].</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">A closely related discussion topic is <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">building AI agents</span>, which extend the functionality of FMs by integrating external tools and <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">orchestrating workflows</span>.
In multi-agent systems, multiple AI agents collaborate using a complex workflow to handle different parts of a task [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p2.1.3">B1166</span>].
These AI agents can autonomously decide which tools to use, retrieve necessary data, and execute predefined plans based on user input or real-time data [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p2.1.4">B414</span>].
Another key feature of AI agents is their ability to leverage working memory, allowing them to retain information from previous interactions or external tool outputs, which can be critical for managing long-term tasks [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p2.1.5">B442</span>].</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Enterprise <span class="ltx_text ltx_font_italic" id="S6.SS2.p3.1.1">development platforms &amp; studios</span> provide support for building FM-based systems based on chaining or AI agents [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p3.1.2">B414,B1166,B4881</span>]. These platforms, such as <span class="ltx_text ltx_font_italic" id="S6.SS2.p3.1.3">FMArts</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib15" title="">15</a>]</cite>, simplify the orchestration of workflows [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p3.1.4">B358</span>], allowing developers to easily integrate FMs with external systems. These platforms also support <span class="ltx_text ltx_font_italic" id="S6.SS2.p3.1.5">implementing guardrails</span>. Guardrails can take the form of filters that are applied to user inputs or LLM outputs [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p3.1.6">B4677</span>], or can be embedded in the prompts to guide the model’s responses [<span class="ltx_text ltx_font_typewriter" id="S6.SS2.p3.1.7">B2456</span>].</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Data management</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p1.1.1">Data management has evolved in the FM era, with new techniques supporting the vast amounts of both structured and unstructured data.</span> At the core of this evolution is the shift to specialized, more dynamic data management techniques. The most frequently discussed task is <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.2">Retrieval-Augmented Generation (RAG)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib22" title="">22</a>]</cite>, which combines the use of private datasets (i.e., data on which the FM was not trained before such as proprietary data) with FMs. With RAG, the FM generates its response based on the prompt and information in the private dataset [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p1.1.3">B4270</span>]. Several practitioners discuss how they build datasets for RAG [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p1.1.4">B6492,B6688</span>] using techniques including document chunking, embedding, and vector storage. Another example of usage of RAG by practitioners is GraphRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib10" title="">10</a>]</cite> which enhances RAG by generating knowledge graphs from the private data which can then be used for prompt augmentation [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p1.1.5">B1155,B1156</span>].</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">There is a great amount of discussion of <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.1.1">specialized databases</span>, particularly vector databases.</span> Specialized databases are key to enabling RAG. These databases support semantic search by indexing unstructured data like text and images, making FM-based retrieval faster and more accurate [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.2">B1571</span>]. Advanced features include multimodal search, where users retrieve image or video content using text queries [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.3">B5035</span>]. This shift from traditional keyword-based search to semantic search also integrates with SQL queries for managing both structured and unstructured data [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.4">B1426</span>]. As data management moves beyond traditional data types (e.g., rows, columns, JSON), vector-based storage and retrieval systems are becoming more important [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.5">B5370</span>]. Likewise, <span class="ltx_text ltx_font_italic" id="S6.SS3.p2.1.6">embedding as feature engineering</span> is becoming increasingly important in FM-based systems, enabling text, images, and structured data to be converted into numerical vectors that FMs can process [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.7">B1982</span>]. Multimodal embeddings, which map both text and images into a shared vector space, are particularly useful in cross-modal applications such as text-to-image search or video retrieval [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p2.1.8">B5815</span>].</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p3.1.1">Synthetic data generation provides scalable, domain-specific data without privacy risks, reducing reliance on real-world datasets.</span>
As FM data requirements grow, synthetic data is increasingly used to address the challenges of high-quality <span class="ltx_text ltx_font_italic" id="S6.SS3.p3.1.2">data collection</span> [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p3.1.3">B35,B4210</span>].
In parallel, <span class="ltx_text ltx_font_italic" id="S6.SS3.p3.1.4">automated data labelling</span> is being transformed by model-assisted approaches. For example, the Recognize Anything Model (RAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib49" title="">49</a>]</cite> can automatically label visual datasets, enabling users to search for images or videos using natural language queries [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p3.1.5">B3649</span>]. Additionally, human-in-the-loop is applied for combining model-generated annotations with manual oversight to ensure accuracy while reducing the time and cost associated with traditional labelling methods [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p3.1.6">B5181</span>].</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.1.1">There is a noticeable push on data privacy in <span class="ltx_text ltx_font_italic" id="S6.SS3.p4.1.1.1">data cleaning &amp; preparation</span>.</span> <span class="ltx_text ltx_font_italic" id="S6.SS3.p4.1.2">Data cleaning &amp; preparation</span> remains essential, but data privacy has become an important focus. Since the data used for customizing FMs could contain Personally Identifiable Information (PII), data anonymization techniques such as differential privacy [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p4.1.3">B5934</span>] are used to remove PII [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p4.1.4">B1286,B1309,B3213</span>]. Beside data privacy, removing duplicate is frequently applied to preprocess the dataset to ensure data quality [<span class="ltx_text ltx_font_typewriter" id="S6.SS3.p4.1.5">B188,B804,B2326</span>].</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Model customization</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS4.p1.1.1">Model customization is achieved through fine-tuning techniques for adapting FMs to specific application needs.</span> <span class="ltx_text ltx_font_italic" id="S6.SS4.p1.1.2">Fine-tuning methods</span> such as supervised fine-tuning (SFT) tune the entire model on domain-specific data, rather than train it from scratch. Enterprise platforms now support no-code fine-tuning, simplifying the process and accelerating development [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.3">B4777</span>]. Open source libraries such as Hugging Face’s PEFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib27" title="">27</a>]</cite> support both full fine-tuning and <span class="ltx_text ltx_font_italic" id="S6.SS4.p1.1.4">Low-Rank Adaptation (LoRA)</span> [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.5">B353</span>]. LoRA is an efficient approach where original model parameters are frozen and injected with trainable matrices. LoRA reduces the number of trainable parameters and lowers GPU requirements, making it cost-effective [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.6">B109</span>]. With different LoRA adapters, a single FM can adapt to handle different tasks. Platforms support dynamic loading and caching of LoRA adapters, offering flexibility and optimizing performance [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.7">B2462</span>]. <span class="ltx_text ltx_font_italic" id="S6.SS4.p1.1.8">RLHF</span> is used to align models with user preferences to improve their experience [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.9">B656</span>] and can also be applied to image models [<span class="ltx_text ltx_font_typewriter" id="S6.SS4.p1.1.10">B2367</span>].</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS5.5.1.1">VI-E</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS5.6.2">Evaluation &amp; quality assurance</span>
</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS5.p1.1.1">SE4FM blog posts outline practical strategies for ensuring the safety, fairness, and trustworthiness of FMs through systematic evaluation &amp; QA processes.</span>
With the diverse applications of FMs, establishing robust <span class="ltx_text ltx_font_italic" id="S6.SS5.p1.1.2">model evaluation</span> frameworks is essential to ensure models meet operational requirements [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p1.1.3">B898</span>]. For example, for FM-based systems with RAG integration, an evaluation framework includes metrics such as answer relevance, context precision, and recall to assess the effectiveness of model outputs [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p1.1.4">B1305</span>]. Ensuring <span class="ltx_text ltx_font_italic" id="S6.SS5.p1.1.5">model safety &amp; compliance</span> is particularly critical in high-stakes industries. Industry blogs highlight the use of adversarial testing to identify model vulnerabilities, while automated raters are often deployed to perform consistent safety assessments [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p1.1.6">B437</span>]. Standardized benchmarks are also leveraged to ensure models meet security and compliance standards, especially in regulated industries [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p1.1.7">B845</span>].</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">Practitioners are increasingly adopting <span class="ltx_text ltx_font_italic" id="S6.SS5.p2.1.1">automated testing strategies for FMs</span>, which often use academic benchmarks like BIG-bench [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.2">B158</span>]. However, custom datasets tailored to specific domain requirements are also vital for evaluating FMs in domain-specific applications [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.3">B845</span>]. One emerging approach in this area is the use of LLM-as-a-judge techniques, which leverage LLMs to provide scalable and consistent evaluations [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.4">B158</span>]. Additionally, adversarial testing is used to strengthen models against potential threats by uncovering weaknesses that may not surface under traditional testing methods [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.5">B845</span>]. Another emerging task is <span class="ltx_text ltx_font_italic" id="S6.SS5.p2.1.6">model explainability &amp; interpretability</span>, which is important particularly in sensitive industries. Tools that generate natural language explanations for model outputs are becoming common, helping developers understand why certain test cases pass or fail [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p2.1.7">B4776</span>]. This increases transparency and aligns FMs with best practices for software engineering.</p>
</div>
<div class="ltx_para" id="S6.SS5.p3">
<p class="ltx_p" id="S6.SS5.p3.1">Some practitioners discuss <span class="ltx_text ltx_font_italic" id="S6.SS5.p3.1.1">model fairness &amp; bias</span>, as biased outputs from FMs can lead to ethical concerns or operational risks. To reduce bias in FM-generated outputs, techniques such as prompt engineering and scenario testing are employed [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p3.1.2">B1295</span>]. Adversarial testing and diverse rater systems are also leveraged to ensure fairness and prevent harmful outputs [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p3.1.3">B437</span>]. Additionally, practitioners highlight the importance of human oversight in critical decision-making processes to mitigate risks associated with harmful or biased outputs [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p3.1.4">B1064</span>]. To enhance trust in FM-based systems, practitioners conclude and follow a set of best practices for AI security, such as the Secure AI Framework (SAIF) [<span class="ltx_text ltx_font_typewriter" id="S6.SS5.p3.1.5">B697</span>].</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS6.6.1.1">VI-F</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS6.7.2">Prompt construction</span>
</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.p1.1.1">Prompt construction receives the least attention, and no discussions were found regarding requirements engineering.</span> <span class="ltx_text ltx_font_italic" id="S6.SS6.p1.1.2">Prompt engineering</span> techniques are discussed in the blog posts, such as structured prompts [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.3">B374</span>] and multi-shot prompts [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.4">B4401</span>]. In database contexts, prompts consider schema, query history, and user-specific factors to generate SQL queries [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.5">B3601</span>]. In addition, <span class="ltx_text ltx_font_italic" id="S6.SS6.p1.1.6">automated prompt generation</span> techniques are explored through dynamic metaprompts which are optimized for greater control and adaptability [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.7">B1172</span>]. For tasks like text-to-image generation, prompts are improved based on semantic search and user context [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.8">B3754</span>]. In addition, prompt compression is proposed to automatically reduce prompt length without sacrificing essential information [<span class="ltx_text ltx_font_typewriter" id="S6.SS6.p1.1.9">B1173</span>].</p>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="S6.SS6.1.p1">
<p class="ltx_p" id="S6.SS6.1.p1.1"><span class="ltx_text" id="S6.SS6.1.p1.1.1" style="border:2px solid black;border-radius:5px;;">
<span class="ltx_inline-block ltx_minipage ltx_align_middle" id="S6.SS6.1.p1.1.1.1" style="width:216.8pt;">
<span class="ltx_p" id="S6.SS6.1.p1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.1.p1.1.1.1.1.1">RQ2 Summary:</span>
The most discussed activities in SE4FM blog posts are model deployment &amp; operation, with a focus on cloud hosting and model serving &amp; scaling. With regards to system architecture, trends include prompt chaining, workflow orchestration for FMs, and AI agents. Data management emphasizes RAG and vector databases to support unstructured data and information retrieval. Model customization relies on fine-tuning methods such as full fine-tuning, LoRA, and RLHF. Evaluation &amp; quality assurance practices focus on automated testing, safety, trustworthiness, and bias mitigation, while prompt engineering receives limited attention. Discussion about requirements engineering is not found in SE4FM blog posts.</span>
</span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Discussion of Future Research Directions</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Research Directions for FM4SE</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.1">Research Direction 1: Using FMs for modernization and transformation of legacy code.</span> Practitioners applied FMs to translate legacy systems into modern languages such as Java [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.2">B769</span>], upgrade to newer language versions [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.3">B369</span>], and migrate to cloud-based infrastructures [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p1.1.4">B6378</span>]. This process mostly relies on FMs for code translation, however, Pan et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib37" title="">37</a>]</cite> highlight challenges in applying FMs to translate code in real-world projects. To address these challenges, researchers should explore methodologies that enhance FM performance for automating complex system migrations, including the transformation of entire legacy codebases and architectures.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Research Direction 2: Evaluating code assistants in software development workflows for tasks other than code generation.</span> Liang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib24" title="">24</a>]</cite> conducted a survey on the usability of code assistants focusing on the code generation task. However, industry discussions highlight that code assistants are employed for other tasks such as code understanding [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p2.1.2">B5668</span>], code summarization [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p2.1.3">B3357</span>], and API recommendation [<span class="ltx_text ltx_font_typewriter" id="S7.SS1.p2.1.4">B3177</span>] as well. This broader integration, including their potential role as AI teammates <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib16" title="">16</a>]</cite>, suggests opportunities for researchers to expand studies on code assistants beyond code generation.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.1.1">Research Direction 3: Real-world validation of research on applying FMs in software management, requirements engineering, and software design.</span> Our analysis of industry blog posts shows that discussions around FM applications in these activities are rare (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5.T3" title="TABLE III ‣ V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">III</span></a>). Likewise, though software engineering researchers have explored using FMs for requirements engineering tasks such as requirements classification and traceability automation, such studies remain relatively rare as well <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib17" title="">17</a>]</cite>. Considering the capabilities of FMs in handling natural languages and programming languages, they should be well-suited for these tasks, hence the lower number of (reported) applications in this area is surprising. Researchers should aim to bridge the gap by identifying the barriers to and developing tools that apply FM techniques in these underreported activities. Such efforts could help bring research advancements in these areas closer to practical industry applications, showcasing the value of FM4SE for a broader range of software engineering activities.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">Research Directions for SE4FM</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1">Research Direction 4: Researchers should expand research on SE4FM.</span> Our findings in Sections <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S5" title="V RQ1: Which FM4SE activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">V</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6" title="VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">VI</span></a> show that practitioners discussed SE4FM much more often than FM4SE, with 997 blog posts focused on SE4FM compared to 155 blog posts on FM4SE. Activities such as model deployment &amp; operation, system architecture, data management, and model customization are frequently mentioned by more than half of the companies (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S6.T4" title="TABLE IV ‣ VI-A Model deployment &amp; operation ‣ VI RQ2: Which SE4FM activities are discussed in industry blog posts? ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IV</span></a>).
Although the lower number of posts on FM4SE does not necessarily indicate a lack of interest in these topics, there seems to be a disconnect between academic research and practitioner activities.
The results of our study highlight the growing importance of adapting SE practices to support the development lifecycle of FM-based systems. Researchers should explore these areas to bridge the gap between industry practices and academic research, thereby supporting the evolving needs of FM-based systems.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">Research Direction 5: Performance engineering for FM-based systems.</span> Performance optimization of FM-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib15" title="">15</a>]</cite> is a critical area that practitioners frequently discussed (in several tasks). Techniques such as model compression and model serving &amp; scaling are employed to reduce the computational resources needed for deploying FMs [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p2.1.2">B701,B5463</span>]. Researchers could contribute by formalizing and standardizing such techniques, as well as exploring new methods to reduce the computational overhead of fine-tuning and deploying FMs. The resource-intensive nature of FMs introduces challenges related to managing large-scale parallelism and ensuring memory efficiency, especially for models with billions of parameters [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p2.1.3">B5192</span>]. Extending traditional load testing methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib18" title="">18</a>]</cite> to FM inference pipelines could help ensure that these models scale effectively and meet the performance requirements of real-time applications.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.1">Research Direction 6: Investigate the impact of the shift from full model training to fine-tuning on software engineering activities such as dependency and asset management.</span> Industry blog posts highlight the increasing reliance on libraries for model fine-tuning and model inference, such as PEFT from Hugging Face [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p3.1.2">B353</span>] and LLaMA.cpp [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p3.1.3">B1355</span>]. Researchers should investigate how these emerging tools and techniques integrate into existing SE workflows and assess their impact on model quality, performance, and usability. For example, the reduced set of trainable parameters used by LoRA can be stored separately as an adapter [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p3.1.4">B2462</span>], which introduces new challenges in asset management. Researchers should investigate how to manage and optimize these adapters within the context of FM-based systems.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.1">Research Direction 7: Supporting FM workflow orchestration and AI agents through software engineering activities.</span> Practitioners are integrating FMs not as standalone components but as part of complex workflows that chain together multiple models, prompts, and external APIs [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p4.1.2">B881,B1357</span>]. Furthermore, AI agents that autonomously manage tasks and orchestrate tools are becoming prevalent in industry [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p4.1.3">B414,B1166</span>]. Researchers should explore design patterns, best practices, and frameworks that support the development of these FM-based systems.</p>
</div>
<div class="ltx_para" id="S7.SS2.p5">
<p class="ltx_p" id="S7.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p5.1.1">Research Direction 8: Supporting the evolving data pipelines in FM-based systems.</span> While traditional data collection, cleaning, and labelling methods remain essential, industry practices are increasingly incorporating synthetic data generation and automated labelling using FMs [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p5.1.2">B4210,B5181</span>]. Synthetic data generation is used to produce large volumes of domain-specific data without privacy concerns [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p5.1.3">B35</span>]. However, as reliance on synthetic and automatically labeled data increases, researchers must investigate the trade-offs between data quality, model performance, and ethical considerations. Human-in-the-loop approaches, where models assist with but do not completely replace manual labelling, also offer a promising area for further exploration [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p5.1.4">B5181</span>]. Additionally, the growing use of specialized databases, such as vector databases for efficient retrieval of unstructured data [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p5.1.5">B1571</span>], demands research into how these evolving data management strategies affect the development lifecycle of FM-based systems, particularly when integrating multimodal data [<span class="ltx_text ltx_font_typewriter" id="S7.SS2.p5.1.6">B5035</span>].</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Threats to Validity</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text ltx_font_italic" id="S8.p1.1.1">Internal Validity.</span> Industry blogs may not fully represent a company’s official or comprehensive views, as posts are often authored by individuals or specific teams. This introduces a risk of bias since not all companies maintain blogs which could potentially skew our dataset. In addition, there could be marketing trends or recent product launches, which could further skew the dataset. To mitigate this threat, we not only report the number of blogs mentioning a specific activity, but also the number of companies discussing those activities in our analysis. Additionally, we realize that any overview of discussed topics in fast moving areas such as FM4SE and SE4FM is only a snapshot of the situation at that time, which can quickly change. We want to emphasize that our FM-powered surveying approach is automated and flexible, and can be rerun easily for different companies, blog posts and possibly even different areas.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">The prompt optimization approach in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#S4" title="IV Methodology ‣ Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models"><span class="ltx_text ltx_ref_tag">IV</span></a> is currently manual. In future work, we plan to explore automated prompt optimization frameworks, such as <span class="ltx_text ltx_font_typewriter" id="S8.p2.1.1">dspy</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1"><span class="ltx_text ltx_font_italic" id="S8.p3.1.1">External Validity.</span> The FM/LLM Jury comprises models of different sizes. If using smaller models, they might lack emergent abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09012v1#bib.bib45" title="">45</a>]</cite> found in larger ones which could affect the accuracy of the labels. Furthermore, our findings are based on blogs from large companies and may not generalize to smaller ones which are limited by resource constraints.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">This study bridges the gap between academic research and industry practices by analyzing 1,152 industry blog posts related to FM4SE and SE4FM from leading technology companies. We used an FM/LLM Jury of multiple FMs to automatically label and summarize the contents. We uncovered key activities and trends discussed by industry practitioners in the intersection of SE and FMs.
Our main findings are:</p>
<ul class="ltx_itemize" id="S9.I1">
<li class="ltx_item" id="S9.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i1.p1">
<p class="ltx_p" id="S9.I1.i1.p1.1">Although code generation is the most prominently discussed task in FM4SE blog posts, FMs are used for many other tasks in the software development process, including code completion, code understanding, and code summarization. Software engineering researchers should investigate how to leverage FMs to support an even broader range of software engineering activities.</p>
</div>
</li>
<li class="ltx_item" id="S9.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S9.I1.i2.p1">
<p class="ltx_p" id="S9.I1.i2.p1.1">In SE4FM blog posts, model deployment &amp; operation and system architecture &amp; orchestration are the most frequently discussed activities. Software engineering researchers should investigate how these activities can be further supported through software engineering.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S9.p2">
<p class="ltx_p" id="S9.p2.1">Our findings offer valuable insights into how industry leaders are leveraging FMs and SE. We provide researchers with eight promising research directions to explore. Additionally, we demonstrate the potential of using an FM-powered surveying approach for automating grey literature surveys in rapidly evolving fields like SE and ML.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Amershi, A. Begel, C. Bird, R. DeLine, H. Gall, E. Kamar, N. Nagappan, B. Nushi, and T. Zimmermann, “Software Engineering for Machine Learning: A Case Study,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)</em>, 2019, pp. 291–300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. A. Bangash, H. Sahar, S. Chowdhury, A. W. Wong, A. Hindle, and K. Ali, “What do Developers Know About Machine Learning: A Study of ML Discussions on StackOverflow,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)</em>, 2019, pp. 260–264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>, “Language Models are Few-Shot Learners,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">Advances in Neural Information Processing Systems (NeurIPS)</em>, vol. 33.   Curran Associates, Inc., 2020, pp. 1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Butler, <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Think write grow: how to become a thought leader and build your business by creating exceptional articles, blogs, speeches, books and more</em>.   John Wiley &amp; Sons, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “A Survey on Evaluation of Large Language Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">ACM Trans. Intell. Syst. Technol.</em>, vol. 15, no. 3, Mar. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Chase, “LangChain,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" title="">https://github.com/langchain-ai/langchain</a>, last visited: Oct 11, Oct. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, “Accelerating Large Language Model Decoding with Speculative Sampling,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2302.01318</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, H. Zhang, B. Zhu, M. Jordan, J. E. Gonzalez <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">arXiv preprint arXiv:2403.04132</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">et al.</em>, “The Llama 3 Herd of Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, “From local to global: A graph rag approach to query-focused summarization,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.16130" title="">https://arxiv.org/abs/2404.16130</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. E. Emam, “Benchmarking Kappa: Interrater Agreement in Software Process Assessments,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Empirical Software Engineering</em>, vol. 4, no. 2, pp. 113–133, Jun 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M. Zhang, “Large Language Models for Software Engineering: Survey and Open Problems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)</em>, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, S. Yih, L. Zettlemoyer, and M. Lewis, “InCoder: A Generative Model for Code Infilling and Synthesis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Eleventh International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Google Cloud, “Prompt iteration strategies,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-iteration" title="">https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-iteration</a>, last visited: Oct 11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. E. Hassan, D. Lin, G. K. Rajbahadur, K. Gallaba, F. R. Cogo, B. Chen, H. Zhang, K. Thangarajah, G. Oliva, J. J. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering (FSE)</em>, 2024, p. 294–305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. E. Hassan, G. A. Oliva, D. Lin, B. Chen, Z. Ming, and Jiang, “Towards AI-Native software engineering (SE 3.0): A vision and a challenge roadmap,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2410.06107</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, and H. Wang, “Large Language Models for Software Engineering: A Systematic Literature Review,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ACM Trans. Softw. Eng. Methodol.</em>, Sep. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z. M. Jiang and A. E. Hassan, “A Survey on Load Testing of Large-Scale Software Systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">IEEE Transactions on Software Engineering</em>, vol. 41, no. 11, pp. 1091–1118, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and M. Zaharia, “Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2212.14024</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts, “Dspy: Compiling declarative language model calls into self-improving pipelines,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2310.03714</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Kocmi and C. Federmann, “Large Language Models Are State-of-the-Art Evaluators of Translation Quality,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>.   European Association for Machine Translation, Jun. 2023, pp. 193–203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS)</em>.   Curran Associates Inc., 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. Li, C.-P. Bezemer, and A. E. Hassan, “Replication package,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/SAILResearch/fmse-blogs" title="">https://github.com/SAILResearch/fmse-blogs</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. T. Liang, C. Yang, and B. A. Myers, “A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. Lin, J. Hilton, and O. Evans, “Teaching Models to Express Their Uncertainty in Words,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ACM Comput. Surv.</em>, vol. 55, no. 9, Jan. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul, and B. Bossan, “Peft: State-of-the-art parameter-efficient fine-tuning methods,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, last visited: Oct 11, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Martínez-Fernández, J. Bogner, X. Franch, M. Oriol, J. Siebert, A. Trendowicz, A. M. Vollmer, and S. Wagner, “Software Engineering for AI-Based Systems: A Survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">ACM Trans. Softw. Eng. Methodol.</em>, vol. 31, no. 2, apr 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Masuda, K. Ono, T. Yasue, and N. Hosokawa, “A Survey of Software Quality for Machine Learning Applications,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)</em>, 2018, pp. 279–284.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Microsoft, “Prompt flow,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/promptflow" title="">https://github.com/microsoft/promptflow</a>, last visited: Oct 11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. J. Mielke, A. Szlam, E. Dinan, and Y.-L. Boureau, “Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Transactions of the Association for Computational Linguistics</em>, vol. 10, pp. 857–872, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. M. Morovati, A. Nikanjam, F. Tambon, F. Khomh, and Z. M. J. Jiang, “Bug characterization in machine learning-based systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Empirical Software Engineering</em>, vol. 29, no. 1, p. 14, Dec 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Murphy and M. Schifrin, “Forbes 2024 global 2000 list,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.forbes.com/lists/global2000/" title="">https://www.forbes.com/lists/global2000/</a>, last visited: Oct 11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
OpenAI, “GPT-4o mini: advancing cost-efficient intelligence,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence" title="">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence</a>, last visited: Oct 11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
——, “Prompt engineering,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/prompt-engineering" title="">https://platform.openai.com/docs/guides/prompt-engineering</a>, last visited: Oct 11, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Training language models to follow instructions with human feedback,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">Advances in Neural Information Processing Systems (NeurIPS)</em>, vol. 35.   Curran Associates, Inc., 2022, pp. 27 730–27 744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, M. Merler, B. Sobolev, R. Pavuluri, S. Sinha, and R. Jabbarvand, “Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Panickssery, S. R. Bowman, and S. Feng, “LLM Evaluators Recognize and Favor Their Own Generations,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2404.13076</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with Latent Diffusion Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2112.10752</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. Schmid, O. Sanseviero, A. Bartolome, L. von Werra, D. Vila, V. Srivastav, M. Sun, and P. Cuenca, “Llama 3.1 – 405B, 70B &amp; 8B with multilinguality and long context,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/blog/llama31#training-memory-requirements" title="">https://huggingface.co/blog/llama31#training-memory-requirements</a>, last visited: Oct 9, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">et al.</em>, “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
P. Verga, S. Hofstatter, S. Althammer, Y. Su, A. Piktus, A. Arkhangorodsky, M. Xu, N. White, and P. Lewis, “Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2404.18796</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
H. Villamizar, T. Escovedo, and M. Kalinowski, “Requirements Engineering for Machine Learning: A Systematic Mapping Study,” in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)</em>, 2021, pp. 29–36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Wang, Y. Huang, C. Chen, Z. Liu, S. Wang, and Q. Wang, “Software Testing With Large Language Models: Survey, Landscape, and Vision,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">IEEE Transactions on Software Engineering</em>, vol. 50, no. 4, pp. 911–936, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">et al.</em>, “Emergent Abilities of Large Language Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">arXiv preprint arXiv:2206.07682</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS)</em>.   Curran Associates Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">et al.</em>, “Qwen2 Technical Report,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.2.2">arXiv preprint arXiv:2407.10671</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
P. Yu, H. Xu, X. Hu, and C. Deng, “Leveraging generative AI and large language models: A comprehensive roadmap for healthcare integration,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Healthcare</em>, vol. 11, no. 20, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Zhang, X. Huang, J. Ma, Z. Li, Z. Luo, Y. Xie, Y. Qin, T. Luo, Y. Li, S. Liu, Y. Guo, and L. Zhang, “Recognize Anything: A Strong Image Tagging Model,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2306.03514</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et al.</em>, “Judging LLM-as-a-judge with MT-bench and Chatbot Arena,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS)</em>.   Curran Associates Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, “A Survey on Model Compression for Large Language Models,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2308.07633</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 17:20:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
