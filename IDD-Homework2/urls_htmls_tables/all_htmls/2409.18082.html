<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation</title>
<!--Generated on Mon Oct  7 11:59:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.18082v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S1" title="In SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S2" title="In SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S2.SS1" title="In II Related Work ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Robotic Garment Manipulation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S2.SS2" title="In II Related Work ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Synthetic Data for Robotic Garment Manipulation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S2.SS3" title="In II Related Work ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Dense Representations for Garment Manipulation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3" title="In SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS1" title="In III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Synthetic Dataset Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS1.SSS1" title="In III-A Synthetic Dataset Generation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span><span class="ltx_text ltx_font_bold">Garment Mesh Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS1.SSS2" title="In III-A Synthetic Dataset Generation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span><span class="ltx_text ltx_font_bold">Garment Mesh Deformation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS1.SSS3" title="In III-A Synthetic Dataset Generation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span><span class="ltx_text ltx_font_bold">Garment Image Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS1.SSS4" title="In III-A Synthetic Dataset Generation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span><span class="ltx_text ltx_font_bold">Keypoint Generation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS2" title="In III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Paired Keypoint Representation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS3" title="In III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Action Tuple Trajectory Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS4" title="In III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Vision Language Model Fine-Tuning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS4.SSS1" title="In III-D Vision Language Model Fine-Tuning ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>1 </span><span class="ltx_text ltx_font_bold">Model Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS4.SSS2" title="In III-D Vision Language Model Fine-Tuning ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>2 </span><span class="ltx_text ltx_font_bold">Fine-tuning Strategy</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4" title="In SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS1" title="In IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Settings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS2" title="In IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS3" title="In IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Experiental Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS3.SSS1" title="In IV-C Experiental Results ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>1 </span>Comparison on type-specific method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS3.SSS2" title="In IV-C Experiental Results ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span>2 </span>Qualitative Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS4" title="In IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.SS5" title="In IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Discussions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S5" title="In SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin Li<sup class="ltx_sup" id="id16.16.id1"><span class="ltx_text ltx_font_italic" id="id16.16.id1.1">1∗</span></sup>, Siyuan Huang<sup class="ltx_sup" id="id17.17.id2"><span class="ltx_text ltx_font_italic" id="id17.17.id2.1">1,6∗</span></sup>, Qiaojun Yu<sup class="ltx_sup" id="id18.18.id3"><span class="ltx_text ltx_font_italic" id="id18.18.id3.1">1</span></sup>, Zhengkai Jiang<sup class="ltx_sup" id="id19.19.id4"><span class="ltx_text ltx_font_italic" id="id19.19.id4.1">2</span></sup>, 
<br class="ltx_break"/>Ce Hao<sup class="ltx_sup" id="id20.20.id5"><span class="ltx_text ltx_font_italic" id="id20.20.id5.1">3</span></sup>, Yimeng Zhu<sup class="ltx_sup" id="id21.21.id6"><span class="ltx_text ltx_font_italic" id="id21.21.id6.1">4</span></sup>, Hongsheng Li<sup class="ltx_sup" id="id22.22.id7"><span class="ltx_text ltx_font_italic" id="id22.22.id7.1">5</span></sup>, Peng Gao<sup class="ltx_sup" id="id23.23.id8"><span class="ltx_text ltx_font_italic" id="id23.23.id8.1">6</span></sup><sup class="ltx_sup" id="id24.24.id9">🖂</sup> and Cewu Lu<sup class="ltx_sup" id="id25.25.id10"><span class="ltx_text ltx_font_italic" id="id25.25.id10.1">1</span></sup><sup class="ltx_sup" id="id26.26.id11">🖂</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id27.27.id1"><span class="ltx_text ltx_font_italic" id="id27.27.id1.1">1</span></sup>Xin Li, Siyuan Huang, Qiaojun Yu, Cewu Lu are with the Shanghai Jiao Tong University, China. <sup class="ltx_sup" id="id28.28.id2">2</sup>Zhengkai Jiang is with The Hong Kong University of Science and Technology, HongKong. <sup class="ltx_sup" id="id29.29.id3">3</sup>Ce Hao is with Department of Computer Science, National University of Singapore, Singapore. <sup class="ltx_sup" id="id30.30.id4"><span class="ltx_text ltx_font_italic" id="id30.30.id4.1">4</span></sup>Yimeng Zhu is with the Yuandao AI. <sup class="ltx_sup" id="id31.31.id5"><span class="ltx_text ltx_font_italic" id="id31.31.id5.1">5</span></sup>Hongsheng Li is with CUHK-MMLab, Hong Kong. <sup class="ltx_sup" id="id32.32.id6">6</sup>Siyuan Huang and Peng Gao are with the Shanghai AI Lab. * indicates an equal contribution. 🖂Peng Gao and Cewu Lu are the equal corresponding authors, <span class="ltx_text ltx_font_typewriter" id="id33.33.id7">gaopeng@pjlab.org.cn, lucewu@sjtu.edu.cn</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id34.id1">Automating garment manipulation poses a significant challenge for assistive robotics due to the diverse and deformable nature of garments. Traditional approaches typically require separate models for each garment type, which limits scalability and adaptability. In contrast, this paper presents a unified approach using vision-language models (VLMs) to improve keypoint prediction across various garment categories. By interpreting both visual and semantic information, our model enables robots to manage different garment states with a single model. We created a large-scale synthetic dataset using advanced simulation techniques, allowing scalable training without extensive real-world data. Experimental results indicate that the VLM-based method significantly enhances keypoint detection accuracy and task success rates, providing a more flexible and general solution for robotic garment manipulation. In addition, this research also underscores the potential of VLMs to unify various garment manipulation tasks within a single framework, paving the way for broader applications in home automation and assistive robotics for future.
The project page is available at <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://sites.google.com/view/keypoint-garment/home" title="">sites.google.com/view/keypoint-garment</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Garments, as one of the most ubiquitous items in home environments, have long been a focal point in assistive robotics research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. Tasks such as washing, folding, and ironing garments exemplify how robots can assist with everyday household activities. However, despite advancements in robotic vision and manipulation technologies, accurately recognizing and manipulating garments remains a challenge due to their diverse shapes and deformability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>. Robots must not only recognize keypoints on garments but also adapt to the constantly changing states of these flexible objects to perform precise manipulations. This adaptability is essential for ensuring reliable performance in complex real-world scenarios.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditional garment manipulation methods often depend on 3D data and class-specific keypoint recognition models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, which are inherently limited by their inability to generalize across various garment types or states. These models typically perform well only in narrow contexts, struggling to infer or adapt to unknown garment configurations or unstructured states. For instance, a model trained to recognize keypoints on flat garments may fail when faced with a crumpled or folded item, limiting its effectiveness in dynamic environments. These limitations underscore the need for a more flexible and scalable approach capable of handling a wide range of garment states without requiring separate models for each garment category.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of Keypoint Detection Methods. The previous method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> struggles with deformed or ambiguous garment states, leading to inconsistent and incomplete keypoint predictions. In contrast, our SKT  utilizing state-aware paired keypoints and vision-language models (VLMs), achieves more robust and accurate keypoint detection, improving generalization across flat, folded, and deformed garment configurations.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these limitations, we propose state-aware paired keypoint formation, which generates <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">S</span>tate-aware <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">K</span>eypoint <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">T</span>rajectories for vision-language models (SKT). By utilizing the unified paired keypoints trajectory formulation, our proposed method generalizes well to various garment environments, such as flat, folded, and deformed garments (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">1</span></a>). Furthermore, by harnessing the combined power of vision and language, our approach enables robots to interpret visual cues alongside textual descriptions of garment parts and manipulation tasks. the system to go beyond traditional 3D data, providing a more holistic understanding of the garment’s current state and its corresponding keypoints. This integration enables the system to transcend traditional 3D data, offering a more holistic understanding of the garment’s current state and its corresponding keypoints. Finally, the use of VLMs enables the robot to dynamically adapt to various garment states, as the model can process both visual features and language-based queries related to the manipulation tasks at hand.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To train our model, we created a synthetic dataset that covers a wide range of garment configurations, including flat, deformed, and folded states. By leveraging advanced physics simulators and rendering technologies, we simulated realistic garment deformations that represent the conditions encountered during robotic manipulation tasks. The synthetic dataset enhances scalability by eliminating the need for labor-intensive real-world data collection, enabling the model to generalize across a broad spectrum of garment types and configurations. Furthermore, by simulating garment deformations and creating associated text queries, we train the robot to predict keypoints using both visual and semantic information. This improves the robot’s ability to perform manipulation tasks, such as folding or rearranging garments, with greater precision and flexibility.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Notably, a key aspect of our approach is the introduction of reasoning-based vision-language tasks. It further optimize keypoint trajectories by enabling the robot to reason about garment states and adjust its actions accordingly. For example, if a garment is partially folded or deformed, the robot can infer the most relevant keypoints from the visual context and the provided semantic descriptions. This level of reasoning is essential for managing the complexities associated with deformable objects such as garments.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contributions of this study include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A unified paired keypoint trajectories formulation that integrates vision-language models to enhance robotic manipulation of garments. By combining visual and semantic information, the method enables robots to adapt to a wide variety of garment states and configurations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The creation of a large-scale synthetic dataset covers diverse garment states, improving the robot’s ability to generalize across various types of garments and manipulation scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose reasoning-based vision-language tasks that further improve keypoint detection by enabling the robot to infer and adjust to changing garment states, enhancing precision and adaptability in real-world applications.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Robotic Garment Manipulation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Robotic manipulation of garments is a critical challenge in assistive robotics due to the deformable and highly variable nature of clothing items <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Much of the existing research has focused on key tasks such as unfolding (flattening) and folding garments. While unfolding systems have made notable progress <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, they often leave garments imperfectly flattened, and their ability to handle a wide variety of clothing types and environmental conditions remains limited.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Traditionally, folding tasks for flattened garments rely on predefined state representations and scripted policies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. Researchers have explored various approaches for generating these state representations, including template fitting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and semantic keypoint detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. Many of these methods leverage depth images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, which are advantageous for capturing geometric data unaffected by lighting or background clutter. However, depth images can omit critical visual information such as garment patterns and seams, which can be important for accurate manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. To address these limitations, our work utilizes RGB images, which offer richer visual information that can be crucial for precise garment manipulation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Synthetic Data for Robotic Garment Manipulation</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The use of synthetic data has become increasingly prevalent in robotic cloth manipulation, particularly for training models that need to generalize across diverse garment configurations and manipulation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. However, creating high-quality 3D assets that represent a wide variety of garments and states remains a significant challenge. Some approaches rely on limited sets of manually annotated pre-made cloth meshes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, while others use procedural generation techniques to produce single-layer meshes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. Although large-scale datasets like Cloth3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> have been developed, they often lack detailed semantic annotations required for precise manipulation tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In this work, we extend the use of synthetic data to improve keypoint detection in garment manipulation tasks. Our pipeline generates diverse garment configurations, including varying deformations, allowing us to train models that can adapt to different garment types, states, and environmental conditions. By leveraging this approach, we aim to enhance the robot’s ability to recognize key manipulation points on garments, enabling more efficient and accurate operations in real-world scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Dense Representations for Garment Manipulation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Dense object descriptors, which capture point- or pixel-level object representations, have been widely applied to various robotic manipulation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>. These descriptors have been extended in numerous works to propose grasping poses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, manipulate deformable objects like ropes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, and smooth fabrics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. Additionally, point-level affordance learning has been explored for articulated objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, deformable objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, and even in tasks involving language-guided <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and bimanual manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. These approaches enable more effective interaction and contact point selection, facilitating a variety of downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>.
In this paper, our work extends the use of dense correspondence by applying these dense point representations specifically to deformable garment manipulation. By leveraging point-level affordance learning, we aim to enhance the robot’s ability to detect manipulation-relevant keypoints and improve performance across diverse garment states.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S2.F2.g1" src="extracted/5907054/figure/architecture3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a)The <span class="ltx_text ltx_font_bold" id="S2.F2.2.1">overall framework</span> of state-aware keypoint trajectory (SKT). SKT generates action trajectories for clothes manipulation by leveraging a fine-tuned vision-language model for state-aware paired keypoint and action generation through the action decoder (b). </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we provide a detailed introduction to our proposed vision language keypoint prediction method for robotic garment manipulation. Our approach is based on the integration of vision-language models to enhance the robot’s ability to recognize keypoints on garments in different states in a unimodel, improving both flexibility and accuracy during manipulation tasks. To achieve this, we have developed a synthetic garment dataset and designed a keypoint prediction framework, which are described in the following subsections.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Synthetic Dataset Generation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To simulate the diverse and deformable nature of garments, we developed a synthetic dataset that encompasses a broad spectrum of garment configurations, ranging from flat to deformed and folded states. By leveraging advanced physics simulation tools like Blender, we modeled various household garments such as shirts, shorts, and towels. The dataset was structured to capture different deformation stages by applying realistic physical forces, thus simulating the conditions typically encountered during robotic manipulation tasks. The challenge of working with deformable objects required not only generating cloth meshes but also simulating a range of possible configurations, ensuring a broad distribution of garment deformations. The following sections provide a detailed explanation of the processes involved in mesh generation, deformation simulation, and the generation of corresponding images and keypoint annotations.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.6.2">Garment Mesh Generation</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The initial step involves the creation of garment meshes in multiple folded states. We began by defining a set of 2D boundary vertices based on templates specific to each garment type, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. These vertices were connected using Bézier curves to form single-layer meshes, enhancing realism by representing features such as the neckline of a T-shirt with smooth edges. Parameters for the mesh skeleton, Bézier curves, and corner rounding radii were sampled from carefully calibrated ranges to ensure variability. The meshes were then triangulated with edge lengths constrained to 1 cm, and UV maps were generated to allow for subsequent texturing. During the folding process, we tracked vertices corresponding to key semantic regions, enabling automatic keypoint labeling at different folding stages.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.5.1.1">III-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.6.2">Garment Mesh Deformation</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Using Blender’s physics simulation capabilities, we deformed the generated garment meshes by applying randomized orientations and allowing them to fall naturally to create wrinkles. Additionally, we simulated folding motions by performing circular grasping actions. The cloth was also lifted and rotated to produce both visible and hidden folds, with the focus on generating realistic fold patterns rather than fully crumpled states. Key physics properties such as bending stiffness, stretching, friction, and drag were randomized to increase diversity, and parameters were fine-tuned to maintain physical plausibility.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.5.1.1">III-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.6.2">Garment Image Generation</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">To generate high-quality synthetic images, we applied textures sourced from PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> to both the environment and the folding surface. Cloth meshes were solidified and textured, and distractor objects from the Google Scanned Objects dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> were strategically placed in the scene to improve keypoint detection in varied environments. Cameras were positioned randomly around the garment, and the Cycles rendering engine was employed to produce photorealistic images with a range of lighting and viewpoint variations. A few examples of the synthetic image
can be found in Fig.  <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.F3" title="Figure 3 ‣ III-B Paired Keypoint Representation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.5.1.1">III-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS4.6.2">Keypoint Generation</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">For keypoint generation, we employed a raycasting technique to verify the visibility of vertices corresponding to each keypoint. Following the methodology in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, we aimed to align synthetic labels with human annotations, recognizing that human-labeled keypoints may not always precisely match the ground truth. A keypoint was considered visible if any vertex within its 2-ring neighborhood was visible in the rendered image. Our experiments demonstrated that this approach better replicated human annotation patterns, resulting in more accurate and robust synthetic labels.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Paired Keypoint Representation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In garment manipulation, a set of keypoints—potentially represented as a nested structure, such as a skeleton—can effectively capture the state of a garment, regardless of its condition (e.g., degrees of wrinkling) or original shapes (including sizes and styles) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Compared to general representation formats used in MLLMs, such as bounding boxes, which may lack visual constraints in certain positions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, keypoints retain critical structural information. This makes them more effective for capturing the nuances of garment shapes and configurations, as illustrated in Fig. 1. Moreover, downstream policies primarily focus on the operational relationships between corresponding keypoints—essentially aligning Point A to Point B. By utilizing keypoint tuples for representation, we ensure compatibility with these existing policies, facilitating seamless integration into various manipulation strategies. To guide our model in learning the keypoint representation, we use the keypoints generated in the previous steps and formulate keypoint detection as a Visual Question Answering (VQA) task.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="488" id="S3.F3.g1" src="extracted/5907054/figure/image-generation2.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A sample set comprises synthetic images depicting different fold states with corresponding paired action keypoints annotations.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Action Tuple Trajectory Generation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3">Based on the keypoint representation proposed before, we can frame the task of garment folding action prediction as first implicitly identifying a set if keypoint coordinates tuple <math alttext="\{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),(x_{4},y_{4})\}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.4"><semantics id="S3.SS3.p1.1.m1.4a"><mrow id="S3.SS3.p1.1.m1.4.4.4" xref="S3.SS3.p1.1.m1.4.4.5.cmml"><mo id="S3.SS3.p1.1.m1.4.4.4.5" stretchy="false" xref="S3.SS3.p1.1.m1.4.4.5.cmml">{</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p1.1.m1.1.1.1.1.2.4" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.1.1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.2.2.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.1.m1.1.1.1.1.2.2.3" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2.3.cmml">1</mn></msub><mo id="S3.SS3.p1.1.m1.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.4.4.4.6" xref="S3.SS3.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS3.p1.1.m1.2.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml"><mo id="S3.SS3.p1.1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.p1.1.m1.2.2.2.2.1.1" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1.cmml"><mi id="S3.SS3.p1.1.m1.2.2.2.2.1.1.2" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1.2.cmml">x</mi><mn id="S3.SS3.p1.1.m1.2.2.2.2.1.1.3" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1.3.cmml">2</mn></msub><mo id="S3.SS3.p1.1.m1.2.2.2.2.2.4" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.2.2.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.4.4.4.7" xref="S3.SS3.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS3.p1.1.m1.3.3.3.3.2" xref="S3.SS3.p1.1.m1.3.3.3.3.3.cmml"><mo id="S3.SS3.p1.1.m1.3.3.3.3.2.3" stretchy="false" xref="S3.SS3.p1.1.m1.3.3.3.3.3.cmml">(</mo><msub id="S3.SS3.p1.1.m1.3.3.3.3.1.1" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1.cmml"><mi id="S3.SS3.p1.1.m1.3.3.3.3.1.1.2" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1.2.cmml">x</mi><mn id="S3.SS3.p1.1.m1.3.3.3.3.1.1.3" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1.3.cmml">3</mn></msub><mo id="S3.SS3.p1.1.m1.3.3.3.3.2.4" xref="S3.SS3.p1.1.m1.3.3.3.3.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.3.3.3.3.2.2" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2.cmml"><mi id="S3.SS3.p1.1.m1.3.3.3.3.2.2.2" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.1.m1.3.3.3.3.2.2.3" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2.3.cmml">3</mn></msub><mo id="S3.SS3.p1.1.m1.3.3.3.3.2.5" stretchy="false" xref="S3.SS3.p1.1.m1.3.3.3.3.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.4.4.4.8" xref="S3.SS3.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS3.p1.1.m1.4.4.4.4.2" xref="S3.SS3.p1.1.m1.4.4.4.4.3.cmml"><mo id="S3.SS3.p1.1.m1.4.4.4.4.2.3" stretchy="false" xref="S3.SS3.p1.1.m1.4.4.4.4.3.cmml">(</mo><msub id="S3.SS3.p1.1.m1.4.4.4.4.1.1" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1.cmml"><mi id="S3.SS3.p1.1.m1.4.4.4.4.1.1.2" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1.2.cmml">x</mi><mn id="S3.SS3.p1.1.m1.4.4.4.4.1.1.3" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1.3.cmml">4</mn></msub><mo id="S3.SS3.p1.1.m1.4.4.4.4.2.4" xref="S3.SS3.p1.1.m1.4.4.4.4.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.4.4.4.4.2.2" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2.cmml"><mi id="S3.SS3.p1.1.m1.4.4.4.4.2.2.2" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.1.m1.4.4.4.4.2.2.3" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2.3.cmml">4</mn></msub><mo id="S3.SS3.p1.1.m1.4.4.4.4.2.5" stretchy="false" xref="S3.SS3.p1.1.m1.4.4.4.4.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.4.4.4.9" stretchy="false" xref="S3.SS3.p1.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.4b"><set id="S3.SS3.p1.1.m1.4.4.5.cmml" xref="S3.SS3.p1.1.m1.4.4.4"><interval closure="open" id="S3.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2"><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2">𝑥</ci><cn id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2.2">𝑦</ci><cn id="S3.SS3.p1.1.m1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.1.1.2.2.3">1</cn></apply></interval><interval closure="open" id="S3.SS3.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2"><apply id="S3.SS3.p1.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1.2">𝑥</ci><cn id="S3.SS3.p1.1.m1.2.2.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.2.2.2.2.1.1.3">2</cn></apply><apply id="S3.SS3.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.1.m1.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.2.2.2.2.2.2.3">2</cn></apply></interval><interval closure="open" id="S3.SS3.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.2"><apply id="S3.SS3.p1.1.m1.3.3.3.3.1.1.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.3.3.3.3.1.1.1.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.3.3.3.3.1.1.2.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1.2">𝑥</ci><cn id="S3.SS3.p1.1.m1.3.3.3.3.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.3.3.3.3.1.1.3">3</cn></apply><apply id="S3.SS3.p1.1.m1.3.3.3.3.2.2.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.3.3.3.3.2.2.1.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.3.3.3.3.2.2.2.cmml" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2.2">𝑦</ci><cn id="S3.SS3.p1.1.m1.3.3.3.3.2.2.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.3.3.3.3.2.2.3">3</cn></apply></interval><interval closure="open" id="S3.SS3.p1.1.m1.4.4.4.4.3.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.2"><apply id="S3.SS3.p1.1.m1.4.4.4.4.1.1.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.4.4.4.4.1.1.1.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.4.4.4.4.1.1.2.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1.2">𝑥</ci><cn id="S3.SS3.p1.1.m1.4.4.4.4.1.1.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.4.4.4.4.1.1.3">4</cn></apply><apply id="S3.SS3.p1.1.m1.4.4.4.4.2.2.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.4.4.4.4.2.2.1.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.4.4.4.4.2.2.2.cmml" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2.2">𝑦</ci><cn id="S3.SS3.p1.1.m1.4.4.4.4.2.2.3.cmml" type="integer" xref="S3.SS3.p1.1.m1.4.4.4.4.2.2.3">4</cn></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.4c">\{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3}),(x_{4},y_{4})\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.4d">{ ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) }</annotation></semantics></math> that correspond to the optimal grasping points for folding, based on the garment’s state. These keypoints are then formulated into action tuples, such as <math alttext="LA((x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}))" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.2"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.2.2.4" xref="S3.SS3.p1.2.m2.2.2.4.cmml">L</mi><mo id="S3.SS3.p1.2.m2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.3.cmml">⁢</mo><mi id="S3.SS3.p1.2.m2.2.2.5" xref="S3.SS3.p1.2.m2.2.2.5.cmml">A</mi><mo id="S3.SS3.p1.2.m2.2.2.3a" xref="S3.SS3.p1.2.m2.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.p1.2.m2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml"><mo id="S3.SS3.p1.2.m2.2.2.2.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">(</mo><mrow id="S3.SS3.p1.2.m2.1.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml"><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml">(</mo><msubsup id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml">x</mi><mn id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml">1</mn><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.3.cmml">l</mi></msubsup><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.2.4" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml">,</mo><msubsup id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.3.cmml">1</mn><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.2.m2.2.2.2.2.4" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.2.m2.2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml"><mo id="S3.SS3.p1.2.m2.2.2.2.2.2.2.3" stretchy="false" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.2.cmml">x</mi><mn id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.3.cmml">2</mn><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.3.cmml">l</mi></msubsup><mo id="S3.SS3.p1.2.m2.2.2.2.2.2.2.4" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.3.cmml">2</mn><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS3.p1.2.m2.2.2.2.2.2.2.5" stretchy="false" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.2.m2.2.2.2.2.5" stretchy="false" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><apply id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2"><times id="S3.SS3.p1.2.m2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.3"></times><ci id="S3.SS3.p1.2.m2.2.2.4.cmml" xref="S3.SS3.p1.2.m2.2.2.4">𝐿</ci><ci id="S3.SS3.p1.2.m2.2.2.5.cmml" xref="S3.SS3.p1.2.m2.2.2.5">𝐴</ci><interval closure="open" id="S3.SS3.p1.2.m2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2"><interval closure="open" id="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2"><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.2">𝑥</ci><cn id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.3">𝑙</ci></apply><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.3">𝑙</ci></apply></interval><interval closure="open" id="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2"><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.2">𝑥</ci><cn id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.2.3">2</cn></apply><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.1.1.3">𝑙</ci></apply><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.3">𝑙</ci></apply></interval></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">LA((x_{1}^{l},y_{1}^{l}),(x_{2}^{l},y_{2}^{l}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.2d">italic_L italic_A ( ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) )</annotation></semantics></math> and <math alttext="RA((x_{1}^{r},y_{1}^{r}),(x_{2}^{r},y_{2}^{r}))" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.2"><semantics id="S3.SS3.p1.3.m3.2a"><mrow id="S3.SS3.p1.3.m3.2.2" xref="S3.SS3.p1.3.m3.2.2.cmml"><mi id="S3.SS3.p1.3.m3.2.2.4" xref="S3.SS3.p1.3.m3.2.2.4.cmml">R</mi><mo id="S3.SS3.p1.3.m3.2.2.3" xref="S3.SS3.p1.3.m3.2.2.3.cmml">⁢</mo><mi id="S3.SS3.p1.3.m3.2.2.5" xref="S3.SS3.p1.3.m3.2.2.5.cmml">A</mi><mo id="S3.SS3.p1.3.m3.2.2.3a" xref="S3.SS3.p1.3.m3.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.p1.3.m3.2.2.2.2" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml"><mo id="S3.SS3.p1.3.m3.2.2.2.2.3" stretchy="false" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml">(</mo><mrow id="S3.SS3.p1.3.m3.1.1.1.1.1.2" xref="S3.SS3.p1.3.m3.1.1.1.1.1.3.cmml"><mo id="S3.SS3.p1.3.m3.1.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.p1.3.m3.1.1.1.1.1.3.cmml">(</mo><msubsup id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.cmml">x</mi><mn id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml">1</mn><mi id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.3.cmml">r</mi></msubsup><mo id="S3.SS3.p1.3.m3.1.1.1.1.1.2.4" xref="S3.SS3.p1.3.m3.1.1.1.1.1.3.cmml">,</mo><msubsup id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.2" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.3" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.3.cmml">1</mn><mi id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.3" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.3.cmml">r</mi></msubsup><mo id="S3.SS3.p1.3.m3.1.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.p1.3.m3.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.3.m3.2.2.2.2.4" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.3.m3.2.2.2.2.2.2" xref="S3.SS3.p1.3.m3.2.2.2.2.2.3.cmml"><mo id="S3.SS3.p1.3.m3.2.2.2.2.2.2.3" stretchy="false" xref="S3.SS3.p1.3.m3.2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.cmml"><mi id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.2" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.2.cmml">x</mi><mn id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.3" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.3.cmml">2</mn><mi id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.3" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.3.cmml">r</mi></msubsup><mo id="S3.SS3.p1.3.m3.2.2.2.2.2.2.4" xref="S3.SS3.p1.3.m3.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.2" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.3" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml">2</mn><mi id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.3" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.3.cmml">r</mi></msubsup><mo id="S3.SS3.p1.3.m3.2.2.2.2.2.2.5" stretchy="false" xref="S3.SS3.p1.3.m3.2.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS3.p1.3.m3.2.2.2.2.5" stretchy="false" xref="S3.SS3.p1.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.2b"><apply id="S3.SS3.p1.3.m3.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2"><times id="S3.SS3.p1.3.m3.2.2.3.cmml" xref="S3.SS3.p1.3.m3.2.2.3"></times><ci id="S3.SS3.p1.3.m3.2.2.4.cmml" xref="S3.SS3.p1.3.m3.2.2.4">𝑅</ci><ci id="S3.SS3.p1.3.m3.2.2.5.cmml" xref="S3.SS3.p1.3.m3.2.2.5">𝐴</ci><interval closure="open" id="S3.SS3.p1.3.m3.2.2.2.3.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2"><interval closure="open" id="S3.SS3.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2"><apply id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2">𝑥</ci><cn id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.1.1.3">𝑟</ci></apply><apply id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1.2.2.3">𝑟</ci></apply></interval><interval closure="open" id="S3.SS3.p1.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2"><apply id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1">superscript</csymbol><apply id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.2">𝑥</ci><cn id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.2.3">2</cn></apply><ci id="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.3.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.1.1.3">𝑟</ci></apply><apply id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.3.m3.2.2.2.2.2.2.2.3">𝑟</ci></apply></interval></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.2c">RA((x_{1}^{r},y_{1}^{r}),(x_{2}^{r},y_{2}^{r}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.2d">italic_R italic_A ( ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) )</annotation></semantics></math>, where LA and RA represent the ”left arm” and ”right arm,” respectively, highlighting our method’s capability to support bi-arm manipulation. This tuple representation not only links points, providing more structured information than discrete keypoints, but also ensures compatibility with robotic action primitives. By focusing on these key coordinates, the model can directly translate them into manipulation actions like grasping and folding, thereby enhancing precision. After obtaining the paired action point, our action decoder generates an action trajectory conditioned on the action primitive. The action model
is based on manually designed rules. Furthermore, the proposed SKT is adaptable to various garment types and folding scenarios, effectively guiding robotic movements and identifying key contact points. Training with multi-task data further enhances the model’s generalization across diverse robotic manipulation tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Vision Language Model Fine-Tuning</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS1.5.1.1">III-D</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.6.2">Model Architecture</span>
</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">As shown in  <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S2.F2" title="Figure 2 ‣ II-C Dense Representations for Garment Manipulation ‣ II Related Work ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">2</span></a>,
our vision language model is built upon the SPHINX-X framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, with LLaMA2 serving as its core language backbone. We chose this model due to its unique capability to concentrate on fine-grained, region-specific details of objects, making it well-suited for tasks requiring detailed visual analysis. Our model employs the ”any resolution” strategy introduced by SPHINX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>. The input images are divided into smaller sub-images, after which the visual encoders process them to extract essential features. Given the dual need for both global and local visual comprehension in manipulation tasks, we integrate several image encoders: CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> and DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> to capture localized semantic information, and QFormer  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> to summarize global features. These local and global features are then concatenated at the channel level. The spatial alignment between visual tokens and their corresponding language tokens is managed through projection layers.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS2.5.1.1">III-D</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.6.2">Fine-tuning Strategy</span>
</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.2">Our SKT training approach follows standard VQA methodology, embedding garment understanding details into natural language structures. To address visual domain gaps between our specialized dataset and generic imagery, as well as task representation gaps between action tuples and general image descriptions, we implement a two-phase fine-tuning approach. In the initial phase, we fine-tune visual projection layers using simple keypoint detection tasks. Templates like ”Please detect the keypoints used to manipulate the [garment-type]” are employed. Outputs are keypoints of the garment, prefixed with ”<math alttext="&lt;kp&gt;" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.1.m1.1"><semantics id="S3.SS4.SSS2.p1.1.m1.1a"><mrow id="S3.SS4.SSS2.p1.1.m1.1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml"><mo fence="true" id="S3.SS4.SSS2.p1.1.m1.1.1.1.2" rspace="0em" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS4.SSS2.p1.1.m1.1.1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.2" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.2.cmml">k</mi><mo id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.3.cmml">p</mi></mrow><mo fence="true" id="S3.SS4.SSS2.p1.1.m1.1.1.1.3" lspace="0em" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.1b"><apply id="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.2">expectation</csymbol><apply id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1"><times id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.2">𝑘</ci><ci id="S3.SS4.SSS2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.1.1.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.1c">&lt;kp&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p1.1.m1.1d">&lt; italic_k italic_p &gt;</annotation></semantics></math>” to indicate their nature and facilitate post-processing. The second phase involves simultaneous fine-tuning of visual projection layers and the language model using an instruction-following dataset. We use templates like ”Please provide actions for folding the [garment-type]”, with the MLLM generating action tuples as described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S3.SS3" title="III-C Action Tuple Trajectory Generation ‣ III Method ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>. These outputs are prefixed with ”<math alttext="&lt;action&gt;" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.2.m2.1"><semantics id="S3.SS4.SSS2.p1.2.m2.1a"><mrow id="S3.SS4.SSS2.p1.2.m2.1.1.1" xref="S3.SS4.SSS2.p1.2.m2.1.1.2.cmml"><mo fence="true" id="S3.SS4.SSS2.p1.2.m2.1.1.1.2" rspace="0em" xref="S3.SS4.SSS2.p1.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS4.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.2" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.2.cmml">a</mi><mo id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.3" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.3.cmml">c</mi><mo id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1a" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.4" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.4.cmml">t</mi><mo id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1b" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.5" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.5.cmml">i</mi><mo id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1c" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.6" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.6.cmml">o</mi><mo id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1d" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.7" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.7.cmml">n</mi></mrow><mo fence="true" id="S3.SS4.SSS2.p1.2.m2.1.1.1.3" lspace="0em" xref="S3.SS4.SSS2.p1.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.2.m2.1b"><apply id="S3.SS4.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.2">expectation</csymbol><apply id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1"><times id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.2">𝑎</ci><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.3">𝑐</ci><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.4.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.4">𝑡</ci><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.5.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.5">𝑖</ci><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.6.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.6">𝑜</ci><ci id="S3.SS4.SSS2.p1.2.m2.1.1.1.1.7.cmml" xref="S3.SS4.SSS2.p1.2.m2.1.1.1.1.7">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.2.m2.1c">&lt;action&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p1.2.m2.1d">&lt; italic_a italic_c italic_t italic_i italic_o italic_n &gt;</annotation></semantics></math>” for clarity and ease of extraction. This staged approach gradually guides the MLLM to first understand garment keypoints, then link these point tuples with specific actions and instructions. In the second stage, we train the MLLM on both keypoint detection and action tuple generation tasks, reducing the ratio of keypoint detection tasks.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Resuls on aRTF dataset.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S4.T1.2">
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_border_tt" id="S4.T1.2.2.3"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.2.2.4"></td>
<td class="ltx_td ltx_border_tt" id="S4.T1.2.2.5"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1">mAP<sub class="ltx_sub" id="S4.T1.1.1.1.1.1">2,4,8</sub> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1">AKD(<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">↓</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3">
<td class="ltx_td ltx_align_left" id="S4.T1.2.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.1.1">Data Source</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.2.1">Type</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.3.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.4">T-shirt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.5">Shorts</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.6">Towel</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.7">T-shirt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.8">Shorts</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.3.9">Towel</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.4.1" rowspan="4"><span class="ltx_text" id="S4.T1.2.4.1.1">Sim-to-Real</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.2" rowspan="3"><span class="ltx_text" id="S4.T1.2.4.2.1">Type-Specific <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.3">T-shirt Detector</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.4">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.7">14.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5">
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.1">Shorts Detector</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.3">51.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.6">27.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.7">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6">
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.1">Towel Detector</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.4">83.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.7">13.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.1">Uni-model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.2">SKT (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.3">63.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.4">56.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.5">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.6">8.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.7">10.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.7.8">3.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="S4.T1.2.8.1" rowspan="5"><span class="ltx_text" id="S4.T1.2.8.1.1">Sim+Real-to-Real</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.2" rowspan="3"><span class="ltx_text" id="S4.T1.2.8.2.1">Type-Specific <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.3">T-shirt Detector</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.4">69.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.7">8.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.8.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9">
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.1">Shorts Detector</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.3">64.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.6">11.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.9.7">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.10">
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.1">Towel Detector</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.4">88.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.10.7">6.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.11">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.11.1" rowspan="2"><span class="ltx_text" id="S4.T1.2.11.1.1">Uni-model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.2">SKT_KP (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.3">71.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.4">60.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.5">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.6">6.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.7">7.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.11.8">2.0</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.1">SKT (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.2">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.3">59.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.4">86.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.5">8.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.6">7.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.12.7">3.0</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Comparison with previous methods under different training data settings. The table compares the performance of type-specific models and the unified SKT model in Sim-to-Real and Sim+Real-to-Real training settings.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Experimental Settings</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To train and evaluate the effectiveness of the proposed method SKT, we not only used synthetic data but also introduced the aRTF clothing dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. aRTF dataset is collected from 14 real-world domestic settings, each characterized by unique environmental features and a variety of clothing items. It is meticulously segmented into 6 training scenes and 8 testing scenes, encompassing 15 towels and T-shirts within the training set, and an increased quantity of 20 in the test set. Similarly, the dataset comprises 8 shorts for training and 9 for testing. Notably, for each category, the dataset offers 210 images for training and an expanded set of 400 images for testing, with the exception of shorts, which are represented by 112 training images and 180 test images. More formally, we train the SKT model on a syntehtic dataset of 20,000 images and integrate the real-world data by employing fine-tuning techniques on the aRTF training dataset, thereby enhancing SKT’s adaptability and accuracy across various garment types, including T-shirts, shorts, and towels.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We fine-tuned the SKT within the SPHINX framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> and utilized eight NVIDIA A100 GPUs, each equipped with 80 GB. The fine-tuning process was completed over 3 epochs, with a total runtime of approximately 4 hours. The
visual encoders were kept frozen throughout the fine-tuning stage to preserve the pre-trained feature quality The SPHINX1K model, obtained directly from the official repository, served as our pre-trained foundation model. The training was performed with a batch size of 4 and the learning rate was set to <math alttext="2\times 10{{}^{-5}}" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1b"><mn id="S4.SS1.p2.1.m1.1.1">2</mn><mo id="S4.SS1.p2.1.m1.1.2" lspace="0.222em" rspace="0.222em">×</mo><mn id="S4.SS1.p2.1.m1.1.3">10</mn><msup id="S4.SS1.p2.1.m1.1.4"><mi id="S4.SS1.p2.1.m1.1.4a"></mi><mrow id="S4.SS1.p2.1.m1.1.4.1"><mo id="S4.SS1.p2.1.m1.1.4.1a">−</mo><mn id="S4.SS1.p2.1.m1.1.4.1.2">5</mn></mrow></msup></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">2\times 10{{}^{-5}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">2 × 10 start_FLOATSUPERSCRIPT - 5 end_FLOATSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Metrics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To quantitatively analyze the advantages of the SKT  method, we adhere to the experimental setup delineated in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> and have conducted the following metric calculations on the aRTF dataset: <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Mean Average Precision (mAP)</span> is crucial for evaluating the accuracy of our model in detecting keypoints. We have calculated the mAP at three distinct L2 distance thresholds 2, 4, and 8 pixels from the ground truth keypoints. This approach allows us to measure the model’s precision at various letolerance levelswhich is essential for understanding its robustness in different scenarios.
<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">Average Keypoint Distance (AKD)</span> serves as a direct measure of the geometric accuracy of the predicted keypoints relative to the ground truth. Leveraging the AKD is instrumental in assessing the model’s spatial localization accuracy, thereby providing overall precision in keypoint detection.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Experiental Results</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS1.5.1.1">IV-C</span>1 </span>Comparison on type-specific method</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">In this experiment, we evaluate the performance of our SKT model and compare it against type-specific models under two different training settings: Sim-to-Real and Sim+Real-to-Real. The Sim-to-Real setting involves training the models solely on synthetic data, while the Sim+Real-to-Real setting incorporates both synthetic and real data during training. The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.T1" title="TABLE I ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p2.1.1">Sim-to-Real:</span> Under the Sim-to-Real setting, the SKT model significantly outperforms the type-specific models across all garment types. The SKT model achieves an mAP<sub class="ltx_sub" id="S4.SS3.SSS1.p2.1.2">2,4,8</sub> of 63.3 for T-shirts, 56.7 for shorts, and 83.9 for towels, notably surpassing the best-performing type-specific models. In contrast, the type-specific T-shirt, Shorts, and Towel Detectors achieve only 58.2, 51.4, and 83.2 in mAP<sub class="ltx_sub" id="S4.SS3.SSS1.p2.1.3">2,4,8</sub> , respectively. More importantly, the SKT model delivers substantially better keypoint accuracy, with an AKD of 6.7 pixels for T-shirts, 2.9 pixels for shorts, and 3.4 pixels for towels, while the type-specific models exhibit much higher error rates (e.g., 14.0 pixels for T-shirts and 27.3 pixels for shorts).</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="472" id="S4.F4.g1" src="x2.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative visual comparison. <span class="ltx_text ltx_font_bold" id="S4.F4.3.1">(a)</span> The previous approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> struggles to handle deformed or ambiguous garment states, often resulting in incomplete and inconsistent keypoint predictions. <span class="ltx_text ltx_font_bold" id="S4.F4.4.2">(b)</span> In contrast, our method provides more robust and accurate keypoint detection across diverse garment configurations, as demonstrated through improved visualization and performance.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">These results highlight the strength of our unified approach. By learning across different garment types, the SKT model generalizes better than the one-to-one type-specific models, which struggle with the complexity and variability in garment configurations. The superior performance of the SKT model under the Sim-to-Real setting demonstrates its ability to generalize well without requiring specific models for each garment type, effectively overcoming the limitations of type-specific learning.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p4">
<p class="ltx_p" id="S4.SS3.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p4.1.1">Sim+Real-to-Real:</span> Incorporating real-world data in training further enhances SKT’s performance. SKT generates action tuple keypoints and trajectories, achieving mAP<sub class="ltx_sub" id="S4.SS3.SSS1.p4.1.2">2,4,8</sub> scores of 66.8, 59.9, and 86.8 for T-shirts, shorts, and towels respectively, with improved keypoint accuracies of 8.1, 7.1, and 3.0 pixels. SKT_KP, which only predicts keypoints (similar to type-specific methods), achieves mAP<sub class="ltx_sub" id="S4.SS3.SSS1.p4.1.3">2,4,8</sub> scores of 71.7, 60.3, and 88.2, with keypoint accuracies improved by 6.8, 3.9, and 2.0 pixels for the same garment types.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p5">
<p class="ltx_p" id="S4.SS3.SSS1.p5.1">Our unified architecture enables transferable learning across garments, performing robustly even with complex deformations or ambiguous states. This single-model approach simplifies structure and training while outperforming type-specific models. It also mitigates overfitting risks, enhancing generalization across varied garment configurations and tasks. SKP_KP results indicate that current metrics primarily reflect keypoint detection and that action tuple detection is more challenging than pure keypoint detection.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS3.SSS2.5.1.1">IV-C</span>2 </span>Qualitative Analysis</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.F4" title="Figure 4 ‣ IV-C1 Comparison on type-specific method ‣ IV-C Experiental Results ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">4</span></a>, we analyze common failure cases of type-specific models during clothes manipulation tasks. These models often struggle with deformed or ambiguous garment states, such as highly crumpled or partially occluded clothing, resulting in incomplete, inconsistent keypoint predictions. For instance, when folds or complex garment shapes are present, type-specific models frequently misidentify semantic locations, generating false positives and misaligned keypoints. In the first row of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.F4" title="Figure 4 ‣ IV-C1 Comparison on type-specific method ‣ IV-C Experiental Results ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">4</span></a>, type-specific models confuse key semantic locations in highly folded garments, while the second row illustrates further failures caused by overlapping fabric layers or ambiguous shapes, leading to significant errors in keypoint detection.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">In contrast, our SKT model (b) offers a more robust and reliable solution by leveraging a fine-tuned vision-language model for general clothes manipulation. This allows the SKT model to handle diverse garment configurations with more consistent and accurate keypoint predictions. The visualizations in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.F4" title="Figure 4 ‣ IV-C1 Comparison on type-specific method ‣ IV-C Experiental Results ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">4</span></a> clearly demonstrate the SKT model’s ability to cope with challenging garment deformations and ambiguous states, resulting in fewer errors and more precise keypoint detection. The improved robustness of our model is also reflected in quantitative performance, where the SKT model significantly reduces false positives and enhances detection accuracy in scenarios where type-specific models struggle.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To analyze SKT’s design contributions, we conducted ablation studies following the original Sim-To-Real setting. Specifically, we examined three variations: 1)Replacing the ”Any-Resolution” method from SPHINX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> with a default 224×224 image resolution for visual input. 2)Eliminating key-point detection tasks during training, focusing solely on direct trajectory tuple learning. 3)Implementing one-stage training instead of two-stage training.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18082v2#S4.T2" title="TABLE II ‣ IV-D Ablation Study ‣ IV Experiments ‣ SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation"><span class="ltx_text ltx_ref_tag">II</span></a> illustrates that without high-resolution image input, model performance significantly deteriorates. We attribute this to the necessity of detailed visual information for cloth manipulation, as manipulation targets typically comprise only a small portion of the garment. Omitting key-point detection tasks also leads to suboptimal performance. We posit that these tasks serve as a bridge between the final action representation and pretrained general VQA tasks, facilitating better learning of action representation. Moreover, key-point detection tasks increase the volume of training samples, benefiting MLLM model training. Lastly, abandoning the staged training strategy results in performance degradation, aligning with previous findings that staged training aids MLLMs in adapting to new task formats.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Ablation study</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:411.9pt;height:118.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(49.6pt,-14.3pt) scale(1.31761307983242,1.31761307983242) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.2.2">
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">High-Res</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">KP Task</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.2.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">Staged Training</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">mAP<sub class="ltx_sub" id="S4.T2.1.1.1.1.1.1">2,4,8</sub> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.2.1">AKD(<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.1.m1.1d">↓</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4">
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">64.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.5">
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✕</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.2.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.2.6.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T2.2.2.6.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.2.6.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.9</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.2.2.6.5" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.7</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">Discussions</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">We manually collected additional data that is not available in the existing dataset to evaluate the model’s robustness, including samples with long pants, various folding states with deformations, and long sleeves for simple testing, as shown in the figure. The results indicate that our proposed method demonstrates relatively robust performance in handling pants and garments with different folding states. Notably, the model’s ability to generalize from short pants to long pants is quite impressive, showcasing its potential for cross-category inference. However, it struggles with long sleeves, particularly folded long sleeves, which were not present in the training data. It indicates the need for further exploration of the model’s generalization capabilities, especially for unseen or diverse garment types and deformations.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="405" id="S4.F5.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Evaluation of the SKT’s performance on manually collected unseen data, including long pants, various folding states with deformations, and long sleeves. The results demonstrate robust handling of long pants and folded garments, while challenges remain in generalizing to unseen long sleeves and complex folded sleeve configurations.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper introduces a novel approach SKT to robotic garment manipulation using vision-language models, addressing the challenge of handling diverse and deformable garments with a unified framework. We propose a state-aware paired keypoint trajectory formulation that enhances the generalization across various garment states, including flat, folded, and deformed configurations. Additionally, we created a large-scale synthetic dataset with diverse garment states, significantly improving scalability by reducing dependence on real-world data. Experiments demonstrate that the integration of reasoning-based vision-language models enhances the robot’s adaptability in complex scenarios for garment manipulation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Avigal et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Y. Avigal, L. Berscheid, T. Asfour, T. Kröger, and K. Goldberg, “Speedfolding: Learning efficient bimanual folding of garments,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.8.2" style="font-size:90%;">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib1.9.3" style="font-size:90%;">.   IEEE, 2022, pp. 1–8.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Canberk et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
A. Canberk, C. Chi, H. Ha, B. Burchfiel, E. Cousineau, S. Feng, and S. Song, “Cloth funnels: Canonicalized-alignment for multi-purpose garment manipulation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.8.2" style="font-size:90%;">2023 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib2.9.3" style="font-size:90%;">.   IEEE, 2023, pp. 5872–5879.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Longhini et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
A. Longhini, Y. Wang, I. Garcia-Camacho, D. Blanco-Mulero, M. Moletta, M. Welle, G. Alenyà, H. Yin, Z. Erickson, D. Held </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib3.9.3" style="font-size:90%;">, “Unfolding the literature: A review of robotic cloth manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.10.4" style="font-size:90%;">arXiv preprint arXiv:2407.01361</em><span class="ltx_text" id="bib.bib3.11.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Bertiche et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
H. Bertiche, M. Madadi, and S. Escalera, “Cloth3d: clothed 3d humans,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.8.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib4.9.3" style="font-size:90%;">.   Springer, 2020, pp. 344–359.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Lips et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
T. Lips, V.-L. De Gusseme </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib5.9.3" style="font-size:90%;">, “Learning keypoints for robotic cloth manipulation using synthetic data,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.10.4" style="font-size:90%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib5.11.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Wu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
R. Wu, H. Lu, Y. Wang, Y. Wang, and H. Dong, “Unigarmentmanip: A unified framework for category-level garment manipulation via dense visual correspondence,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.8.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib6.9.3" style="font-size:90%;">, 2024, pp. 16 340–16 350.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Tabernik et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
D. Tabernik, J. Muhovič, M. Urbas, and D. Skočaj, “Center direction network for grasping point localization on cloths,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.8.2" style="font-size:90%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib7.9.3" style="font-size:90%;">, vol. 9, no. 10, pp. 8913–8920, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Zhu et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
J. Zhu, A. Cherubini, C. Dune, D. Navarro-Alarcon, F. Alambeigi, D. Berenson, F. Ficuciello, K. Harada, J. Kober, X. Li </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib8.9.3" style="font-size:90%;">, “Challenges and outlook in robotic manipulation of deformable objects,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.4" style="font-size:90%;">IEEE Robotics &amp; Automation Magazine</em><span class="ltx_text" id="bib.bib8.11.5" style="font-size:90%;">, vol. 29, no. 3, pp. 67–77, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Doumanoglou et al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
A. Doumanoglou, J. Stria, G. Peleka, I. Mariolis, V. Petrik, A. Kargakos, L. Wagner, V. Hlaváč, T.-K. Kim, and S. Malassiotis, “Folding clothes autonomously: A complete pipeline,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.8.2" style="font-size:90%;">IEEE Transactions on Robotics</em><span class="ltx_text" id="bib.bib9.9.3" style="font-size:90%;">, vol. 32, no. 6, pp. 1461–1478, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.4.4.1" style="font-size:90%;">Ha and Song [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.6.1" style="font-size:90%;">
H. Ha and S. Song, “Flingbot: The unreasonable effectiveness of dynamic manipulation for cloth unfolding,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.7.2" style="font-size:90%;">Conference on Robot Learning</em><span class="ltx_text" id="bib.bib10.8.3" style="font-size:90%;">.   PMLR, 2022, pp. 24–33.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Proesmans et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
R. Proesmans, A. Verleysen, and F. wyffels, “Unfoldir: Tactile robotic unfolding of cloth,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.8.2" style="font-size:90%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib11.9.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.4.4.1" style="font-size:90%;">De Gusseme and wyffels [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.6.1" style="font-size:90%;">
V.-L. De Gusseme and F. wyffels, “Effective cloth folding trajectories in simulation with only two parameters,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.7.2" style="font-size:90%;">Frontiers in Neurorobotics</em><span class="ltx_text" id="bib.bib12.8.3" style="font-size:90%;">, vol. 16, p. 989702, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.4.4.1" style="font-size:90%;">Lips and De Gusseme [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.6.1" style="font-size:90%;">
T. Lips and F. De Gusseme, Victor-Louis wyffels, “Learning keypoints from synthetic data for robotic cloth folding,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.7.2" style="font-size:90%;">RMDO Workshop ICRA</em><span class="ltx_text" id="bib.bib13.8.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Corona et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
E. Corona, G. Alenya, A. Gabas, and C. Torras, “Active garment recognition and target grasping point detection using deep learning,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.8.2" style="font-size:90%;">Pattern Recognition</em><span class="ltx_text" id="bib.bib14.9.3" style="font-size:90%;">, vol. 74, pp. 629–641, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Seita et al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
D. Seita, N. Jamali, M. Laskey, A. K. Tanwani, R. Berenstein, P. Baskaran, S. Iba, J. Canny, and K. Goldberg, “Deep transfer learning of pick points on fabric for robot bed-making,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.8.2" style="font-size:90%;">The International Symposium of Robotics Research</em><span class="ltx_text" id="bib.bib15.9.3" style="font-size:90%;">.   Springer, 2019, pp. 275–290.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Qian et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
J. Qian, T. Weng, L. Zhang, B. Okorn, and D. Held, “Cloth region segmentation for robust grasp selection,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.8.2" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib16.9.3" style="font-size:90%;">.   IEEE, 2020, pp. 9553–9560.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Matas et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
J. Matas, S. James, and A. J. Davison, “Sim-to-real reinforcement learning for deformable object manipulation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.8.2" style="font-size:90%;">Conference on Robot Learning</em><span class="ltx_text" id="bib.bib17.9.3" style="font-size:90%;">.   PMLR, 2018, pp. 734–743.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Seita et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
D. Seita, A. Ganapathi, R. Hoque, M. Hwang, E. Cen, A. K. Tanwani, A. Balakrishna, B. Thananjeyan, J. Ichnowski, N. Jamali </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib18.9.3" style="font-size:90%;">, “Deep imitation learning of sequential fabric smoothing from an algorithmic supervisor,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.10.4" style="font-size:90%;">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span class="ltx_text" id="bib.bib18.11.5" style="font-size:90%;">.   IEEE, 2020, pp. 9651–9658.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Ganapathi et al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
A. Ganapathi, P. Sundaresan, B. Thananjeyan, A. Balakrishna, D. Seita, J. Grannen, M. Hwang, R. Hoque, J. E. Gonzalez, N. Jamali </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib19.9.3" style="font-size:90%;">, “Learning dense visual correspondences in simulation to smooth and fold real fabrics,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.10.4" style="font-size:90%;">2021 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib19.11.5" style="font-size:90%;">.   IEEE, 2021, pp. 11 515–11 522.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Florence et al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
P. R. Florence, L. Manuelli, and R. Tedrake, “Dense object nets: Learning dense visual object descriptors by and for robotic manipulation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.8.2" style="font-size:90%;">Conference on Robot Learning</em><span class="ltx_text" id="bib.bib20.9.3" style="font-size:90%;">.   PMLR, 2018, pp. 373–385.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Simeonov et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitzmann, “Neural descriptor fields: Se (3)-equivariant object representations for manipulation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.8.2" style="font-size:90%;">2022 International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib21.9.3" style="font-size:90%;">.   IEEE, 2022, pp. 6394–6400.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Yen-Chen et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
L. Yen-Chen, P. Florence, J. T. Barron, T.-Y. Lin, A. Rodriguez, and P. Isola, “Nerf-supervision: Learning dense object descriptors from neural radiance fields,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.8.2" style="font-size:90%;">2022 international conference on robotics and automation (ICRA)</em><span class="ltx_text" id="bib.bib22.9.3" style="font-size:90%;">.   IEEE, 2022, pp. 6496–6503.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Sundaresan et al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
P. Sundaresan, J. Grannen, B. Thananjeyan, A. Balakrishna, M. Laskey, K. Stone, J. E. Gonzalez, and K. Goldberg, “Learning rope manipulation policies using dense object descriptors trained on synthetic depth data,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.8.2" style="font-size:90%;">2020 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib23.9.3" style="font-size:90%;">.   IEEE, 2020, pp. 9411–9418.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Ganapathi et al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
A. Ganapathi, P. Sundaresan, B. Thananjeyan, A. Balakrishna, D. Seita, J. Grannen, M. Hwang, R. Hoque, J. E. Gonzalez, N. Jamali </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib24.9.3" style="font-size:90%;">, “Learning dense visual correspondences in simulation to smooth and fold real fabrics,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.10.4" style="font-size:90%;">2021 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib24.11.5" style="font-size:90%;">.   IEEE, 2021, pp. 11 515–11 522.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Ling et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
S. Ling, Y. Wang, R. Wu, S. Wu, Y. Zhuang, T. Xu, Y. Li, C. Liu, and H. Dong, “Articulated object manipulation with coarse-to-fine affordance for mitigating the effect of point cloud noise,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.8.2" style="font-size:90%;">2024 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib25.9.3" style="font-size:90%;">.   IEEE, 2024, pp. 10 895–10 901.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Li et al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Y. Li, X. Zhang, R. Wu, Z. Zhang, Y. Geng, H. Dong, and Z. He, “Unidoormanip: Learning universal door manipulation policy over large-scale and diverse door manipulation environments,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.8.2" style="font-size:90%;">arXiv preprint arXiv:2403.02604</em><span class="ltx_text" id="bib.bib26.9.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.4.4.1" style="font-size:90%;">Zhang and Demiris [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.6.1" style="font-size:90%;">
F. Zhang and Y. Demiris, “Learning grasping points for garment manipulation in robot-assisted dressing,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.7.2" style="font-size:90%;">2020 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib27.8.3" style="font-size:90%;">.   IEEE, 2020, pp. 9114–9120.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Xu et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
R. Xu, Y. Shen, X. Li, R. Wu, and H. Dong, “Naturalvlm: Leveraging fine-grained natural language for affordance-guided visual manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.8.2" style="font-size:90%;">arXiv preprint arXiv:2403.08355</em><span class="ltx_text" id="bib.bib28.9.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Zhao et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Y. Zhao, R. Wu, Z. Chen, Y. Zhang, Q. Fan, K. Mo, and H. Dong, “Dualafford: Learning collaborative visual affordance for dual-gripper object manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.8.2" style="font-size:90%;">arXiv preprint arXiv:2207.01971</em><span class="ltx_text" id="bib.bib29.9.3" style="font-size:90%;">, vol. 3, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Seita et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
D. Seita, P. Florence, J. Tompson, E. Coumans, V. Sindhwani, K. Goldberg, and A. Zeng, “Learning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.8.2" style="font-size:90%;">2021 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib30.9.3" style="font-size:90%;">.   IEEE, 2021, pp. 4568–4575.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.4.4.1" style="font-size:90%;">PolyHaven [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.6.1" style="font-size:90%;">
A. PolyHaven, “Poly haven: The public 3d asset library,” 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Downs et al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke, “Google scanned objects: A high-quality dataset of 3d scanned household items,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.8.2" style="font-size:90%;">2022 International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib32.9.3" style="font-size:90%;">.   IEEE, 2022, pp. 2553–2560.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Gao et al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib33.9.3" style="font-size:90%;">, “Sphinx-x: Scaling data and parameters for a family of multi-modal large language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.10.4" style="font-size:90%;">arXiv preprint arXiv:2402.05935</em><span class="ltx_text" id="bib.bib33.11.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib34.9.3" style="font-size:90%;">, “Learning transferable visual models from natural language supervision,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.4" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib34.11.5" style="font-size:90%;">.   PMLR, 2021, pp. 8748–8763.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Oquab et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.8.2" style="font-size:90%;">et al.</em><span class="ltx_text" id="bib.bib35.9.3" style="font-size:90%;">, “Dinov2: Learning robust visual features without supervision,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.4" style="font-size:90%;">arXiv preprint arXiv:2304.07193</em><span class="ltx_text" id="bib.bib35.11.5" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.8.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib36.9.3" style="font-size:90%;">.   PMLR, 2023, pp. 19 730–19 742.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Li et al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
G. Li, N. Tsagkas, J. Song, R. Mon-Williams, S. Vijayakumar, K. Shao, and L. Sevilla-Lara, “Learning precise affordances from egocentric videos for robotic manipulation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.8.2" style="font-size:90%;">arXiv preprint arXiv:2408.10123</em><span class="ltx_text" id="bib.bib37.9.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 11:59:53 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
