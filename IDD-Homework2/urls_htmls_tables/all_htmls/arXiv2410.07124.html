<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation</title>
<!--Generated on Fri Sep 20 20:52:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="" lang="en" name="keywords"/>
<base href="/html/2410.07124v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S1" title="In Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S2" title="In Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S2.SS1" title="In 2 Methodology ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Segmentation Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S2.SS2" title="In 2 Methodology ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Engineering for improving Domain Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S3" title="In Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S3.SS1" title="In 3 Experimental analysis ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets and Performance Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S3.SS2" title="In 3 Experimental analysis ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Numerical Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S4" title="In Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Center, Universitat Aut√≤noma de Barcelona, Spain. </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Universitat Pompeu Fabra, Barcelona, Spain. <span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>adrian.galdran@upf.edu</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adrian Galdran<math alttext="{}^{\textrm{({\char 0\relax})}}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mtext id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1a.cmml">(‚úâ)</mtext></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1a.cmml" xref="id1.1.m1.1.1.1"><mtext id="id1.1.m1.1.1.1.cmml" mathsize="70%" xref="id1.1.m1.1.1.1">(‚úâ)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\textrm{({\char 0\relax})}}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT (‚úâ) end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span><span class="ltx_author_notes">1122</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1"><span class="ltx_text" id="id2.id1.1">This short abstract describes a solution to the COSAS 2024 competition on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation from histopathological image patches.
The main challenge in the task of segmenting this type of cancer is a noticeable domain shift encountered when changing acquisition devices (microscopes) and also when tissue comes from different organs.
The two tasks proposed in COSAS were to train on a dataset of images from three different organs, and then predict segmentations on data from unseen organs (dataset T1), and to train on a dataset of images acquired on three different scanners and then segment images acquired with another unseen microscope.
We attempted to bridge the domain shift gap by experimenting with three different strategies: standard training for each dataset, pretraining on dataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we call <span class="ltx_text ltx_font_italic" id="id2.id1.1.1">Cross-Task Pretraining</span>), and training on the combination of dataset A and B.
Our experiments showed that Cross-Task Pre-training is a more promising approach to domain generalization.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span class="ltx_text" id="id3.id1">Adenocarcinoma Segmentation Domain Shift</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Effective segmentation of tumoral areas from histopathology images remains a central problem in the field of computational pathology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#bib.bib2" title="">2</a>]</cite>.
Even when circumventing the challenge of processing gigapixel-size whole-slide images by breaking them down into local patches, it is still extremely hard to train segmentation models that generalize across an array of diverse scenarios, like different acquisition devices or tissue organs.
Such a demanding problem is known as domain shift, and has been the focus of intense research in latest years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the above context, the Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS) competition was held in MICCAI 2024, in order to benchmark domain adaptation capabilities of current segmentation models.
The challenge was divided into two tracks:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Task 1: Cross-Organ Generalization</span> The goal is to train a segmentation model on images corresponding two three different organs affected by adenocarcinoma (gastric adenocarcinoma, colorectal adenocarcinoma, and pancreatic ductal adenocarcinoma). The test set contains images from these three organs and another three unseen ones.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Task 2: Cross-Scanner Generalization:</span> In this track, the objective is to segment adenocarcinoma regions from images digitized with six different scanners. Three of them are used to generate the training set, see Fig. (<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We succinctly describe next our approach to the COSAS competition, with a detailed numerical comparison of several different domain adaptation strategies.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="169" id="S1.F1.sf1.g1" src="extracted/5869297/images/3d-1000_2.jpg" width="180"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="183" id="S1.F1.sf2.g1" src="extracted/5869297/images/kfbio-400.jpg" width="180"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="185" id="S1.F1.sf3.g1" src="extracted/5869297/images/teksqray-600p.jpg" width="180"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The image in (a) was digitized with a 3DHISTECH PANNORAMIC 1000 scanner, (b) was extracted from an image acquired with a KFBIO KF-PRO-400 scanner, and (c) was obtained with a TEKSQRAY SQS-600P scanner. The test set had images acquired with another three unseen scanners.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Segmentation Model</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.2">The segmentation architecture we selected for this task was a Feature-Pyramid Network with a Mix-Vision Transformer encoder pretrained on the Imagenet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#bib.bib3" title="">3</a>]</cite>.
Images were resized to a common resolution of <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">1024</mn><mo id="S2.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn id="S2.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.2">1024</cn><cn id="S2.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">1024 √ó 1024</annotation></semantics></math>, and models were all trained using an Adam optimizer with an initial learning rate of <math alttext="l=1e-4" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">l</mi><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mrow id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml"><mn id="S2.SS1.p1.2.m2.1.1.3.2.2" xref="S2.SS1.p1.2.m2.1.1.3.2.2.cmml">1</mn><mo id="S2.SS1.p1.2.m2.1.1.3.2.1" xref="S2.SS1.p1.2.m2.1.1.3.2.1.cmml">‚Å¢</mo><mi id="S2.SS1.p1.2.m2.1.1.3.2.3" xref="S2.SS1.p1.2.m2.1.1.3.2.3.cmml">e</mi></mrow><mo id="S2.SS1.p1.2.m2.1.1.3.1" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">‚àí</mo><mn id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><eq id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></eq><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ùëô</ci><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><minus id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.1"></minus><apply id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2"><times id="S2.SS1.p1.2.m2.1.1.3.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.1"></times><cn id="S2.SS1.p1.2.m2.1.1.3.2.2.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1.3.2.2">1</cn><ci id="S2.SS1.p1.2.m2.1.1.3.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.3">ùëí</ci></apply><cn id="S2.SS1.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">l=1e-4</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_l = 1 italic_e - 4</annotation></semantics></math> that was cosine-annealed to zero cyclically during 30 epochs.
The binary cross-entropy loss was minimized, as we found no improvement when adding a Dice loss component <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Engineering for improving Domain Generalization</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The above model was trained on different datasets, using three strategies, in an attempt to obtain better performance but also improve generalization ability:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Standard Training</span>: We used the training data for each task independently. We conducted cross-validation, resulting in a five-fold ensemble submission.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Cross-Task Training</span>: Weights after doing Standard Training for each task were used as initialization when fine-tuning a model to solve the other task.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Task Union</span>: We simply trained for each task on the combination of both datasets. Note that validation sets were different for each task.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental analysis</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets and Performance Evaluation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The dataset for Task 1 is composed of 290 image patches showing six different adenocarcinomas. Patches have a resolution of 1500x1500 pixels, and they all come from WSIs digitised using the same scanner. Three organs are present in the training set, but the preliminary test set has 4 and the final test set six organ types, so we can expect some performance gap between cross-validation scores, our submissions to the preliminary test set (all three approaches were submitted) and performance on the final test set (only the best approach was submitted). Similarly, for Task 2 the dataset has images from six scanners, but the training set only has images from three of them, whereas the preliminary test set has four scanners and the final test set has the six of them. Performance in this case was measured by means of the standard Dice Similarity Coefficient.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Numerical Results</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As can be seen from Table <a class="ltx_ref" href="https://arxiv.org/html/2410.07124v1#S3.T1" title="Table 1 ‚Ä£ 3.2 Numerical Results ‚Ä£ 3 Experimental analysis ‚Ä£ Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, a conventional strategy leads to the lowest cross-validation results in both tasks, a trend that is reproduced in the preliminary test set.
In contrast, both using the union of the two datasets for training, and specially pretraining on one task and then fine-tuning on the other one (Cross-Task Pretraining improve performance across the board, with the latter obtaining sizeable improvements.
Note that we made allowed six submissions to the preliminary test set, but only one to the final one, so we selected the best performing approach (Cross-Task Pretraining) for our final submission.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.6.6.7.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S3.T1.6.6.7.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1.2.1">Conventional</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.6.6.7.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.6.7.1.3.1">
<tr class="ltx_tr" id="S3.T1.6.6.7.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.7.1.3.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1.3.1.1.1.1">Crossed</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.7.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.7.1.3.1.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1.3.1.2.1.1">Pre-Training</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.6.6.7.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.6.6.7.1.4.1">
<tr class="ltx_tr" id="S3.T1.6.6.7.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.7.1.4.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1.4.1.1.1.1">Dataset</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.7.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.6.6.7.1.4.1.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1.4.1.2.1.1">Union</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.3.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.4.1">T1: Cross-Validation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">75.95 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2"> 32.36</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.2.1">82.82 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.2.2"> 23.26</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.1">80.57 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.3.m1.1a"><mo id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.3.2"> 28.60</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.8.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.8.1.1.1">T1: Preliminary Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.8.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.8.1.2.1">76.32</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.8.1.3.1">78.65</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.8.1.4.1">75.07</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.9.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.9.2.1.1">T1: Final Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.9.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.9.2.2.1">n/a</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.9.2.3.1">76.07</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.9.2.4.1">n/a</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.6.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.4.1">T2: Cross-Validation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.4.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.4.4.4.1.1">85.57 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.1.m1.1a"><mo id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.4.4.4.1.2"> 18.34</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.5.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.2.1">86.11 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.5.5.5.2.m1.1"><semantics id="S3.T1.5.5.5.2.m1.1a"><mo id="S3.T1.5.5.5.2.m1.1.1" xref="S3.T1.5.5.5.2.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.2.m1.1b"><csymbol cd="latexml" id="S3.T1.5.5.5.2.m1.1.1.cmml" xref="S3.T1.5.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.2.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.2.2"> 17.76</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.3.1">88.17 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S3.T1.6.6.6.3.m1.1"><semantics id="S3.T1.6.6.6.3.m1.1a"><mo id="S3.T1.6.6.6.3.m1.1.1" xref="S3.T1.6.6.6.3.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.3.m1.1b"><csymbol cd="latexml" id="S3.T1.6.6.6.3.m1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.3.m1.1d">¬±</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.3.2"> 16.74</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.10.3.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.10.3.1.1">T2: Preliminary Test Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.6.6.10.3.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.10.3.2.1">81.70</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.3.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.10.3.3.1">85.69</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.3.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.10.3.4.1">85.49</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.6.6.11.4.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.11.4.1.1">T2: Final Test Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.6.6.11.4.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.11.4.2.1">n/a</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.11.4.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.11.4.3.1">81.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.11.4.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.11.4.4.1">n/a</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S3.T1.33.1.1">Table 1</span>: </span>Results from Cross-validating each strategy on the training set (3 organs/scanners, rows 1<sup class="ltx_sup" id="S3.T1.34.2"><span class="ltx_text ltx_font_medium" id="S3.T1.34.2.1">st</span></sup>/4<sup class="ltx_sup" id="S3.T1.35.3"><span class="ltx_text ltx_font_medium" id="S3.T1.35.3.1">th</span></sup>), submitting to the preliminary test set (4 organs/scanners, rows 2<sup class="ltx_sup" id="S3.T1.36.4"><span class="ltx_text ltx_font_medium" id="S3.T1.36.4.1">nd</span></sup>/5<sup class="ltx_sup" id="S3.T1.37.5"><span class="ltx_text ltx_font_medium" id="S3.T1.37.5.1">th</span></sup>) and to the final test set (6 organs/scanners, rows 3<sup class="ltx_sup" id="S3.T1.38.6"><span class="ltx_text ltx_font_medium" id="S3.T1.38.6.1">rd</span></sup>/6<sup class="ltx_sup" id="S3.T1.39.7"><span class="ltx_text ltx_font_medium" id="S3.T1.39.7.1">th</span></sup>).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We summarized and experimentally tested several strategies for dealing with domain shift in adenocarcinoma segmentation from histopathological images.
Cross-Task Pretraining, <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">i.e.</span> training on one task and then fine-tuning on the other resulted in the highest performance for both tasks.</p>
</div>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Acknowledgments</h3>
<div class="ltx_para" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">A. Galdran is funded by a Ramon y Cajal fellowship RYC2022-037144-I.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aubreville, M., et¬†al.: Domain generalization across tumor types, laboratories,
and species ‚Äî insights from the 2022 edition of the mitosis domain
generalization challenge. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib1.1.1">94</span>, 103155
(2024). https://doi.org/https://doi.org/10.1016/j.media.2024.103155

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Foucart, A., al., e.: Shortcomings and areas for improvement in digital
pathology image segmentation challenges. Computerized Medical Imaging and
Graphics <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">103</span>, 102155 (Jan 2023).
https://doi.org/10.1016/j.compmedimag.2022.102155

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Iakubovskii, P.: Segmentation models pytorch.
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/qubvel/segmentation_models.pytorch" title="">https://github.com/qubvel/segmentation_models.pytorch</a> (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Liu, B., al., e.: Do we really need dice? The hidden region-size biases of
segmentation losses. Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">91</span>, 103015 (Jan 2024).
https://doi.org/10.1016/j.media.2023.103015

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zhou, H., al., e.: Unsupervised domain adaptation for histopathology image
segmentation with incomplete labels. Computers in Biology and Medicine
<span class="ltx_text ltx_font_bold" id="bib.bib5.1.1">171</span>, 108226 (Mar 2024). https://doi.org/10.1016/j.compbiomed.2024.108226

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 20:52:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
