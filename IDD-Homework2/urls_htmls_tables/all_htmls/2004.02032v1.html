<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.02032] Generating Rationales in Visual Question Answering</title><meta property="og:description" content="Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability.
We seek to investigate this question by pâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generating Rationales in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generating Rationales in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.02032">

<!--Generated on Mon Mar 18 03:51:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Generating Rationales in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hammad A. Ayyubi<sup id="id1.1.id1" class="ltx_sup">*</sup>, Md. Mehrab Tanjim<sup id="id2.2.id2" class="ltx_sup">*</sup>, Julian J. McAuley, and Garrison W. Cottrell
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">UC San Diego
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">{hayyubi, mtanjim, jmcauley, gary}@eng.ucsd.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Despite recent advances in Visual Question Answering (VQA), it remains a challenge to determine how much success can be attributed to sound reasoning and comprehension ability.
We seek to investigate this question by proposing a new task of <span id="id4.id1.1" class="ltx_text ltx_font_italic">rationale generation</span>. Essentially, we task a VQA model with
generating rationales
for the answers it predicts. We use
data from the
Visual Commonsense Reasoning (VCR) task, as it contains
ground-truth
rationales along with visual questions and answers.
We first
investigate commonsense understanding in one of the leading
VCR models,
ViLBERT, by generating rationales from
pretrained
weights using a state-of-the-art language model, GPT-2.
Next, we seek to
jointly train ViLBERT with GPT-2 in an end-to-end fashion with the dual task of predicting the answer in VQA and generating rationales. We show that this kind of
training
injects commonsense understanding in the VQA model
through quantitative and qualitative evaluation metrics.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2016b</a>)</cite>
tasks are an important assessment of joint language-vision understanding. To perform well on VQA, a model must
understand
the given question and then find a relevant answer from the image. A great deal of success
has
been achieved
in this task with state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> achieving high accuracy on challenging VQA datasets <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>); Ren etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, a
critical question
is how well
these models
â€œunderstandâ€
the image, questions, and the answers that they are predicting.
Are they just exploiting biases in the questions <cite class="ltx_cite ltx_citemacro_cite">Ramakrishnan etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2018</a>); Johnson etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>); Cadene etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>,
images <cite class="ltx_cite ltx_citemacro_cite">Agrawal etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Goyal etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>
or
the data <cite class="ltx_cite ltx_citemacro_cite">Jabri etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2016</a>); Manjunatha etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>?
Answering these questions can help shed light on the limitations of existing VQA approaches, and could also lead to more interpretable/explainable VQA systems.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">It is a non-trivial task to
evaluate a modelâ€™s simultaneous understanding of the three components (questions, images, and answers).
Previous work has analyzed modelsâ€™ understanding of questions <cite class="ltx_cite ltx_citemacro_cite">Shah etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>); Agrawal etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2016a</a>)</cite>,
images <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>); Goyal etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite>
and answers <cite class="ltx_cite ltx_citemacro_cite">Chandrasekaran etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> individually.
They have done so by perturbing words (language modality) or investigating heat-maps of images (vision modality). A joint measure of question, image and answer understanding requires an approach which can simultaneously understand and test both linguistic and visual modalities.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address the
above
challenges, we propose the
novel
task<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Source code will be provided at the time of publication</span></span></span> of generating rationales in VQA as a measure of modelâ€™s comprehensive understanding.
This task not only requires the model to understand the questions (linguistic modality) and the images (visual modality), but it also requires the model to rationalize the predicted answer in relation to the question and image.
</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As we need gold standard rationales to compare
to
the generated rationales, we use the dataset provided by the Visual Commonsense Reasoning (VCR) task <cite class="ltx_cite ltx_citemacro_cite">Zellers etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite>. This dataset contains questions, images,
multiple-choice
answers (four choices) and four options for rationales, out of which one is correct.
We train
models for this particular
VQA task in this dataset: choose a correct answer from four options, given a question and image. Then, we
task the model with
generating rationales for the answers they predict. We compare the generated rationale against the
ground-truth
rationale from the dataset, as a measure of the modelâ€™s comprehensive ability.
</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We employ this approach to investigate one of the leading models on the VCR taskâ€”ViLBERT <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.
Further, we propose a way
to explicitly inject commonsense understanding into the model by jointly training ViLBERT and the language model, GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> on the multi-task
objective
of
predicting the answers and generating rationales. The idea is that by backpropagating the loss of rationale generation into the answer prediction model, sound reasoning can be injected to improve modelâ€™s comprehensive understanding.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approach</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.4" class="ltx_p">The proposed task is illustrated in <a href="#S2.F1" title="In 2.1 Predicted Answer Embedding â€£ 2 Approach â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.
We approach our task of generating rationale by breaking
it into two essential components:
first calculating a predicted answer embedding, <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="E_{A_{p}}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">E</mi><msub id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml"><mi id="S2.p1.1.m1.1.1.3.2" xref="S2.p1.1.m1.1.1.3.2.cmml">A</mi><mi id="S2.p1.1.m1.1.1.3.3" xref="S2.p1.1.m1.1.1.3.3.cmml">p</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ğ¸</ci><apply id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.3.1.cmml" xref="S2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.p1.1.m1.1.1.3.2.cmml" xref="S2.p1.1.m1.1.1.3.2">ğ´</ci><ci id="S2.p1.1.m1.1.1.3.3.cmml" xref="S2.p1.1.m1.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">E_{A_{p}}</annotation></semantics></math>, from the VQA model (pretrained ViLBERT)
by
providing it with the given image <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">I</annotation></semantics></math> and the question <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">Q</annotation></semantics></math>; and second, feeding the predicted answer embedding <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="E_{A_{p}}" display="inline"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">E</mi><msub id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">A</mi><mi id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml">p</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">ğ¸</ci><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">ğ´</ci><ci id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">E_{A_{p}}</annotation></semantics></math> to a language model (pretrained GPT2).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Predicted Answer Embedding</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">Our VQA task
is represented in
terms of a
question <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">Q</annotation></semantics></math>, an image <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">I</annotation></semantics></math>, and
four answer choices
<math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="A_{1}\ldots A_{4}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2.2" xref="S2.SS1.p1.3.m3.1.1.2.2.cmml">A</mi><mn id="S2.SS1.p1.3.m3.1.1.2.3" xref="S2.SS1.p1.3.m3.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.1a" xref="S2.SS1.p1.3.m3.1.1.1.cmml">â€‹</mo><msub id="S2.SS1.p1.3.m3.1.1.4" xref="S2.SS1.p1.3.m3.1.1.4.cmml"><mi id="S2.SS1.p1.3.m3.1.1.4.2" xref="S2.SS1.p1.3.m3.1.1.4.2.cmml">A</mi><mn id="S2.SS1.p1.3.m3.1.1.4.3" xref="S2.SS1.p1.3.m3.1.1.4.3.cmml">4</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><times id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></times><apply id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.2">ğ´</ci><cn type="integer" id="S2.SS1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3">1</cn></apply><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">â€¦</ci><apply id="S2.SS1.p1.3.m3.1.1.4.cmml" xref="S2.SS1.p1.3.m3.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.4.1.cmml" xref="S2.SS1.p1.3.m3.1.1.4">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.4.2.cmml" xref="S2.SS1.p1.3.m3.1.1.4.2">ğ´</ci><cn type="integer" id="S2.SS1.p1.3.m3.1.1.4.3.cmml" xref="S2.SS1.p1.3.m3.1.1.4.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">A_{1}\ldots A_{4}</annotation></semantics></math>,
among which
the model must choose the correct option.
The models approach this task by outputting an embedding for each answer options, <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="E_{A_{i}}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">E</mi><msub id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">A</mi><mi id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">ğ¸</ci><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">ğ´</ci><ci id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">E_{A_{i}}</annotation></semantics></math>. The four answer embeddings are later passed through a linear-softmax layer to predict answer scores. So, if <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">f</annotation></semantics></math> is a VQA model (e.g., ViLBERT) parameterized by <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\theta</annotation></semantics></math>, then the embeddings and softmax scores are calculated as follows:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.5" class="ltx_Math" alttext="\displaystyle E_{A_{i}}=f(Q,I,A_{i};\theta)\quad\leavevmode\resizebox{84.55244pt}{}{
$\forall i\in\{1,4\}$
}" display="block"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><msub id="S2.E1.m1.5.5.3" xref="S2.E1.m1.5.5.3.cmml"><mi id="S2.E1.m1.5.5.3.2" xref="S2.E1.m1.5.5.3.2.cmml">E</mi><msub id="S2.E1.m1.5.5.3.3" xref="S2.E1.m1.5.5.3.3.cmml"><mi id="S2.E1.m1.5.5.3.3.2" xref="S2.E1.m1.5.5.3.3.2.cmml">A</mi><mi id="S2.E1.m1.5.5.3.3.3" xref="S2.E1.m1.5.5.3.3.3.cmml">i</mi></msub></msub><mo id="S2.E1.m1.5.5.2" xref="S2.E1.m1.5.5.2.cmml">=</mo><mrow id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.2.cmml"><mrow id="S2.E1.m1.5.5.1.1.1" xref="S2.E1.m1.5.5.1.1.1.cmml"><mi id="S2.E1.m1.5.5.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Q</mi><mo id="S2.E1.m1.5.5.1.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">,</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">I</mi><mo id="S2.E1.m1.5.5.1.1.1.1.1.4" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">,</mo><msub id="S2.E1.m1.5.5.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.5.5.1.1.1.1.1.5" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">;</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">Î¸</mi><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.6" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow><mspace width="1em" id="S2.E1.m1.5.5.1.1.2" xref="S2.E1.m1.5.5.1.2.cmml"></mspace><mpadded depth="4.1pt" height="12.4pt" width="84.6pt" id="S2.E1.m1.1.1.1.1.m1.2.3" xref="S2.E1.m1.1.1.1.1.m1.2.3.cmml"><mrow id="S2.E1.m1.1.1.1.1.m1.2.3.2" xref="S2.E1.m1.1.1.1.1.m1.2.3.2.cmml"><mo rspace="0.167em" id="S2.E1.m1.1.1.1.1.m1.2.3.2.1" xref="S2.E1.m1.1.1.1.1.m1.2.3.2.1.cmml">âˆ€</mo><mi id="S2.E1.m1.1.1.1.1.m1.2.3.2.2" xref="S2.E1.m1.1.1.1.1.m1.2.3.2.2.cmml">i</mi></mrow><mo id="S2.E1.m1.1.1.1.1.m1.2.3.1" xref="S2.E1.m1.1.1.1.1.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S2.E1.m1.1.1.1.1.m1.2.3.3.2" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.1" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2.cmml">{</mo><mn id="S2.E1.m1.1.1.1.1.m1.1.1" xref="S2.E1.m1.1.1.1.1.m1.1.1.cmml">1</mn><mo id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.2" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2.cmml">,</mo><mn id="S2.E1.m1.1.1.1.1.m1.2.2" xref="S2.E1.m1.1.1.1.1.m1.2.2.cmml">4</mn><mo stretchy="false" id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.3" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2.cmml">}</mo></mrow></mpadded></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.2.cmml" xref="S2.E1.m1.5.5.2"></eq><apply id="S2.E1.m1.5.5.3.cmml" xref="S2.E1.m1.5.5.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.1.cmml" xref="S2.E1.m1.5.5.3">subscript</csymbol><ci id="S2.E1.m1.5.5.3.2.cmml" xref="S2.E1.m1.5.5.3.2">ğ¸</ci><apply id="S2.E1.m1.5.5.3.3.cmml" xref="S2.E1.m1.5.5.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.3.3.1.cmml" xref="S2.E1.m1.5.5.3.3">subscript</csymbol><ci id="S2.E1.m1.5.5.3.3.2.cmml" xref="S2.E1.m1.5.5.3.3.2">ğ´</ci><ci id="S2.E1.m1.5.5.3.3.3.cmml" xref="S2.E1.m1.5.5.3.3.3">ğ‘–</ci></apply></apply><list id="S2.E1.m1.5.5.1.2.cmml" xref="S2.E1.m1.5.5.1.1"><apply id="S2.E1.m1.5.5.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1"><times id="S2.E1.m1.5.5.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.2"></times><ci id="S2.E1.m1.5.5.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3">ğ‘“</ci><vector id="S2.E1.m1.5.5.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1"><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘„</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğ¼</ci><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2">ğ´</ci><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">ğœƒ</ci></vector></apply><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1"><mpadded depth="4.1pt" height="12.4pt" width="84.6pt" id="S2.E1.m1.1.1.1.1.m1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3"><mrow id="S2.E1.m1.1.1.1.1.m1.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.2"><mo id="S2.E1.m1.1.1.1.1.m1.2.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.2.1">âˆ€</mo><mi id="S2.E1.m1.1.1.1.1.m1.2.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.2.2">i</mi></mrow><mo id="S2.E1.m1.1.1.1.1.m1.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.1">âˆˆ</mo><mrow id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2">{</mo><mn id="S2.E1.m1.1.1.1.1.m1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.m1.1.1">1</mn><mo id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2">,</mo><mn id="S2.E1.m1.1.1.1.1.m1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.2">4</mn><mo stretchy="false" id="S2.E1.m1.1.1.1.1.m1.2.3.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.m1.2.3.3.2">}</mo></mrow></mpadded></ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">\displaystyle E_{A_{i}}=f(Q,I,A_{i};\theta)\quad\leavevmode\resizebox{84.55244pt}{}{
$\forall i\in\{1,4\}$
}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\displaystyle{s_{i}=\mathit{Softmax}(\mathit{Linear}(E_{A_{i}};\theta_{l}))\quad\leavevmode\resizebox{84.55244pt}{}{
$\forall i\in\{1,4\}$
}}" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><msub id="S2.E2.m1.2.2.3" xref="S2.E2.m1.2.2.3.cmml"><mi id="S2.E2.m1.2.2.3.2" xref="S2.E2.m1.2.2.3.2.cmml">s</mi><mi id="S2.E2.m1.2.2.3.3" xref="S2.E2.m1.2.2.3.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.2.cmml"><mrow id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.3.cmml">ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.4" xref="S2.E2.m1.2.2.1.1.1.1.1.1.4.cmml">ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">E</mi><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">A</mi><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.4" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">;</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml">Î¸</mi><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml">l</mi></msub><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.5" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mspace width="1em" id="S2.E2.m1.2.2.1.1.2" xref="S2.E2.m1.2.2.1.2.cmml"></mspace><mtext id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1u.cmml">
<div id="S2.E2.m1.1.1.1nest" class="ltx_inline-block ltx_transformed_outer" style="width:84.6pt;height:16.5pt;vertical-align:-4.1pt;"><span class="ltx_transformed_inner" style="transform:translate(16.7pt,-2.4pt) scale(1.65067810247484,1.65067810247484) ;">
<p id="S2.E2.m1.1.1.1.1nest" class="ltx_p"><XMath xmlns="http://dlmf.nist.gov/LaTeXML" id="S2.E2.m1.1.1d" fragid="S2.E2.m1.1.1.1.1.m1.2nest" xref="S2.E2.m1.1.1u.cmml"><XMApp id="S2.E2.m1.1.1e" fragid="S2.E2.m1.1.1.1.1.m1.2.3nest" xref="S2.E2.m1.1.1u.cmml"><XMTok meaning="element-of" name="in" role="RELOP" id="S2.E2.m1.1.1f" fragid="S2.E2.m1.1.1.1.1.m1.2.3.1nest" xref="S2.E2.m1.1.1u.cmml">âˆˆ</XMTok><XMApp id="S2.E2.m1.1.1g" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2nest" xref="S2.E2.m1.1.1u.cmml"><XMTok meaning="for-all" name="forall" role="BIGOP" id="S2.E2.m1.1.1h" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2.1nest" xref="S2.E2.m1.1.1u.cmml">âˆ€</XMTok><XMTok role="UNKNOWN" font="italic" id="S2.E2.m1.1.1i" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2.2nest" xref="S2.E2.m1.1.1u.cmml">i</XMTok></XMApp><XMDual id="S2.E2.m1.1.1j" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3nest" xref="S2.E2.m1.1.1u.cmml"><XMApp id="S2.E2.m1.1.1k" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1nest" xref="S2.E2.m1.1.1u.cmml"><XMTok meaning="set" id="S2.E2.m1.1.1l" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.1nest" xref="S2.E2.m1.1.1u.cmml"></XMTok><XMRef idref="S2.E2.m1.1.1.1.1.m1.1.1nest" id="S2.E2.m1.1.1m" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.2nest" xref="S2.E2.m1.1.1u.cmml"></XMRef><XMRef idref="S2.E2.m1.1.1.1.1.m1.2.2nest" id="S2.E2.m1.1.1n" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.3nest" xref="S2.E2.m1.1.1u.cmml"></XMRef></XMApp><XMWrap id="S2.E2.m1.1.1o" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2nest" xref="S2.E2.m1.1.1u.cmml"><XMTok role="OPEN" stretchy="false" id="S2.E2.m1.1.1p" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.1nest" xref="S2.E2.m1.1.1u.cmml">{</XMTok><XMTok meaning="1" role="NUMBER" id="S2.E2.m1.1.1q" fragid="S2.E2.m1.1.1.1.1.m1.1.1nest" xref="S2.E2.m1.1.1u.cmml">1</XMTok><XMTok role="PUNCT" id="S2.E2.m1.1.1r" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.2nest" xref="S2.E2.m1.1.1u.cmml">,</XMTok><XMTok meaning="4" role="NUMBER" id="S2.E2.m1.1.1s" fragid="S2.E2.m1.1.1.1.1.m1.2.2nest" xref="S2.E2.m1.1.1u.cmml">4</XMTok><XMTok role="CLOSE" stretchy="false" id="S2.E2.m1.1.1t" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.3nest" xref="S2.E2.m1.1.1u.cmml">}</XMTok></XMWrap></XMDual></XMApp></XMath></p>
</span></div></mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><eq id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"></eq><apply id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.3.1.cmml" xref="S2.E2.m1.2.2.3">subscript</csymbol><ci id="S2.E2.m1.2.2.3.2.cmml" xref="S2.E2.m1.2.2.3.2">ğ‘ </ci><ci id="S2.E2.m1.2.2.3.3.cmml" xref="S2.E2.m1.2.2.3.3">ğ‘–</ci></apply><list id="S2.E2.m1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.1"><apply id="S2.E2.m1.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.2"></times><ci id="S2.E2.m1.2.2.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.3">ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3"></times><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.4">ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ</ci><list id="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2"><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2">ğ¸</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">ğ´</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.2">ğœƒ</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.3">ğ‘™</ci></apply></list></apply></apply><ci id="S2.E2.m1.1.1u.cmml" xref="S2.E2.m1.1.1"><mtext id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">
<div id="S2.E2.m1.1.1.1anest" class="ltx_inline-block ltx_transformed_outer" style="width:84.6pt;height:16.5pt;vertical-align:-4.1pt;"><span class="ltx_transformed_inner" style="transform:translate(16.7pt,-2.4pt) scale(1.65067810247484,1.65067810247484) ;">
<p id="S2.E2.m1.1.1.1.1anest" class="ltx_p"><XMath xmlns="http://dlmf.nist.gov/LaTeXML" id="S2.E2.m1.1.1d.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2anest" xref="S2.E2.m1.1.1"><XMApp id="S2.E2.m1.1.1e.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3anest" xref="S2.E2.m1.1.1"><XMTok meaning="element-of" name="in" role="RELOP" id="S2.E2.m1.1.1f.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.1anest" xref="S2.E2.m1.1.1">âˆˆ</XMTok><XMApp id="S2.E2.m1.1.1g.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2anest" xref="S2.E2.m1.1.1"><XMTok meaning="for-all" name="forall" role="BIGOP" id="S2.E2.m1.1.1h.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2.1anest" xref="S2.E2.m1.1.1">âˆ€</XMTok><XMTok role="UNKNOWN" font="italic" id="S2.E2.m1.1.1i.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.2.2anest" xref="S2.E2.m1.1.1">i</XMTok></XMApp><XMDual id="S2.E2.m1.1.1j.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3anest" xref="S2.E2.m1.1.1"><XMApp id="S2.E2.m1.1.1k.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1anest" xref="S2.E2.m1.1.1"><XMTok meaning="set" id="S2.E2.m1.1.1l.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.1anest" xref="S2.E2.m1.1.1"></XMTok><XMRef idref="S2.E2.m1.1.1.1.1.m1.1.1anest" id="S2.E2.m1.1.1m.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.2anest" xref="S2.E2.m1.1.1"></XMRef><XMRef idref="S2.E2.m1.1.1.1.1.m1.2.2anest" id="S2.E2.m1.1.1n.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.1.3anest" xref="S2.E2.m1.1.1"></XMRef></XMApp><XMWrap id="S2.E2.m1.1.1o.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2anest" xref="S2.E2.m1.1.1"><XMTok role="OPEN" stretchy="false" id="S2.E2.m1.1.1p.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.1anest" xref="S2.E2.m1.1.1">{</XMTok><XMTok meaning="1" role="NUMBER" id="S2.E2.m1.1.1q.cmml" fragid="S2.E2.m1.1.1.1.1.m1.1.1anest" xref="S2.E2.m1.1.1">1</XMTok><XMTok role="PUNCT" id="S2.E2.m1.1.1r.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.2anest" xref="S2.E2.m1.1.1">,</XMTok><XMTok meaning="4" role="NUMBER" id="S2.E2.m1.1.1s.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.2anest" xref="S2.E2.m1.1.1">4</XMTok><XMTok role="CLOSE" stretchy="false" id="S2.E2.m1.1.1t.cmml" fragid="S2.E2.m1.1.1.1.1.m1.2.3.3.2.3anest" xref="S2.E2.m1.1.1">}</XMTok></XMWrap></XMDual></XMApp></XMath></p>
</span></div></mtext></ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\displaystyle{s_{i}=\mathit{Softmax}(\mathit{Linear}(E_{A_{i}};\theta_{l}))\quad\leavevmode\resizebox{84.55244pt}{}{
$\forall i\in\{1,4\}$
}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2004.02032/assets/vilbert_ra.png" id="S2.F1.g1" class="ltx_graphics ltx_img_square" width="598" height="539" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>High-level overview of the proposed task. We ask the VQA model to generate rationale for the answer it is predicting.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The act of choosing the most probable answer embedding out of four options will make the network non-differentiable. To address this issue, we calculate the predicted answer embedding by taking the average of each answer option embedding <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="E_{A_{i}}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msub id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">E</mi><msub id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">A</mi><mi id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ¸</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ğ´</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">E_{A_{i}}</annotation></semantics></math> weighted by their softmax scores:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div id="S2.E3.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:211.4pt;height:44.2pt;vertical-align:-26.3pt;"><span class="ltx_transformed_inner" style="transform:translate(65.4pt,-5.6pt) scale(2.62566745492352,2.62566745492352) ;">
<p id="S2.E3.m1.1.1.1.1" class="ltx_p"><math id="S2.E3.m1.1.1.1.1.m1.1" class="ltx_Math" alttext="E_{A_{p}}=\sum_{i=1}^{4}E_{A_{i}}\times s_{i}" display="inline"><semantics id="S2.E3.m1.1.1.1.1.m1.1a"><mrow id="S2.E3.m1.1.1.1.1.m1.1.1" xref="S2.E3.m1.1.1.1.1.m1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.m1.1.1.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.2.cmml">E</mi><msub id="S2.E3.m1.1.1.1.1.m1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3.2.cmml">A</mi><mi id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3.3.cmml">p</mi></msub></msub><mo rspace="0.111em" id="S2.E3.m1.1.1.1.1.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.m1.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.m1.1.1.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.cmml"><msubsup id="S2.E3.m1.1.1.1.1.m1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.cmml"><mo id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.1" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mn id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.3.cmml">4</mn></msubsup><mrow id="S2.E3.m1.1.1.1.1.m1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.cmml"><msub id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.2.cmml">E</mi><msub id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.2.cmml">A</mi><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.3.cmml">i</mi></msub></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.1" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.1.cmml">Ã—</mo><msub id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.2" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.2.cmml">s</mi><mi id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.3" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1.1.1.1.m1.1b"><apply id="S2.E3.m1.1.1.1.1.m1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1"><eq id="S2.E3.m1.1.1.1.1.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.1"></eq><apply id="S2.E3.m1.1.1.1.1.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.2">ğ¸</ci><apply id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3.2">ğ´</ci><ci id="S2.E3.m1.1.1.1.1.m1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3"><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.2"></sum><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3"><eq id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.1"></eq><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><cn type="integer" id="S2.E3.m1.1.1.1.1.m1.1.1.3.1.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.1.3">4</cn></apply><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2"><times id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.1"></times><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.2">ğ¸</ci><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.2">ğ´</ci><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.2.3.3">ğ‘–</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.2">ğ‘ </ci><ci id="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.m1.1.1.3.2.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1.1.1.1.m1.1c">E_{A_{p}}=\sum_{i=1}^{4}E_{A_{i}}\times s_{i}</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Generating Rationales</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">We formulate the task of generating rationale as conditional language generation, conditioned on the predicted answer embedding and previously generated rationale tokens. Specifically,
if <math id="S2.SS2.p1.1.m1.3" class="ltx_Math" alttext="r=r_{1},\dots,r_{n}" display="inline"><semantics id="S2.SS2.p1.1.m1.3a"><mrow id="S2.SS2.p1.1.m1.3.3" xref="S2.SS2.p1.1.m1.3.3.cmml"><mi id="S2.SS2.p1.1.m1.3.3.4" xref="S2.SS2.p1.1.m1.3.3.4.cmml">r</mi><mo id="S2.SS2.p1.1.m1.3.3.3" xref="S2.SS2.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S2.SS2.p1.1.m1.3.3.2.2" xref="S2.SS2.p1.1.m1.3.3.2.3.cmml"><msub id="S2.SS2.p1.1.m1.2.2.1.1.1" xref="S2.SS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS2.p1.1.m1.2.2.1.1.1.2.cmml">r</mi><mn id="S2.SS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p1.1.m1.3.3.2.2.3" xref="S2.SS2.p1.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S2.SS2.p1.1.m1.3.3.2.2.4" xref="S2.SS2.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S2.SS2.p1.1.m1.3.3.2.2.2" xref="S2.SS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS2.p1.1.m1.3.3.2.2.2.2.cmml">r</mi><mi id="S2.SS2.p1.1.m1.3.3.2.2.2.3" xref="S2.SS2.p1.1.m1.3.3.2.2.2.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.3b"><apply id="S2.SS2.p1.1.m1.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3"><eq id="S2.SS2.p1.1.m1.3.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3.3"></eq><ci id="S2.SS2.p1.1.m1.3.3.4.cmml" xref="S2.SS2.p1.1.m1.3.3.4">ğ‘Ÿ</ci><list id="S2.SS2.p1.1.m1.3.3.2.3.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2"><apply id="S2.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1.2">ğ‘Ÿ</ci><cn type="integer" id="S2.SS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">â€¦</ci><apply id="S2.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2.2">ğ‘Ÿ</ci><ci id="S2.SS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2.3">ğ‘›</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.3c">r=r_{1},\dots,r_{n}</annotation></semantics></math> is the rationale, and
<math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="E_{A_{p}}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">E</mi><msub id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS2.p1.2.m2.1.1.3.2" xref="S2.SS2.p1.2.m2.1.1.3.2.cmml">A</mi><mi id="S2.SS2.p1.2.m2.1.1.3.3" xref="S2.SS2.p1.2.m2.1.1.3.3.cmml">p</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ğ¸</ci><apply id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2">ğ´</ci><ci id="S2.SS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">E_{A_{p}}</annotation></semantics></math> is the predicted answer embedding,
we maximize the following log likelihood:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.6" class="ltx_Math" alttext="\log(P(r))=\sum_{i=1}^{n}\log(P(r_{i}|E_{A_{p}},r_{1},\ldots,r_{i-1}))" display="block"><semantics id="S2.Ex1.m1.6a"><mrow id="S2.Ex1.m1.6.6" xref="S2.Ex1.m1.6.6.cmml"><mrow id="S2.Ex1.m1.5.5.1.1" xref="S2.Ex1.m1.5.5.1.2.cmml"><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">log</mi><mo id="S2.Ex1.m1.5.5.1.1a" xref="S2.Ex1.m1.5.5.1.2.cmml">â¡</mo><mrow id="S2.Ex1.m1.5.5.1.1.1" xref="S2.Ex1.m1.5.5.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.2" xref="S2.Ex1.m1.5.5.1.2.cmml">(</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml"><mi id="S2.Ex1.m1.5.5.1.1.1.1.2" xref="S2.Ex1.m1.5.5.1.1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.5.5.1.1.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.5.5.1.1.1.1.3.2" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.3.2.1" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">r</mi><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.1.3.2.2" xref="S2.Ex1.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.1.3" xref="S2.Ex1.m1.5.5.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S2.Ex1.m1.6.6.3" xref="S2.Ex1.m1.6.6.3.cmml">=</mo><mrow id="S2.Ex1.m1.6.6.2" xref="S2.Ex1.m1.6.6.2.cmml"><munderover id="S2.Ex1.m1.6.6.2.2" xref="S2.Ex1.m1.6.6.2.2.cmml"><mo movablelimits="false" id="S2.Ex1.m1.6.6.2.2.2.2" xref="S2.Ex1.m1.6.6.2.2.2.2.cmml">âˆ‘</mo><mrow id="S2.Ex1.m1.6.6.2.2.2.3" xref="S2.Ex1.m1.6.6.2.2.2.3.cmml"><mi id="S2.Ex1.m1.6.6.2.2.2.3.2" xref="S2.Ex1.m1.6.6.2.2.2.3.2.cmml">i</mi><mo id="S2.Ex1.m1.6.6.2.2.2.3.1" xref="S2.Ex1.m1.6.6.2.2.2.3.1.cmml">=</mo><mn id="S2.Ex1.m1.6.6.2.2.2.3.3" xref="S2.Ex1.m1.6.6.2.2.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex1.m1.6.6.2.2.3" xref="S2.Ex1.m1.6.6.2.2.3.cmml">n</mi></munderover><mrow id="S2.Ex1.m1.6.6.2.1.1" xref="S2.Ex1.m1.6.6.2.1.2.cmml"><mi id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml">log</mi><mo id="S2.Ex1.m1.6.6.2.1.1a" xref="S2.Ex1.m1.6.6.2.1.2.cmml">â¡</mo><mrow id="S2.Ex1.m1.6.6.2.1.1.1" xref="S2.Ex1.m1.6.6.2.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.6.6.2.1.1.1.2" xref="S2.Ex1.m1.6.6.2.1.2.cmml">(</mo><mrow id="S2.Ex1.m1.6.6.2.1.1.1.1" xref="S2.Ex1.m1.6.6.2.1.1.1.1.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.6.6.2.1.1.1.1.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.2.cmml">r</mi><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.4" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.4.cmml"><msub id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.2.cmml">E</mi><msub id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">A</mi><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml">p</mi></msub></msub><mo id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.4" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.2.cmml">r</mi><mn id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.5" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">â€¦</mi><mo id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.6" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.2.cmml">r</mi><mrow id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.2" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.1" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.3" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.Ex1.m1.6.6.2.1.1.1.3" xref="S2.Ex1.m1.6.6.2.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.6b"><apply id="S2.Ex1.m1.6.6.cmml" xref="S2.Ex1.m1.6.6"><eq id="S2.Ex1.m1.6.6.3.cmml" xref="S2.Ex1.m1.6.6.3"></eq><apply id="S2.Ex1.m1.5.5.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1"><log id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2"></log><apply id="S2.Ex1.m1.5.5.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1"><times id="S2.Ex1.m1.5.5.1.1.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.1"></times><ci id="S2.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.1.1.2">ğ‘ƒ</ci><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">ğ‘Ÿ</ci></apply></apply><apply id="S2.Ex1.m1.6.6.2.cmml" xref="S2.Ex1.m1.6.6.2"><apply id="S2.Ex1.m1.6.6.2.2.cmml" xref="S2.Ex1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.2.1.cmml" xref="S2.Ex1.m1.6.6.2.2">superscript</csymbol><apply id="S2.Ex1.m1.6.6.2.2.2.cmml" xref="S2.Ex1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.2.2.1.cmml" xref="S2.Ex1.m1.6.6.2.2">subscript</csymbol><sum id="S2.Ex1.m1.6.6.2.2.2.2.cmml" xref="S2.Ex1.m1.6.6.2.2.2.2"></sum><apply id="S2.Ex1.m1.6.6.2.2.2.3.cmml" xref="S2.Ex1.m1.6.6.2.2.2.3"><eq id="S2.Ex1.m1.6.6.2.2.2.3.1.cmml" xref="S2.Ex1.m1.6.6.2.2.2.3.1"></eq><ci id="S2.Ex1.m1.6.6.2.2.2.3.2.cmml" xref="S2.Ex1.m1.6.6.2.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.Ex1.m1.6.6.2.2.2.3.3.cmml" xref="S2.Ex1.m1.6.6.2.2.2.3.3">1</cn></apply></apply><ci id="S2.Ex1.m1.6.6.2.2.3.cmml" xref="S2.Ex1.m1.6.6.2.2.3">ğ‘›</ci></apply><apply id="S2.Ex1.m1.6.6.2.1.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1"><log id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4"></log><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1"><times id="S2.Ex1.m1.6.6.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.2"></times><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.3">ğ‘ƒ</ci><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.4.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.2">ğ‘Ÿ</ci><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.5.3">ğ‘–</ci></apply><list id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.4.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3"><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.2">ğ¸</ci><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.2">ğ´</ci><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘</ci></apply></apply><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.2">ğ‘Ÿ</ci><cn type="integer" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">â€¦</ci><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.2">ğ‘Ÿ</ci><apply id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3"><minus id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn type="integer" id="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.Ex1.m1.6.6.2.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.6c">\log(P(r))=\sum_{i=1}^{n}\log(P(r_{i}|E_{A_{p}},r_{1},\ldots,r_{i-1}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.4" class="ltx_p">Here, <math id="S2.SS2.p1.3.m1.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S2.SS2.p1.3.m1.1a"><msub id="S2.SS2.p1.3.m1.1.1" xref="S2.SS2.p1.3.m1.1.1.cmml"><mi id="S2.SS2.p1.3.m1.1.1.2" xref="S2.SS2.p1.3.m1.1.1.2.cmml">r</mi><mi id="S2.SS2.p1.3.m1.1.1.3" xref="S2.SS2.p1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m1.1b"><apply id="S2.SS2.p1.3.m1.1.1.cmml" xref="S2.SS2.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m1.1.1.1.cmml" xref="S2.SS2.p1.3.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m1.1.1.2.cmml" xref="S2.SS2.p1.3.m1.1.1.2">ğ‘Ÿ</ci><ci id="S2.SS2.p1.3.m1.1.1.3.cmml" xref="S2.SS2.p1.3.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m1.1c">r_{i}</annotation></semantics></math> is the <math id="S2.SS2.p1.4.m2.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S2.SS2.p1.4.m2.1a"><msup id="S2.SS2.p1.4.m2.1.1" xref="S2.SS2.p1.4.m2.1.1.cmml"><mi id="S2.SS2.p1.4.m2.1.1.2" xref="S2.SS2.p1.4.m2.1.1.2.cmml">i</mi><mrow id="S2.SS2.p1.4.m2.1.1.3" xref="S2.SS2.p1.4.m2.1.1.3.cmml"><mi id="S2.SS2.p1.4.m2.1.1.3.2" xref="S2.SS2.p1.4.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.4.m2.1.1.3.1" xref="S2.SS2.p1.4.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.p1.4.m2.1.1.3.3" xref="S2.SS2.p1.4.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m2.1b"><apply id="S2.SS2.p1.4.m2.1.1.cmml" xref="S2.SS2.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m2.1.1.1.cmml" xref="S2.SS2.p1.4.m2.1.1">superscript</csymbol><ci id="S2.SS2.p1.4.m2.1.1.2.cmml" xref="S2.SS2.p1.4.m2.1.1.2">ğ‘–</ci><apply id="S2.SS2.p1.4.m2.1.1.3.cmml" xref="S2.SS2.p1.4.m2.1.1.3"><times id="S2.SS2.p1.4.m2.1.1.3.1.cmml" xref="S2.SS2.p1.4.m2.1.1.3.1"></times><ci id="S2.SS2.p1.4.m2.1.1.3.2.cmml" xref="S2.SS2.p1.4.m2.1.1.3.2">ğ‘¡</ci><ci id="S2.SS2.p1.4.m2.1.1.3.3.cmml" xref="S2.SS2.p1.4.m2.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m2.1c">i^{th}</annotation></semantics></math> token of the rationale. The language model is then fine-tuned using the gold standard rationale for
the corresponding visual question-answer from the VCR dataset.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use the Visual Commonsense Reasoning (VCR) dataset <cite class="ltx_cite ltx_citemacro_cite">Zellers etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite>
since (in addition to
visual questions and answers) this dataset
includes rationales. The dataset has 290,000 multiple choice questions derived from 110,000 movie scenes. We report all results on the validation set as the test set labels are not available
while the VCR challenge is ongoing.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We use
ViLBERT <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> as the reference VQA model. For the language model, we use
the 124 million parameter pretrained GPT-2 (small) <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>.
We use a batch size of 32, initial learning rate of 2e-5 and train the models for 20 epochs for all our experiments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluating VQA Model Understanding</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Since we want to investigate how well the VQA reference model (ViLBERT) already â€œunderstandsâ€ the image, the question and the answer, we freeze the pretrained weights of the model. We extract predicted answer embeddings using
<a href="#S2.E3" title="In 2.1 Predicted Answer Embedding â€£ 2 Approach â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">eq.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, and generate rationales using GPT-2,
conditioned on this answer embedding.
We fine-tune GPT-2 using the ground-truth rationale from the dataset. We call this model ViLBERT-Fr (ViLBERT-Frozen).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Injecting Commonsense into VQA</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">In this setting, we want to explicitly enforce commonsense understanding in the VQA model <span id="S3.SS2.p1.2.1" class="ltx_text ltx_font_italic">while</span> predicting the answer. We follow the same procedure as in
<a href="#S3.SS1" title="3.1 Evaluating VQA Model Understanding â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>, except
the weights of ViLBERT are fine-tuned as well. We train ViLBERT with GPT-2 in an end-to-end fashion with the dual loss of answer prediction, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L_{A}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">ğ’œ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">â„’</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ’œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{L_{A}}</annotation></semantics></math> (via a cross-entropy loss) and rationale generation, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L_{R}}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">â„›</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">â„’</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">â„›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{L_{R}}</annotation></semantics></math> (via a causal language modeling loss). The final loss is:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\mathcal{L}=\lambda\mathcal{L_{A}}+\mathcal{L_{R}}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">â„’</mi><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.1" xref="S3.E4.m1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml">ğ’œ</mi></msub></mrow><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml">â„›</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">â„’</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><times id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2.1"></times><ci id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2">ğœ†</ci><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">â„’</ci><ci id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3">ğ’œ</ci></apply></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">â„’</ci><ci id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">â„›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathcal{L}=\lambda\mathcal{L_{A}}+\mathcal{L_{R}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.3" class="ltx_p">where <math id="S3.SS2.p1.3.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p1.3.m1.1a"><mi id="S3.SS2.p1.3.m1.1.1" xref="S3.SS2.p1.3.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m1.1b"><ci id="S3.SS2.p1.3.m1.1.1.cmml" xref="S3.SS2.p1.3.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m1.1c">\lambda</annotation></semantics></math> is the weight
fine-tuned during our experiments.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.5.1" class="ltx_tr">
<th id="S3.T1.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.4.5.1.1.1" class="ltx_text ltx_font_bold">Loss</span></th>
<th id="S3.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.5.1.2.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></th>
<th id="S3.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.5.1.3.1" class="ltx_text ltx_font_bold">BLEU-1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\lambda=1" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">Î»</mi><mo id="S3.T1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><eq id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1.1"></eq><ci id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\lambda=1</annotation></semantics></math></th>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">70.18</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">11.24</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><math id="S3.T1.2.2.1.m1.1" class="ltx_Math" alttext="\lambda=3" display="inline"><semantics id="S3.T1.2.2.1.m1.1a"><mrow id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml"><mi id="S3.T1.2.2.1.m1.1.1.2" xref="S3.T1.2.2.1.m1.1.1.2.cmml">Î»</mi><mo id="S3.T1.2.2.1.m1.1.1.1" xref="S3.T1.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S3.T1.2.2.1.m1.1.1.3" xref="S3.T1.2.2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><apply id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"><eq id="S3.T1.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1.1"></eq><ci id="S3.T1.2.2.1.m1.1.1.2.cmml" xref="S3.T1.2.2.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S3.T1.2.2.1.m1.1.1.3.cmml" xref="S3.T1.2.2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\lambda=3</annotation></semantics></math></th>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center">69.96</td>
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_center">10.78</td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<th id="S3.T1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><math id="S3.T1.3.3.1.m1.1" class="ltx_Math" alttext="\lambda=10" display="inline"><semantics id="S3.T1.3.3.1.m1.1a"><mrow id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml"><mi id="S3.T1.3.3.1.m1.1.1.2" xref="S3.T1.3.3.1.m1.1.1.2.cmml">Î»</mi><mo id="S3.T1.3.3.1.m1.1.1.1" xref="S3.T1.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S3.T1.3.3.1.m1.1.1.3" xref="S3.T1.3.3.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><apply id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"><eq id="S3.T1.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1.1"></eq><ci id="S3.T1.3.3.1.m1.1.1.2.cmml" xref="S3.T1.3.3.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S3.T1.3.3.1.m1.1.1.3.cmml" xref="S3.T1.3.3.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">\lambda=10</annotation></semantics></math></th>
<td id="S3.T1.3.3.2" class="ltx_td ltx_align_center">70.24</td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center">10.55</td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><math id="S3.T1.4.4.1.m1.1" class="ltx_Math" alttext="\lambda=1000" display="inline"><semantics id="S3.T1.4.4.1.m1.1a"><mrow id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml"><mi id="S3.T1.4.4.1.m1.1.1.2" xref="S3.T1.4.4.1.m1.1.1.2.cmml">Î»</mi><mo id="S3.T1.4.4.1.m1.1.1.1" xref="S3.T1.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S3.T1.4.4.1.m1.1.1.3" xref="S3.T1.4.4.1.m1.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><apply id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"><eq id="S3.T1.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1.1"></eq><ci id="S3.T1.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.1.m1.1.1.2">ğœ†</ci><cn type="integer" id="S3.T1.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.1.m1.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">\lambda=1000</annotation></semantics></math></th>
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_center">69.84</td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.4.4.3.1" class="ltx_text ltx_font_bold">11.27</span></td>
</tr>
<tr id="S3.T1.4.6.1" class="ltx_tr">
<th id="S3.T1.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">var</th>
<td id="S3.T1.4.6.1.2" class="ltx_td ltx_align_center">70.19</td>
<td id="S3.T1.4.6.1.3" class="ltx_td ltx_align_center">11.15</td>
</tr>
<tr id="S3.T1.4.7.2" class="ltx_tr">
<th id="S3.T1.4.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">kldiv (ViLBERT-Ra)</th>
<td id="S3.T1.4.7.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.4.7.2.2.1" class="ltx_text ltx_font_bold">70.45</span></td>
<td id="S3.T1.4.7.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.4.7.2.3.1" class="ltx_text ltx_font_bold">11.27</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Losses. <math id="S3.T1.6.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.T1.6.m1.1b"><mi id="S3.T1.6.m1.1.1" xref="S3.T1.6.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.T1.6.m1.1c"><ci id="S3.T1.6.m1.1.1.cmml" xref="S3.T1.6.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.m1.1d">\lambda</annotation></semantics></math> is from <a href="#S3.E4" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">eq.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>, var = Homoscedastic Uncertainty loss and kldiv = KL-Divergence regularizer added.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metric</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We use BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2002</a>)</cite> and
ROUGE
<cite class="ltx_cite ltx_citemacro_cite">Lin (<a href="#bib.bib21" title="" class="ltx_ref">2004</a>)</cite> to compare generated and gold standard rationales.
In addition to these n-gram metrics, we are also interested in comparing the semantic similarity of rationales.
We follow <cite class="ltx_cite ltx_citemacro_citet">Huang (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>
and calculate sentence embeddings using the InferSent model proposed by <cite class="ltx_cite ltx_citemacro_citet">Conneau etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, followed by cosine similarity measurement to compare generated rationales with the gold standard.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Metrics</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ViLBERT-Fr</th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S3.T2.1.2.2.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:59.8pt;">
<span id="S3.T2.1.2.2.2.1.1" class="ltx_p">ViLBERT-Ra</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<th id="S3.T2.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T2.1.3.1.1.1" class="ltx_text ltx_font_bold">BLEU-1</span></th>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">8.92</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.3.1.3.1" class="ltx_text ltx_font_bold">11.27</span></td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<th id="S3.T2.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T2.1.4.2.1.1" class="ltx_text ltx_font_bold">BLEU-4</span></th>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center">0.56</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.2.3.1" class="ltx_text ltx_font_bold">0.68</span></td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<th id="S3.T2.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T2.1.5.3.1.1" class="ltx_text ltx_font_bold">ROUGE-1</span></th>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_center">13.52</td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.5.3.3.1" class="ltx_text ltx_font_bold">17.08</span></td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<th id="S3.T2.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T2.1.6.4.1.1" class="ltx_text ltx_font_bold">ROUGE-L</span></th>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_center">11.28</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.4.3.1" class="ltx_text ltx_font_bold">14.15</span></td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<th id="S3.T2.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T2.1.7.5.1.1" class="ltx_text ltx_font_bold">Cosine Similarity</span></th>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_center">0.57</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.7.5.3.1" class="ltx_text ltx_font_bold">0.60</span></td>
</tr>
<tr id="S3.T2.1.8.6" class="ltx_tr">
<th id="S3.T2.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S3.T2.1.8.6.1.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></th>
<td id="S3.T2.1.8.6.2" class="ltx_td ltx_align_center ltx_border_bb">69.58</td>
<td id="S3.T2.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.8.6.3.1" class="ltx_text ltx_font_bold">70.45</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of generated rationale vs gold standard rationale on the validation set of VCR dataset.
</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.T3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:173.4pt;">
<img src="/html/2004.02032/assets/x1.png" id="S3.T3.1.g1" class="ltx_graphics ltx_img_landscape" width="261" height="147" alt="[Uncaptioned image]">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S3.T3.2" class="ltx_tabular ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:260.2pt;">
<thead class="ltx_thead">
<tr id="S3.T3.2.1.1" class="ltx_tr">
<th id="S3.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S3.T3.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.1.1.2.1.1" class="ltx_p" style="width:184.9pt;"><span id="S3.T3.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Text</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.1" class="ltx_tr">
<th id="S3.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.2.2.1.1.1" class="ltx_text ltx_font_bold">Question</span></th>
<td id="S3.T3.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.1.2.1.1" class="ltx_p" style="width:184.9pt;">What are the occupations of [person1], [person2], [person3], and [person4] ?</span>
</span>
</td>
</tr>
<tr id="S3.T3.2.3.2" class="ltx_tr">
<th id="S3.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T3.2.3.2.1.1" class="ltx_text ltx_font_bold">Answer</span></th>
<td id="S3.T3.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.3.2.2.1.1" class="ltx_p" style="width:184.9pt;">[person1], [person2] and [person3] , and [person4] are among the others military officers.</span>
</span>
</td>
</tr>
<tr id="S3.T3.2.4.3" class="ltx_tr">
<th id="S3.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.T3.2.4.3.1.1" class="ltx_text ltx_font_bold">Gold Rationale</span></th>
<td id="S3.T3.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.4.3.2.1.1" class="ltx_p" style="width:184.9pt;">They are all <span id="S3.T3.2.4.3.2.1.1.1" class="ltx_text" style="color:#808080;background-color:#FFFF00;">wearing</span> decorated <span id="S3.T3.2.4.3.2.1.1.2" class="ltx_text" style="color:#808080;background-color:#FFFF00;">military uniforms</span>.</span>
</span>
</td>
</tr>
<tr id="S3.T3.2.5.4" class="ltx_tr">
<th id="S3.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.2.5.4.1.1" class="ltx_text ltx_font_bold">ViLBERT-Fr</span></th>
<td id="S3.T3.2.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T3.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.5.4.2.1.1" class="ltx_p" style="width:184.9pt;">Avon is about to cross the range to save Osiris</span>
</span>
</td>
</tr>
<tr id="S3.T3.2.6.5" class="ltx_tr">
<th id="S3.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t"><span id="S3.T3.2.6.5.1.1" class="ltx_text ltx_font_bold">ViLBERT-Ra</span></th>
<td id="S3.T3.2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t">
<span id="S3.T3.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.6.5.2.1.1" class="ltx_p" style="width:184.9pt;">Myrl Tommie are <span id="S3.T3.2.6.5.2.1.1.1" class="ltx_text" style="color:#808080;background-color:#FFFF00;">wearing military uniforms</span>.</span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Generated rationales samples. Highlighted portions show key points of the rationale for the given answer.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Results</h3>

<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Multi-task objective</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Since we are dealing with multiple losses (and objectives) of answer prediction and rationale generation (<a href="#S3.E4" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">eq.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>), we report a comparative study on different losses we explored in <a href="#S3.T1" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p"><span id="S3.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Weighted Losses</span>: We vary the <math id="S3.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS4.SSS1.p2.1.m1.1a"><mi id="S3.SS4.SSS1.p2.1.m1.1.1" xref="S3.SS4.SSS1.p2.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.1.m1.1b"><ci id="S3.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.1.m1.1c">\lambda</annotation></semantics></math> in <a href="#S3.E4" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">eq.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> as 1, 3, 10 and 1000.</p>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para">
<p id="S3.SS4.SSS1.p3.2" class="ltx_p"><span id="S3.SS4.SSS1.p3.2.1" class="ltx_text ltx_font_italic">Uncertainty Loss (var)</span>: We weight the losses <math id="S3.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L_{A}}" display="inline"><semantics id="S3.SS4.SSS1.p3.1.m1.1a"><msub id="S3.SS4.SSS1.p3.1.m1.1.1" xref="S3.SS4.SSS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.SSS1.p3.1.m1.1.1.2" xref="S3.SS4.SSS1.p3.1.m1.1.1.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS4.SSS1.p3.1.m1.1.1.3" xref="S3.SS4.SSS1.p3.1.m1.1.1.3.cmml">ğ’œ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.1.m1.1b"><apply id="S3.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.2">â„’</ci><ci id="S3.SS4.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.p3.1.m1.1.1.3">ğ’œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.1.m1.1c">\mathcal{L_{A}}</annotation></semantics></math> and <math id="S3.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{L_{R}}" display="inline"><semantics id="S3.SS4.SSS1.p3.2.m2.1a"><msub id="S3.SS4.SSS1.p3.2.m2.1.1" xref="S3.SS4.SSS1.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.SSS1.p3.2.m2.1.1.2" xref="S3.SS4.SSS1.p3.2.m2.1.1.2.cmml">â„’</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS4.SSS1.p3.2.m2.1.1.3" xref="S3.SS4.SSS1.p3.2.m2.1.1.3.cmml">â„›</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p3.2.m2.1b"><apply id="S3.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1.2">â„’</ci><ci id="S3.SS4.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS4.SSS1.p3.2.m2.1.1.3">â„›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p3.2.m2.1c">\mathcal{L_{R}}</annotation></semantics></math> by considering the homoscedastic uncertainty of each task as in <cite class="ltx_cite ltx_citemacro_citet">Cipolla etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S3.SS4.SSS1.p4" class="ltx_para">
<p id="S3.SS4.SSS1.p4.1" class="ltx_p"><span id="S3.SS4.SSS1.p4.1.1" class="ltx_text ltx_font_italic">KL-Divergence (kldiv)</span>: We add Kullbackâ€“Leibler divergence <cite class="ltx_cite ltx_citemacro_cite">Kullback and Leibler (<a href="#bib.bib20" title="" class="ltx_ref">1951</a>)</cite> loss between predicted answer scores <a href="#S2.E2" title="In 2.1 Predicted Answer Embedding â€£ 2 Approach â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">eq.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> and answer scores from pretrained ViLBERT as an added regularizer. This was done to prevent our model from diverging too much from the trained ViLBERT on the original VQA task.</p>
</div>
<div id="S3.SS4.SSS1.p5" class="ltx_para">
<p id="S3.SS4.SSS1.p5.1" class="ltx_p">We see from <a href="#S3.T1" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> that the model trained with KL-Divergence loss performs best on both the VQA task and rationale generation task. As such, we do all further comparison with this model and name it ViLBERT-Ra (ViLBERT-Rationale).</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Quantitative Results</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p"><span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Rationale generation: </span>We compare the performance of ViLBERT-Fr and our model, ViLBERT-Ra in <a href="#S3.T2" title="In 3.3 Evaluation Metric â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. We see that our model consistently out performs ViLBERT-Fr over both n-gram metrics â€“ BLEU and ROUGE and semantic similarity measurement metric â€“ cosine similarity. This demonstrates how we can leverage rationale generation task to improve model comprehension abilities of existing VQA models.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p"><span id="S3.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_italic">VQA task: </span>We also compare the performance of the two models on the original VQA task in <a href="#S3.T1" title="In 3.2 Injecting Commonsense into VQA â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.
Since we trained all our models with a batch size of 32 (due to limited compute resources),
we ran a control experiment to train ViLBERT with the same batch size instead of original 64 for fair comparison.
We find that ViLBERT-Ra gives superior performance on the rationale generation task without compromising accuracy on the original VQA task (<a href="#S3.T2" title="In 3.3 Evaluation Metric â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>). In fact, VQA performance is slightly improved. This suggests that training the model to generate rationales can improve modelâ€™s comprehension which in turn can lead to better answer prediction judgement.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Qualitative Results</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p"><span id="S3.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">Human Evaluation:</span> We presented 100 randomly selected samples from the VCR validation set containing an image, question, correct answer and rationales generated by ViLBERT-Fr and ViLBERT-Ra to human evaluators. The generated rationales were shuffled randomly to hide which rationale came from which model. We then asked them to choose which of the two candidate rationales better explains the answer in the given sample. The results are summarized in <a href="#S3.T4" title="In 3.4.3 Qualitative Results â€£ 3.4 Results â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. Humans consistently rate the rationales generated by ViLBERT-Ra as better explanations for the answers.</p>
</div>
<div id="S3.SS4.SSS3.p2" class="ltx_para">
<p id="S3.SS4.SSS3.p2.1" class="ltx_p">We show an illustrative example of a rationale generated by the two models in <a href="#S3.T3" title="In 3.3 Evaluation Metric â€£ 3 Experiments â€£ Generating Rationales in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>. During training, we replaced tags like [person1], [person2] with random names, so in both rationales we can observe random names being generated. However, we note that our model was able to generate key words of the gold rationale and convey the relevant meaning. We have provided more such examples in the attached appendix.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">H-1</span></th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">H-2</span></th>
<th id="S3.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">H-3</span></th>
<th id="S3.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">Majority Voting</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<th id="S3.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT-Fr</th>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">31</td>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">29</td>
<td id="S3.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">38</td>
<td id="S3.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">20</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<th id="S3.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">ViLBERT-Ra</th>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.3.2.2.1" class="ltx_text ltx_font_bold">69</span></td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.3.2.3.1" class="ltx_text ltx_font_bold">71</span></td>
<td id="S3.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.3.2.4.1" class="ltx_text ltx_font_bold">62</span></td>
<td id="S3.T4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.1.3.2.5.1" class="ltx_text ltx_font_bold">80</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Percent of rationales preferred by three human judges. Here, H stands for Human.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluating VQA model comprehension.</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">High performance of VQA models <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite> have naturally led to calls for investigating biases <cite class="ltx_cite ltx_citemacro_cite">Manjunatha etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> and interpretability.
Researchers have employed various kinds of attention mechanisms over words and images to point out sections of images and words that the model attends to while answering the question <cite class="ltx_cite ltx_citemacro_cite">Das etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>); Goyal etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>); Agrawal etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2016a</a>)</cite>.
Another set of approaches use various kinds of
â€˜selectionâ€™ tasks
as a means to interpret models. <cite class="ltx_cite ltx_citemacro_citet">Goyal etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> propose picking another image for the same question that has a different answer. <cite class="ltx_cite ltx_citemacro_citet">Berg and Belhumeur (<a href="#bib.bib5" title="" class="ltx_ref">2013</a>)</cite> propose selecting visual facts (image regions) from the image while <cite class="ltx_cite ltx_citemacro_citet">Zellers etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite> propose picking one rationale from a set of four options.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">An orthogonal set of methods <cite class="ltx_cite ltx_citemacro_cite">Andreas etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2016</a>); Hu etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>); Mascharka etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>); Vedantam etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> approach this task by generating symbolic programs to reason about the question. We note that such an approach would quickly become intractable given the size of the symbol vocabulary required to cover free-form rationale generation, as we are considering.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generating rationales.</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">The task of generating explanations has previously been employed by <cite class="ltx_cite ltx_citemacro_citet">Hendricks etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite> to explain fine-grained bird recognition decisions. In our case, we explain answers to visual questions with rationales.
<cite class="ltx_cite ltx_citemacro_citet">Rajani etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> generate reasons and rationales to explain question answering tasks, but only in a purely textual mode.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">To the best of our knowledge no prior work has proposed the task of generating rationales as a measure of evaluating comprehensive understanding of images, questions and answers in VQA models.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we proposed the novel task of generating rationales as a measure of model understanding for Visual Question Answering tasks. A well-reasoned explanation implies a thorough understanding of all components of the task: the image, the question and the answer. We further proposed an end-to-end training method to improve the modelâ€™s commonsense understanding. We demonstrated the effectiveness of our proposed method through
quantitative and qualitative results.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/d16-1203" title="" class="ltx_ref ltx_href">Analyzing the behavior
of visual question answering models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2018.00522" title="" class="ltx_ref ltx_href">Donâ€™t just assume;
look and answer: Overcoming priors for visual question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C.Â Lawrence
Zitnick, Devi Parikh, and Dhruv Batra. 2016b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s11263-016-0966-6" title="" class="ltx_ref ltx_href">Vqa: Visual
question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):4â€“31.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas etÂ al. (2016)</span>
<span class="ltx_bibblock">
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2016.12" title="" class="ltx_ref ltx_href">Neural module
networks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berg and Belhumeur (2013)</span>
<span class="ltx_bibblock">
Thomas Berg and PeterÂ N. Belhumeur. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2013.9" title="" class="ltx_ref ltx_href">How do you tell a
blackbird from a crow?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2013 IEEE International Conference on
Computer Vision</em>, ICCV â€™13, pages 9â€“16, Washington, DC, USA. IEEE Computer
Society.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadene etÂ al. (2019)</span>
<span class="ltx_bibblock">
Remi Cadene, Corentin Dancette, Hedi Ben-younes, Matthieu Cord, and Devi
Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.10169" title="" class="ltx_ref ltx_href">Rubi: Reducing unimodal
biases in visual question answering</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandrasekaran etÂ al. (2018)</span>
<span class="ltx_bibblock">
Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay,
and Devi Parikh. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/d18-1128" title="" class="ltx_ref ltx_href">Do explanations make
vqa models more predictable to a human?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, AhmedÂ El Kholy, Faisal Ahmed, Zhe Gan,
YuÂ Cheng, and Jingjing Liu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1909.11740" title="" class="ltx_ref ltx_href">Uniter: Learning universal
image-text representations</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cipolla etÂ al. (2018)</span>
<span class="ltx_bibblock">
Roberto Cipolla, Yarin Gal, and Alex Kendall. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2018.00781" title="" class="ltx_ref ltx_href">Multi-task learning
using uncertainty to weigh losses for scene geometry and semantics</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2017)</span>
<span class="ltx_bibblock">
Alexis Conneau, Douwe Kiela, Holger Schwenk, LoÃ¯c Barrault, and Antoine
Bordes. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/d17-1070" title="" class="ltx_ref ltx_href">Supervised learning of
universal sentence representations from natural language inference data</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.cviu.2017.10.001" title="" class="ltx_ref ltx_href">Human attention
in visual question answering: Do humans and deep networks look at the same
regions?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 163:90â€“100.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2017.670" title="" class="ltx_ref ltx_href">Making the v in vqa
matter: Elevating the role of image understanding in visual question
answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2016)</span>
<span class="ltx_bibblock">
Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1608.08974" title="" class="ltx_ref ltx_href">Towards transparent ai
systems: Interpreting visual question answering models</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks etÂ al. (2016)</span>
<span class="ltx_bibblock">
LisaÂ Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt
Schiele, and Trevor Darrell. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-46493-0_1" title="" class="ltx_ref ltx_href">Generating
visual explanations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, page 3â€“19.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/iccv.2017.93" title="" class="ltx_ref ltx_href">Learning to reason:
End-to-end module networks for visual question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang (2018)</span>
<span class="ltx_bibblock">
Kexin Huang. 2018.

</span>
<span class="ltx_bibblock">Content-based image retrieval using generated textual meta-data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICAAI 2018</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri etÂ al. (2016)</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens vanÂ der Maaten. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-319-46484-8_44" title="" class="ltx_ref ltx_href">Revisiting
visual question answering baselines</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, page 727â€“739.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2018)</span>
<span class="ltx_bibblock">
YuÂ Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1807.09956" title="" class="ltx_ref ltx_href">Pythia v0.1: the winning
entry to the vqa challenge 2018</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens vanÂ der Maaten, LiÂ Fei-Fei,
C.Â Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2017.215" title="" class="ltx_ref ltx_href">Clevr: A diagnostic
dataset for compositional language and elementary visual reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kullback and Leibler (1951)</span>
<span class="ltx_bibblock">
S.Â Kullback and R.Â A. Leibler. 1951.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1214/aoms/1177729694" title="" class="ltx_ref ltx_href">On information and
sufficiency</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Ann. Math. Statist.</em>, 22(1):79â€“86.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/W04-1013" title="" class="ltx_ref ltx_href">ROUGE: A package
for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, pages 74â€“81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1908.02265" title="" class="ltx_ref ltx_href">Vilbert: Pretraining
task-agnostic visiolinguistic representations for vision-and-language tasks</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manjunatha etÂ al. (2018)</span>
<span class="ltx_bibblock">
Varun Manjunatha, Nirat Saini, and LarryÂ S. Davis. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1811.07789" title="" class="ltx_ref ltx_href">Explicit bias discovery in
visual question answering models</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mascharka etÂ al. (2018)</span>
<span class="ltx_bibblock">
David Mascharka, Philip Tran, Ryan Soklaski, and Arjun Majumdar. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/cvpr.2018.00519" title="" class="ltx_ref ltx_href">Transparency by
design: Closing the gap between performance and interpretability in visual
reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311â€“318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajani etÂ al. (2019)</span>
<span class="ltx_bibblock">
NazneenÂ Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/p19-1487" title="" class="ltx_ref ltx_href">Explain yourself!
leveraging language models for commonsense reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishnan etÂ al. (2018)</span>
<span class="ltx_bibblock">
Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1810.03649" title="" class="ltx_ref ltx_href">Overcoming language priors
in visual question answering with adversarial regularization</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1505.02074" title="" class="ltx_ref ltx_href">Exploring models and data
for image question answering</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah etÂ al. (2019)</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.05660" title="" class="ltx_ref ltx_href">Cycle-consistency for robust
visual question answering</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra,
and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1902.07864" title="" class="ltx_ref ltx_href">Probabilistic
neural-symbolic models for interpretable visual question answering</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers etÂ al. (2018)</span>
<span class="ltx_bibblock">
Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1811.10830" title="" class="ltx_ref ltx_href">From recognition to
cognition: Visual commonsense reasoning</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.02031" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.02032" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.02032">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.02032" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.02033" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 18 03:51:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
