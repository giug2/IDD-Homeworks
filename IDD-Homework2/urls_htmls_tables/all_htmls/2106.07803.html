<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.07803] SynthASR: Unlocking Synthetic Data for Speech Recognition</title><meta property="og:description" content="End-to-end (E2E) automatic speech recognition (ASR) models
have recently demonstrated superior performance over the traditional hybrid ASR models.
Training an E2E ASR model requires a large amount of data which is not …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SynthASR: Unlocking Synthetic Data for Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SynthASR: Unlocking Synthetic Data for Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.07803">

<!--Generated on Tue Mar 19 12:02:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">SynthASR: Unlocking Synthetic Data for Speech Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">End-to-end (E2E) automatic speech recognition (ASR) models
have recently demonstrated superior performance over the traditional hybrid ASR models.
Training an E2E ASR model requires a large amount of data which is not only expensive
but may also raise dependency on production data.
At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS)
engines has advanced to near-human naturalness.
In this work, we propose to utilize synthetic speech
for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training.
In addition, we apply continual learning with a novel multi-stage training strategy
to address catastrophic forgetting, achieved by
a mix of weighted multi-style training, data augmentation,
encoder freezing, and parameter regularization.
In our experiments conducted on in-house datasets for a new application of recognizing
medication names, training ASR RNN-T models with synthetic audio via the proposed
multi-stage training improved the recognition performance on new application by
more than 65% relative, without degradation on existing general applications.
Our observations show that SynthASR holds great promise in training
the state-of-the-art large-scale E2E ASR models for new applications while reducing
the costs and dependency on production data.</span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Index Terms</span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">: speech recognition, data efficient machine learning, synthetic speech</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">End-to-end (E2E) designs, such as those based on connectionist temporal classification
(CTC) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">,
recurrent neural network transducer (RNN-T) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;">,
and Listen, Attend and Spell (LAS) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.10" class="ltx_text" style="font-size:90%;">, have several advantages over the
older hybrid designs for automatic speech recognition (ASR) tasks.
These designs jointly optimize the model parameters to improve the accuracy
at text level, and they learn the tasks directly from data.
The highly integrated model structure in E2E designs reduces the overall model size and simplifies both
training and inference, making it more attractive to on-device applications </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">To perform well in real applications, E2E ASR systems need to be trained on thousands of hours
of transcribed speech data. One way to meet this data requirement is to learn directly
from transcribed production data.
To work with production data, one must be careful to handle
the data properly.
This includes the methods by which the
data is collected,
transferred, stored, accessed, and deleted.
It also includes minimizing
the amount of human exposure to the data, such as when it
is transcribed.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">The goal of this work is to reduce the reliance on human transcribed data by training ASR models on production-like
data synthesized from text by text-to-speech (TTS) engines </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.4" class="ltx_text" style="font-size:90%;">.
The contribution of this work comes in three folds. First, we validate the effectiveness of using TTS based
synthetic data as a general approach to reduce reliance on transcribed data. Second, we study how to improve ASR models
for new applications without relying on corresponding production data. Third, we propose a multi-stage training strategy,
and demonstrate that continual learning via this strategy significantly improves ASR performance on the new applications
without degradation on existing applications.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">Recent research has made significant progress in using synthetic speech data for ASR model training.
In </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.4" class="ltx_text" style="font-size:90%;">, a TTS engine based on Tacotron-2 is used to synthesize audio for new vocabulary to teach an acoustic-to-word speech recognition model new words. In a follow-up work </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.7" class="ltx_text" style="font-size:90%;">, multi-speaker TTS is used to improve the acoustic diversity in synthetic data, where speaker embeddings are added to Tacotron-2 architecture. Later in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.10" class="ltx_text" style="font-size:90%;">, global style token (GST) based embeddings are introduced to modify a version of Tacotron-2 to further increase the acoustic diversity of synthetic data where GST is found superior to i-vector based embeddings. In addition, this work also demonstrates that adding TTS based synthetic data, LM approaches, and general data augmentation method SpecAugment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.13" class="ltx_text" style="font-size:90%;"> are mostly independent and complementary. In </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.16" class="ltx_text" style="font-size:90%;">, E2E TTS with speaker presentations from a variational autoencoder (VAE) is explored to increase the acoustic diversity in low-resource data. All these previous works in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.19" class="ltx_text" style="font-size:90%;"> have shown the benefit of increased acoustic diversity in synthetic data for ASR model improvement, especially in low resource scenarios.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="font-size:90%;">The works in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.4" class="ltx_text" style="font-size:90%;"> have shown that E2E ASR models can learn new vocabularies from TTS based synthetic data, which is crucial for feature expansion of ASR into new applications.
Recent work by </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.7" class="ltx_text" style="font-size:90%;"> pointed out some practical challenges in such vocabulary expansion strategy in terms of learning to recognize new words without recognition degradation on already learned words. Authors have explored several strategies to address this challenge including combining real data with synthetic data with weighted sampling and applying different regularizations on each model components.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text" style="font-size:90%;">Inspired by previous research, this work reduces the dependency of ASR model training on production data by using TTS synthetic data.
While a number of existing methods can achieve similar goals, each has their own limitations. For example, in semi-supervised learning (SSL) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.4" class="ltx_text" style="font-size:90%;">, where speech audio is transcribed by machine rather than human, audio signals are still required to be collected, transferred, and stored in the cloud to generate machine transcribed labels. In federated learning (FL) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.7" class="ltx_text" style="font-size:90%;">, where many devices collaboratively train a shared global model, comprehensive infrastructure updates are needed, which puts additional cost burdens on the customers for device upgrades in order to support new features.</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="font-size:90%;">Acknowledging the importance of language model (LM) approaches as alternative methods to utilize text-only data for E2E ASR performance improvement </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S2.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p4.1.4" class="ltx_text" style="font-size:90%;">, this work focuses on the WER improvement from the first pass ASR model to provide LM approaches a better starting point. In addition, we applied both SpecAugment and classical signal processing based data augmentation methods on TTS synthetic data.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Technical Approaches</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">The schematic diagram of our proposed system is illustrated in Figure </span><a href="#S3.F1" title="Figure 1 ‣ 3.1 Overview ‣ 3 Technical Approaches ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS1.p1.1.2" class="ltx_text" style="font-size:90%;">.
It consists of a multi-context TTS engine to generate synthetic speech, and an RNN-T model for speech recognition.
The model for TTS engine is trained and evaluated independently from the speech recognition model.
RNN-T models are trained in a multi-style training (MST) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S3.SS1.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.5" class="ltx_text" style="font-size:90%;">.
With MST, the training data are sampled from a combination of real speech recordings and TTS based synthetic speech audio.
The ratio between real recordings and synthetic audio seen during RNN-T training is optimized with sampling weights, as suggested by </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S3.SS1.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.8" class="ltx_text" style="font-size:90%;">.
This method well mixes the real and synthetic data in each batch so that the ASR model sees both data.
In addition, data augmentation is applied at both audio level and feature level for RNN-T training.
For one production scale application covered in this work, instead of training RNN-T models from scratch, we adopted continual learning (CL) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS1.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.11" class="ltx_text" style="font-size:90%;"> where an established RNN-T model incrementally learns from data with new information without catastrophic forgetting via the proposed multi-stage training approach.</span></p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2106.07803/assets/overal_system.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="349" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of the proposed SynthASR employing TTS synthetic data for ASR training.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-context TTS</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">We use a multi-context TTS to generate clean synthetic speech with diverse speaker and prosody attributes (Figure </span><a href="#S3.F2" title="Figure 2 ‣ 3.4 ASR RNN-T model ‣ 3 Technical Approaches ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS2.p1.1.2" class="ltx_text" style="font-size:90%;">).
This system consists of two main modules: a context generation module and a neural vocoder module.
The context generation module is an attention-based sequence-to-sequence network </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.SS2.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.5" class="ltx_text" style="font-size:90%;"> that predicts a Mel-spectrogram given an input text.
We control the speaker identity with voice profile embeddings, which introduces a bias in the training that makes the reference encoder to be speaker independent.
At inference time we guide the TTS with a reference spectrogram generated with a high-fidelity speaker-dependent TTS system trained with more that 20 hours high quality data.
This reference spectrogram provides a natural prosodic contour to the attention module.
Since the reference encoder has been trained following a variational auto-encoder (VAE) approach, where we learn the posterior distributions over the prosody latent space given the reference spectrogram, we later can sample from the posteriors to generate different speech realizations of the same text.
This one-to-many capability increases the inter- and intra- speaker variability, which is crucial for ASR training
as it increases the speaker diversity of synthetic data.
The neural vocoder module consists of the architecture similar to the universal vocoder described in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.SS2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.8" class="ltx_text" style="font-size:90%;"> but without any VAE reference.
This Universal Neural Vocoder (UNV) was pretrained with more than 100 hours from more than 100 speakers in 27 languages from a proprietary database of paid voice actors. The UNV synthesizes speech audio out of the Mel-spectrograms generated by the first module.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">A speaker verification system is used to produce the voice profile embeddings for the context generation module. The speaker verification system was trained on an internal Amazon data. This speaker verification model uses the architecture introduced in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.SS2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p2.1.4" class="ltx_text" style="font-size:90%;">. This pre-trained speaker verification system is used to provide voice profile embeddings in the Multi-context TTS system as in Figure </span><a href="#S3.F2" title="Figure 2 ‣ 3.4 ASR RNN-T model ‣ 3 Technical Approaches ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS2.p2.1.5" class="ltx_text" style="font-size:90%;">. We first generate the speaker embedding for each utterance of training data in TTS system, then we average all utterance-level embeddings from a given speaker as voice profile embedding for this speaker.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data augmentation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">Reverberation is introduced to TTS synthetic audio by convolving the audio with an acoustic impulse response (AIR) randomly selected from a pool of 10,000 available AIRs estimated from chirp signal measurements in real rooms.
Then an audio segment randomly sampled from an in-house dataset is added on top as background noise, with an SNR ranging from 10 to 20 dB.
To increase the acoustic diversity, for each on-the-fly audio corruption, there is a 60% probability of reverberation and an independently 60% probability of noise addition.
This provides a mixture of clean synthetic speech, synthetic speech with reverberation only, synthetic speech with background noise only, as well as synthetic speech with both reverberation and background noise.
In addition, SpecAugment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p1.1.4" class="ltx_text" style="font-size:90%;"> is applied on the log Mel filter bank features generated from both synthetic data and real data for RNN-T training.
Two frequency masks are applied to each utterances and the maximal masked frequency percentage is 37.5%.
The maximal ratio of each time mask is 5% of the utterance duration,
and the number of time masks is proportional to utterance length, i.e. 5% of the frame numbers
without being larger than 10.
Where masks applies, Gaussian noise is used with the same mean and variance from the masked values.</span></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>ASR RNN-T model</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.3" class="ltx_p"><span id="S3.SS4.p1.3.1" class="ltx_text" style="font-size:90%;">RNN-T is an E2E ASR model architecture suitable for streaming applications with proven competitive performance </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.SS4.p1.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS4.p1.3.4" class="ltx_text" style="font-size:90%;">.
It consists of a transcription network (or encoder), a prediction network (or decoder), and a joint network.
Targeting at streaming applications, we use LSTM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.3.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.SS4.p1.3.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS4.p1.3.7" class="ltx_text" style="font-size:90%;"> layers for both the encoder and decoder.
The encoder sequentially maps acoustic feature </span><math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi mathsize="90%" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathbf{x}</annotation></semantics></math><span id="S3.SS4.p1.3.8" class="ltx_text" style="font-size:90%;"> to a high-level feature representation </span><math id="S3.SS4.p1.2.m2.2" class="ltx_Math" alttext="\mathbf{h}=\operatorname{Enc}(\mathbf{x})" display="inline"><semantics id="S3.SS4.p1.2.m2.2a"><mrow id="S3.SS4.p1.2.m2.2.3" xref="S3.SS4.p1.2.m2.2.3.cmml"><mi mathsize="90%" id="S3.SS4.p1.2.m2.2.3.2" xref="S3.SS4.p1.2.m2.2.3.2.cmml">𝐡</mi><mo mathsize="90%" id="S3.SS4.p1.2.m2.2.3.1" xref="S3.SS4.p1.2.m2.2.3.1.cmml">=</mo><mrow id="S3.SS4.p1.2.m2.2.3.3.2" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">Enc</mi><mo id="S3.SS4.p1.2.m2.2.3.3.2a" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">⁡</mo><mrow id="S3.SS4.p1.2.m2.2.3.3.2.1" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.2.m2.2.3.3.2.1.1" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">(</mo><mi mathsize="90%" id="S3.SS4.p1.2.m2.2.2" xref="S3.SS4.p1.2.m2.2.2.cmml">𝐱</mi><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.2.m2.2.3.3.2.1.2" xref="S3.SS4.p1.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.2b"><apply id="S3.SS4.p1.2.m2.2.3.cmml" xref="S3.SS4.p1.2.m2.2.3"><eq id="S3.SS4.p1.2.m2.2.3.1.cmml" xref="S3.SS4.p1.2.m2.2.3.1"></eq><ci id="S3.SS4.p1.2.m2.2.3.2.cmml" xref="S3.SS4.p1.2.m2.2.3.2">𝐡</ci><apply id="S3.SS4.p1.2.m2.2.3.3.1.cmml" xref="S3.SS4.p1.2.m2.2.3.3.2"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">Enc</ci><ci id="S3.SS4.p1.2.m2.2.2.cmml" xref="S3.SS4.p1.2.m2.2.2">𝐱</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.2c">\mathbf{h}=\operatorname{Enc}(\mathbf{x})</annotation></semantics></math><span id="S3.SS4.p1.3.9" class="ltx_text" style="font-size:90%;">.
The decoder network take previous labels in the sequence and generates a high level representation for next prediction.
We use a feed-forward network for the joint network to combine the information from both the acoustic representations from encoder and the linguistic representations from decoder to make a joint prediction of next word piece.
The loss function for RNN-T parameter optimization is the negative log-posterior of the target label sequence </span><math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mi mathsize="90%" id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\mathbf{y}</annotation></semantics></math><span id="S3.SS4.p1.3.10" class="ltx_text" style="font-size:90%;">:</span></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathcal{L}=-\log P(\mathbf{y}|\mathbf{h})," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">ℒ</mi><mo mathsize="90%" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mo mathsize="90%" rspace="0.167em" id="S3.E1.m1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.1.1.1.1.1.1.3a" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">⁡</mo><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">𝐲</mi><mo fence="false" mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">𝐡</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><log id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1"></log><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">𝐲</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">𝐡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}=-\log P(\mathbf{y}|\mathbf{h}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.8" class="ltx_p"><span id="S3.SS4.p1.8.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.SS4.p1.4.m1.2" class="ltx_Math" alttext="P(\mathbf{y}|\mathbf{h})=\sum_{\mathbf{\hat{y}}}P(\mathbf{\hat{y}}|\mathbf{h})" display="inline"><semantics id="S3.SS4.p1.4.m1.2a"><mrow id="S3.SS4.p1.4.m1.2.2" xref="S3.SS4.p1.4.m1.2.2.cmml"><mrow id="S3.SS4.p1.4.m1.1.1.1" xref="S3.SS4.p1.4.m1.1.1.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.4.m1.1.1.1.3" xref="S3.SS4.p1.4.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.4.m1.1.1.1.2" xref="S3.SS4.p1.4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.SS4.p1.4.m1.1.1.1.1.1" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.4.m1.1.1.1.1.1.2" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.4.m1.1.1.1.1.1.1" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.4.m1.1.1.1.1.1.1.2" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.2.cmml">𝐲</mi><mo fence="false" mathsize="90%" id="S3.SS4.p1.4.m1.1.1.1.1.1.1.1" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.1.cmml">|</mo><mi mathsize="90%" id="S3.SS4.p1.4.m1.1.1.1.1.1.1.3" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.3.cmml">𝐡</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.4.m1.1.1.1.1.1.3" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.111em" id="S3.SS4.p1.4.m1.2.2.3" xref="S3.SS4.p1.4.m1.2.2.3.cmml">=</mo><mrow id="S3.SS4.p1.4.m1.2.2.2" xref="S3.SS4.p1.4.m1.2.2.2.cmml"><msub id="S3.SS4.p1.4.m1.2.2.2.2" xref="S3.SS4.p1.4.m1.2.2.2.2.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S3.SS4.p1.4.m1.2.2.2.2.2" xref="S3.SS4.p1.4.m1.2.2.2.2.2.cmml">∑</mo><mover accent="true" id="S3.SS4.p1.4.m1.2.2.2.2.3" xref="S3.SS4.p1.4.m1.2.2.2.2.3.cmml"><mi mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.2.3.2" xref="S3.SS4.p1.4.m1.2.2.2.2.3.2.cmml">𝐲</mi><mo mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.2.3.1" xref="S3.SS4.p1.4.m1.2.2.2.2.3.1.cmml">^</mo></mover></msub><mrow id="S3.SS4.p1.4.m1.2.2.2.1" xref="S3.SS4.p1.4.m1.2.2.2.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.3" xref="S3.SS4.p1.4.m1.2.2.2.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.4.m1.2.2.2.1.2" xref="S3.SS4.p1.4.m1.2.2.2.1.2.cmml">​</mo><mrow id="S3.SS4.p1.4.m1.2.2.2.1.1.1" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.2" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.cmml"><mover accent="true" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.2" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.2.cmml">𝐲</mi><mo mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.1" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.1.cmml">^</mo></mover><mo fence="false" mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.1" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.1.cmml">|</mo><mi mathsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.3" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.3.cmml">𝐡</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.3" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m1.2b"><apply id="S3.SS4.p1.4.m1.2.2.cmml" xref="S3.SS4.p1.4.m1.2.2"><eq id="S3.SS4.p1.4.m1.2.2.3.cmml" xref="S3.SS4.p1.4.m1.2.2.3"></eq><apply id="S3.SS4.p1.4.m1.1.1.1.cmml" xref="S3.SS4.p1.4.m1.1.1.1"><times id="S3.SS4.p1.4.m1.1.1.1.2.cmml" xref="S3.SS4.p1.4.m1.1.1.1.2"></times><ci id="S3.SS4.p1.4.m1.1.1.1.3.cmml" xref="S3.SS4.p1.4.m1.1.1.1.3">𝑃</ci><apply id="S3.SS4.p1.4.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.p1.4.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS4.p1.4.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.2">𝐲</ci><ci id="S3.SS4.p1.4.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.4.m1.1.1.1.1.1.1.3">𝐡</ci></apply></apply><apply id="S3.SS4.p1.4.m1.2.2.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2"><apply id="S3.SS4.p1.4.m1.2.2.2.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m1.2.2.2.2.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2">subscript</csymbol><sum id="S3.SS4.p1.4.m1.2.2.2.2.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2.2"></sum><apply id="S3.SS4.p1.4.m1.2.2.2.2.3.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2.3"><ci id="S3.SS4.p1.4.m1.2.2.2.2.3.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2.3.1">^</ci><ci id="S3.SS4.p1.4.m1.2.2.2.2.3.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.2.3.2">𝐲</ci></apply></apply><apply id="S3.SS4.p1.4.m1.2.2.2.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1"><times id="S3.SS4.p1.4.m1.2.2.2.1.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.2"></times><ci id="S3.SS4.p1.4.m1.2.2.2.1.3.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.3">𝑃</ci><apply id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.1">conditional</csymbol><apply id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2"><ci id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.1">^</ci><ci id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.2.2">𝐲</ci></apply><ci id="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.SS4.p1.4.m1.2.2.2.1.1.1.1.3">𝐡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m1.2c">P(\mathbf{y}|\mathbf{h})=\sum_{\mathbf{\hat{y}}}P(\mathbf{\hat{y}}|\mathbf{h})</annotation></semantics></math><span id="S3.SS4.p1.8.2" class="ltx_text" style="font-size:90%;">, </span><math id="S3.SS4.p1.5.m2.1" class="ltx_Math" alttext="\mathbf{\hat{y}}\in\mathcal{A}" display="inline"><semantics id="S3.SS4.p1.5.m2.1a"><mrow id="S3.SS4.p1.5.m2.1.1" xref="S3.SS4.p1.5.m2.1.1.cmml"><mover accent="true" id="S3.SS4.p1.5.m2.1.1.2" xref="S3.SS4.p1.5.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.5.m2.1.1.2.2" xref="S3.SS4.p1.5.m2.1.1.2.2.cmml">𝐲</mi><mo mathsize="90%" id="S3.SS4.p1.5.m2.1.1.2.1" xref="S3.SS4.p1.5.m2.1.1.2.1.cmml">^</mo></mover><mo mathsize="90%" id="S3.SS4.p1.5.m2.1.1.1" xref="S3.SS4.p1.5.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.SS4.p1.5.m2.1.1.3" xref="S3.SS4.p1.5.m2.1.1.3.cmml">𝒜</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m2.1b"><apply id="S3.SS4.p1.5.m2.1.1.cmml" xref="S3.SS4.p1.5.m2.1.1"><in id="S3.SS4.p1.5.m2.1.1.1.cmml" xref="S3.SS4.p1.5.m2.1.1.1"></in><apply id="S3.SS4.p1.5.m2.1.1.2.cmml" xref="S3.SS4.p1.5.m2.1.1.2"><ci id="S3.SS4.p1.5.m2.1.1.2.1.cmml" xref="S3.SS4.p1.5.m2.1.1.2.1">^</ci><ci id="S3.SS4.p1.5.m2.1.1.2.2.cmml" xref="S3.SS4.p1.5.m2.1.1.2.2">𝐲</ci></apply><ci id="S3.SS4.p1.5.m2.1.1.3.cmml" xref="S3.SS4.p1.5.m2.1.1.3">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m2.1c">\mathbf{\hat{y}}\in\mathcal{A}</annotation></semantics></math><span id="S3.SS4.p1.8.3" class="ltx_text" style="font-size:90%;">. </span><math id="S3.SS4.p1.6.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S3.SS4.p1.6.m3.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.SS4.p1.6.m3.1.1" xref="S3.SS4.p1.6.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m3.1b"><ci id="S3.SS4.p1.6.m3.1.1.cmml" xref="S3.SS4.p1.6.m3.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m3.1c">\mathcal{A}</annotation></semantics></math><span id="S3.SS4.p1.8.4" class="ltx_text" style="font-size:90%;"> is the set of all possible alignments including blank labels between encoder feature representation </span><math id="S3.SS4.p1.7.m4.1" class="ltx_Math" alttext="\mathbf{h}" display="inline"><semantics id="S3.SS4.p1.7.m4.1a"><mi mathsize="90%" id="S3.SS4.p1.7.m4.1.1" xref="S3.SS4.p1.7.m4.1.1.cmml">𝐡</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m4.1b"><ci id="S3.SS4.p1.7.m4.1.1.cmml" xref="S3.SS4.p1.7.m4.1.1">𝐡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m4.1c">\mathbf{h}</annotation></semantics></math><span id="S3.SS4.p1.8.5" class="ltx_text" style="font-size:90%;"> and target label sequence </span><math id="S3.SS4.p1.8.m5.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.SS4.p1.8.m5.1a"><mi mathsize="90%" id="S3.SS4.p1.8.m5.1.1" xref="S3.SS4.p1.8.m5.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m5.1b"><ci id="S3.SS4.p1.8.m5.1.1.cmml" xref="S3.SS4.p1.8.m5.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m5.1c">\mathbf{y}</annotation></semantics></math><span id="S3.SS4.p1.8.6" class="ltx_text" style="font-size:90%;">.
The Adam algorithm </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.8.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.SS4.p1.8.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS4.p1.8.9" class="ltx_text" style="font-size:90%;"> is used for the numerical optimization during training, with a learning rate scheduler in three stages: linear warm-up, hold and exponential decay.</span></p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2106.07803/assets/multi_style_tts.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="551" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of proposed Multi-context TTS system.</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Continual learning with multi-stage training</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p"><span id="S3.SS5.p1.1.1" class="ltx_text" style="font-size:90%;">Continual learning (CL) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS5.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.SS5.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS5.p1.1.4" class="ltx_text" style="font-size:90%;"> imitates the incremental life-long learning ability in humans and animals by machine, through acquiring data over time, and learning, fine-tuning, and transferring knowledge over various tasks.
CL reduces the cost from model training with repeated visit to a large amount of data, and it also reduces or even removes the dependency on previously harvested training data.</span></p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.6" class="ltx_p"><span id="S3.SS5.p2.6.1" class="ltx_text" style="font-size:90%;">In this work, we apply CL and let a general-purpose ASR model learn for a new application task of recognizing medication names, by
continually exposing the ASR model to the synthetic speech containing the medication names.
To prevent catastrophic forgetting, we propose a multi-stage training strategy for continual learning,
including freezing the LSTM layers of encoder during fine-tuning with synthetic speech data and unfreezing all layers in later stages.
When parameters are unfrozen, to ensure that the learned parameters do not deviate too much when trained with synthetic speech for a new application, an elastic penalty term is introduced to the RNN-T loss function that minimizes the distance between the new and previous model parameters:</span></p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="\mathcal{J}=\lambda\sum(\theta_{i,pre}-\theta^{*}_{i,cur})^{2}," display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml">𝒥</mi><mo mathsize="90%" id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.cmml"><mi mathsize="90%" id="S3.E2.m1.5.5.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" rspace="0em" stretchy="true" id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">∑</mo><msup id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml">θ</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">i</mi><mo mathsize="90%" id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1a" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.4" xref="S3.E2.m1.2.2.2.2.1.4.cmml">e</mi></mrow></mrow></msub><mo mathsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2.cmml">θ</mi><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">i</mi><mo mathsize="90%" id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.2.1.cmml"><mi mathsize="90%" id="S3.E2.m1.4.4.2.2.1.2" xref="S3.E2.m1.4.4.2.2.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.4.4.2.2.1.3" xref="S3.E2.m1.4.4.2.2.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1a" xref="S3.E2.m1.4.4.2.2.1.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.4.4.2.2.1.4" xref="S3.E2.m1.4.4.2.2.1.4.cmml">r</mi></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3.cmml">∗</mo></msubsup></mrow><mo maxsize="90%" minsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn mathsize="90%" id="S3.E2.m1.5.5.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"></eq><ci id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3">𝒥</ci><apply id="S3.E2.m1.5.5.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><times id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2"></times><ci id="S3.E2.m1.5.5.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.3">𝜆</ci><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><sum id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2"></sum><apply id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.2">𝜃</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑖</ci><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><times id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></times><ci id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">𝑝</ci><ci id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3">𝑟</ci><ci id="S3.E2.m1.2.2.2.2.1.4.cmml" xref="S3.E2.m1.2.2.2.2.1.4">𝑒</ci></apply></list></apply><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2">𝜃</ci><times id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3"></times></apply><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2"><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">𝑖</ci><apply id="S3.E2.m1.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1"><times id="S3.E2.m1.4.4.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"></times><ci id="S3.E2.m1.4.4.2.2.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.2">𝑐</ci><ci id="S3.E2.m1.4.4.2.2.1.3.cmml" xref="S3.E2.m1.4.4.2.2.1.3">𝑢</ci><ci id="S3.E2.m1.4.4.2.2.1.4.cmml" xref="S3.E2.m1.4.4.2.2.1.4">𝑟</ci></apply></list></apply></apply><cn type="integer" id="S3.E2.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\mathcal{J}=\lambda\sum(\theta_{i,pre}-\theta^{*}_{i,cur})^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p2.5" class="ltx_p"><span id="S3.SS5.p2.5.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi mathsize="90%" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">i</annotation></semantics></math><span id="S3.SS5.p2.5.2" class="ltx_text" style="font-size:90%;"> indexes the RNN-T decoder parameters, </span><math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="pre" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mrow id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.1" xref="S3.SS5.p2.2.m2.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.2.m2.1.1.1a" xref="S3.SS5.p2.2.m2.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.SS5.p2.2.m2.1.1.4" xref="S3.SS5.p2.2.m2.1.1.4.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><times id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1.1"></times><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">𝑝</ci><ci id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3">𝑟</ci><ci id="S3.SS5.p2.2.m2.1.1.4.cmml" xref="S3.SS5.p2.2.m2.1.1.4">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">pre</annotation></semantics></math><span id="S3.SS5.p2.5.3" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="cur" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mrow id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.1" xref="S3.SS5.p2.3.m3.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.3.m3.1.1.1a" xref="S3.SS5.p2.3.m3.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.SS5.p2.3.m3.1.1.4" xref="S3.SS5.p2.3.m3.1.1.4.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><times id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1.1"></times><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">𝑐</ci><ci id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3">𝑢</ci><ci id="S3.SS5.p2.3.m3.1.1.4.cmml" xref="S3.SS5.p2.3.m3.1.1.4">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">cur</annotation></semantics></math><span id="S3.SS5.p2.5.4" class="ltx_text" style="font-size:90%;"> are the previous and current training stages, respectively, </span><math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><mo mathsize="90%" id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><times id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">*</annotation></semantics></math><span id="S3.SS5.p2.5.5" class="ltx_text" style="font-size:90%;"> indicates the trainable parameters and </span><math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><mi mathsize="90%" id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><ci id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">\lambda</annotation></semantics></math><span id="S3.SS5.p2.5.6" class="ltx_text" style="font-size:90%;"> adjusts forgetting speed.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>WERs on the LibriSpeech.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<td id="S4.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></td>
<td id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training sets</span></td>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">WER</span></td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<td id="S4.T1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.2.2.1.1" class="ltx_text" style="font-size:90%;">real</span></td>
<td id="S4.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.2.2.2.1" class="ltx_text" style="font-size:90%;">synthetic</span></td>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.2.2.3.1" class="ltx_text" style="font-size:90%;">test-clean</span></td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.2.2.4.1" class="ltx_text" style="font-size:90%;">test-other</span></td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.3.1.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.3.2.1" class="ltx_text" style="font-size:90%;">960</span></td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.3.4.1" class="ltx_text" style="font-size:90%;">7.29</span></td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.5.1" class="ltx_text" style="font-size:90%;">17.41</span></td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<td id="S4.T1.3.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.1.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S4.T1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.2.1" class="ltx_text" style="font-size:90%;">480</span></td>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.4.4.4.1" class="ltx_text" style="font-size:90%;">9.90</span></td>
<td id="S4.T1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.4.4.5.1" class="ltx_text" style="font-size:90%;">22.64</span></td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<td id="S4.T1.3.5.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.5.5.1.1" class="ltx_text" style="font-size:90%;">Baseline + TTS</span></td>
<td id="S4.T1.3.5.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.5.5.2.1" class="ltx_text" style="font-size:90%;">480</span></td>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.5.5.3.1" class="ltx_text" style="font-size:90%;">1150</span></td>
<td id="S4.T1.3.5.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.3.5.5.4.1" class="ltx_text" style="font-size:90%;">8.66</span></td>
<td id="S4.T1.3.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.5.5.5.1" class="ltx_text" style="font-size:90%;">20.78</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>NWERs for the application of recognition of medication name. The weight on the left in the column <span id="S4.T2.7.1" class="ltx_text ltx_font_bold">Weights%</span> is the percentage of samples in MST from real data and the weight on the right is the percentage of samples from synthetic data. <span id="S4.T2.8.2" class="ltx_text ltx_font_bold">(R, S)</span> indicates whether real (R) or synthetic (S) audio is used during each stage of training.</figcaption>
<table id="S4.T2.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.9.1.1" class="ltx_tr">
<td id="S4.T2.9.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.9.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></td>
<td id="S4.T2.9.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.9.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Real Data</span></td>
<td id="S4.T2.9.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.9.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Synthetic Data</span></td>
<td id="S4.T2.9.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.9.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Weights%</span></td>
<td id="S4.T2.9.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.9.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Freeze</span></td>
<td id="S4.T2.9.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.9.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Elastic</span></td>
<td id="S4.T2.9.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T2.9.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">NWER</span></td>
</tr>
<tr id="S4.T2.9.2.2" class="ltx_tr">
<td id="S4.T2.9.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.1.1" class="ltx_text" style="font-size:90%;">(hours)</span></td>
<td id="S4.T2.9.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.2.1" class="ltx_text" style="font-size:90%;">(hours)</span></td>
<td id="S4.T2.9.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.3.1" class="ltx_text" style="font-size:90%;">(R, S)</span></td>
<td id="S4.T2.9.2.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Encoder</span></td>
<td id="S4.T2.9.2.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Penalty</span></td>
<td id="S4.T2.9.2.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.6.1" class="ltx_text" style="font-size:90%;">Dev-Gen</span></td>
<td id="S4.T2.9.2.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.2.2.7.1" class="ltx_text" style="font-size:90%;">Eval-Gen</span></td>
<td id="S4.T2.9.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T2.9.2.2.8.1" class="ltx_text" style="font-size:90%;">Eval-Med</span></td>
</tr>
<tr id="S4.T2.9.3.3" class="ltx_tr">
<td id="S4.T2.9.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.1.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S4.T2.9.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.2.1" class="ltx_text" style="font-size:90%;">50K</span></td>
<td id="S4.T2.9.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.5.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.7.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T2.9.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.3.3.8.1" class="ltx_text" style="font-size:90%;">100</span></td>
<td id="S4.T2.9.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.3.3.9.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T2.9.4.4" class="ltx_tr">
<td id="S4.T2.9.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.1.1" class="ltx_text" style="font-size:90%;">Stage 1</span></td>
<td id="S4.T2.9.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.2.1" class="ltx_text" style="font-size:90%;">50K</span></td>
<td id="S4.T2.9.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.3.1" class="ltx_text" style="font-size:90%;">5k</span></td>
<td id="S4.T2.9.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.4.1" class="ltx_text" style="font-size:90%;">(95, 5)</span></td>
<td id="S4.T2.9.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.5.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T2.9.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.6.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.7.1" class="ltx_text" style="font-size:90%;">100.99</span></td>
<td id="S4.T2.9.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.9.4.4.8.1" class="ltx_text" style="font-size:90%;">101.32</span></td>
<td id="S4.T2.9.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.9.4.4.9.1" class="ltx_text" style="font-size:90%;">21.06</span></td>
</tr>
<tr id="S4.T2.9.5.5" class="ltx_tr">
<td id="S4.T2.9.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.1.1" class="ltx_text" style="font-size:90%;">Stage 2</span></td>
<td id="S4.T2.9.5.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.2.1" class="ltx_text" style="font-size:90%;">50K</span></td>
<td id="S4.T2.9.5.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.3.1" class="ltx_text" style="font-size:90%;">5k</span></td>
<td id="S4.T2.9.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.4.1" class="ltx_text" style="font-size:90%;">(98, 2)</span></td>
<td id="S4.T2.9.5.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.5.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.5.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.6.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.5.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.7.1" class="ltx_text" style="font-size:90%;">100.54</span></td>
<td id="S4.T2.9.5.5.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.5.5.8.1" class="ltx_text" style="font-size:90%;">100.98</span></td>
<td id="S4.T2.9.5.5.9" class="ltx_td ltx_align_center"><span id="S4.T2.9.5.5.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">13.70</span></td>
</tr>
<tr id="S4.T2.9.6.6" class="ltx_tr">
<td id="S4.T2.9.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.1.1" class="ltx_text" style="font-size:90%;">Stage 3</span></td>
<td id="S4.T2.9.6.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.2.1" class="ltx_text" style="font-size:90%;">50K</span></td>
<td id="S4.T2.9.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.6.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.5.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.6.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.6.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T2.9.6.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.7.1" class="ltx_text" style="font-size:90%;">100.14</span></td>
<td id="S4.T2.9.6.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.9.6.6.8.1" class="ltx_text" style="font-size:90%;">100.89</span></td>
<td id="S4.T2.9.6.6.9" class="ltx_td ltx_align_center"><span id="S4.T2.9.6.6.9.1" class="ltx_text" style="font-size:90%;">21.47</span></td>
</tr>
<tr id="S4.T2.9.7.7" class="ltx_tr">
<td id="S4.T2.9.7.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.1.1" class="ltx_text" style="font-size:90%;">Stage 4</span></td>
<td id="S4.T2.9.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.2.1" class="ltx_text" style="font-size:90%;">50K</span></td>
<td id="S4.T2.9.7.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.7.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.9.7.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.5.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.7.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.6.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T2.9.7.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">99.18</span></td>
<td id="S4.T2.9.7.7.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.9.7.7.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">99.72</span></td>
<td id="S4.T2.9.7.7.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.9.7.7.9.1" class="ltx_text" style="font-size:90%;">34.56</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiments on LibriSpeech: synthetic speech for reducing transcribed data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">To evaluate the effectiveness of synthetic data, we first perform experiments on the LibriSpeech dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">.
In these experiments, we use 64-dimensional Log-Mel-Frequency features extracted with 25ms window and 10ms shift. Each feature vector is stacked with 2 frames to the left and downsampled by a factor of 3 corresponding to a frame rate of 30msec. For the experiments on LibriSpeech data, we use six-layer LSTMs with 1024 units in the encoder. The decoder is a two-layer LSTM with 1024 units in each layer. The output size of the recognition encoder and the decoder is set to 640. We use a one-layer feed-forward joint network with 512 units and tanh activation. The output softmax layer dimensionality is 2501 which corresponds to blank label and 2500 word pieces: the most likely subword segmentation from a unigram word piece model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S4.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.7" class="ltx_text" style="font-size:90%;">. We use an adaptive variant of SpecAugment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.10" class="ltx_text" style="font-size:90%;">, as proposed in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S4.SS1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.13" class="ltx_text" style="font-size:90%;">. We use Adam algorithm </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S4.SS1.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.16" class="ltx_text" style="font-size:90%;"> for optimization of all models, and the learning rate is scheduled based on warm-up, hold and decay strategy as proposed in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS1.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.19" class="ltx_text" style="font-size:90%;">. For each experimental run, we chose the best model based on its performance on the development set.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">LibriSpeech contains 960 hours of read speech data for training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S4.SS1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p2.1.4" class="ltx_text" style="font-size:90%;">.
As an ASR baseline with a limited amount of audio data, we assume that only half of the LibriSpeech training data is available,
i.e. 480 hours training data randomly selected from the all training data.
As shown in Table </span><a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.1.5" class="ltx_text" style="font-size:90%;">, an RNN-T model trained with 480 hours of data is 35.8% relatively worse on test-clean
when compared to an RNN-T model trained with all 960 hours training data.
We then synthesize about 1150 hours audio data using our multi-context TTS system, and the input texts for TTS are the transcriptions of the missing 480 hours data.
This TTS training set contains about 48k unique input texts, and each text utterance is synthesized with randomly selected 24 voice profiles from the total of 500 available voice profiles.
We then trained an RNN-T model using MST combining with 480 hours real data and 1150 hours synthetic speech.
Compared to the baseline model trained with 480 hours real data alone, this improves the performance on test-clean by 12.5% relative (Table </span><a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.1.6" class="ltx_text" style="font-size:90%;">).</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiments in real application: synthetic speech for medication names recognition</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">We then expand a general-purpose ASR model for a new application of medication names recognition.
This new application has no available real recordings for ASR model training.
In the following set of experiments, we use synthetic speech to teach a general-purpose ASR model to recognize medication names.</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">We use a slightly different RNN-T architecture from previous LibriSpeech experiments. The encoder now consists of 5 LSTM layers and the softmax layer has an output vocabulary size of 4001 word pieces including the blank label.
Note that, the results in this section are reported in normalized WER (NWER) numbers, which is the regular word error rate (WER) divided by the WER of the baseline model on the same test set and then multiplied by 100. Therefore, the NWERs for baseline model are 100 for all test sets as shown in Table </span><a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS2.p2.1.2" class="ltx_text" style="font-size:90%;">.
The baseline RNN-T model for general-purpose application is trained with a dataset of 50,000 hours real human utterances.
This dataset is a collection of de-identified production utterances from voice-controlled far-field devices. A development set (Dev-Gen) and an evaluation set (Eval-Gen) are constructed with the same type of utterances, consisting of about 50 hours and 160 hours of data respectively. These two test sets are used to monitor the performance change on existing applications.
To evaluate the performance on the new application of medication name recognition, we collected 8 hours of real human data containing utterances with medication names (Eval-Med).</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">To support feature expansion without real training data, we prepared 5000 hours TTS synthetic data.
The texts are generated by randomly combining 150 unique text utterance
templates and 600 common medication names.
For each text utterance generated, we sample 32 voice profiles from 500 voice profiles.
The generated clean synthetic audio is corrupted with noise and reverberation on-the-fly for RNN-T model training.
With MST, the corrupted synthetic audio is combined with real recordings with configured data ratio parameters.
With continual learning, we start with the baseline general-purpose RNN-T model and fine-tune it so that the recognition performance on general test sets (Eval-Gen) maintains while the performance on the test set for medication names (Eval-Med) is largely improved.</span></p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text" style="font-size:90%;">One challenge in such continual learning is a balance between forgetting learned knowledge which causes degradation on Eval-Gen and learning new knowledge which strives to improve performance on Eval-Med.
We use multi-stage training to address this challenge, and our experiments concluded with 4 critical stages as shown in Table </span><a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS2.p4.1.2" class="ltx_text" style="font-size:90%;">.
The first stage is to fine-tune the baseline model with batches of data that contain 95% real data and 5% synthetic data where we fix the RNN-T encoder parameters.
We train in this way for 57k iterations, during which process the learning rate decays from 5e-5 to 1e-5. This stages ramps up the parameters for decoder and joint network for the new application of medication name recognition.
In the second stage, we further fine-tune the model with both real and synthetic utterances with the portion of 98% and 2% in each batch and fixed learning rate of 1e-5.
As shown in Table </span><a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS2.p4.1.3" class="ltx_text" style="font-size:90%;">, this improves the recognition performance on Eval-Med further from an NWER of 21.06 to 13.70.
In our experiments we found the first training stage with encoder freezing critical, and removing Stage 1 led to performance degradation.
At the same time, model at the end of stage 2 showed a small degradation compared to baseline on general test sets.</span></p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text" style="font-size:90%;">To recover the degradation, in the third and fourth stages, we only fine-tune models with real human speech. In the third stage, we include an elastic penalty as described in section </span><a href="#S3.SS5" title="3.5 Continual learning with multi-stage training ‣ 3 Technical Approaches ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3.5</span></a><span id="S4.SS2.p5.1.2" class="ltx_text" style="font-size:90%;"> to minimize the deviation of the model parameters from the previous stage as the model has well learned medication names. We further fine-tune the model in the fourth stage without such regularization but with a small learning rate of 1e-5 to ensure the performance of the model doesn’t degrade from the baseline.
Note that in our experiments, we found the third stage critical and directly jumping from Stage 2 to Stage 4 led to worse results.
As shown in Table </span><a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ SynthASR: Unlocking Synthetic Data for Speech Recognition" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS2.p5.1.3" class="ltx_text" style="font-size:90%;">, the final model from Stage 4 achieved slightly better performance on both general test sets compared to baseline model, and at the same time the recognition performance on Eval-Med is more than 65% better than the baseline model.
This is achieved with 5k hours of synthetic training data without real recordings for medication names .</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">In this work, we propose to use synthetic speech for E2E ASR model training to reduce both data costs and production data reliance. In addition, we demonstrated how to effectively and incrementally improve ASR for a new application that customer audio data is not available at all. Using continual learning with our proposed multi-stage training, the best system relatively improves the WER on the new application by more than 65% without compromise on the existing application. While the value of synthetic speech as ASR training data remains less than that of real speech, but synthetic speech shows great promise in training large-scale ASR for new applications.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">We would like to thank Charles Chang and Paul MacCabe for the high-level support of this research.
In addition, we would like to acknowledge the Alexa Data Synthetic and Alexa ASR teams for
providing the infrastructure that this work has benefited from.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, “Connectionist
temporal classification: Labelling unsegmented sequence data with recurrent
neural networks,” in </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd International Conference
on Machine Learning</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, ser. ICML ’06.   New York, NY, USA: ACM, 2006, pp. 369–376.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent
neural networks,” in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 31st International Conference
on Machine Learning</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, ser. Proceedings of Machine Learning Research, E. P.
Xing and T. Jebara, Eds., vol. 32, no. 2.   Bejing, China: PMLR, 22–24 Jun 2014, pp. 1764–1772.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Graves, “Sequence transduction with recurrent neural networks,”
</span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/1211.3711, 2012.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell: A
neural network for large vocabulary conversational speech recognition,” in
</span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,
D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shangguan,
B. Li, G. Pundak, K. C. Sim, T. Bagby, S. yiin Chang, K. Rao, and
A. Gruenstein, “Streaming end-to-end speech recognition for mobile
devices,” in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">.   IEEE,
2019, pp. 6381–6385.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
S. Ueno, M. Mimura, S. Sakai, and T. Kawahara, “Multi-speaker
sequence-to-sequence speech synthesis for data augmentation in
acoustic-to-word speech recognition,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 6161–6165.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
N. Rossenbach, A. Zeyer, R. Schlüter, and H. Ney, “Generating
synthetic audio data for attention-based speech recognition systems,” in
</span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 7069–7073.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
C. Du and K. Yu, “Speaker augmentation for low resource speech
recognition,” in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 7719–7723.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
X. Zheng, Y. Liu, D. Gunceler, and D. Willett, “Using synthetic audio to
improve the recognition of out-of-vocabulary words in end-to-end ASR
systems,” in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
M. Mimura, S. Ueno, H. Inaguma, S. Sakai, and T. Kawahara,
“Leveraging sequence-to-sequence speech synthesis for enhancing
acoustic-to-word speech recognition,” in </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE Spoken Language
Technology Workshop (SLT)</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 477–484.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,
“SpecAugment: A simple data augmentation method for automatic speech
recognition,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interspeech</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, Sep 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap, A. Sriram,
V. Liptchinsky, and R. Collobert, “End-to-end ASR: from supervised to
semi-supervised learning with modern architectures,” </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, vol.
abs/1911.08460, 2019. [Online]. Available:
http://arxiv.org/abs/1911.08460
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
S. Ling, Y. Liu, J. Salazar, and K. Kirchhoff, “Deep contextualized
acoustic representations for semi-supervised speech recognition,” in
</span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 6429–6433.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of
deep networks using model averaging,” </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/1602.05629,
2016. [Online]. Available: http://arxiv.org/abs/1602.05629
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally, “Deep gradient compression:
Reducing the communication bandwidth for distributed training,” in </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">6th
International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
E. McDermott, H. Sak, and E. Variani, “A density ratio approach to language
model fusion in end-to-end automatic speech recognition,” in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
Automatic Speech Recognition and Understanding Workshop, ASRU 2019,
Singapore, December 14-18, 2019</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2019, pp. 434–441.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid autoregressive
transducer (hat),” in </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 6139–6143.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen, R. Zhao,
J. Li, and Y. Gong, “Internal language model estimation for domain-adaptive
end-to-end speech recognition,” in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE Spoken Language
Technology Workshop (SLT)</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 243–250.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
R. Lippmann, E. Martin, and D. Paul, “Multi-style training for robust
isolated-word speech recognition,” in </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP ’87. IEEE International
Conference on Acoustics, Speech, and Signal Processing</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, vol. 12, 1987, pp.
705–708.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A
study on data augmentation of reverberant speech for robust speech
recognition,” in </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 5220–5224.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual
lifelong learning with neural networks: A review,” </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Networks</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">,
vol. 113, pp. 54–71, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
A. Gepperth and C. Karaoguz, “A bio-inspired incremental learning architecture
for applied perceptual problems,” </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Cognitive Computation</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, vol. 8, pp.
924 – 934, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
N. Prateek, M. Lajszczak, R. Barra-Chicote, T. Drugman, J. Lorenzo-Trueba,
T. Merritt, S. Ronanki, and T. Wood, “In other news: a bi-style
text-to-speech model for synthesizing newscaster voice with limited data,”
in </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 2 (Industry
Papers)</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, A. Loukina, M. Morales, and R. Kumar, Eds.   Association for Computational Linguistics, 2019, pp.
205–213.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
J. Rohnke, T. Merritt, J. Lorenzo-Trueba, A. Gabrys, V. Aggarwal, A. Moinet,
and R. Barra-Chicote, “Parallel WaveNet conditioned on VAE latent
vectors,” 2020. [Online]. Available: https://arxiv.org/abs/2012.09703
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
J. S. Chung, J. Huh, S. Mun, M. Lee, H.-S. Heo, S. Choe, C. Ham, S. Jung, B.-J.
Lee, and I. Han, “In defence of metric learning for speaker recognition,”
in </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech 2020</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 2977–2981.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
B. Li, S. yiin Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and Y. Wu,
“Towards fast and accurate streaming end-to-end ASR,” in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP
2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 6069–6073.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural
Computation</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, vol. 9, no. 8, pp. 1735–1780, 1997.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in
</span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR
corpus based on public domain audio books,” in </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp.
5206–5210.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
T. Kudo and J. Richardson, “SentencePiece: A simple and language
independent subword tokenizer and detokenizer for neural text processing,”
in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">, Nov. 2018, pp. 66–71.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu,
“SpecAugment on large scale datasets,” </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, May 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.07802" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.07803" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.07803">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.07803" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.07804" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 12:02:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
