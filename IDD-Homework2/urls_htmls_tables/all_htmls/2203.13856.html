<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2203.13856] Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets</title><meta property="og:description" content="Deep learning has been proposed for the assessment and classification of medical images. However, many medical image databases with appropriately labeled and annotated images are small and imbalanced, and thus unsuitab‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2203.13856">

<!--Generated on Mon Mar 11 09:18:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\credit</span>
<p id="p1.2" class="ltx_p">developed the algorithms, performed the analysis and finalized the manuscript</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">[orcid=0000-0002-6442-8343]
<span id="p2.1.1" class="ltx_ERROR undefined">\credit</span>managed the data and designed the software</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">[orcid=0000-0002-2867-4838]
<span id="p3.1.1" class="ltx_ERROR undefined">\credit</span>managed the data and designed the software</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">[orcid=0000-0002-6494-7514]
<span id="p4.1.1" class="ltx_ERROR undefined">\credit</span>conceptualized, designed the project and finalized the manuscript</p>
</div>
<div id="p5" class="ltx_para">
<span id="p5.1" class="ltx_ERROR undefined">\credit</span>
<p id="p5.2" class="ltx_p">responsible for the clinical assessment of the images and script</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">[orcid=0000-0003-3529-3109]
<span id="p6.1.1" class="ltx_ERROR undefined">\credit</span>managed the data and designed the software</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">[orcid=0000-0003-3602-4023]
<span id="p7.1.1" class="ltx_ERROR undefined">\cormark</span>[1]
<span id="p7.1.2" class="ltx_ERROR undefined">\credit</span>conceptualized, designed the project and finalized the manuscript</p>
</div>
<div id="p8" class="ltx_para">
<span id="p8.1" class="ltx_ERROR undefined">\cortext</span>
<p id="p8.2" class="ltx_p">[cor1]Corresponding author</p>
</div>
<h1 class="ltx_title ltx_title_document">Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Guilherme C. Oliveira
</span><span class="ltx_author_notes">gc.oliveira@unesp.br</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Gustavo H. Rosa
</span><span class="ltx_author_notes">gustavo.rosa@unesp.br</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Daniel C. G. Pedronette
</span><span class="ltx_author_notes">daniel.pedronette@unesp.br</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jo√£o P. Papa
</span><span class="ltx_author_notes">joao.papa@unesp.br</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Himeesh Kumar
</span><span class="ltx_author_notes">himeesh@gmail.com</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Leandro A. Passos
</span><span class="ltx_author_notes">L.passosjunior@wlv.ac.uk</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Dinesh Kumar
</span><span class="ltx_author_notes">dinesh@rmit.edu.au
<span class="ltx_contact ltx_role_address">School of Sciences, S√£o Paulo State University,
S√£o Paulo,
Brazil
</span>
<span class="ltx_contact ltx_role_address">School of Engineering, Royal Melbourne Institute of Technology,
Victoria,
Australia
</span>
<span class="ltx_contact ltx_role_address">Centre of Eye Research, University of Melbourne,
Victoria,
Australia
</span>
<span class="ltx_contact ltx_role_address">CMI Lab, School of Engineering and Informatics, University of Wolverhampton,
Wolverhampton,
UK
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep learning has been proposed for the assessment and classification of medical images. However, many medical image databases with appropriately labeled and annotated images are small and imbalanced, and thus unsuitable to train and validate such models. The option is to generate synthetic images and one successful technique has been patented which limits its use for others. We have developed a free-access, alternate method for generating synthetic high-resolution images using Generative Adversarial Networks (GAN) for data augmentation and showed their effectiveness using eye-fundus images for Age-Related Macular Degeneration (AMD) identification. Ten different GAN architectures were compared to generate synthetic eye-fundus images with and without AMD. Data from three public databases were evaluated using the Fr√©chet Inception Distance (FID), two clinical experts and deep-learning classification. The results show that StyleGAN2 reached the lowest FID (166.17), and clinicians could not accurately differentiate between real and synthetic images. ResNet-18 architecture obtained the best performance with 85% accuracy and outperformed the two experts in detecting AMD fundus images, whose average accuracy was 77.5%. These results are similar to a recently patented method, and will provide an alternative to generating high-quality synthetic medical images. Free access has been provided to the entire method to facilitate the further development of this field.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Medical images <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>Age-Related Macular Degeneration <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>Data Augmentation <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>Deep Learning <span id="id5.id4" class="ltx_ERROR undefined">\sep</span>Generative Adversarial Networks <span id="id6.id5" class="ltx_ERROR undefined">\sep</span>StyleGAN2

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning (DL) techniques are suitable for automated analysis of medical images and have been shown to outperform experts in several fields. Examples of applications of such approaches are to perform discriminative retinal image analysis, such as automated classification of fundus images for detection of referable diabetic retinopathy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">40</span></a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, retinopathy of prematurity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, exudates on the retina¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and age-related macular degeneration (AMD)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and also for granular AMD severity classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. However the potential of this approach for medical images is limited because they require a large numbers of annotated images that are not currently available. It is also essential that the datasets should be balanced to avoid biased training. Usually, annotated medical image datasets are not sufficiently large nor balanced, and this is often due to issues of privacy of medical data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2203.13856/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="108" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Retina fundus image positive to age-related macular degeneration identified by the presence of drusen. The image was extracted from the iChallenge-AMD dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Different approaches have been used to address this shortcoming such as transfer learning using previously trained models, and data augmentation. Pre-trained networks have limitations, for they are usually trained in a different learning domain. Additional images generated by spatial transformations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">39</span></a>]</cite> are acknowledged not to guarantee data variability. A promising solution to generate synthetic images lies in the Generative Adversarial Networks (GANs)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Such an approach performs data augmentation by competitively creating new samples, i.e., a generator attempts to create synthetic images to fool the discriminator, which then tries to identify whether they are fake or real. GANs have provided exceptional results over a wide variety of medical applications, such as liver lesion classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, Barrett‚Äôs esophagus identification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">44</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">45</span></a>]</cite>, and chest pathology recognition¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">42</span></a>]</cite>. However, these did not address problems with multi-lesion medical images in a class, when there is large class imbalance, or where there is a variable image quality in the dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.5" class="ltx_p">Bellemo et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> described the possible advantages and limitations towards synthetic retina image generation using GANs. The authors highlighted the potential clinical applications of GANs concerning early- and late-stage AMD classification. Das et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed a quick and reliable super-resolution approach concerning Optical Coherence Tomography (OCT) imagery using GANs, achieving a classification accuracy of <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="96.54\%" display="inline"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mn id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">96.54</mn><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">96.54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">96.54\%</annotation></semantics></math>. Burlina et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> trained a Progressive GAN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on <math id="S1.p3.2.m2.2" class="ltx_Math" alttext="133,821" display="inline"><semantics id="S1.p3.2.m2.2a"><mrow id="S1.p3.2.m2.2.3.2" xref="S1.p3.2.m2.2.3.1.cmml"><mn id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">133</mn><mo id="S1.p3.2.m2.2.3.2.1" xref="S1.p3.2.m2.2.3.1.cmml">,</mo><mn id="S1.p3.2.m2.2.2" xref="S1.p3.2.m2.2.2.cmml">821</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.2b"><list id="S1.p3.2.m2.2.3.1.cmml" xref="S1.p3.2.m2.2.3.2"><cn type="integer" id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">133</cn><cn type="integer" id="S1.p3.2.m2.2.2.cmml" xref="S1.p3.2.m2.2.2">821</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.2c">133,821</annotation></semantics></math> color fundus images from <math id="S1.p3.3.m3.2" class="ltx_Math" alttext="4,613" display="inline"><semantics id="S1.p3.3.m3.2a"><mrow id="S1.p3.3.m3.2.3.2" xref="S1.p3.3.m3.2.3.1.cmml"><mn id="S1.p3.3.m3.1.1" xref="S1.p3.3.m3.1.1.cmml">4</mn><mo id="S1.p3.3.m3.2.3.2.1" xref="S1.p3.3.m3.2.3.1.cmml">,</mo><mn id="S1.p3.3.m3.2.2" xref="S1.p3.3.m3.2.2.cmml">613</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.3.m3.2b"><list id="S1.p3.3.m3.2.3.1.cmml" xref="S1.p3.3.m3.2.3.2"><cn type="integer" id="S1.p3.3.m3.1.1.cmml" xref="S1.p3.3.m3.1.1">4</cn><cn type="integer" id="S1.p3.3.m3.2.2.cmml" xref="S1.p3.3.m3.2.2">613</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.m3.2c">4,613</annotation></semantics></math> age-related eye disease individuals to learn how to generate synthetic fundus images with and without AMD. Two retina specialists were asked to distinguish between images with and without AMD for original and synthetic images. Recognition rates varied from around <math id="S1.p3.4.m4.1" class="ltx_Math" alttext="84\%" display="inline"><semantics id="S1.p3.4.m4.1a"><mrow id="S1.p3.4.m4.1.1" xref="S1.p3.4.m4.1.1.cmml"><mn id="S1.p3.4.m4.1.1.2" xref="S1.p3.4.m4.1.1.2.cmml">84</mn><mo id="S1.p3.4.m4.1.1.1" xref="S1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.4.m4.1b"><apply id="S1.p3.4.m4.1.1.cmml" xref="S1.p3.4.m4.1.1"><csymbol cd="latexml" id="S1.p3.4.m4.1.1.1.cmml" xref="S1.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S1.p3.4.m4.1.1.2.cmml" xref="S1.p3.4.m4.1.1.2">84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.4.m4.1c">84\%</annotation></semantics></math> for the first specialist to about <math id="S1.p3.5.m5.1" class="ltx_Math" alttext="89\%" display="inline"><semantics id="S1.p3.5.m5.1a"><mrow id="S1.p3.5.m5.1.1" xref="S1.p3.5.m5.1.1.cmml"><mn id="S1.p3.5.m5.1.1.2" xref="S1.p3.5.m5.1.1.2.cmml">89</mn><mo id="S1.p3.5.m5.1.1.1" xref="S1.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.5.m5.1b"><apply id="S1.p3.5.m5.1.1.cmml" xref="S1.p3.5.m5.1.1"><csymbol cd="latexml" id="S1.p3.5.m5.1.1.1.cmml" xref="S1.p3.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S1.p3.5.m5.1.1.2.cmml" xref="S1.p3.5.m5.1.1.2">89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.5.m5.1c">89\%</annotation></semantics></math> for the second. The accuracies between synthetic and real images did vary slightly for both specialists. Although results are very promising, the authors have patented their methodology, and hence an alternate method is required.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">Age-related macular degeneration is a major cause of vision impairment and has affected approximately <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S1.p4.1.m1.1a"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><cn type="integer" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">200</annotation></semantics></math> million people worldwide in 2020¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. With an aging population, such numbers are expected to rise to <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="288" display="inline"><semantics id="S1.p4.2.m2.1a"><mn id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">288</mn><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><cn type="integer" id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">288</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">288</annotation></semantics></math> million by 2040¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">46</span></a>]</cite>. AMD is a progressive disorder of the macular region that causes central vision loss and is one of the most common causes of irreversible vision impairment in people over <math id="S1.p4.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S1.p4.3.m3.1a"><mn id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><cn type="integer" id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">50</annotation></semantics></math> years-old¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Figure¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts an example of a retina fundus affected by AMD.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This work has introduced an alternative approach for generating synthetic images for training deep networks and tested it for AMD identification. Retina images, positive and negative to AMD, from multiple databases were used to feed a GAN to provide a range of image qualities and lesions. Ten different GAN architectures were compared to generate synthetic eye-fundus images and trhe quality was assessed using the Fr√©chet Inception Distance (FID), two independent clinical experts who were label blinded and deep-learning classification.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The primary contributions of this work are fourfold:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To provide an open-source tool that can generate high-quality medical images of both, disease and healthy conditions;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Generation and validation of eye-fundus images suitable for training of deep-learning networks for computerized detection of AMD.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">To introduce StyleGAN2-ADA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for eye fundus image generation; and</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">To present a comparison among several GAN architectures for synthetic image generation.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of this paper is organized as follows. Sections¬†<a href="#S2" title="2 Theoretical Background ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and¬†<a href="#S3" title="3 Methodology ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> present the theoretical background and methodology, respectively. Section¬†<a href="#S4" title="4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the experimental results and Section¬†<a href="#S6" title="6 Conclusion and Future Works ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> states conclusions and future works.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Theoretical Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generative Adversarial Networks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">GANs are deep-learning based data generative networks. A significant improvement in more recent GANs is obtaining training stability to prevent the well-known ‚Äúmode collapse", i.e., the generator is trapped in a local minimum, thus producing images that are very similar to each other. Gulrajani et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed the Wasserstein GAN with gradient penalty (WGAN-GP) to mitigate this problem, which implements a distinct loss function and eases the training convergence stability. Meanwhile, Karras et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed the Progressive GAN, showing that performance gains and learning capacity can be obtained by increasing the image resolution gradually during training.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Style-based Generative Adversarial Networks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The Style-based Generative Adversarial Network (StyleGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, depicted in Figure¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2.2 Style-based Generative Adversarial Networks ‚Ä£ 2 Theoretical Background ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, is a state-of-the-art generative model that aims to create high-resolution images with greater fidelity. Additionally, such an architecture aims to enhance the diversity of outputs and simultaneously control image features. This gives more reliable control over the latent space which may allow users to edit image features, such as skin color, age, gender, and facial expression.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">StyleGAN2 with Adaptive Discriminator Augmentation (StyleGAN2-ADA) has shown promising results in synthesizing high-quality images¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. This variant uses an adaptive discriminator augmentation technique that stabilizes training in data-constrained environments. The method does not require modifications in the loss functions or architectures and can be used to train the network from scratch and fine-tuning.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2203.13856/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="178" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Standard StyleGAN architecture. In this figure, FC stands for fully connected layer, ‚ÄòA‚Äô denotes an affine transformation that creates the style, and ‚ÄòB‚Äô corresponds to noise broadcast operation. Adapted from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">While traditional generators are designed as single neural networks, StyleGAN introduces a dual-network-based architecture, which is described below:</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.2" class="ltx_p"><span id="S2.I1.i1.p1.2.1" class="ltx_text ltx_font_bold">Mapping Network</span>: it maps a latent input to an intermediate representation using eight fully connected layers, mitigating the entanglement issue (e.g., an <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><mi id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">n</annotation></semantics></math>-dimensional latent space is converted into an <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><mi id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><ci id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">n</annotation></semantics></math>-dimensional intermediate space).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.3" class="ltx_p"><span id="S2.I1.i2.p1.3.1" class="ltx_text ltx_font_bold">Synthesis Network</span>: the outcome of the previous layer serves as an input to another network that uses different blocks, one for each image resolution. Each block comprises an upsampling operation (resolutions are increased progressively) followed by a convolutional layer with a <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mn id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.1.m1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">3\times 3</annotation></semantics></math> kernel (CONV <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mn id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.2.m2.1.1.1" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><times id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">3\times 3</annotation></semantics></math>). Further, Gaussian noise (B) is added to the output to feed an Adaptive Instance Normalization (AdaIn) layer, which also requires style input (high-level features) obtained by the affine transformation (A). The noise is vital to produce fine variations such as in hair of a facial image. The aforementioned process, i.e., convolution followed by an AdaIn layer, is executed once more, and the output is used to feed the next block. The only exception concerns the first block, which replaces the upsampling operation with a constant input (CONST <math id="S2.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="4\times 4\times 512" display="inline"><semantics id="S2.I1.i2.p1.3.m3.1a"><mrow id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mn id="S2.I1.i2.p1.3.m3.1.1.2" xref="S2.I1.i2.p1.3.m3.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.3.m3.1.1.1" xref="S2.I1.i2.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.3.m3.1.1.1a" xref="S2.I1.i2.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S2.I1.i2.p1.3.m3.1.1.4" xref="S2.I1.i2.p1.3.m3.1.1.4.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><times id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.1"></times><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2">4</cn><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3">4</cn><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.4.cmml" xref="S2.I1.i2.p1.3.m3.1.1.4">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">4\times 4\times 512</annotation></semantics></math>), and it has one convolutional layer.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Style-based Generative Adversarial Networks 2</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">One recurring problem of StyleGAN lies in creating blob-like pixels (resembles water droplets), which are more visible when examining the intermediate feature maps of the generator. Such a problem initially appears in all feature maps at a low resolution such as of <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mn id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">64\times 64</annotation></semantics></math> and is deepened when employing higher resolutions. Karras et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> traced the problem and found out that AdaIn loses some crucial information when performing the instance normalization. They proposed the StyleGAN2, an alternative architecture that stabilizes the generation of high-resolution images. This has a simplified first processing step achieved by normalizing the features without their mean, and removing the noise addition. Figure¬†<a href="#S2.F3" title="Figure 3 ‚Ä£ 2.3 Style-based Generative Adversarial Networks 2 ‚Ä£ 2 Theoretical Background ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates a comparison between StyleGAN and StyleGAN2 architectures.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<table id="S2.F3.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.F3.2.2.2" class="ltx_tr">
<td id="S2.F3.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x3.png" id="S2.F3.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="98" height="141" alt="Refer to caption"></td>
<td id="S2.F3.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x4.png" id="S2.F3.2.2.2.2.g1" class="ltx_graphics ltx_img_portrait" width="98" height="138" alt="Refer to caption"></td>
</tr>
<tr id="S2.F3.2.2.3" class="ltx_tr">
<td id="S2.F3.2.2.3.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S2.F3.2.2.3.2" class="ltx_td ltx_align_center">(b)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> A closer look at (a) StyleGAN and (b) StyleGAN2 architectures. The AdaIN layer has been expanded for both models for the sake of better visualization. For StyleGAN, AdaIn layer performs normalization and modulation operations using both mean and standard deviation per feature map; bias ‚Äòb‚Äô that comes from the previous convolutional layer is added to the Gaussian noise and used as an input to the AdaIn layer. The mean operation is not employed in the StyleGAN2 block, and the bias is added to the noise after the standard deviation normalization. Adapted from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section describes the datasets that were used in the study, the techniques employed to generate the synthetic images, and the methodology to evaluate the different neural architectures considered in the experimental section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This study used eye-fundus images from three public datasets reported in the literature to capture the typical differences due to equipment and demographics. Below, we summarized the datasets‚Äô preliminary information:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">iChallenge-AMD:</span> Comprises of <math id="S3.I1.i1.p1.1.m1.2" class="ltx_Math" alttext="1,200" display="inline"><semantics id="S3.I1.i1.p1.1.m1.2a"><mrow id="S3.I1.i1.p1.1.m1.2.3.2" xref="S3.I1.i1.p1.1.m1.2.3.1.cmml"><mn id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">1</mn><mo id="S3.I1.i1.p1.1.m1.2.3.2.1" xref="S3.I1.i1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.I1.i1.p1.1.m1.2.2" xref="S3.I1.i1.p1.1.m1.2.2.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.2b"><list id="S3.I1.i1.p1.1.m1.2.3.1.cmml" xref="S3.I1.i1.p1.1.m1.2.3.2"><cn type="integer" id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">1</cn><cn type="integer" id="S3.I1.i1.p1.1.m1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.2.2">200</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.2c">1,200</annotation></semantics></math> retinal fundus images that have been annotated for drusen and hemorrhage. The training set was made of 400 images (89 images of eyes with AMD and 311 from eyes without AMD), while the test sets contained the remaining images¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.4" class="ltx_p"><span id="S3.I1.i2.p1.4.1" class="ltx_text ltx_font_bold">ODIR-2019:</span> Contains colored fundus images from both left and right eyes of <math id="S3.I1.i2.p1.1.m1.2" class="ltx_Math" alttext="5,000" display="inline"><semantics id="S3.I1.i2.p1.1.m1.2a"><mrow id="S3.I1.i2.p1.1.m1.2.3.2" xref="S3.I1.i2.p1.1.m1.2.3.1.cmml"><mn id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">5</mn><mo id="S3.I1.i2.p1.1.m1.2.3.2.1" xref="S3.I1.i2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.I1.i2.p1.1.m1.2.2" xref="S3.I1.i2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.2b"><list id="S3.I1.i2.p1.1.m1.2.3.1.cmml" xref="S3.I1.i2.p1.1.m1.2.3.2"><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">5</cn><cn type="integer" id="S3.I1.i2.p1.1.m1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.2c">5,000</annotation></semantics></math> patients obtained from multiple hospitals/medical centers in China, with varying image resolutions and observations from several specialists. The dataset has was designed to address normal and six diseases: diabetes, glaucoma, cataract, AMD, hypertension, myopia, and other diseases/abnormalities<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://odir2019.grand-challenge.org/dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://odir2019.grand-challenge.org/dataset/</a></span></span></span>. The training set is made up of a structured dataset with <math id="S3.I1.i2.p1.2.m2.2" class="ltx_Math" alttext="7,000" display="inline"><semantics id="S3.I1.i2.p1.2.m2.2a"><mrow id="S3.I1.i2.p1.2.m2.2.3.2" xref="S3.I1.i2.p1.2.m2.2.3.1.cmml"><mn id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">7</mn><mo id="S3.I1.i2.p1.2.m2.2.3.2.1" xref="S3.I1.i2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.I1.i2.p1.2.m2.2.2" xref="S3.I1.i2.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.2b"><list id="S3.I1.i2.p1.2.m2.2.3.1.cmml" xref="S3.I1.i2.p1.2.m2.2.3.2"><cn type="integer" id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">7</cn><cn type="integer" id="S3.I1.i2.p1.2.m2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.2c">7,000</annotation></semantics></math> images, of which <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="280" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><mn id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">280</mn><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><cn type="integer" id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">280</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">280</annotation></semantics></math> images are labelled as having AMD. The testing set consists of <math id="S3.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S3.I1.i2.p1.4.m4.1a"><mn id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><cn type="integer" id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">500</annotation></semantics></math> colored fundus images, eliminating age and gender.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.7" class="ltx_p"><span id="S3.I1.i3.p1.7.1" class="ltx_text ltx_font_bold">RIADD:</span> contains <math id="S3.I1.i3.p1.1.m1.2" class="ltx_Math" alttext="3,200" display="inline"><semantics id="S3.I1.i3.p1.1.m1.2a"><mrow id="S3.I1.i3.p1.1.m1.2.3.2" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml"><mn id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">3</mn><mo id="S3.I1.i3.p1.1.m1.2.3.2.1" xref="S3.I1.i3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.I1.i3.p1.1.m1.2.2" xref="S3.I1.i3.p1.1.m1.2.2.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><list id="S3.I1.i3.p1.1.m1.2.3.1.cmml" xref="S3.I1.i3.p1.1.m1.2.3.2"><cn type="integer" id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">3</cn><cn type="integer" id="S3.I1.i3.p1.1.m1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2">200</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">3,200</annotation></semantics></math> fundus images recorded using 3 different cameras and multiple conditions. The images have been annotated through the consensus of two retina experts. The dataset has been sub-divided based on six diseases/abnormalities; diabetic retinopathy, AMD, media haze, drusen, myopia, and branch retinal vein occlusion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">38</span></a>]</cite>. The dataset was subdivided into three subsets: <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><mrow id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mn id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">60</mn><mo id="S3.I1.i3.p1.2.m2.1.1.1" xref="S3.I1.i3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">60\%</annotation></semantics></math> for training (<math id="S3.I1.i3.p1.3.m3.2" class="ltx_Math" alttext="1,920" display="inline"><semantics id="S3.I1.i3.p1.3.m3.2a"><mrow id="S3.I1.i3.p1.3.m3.2.3.2" xref="S3.I1.i3.p1.3.m3.2.3.1.cmml"><mn id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml">1</mn><mo id="S3.I1.i3.p1.3.m3.2.3.2.1" xref="S3.I1.i3.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.I1.i3.p1.3.m3.2.2" xref="S3.I1.i3.p1.3.m3.2.2.cmml">920</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.2b"><list id="S3.I1.i3.p1.3.m3.2.3.1.cmml" xref="S3.I1.i3.p1.3.m3.2.3.2"><cn type="integer" id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">1</cn><cn type="integer" id="S3.I1.i3.p1.3.m3.2.2.cmml" xref="S3.I1.i3.p1.3.m3.2.2">920</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.2c">1,920</annotation></semantics></math> images), <math id="S3.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.I1.i3.p1.4.m4.1a"><mrow id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml"><mn id="S3.I1.i3.p1.4.m4.1.1.2" xref="S3.I1.i3.p1.4.m4.1.1.2.cmml">20</mn><mo id="S3.I1.i3.p1.4.m4.1.1.1" xref="S3.I1.i3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><apply id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.4.m4.1.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i3.p1.4.m4.1.1.2.cmml" xref="S3.I1.i3.p1.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">20\%</annotation></semantics></math> for validation (<math id="S3.I1.i3.p1.5.m5.1" class="ltx_Math" alttext="640" display="inline"><semantics id="S3.I1.i3.p1.5.m5.1a"><mn id="S3.I1.i3.p1.5.m5.1.1" xref="S3.I1.i3.p1.5.m5.1.1.cmml">640</mn><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.5.m5.1b"><cn type="integer" id="S3.I1.i3.p1.5.m5.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1">640</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.5.m5.1c">640</annotation></semantics></math> images), and the remaining <math id="S3.I1.i3.p1.6.m6.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.I1.i3.p1.6.m6.1a"><mrow id="S3.I1.i3.p1.6.m6.1.1" xref="S3.I1.i3.p1.6.m6.1.1.cmml"><mn id="S3.I1.i3.p1.6.m6.1.1.2" xref="S3.I1.i3.p1.6.m6.1.1.2.cmml">20</mn><mo id="S3.I1.i3.p1.6.m6.1.1.1" xref="S3.I1.i3.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.6.m6.1b"><apply id="S3.I1.i3.p1.6.m6.1.1.cmml" xref="S3.I1.i3.p1.6.m6.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.6.m6.1.1.1.cmml" xref="S3.I1.i3.p1.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S3.I1.i3.p1.6.m6.1.1.2.cmml" xref="S3.I1.i3.p1.6.m6.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.6.m6.1c">20\%</annotation></semantics></math> for testing purposes (<math id="S3.I1.i3.p1.7.m7.1" class="ltx_Math" alttext="640" display="inline"><semantics id="S3.I1.i3.p1.7.m7.1a"><mn id="S3.I1.i3.p1.7.m7.1.1" xref="S3.I1.i3.p1.7.m7.1.1.cmml">640</mn><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.7.m7.1b"><cn type="integer" id="S3.I1.i3.p1.7.m7.1.1.cmml" xref="S3.I1.i3.p1.7.m7.1.1">640</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.7.m7.1c">640</annotation></semantics></math> images).</p>
</div>
<div id="S3.I1.i3.p2" class="ltx_para">
<p id="S3.I1.i3.p2.1" class="ltx_p">We have used publicly available and online datasets, each of which have described their patient recruitment, clinical outcomes and human experiments ethics approval in their associated publications. Registration was necessary for access to the data from the three sources.</p>
</div>
<div id="S3.I1.i3.p3" class="ltx_para">
<p id="S3.I1.i3.p3.1" class="ltx_p">i-Challenge datset authors confirm in their publications they received ethics approval guidelines from Sun-Yat Sen University, China and Sixth People‚Äôs Hospital Affiliated to Shanghai Jiao Tong University, Shanghai, China. Details on https://amd.grand-challenge.org/Home/</p>
</div>
<div id="S3.I1.i3.p4" class="ltx_para">
<p id="S3.I1.i3.p4.1" class="ltx_p">ODIR dataset confirm that all data is from routine clinical examinations, have been de-identified and published after receiving clearance from Peiking University board. Details on https://odir2019.grand-challenge.org/dataset/</p>
</div>
<div id="S3.I1.i3.p5" class="ltx_para">
<p id="S3.I1.i3.p5.1" class="ltx_p">RIADD dataset confirm that data collection was conducted after ethics approval from Review Board of Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, India. Details on https://riadd.grand-challenge.org/</p>
</div>
</li>
</ul>
<p id="S3.SS1.p2.1" class="ltx_p">The datasets mentioned above were designed to address a challenge and hence the labels of the test subset were not available. Therefore, we used only the training subset.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">These datasets have an imbalanced class distribution, which is an inherent problem for most medical image datasets and using such a set generally leads to a bias towards the predominant class. Different research groups have developed these datasets with differences in the quality of the images and the demographics. Thus, these datasets offered both, data imbalance and wide range of image quality.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.13" class="ltx_p">The ODIR-2019 and RIADD datasets were organized into two subsets, AMD and non-AMD images. Preprocessing methodology was same as proposed by Fu et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which comprises of a preprocessing step that detects the retinal mask using the Hough Circle Transform and then crops it to remove the impact of the black background. The cropped region was then scaled to a <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mn id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">224\times 224</annotation></semantics></math> resolution. The resultant images were submitted to a three-level manual quality grading: ‚ÄúGood‚Äô, ‚ÄúUsable‚Äô and ‚ÄúReject‚Äô. Only those images graded with ‚ÄúGood" and ‚ÄúUsable" scores were used for further analysis. As a result, the number of images positive to AMD decreased from <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="89" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">89</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><cn type="integer" id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">89</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">89</annotation></semantics></math> to <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="74" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">74</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">74</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">74</annotation></semantics></math>, <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="280" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mn id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">280</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><cn type="integer" id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">280</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">280</annotation></semantics></math> to <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="227" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mn id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">227</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><cn type="integer" id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">227</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">227</annotation></semantics></math>, and <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mn id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><cn type="integer" id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">100</annotation></semantics></math> to <math id="S3.SS1.p4.7.m7.1" class="ltx_Math" alttext="79" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><mn id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml">79</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><cn type="integer" id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">79</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">79</annotation></semantics></math> images in the iChallenge-AMD, ODIR-2019, and RIADD datasets, respectively while the number of non-AMD images decreased from <math id="S3.SS1.p4.8.m8.1" class="ltx_Math" alttext="311" display="inline"><semantics id="S3.SS1.p4.8.m8.1a"><mn id="S3.SS1.p4.8.m8.1.1" xref="S3.SS1.p4.8.m8.1.1.cmml">311</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.8.m8.1b"><cn type="integer" id="S3.SS1.p4.8.m8.1.1.cmml" xref="S3.SS1.p4.8.m8.1.1">311</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.8.m8.1c">311</annotation></semantics></math> to <math id="S3.SS1.p4.9.m9.1" class="ltx_Math" alttext="290" display="inline"><semantics id="S3.SS1.p4.9.m9.1a"><mn id="S3.SS1.p4.9.m9.1.1" xref="S3.SS1.p4.9.m9.1.1.cmml">290</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.9.m9.1b"><cn type="integer" id="S3.SS1.p4.9.m9.1.1.cmml" xref="S3.SS1.p4.9.m9.1.1">290</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.9.m9.1c">290</annotation></semantics></math>, <math id="S3.SS1.p4.10.m10.2" class="ltx_Math" alttext="6,720" display="inline"><semantics id="S3.SS1.p4.10.m10.2a"><mrow id="S3.SS1.p4.10.m10.2.3.2" xref="S3.SS1.p4.10.m10.2.3.1.cmml"><mn id="S3.SS1.p4.10.m10.1.1" xref="S3.SS1.p4.10.m10.1.1.cmml">6</mn><mo id="S3.SS1.p4.10.m10.2.3.2.1" xref="S3.SS1.p4.10.m10.2.3.1.cmml">,</mo><mn id="S3.SS1.p4.10.m10.2.2" xref="S3.SS1.p4.10.m10.2.2.cmml">720</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.10.m10.2b"><list id="S3.SS1.p4.10.m10.2.3.1.cmml" xref="S3.SS1.p4.10.m10.2.3.2"><cn type="integer" id="S3.SS1.p4.10.m10.1.1.cmml" xref="S3.SS1.p4.10.m10.1.1">6</cn><cn type="integer" id="S3.SS1.p4.10.m10.2.2.cmml" xref="S3.SS1.p4.10.m10.2.2">720</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.10.m10.2c">6,720</annotation></semantics></math> to <math id="S3.SS1.p4.11.m11.2" class="ltx_Math" alttext="4,993" display="inline"><semantics id="S3.SS1.p4.11.m11.2a"><mrow id="S3.SS1.p4.11.m11.2.3.2" xref="S3.SS1.p4.11.m11.2.3.1.cmml"><mn id="S3.SS1.p4.11.m11.1.1" xref="S3.SS1.p4.11.m11.1.1.cmml">4</mn><mo id="S3.SS1.p4.11.m11.2.3.2.1" xref="S3.SS1.p4.11.m11.2.3.1.cmml">,</mo><mn id="S3.SS1.p4.11.m11.2.2" xref="S3.SS1.p4.11.m11.2.2.cmml">993</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.11.m11.2b"><list id="S3.SS1.p4.11.m11.2.3.1.cmml" xref="S3.SS1.p4.11.m11.2.3.2"><cn type="integer" id="S3.SS1.p4.11.m11.1.1.cmml" xref="S3.SS1.p4.11.m11.1.1">4</cn><cn type="integer" id="S3.SS1.p4.11.m11.2.2.cmml" xref="S3.SS1.p4.11.m11.2.2">993</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.11.m11.2c">4,993</annotation></semantics></math>, and <math id="S3.SS1.p4.12.m12.2" class="ltx_Math" alttext="1,820" display="inline"><semantics id="S3.SS1.p4.12.m12.2a"><mrow id="S3.SS1.p4.12.m12.2.3.2" xref="S3.SS1.p4.12.m12.2.3.1.cmml"><mn id="S3.SS1.p4.12.m12.1.1" xref="S3.SS1.p4.12.m12.1.1.cmml">1</mn><mo id="S3.SS1.p4.12.m12.2.3.2.1" xref="S3.SS1.p4.12.m12.2.3.1.cmml">,</mo><mn id="S3.SS1.p4.12.m12.2.2" xref="S3.SS1.p4.12.m12.2.2.cmml">820</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.12.m12.2b"><list id="S3.SS1.p4.12.m12.2.3.1.cmml" xref="S3.SS1.p4.12.m12.2.3.2"><cn type="integer" id="S3.SS1.p4.12.m12.1.1.cmml" xref="S3.SS1.p4.12.m12.1.1">1</cn><cn type="integer" id="S3.SS1.p4.12.m12.2.2.cmml" xref="S3.SS1.p4.12.m12.2.2">820</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.12.m12.2c">1,820</annotation></semantics></math> to <math id="S3.SS1.p4.13.m13.2" class="ltx_Math" alttext="1,143" display="inline"><semantics id="S3.SS1.p4.13.m13.2a"><mrow id="S3.SS1.p4.13.m13.2.3.2" xref="S3.SS1.p4.13.m13.2.3.1.cmml"><mn id="S3.SS1.p4.13.m13.1.1" xref="S3.SS1.p4.13.m13.1.1.cmml">1</mn><mo id="S3.SS1.p4.13.m13.2.3.2.1" xref="S3.SS1.p4.13.m13.2.3.1.cmml">,</mo><mn id="S3.SS1.p4.13.m13.2.2" xref="S3.SS1.p4.13.m13.2.2.cmml">143</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.13.m13.2b"><list id="S3.SS1.p4.13.m13.2.3.1.cmml" xref="S3.SS1.p4.13.m13.2.3.2"><cn type="integer" id="S3.SS1.p4.13.m13.1.1.cmml" xref="S3.SS1.p4.13.m13.1.1">1</cn><cn type="integer" id="S3.SS1.p4.13.m13.2.2.cmml" xref="S3.SS1.p4.13.m13.2.2">143</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.13.m13.2c">1,143</annotation></semantics></math> images concerning the iChallenge-AMD, ODIR-2019, and RIADD datasets, respectively. Figure¬†<a href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Dataset ‚Ä£ 3 Methodology ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the steps mentioned above for a sample image from the ODIR-2019 dataset.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<table id="S3.F4.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.F4.3.3" class="ltx_tr">
<td id="S3.F4.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x5.png" id="S3.F4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></td>
<td id="S3.F4.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x6.png" id="S3.F4.2.2.2.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></td>
<td id="S3.F4.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x7.png" id="S3.F4.3.3.3.g1" class="ltx_graphics ltx_img_square" width="70" height="70" alt="Refer to caption"></td>
</tr>
<tr id="S3.F4.3.4" class="ltx_tr">
<td id="S3.F4.3.4.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S3.F4.3.4.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S3.F4.3.4.3" class="ltx_td ltx_align_center">(c)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Sample image extracted from ODIR-2019 dataset and its corresponding transformations: (a) original image, (b) background removal using Hough Circle Transform and resizing, and (c) central cropping.</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.4" class="ltx_p">These images were then resampled to <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="390\times 390" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mrow id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mn id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">390</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.1.m1.1.1.1" xref="S3.SS1.p5.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">390</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><times id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">390</cn><cn type="integer" id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">390</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">390\times 390</annotation></semantics></math> pixels, followed by a cropping procedure keeping the center of the image to <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml"><mn id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.2.m2.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p5.2.m2.1.1.3" xref="S3.SS1.p5.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1"><times id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p5.2.m2.1.1.2.cmml" xref="S3.SS1.p5.2.m2.1.1.2">256</cn><cn type="integer" id="S3.SS1.p5.2.m2.1.1.3.cmml" xref="S3.SS1.p5.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">256\times 256</annotation></semantics></math> pixels. Such a procedure is required to drive StyleGAN2-ADA generating images focused on the macula area (Figure¬†<a href="#S3.F4" title="Figure 4 ‚Ä£ 3.1 Dataset ‚Ä£ 3 Methodology ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-c). Ultimately, images were resized <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mrow id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mn id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.3.m3.1.1.1" xref="S3.SS1.p5.3.m3.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><times id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">224</cn><cn type="integer" id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">224\times 224</annotation></semantics></math> pixels and normalized within the range <math id="S3.SS1.p5.4.m4.2" class="ltx_Math" alttext="[-1,1]" display="inline"><semantics id="S3.SS1.p5.4.m4.2a"><mrow id="S3.SS1.p5.4.m4.2.2.1" xref="S3.SS1.p5.4.m4.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.p5.4.m4.2.2.1.2" xref="S3.SS1.p5.4.m4.2.2.2.cmml">[</mo><mrow id="S3.SS1.p5.4.m4.2.2.1.1" xref="S3.SS1.p5.4.m4.2.2.1.1.cmml"><mo id="S3.SS1.p5.4.m4.2.2.1.1a" xref="S3.SS1.p5.4.m4.2.2.1.1.cmml">‚àí</mo><mn id="S3.SS1.p5.4.m4.2.2.1.1.2" xref="S3.SS1.p5.4.m4.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p5.4.m4.2.2.1.3" xref="S3.SS1.p5.4.m4.2.2.2.cmml">,</mo><mn id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">1</mn><mo stretchy="false" id="S3.SS1.p5.4.m4.2.2.1.4" xref="S3.SS1.p5.4.m4.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.2b"><interval closure="closed" id="S3.SS1.p5.4.m4.2.2.2.cmml" xref="S3.SS1.p5.4.m4.2.2.1"><apply id="S3.SS1.p5.4.m4.2.2.1.1.cmml" xref="S3.SS1.p5.4.m4.2.2.1.1"><minus id="S3.SS1.p5.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.p5.4.m4.2.2.1.1"></minus><cn type="integer" id="S3.SS1.p5.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.p5.4.m4.2.2.1.1.2">1</cn></apply><cn type="integer" id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.2c">[-1,1]</annotation></semantics></math> to be used as proper inputs to the deeper architectures considered in the manuscript.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.6" class="ltx_p">After quality assessment and selecting the images for the final dataset using the criterion described earlier, the resulting single dataset comprised totaL of <math id="S3.SS1.p6.1.m1.2" class="ltx_Math" alttext="7,106" display="inline"><semantics id="S3.SS1.p6.1.m1.2a"><mrow id="S3.SS1.p6.1.m1.2.3.2" xref="S3.SS1.p6.1.m1.2.3.1.cmml"><mn id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">7</mn><mo id="S3.SS1.p6.1.m1.2.3.2.1" xref="S3.SS1.p6.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p6.1.m1.2.2" xref="S3.SS1.p6.1.m1.2.2.cmml">106</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.2b"><list id="S3.SS1.p6.1.m1.2.3.1.cmml" xref="S3.SS1.p6.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">7</cn><cn type="integer" id="S3.SS1.p6.1.m1.2.2.cmml" xref="S3.SS1.p6.1.m1.2.2">106</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.2c">7,106</annotation></semantics></math> images. In this, <math id="S3.SS1.p6.2.m2.2" class="ltx_Math" alttext="6,896" display="inline"><semantics id="S3.SS1.p6.2.m2.2a"><mrow id="S3.SS1.p6.2.m2.2.3.2" xref="S3.SS1.p6.2.m2.2.3.1.cmml"><mn id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">6</mn><mo id="S3.SS1.p6.2.m2.2.3.2.1" xref="S3.SS1.p6.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.p6.2.m2.2.2" xref="S3.SS1.p6.2.m2.2.2.cmml">896</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.2b"><list id="S3.SS1.p6.2.m2.2.3.1.cmml" xref="S3.SS1.p6.2.m2.2.3.2"><cn type="integer" id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">6</cn><cn type="integer" id="S3.SS1.p6.2.m2.2.2.cmml" xref="S3.SS1.p6.2.m2.2.2">896</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.2c">6,896</annotation></semantics></math> images were used to train the models (<math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="275" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mn id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">275</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><cn type="integer" id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">275</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">275</annotation></semantics></math> with AMD and <math id="S3.SS1.p6.4.m4.2" class="ltx_Math" alttext="6,621" display="inline"><semantics id="S3.SS1.p6.4.m4.2a"><mrow id="S3.SS1.p6.4.m4.2.3.2" xref="S3.SS1.p6.4.m4.2.3.1.cmml"><mn id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml">6</mn><mo id="S3.SS1.p6.4.m4.2.3.2.1" xref="S3.SS1.p6.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS1.p6.4.m4.2.2" xref="S3.SS1.p6.4.m4.2.2.cmml">621</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.2b"><list id="S3.SS1.p6.4.m4.2.3.1.cmml" xref="S3.SS1.p6.4.m4.2.3.2"><cn type="integer" id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">6</cn><cn type="integer" id="S3.SS1.p6.4.m4.2.2.cmml" xref="S3.SS1.p6.4.m4.2.2">621</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.2c">6,621</annotation></semantics></math> without AMD) and the remaining <math id="S3.SS1.p6.5.m5.1" class="ltx_Math" alttext="210" display="inline"><semantics id="S3.SS1.p6.5.m5.1a"><mn id="S3.SS1.p6.5.m5.1.1" xref="S3.SS1.p6.5.m5.1.1.cmml">210</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.5.m5.1b"><cn type="integer" id="S3.SS1.p6.5.m5.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1">210</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.5.m5.1c">210</annotation></semantics></math> images (<math id="S3.SS1.p6.6.m6.1" class="ltx_Math" alttext="105" display="inline"><semantics id="S3.SS1.p6.6.m6.1a"><mn id="S3.SS1.p6.6.m6.1.1" xref="S3.SS1.p6.6.m6.1.1.cmml">105</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.6.m6.1b"><cn type="integer" id="S3.SS1.p6.6.m6.1.1.cmml" xref="S3.SS1.p6.6.m6.1.1">105</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.6.m6.1c">105</annotation></semantics></math> with AMD) were used as a holdout test set. Figure¬†<a href="#S3.F5" title="Figure 5 ‚Ä£ 3.1 Dataset ‚Ä£ 3 Methodology ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays the number of images used per dataset to compose the final test set.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<table id="S3.F5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.F5.2.2" class="ltx_tr">
<td id="S3.F5.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x8.png" id="S3.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="96" height="71" alt="Refer to caption"></td>
<td id="S3.F5.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x9.png" id="S3.F5.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="98" height="71" alt="Refer to caption"></td>
</tr>
<tr id="S3.F5.2.3" class="ltx_tr">
<td id="S3.F5.2.3.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S3.F5.2.3.2" class="ltx_td ltx_align_center">(b)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> Number of images per dataset to compose the test set: (a) images positive to AMD and (b) non-AMD images.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Measures</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We employed three evaluation measures, i.e., the Fr√©chet Inception Distance (FID), a well-known GAN evaluation score¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the experts ability to identify the syntetic images and the classification accuracy. FID is often used to assess the quality and variety of the generated images and, even though it has been proposed to improve the standard Inception Score, it still uses the Inception-v3 architecture to extract features from both, synthetic and real images.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experimental Setup</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The experiments were divided into four rounds: (i) a FID comparison among StyleGAN2-ADA and nine different GAN architectures to assess the quality of the images generated, (ii) evaluation of the human experts to distinguish real and synthetic images, (iii) data perturbation with synthetic images generated by StyleGAN2-ADA and evaluated in three different deep networks and (iv) a comparison of the best-augmented model obtained in the previous step with human experts when identifying images that have AMD verses those that do not.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.6" class="ltx_p">The first experiment employed FID to compare StyleGAN2-ADA and nine distinct GAN models. The following models were considered in the experiment: Deep Convolutional GAN (DCGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">41</span></a>]</cite>, Least Squares Generative Adversarial Networks (LSGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">35</span></a>]</cite>, Wasserstein GAN (WGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, Wasserstein GAN with Gradient Penalty (WGAN-GP)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Deep Regret Analytic Generative Adversarial Networks (DRAGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">33</span></a>]</cite>, Energy-based Generative Adversarial Network (EBGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">47</span></a>]</cite>, Boundary Equilibrium Generative Adversarial Networks (BEGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Conditional GAN (CGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">36</span></a>]</cite>, and Auxiliary Classifier GAN (ACGAN)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">37</span></a>]</cite>. All models were trained with <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><cn type="integer" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">50</annotation></semantics></math> epochs, considering samples of size <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="100\times 100" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mn id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">100</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><times id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">100</cn><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">100\times 100</annotation></semantics></math> pixels and a batch size of <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mn id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><cn type="integer" id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">32</annotation></semantics></math>. The training step used the ADAM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> optimizer with a learning rate of <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="0.0002" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">0.0002</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><cn type="float" id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">0.0002</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">0.0002</annotation></semantics></math> and decay rates of <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><cn type="float" id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">0.5</annotation></semantics></math> and <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="0.999" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mn id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">0.999</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><cn type="float" id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">0.999</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">0.999</annotation></semantics></math> regarding the generator and the discriminator, respectively. The experiments were conducted using the training set with an Nvidia RTX 2060 GPU. Therefore, after training each GAN model, new images were generated and FID was computed.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The second experiment determines whether clinical experts, who are very experienced with the analysis of eye fundus images, can distinguish between synthetic and real images. Such a step is essential to evaluate the effectiveness of StyleGAN2-ADA for generating synthetic eye fundus images. The experts were provided with randomly generated image sets, one for AMD-diagnosed images and the other for non-AMD images, each consisting of ten synthetic images and ten real images. They were asked to identify the syntetic images in the mix.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.9" class="ltx_p">In the next experiment, we considered three deep architectures pre-trained with ImageNet dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for performance comparison when the model is perturbed with synthetic images, i.e., SqueezeNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, AlexNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">34</span></a>]</cite>, and ResNet18¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. During training, synthetic and real images were mixed within each batch according to a pre-determined hyperparameter <math id="S3.SS3.p4.1.m1.2" class="ltx_Math" alttext="p\in[0,1]" display="inline"><semantics id="S3.SS3.p4.1.m1.2a"><mrow id="S3.SS3.p4.1.m1.2.3" xref="S3.SS3.p4.1.m1.2.3.cmml"><mi id="S3.SS3.p4.1.m1.2.3.2" xref="S3.SS3.p4.1.m1.2.3.2.cmml">p</mi><mo id="S3.SS3.p4.1.m1.2.3.1" xref="S3.SS3.p4.1.m1.2.3.1.cmml">‚àà</mo><mrow id="S3.SS3.p4.1.m1.2.3.3.2" xref="S3.SS3.p4.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.2.3.3.2.1" xref="S3.SS3.p4.1.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">0</mn><mo id="S3.SS3.p4.1.m1.2.3.3.2.2" xref="S3.SS3.p4.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS3.p4.1.m1.2.3.3.2.3" xref="S3.SS3.p4.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.2b"><apply id="S3.SS3.p4.1.m1.2.3.cmml" xref="S3.SS3.p4.1.m1.2.3"><in id="S3.SS3.p4.1.m1.2.3.1.cmml" xref="S3.SS3.p4.1.m1.2.3.1"></in><ci id="S3.SS3.p4.1.m1.2.3.2.cmml" xref="S3.SS3.p4.1.m1.2.3.2">ùëù</ci><interval closure="closed" id="S3.SS3.p4.1.m1.2.3.3.1.cmml" xref="S3.SS3.p4.1.m1.2.3.3.2"><cn type="integer" id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">0</cn><cn type="integer" id="S3.SS3.p4.1.m1.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.2c">p\in[0,1]</annotation></semantics></math>. For each image, a uniform distributed number <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">x</annotation></semantics></math> was sampled and compared with <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">p</annotation></semantics></math>: if the latter was greater than <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">x</annotation></semantics></math>, the image was replaced by a synthetic one. We considered images generated by StyleGAN2-ADA, for it had obtained the best FID values in the first experiment (such outcomes are later described in Section¬†<a href="#S4" title="4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The deep networks were trained using a learning rate of <math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mn id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><cn type="float" id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">0.0001</annotation></semantics></math>, a decay rate of <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><mn id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><cn type="float" id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">0.9</annotation></semantics></math>, batch size of <math id="S3.SS3.p4.7.m7.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS3.p4.7.m7.1a"><mn id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><cn type="integer" id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">32</annotation></semantics></math>, number of epochs equal to <math id="S3.SS3.p4.8.m8.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS3.p4.8.m8.1a"><mn id="S3.SS3.p4.8.m8.1.1" xref="S3.SS3.p4.8.m8.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.1b"><cn type="integer" id="S3.SS3.p4.8.m8.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.1c">5</annotation></semantics></math>, and samples of size <math id="S3.SS3.p4.9.m9.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS3.p4.9.m9.1a"><mrow id="S3.SS3.p4.9.m9.1.1" xref="S3.SS3.p4.9.m9.1.1.cmml"><mn id="S3.SS3.p4.9.m9.1.1.2" xref="S3.SS3.p4.9.m9.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.9.m9.1.1.1" xref="S3.SS3.p4.9.m9.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p4.9.m9.1.1.3" xref="S3.SS3.p4.9.m9.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m9.1b"><apply id="S3.SS3.p4.9.m9.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1"><times id="S3.SS3.p4.9.m9.1.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1.1"></times><cn type="integer" id="S3.SS3.p4.9.m9.1.1.2.cmml" xref="S3.SS3.p4.9.m9.1.1.2">224</cn><cn type="integer" id="S3.SS3.p4.9.m9.1.1.3.cmml" xref="S3.SS3.p4.9.m9.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m9.1c">224\times 224</annotation></semantics></math> pixels.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">To overcome the issue of unbalanced dataset, Weighted Random Sampler¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, was used which performs oversampling in the minority class. This assigns a higher weight to the minority classes, thus affecting the probability of drawing a point from the majority class by moving from a uniform distribution to a multinomial distribution. Further augmentation was performed using classical image transformations, such as resizing and color jittering, which changes the brightness, contrast, saturation, and horizontal flipping. The test set was kept intact.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">The final round was aimed at comparing the best deep model obtained in the previous phase, ResNet-18, against human experts concerning the task of AMD identification. In this step, twenty real images (ten AMD images and ten non-AMD images) were randomly selected from the test set and provided to human experts for classification purposes. Following that, the same images were submitted to a ResNet-18 for comparison purposes. To allow a fair comparison between humans and deep models, we considered the following measures: standard accuracy (ACC), sensitivity, and specificity.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.7" class="ltx_p">In this work, we used the StyleGAN2-ADA official source code, and the hyperparameters suggested by Karras et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Concerning StyleGAN2-ADA hyperparameters, we used a batch size of <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mn id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><cn type="integer" id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">12</annotation></semantics></math>, ADA target equal to <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><mn id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><cn type="float" id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">0.8</annotation></semantics></math> (i.e., the probability of using ADA mechanism), the Adam algorithm with a learning rate of <math id="S3.SS3.p7.3.m3.1" class="ltx_Math" alttext="0.0025" display="inline"><semantics id="S3.SS3.p7.3.m3.1a"><mn id="S3.SS3.p7.3.m3.1.1" xref="S3.SS3.p7.3.m3.1.1.cmml">0.0025</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m3.1b"><cn type="float" id="S3.SS3.p7.3.m3.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1">0.0025</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m3.1c">0.0025</annotation></semantics></math>, decay values of <math id="S3.SS3.p7.4.m4.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS3.p7.4.m4.1a"><mn id="S3.SS3.p7.4.m4.1.1" xref="S3.SS3.p7.4.m4.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.4.m4.1b"><cn type="integer" id="S3.SS3.p7.4.m4.1.1.cmml" xref="S3.SS3.p7.4.m4.1.1">0</cn></annotation-xml></semantics></math> and <math id="S3.SS3.p7.5.m5.1" class="ltx_Math" alttext="0.99" display="inline"><semantics id="S3.SS3.p7.5.m5.1a"><mn id="S3.SS3.p7.5.m5.1.1" xref="S3.SS3.p7.5.m5.1.1.cmml">0.99</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.5.m5.1b"><cn type="float" id="S3.SS3.p7.5.m5.1.1.cmml" xref="S3.SS3.p7.5.m5.1.1">0.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.5.m5.1c">0.99</annotation></semantics></math>, and a convergence error of <math id="S3.SS3.p7.6.m6.1" class="ltx_Math" alttext="1e^{-8}" display="inline"><semantics id="S3.SS3.p7.6.m6.1a"><mrow id="S3.SS3.p7.6.m6.1.1" xref="S3.SS3.p7.6.m6.1.1.cmml"><mn id="S3.SS3.p7.6.m6.1.1.2" xref="S3.SS3.p7.6.m6.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p7.6.m6.1.1.1" xref="S3.SS3.p7.6.m6.1.1.1.cmml">‚Äã</mo><msup id="S3.SS3.p7.6.m6.1.1.3" xref="S3.SS3.p7.6.m6.1.1.3.cmml"><mi id="S3.SS3.p7.6.m6.1.1.3.2" xref="S3.SS3.p7.6.m6.1.1.3.2.cmml">e</mi><mrow id="S3.SS3.p7.6.m6.1.1.3.3" xref="S3.SS3.p7.6.m6.1.1.3.3.cmml"><mo id="S3.SS3.p7.6.m6.1.1.3.3a" xref="S3.SS3.p7.6.m6.1.1.3.3.cmml">‚àí</mo><mn id="S3.SS3.p7.6.m6.1.1.3.3.2" xref="S3.SS3.p7.6.m6.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.6.m6.1b"><apply id="S3.SS3.p7.6.m6.1.1.cmml" xref="S3.SS3.p7.6.m6.1.1"><times id="S3.SS3.p7.6.m6.1.1.1.cmml" xref="S3.SS3.p7.6.m6.1.1.1"></times><cn type="integer" id="S3.SS3.p7.6.m6.1.1.2.cmml" xref="S3.SS3.p7.6.m6.1.1.2">1</cn><apply id="S3.SS3.p7.6.m6.1.1.3.cmml" xref="S3.SS3.p7.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p7.6.m6.1.1.3.1.cmml" xref="S3.SS3.p7.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS3.p7.6.m6.1.1.3.2.cmml" xref="S3.SS3.p7.6.m6.1.1.3.2">ùëí</ci><apply id="S3.SS3.p7.6.m6.1.1.3.3.cmml" xref="S3.SS3.p7.6.m6.1.1.3.3"><minus id="S3.SS3.p7.6.m6.1.1.3.3.1.cmml" xref="S3.SS3.p7.6.m6.1.1.3.3"></minus><cn type="integer" id="S3.SS3.p7.6.m6.1.1.3.3.2.cmml" xref="S3.SS3.p7.6.m6.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.6.m6.1c">1e^{-8}</annotation></semantics></math> for the generator and discriminator. StyleGAN2-ADA framework enables different augmentations (rotation, geometric transformations, and color transformations) and class-conditional training. The output image resolution was set to <math id="S3.SS3.p7.7.m7.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS3.p7.7.m7.1a"><mrow id="S3.SS3.p7.7.m7.1.1" xref="S3.SS3.p7.7.m7.1.1.cmml"><mn id="S3.SS3.p7.7.m7.1.1.2" xref="S3.SS3.p7.7.m7.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p7.7.m7.1.1.1" xref="S3.SS3.p7.7.m7.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p7.7.m7.1.1.3" xref="S3.SS3.p7.7.m7.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.7.m7.1b"><apply id="S3.SS3.p7.7.m7.1.1.cmml" xref="S3.SS3.p7.7.m7.1.1"><times id="S3.SS3.p7.7.m7.1.1.1.cmml" xref="S3.SS3.p7.7.m7.1.1.1"></times><cn type="integer" id="S3.SS3.p7.7.m7.1.1.2.cmml" xref="S3.SS3.p7.7.m7.1.1.2">256</cn><cn type="integer" id="S3.SS3.p7.7.m7.1.1.3.cmml" xref="S3.SS3.p7.7.m7.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.7.m7.1c">256\times 256</annotation></semantics></math> pixels.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The experimental results are presented in four sub-sections: (i) FID for synthetic image assessment, (ii) human experts detecting synthetic images, (iii) image perturbation assessment, and (iv) comparison between human experts and deep models for detecting AMD images.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Synthetic Image Assessment</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">Table¬†<a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Synthetic Image Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the FID values for each GAN-based architecture. StyleGAN2-ADA achieved the lowest FID value of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="166" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">166</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">166</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">166</annotation></semantics></math>, while EBGAN was placed in last and its FID value of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="380" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">380</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">380</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">380</annotation></semantics></math> was the highest. The smaller the FID value, the better is the quality of the generated image. Therefore, all further experiments only considered the StyleGAN2-ADA architecture for synthetic image generation.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean FID values for image quality assessment (the best result is highlighted in bold).</figcaption>
<table id="S4.T1.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.10.11" class="ltx_tr">
<td id="S4.T1.10.11.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 9.0pt;"><span id="S4.T1.10.11.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S4.T1.10.11.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 9.0pt;"><span id="S4.T1.10.11.2.1" class="ltx_text ltx_font_bold">Architecture</span></td>
<td id="S4.T1.10.11.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.5pt 9.0pt;"><span id="S4.T1.10.11.3.1" class="ltx_text ltx_font_bold">FID</span></td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 9.0pt;" rowspan="7"><span id="S4.T1.1.1.2.1" class="ltx_text">Unconditional</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 9.0pt;">EBGAN</td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="380.18" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mn id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">380.18</mn><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><cn type="float" id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">380.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">380.18</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">DCGAN</td>
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="326.85" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mn id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">326.85</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><cn type="float" id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">326.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">326.85</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">DRAGAN</td>
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="317.82" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mn id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">317.82</mn><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><cn type="float" id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">317.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">317.82</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">WGAN</td>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="307.00" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mn id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">307.00</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><cn type="float" id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">307.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">307.00</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">LSGAN</td>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="305.59" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mn id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">305.59</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><cn type="float" id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1">305.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">305.59</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">WGAN-GP</td>
<td id="S4.T1.6.6.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.6.6.1.m1.1" class="ltx_Math" alttext="295.23" display="inline"><semantics id="S4.T1.6.6.1.m1.1a"><mn id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml">295.23</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><cn type="float" id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1">295.23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">295.23</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<td id="S4.T1.7.7.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">BEGAN</td>
<td id="S4.T1.7.7.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="225.89" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mn id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">225.89</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><cn type="float" id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">225.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">225.89</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:-0.5pt 9.0pt;" rowspan="3"><span id="S4.T1.8.8.2.1" class="ltx_text">Conditional</span></td>
<td id="S4.T1.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 9.0pt;">CGAN</td>
<td id="S4.T1.8.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.8.8.1.m1.1" class="ltx_Math" alttext="342.59" display="inline"><semantics id="S4.T1.8.8.1.m1.1a"><mn id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml">342.59</mn><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><cn type="float" id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1">342.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">342.59</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.9.9" class="ltx_tr">
<td id="S4.T1.9.9.2" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;">ACGAN</td>
<td id="S4.T1.9.9.1" class="ltx_td ltx_align_center" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.9.9.1.m1.1" class="ltx_Math" alttext="315.36" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><mn id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml">315.36</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><cn type="float" id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1">315.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">315.36</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<td id="S4.T1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 9.0pt;"><span id="S4.T1.10.10.2.1" class="ltx_text ltx_font_bold">StyleGAN2-ADA</span></td>
<td id="S4.T1.10.10.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.5pt 9.0pt;"><math id="S4.T1.10.10.1.m1.1" class="ltx_Math" alttext="\bm{166.17}" display="inline"><semantics id="S4.T1.10.10.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T1.10.10.1.m1.1.1" xref="S4.T1.10.10.1.m1.1.1.cmml">166.17</mn><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><cn type="float" id="S4.T1.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1">166.17</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">\bm{166.17}</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Figure¬†<a href="#S4.F6" title="Figure 6 ‚Ä£ 4.1 Synthetic Image Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows some examples of real and synthetic images produced by StyleGAN2-ADA. The trained model yields realistic-looking images for both, with and without AMD, conditioned by sampling from latent representations. Visual inspection shows that the generated images are similar to the real images. In the AMD images, macula degeneration is evident.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<table id="S4.F6.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.F6.2.2" class="ltx_tr">
<td id="S4.F6.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x10.png" id="S4.F6.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="155" height="61" alt="Refer to caption"></td>
<td id="S4.F6.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x11.png" id="S4.F6.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="155" height="61" alt="Refer to caption"></td>
</tr>
<tr id="S4.F6.2.3" class="ltx_tr">
<td id="S4.F6.2.3.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S4.F6.2.3.2" class="ltx_td ltx_align_center">(b)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Some examples of (a) real and (b) synthetic images (AMD and non-AMD) generated by StyleGAN2-ADA.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Figure¬†<a href="#S4.F7" title="Figure 7 ‚Ä£ 4.1 Synthetic Image Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> provides examples of real and synthetic images that are from eyes, positive and negative to AMD. One can observe the high-quality images that were generated for both, AMD and non-AMD images.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<table id="S4.F7.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.F7.4.4" class="ltx_tr">
<td id="S4.F7.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x12.png" id="S4.F7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></td>
<td id="S4.F7.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x13.png" id="S4.F7.2.2.2.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></td>
<td id="S4.F7.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x14.png" id="S4.F7.3.3.3.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></td>
<td id="S4.F7.4.4.4" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x15.png" id="S4.F7.4.4.4.g1" class="ltx_graphics ltx_img_square" width="52" height="52" alt="Refer to caption"></td>
</tr>
<tr id="S4.F7.4.5" class="ltx_tr">
<td id="S4.F7.4.5.1" class="ltx_td ltx_align_center">(a)</td>
<td id="S4.F7.4.5.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S4.F7.4.5.3" class="ltx_td ltx_align_center">(c)</td>
<td id="S4.F7.4.5.4" class="ltx_td ltx_align_center">(d)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of synthetic and real images for AMD and Non_AMD. (a) real, positive AMD, (b) synthetic, positive AMD, (c) real, Non-AMD and (d) synthetic, non-AMD.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Distinguishing between Synthetic and Real Images</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.8" class="ltx_p">Table¬†<a href="#S4.T2" title="Table 2 ‚Ä£ 4.2 Distinguishing between Synthetic and Real Images ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the outcomes of each clinical expert. For AMD images, the accuracy was <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">50</mn><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">50\%</annotation></semantics></math> (standard deviation of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="21.91\%" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">21.91</mn><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">21.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">21.91\%</annotation></semantics></math>) for clinician #1 and <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="55\%" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">55</mn><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">55\%</annotation></semantics></math> (standard deviation of <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="21.80\%" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">21.80</mn><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">21.80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">21.80\%</annotation></semantics></math>) for clinician #2. For Non-AMD images, clinician #1 achieved an accuracy of <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mn id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">60</mn><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">60\%</annotation></semantics></math> (standard deviation of <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="21.47\%" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mn id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">21.47</mn><mo id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">21.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">21.47\%</annotation></semantics></math>), and clinician #2 obtained an accuracy of <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mrow id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mn id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">50</mn><mo id="S4.SS2.p1.7.m7.1.1.1" xref="S4.SS2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">50\%</annotation></semantics></math> (standard deviation of <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="21.47\%" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mrow id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml"><mn id="S4.SS2.p1.8.m8.1.1.2" xref="S4.SS2.p1.8.m8.1.1.2.cmml">21.47</mn><mo id="S4.SS2.p1.8.m8.1.1.1" xref="S4.SS2.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><apply id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"><csymbol cd="latexml" id="S4.SS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.p1.8.m8.1.1.2">21.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">21.47\%</annotation></semantics></math>). These results highlight that both clinicians could not differentiate between real and synthetic images.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Synthetic versus real images by humans experts.</figcaption>
<table id="S4.T2.12" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.12.13" class="ltx_tr">
<td id="S4.T2.12.13.1" class="ltx_td ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td id="S4.T2.12.13.2" class="ltx_td ltx_border_r ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"></td>
<td id="S4.T2.12.13.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.12.13.3.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T2.12.13.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.12.13.4.1" class="ltx_text ltx_font_bold">Sensitivity</span></td>
<td id="S4.T2.12.13.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.12.13.5.1" class="ltx_text ltx_font_bold">Specificity</span></td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;" rowspan="2"><span id="S4.T2.3.3.4.1" class="ltx_text">AMD</span></td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.3.3.5.1" class="ltx_text ltx_font_bold">Clinician #1</span></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mn id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><cn type="float" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">0.50</annotation></semantics></math></td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mn id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><cn type="float" id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">0.50</annotation></semantics></math></td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mn id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><cn type="float" id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">0.50</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.6.6.4.1" class="ltx_text ltx_font_bold">Clinician #2</span></td>
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="0.55" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mn id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml">0.55</mn><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><cn type="float" id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">0.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">0.55</annotation></semantics></math></td>
<td id="S4.T2.5.5.2" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.5.5.2.m1.1" class="ltx_Math" alttext="0.40" display="inline"><semantics id="S4.T2.5.5.2.m1.1a"><mn id="S4.T2.5.5.2.m1.1.1" xref="S4.T2.5.5.2.m1.1.1.cmml">0.40</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.2.m1.1b"><cn type="float" id="S4.T2.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.2.m1.1.1">0.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.2.m1.1c">0.40</annotation></semantics></math></td>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.6.6.3.m1.1" class="ltx_Math" alttext="0.70" display="inline"><semantics id="S4.T2.6.6.3.m1.1a"><mn id="S4.T2.6.6.3.m1.1.1" xref="S4.T2.6.6.3.m1.1.1.cmml">0.70</mn><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.3.m1.1b"><cn type="float" id="S4.T2.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.3.m1.1.1">0.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.3.m1.1c">0.70</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;" rowspan="2"><span id="S4.T2.9.9.4.1" class="ltx_text">
<span id="S4.T2.9.9.4.1.1" class="ltx_inline-block">
<span id="S4.T2.9.9.4.1.1.1" class="ltx_p">Non-</span>
<span id="S4.T2.9.9.4.1.1.2" class="ltx_p">AMD</span>
</span></span></td>
<td id="S4.T2.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.9.9.5.1" class="ltx_text ltx_font_bold">Clinician #1</span></td>
<td id="S4.T2.7.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.7.7.1.m1.1" class="ltx_Math" alttext="0.60" display="inline"><semantics id="S4.T2.7.7.1.m1.1a"><mn id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml">0.60</mn><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><cn type="float" id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1">0.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">0.60</annotation></semantics></math></td>
<td id="S4.T2.8.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.8.8.2.m1.1" class="ltx_Math" alttext="0.60" display="inline"><semantics id="S4.T2.8.8.2.m1.1a"><mn id="S4.T2.8.8.2.m1.1.1" xref="S4.T2.8.8.2.m1.1.1.cmml">0.60</mn><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.2.m1.1b"><cn type="float" id="S4.T2.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.2.m1.1.1">0.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.2.m1.1c">0.60</annotation></semantics></math></td>
<td id="S4.T2.9.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.9.9.3.m1.1" class="ltx_Math" alttext="0.60" display="inline"><semantics id="S4.T2.9.9.3.m1.1a"><mn id="S4.T2.9.9.3.m1.1.1" xref="S4.T2.9.9.3.m1.1.1.cmml">0.60</mn><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.3.m1.1b"><cn type="float" id="S4.T2.9.9.3.m1.1.1.cmml" xref="S4.T2.9.9.3.m1.1.1">0.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.3.m1.1c">0.60</annotation></semantics></math></td>
</tr>
<tr id="S4.T2.12.12" class="ltx_tr">
<td id="S4.T2.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.12.12.4.1" class="ltx_text ltx_font_bold">Clinician #2</span></td>
<td id="S4.T2.10.10.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.10.10.1.m1.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.T2.10.10.1.m1.1a"><mn id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><cn type="float" id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">0.50</annotation></semantics></math></td>
<td id="S4.T2.11.11.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.11.11.2.m1.1" class="ltx_Math" alttext="0.40" display="inline"><semantics id="S4.T2.11.11.2.m1.1a"><mn id="S4.T2.11.11.2.m1.1.1" xref="S4.T2.11.11.2.m1.1.1.cmml">0.40</mn><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.2.m1.1b"><cn type="float" id="S4.T2.11.11.2.m1.1.1.cmml" xref="S4.T2.11.11.2.m1.1.1">0.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.2.m1.1c">0.40</annotation></semantics></math></td>
<td id="S4.T2.12.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><math id="S4.T2.12.12.3.m1.1" class="ltx_Math" alttext="0.60" display="inline"><semantics id="S4.T2.12.12.3.m1.1a"><mn id="S4.T2.12.12.3.m1.1.1" xref="S4.T2.12.12.3.m1.1.1.cmml">0.60</mn><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.3.m1.1b"><cn type="float" id="S4.T2.12.12.3.m1.1.1.cmml" xref="S4.T2.12.12.3.m1.1.1">0.60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.3.m1.1c">0.60</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Image Perturbation Assessment</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.12" class="ltx_p">Figure¬†<a href="#S4.F8" title="Figure 8 ‚Ä£ 4.3 Image Perturbation Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the accuracy over the test set concerning different percentages (<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">p</annotation></semantics></math> value) of real images that were replaced by synthetic images. Overall, the accuracy improved when combining synthetic and real images. While the accuracy lies between <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">50</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">50\%</annotation></semantics></math> and <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="55\%" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">55</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">55\%</annotation></semantics></math> when using only synthetic images, and between <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="78\%" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">78</mn><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">78\%</annotation></semantics></math> and <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="81\%" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mn id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">81</mn><mo id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">81\%</annotation></semantics></math> when using only real images for training, the combination of both types of images gave the best results. However, this was network dependent: while SqueezeNet accuracy peaked at <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="81\%" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><mrow id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mn id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">81</mn><mo id="S4.SS3.p1.6.m6.1.1.1" xref="S4.SS3.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><csymbol cd="latexml" id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">81\%</annotation></semantics></math> when using <math id="S4.SS3.p1.7.m7.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S4.SS3.p1.7.m7.1a"><mrow id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml"><mn id="S4.SS3.p1.7.m7.1.1.2" xref="S4.SS3.p1.7.m7.1.1.2.cmml">70</mn><mo id="S4.SS3.p1.7.m7.1.1.1" xref="S4.SS3.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><apply id="S4.SS3.p1.7.m7.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1"><csymbol cd="latexml" id="S4.SS3.p1.7.m7.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.7.m7.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">70\%</annotation></semantics></math> of synthetic images, AlexNet obtained its highest accuracy (<math id="S4.SS3.p1.8.m8.1" class="ltx_Math" alttext="82\%" display="inline"><semantics id="S4.SS3.p1.8.m8.1a"><mrow id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml"><mn id="S4.SS3.p1.8.m8.1.1.2" xref="S4.SS3.p1.8.m8.1.1.2.cmml">82</mn><mo id="S4.SS3.p1.8.m8.1.1.1" xref="S4.SS3.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.1b"><apply id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1"><csymbol cd="latexml" id="S4.SS3.p1.8.m8.1.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.8.m8.1.1.2.cmml" xref="S4.SS3.p1.8.m8.1.1.2">82</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.1c">82\%</annotation></semantics></math>) when using only <math id="S4.SS3.p1.9.m9.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS3.p1.9.m9.1a"><mrow id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml"><mn id="S4.SS3.p1.9.m9.1.1.2" xref="S4.SS3.p1.9.m9.1.1.2.cmml">20</mn><mo id="S4.SS3.p1.9.m9.1.1.1" xref="S4.SS3.p1.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.1b"><apply id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1"><csymbol cd="latexml" id="S4.SS3.p1.9.m9.1.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.9.m9.1.1.2.cmml" xref="S4.SS3.p1.9.m9.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.1c">20\%</annotation></semantics></math> of synthetic images. ResNet18 achieved its best of <math id="S4.SS3.p1.10.m10.1" class="ltx_Math" alttext="83\%" display="inline"><semantics id="S4.SS3.p1.10.m10.1a"><mrow id="S4.SS3.p1.10.m10.1.1" xref="S4.SS3.p1.10.m10.1.1.cmml"><mn id="S4.SS3.p1.10.m10.1.1.2" xref="S4.SS3.p1.10.m10.1.1.2.cmml">83</mn><mo id="S4.SS3.p1.10.m10.1.1.1" xref="S4.SS3.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.10.m10.1b"><apply id="S4.SS3.p1.10.m10.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1"><csymbol cd="latexml" id="S4.SS3.p1.10.m10.1.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.10.m10.1.1.2.cmml" xref="S4.SS3.p1.10.m10.1.1.2">83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.10.m10.1c">83\%</annotation></semantics></math> with <math id="S4.SS3.p1.11.m11.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S4.SS3.p1.11.m11.1a"><mrow id="S4.SS3.p1.11.m11.1.1" xref="S4.SS3.p1.11.m11.1.1.cmml"><mn id="S4.SS3.p1.11.m11.1.1.2" xref="S4.SS3.p1.11.m11.1.1.2.cmml">60</mn><mo id="S4.SS3.p1.11.m11.1.1.1" xref="S4.SS3.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.11.m11.1b"><apply id="S4.SS3.p1.11.m11.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1"><csymbol cd="latexml" id="S4.SS3.p1.11.m11.1.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.11.m11.1.1.2.cmml" xref="S4.SS3.p1.11.m11.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.11.m11.1c">60\%</annotation></semantics></math> of synthetic images. In general, the networks performed poorly when the percentage of synthetic images exceeded <math id="S4.SS3.p1.12.m12.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S4.SS3.p1.12.m12.1a"><mrow id="S4.SS3.p1.12.m12.1.1" xref="S4.SS3.p1.12.m12.1.1.cmml"><mn id="S4.SS3.p1.12.m12.1.1.2" xref="S4.SS3.p1.12.m12.1.1.2.cmml">70</mn><mo id="S4.SS3.p1.12.m12.1.1.1" xref="S4.SS3.p1.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.12.m12.1b"><apply id="S4.SS3.p1.12.m12.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1"><csymbol cd="latexml" id="S4.SS3.p1.12.m12.1.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.12.m12.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.12.m12.1c">70\%</annotation></semantics></math>.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2203.13856/assets/x16.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="252" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span> Accuracy over the test set for different percentages of synthetic image for perturbation purposes.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">Figure¬†<a href="#S4.F9" title="Figure 9 ‚Ä£ 4.3 Image Perturbation Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> compares the improvement due to the mixing of the synthetic with real images on the three evaluated architectures. The most significant improvement was by AlexNet, its accuracy increased by approximately <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">8</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">8\%</annotation></semantics></math>, while ResNet18 had highest accuracy (about <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="83\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">83</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">83\%</annotation></semantics></math>) using a mix of synthetic and real images.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2203.13856/assets/x17.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span> Accuracy over the test set using only real (<math id="S4.F9.2.m1.1" class="ltx_Math" alttext="p=0" display="inline"><semantics id="S4.F9.2.m1.1b"><mrow id="S4.F9.2.m1.1.1" xref="S4.F9.2.m1.1.1.cmml"><mi id="S4.F9.2.m1.1.1.2" xref="S4.F9.2.m1.1.1.2.cmml">p</mi><mo id="S4.F9.2.m1.1.1.1" xref="S4.F9.2.m1.1.1.1.cmml">=</mo><mn id="S4.F9.2.m1.1.1.3" xref="S4.F9.2.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F9.2.m1.1c"><apply id="S4.F9.2.m1.1.1.cmml" xref="S4.F9.2.m1.1.1"><eq id="S4.F9.2.m1.1.1.1.cmml" xref="S4.F9.2.m1.1.1.1"></eq><ci id="S4.F9.2.m1.1.1.2.cmml" xref="S4.F9.2.m1.1.1.2">ùëù</ci><cn type="integer" id="S4.F9.2.m1.1.1.3.cmml" xref="S4.F9.2.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.2.m1.1d">p=0</annotation></semantics></math>) and also mixed data concerning ResNet18, AlexNet, and SqueezeNet architectures.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Tables¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Image Perturbation Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.3 Image Perturbation Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> present the confusion matrix and class-wise performance measures (precision, recall, and F1-score) when ResNet18 architecture was trained with (i) real data only and (ii) mixed data (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="p=0.6" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><eq id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></eq><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">p=0.6</annotation></semantics></math>), respectively. We can observe that ResNet18 trained with mixed data provided more reliable outcomes, where there was a minor difference between precision and recall values, attesting that the network can better distinguish between classes and not be prone to overfit on a specific class.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Confusion matrix and evaluation measures for ResNet18 architecture trained with synthetic data only.</figcaption>
<div id="S4.T3.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:294.1pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T3.10.10" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.10.10.11" class="ltx_tr">
<td id="S4.T3.10.10.11.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T3.10.10.11.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.10.10.11.2.1" class="ltx_text ltx_font_bold">AMD</span></td>
<td id="S4.T3.10.10.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.10.10.11.3.1" class="ltx_text ltx_font_bold">Non-AMD</span></td>
<td id="S4.T3.10.10.11.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.10.10.11.4.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T3.10.10.11.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.10.10.11.5.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T3.10.10.11.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.10.10.11.6.1" class="ltx_text ltx_font_bold">F-Score</span></td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.5.5.5.6.1" class="ltx_text ltx_font_bold">AMD</span></td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="77" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mn id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">77</mn><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><cn type="integer" id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">77</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">77</annotation></semantics></math></td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="28" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mn id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><cn type="integer" id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">28</annotation></semantics></math></td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="0.87" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mn id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">0.87</mn><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><cn type="float" id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">0.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">0.87</annotation></semantics></math></td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="0.73" display="inline"><semantics id="S4.T3.4.4.4.4.m1.1a"><mn id="S4.T3.4.4.4.4.m1.1.1" xref="S4.T3.4.4.4.4.m1.1.1.cmml">0.73</mn><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><cn type="float" id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">0.73</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">0.73</annotation></semantics></math></td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="0.79" display="inline"><semantics id="S4.T3.5.5.5.5.m1.1a"><mn id="S4.T3.5.5.5.5.m1.1.1" xref="S4.T3.5.5.5.5.m1.1.1.cmml">0.79</mn><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><cn type="float" id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">0.79</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">0.79</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.10.10.10" class="ltx_tr">
<td id="S4.T3.10.10.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T3.10.10.10.6.1" class="ltx_text ltx_font_bold">Non-AMD</span></td>
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S4.T3.6.6.6.1.m1.1a"><mn id="S4.T3.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b"><cn type="integer" id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">12</annotation></semantics></math></td>
<td id="S4.T3.7.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T3.7.7.7.2.m1.1" class="ltx_Math" alttext="93" display="inline"><semantics id="S4.T3.7.7.7.2.m1.1a"><mn id="S4.T3.7.7.7.2.m1.1.1" xref="S4.T3.7.7.7.2.m1.1.1.cmml">93</mn><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.2.m1.1b"><cn type="integer" id="S4.T3.7.7.7.2.m1.1.1.cmml" xref="S4.T3.7.7.7.2.m1.1.1">93</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.2.m1.1c">93</annotation></semantics></math></td>
<td id="S4.T3.8.8.8.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T3.8.8.8.3.m1.1" class="ltx_Math" alttext="0.77" display="inline"><semantics id="S4.T3.8.8.8.3.m1.1a"><mn id="S4.T3.8.8.8.3.m1.1.1" xref="S4.T3.8.8.8.3.m1.1.1.cmml">0.77</mn><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.3.m1.1b"><cn type="float" id="S4.T3.8.8.8.3.m1.1.1.cmml" xref="S4.T3.8.8.8.3.m1.1.1">0.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.3.m1.1c">0.77</annotation></semantics></math></td>
<td id="S4.T3.9.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T3.9.9.9.4.m1.1" class="ltx_Math" alttext="0.89" display="inline"><semantics id="S4.T3.9.9.9.4.m1.1a"><mn id="S4.T3.9.9.9.4.m1.1.1" xref="S4.T3.9.9.9.4.m1.1.1.cmml">0.89</mn><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.4.m1.1b"><cn type="float" id="S4.T3.9.9.9.4.m1.1.1.cmml" xref="S4.T3.9.9.9.4.m1.1.1">0.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.4.m1.1c">0.89</annotation></semantics></math></td>
<td id="S4.T3.10.10.10.5" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T3.10.10.10.5.m1.1" class="ltx_Math" alttext="0.82" display="inline"><semantics id="S4.T3.10.10.10.5.m1.1a"><mn id="S4.T3.10.10.10.5.m1.1.1" xref="S4.T3.10.10.10.5.m1.1.1.cmml">0.82</mn><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.5.m1.1b"><cn type="float" id="S4.T3.10.10.10.5.m1.1.1.cmml" xref="S4.T3.10.10.10.5.m1.1.1">0.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.5.m1.1c">0.82</annotation></semantics></math></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Confusion matrix and evaluation measures for ResNet18 architecture trained with mixed data (<math id="S4.T4.2.m1.1" class="ltx_Math" alttext="p=0.6" display="inline"><semantics id="S4.T4.2.m1.1b"><mrow id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml"><mi id="S4.T4.2.m1.1.1.2" xref="S4.T4.2.m1.1.1.2.cmml">p</mi><mo id="S4.T4.2.m1.1.1.1" xref="S4.T4.2.m1.1.1.1.cmml">=</mo><mn id="S4.T4.2.m1.1.1.3" xref="S4.T4.2.m1.1.1.3.cmml">0.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><apply id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1"><eq id="S4.T4.2.m1.1.1.1.cmml" xref="S4.T4.2.m1.1.1.1"></eq><ci id="S4.T4.2.m1.1.1.2.cmml" xref="S4.T4.2.m1.1.1.2">ùëù</ci><cn type="float" id="S4.T4.2.m1.1.1.3.cmml" xref="S4.T4.2.m1.1.1.3">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">p=0.6</annotation></semantics></math>).</figcaption>
<div id="S4.T4.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:294.1pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T4.12.10" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.12.10.11" class="ltx_tr">
<td id="S4.T4.12.10.11.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T4.12.10.11.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.12.10.11.2.1" class="ltx_text ltx_font_bold">AMD</span></td>
<td id="S4.T4.12.10.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.12.10.11.3.1" class="ltx_text ltx_font_bold">Non-AMD</span></td>
<td id="S4.T4.12.10.11.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.12.10.11.4.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T4.12.10.11.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.12.10.11.5.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T4.12.10.11.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.12.10.11.6.1" class="ltx_text ltx_font_bold">F-Score</span></td>
</tr>
<tr id="S4.T4.7.5.5" class="ltx_tr">
<td id="S4.T4.7.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.7.5.5.6.1" class="ltx_text ltx_font_bold">AMD</span></td>
<td id="S4.T4.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.3.1.1.1.m1.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S4.T4.3.1.1.1.m1.1a"><mn id="S4.T4.3.1.1.1.m1.1.1" xref="S4.T4.3.1.1.1.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S4.T4.3.1.1.1.m1.1b"><cn type="integer" id="S4.T4.3.1.1.1.m1.1.1.cmml" xref="S4.T4.3.1.1.1.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.1.1.1.m1.1c">90</annotation></semantics></math></td>
<td id="S4.T4.4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T4.4.2.2.2.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S4.T4.4.2.2.2.m1.1a"><mn id="S4.T4.4.2.2.2.m1.1.1" xref="S4.T4.4.2.2.2.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S4.T4.4.2.2.2.m1.1b"><cn type="integer" id="S4.T4.4.2.2.2.m1.1.1.cmml" xref="S4.T4.4.2.2.2.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.2.2.2.m1.1c">15</annotation></semantics></math></td>
<td id="S4.T4.5.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.5.3.3.3.m1.1" class="ltx_Math" alttext="0.81" display="inline"><semantics id="S4.T4.5.3.3.3.m1.1a"><mn id="S4.T4.5.3.3.3.m1.1.1" xref="S4.T4.5.3.3.3.m1.1.1.cmml">0.81</mn><annotation-xml encoding="MathML-Content" id="S4.T4.5.3.3.3.m1.1b"><cn type="float" id="S4.T4.5.3.3.3.m1.1.1.cmml" xref="S4.T4.5.3.3.3.m1.1.1">0.81</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.3.3.3.m1.1c">0.81</annotation></semantics></math></td>
<td id="S4.T4.6.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.6.4.4.4.m1.1" class="ltx_Math" alttext="0.86" display="inline"><semantics id="S4.T4.6.4.4.4.m1.1a"><mn id="S4.T4.6.4.4.4.m1.1.1" xref="S4.T4.6.4.4.4.m1.1.1.cmml">0.86</mn><annotation-xml encoding="MathML-Content" id="S4.T4.6.4.4.4.m1.1b"><cn type="float" id="S4.T4.6.4.4.4.m1.1.1.cmml" xref="S4.T4.6.4.4.4.m1.1.1">0.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.4.4.4.m1.1c">0.86</annotation></semantics></math></td>
<td id="S4.T4.7.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.7.5.5.5.m1.1" class="ltx_Math" alttext="0.83" display="inline"><semantics id="S4.T4.7.5.5.5.m1.1a"><mn id="S4.T4.7.5.5.5.m1.1.1" xref="S4.T4.7.5.5.5.m1.1.1.cmml">0.83</mn><annotation-xml encoding="MathML-Content" id="S4.T4.7.5.5.5.m1.1b"><cn type="float" id="S4.T4.7.5.5.5.m1.1.1.cmml" xref="S4.T4.7.5.5.5.m1.1.1">0.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.5.5.5.m1.1c">0.83</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.12.10.10" class="ltx_tr">
<td id="S4.T4.12.10.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T4.12.10.10.6.1" class="ltx_text ltx_font_bold">Non-AMD</span></td>
<td id="S4.T4.8.6.6.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.8.6.6.1.m1.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S4.T4.8.6.6.1.m1.1a"><mn id="S4.T4.8.6.6.1.m1.1.1" xref="S4.T4.8.6.6.1.m1.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S4.T4.8.6.6.1.m1.1b"><cn type="integer" id="S4.T4.8.6.6.1.m1.1.1.cmml" xref="S4.T4.8.6.6.1.m1.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.6.6.1.m1.1c">21</annotation></semantics></math></td>
<td id="S4.T4.9.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T4.9.7.7.2.m1.1" class="ltx_Math" alttext="84" display="inline"><semantics id="S4.T4.9.7.7.2.m1.1a"><mn id="S4.T4.9.7.7.2.m1.1.1" xref="S4.T4.9.7.7.2.m1.1.1.cmml">84</mn><annotation-xml encoding="MathML-Content" id="S4.T4.9.7.7.2.m1.1b"><cn type="integer" id="S4.T4.9.7.7.2.m1.1.1.cmml" xref="S4.T4.9.7.7.2.m1.1.1">84</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.7.7.2.m1.1c">84</annotation></semantics></math></td>
<td id="S4.T4.10.8.8.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.10.8.8.3.m1.1" class="ltx_Math" alttext="0.85" display="inline"><semantics id="S4.T4.10.8.8.3.m1.1a"><mn id="S4.T4.10.8.8.3.m1.1.1" xref="S4.T4.10.8.8.3.m1.1.1.cmml">0.85</mn><annotation-xml encoding="MathML-Content" id="S4.T4.10.8.8.3.m1.1b"><cn type="float" id="S4.T4.10.8.8.3.m1.1.1.cmml" xref="S4.T4.10.8.8.3.m1.1.1">0.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.8.8.3.m1.1c">0.85</annotation></semantics></math></td>
<td id="S4.T4.11.9.9.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.11.9.9.4.m1.1" class="ltx_Math" alttext="0.80" display="inline"><semantics id="S4.T4.11.9.9.4.m1.1a"><mn id="S4.T4.11.9.9.4.m1.1.1" xref="S4.T4.11.9.9.4.m1.1.1.cmml">0.80</mn><annotation-xml encoding="MathML-Content" id="S4.T4.11.9.9.4.m1.1b"><cn type="float" id="S4.T4.11.9.9.4.m1.1.1.cmml" xref="S4.T4.11.9.9.4.m1.1.1">0.80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.9.9.4.m1.1c">0.80</annotation></semantics></math></td>
<td id="S4.T4.12.10.10.5" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.12.10.10.5.m1.1" class="ltx_Math" alttext="0.82" display="inline"><semantics id="S4.T4.12.10.10.5.m1.1a"><mn id="S4.T4.12.10.10.5.m1.1.1" xref="S4.T4.12.10.10.5.m1.1.1.cmml">0.82</mn><annotation-xml encoding="MathML-Content" id="S4.T4.12.10.10.5.m1.1b"><cn type="float" id="S4.T4.12.10.10.5.m1.1.1.cmml" xref="S4.T4.12.10.10.5.m1.1.1">0.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.10.10.5.m1.1c">0.82</annotation></semantics></math></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">One of the main issues regarding deep learning in medicine is the difficulty to interpret the decision mechanisms. Techniques, such as heatmaps from the Gradient-weighted Class Activation Mapping (Grad-CAM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">43</span></a>]</cite> can be used to identify and highlight regions of interest for visualization. Figure¬†<a href="#S4.F10" title="Figure 10 ‚Ä£ 4.3 Image Perturbation Assessment ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> depicts such heatmaps in images with AMD, which can help clinicians better interpret the decision-making process. The results also show that ResNet18 architecture trained with mixed data can highlight the location of the lesion better.</p>
</div>
<figure id="S4.F10" class="ltx_figure">
<p id="S4.F10.4" class="ltx_p ltx_align_center"><span id="S4.F10.4.4" class="ltx_text">
<span id="S4.F10.4.4.4" class="ltx_tabular ltx_align_middle">
<span id="S4.F10.4.4.4.4" class="ltx_tr">
<span id="S4.F10.1.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x18.png" id="S4.F10.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="45" height="45" alt="Refer to caption"></span>
<span id="S4.F10.2.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x19.png" id="S4.F10.2.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="45" height="45" alt="Refer to caption"></span>
<span id="S4.F10.3.3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x20.png" id="S4.F10.3.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="45" height="45" alt="Refer to caption"></span>
<span id="S4.F10.4.4.4.4.4" class="ltx_td ltx_align_center"><img src="/html/2203.13856/assets/x21.png" id="S4.F10.4.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="45" height="45" alt="Refer to caption"></span></span>
<span id="S4.F10.4.4.4.5" class="ltx_tr">
<span id="S4.F10.4.4.4.5.1" class="ltx_td ltx_align_center">(a)</span>
<span id="S4.F10.4.4.4.5.2" class="ltx_td ltx_align_center">(b)</span>
<span id="S4.F10.4.4.4.5.3" class="ltx_td ltx_align_center">(c)</span>
<span id="S4.F10.4.4.4.5.4" class="ltx_td ltx_align_center">(d)</span></span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Some images positive to AMD in (a) and (c) and their corresponding heatmaps generated by Grad-CAM in (b) and (d).</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison between Human Experts and Deep Models</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Table¬†<a href="#S4.T5" title="Table 5 ‚Ä£ 4.4 Comparison between Human Experts and Deep Models ‚Ä£ 4 Results ‚Ä£ Which Generative Adversarial Network Yields High-Quality Synthetic Medical Images: Investigation Using AMD Image Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the comparison of AMD detection by human experts and the best deep model, Resnet-18. Overall, the results of both clinicains and deep-learning were similar, with the best performance by deep-learning while the lowest specificity was by clinician #2. This shows that deep models can outperform clinicians for diagnosing AMD in eye fundus images.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison between human experts and deep models to classify AMD and real Non-AMD images.</figcaption>
<table id="S4.T5.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.9.10" class="ltx_tr">
<td id="S4.T5.9.10.1" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T5.9.10.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.9.10.2.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T5.9.10.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.9.10.3.1" class="ltx_text ltx_font_bold">Sensitivity</span></td>
<td id="S4.T5.9.10.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.9.10.4.1" class="ltx_text ltx_font_bold">Specificity</span></td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.3.3.4.1" class="ltx_text ltx_font_bold">Clinician #1</span></td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mn id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><cn type="float" id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">0.8</annotation></semantics></math></td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T5.2.2.2.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.T5.2.2.2.m1.1a"><mn id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><cn type="float" id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">0.8</annotation></semantics></math></td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T5.3.3.3.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.T5.3.3.3.m1.1a"><mn id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><cn type="float" id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">0.8</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<td id="S4.T5.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.6.6.4.1" class="ltx_text ltx_font_bold">Clinician #2</span></td>
<td id="S4.T5.4.4.1" class="ltx_td ltx_align_center"><math id="S4.T5.4.4.1.m1.1" class="ltx_Math" alttext="0.75" display="inline"><semantics id="S4.T5.4.4.1.m1.1a"><mn id="S4.T5.4.4.1.m1.1.1" xref="S4.T5.4.4.1.m1.1.1.cmml">0.75</mn><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.m1.1b"><cn type="float" id="S4.T5.4.4.1.m1.1.1.cmml" xref="S4.T5.4.4.1.m1.1.1">0.75</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.m1.1c">0.75</annotation></semantics></math></td>
<td id="S4.T5.5.5.2" class="ltx_td ltx_align_center"><math id="S4.T5.5.5.2.m1.1" class="ltx_Math" alttext="1.00" display="inline"><semantics id="S4.T5.5.5.2.m1.1a"><mn id="S4.T5.5.5.2.m1.1.1" xref="S4.T5.5.5.2.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.2.m1.1b"><cn type="float" id="S4.T5.5.5.2.m1.1.1.cmml" xref="S4.T5.5.5.2.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.2.m1.1c">1.00</annotation></semantics></math></td>
<td id="S4.T5.6.6.3" class="ltx_td ltx_align_center"><math id="S4.T5.6.6.3.m1.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.T5.6.6.3.m1.1a"><mn id="S4.T5.6.6.3.m1.1.1" xref="S4.T5.6.6.3.m1.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.3.m1.1b"><cn type="float" id="S4.T5.6.6.3.m1.1.1.cmml" xref="S4.T5.6.6.3.m1.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.3.m1.1c">0.50</annotation></semantics></math></td>
</tr>
<tr id="S4.T5.9.9" class="ltx_tr">
<td id="S4.T5.9.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T5.9.9.4.1" class="ltx_text ltx_font_bold">Resnet-18</span></td>
<td id="S4.T5.7.7.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T5.7.7.1.m1.1" class="ltx_Math" alttext="0.85" display="inline"><semantics id="S4.T5.7.7.1.m1.1a"><mn id="S4.T5.7.7.1.m1.1.1" xref="S4.T5.7.7.1.m1.1.1.cmml">0.85</mn><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.1.m1.1b"><cn type="float" id="S4.T5.7.7.1.m1.1.1.cmml" xref="S4.T5.7.7.1.m1.1.1">0.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.1.m1.1c">0.85</annotation></semantics></math></td>
<td id="S4.T5.8.8.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T5.8.8.2.m1.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.T5.8.8.2.m1.1a"><mn id="S4.T5.8.8.2.m1.1.1" xref="S4.T5.8.8.2.m1.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.2.m1.1b"><cn type="float" id="S4.T5.8.8.2.m1.1.1.cmml" xref="S4.T5.8.8.2.m1.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.2.m1.1c">0.9</annotation></semantics></math></td>
<td id="S4.T5.9.9.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T5.9.9.3.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.T5.9.9.3.m1.1a"><mn id="S4.T5.9.9.3.m1.1.1" xref="S4.T5.9.9.3.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.3.m1.1b"><cn type="float" id="S4.T5.9.9.3.m1.1.1.cmml" xref="S4.T5.9.9.3.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.3.m1.1c">0.8</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">DLN are suitable for automated and accurate analysis of medical images, and these are faster and often more accurate than the experts. These have been used for computerized classification of eye fundus images for detection of referable diabetic retinopathy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text ltx_font_typewriter">40</span></a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, retinopathy of prematurity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, exudates on the retina¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, AMD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and AMD severity ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. However DLN require large numbers of annotated images, and the datasets should be balanced. Unbalanced datasets when used for training can lead to bias and erroneous outcomes. However, most suitably annotated medical image datasets are neither large nor balanced. There are number of diverse reasons why this happens, such as commercial interests, privacy of medical data, and issues with obtaining clearance from ethics boards of the hospitals ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. While efforts are being made to develop bigger databases, the other alternate that has been proposed is to develop suitable synthetic images to overcome these shortcomings. Earlier efforts to develop synthetic images were by transforming the images, the use of deep-learning approach for this purpose has been found to be more effective ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Burlina et al have successfully developed a method for ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> generating the synthetic images. Their method is based on GAN, and they tested their method by showing that experts were unable to identify the synthetic images. However, their method has been patented and hence not available for being used by others. We have developed an alternate method based on StyleGAN2, an extension of the progressive GAN, to generate synthetic medical images that human experts were unable to identify. We have also shown that these images were suitable for increase the performance of deep-learning networks. The results were similar to those reported in the literature using patented technology. While Burlina et al. trained a Progressive GAN over a large number of images positive to AMD, our technique has the potential to generate similar high synthetic images through a small number of images positive to AMD.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">ResNet18 architecture trained over real and synthetic images provided the best results, marginally outperforming the human experts‚Äô performance. Therefore, we can conclude that deep models appropriately trained can be as accurate as humans for specific medical image classification applications and datasets. We can extend such an argument to the problem of diagnosing AMD in eye fundus images. On the other hand, one limitation of this study is that these methods have handled the problem only as a binary classification task, i.e., AMD versus non-AMD images. However, medical images are typically We believe more classes will make the problem more challenging to cope with.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">We have made available an online system with this trained network so that anyone can use it and test it, simply by uploading images. The software automatically labels the images as positive or negative to AMD. We have also provided the source code of the entire software and it is available publicly to facilitate researchers to use this as it is, or improve it. We are focused on fostering partnerships to facilitate and conduct research towards the usage of deep-learning to generate and recognize medical images.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Works</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Deep-learning-based analysis of medical images has been found to be very useful, for it can learn from examples without requiring prior knowledge of features that matter. However, its applications are limited because labeled and/ or annotated medical image datasets of eye fundus are usually small and imbalanced.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">This manuscript has developed a method to generate synthetic images that is not patented and the source code is publicly available. We have investigated the use of StyleGAN2-ADA to generate synthetic images using examples from publicly available databases to distinguish between AMD and healthy eyes. We found that experienced clinical experts were unable to differentiate between the synthetic and real images. We also demonstrated that synthetic images generated using StyleGAN2-ADA when mixed with real images improved the classification accuracy of deep learning networks, and these maginally outperformed clinical experts in separating the AMD and Non-AMD retinal images.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Future works regarding the application of this for AMD detection will need to broaden the scope to detect the severity of AMD, and for differentiating from other diseases. For generating medical images, there is the need to consider a broader range of deep architectures and applications. We would also need to investigate the effectiveness of heatmaps generated by Grad-CAM for helping the clinicians.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Data availability</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The iChallenge-AMD dataset can be found in¬†<a target="_blank" href="https://ai.baidu.com/broad/introduction?dataset=amd" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.baidu.com/broad/introduction?dataset=amd</a>, while ODIR-2019 dataset is available on¬†<a target="_blank" href="https://odir2019.grand-challenge.org/dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://odir2019.grand-challenge.org/dataset/</a> and <span id="S7.p1.1.1" class="ltx_text ltx_font_bold">RIADD</span> is available on ¬†<a target="_blank" href="https://riadd.grand-challenge.org/Home/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://riadd.grand-challenge.org/Home/</a>.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Code availability</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The official code for StyleGAN2-ADA is available at¬†<a target="_blank" href="https://github.com/NVlabs/stylegan2-ada-pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVlabs/stylegan2-ada-pytorch</a>. Our implementation of Deep Convolutional GAN (DCGAN), Least Squares Generative Adversarial Networks (LSGAN), Wasserstein GAN (WGAN), Wasserstein GAN with Gradient Penalty (WGAN-GP), Deep Regret Analytic Generative Adversarial Networks (DRAGAN), Energy-based Generative Adversarial Network (EBGAN), Boundary Equilibrium Generative Adversarial Networks (BEGAN), Conditional GAN (CGAN), and Auxiliary classifier GAN (ACGAN) are available for download at¬†<a target="_blank" href="https://github.com/GuiCamargoX/gans_pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/GuiCamargoX/gans_pytorch</a>. All source code concerning image processing, Style-GAN2-ADA data generation, and the pre-trained networks are available at <a target="_blank" href="https://github.com/GuiCamargoX/amd-synthetic" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/GuiCamargoX/amd-synthetic</a>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors thank Dr. Ione F. Alexim from BP Hospital - <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">A Benefic√™ncia Portuguesa de S√£o Paulo</span>, who provided her eye fundus grading expertise to the project. We are grateful to S√£o Paulo Research Foundation (FAPESP) under the grants #2013/07375-0, #2014/12236-1, #2018/15597-6, #2019/00585-5, #2019/07665-4, and #2019/02205-5, to the Brazilian National Council for Research and Development (CNPq) grants #307066/2017-7, #427968/2018-6, #309439/2020-5, and #606573/2021-0, as well as the Engineering and Physical Sciences Research Council (EPSRC) grant EP/T021063/1 and its principal investigator Ahsan Adeel. We also acknowledge the funding from Promobilia Foundation, Sweden, and SPARC grant (P-134, 2019), Department of Biotechnology (India) for financial their support.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author Information</h2>

<section id="Sx2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Contributions</h3>

<div id="Sx2.SS1.p1" class="ltx_para">
<p id="Sx2.SS1.p1.1" class="ltx_p">G.O. developed the algorithms and performed the analysis.
J.P. and D.K. conceptualized and designed the project.
G.R., L.P., D.P. managed the data and designed the software.
H.K. was responsible for the clinical assessment of the images and script.
All authors contributed to writing the manuscript.
D.K, G.O. and JP finalized the manuscript.</p>
</div>
</section>
<section id="Sx2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Corresponding authors</h3>

<div id="Sx2.SS2.p1" class="ltx_para">
<p id="Sx2.SS2.p1.1" class="ltx_p">Correspondence to Guilherme C. Oliveira or Dinesh Kumar.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conflict of Interest declarations</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The authors declare no conflict of interest for this manuscript.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<span id="Sx3.p2.1" class="ltx_ERROR undefined">\printcredits</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arjovsky et¬†al. [2017]</span>
<span class="ltx_bibblock">
Arjovsky, M., Chintala, S.,
Bottou, L., 2017.

</span>
<span class="ltx_bibblock">Wasserstein generative adversarial networks, in:
International conference on machine learning,
PMLR. pp. 214‚Äì223.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellemo et¬†al. [2018]</span>
<span class="ltx_bibblock">
Bellemo, V., Burlina, P.,
Yong, L., Wong, T.Y.,
Ting, D.S.W., 2018.

</span>
<span class="ltx_bibblock">Generative adversarial networks (gans) for retinal
fundus image synthesis, in: Asian Conference on Computer
Vision, Springer. pp. 289‚Äì302.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berthelot et¬†al. [2017]</span>
<span class="ltx_bibblock">
Berthelot, D., Schumm, T.,
Metz, L., 2017.

</span>
<span class="ltx_bibblock">Began: Boundary equilibrium generative adversarial
networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1703.10717 .

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et¬†al. [2018]</span>
<span class="ltx_bibblock">
Brown, J.M., Campbell, J.P.,
Beers, A., Chang, K.,
Ostmo, S., Chan, R.P.,
Dy, J., Erdogmus, D.,
Ioannidis, S., Kalpathy-Cramer, J.,
et¬†al., 2018.

</span>
<span class="ltx_bibblock">Automated diagnosis of plus disease in retinopathy of
prematurity using deep convolutional neural networks.

</span>
<span class="ltx_bibblock">JAMA ophthalmology 136,
803‚Äì810.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burlina et¬†al. [2018a]</span>
<span class="ltx_bibblock">
Burlina, P., Joshi, N.,
Pacheco, K.D., Freund, D.E.,
Kong, J., Bressler, N.M.,
2018a.

</span>
<span class="ltx_bibblock">Utility of deep learning methods for referability
classification of age-related macular degeneration.

</span>
<span class="ltx_bibblock">JAMA ophthalmology 136,
1305‚Äì1307.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burlina et¬†al. [2018b]</span>
<span class="ltx_bibblock">
Burlina, P.M., Joshi, N.,
Pacheco, K.D., Freund, D.E.,
Kong, J., Bressler, N.M.,
2018b.

</span>
<span class="ltx_bibblock">Use of deep learning for detailed severity
characterization and estimation of 5-year risk among patients with
age-related macular degeneration.

</span>
<span class="ltx_bibblock">JAMA ophthalmology 136,
1359‚Äì1366.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burlina et¬†al. [2019]</span>
<span class="ltx_bibblock">
Burlina, P.M., Joshi, N.,
Pacheco, K.D., Liu, T.A.,
Bressler, N.M., 2019.

</span>
<span class="ltx_bibblock">Assessment of deep generative models for
high-resolution synthetic retinal image generation of age-related macular
degeneration.

</span>
<span class="ltx_bibblock">JAMA ophthalmology 137,
258‚Äì264.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burlina et¬†al. [2017]</span>
<span class="ltx_bibblock">
Burlina, P.M., Joshi, N.,
Pekala, M., Pacheco, K.D.,
Freund, D.E., Bressler, N.M.,
2017.

</span>
<span class="ltx_bibblock">Automated grading of age-related macular degeneration
from color fundus images using deep convolutional neural networks.

</span>
<span class="ltx_bibblock">JAMA ophthalmology 135,
1170‚Äì1176.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et¬†al. [2020]</span>
<span class="ltx_bibblock">
Das, V., Dandapat, S.,
Bora, P.K., 2020.

</span>
<span class="ltx_bibblock">Unsupervised super-resolution of oct images using
generative adversarial network for improved age-related macular degeneration
diagnosis.

</span>
<span class="ltx_bibblock">IEEE Sensors Journal 20,
8746‚Äì8756.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De¬†Fauw et¬†al. [2018]</span>
<span class="ltx_bibblock">
De¬†Fauw, J., Ledsam, J.R.,
Romera-Paredes, B., Nikolov, S.,
Tomasev, N., Blackwell, S.,
Askham, H., Glorot, X.,
O‚ÄôDonoghue, B., Visentin, D., et¬†al.,
2018.

</span>
<span class="ltx_bibblock">Clinically applicable deep learning for diagnosis and
referral in retinal disease.

</span>
<span class="ltx_bibblock">Nature medicine 24,
1342‚Äì1350.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et¬†al. [2009]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W.,
Socher, R., Li, L.J.,
Li, K., Fei-Fei, L.,
2009.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database,
in: 2009 IEEE conference on computer vision and pattern
recognition, Ieee. pp. 248‚Äì255.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Efraimidis and Spirakis [2008]</span>
<span class="ltx_bibblock">
Efraimidis, P., Spirakis, P.,
2008.

</span>
<span class="ltx_bibblock">Weighted random sampling, in: Kao,
M.Y. (Ed.), Encyclopedia of Algorithms,
Springer US, Boston, MA. pp.
1024‚Äì1027.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://doi.org/10.1007/978-0-387-30162-4_478" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-0-387-30162-4_478</a>,
doi:<a target="_blank" href="https:/doi.org/10.1007/978-0-387-30162-4_478" title="" class="ltx_ref">10.1007/978-0-387-30162-4_478</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esteva et¬†al. [2019]</span>
<span class="ltx_bibblock">
Esteva, A., Robicquet, A.,
Ramsundar, B., Kuleshov, V.,
DePristo, M., Chou, K.,
Cui, C., Corrado, G.,
Thrun, S., Dean, J.,
2019.

</span>
<span class="ltx_bibblock">A guide to deep learning in healthcare.

</span>
<span class="ltx_bibblock">Nature medicine 25,
24‚Äì29.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frid-Adar et¬†al. [2018]</span>
<span class="ltx_bibblock">
Frid-Adar, M., Diamant, I.,
Klang, E., Amitai, M.,
Goldberger, J., Greenspan, H.,
2018.

</span>
<span class="ltx_bibblock">Gan-based synthetic medical image augmentation for
increased cnn performance in liver lesion classification.

</span>
<span class="ltx_bibblock">Neurocomputing 321,
321‚Äì331.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et¬†al. [2020]</span>
<span class="ltx_bibblock">
Fu, H., Li, F., Orlando,
J., Bogunovic, H., Sun, X.,
Liao, J., Xu, Y., Zhang,
S., Zhang, X., 2020.

</span>
<span class="ltx_bibblock">Adam: Automatic detection challenge on age-related
macular degeneration.

</span>
<span class="ltx_bibblock">IEEE Dataport .

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et¬†al. [2019]</span>
<span class="ltx_bibblock">
Fu, H., Wang, B., Shen,
J., Cui, S., Xu, Y.,
Liu, J., Shao, L., 2019.

</span>
<span class="ltx_bibblock">Evaluation of retinal image quality assessment
networks in different color-spaces, in: International
Conference on Medical Image Computing and Computer-Assisted Intervention,
Springer. pp. 48‚Äì56.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gargeya and Leng [2017]</span>
<span class="ltx_bibblock">
Gargeya, R., Leng, T.,
2017.

</span>
<span class="ltx_bibblock">Automated identification of diabetic retinopathy
using deep learning.

</span>
<span class="ltx_bibblock">Ophthalmology 124,
962‚Äì969.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et¬†al. [2014]</span>
<span class="ltx_bibblock">
Goodfellow, I.J., Pouget-Abadie, J.,
Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S.,
Courville, A., Bengio, Y.,
2014.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1406.2661 .

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grassmann et¬†al. [2018]</span>
<span class="ltx_bibblock">
Grassmann, F., Mengelkamp, J.,
Brandl, C., Harsch, S.,
Zimmermann, M.E., Linkohr, B.,
Peters, A., Heid, I.M.,
Palm, C., Weber, B.H.,
2018.

</span>
<span class="ltx_bibblock">A deep learning algorithm for prediction of
age-related eye disease study severity scale for age-related macular
degeneration from color fundus photography.

</span>
<span class="ltx_bibblock">Ophthalmology 125,
1410‚Äì1420.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulrajani et¬†al. [2017]</span>
<span class="ltx_bibblock">
Gulrajani, I., Ahmed, F.,
Arjovsky, M., Dumoulin, V.,
Courville, A., 2017.

</span>
<span class="ltx_bibblock">Improved training of wasserstein gans.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1704.00028 .

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulshan et¬†al. [2016]</span>
<span class="ltx_bibblock">
Gulshan, V., Peng, L.,
Coram, M., Stumpe, M.C.,
Wu, D., Narayanaswamy, A.,
Venugopalan, S., Widner, K.,
Madams, T., Cuadros, J., et¬†al.,
2016.

</span>
<span class="ltx_bibblock">Development and validation of a deep learning
algorithm for detection of diabetic retinopathy in retinal fundus
photographs.

</span>
<span class="ltx_bibblock">Jama 316,
2402‚Äì2410.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harvey [2003]</span>
<span class="ltx_bibblock">
Harvey, P.T., 2003.

</span>
<span class="ltx_bibblock">Common eye diseases of elderly people: identifying
and treating causes of vision loss.

</span>
<span class="ltx_bibblock">Gerontology 49,
1‚Äì11.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. [2016]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren,
S., Sun, J., 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition, in:
Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 770‚Äì778.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et¬†al. [2017]</span>
<span class="ltx_bibblock">
Heusel, M., Ramsauer, H.,
Unterthiner, T., Nessler, B.,
Hochreiter, S., 2017.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge
to a local nash equilibrium.

</span>
<span class="ltx_bibblock">Advances in neural information processing systems
30.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iandola et¬†al. [2016]</span>
<span class="ltx_bibblock">
Iandola, F.N., Han, S.,
Moskewicz, M.W., Ashraf, K.,
Dally, W.J., Keutzer, K.,
2016.

</span>
<span class="ltx_bibblock">Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and&lt; 0.5 mb model size.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1602.07360 .

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jonas [2014]</span>
<span class="ltx_bibblock">
Jonas, J.B., 2014.

</span>
<span class="ltx_bibblock">Global prevalence of age-related macular
degeneration.

</span>
<span class="ltx_bibblock">The Lancet Global Health 2,
e65‚Äìe66.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et¬†al. [2017]</span>
<span class="ltx_bibblock">
Karras, T., Aila, T.,
Laine, S., Lehtinen, J.,
2017.

</span>
<span class="ltx_bibblock">Progressive growing of gans for improved quality,
stability, and variation.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1710.10196 .

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et¬†al. [2020a]</span>
<span class="ltx_bibblock">
Karras, T., Aittala, M.,
Hellsten, J., Laine, S.,
Lehtinen, J., Aila, T.,
2020a.

</span>
<span class="ltx_bibblock">Training generative adversarial networks with limited
data, in: Proc. NeurIPS.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et¬†al. [2019]</span>
<span class="ltx_bibblock">
Karras, T., Laine, S.,
Aila, T., 2019.

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative
adversarial networks, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp.
4401‚Äì4410.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras et¬†al. [2020b]</span>
<span class="ltx_bibblock">
Karras, T., Laine, S.,
Aittala, M., Hellsten, J.,
Lehtinen, J., Aila, T.,
2020b.

</span>
<span class="ltx_bibblock">Analyzing and improving the image quality of
stylegan, in: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 8110‚Äì8119.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khojasteh et¬†al. [2019]</span>
<span class="ltx_bibblock">
Khojasteh, P., Passos, L.A.,
Carvalho, T., Rezende, E.,
Aliahmad, B., Papa, J.P.,
Kumar, D.K., 2019.

</span>
<span class="ltx_bibblock">Exudate detection in fundus images using
deeply-learnable features.

</span>
<span class="ltx_bibblock">Computers in biology and medicine
104, 62‚Äì69.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba [2017]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J., 2017.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1412.6980</a><span id="bib.bib32.1.1" class="ltx_text ltx_font_typewriter">.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text ltx_font_typewriter">Kodali et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text ltx_font_typewriter">
Kodali, N., Abernethy, J.,
Hays, J., Kira, Z., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text ltx_font_typewriter">On convergence and stability of gans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:1705.07215 .
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text ltx_font_typewriter">Krizhevsky et¬†al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text ltx_font_typewriter">
Krizhevsky, A., Sutskever, I.,
Hinton, G.E., 2012.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text ltx_font_typewriter">Imagenet classification with deep convolutional
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text ltx_font_typewriter">Advances in neural information processing systems
25, 1097--1105.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text ltx_font_typewriter">Mao et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text ltx_font_typewriter">
Mao, X., Li, Q., Xie, H.,
Lau, R.Y., Wang, Z.,
Paul¬†Smolley, S., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text ltx_font_typewriter">Least squares generative adversarial networks, in:
Proceedings of the IEEE international conference on
computer vision, pp. 2794--2802.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text ltx_font_typewriter">Mirza and Osindero [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text ltx_font_typewriter">
Mirza, M., Osindero, S.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text ltx_font_typewriter">Conditional generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:1411.1784 .
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text ltx_font_typewriter">Odena et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text ltx_font_typewriter">
Odena, A., Olah, C.,
Shlens, J., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text ltx_font_typewriter">Conditional image synthesis with auxiliary classifier
gans, in: International conference on machine learning,
PMLR. pp. 2642--2651.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text ltx_font_typewriter">Pachade et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text ltx_font_typewriter">
Pachade, S., Porwal, P.,
Thulkar, D., Kokare, M.,
Deshmukh, G., Sahasrabuddhe, V.,
Giancardo, L., Quellec, G.,
M√©riaudeau, F., 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text ltx_font_typewriter">Retinal fundus multi-disease image dataset (rfmid): A
dataset for multi-disease detection research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text ltx_font_typewriter">Data 6, 14.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.4.4.1" class="ltx_text ltx_font_typewriter">Perez and Wang [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text ltx_font_typewriter">
Perez, L., Wang, J., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text ltx_font_typewriter">The effectiveness of data augmentation in image
classification using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:1712.04621 .
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text ltx_font_typewriter">Quellec et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text ltx_font_typewriter">
Quellec, G., Charri√®re, K.,
Boudi, Y., Cochener, B.,
Lamard, M., 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text ltx_font_typewriter">Deep image mining for diabetic retinopathy
screening.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text ltx_font_typewriter">Medical image analysis 39,
178--193.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text ltx_font_typewriter">Radford et¬†al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text ltx_font_typewriter">
Radford, A., Metz, L.,
Chintala, S., 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text ltx_font_typewriter">Unsupervised representation learning with deep
convolutional generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:1511.06434 .
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text ltx_font_typewriter">Salehinejad et¬†al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text ltx_font_typewriter">
Salehinejad, H., Valaee, S.,
Dowdell, T., Colak, E.,
Barfett, J., 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text ltx_font_typewriter">Generalization of deep neural networks for chest
pathology classification in x-rays using generative adversarial networks,
in: 2018 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE. pp.
990--994.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text ltx_font_typewriter">Selvaraju et¬†al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text ltx_font_typewriter">
Selvaraju, R.R., Cogswell, M.,
Das, A., Vedantam, R.,
Parikh, D., Batra, D.,
2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text ltx_font_typewriter">Grad-cam: Visual explanations from deep networks via
gradient-based localization, in: Proceedings of the IEEE
international conference on computer vision, pp. 618--626.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text ltx_font_typewriter">Souza¬†Jr et¬†al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text ltx_font_typewriter">
Souza¬†Jr, L.A., Passos, L.A.,
Mendel, R., Ebigbo, A.,
Probst, A., Messmann, H.,
Palm, C., Papa, J.P.,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text ltx_font_typewriter">Assisting barrett‚Äôs esophagus identification using
endoscopic data augmentation based on generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text ltx_font_typewriter">Computers in Biology and Medicine ,
104029.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text ltx_font_typewriter">Souza¬†Jr et¬†al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text ltx_font_typewriter">
Souza¬†Jr, L.A., Passos, L.A.,
Mendel, R., Ebigbo, A.,
Probst, A., Messmann, H.,
Palm, C., Papa, J.P.,
2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text ltx_font_typewriter">Fine-tuning generative adversarial networks using
metaheuristics: A case study on barrett‚Äôs esophagus identification, in:
Bildverarbeitung f√ºr die Medizin 2021. Proceedings,
German Workshop on Medical Image Computing, Regensburg, March 7-9, 2021,
Springer Vieweg. pp. 205--210.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text ltx_font_typewriter">Wong et¬†al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text ltx_font_typewriter">
Wong, W.L., Su, X., Li,
X., Cheung, C.M.G., Klein, R.,
Cheng, C.Y., Wong, T.Y.,
2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text ltx_font_typewriter">Global prevalence of age-related macular degeneration
and disease burden projection for 2020 and 2040: a systematic review and
meta-analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text ltx_font_typewriter">The Lancet Global Health 2,
e106--e116.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text ltx_font_typewriter">Zhao et¬†al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text ltx_font_typewriter">
Zhao, J., Mathieu, M.,
LeCun, Y., 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text ltx_font_typewriter">Energy-based generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text ltx_font_typewriter">arXiv preprint arXiv:1609.03126 .
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2203.13855" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2203.13856" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2203.13856">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2203.13856" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2203.13857" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 09:18:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
