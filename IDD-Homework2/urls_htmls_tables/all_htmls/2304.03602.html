<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.03602] Pallet Detection from Synthetic Data using Game Engines</title><meta property="og:description" content="This research sets out to assess the viability of using game engines to generate synthetic training data for machine learning in the context of pallet segmentation. Using synthetic data has been proven in prior researc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pallet Detection from Synthetic Data using Game Engines">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Pallet Detection from Synthetic Data using Game Engines">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.03602">

<!--Generated on Thu Feb 29 16:01:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pallet Detection from Synthetic Data using Game Engines</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jouveer Naidoo<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>, Nicholas Bates<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">∗</span></sup>,
<br class="ltx_break"><span id="id3.3.1" class="ltx_text ltx_font_bold">Trevor Gee, Mahla Nejati<sup id="id3.3.1.1" class="ltx_sup"><span id="id3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗∗</span></sup></span> 
<br class="ltx_break">Centre for Automation and Robotic Engineering Science
<br class="ltx_break">The University of Auckland, New Zealand
<br class="ltx_break"><sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">∗</span></sup>{jnai894, nbat773}@aucklanduni.ac.nz, <sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">∗∗</span></sup>m.nejati@auckland.ac.nz
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">This research sets out to assess the viability of using game engines to generate synthetic training data for machine learning in the context of pallet segmentation. Using synthetic data has been proven in prior research to be a viable means of training neural networks, and saves hours of manual labour due to the reduced need for manual image annotation. Machine vision for pallet detection can benefit from synthetic data as the industry increases the development of autonomous warehousing technologies. As per our methodology, we developed a tool capable of automatically generating large amounts of annotated training data from 3D models at pixel-perfect accuracy and a much faster rate than manual approaches. Regarding image segmentation, a Mask R-CNN pipeline was used, which achieved an AP50 of 86% for individual pallets.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Supervised machine learning typically requires a large amount of training data that is often manually labelled. Manual labelling is tedious, work-intensive, expensive, and error-prone. To illustrate this, in the context of the pallet segmentation problem that is the focus of this research, a small set of 70 images of pallets in a warehouse took one person approximately three hours to complete.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Of course, if synthetic data resulted in models of equivalent quality to ones generated from manually labelled data, this could overcome many problems associated with manually labelled data. Additionally, this notion is greatly encouraged because modern graphic engines can now generate high-quality “photorealistic” renderings.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The aims of this research are to validate the feasibility of using synthetically generated pallets in order to train a neural network to operate on real images. This is useful for our research partner who is developing a self-driving forklift system where the localisation of pallets in images would aid in the movement planning of forklifts, as well as other autonomous vehicles in general.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A State-of-the-art graphics engine will be used to generate realistic models of warehouses and pallets, along with accurate labels and annotations, to train a neural network capable of detecting and classifying pallets in real-world scenarios. In addition, it identifies certain factors that affect the performance of detection and classification of pallets when using synthetic data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main driver for this work was the pallet detection problem, which is an essential step toward fully autonomous factory floors.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This part of the paper focuses on existing research into machine vision, synthetic data and pallet detection. Existing literature broadly falls into these three categories and is crucial in order to define a starting point for the project.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Machine Vision</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This section presents an overview of current and recent deep learning object detection methodologies, models, and tools in order to establish a background on how pallets are to be detected with machine learning.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Many object detection problems involve identifying and classifying several common objects, such as people and vehicles <span id="S2.SS1.p2.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p2.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. In order to train models to do this, image data-sets such as MS COCO <span id="S2.SS1.p2.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p2.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> and Open Image <span id="S2.SS1.p2.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p2.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p2.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> exist, which together contain millions of images and annotations for everyday objects <span id="S2.SS1.p2.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p2.1.8" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p2.1.8.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. It is thus possible to use these sets to train a neural network for simple object detection <span id="S2.SS1.p2.1.9" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p2.1.10" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS1.p2.1.11" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p2.1.11.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">A number of different object detectors are used, such as YOLOv4 <span id="S2.SS1.p3.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> from the YOLO (You Only Look Once) <span id="S2.SS1.p3.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> family. Swin-L <span id="S2.SS1.p3.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> was found by <span id="S2.SS1.p3.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.8" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.8.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> to have the highest average precision of 57.70% on MS COCO at the time of publication. “While CNNs have been the backbone on advancement in vision, they have some inherent shortcomings” <span id="S2.SS1.p3.1.9" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.10" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.10.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, which is what Swin-L overcomes by the use of transformers <span id="S2.SS1.p3.1.11" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p3.1.12" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p3.1.12.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Some more popular models, which are supported by tools such as Detectron2 <span id="S2.SS1.p4.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> and MMDetection <span id="S2.SS1.p4.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, include R-CNN <span id="S2.SS1.p4.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, Fast R-CNN <span id="S2.SS1.p4.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.8" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.8.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, Faster R-CNN <span id="S2.SS1.p4.1.9" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.10" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.10.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> and Mask R-CNN <span id="S2.SS1.p4.1.11" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.12" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.12.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, which are all in the Region-Convolutional Neural Network (R-CNN) family, as well as Single Shot MultiBox Detector <span id="S2.SS1.p4.1.13" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS1.p4.1.14" class="ltx_text ltx_font_bold">?<span id="S2.SS1.p4.1.14.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. These models could be looked into more for future work with the paper, as certain models may offer better detection rates than others.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Machine vision is currently solved using machine learning, which requires training data in all cases.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pallet Detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">One of the most common methods of detecting pallets was through the use of a monocular vision sensor, also known as a camera <span id="S2.SS2.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.8" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. A neural network is then used, typically with R-CNN <span id="S2.SS2.p1.1.10" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.11" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.11.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, YOLOv4 <span id="S2.SS2.p1.1.12" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.13" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.13.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> or SSD <span id="S2.SS2.p1.1.14" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.15" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p1.1.16" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.16.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. In <span id="S2.SS2.p1.1.17" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p1.1.18" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p1.1.18.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, it was found that R-CNN and SSD perform better than YOLOv4.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p2.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> found that they achieved a successful detection rate using a three-stage method of running an image through the CNN, which would detect pallet pockets and faces before finally linking the pockets to faces. They could even take into account pallets wrapped in plastic film, as well as varying elevations, racks, and orientations. This was successful with a camera resolution of 3280x2464, with 1344 images: 991 training and 353 testing <span id="S2.SS2.p2.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p2.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Another common method of pallet detection uses laser scanners <span id="S2.SS2.p3.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p3.1.5" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p3.1.6" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p3.1.7" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p3.1.7.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. 2D laser rangefinders were used in <span id="S2.SS2.p3.1.8" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p3.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p3.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, where the data was run through a two-stage system consisting of an R-CNN detector and CNN classifier, with a Kalman filter <span id="S2.SS2.p3.1.10" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p3.1.11" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p3.1.11.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> used to localise and track pallets. This method yielded 99.58% testing accuracy on 340 labelled scans <span id="S2.SS2.p3.1.12" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p3.1.13" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p3.1.13.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p4.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> used a time-of-flight camera to collect point cloud information to run through a ResNet <span id="S2.SS2.p4.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p4.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p4.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> model. Pallet pocket locations are extracted from the point clouds, and the centres are then determined. The 03D303 IFM Electronics Ltd. camera used outputs 93000 distance and grey values for each measurement and is thus highly detailed. The accuracy achieved was up to 94.5% and included pallets with and without loads, as well as those in stacks or individually positioned.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Pallet detection has also been achieved without machine learning. One such way is by using fiducial markers positioned in the centre and on the sides of the pallets to determine edge positions <span id="S2.SS2.p5.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p5.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p5.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Another method makes use of the rectangular shape of pallets for Haar-like features detection <span id="S2.SS2.p5.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p5.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p5.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, making use of <span id="S2.SS2.p5.1.5" class="ltx_text ltx_font_italic">OpenCV<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span id="footnote1.5" class="ltx_text ltx_font_upright">https://opencv.org/</span></span></span></span></span>. Additionally, methods involving the use of ultrasonic chirping, radio waves <span id="S2.SS2.p5.1.6" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p5.1.7" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p5.1.7.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, and RFID tags positioned throughout the environment <span id="S2.SS2.p5.1.8" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p5.1.9" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p5.1.9.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, were used; however, these were found to typically be no more accurate than with sensors just on the trucks <span id="S2.SS2.p5.1.10" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS2.p5.1.11" class="ltx_text ltx_font_bold">?<span id="S2.SS2.p5.1.11.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Machine learning methods offer advantages in the way that no external sensors or marker placements are required to be put on the pallets or any of the warehouse infrastructure besides the camera placed on the vehicle.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Synthetic Data</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">A general advantage of using synthetic data is anonymity, as there is no personally identifiable information, thus avoiding privacy, ethical and legal issues <span id="S2.SS3.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. It also removes the need for manual image collection for training data in the ideal case.
Synthetic data has been shown to achieve high accuracy in the right environments, such as 87% in inventory tracking <span id="S2.SS3.p1.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, and about 90% for CSV data predictions <span id="S2.SS3.p1.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. In one example, adding labelled synthetic data had practically no effect on the quality of classification <span id="S2.SS3.p1.1.7" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p1.1.8" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p1.1.8.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">There are several available synthetic data generation solutions, including those provided commercially by <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">mostly.ai<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_upright">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_upright">https://mostly.ai/</span></span></span></span></span>, or, the Synthetic Data Vault<span id="S2.SS3.p2.1.2" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p2.1.3" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p2.1.3.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span> or Data Synthesizer<span id="S2.SS3.p2.1.4" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p2.1.5" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p2.1.5.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. Data Synthesizer is proven to be suitable for the generation of text-based data sets by <span id="S2.SS3.p2.1.6" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p2.1.7" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p2.1.7.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. These tools are geared towards the generation of synthetic text data. <span id="S2.SS3.p2.1.8" class="ltx_text ltx_font_italic">POVRay<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><span id="footnote3.5" class="ltx_text ltx_font_upright">http://www.povray.org/</span></span></span></span></span> is a tool that enables the programming of 3D environments to generate synthetic data.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Additionally, Game engines such as <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">Unity<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_upright">4</span></span><span id="footnote4.5" class="ltx_text ltx_font_upright">https://unity.com/</span></span></span></span></span> and <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">Unreal Engine<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_upright">5</span></span><span id="footnote5.5" class="ltx_text ltx_font_upright">https://www.unrealengine.com/</span></span></span></span></span> are becoming popular for synthetic data generation due to their ease of use <span id="S2.SS3.p3.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p3.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p3.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.
The only identified downsides of this approach are the domain gap and the need for human labour to generate 3D models <span id="S2.SS3.p3.1.5" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p3.1.6" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p3.1.6.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.
In most situations where much training data is required, these are likely heavily outweighed by the reduced number of hours needed to label images manually.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Some models trained on synthetic data sets lose significant accuracy when run against actual data. For example, a model that achieved greater than 90% accuracy on synthetic captcha images was reduced to approximately 0% on real images <span id="S2.SS3.p4.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p4.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>. This loss of utility was due to humanly in-perceivable miscalibration. Thus, this kind of difference between synthetic and real data is not acceptable, even if some broadness mismatch is fine <span id="S2.SS3.p4.1.3" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S2.SS3.p4.1.4" class="ltx_text ltx_font_bold">?<span id="S2.SS3.p4.1.4.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Our contribution</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Prior work has proven methodologies to detect pallets, as well as to solve machine vision problems. Additionally, synthetically generated data has been shown to be effective in machine learning. As such, the research gap that this paper hopes to fill is the detection of objects in real images based on synthetically trained neural networks, with only the use of 2D images, rather than 3D implementations.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For our training and validation, a pipeline that included predominantly Unity and Detectron2 was used. An outline of the pipeline can be seen below in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2304.03602/assets/images/ACRA.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="245" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Project pipeline.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Data generation was handled through the Unity game engine. While normally used for creating games, the engine can be used for other things, such as research and development. Unity was chosen over some of the other 3D rendering software due to its widely available documentation, user support, and longevity in the industry. When compared to other tools like Unreal Engine, which also has some presence in the 3D rendering scene, it was found that unity was easier to use for beginners and to get the initial setup working.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">3D models of pallets, racking, and other warehouse paraphernalia are added to a scene and rendered via a 2D camera at certain angles and positions. Two modes were implemented in which to capture these images: manual and automatic. Using the manual mode, the user is able to navigate the camera to any preferred position and angle, whereas in automatic, the camera follows predetermined spherical routes set by the user.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">While this process captures the images successfully, a number of algorithms had to be developed to generate COCO-style annotations for each of them.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The scene in Unity that was used to render all of the synthetic data can be seen in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, along with all of the ”Spheres of Interest” that the camera moved around to capture different angles of the scene for training.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2304.03602/assets/images/UnityScene.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="244" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Unity scene used for rendering synthetic data.</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">When generating the synthetic data, the generation was broken up into smaller categories based on different scenarios that could occur in a warehouse environment. These included:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Individual Pallets</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Stacked Pallets</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Pallets on Racking</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Pallets on Forklifts</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">All above combined</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">This was done in order to test if and which configurations of pallets yielded better or worse detection results, or if the configurations of the pallets in the scenes did not matter at all.
It is expected that Individual pallets will be the easiest to detect as these have the most simple features compared to the other scenarios. Stacked pallets and all metrics combined are expected to be the worst as stacked pallets will have less distinguishable features as only the faces are visible and not the body.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The end result of these algorithms and synthetic output with unity can be seen in Figures <a href="#S3.F4" title="Figure 4 ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S3.F5" title="Figure 5 ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> where the synthetic data labels are visualised. While these labels that are present match the images to the exact pixel, some portions of the images are not labelled at all. This is due to limitations in the algorithms that were implemented where pallets that were partially behind objects could not be labelled correctly. This leaves certain parts that a human would have labelled, unlabelled by Unity. An example of a pallet that would not be labelled can be seen below in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where the pallet in the back left of the scene would not be labelled since part of it is behind the large stack.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2304.03602/assets/x1.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="170" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Back left unlabelled pallet.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2304.03602/assets/images/MultiplePalletsUnity.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Pallets labelled by Unity engine.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2304.03602/assets/images/MultiplePalletsUnity2.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Pallets labelled by Unity engine.</figcaption>
</figure>
<section id="S3.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Pallet Body Detection</h4>

<div id="S3.SS1.SSSx1.p1" class="ltx_para">
<p id="S3.SS1.SSSx1.p1.1" class="ltx_p">In addition to pallet faces, overall pallet outlines had to be considered. This was particularly challenging as certain vertices had to be added and removed depending on whether the pallet was partially off the screen or if the vertex was not part of the outline. This was done using the Jarvis march algorithm <span id="S3.SS1.SSSx1.p1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span><span id="S3.SS1.SSSx1.p1.1.2" class="ltx_text ltx_font_bold">?<span id="S3.SS1.SSSx1.p1.1.2.1" class="ltx_text ltx_font_medium" style="position:relative; bottom:0.9pt;">]</span></span>, which can be seen in Figure <a href="#S3.F6" title="Figure 6 ‣ Vertex Blocked Check ‣ Algorithms Overview ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and will be discussed later.</p>
</div>
<div id="S3.SS1.SSSx1.p2" class="ltx_para">
<p id="S3.SS1.SSSx1.p2.1" class="ltx_p">The first step in handling body detection was to remove any pallets which had any vertices that were behind the camera. After this, the 3D points were sorted into a clockwise orientation based on their 2D screen space representation around the centroid of the list of points. This was in preparation for another layer of filtering that needed to be done. Another aspect that needed to be taken into account was the fact that if any points in the outer shape of the pallet were blocked by another pallet, the whole pallet body should be excluded from the labelling. This was because pallet-to-pallet occlusion was to be ignored for the scope of the project. This proved hard to compute as after the vertices had been passed through the Jarvis March algorithm, they were represented as 2D coordinates and, therefore could not be checked if there was anything blocking its view to the camera in 3D space. In order to fix this, the vertices needed to be correlated to their index position and then the whole pallet body label was removed if any of the convex hull elements matched the originals and were blocked from the camera view. The pallet bodies were then run through the screen occlusion algorithm to add or remove additional vertices as required.</p>
</div>
</section>
<section id="S3.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Face Detection</h4>

<div id="S3.SS1.SSSx2.p1" class="ltx_para">
<p id="S3.SS1.SSSx2.p1.1" class="ltx_p">In order to handle the detection of pallet faces in the synthetic data, several algorithms were developed to determine which vertices needed to be modified in order to handle cases where they could not be seen or where portions of the faces were occluded by the camera. The first step in this process was determining whether the camera had a direct line of sight to the vertices on the face in question. This could either be out of the camera view or within the camera view; as long as no physical object was between the vertex and the camera, the point was included in the filter. After the faces were filtered, each point in the face was checked to see if it was behind the camera; if it was, the whole face was ignored. Each 3D coordinate point was then converted to a 2D screen space coordinate, and an algorithm was run to determine any modifications that needed to be done in order to handle screen occlusion.</p>
</div>
</section>
<section id="S3.SS1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Algorithms Overview</h4>

<div id="S3.SS1.SSSx3.p1" class="ltx_para">
<p id="S3.SS1.SSSx3.p1.1" class="ltx_p">Several algorithms were used in the creation of the automated labelling data in order to handle the vertex detection within Unity; this was because, while the bounding boxes of each pallet were provided, certain points had to be added or removed if the pallets were partially blocked or off of the screen. The following are some of the algorithms used to handle this task.</p>
</div>
<section id="S3.SS1.SSSx3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Clockwise Point Algorithm</h5>

<div id="S3.SS1.SSSx3.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSSx3.Px1.p1.2" class="ltx_p">Clockwise points were handled using the built-in <math id="S3.SS1.SSSx3.Px1.p1.1.m1.1" class="ltx_Math" alttext="Atan2" display="inline"><semantics id="S3.SS1.SSSx3.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1a" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.4" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1b" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.5" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1c" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1.cmml">​</mo><mn id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.6" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.6.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSSx3.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1"><times id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.3">𝑡</ci><ci id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.4">𝑎</ci><ci id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.5.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.5">𝑛</ci><cn type="integer" id="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.6.cmml" xref="S3.SS1.SSSx3.Px1.p1.1.m1.1.1.6">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSSx3.Px1.p1.1.m1.1c">Atan2</annotation></semantics></math> maths function, which takes an x and y coordinate and returns the angle from the x-axis of the corresponding line from the origin. This angle could then be used to determine the angle of the coordinate point and sort it accordingly. However, this had to be tweaked a little bit to move the <math id="S3.SS1.SSSx3.Px1.p1.2.m2.1" class="ltx_Math" alttext="Atan2" display="inline"><semantics id="S3.SS1.SSSx3.Px1.p1.2.m2.1a"><mrow id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1a" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.4" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1b" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.5" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1c" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1.cmml">​</mo><mn id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.6" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.6.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSSx3.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1"><times id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.2">𝐴</ci><ci id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.3">𝑡</ci><ci id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.4.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.4">𝑎</ci><ci id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.5.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.5">𝑛</ci><cn type="integer" id="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.6.cmml" xref="S3.SS1.SSSx3.Px1.p1.2.m2.1.1.6">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSSx3.Px1.p1.2.m2.1c">Atan2</annotation></semantics></math> point from the origin to the centroid of all the points. Therefore, the centroid was calculated beforehand and subtracted from all the points to create the required offset.</p>
</div>
</section>
<section id="S3.SS1.SSSx3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Jarvis March Algorithm</h5>

<div id="S3.SS1.SSSx3.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSSx3.Px2.p1.1" class="ltx_p">The Jarvis March algorithm was used to take a list of coordinates and convert them into a convex hull. Because certain points were not required when labelling the pallet body, these had to be removed. This algorithm worked well for our implementation as the pallet bodies were always convex. An example of excluded points can be seen in figure <a href="#S3.F6" title="Figure 6 ‣ Vertex Blocked Check ‣ Algorithms Overview ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSSx3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Vertex Blocked Check</h5>

<div id="S3.SS1.SSSx3.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSSx3.Px3.p1.1" class="ltx_p">Vertices were determined to be blocked or not through the use of ray casts. A single ray is drawn from the camera in the direction of the vertex. If the ray cast collides with anything that is not a vertex on a pallet, the point is determined to be blocked by something in front of the camera.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2304.03602/assets/images/JarvisMarch.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Convex hull visualisation.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Neural Network</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A base neural network pipeline is required to compare both real-world training data and synthetically generated training data. The chosen pipeline uses Detectron2, with the included COCO Instance Segmentation Mask R-CNN R50-FPN baseline model. This was chosen as it was the most common and easiest to set up to achieve our goal of determining the validity of using synthetic data for machine learning.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Real-world images of pallets were labelled with <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">Label Studio<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_upright">6</span></span><span id="footnote6.5" class="ltx_text ltx_font_upright">https://labelstud.io/</span></span></span></span></span>, and exported to MS COCO format for use in the aforementioned pipeline. The provided CocoEvaulator calculates quantification metrics, including average precision and average recall.
Images were captured with a high-quality camera in good lighting with an image resolution of 4000x3000 pixels each.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">Multiple sets of synthetic data were generated from automated screen capturing of the Unity scene, as shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The average precision (AP) of pallet faces (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="AP_{f}" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">​</mo><msub id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mi id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">P</mi><mi id="S4.p1.1.m1.1.1.3.3" xref="S4.p1.1.m1.1.1.3.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝐴</ci><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2">𝑃</ci><ci id="S4.p1.1.m1.1.1.3.3.cmml" xref="S4.p1.1.m1.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">AP_{f}</annotation></semantics></math>) and bodies (<math id="S4.p1.2.m2.1" class="ltx_Math" alttext="AP_{b}" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">​</mo><msub id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mi id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml">P</mi><mi id="S4.p1.2.m2.1.1.3.3" xref="S4.p1.2.m2.1.1.3.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><times id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"></times><ci id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">𝐴</ci><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.p1.2.m2.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.3.2">𝑃</ci><ci id="S4.p1.2.m2.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">AP_{b}</annotation></semantics></math>) is shown, as well as the overall AP, AP50 and AP75.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the number of images used for the training and testing of each category.
Additionally, the average recall and F1 score for each category can be seen in Table <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="AP_{f}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">​</mo><msub id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml"><mi id="S4.T1.1.1.1.m1.1.1.3.2" xref="S4.T1.1.1.1.m1.1.1.3.2.cmml">P</mi><mi id="S4.T1.1.1.1.m1.1.1.3.3" xref="S4.T1.1.1.1.m1.1.1.3.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><times id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1"></times><ci id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">𝐴</ci><apply id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T1.1.1.1.m1.1.1.3.2">𝑃</ci><ci id="S4.T1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">AP_{f}</annotation></semantics></math></th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="AP_{b}" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mrow id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml"><mi id="S4.T1.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.2.m1.1.1.1" xref="S4.T1.2.2.2.m1.1.1.1.cmml">​</mo><msub id="S4.T1.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.m1.1.1.3.cmml"><mi id="S4.T1.2.2.2.m1.1.1.3.2" xref="S4.T1.2.2.2.m1.1.1.3.2.cmml">P</mi><mi id="S4.T1.2.2.2.m1.1.1.3.3" xref="S4.T1.2.2.2.m1.1.1.3.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1"><times id="S4.T1.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1.1"></times><ci id="S4.T1.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.m1.1.1.2">𝐴</ci><apply id="S4.T1.2.2.2.m1.1.1.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.2.2.2.m1.1.1.3.1.cmml" xref="S4.T1.2.2.2.m1.1.1.3">subscript</csymbol><ci id="S4.T1.2.2.2.m1.1.1.3.2.cmml" xref="S4.T1.2.2.2.m1.1.1.3.2">𝑃</ci><ci id="S4.T1.2.2.2.m1.1.1.3.3.cmml" xref="S4.T1.2.2.2.m1.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">AP_{b}</annotation></semantics></math></th>
<th id="S4.T1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
<th id="S4.T1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP50</th>
<th id="S4.T1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP75</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<th id="S4.T1.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Individual</th>
<td id="S4.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.45</td>
<td id="S4.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.85</td>
<td id="S4.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.65</td>
<td id="S4.T1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.86</td>
<td id="S4.T1.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.64</td>
</tr>
<tr id="S4.T1.2.4.2" class="ltx_tr">
<th id="S4.T1.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Stacked</th>
<td id="S4.T1.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.03</td>
<td id="S4.T1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T1.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T1.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.05</td>
<td id="S4.T1.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
</tr>
<tr id="S4.T1.2.5.3" class="ltx_tr">
<th id="S4.T1.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On</th>
<td id="S4.T1.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.17</td>
<td id="S4.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.05</td>
<td id="S4.T1.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.11</td>
<td id="S4.T1.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.21</td>
<td id="S4.T1.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.10</td>
</tr>
<tr id="S4.T1.2.6.4" class="ltx_tr">
<th id="S4.T1.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr">Racking</th>
<td id="S4.T1.2.6.4.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.6.4.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.6.4.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.6.4.5" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.6.4.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T1.2.7.5" class="ltx_tr">
<th id="S4.T1.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On</th>
<td id="S4.T1.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.21</td>
<td id="S4.T1.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.31</td>
<td id="S4.T1.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.26</td>
<td id="S4.T1.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66</td>
<td id="S4.T1.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.10</td>
</tr>
<tr id="S4.T1.2.8.6" class="ltx_tr">
<th id="S4.T1.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr">Forklifts</th>
<td id="S4.T1.2.8.6.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.8.6.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.8.6.4" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.8.6.5" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.8.6.6" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S4.T1.2.9.7" class="ltx_tr">
<th id="S4.T1.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Combined</th>
<td id="S4.T1.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.03</td>
<td id="S4.T1.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.17</td>
<td id="S4.T1.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.10</td>
<td id="S4.T1.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.13</td>
<td id="S4.T1.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.12</td>
</tr>
<tr id="S4.T1.2.10.8" class="ltx_tr">
<th id="S4.T1.2.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Synthetic</th>
<td id="S4.T1.2.10.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.02</td>
<td id="S4.T1.2.10.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.18</td>
<td id="S4.T1.2.10.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.10</td>
<td id="S4.T1.2.10.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.19</td>
<td id="S4.T1.2.10.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.10</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Classification results (%).</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Testing</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Individual</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">840</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">15</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Stacked</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2100</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On Racking</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2520</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On Forklifts</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1680</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Combined</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7140</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Combined Synthetic</th>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4620</td>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2520</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Data set quantities (number of images).</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Recall</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt">Individual</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.746</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.694</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Stacked</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.066</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.047</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On Racking</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.187</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.139</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">On Forklifts</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.386</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.314</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Combined</th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.121</td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.110</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<th id="S4.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Combined Synthetic</th>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.184</td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.130</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average recall and F1 score.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">As Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows, a model trained on synthetically generated pallets placed by themselves in a warehouse environment, and tested against real-world images of the same, performed the best, yielding an AP50 of 86%. With an AP50 of 66%, pallets on forklifts perform well. Stacked pallets and pallets on racking perform poorly in every metric, even AP50. Additionally, test data sets are very small compared to their training data set in certain scenarios. This is due to the limitations of the real images that could be acquired in the given time frame, where only a small portion of them fit the exact categories of interest.
The model, trained on synthetic data, cannot be expected to perform better on real images than on a test set of rendered images. For this reason, in addition to testing the neural network on real images, an overall test was done on a separate synthetic test data set containing all the categories. As can be seen from the results table in <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this did not perform very well, so expectations for the network’s performance on real data were not very high. Synthetic detection rates could be improved if they were split into real data categories, as this has seen a noticeable uplift in detection performance. Some of the results of predictions generated by the neural networks can be seen below in Figures <a href="#S4.F7" title="Figure 7 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <a href="#S4.F8" title="Figure 8 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <a href="#S4.F9" title="Figure 9 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, and <a href="#S4.F10" title="Figure 10 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2304.03602/assets/images/CorrectPalletForklift.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Pallet detection on real forklift.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2304.03602/assets/images/GroundSplodge.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="241" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>False detection of face on ground.</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2304.03602/assets/images/GoodSyntheticDetection.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="235" height="119" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Good synthetic data test result.</figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2304.03602/assets/images/BadSyntheticDetection.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="235" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Bad synthetic data test result.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The selected pallet detection method differs from other methods explored in the literature review. This is due to the fact that other methods used machine learning with other sensors or omitted the use of machine learning and instead used markers or image manipulation.
As seen in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the results obtained from the neural network varied vastly depending on the scenarios in which they were trained. Individual pallets yield an average precision of 64.9%, increasing to 85.7% when looking at the AP50 metric. While these results are good, it can be seen that the precision drops heavily once other scenarios are added into the scenes, such as staked, racking and forklift pallets. The Recall and F1 results in Table <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> confirm this trend, with 75% and 69% respectively for individual pallets, dropping substantially for the other scenarios.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">One of the major drawbacks identified when using synthetic data for machine learning training was the lack of realism between rendered and real images. This is still a large concern when analysing the results of the neural network here. While the pallets that are used within Unity are very similar to that in the rendering, they are nowhere near identical. This could lead to substantial issues when detecting based on models trained on them. The neural network struggled most with the stacked pallets section. This is thought to be because the features that are available on the label are very similar and are in very close proximity to each other. For example, when the pallets are stacked, only the pallet faces are visible and not the pallet bodies. This results in many pallet face annotations right next to each other, which often confuses the neural network into labelling the entire pallet stack as one pallet body or one pallet face. An example of this can be seen in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 Data Generation ‣ 3 Methods ‣ Pallet Detection from Synthetic Data using Game Engines" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">As mentioned in the results section above, the test data sets for certain categories are very small, while others are substantially larger. This is an additional concern with the accuracy of the trained model, as there may not be enough images in the testing data sets for accurate and meaningful results to be obtained.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Additionally, there is slightly more room to tune the training parameters within Detectron2; this could lead to slightly higher detection rates within the network.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">As is made clear by the results obtained, the viability of using synthetically generated data for simple pallet detection is proven.
With an AP50 of 86% for individual pallets, there is clearly a path forward for industrial use, reducing manual data annotation requirements. However, there is much more work to be done in improving the classification accuracy for scenes in which the pallets are placed in more complex configurations, including being stacked, carrying loads, and obscured behind other objects. Testing data sets could also be improved to provide a more balanced overview of the performance of the neural networks by increasing the number of images inside each category measured.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Due to work being limited by time constraints, it was not possible to write truly comprehensive annotation algorithms for Unity. This functionality can be extended to avoid discarding annotations for a face or body in which any of its edge vertices are blocked by a solid object (pallet, racking, etc.).</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">There is also the possibility for additional neural network models to be explored rather than just Mask R-CNN, such as the YOLO family, or Swin-L, which performed the best on the MS COCO data set. This could lead to the yield of higher detection rates.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In addition, pallets with loads ought to be accounted for and tested accordingly. Varying models of pallets can also be used, as well as wrapped and otherwise modified pallets. Finally, pinpointing pallet engagement points can be useful for the development of autonomous lifting trucks.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Thanks to Crown Equipment Corporation for sponsoring this research project and to their representatives Sian Phillips and Abigail Birkin-Hall, for providing the real-world image data set and other invaluable assistance.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Thanks also to Mahla Nejati and Trevor Gee, for supervising the project and providing much-needed insight.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx1.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Bochkovskiy <span id="bib.bibx1.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx1.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov4: Optimal speed and accuracy of object detection, 2020.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx2.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Chen <span id="bib.bibx2.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2012<span id="bib.bibx2.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Gang Chen, Rui Peng, Zhicheng Wang, and Weidong Zhao.

</span>
<span class="ltx_bibblock">Pallet recognition and localization method for vision guided
forklift.

</span>
<span class="ltx_bibblock">2012.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx3.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Contributors, 2018<span id="bib.bibx3.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
MMDetection Contributors.

</span>
<span class="ltx_bibblock">Openmmlab detection toolbox and benchmark, 4 2018.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx4.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Cui <span id="bib.bibx4.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2010<span id="bib.bibx4.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Guang Zhao Cui, Lin Sha Lu, Zhen Dong He, Li Na Yao, Cun Xiang Yang, Bu Yi
Huang, and Zhi Hong Hu.

</span>
<span class="ltx_bibblock">A robust autonomous mobile forklift pallet recognition.

</span>
<span class="ltx_bibblock">volume 3, 2010.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx5.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Falcao <span id="bib.bibx5.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2021<span id="bib.bibx5.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Joao Diogo Falcao, Prabh Simran S Baweja, Yi Wang, Akkarit Sangpetch, Hae Young
Noh, Orathai Sangpetch, and Pei Zhang.

</span>
<span class="ltx_bibblock">Piwims: Physics informed warehouse inventory monitory via synthetic
data generation.

</span>
<span class="ltx_bibblock">pages 613–618. ACM, 9 2021.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx6.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Fogel <span id="bib.bibx6.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2007<span id="bib.bibx6.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Menasheh Fogel, Nathan Burkhart, Hongliang Ren, Jeremy Schift, Max Meng, and
Ken Goldberg.

</span>
<span class="ltx_bibblock">Automated tracking of pallets in warehouses: Beacon layout and
asymmetric ultrasound observation models.

</span>
<span class="ltx_bibblock">2007.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx7.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Garibotto <span id="bib.bibx7.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 1996<span id="bib.bibx7.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Giovanni Garibotto, Stefano Masciangelo, Marco Ilic, and Paolo Bassino.

</span>
<span class="ltx_bibblock">Robolift: a vision guided autonomous fork-lift for pallet handling.

</span>
<span class="ltx_bibblock">volume 2, 1996.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx8.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Girshick <span id="bib.bibx8.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx8.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Rich feature hierarchies for accurate object detection and semantic
segmentation.

</span>
<span class="ltx_bibblock">2014.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx9.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Girshick, 2015<span id="bib.bibx9.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ross Girshick.

</span>
<span class="ltx_bibblock">Fast r-cnn.

</span>
<span class="ltx_bibblock">pages 1440–1448. IEEE, 12 2015.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx10.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>He <span id="bib.bibx10.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2010<span id="bib.bibx10.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Zhendong He, Xinjin Wan, Jie Liu, Junman Sun, and Guangzhao Cui.

</span>
<span class="ltx_bibblock">Feature-to-feature based laser scan matching for pallet recognition.

</span>
<span class="ltx_bibblock">volume 2, 2010.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx11.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>He <span id="bib.bibx11.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx11.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">volume 2016-December, pages 770–778. IEEE Computer Society, 12 2016.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx12.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>He <span id="bib.bibx12.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx12.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
42, 2020.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx13.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Hittmeir <span id="bib.bibx13.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx13.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Markus Hittmeir, Andreas Ekelhart, and Rudolf Mayer.

</span>
<span class="ltx_bibblock">On the utility of synthetic data: An empirical evaluation on machine
learning tasks.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx14.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Jeon <span id="bib.bibx14.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2010<span id="bib.bibx14.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Seungwoo Jeon, Mikyung Choi, Gihong Kim, and Bonghee Hong.

</span>
<span class="ltx_bibblock">Localization of pallets based on passive rfid tags.

</span>
<span class="ltx_bibblock">pages 834–839. IEEE, 2010.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx15.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Jia <span id="bib.bibx15.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2021<span id="bib.bibx15.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Fengyuan Jia, Zhaosheng Tao, and Fusong Wang.

</span>
<span class="ltx_bibblock">Pallet detection based on halcon for warehouse robots.

</span>
<span class="ltx_bibblock">pages 401–404. Institute of Electrical and Electronics Engineers
Inc., 5 2021.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx16.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kuchin <span id="bib.bibx16.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx16.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Yan I. Kuchin, Ravil I. Mukhamediev, and Kirill O. Yakunin.

</span>
<span class="ltx_bibblock">One method of generating synthetic data to assess the upper limit of
machine learning algorithms performance.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.6.1" class="ltx_text ltx_font_italic">Cogent Engineering</span>, 7:1718821, 1 2020.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx17.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Kuznetsova <span id="bib.bibx17.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx17.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, Tom Duerig, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.6.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 128, 2020.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx18.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Le <span id="bib.bibx18.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx18.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Tuan Anh Le, Atilim Giines Baydin, Robert Zinkov, and Frank Wood.

</span>
<span class="ltx_bibblock">Using synthetic data to train neural networks is model-based
reasoning.

</span>
<span class="ltx_bibblock">volume 2017-May, pages 3514–3521. Institute of Electrical and
Electronics Engineers Inc., 6 2017.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx19.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lecking <span id="bib.bibx19.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2006<span id="bib.bibx19.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Daniel Lecking, Oliver Wulf, and Bernardo Wagner.

</span>
<span class="ltx_bibblock">Variable pallet pick-up for automatic guided vehicles in industrial
environments.

</span>
<span class="ltx_bibblock">2006.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx20.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Li <span id="bib.bibx20.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx20.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Qiang Li, Ranyang Li, Kaifan Ji, and Wei Dai.

</span>
<span class="ltx_bibblock">Kalman filter and its application.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx21.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Li <span id="bib.bibx21.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx21.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Tianjian Li, Bin Huang, Chang Li, and Min Huang.

</span>
<span class="ltx_bibblock">Application of convolution neural network object detection algorithm
in logistics warehouse.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.6.1" class="ltx_text ltx_font_italic">The Journal of Engineering</span>, 2019:9053–9058, 12 2019.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx22.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Lin <span id="bib.bibx22.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2014<span id="bib.bibx22.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Tsung Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">volume 8693 LNCS, 2014.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx23.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Liu <span id="bib.bibx23.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx23.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C. Berg.

</span>
<span class="ltx_bibblock">Ssd: Single shot multibox detector, 2016.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx24.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Liu <span id="bib.bibx24.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2021<span id="bib.bibx24.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">pages 9992–10002, 2021.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx25.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Mohamed <span id="bib.bibx25.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx25.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ihab S. Mohamed, Alessio Capitanelli, Fulvio Mastrogiovanni, Stefano Rovetta,
and Renato Zaccaria.

</span>
<span class="ltx_bibblock">Detection, localisation and tracking of pallets using machine
learning techniques and 2d range data.

</span>
<span class="ltx_bibblock"><span id="bib.bibx25.6.1" class="ltx_text ltx_font_italic">Neural Computing and Applications</span>, 32:8811–8828, 7 2020.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx26.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Nejati <span id="bib.bibx26.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx26.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Mahla Nejati, Nicky Penhall, Henry Williams, Jamie Bell, Jong Yoon Lim, Ho Seok
Ahn, and Bruce MacDonald.

</span>
<span class="ltx_bibblock">Kiwifruit detection in challenging conditions.

</span>
<span class="ltx_bibblock"><span id="bib.bibx26.6.1" class="ltx_text ltx_font_italic">Australasian Conference on Robotics and Automation, ACRA</span>,
2019-Decem, 2019.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx27.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Pagès <span id="bib.bibx27.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2001<span id="bib.bibx27.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
J. Pagès, X. Armangué, J. Salvi, J. Freixenet, and J. Martí.

</span>
<span class="ltx_bibblock">A computer vision system for autonomous forklift vehicles in
industrial environments.

</span>
<span class="ltx_bibblock">2001.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx28.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Patki <span id="bib.bibx28.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx28.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Neha Patki, Roy Wedge, and Kalyan Veeramachaneni.

</span>
<span class="ltx_bibblock">The synthetic data vault.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx29.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ping <span id="bib.bibx29.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx29.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Haoyue Ping, Julia Stoyanovich, and Bill Howe.

</span>
<span class="ltx_bibblock">Datasynthesizer: Privacy-preserving synthetic datasets.

</span>
<span class="ltx_bibblock">volume Part F128636, 2017.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx30.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Preparata and
Shamos, 2012<span id="bib.bibx30.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Franco P Preparata and Michael I Shamos.

</span>
<span class="ltx_bibblock"><span id="bib.bibx30.3.1" class="ltx_text ltx_font_italic">Computational geometry: an introduction</span>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2012.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx31.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Redmon <span id="bib.bibx31.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2016<span id="bib.bibx31.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">volume 2016-December, pages 779–788. IEEE Computer Society, 12 2016.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx32.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Ren <span id="bib.bibx32.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx32.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bibx32.6.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
39:1137–1149, 6 2017.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx33.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Seelinger and Yoder, 2005<span id="bib.bibx33.2.2.2" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Michael Seelinger and John David Yoder.

</span>
<span class="ltx_bibblock">Automatic pallet engagment by a vision guided forklift.

</span>
<span class="ltx_bibblock">volume 2005, pages 4068–4073, 2005.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx34.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Syu <span id="bib.bibx34.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx34.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Jia Liang Syu, Hsin Ting Li, Jen Shiun Chiang, Chih Hsien Hsia, Po Han Wu,
Chi Fang Hsieh, and Shih An Li.

</span>
<span class="ltx_bibblock">A computer vision assisted system for autonomous forklift vehicles in
real factory environment.

</span>
<span class="ltx_bibblock"><span id="bib.bibx34.6.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, 76:18387–18407, 9 2017.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx35.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Vaswani <span id="bib.bibx35.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2017<span id="bib.bibx35.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">volume 2017-December, 2017.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx36.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Williams <span id="bib.bibx36.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx36.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Henry Williams, Mahla Nejati, Salome Hussein, Nicky Penhall, Jong Yoon Lim,
Mark Hedley Jones, Jamie Bell, Ho Seok Ahn, Stuart Bradley, Peter Schaare,
Paul Martinsen, Mohammad Alomar, Purak Patel, Matthew Seabright, Mike Duke,
Alistair Scarfe, and Bruce MacDonald.

</span>
<span class="ltx_bibblock">Autonomous pollination of individual kiwifruit flowers: Toward a
robotic kiwifruit pollinator.

</span>
<span class="ltx_bibblock"><span id="bib.bibx36.6.1" class="ltx_text ltx_font_italic">Journal of Field Robotics</span>, (August 2018), 2019.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx37.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Wu <span id="bib.bibx37.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2019<span id="bib.bibx37.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.

</span>
<span class="ltx_bibblock">Detectron2, 2019.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx38.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zaccaria <span id="bib.bibx38.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2020<span id="bib.bibx38.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Michela Zaccaria, Riccardo Monica, and Jacopo Aleotti.

</span>
<span class="ltx_bibblock">A comparison of deep learning models for pallet detection in
industrial warehouses.

</span>
<span class="ltx_bibblock">pages 417–422. Institute of Electrical and Electronics Engineers
Inc., 9 2020.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem"><span id="bib.bibx39.1.1.1" class="ltx_text" style="position:relative; bottom:0.9pt;">[</span>Zaidi <span id="bib.bibx39.2.2.2" class="ltx_text ltx_font_italic">et al.</span>, 2022<span id="bib.bibx39.3.3.3" class="ltx_text" style="position:relative; bottom:0.9pt;">]</span> </span>
<span class="ltx_bibblock">
Syed Sahil Abbas Zaidi, Mohammad Samar Ansari, Asra Aslam, Nadia Kanwal,
Mamoona Asghar, and Brian Lee.

</span>
<span class="ltx_bibblock">A survey of modern deep learning based object detection models.

</span>
<span class="ltx_bibblock"><span id="bib.bibx39.6.1" class="ltx_text ltx_font_italic">Digital Signal Processing</span>, 126:103514, 6 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.03601" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.03602" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.03602">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.03602" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.03603" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 16:01:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
