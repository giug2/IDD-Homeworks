<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models</title>
<!--Generated on Fri Oct  4 03:29:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02429v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S1" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S2" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S2.SS0.SSS0.Px1" title="In 2 Related work ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">ML/DL methods in IoT tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S2.SS0.SSS0.Px2" title="In 2 Related work ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">LLMs in IoT tasks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.SS1" title="In 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>IoT data simplification and enrichment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.SS1.SSS0.Px1" title="In 3.1 IoT data simplification and enrichment ‣ 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">IoT data simplification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.SS1.SSS0.Px2" title="In 3.1 IoT data simplification and enrichment ‣ 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">IoT data enrichment.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.SS2" title="In 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>IoT-oriented Knowledge Retrieval Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.SS3" title="In 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Prompt Configuration</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1" title="In 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>A benchmark on IoT task reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS1" title="In 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>IoT tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2" title="In 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>IoT datasets.</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2.Px1" title="In 4.1.2 IoT datasets. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Human Activity Recognition.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2.Px2" title="In 4.1.2 IoT datasets. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Industrial anomaly detection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2.Px3" title="In 4.1.2 IoT datasets. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Heartbeat anomaly detection.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2.Px4" title="In 4.1.2 IoT datasets. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Human sensing task.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS2.Px5" title="In 4.1.2 IoT datasets. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Indoor localization task.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS1.SSS3" title="In 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>LLM Baselines.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS2" title="In 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>results and analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS2.SSS0.Px1" title="In 4.2 results and analysis ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">LLMs excel in various IoT tasks but struggle with complex data challenge.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS2.SSS0.Px2" title="In 4.2 results and analysis ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">LLMs are excellent learners in IoT task reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS2.SSS0.Px3" title="In 4.2 results and analysis ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">LLMs can act as experts, not just classifiers or predictors.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.SS3" title="In 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>ablation study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S5" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S5.SS0.SSS0.Px1" title="In 5 conclusion ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title">Limitations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A1" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Response examples of LLMs for IoT tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2" title="In IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Prompt template</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tuo An<sup class="ltx_sup" id="id7.7.id1"><span class="ltx_text ltx_font_italic" id="id7.7.id1.1">1,2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,  Yunjiao Zhou<sup class="ltx_sup" id="id8.8.id2"><span class="ltx_text ltx_font_italic" id="id8.8.id2.1">1</span></sup>,  Han Zou<sup class="ltx_sup" id="id9.9.id3">1</sup>,  Jianfei Yang<sup class="ltx_sup" id="id10.10.id4">1</sup>.

<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id5"><span class="ltx_text ltx_font_italic" id="id11.11.id5.1">1</span></sup>Nanyang Technological University  <sup class="ltx_sup" id="id12.12.id6"><span class="ltx_text ltx_font_italic" id="id12.12.id6.1">2</span></sup>Nanjing University
</span><span class="ltx_author_notes">This work was conducted during the author’s research internship at MARS Lab, NTU.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id13.id1">Large Language Models (LLMs) have demonstrated remarkable capabilities across textual and visual domains but often generate outputs that violate physical laws, revealing a gap in their understanding of the physical world. Inspired by human cognition—where perception is fundamental to reasoning—we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT task reasoning in the physical world.
In this work, we systematically study LLMs’ capability to address real-world IoT tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability.
In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions, and expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning. To evaluate the performance, We design a new benchmark with five real-world IoT tasks with different data types and reasoning difficulties and provide the benchmarking results on six open-source and close-source LLMs. Experimental results demonstrate the limitations of existing LLMs with naive textual inputs that cannot perform these tasks effectively. We show that IoT-LLM significantly enhances the performance of IoT tasks reasoning of LLM, such as GPT-4, achieving an average improvement of 65% across various tasks against previous methods. The results also showcase LLMs’ ability to comprehend IoT data and the physical law behind data by providing a reasoning process. Limitations of our work are claimed to inspire future research in this new era.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advancements in large generative models have showcased their exceptional performance and versatility in handling complex tasks across textual and visual domains, as evidenced by the GPT series <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2018improving</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">radford2019language</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">brown2020language</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">achiam2023gpt</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">openai2023gpt</span>)</cite> and visual generation models <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Dosovitskiy2020AnII</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Liu2021SwinTH</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Ho2020DenoisingDP</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">peebles2023scalable</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">blattmann2023stable</span>)</cite>. However, these models could occasionally generate outputs that are physically implausible, often referred to as “hallucinations” <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">alkaissi2023artificial</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2023survey</span>)</cite>. Even advanced video generation models, e.g., Sora <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">videoworldsimulators2024</span>)</cite>, are susceptible to producing animations that contravene fundamental physical laws, such as a video clip containing a tipping water glass that appears to defy gravity. These observations suggest that generative models may not really comprehend and apply physical laws of the physical world as accurately as humans when acting as world simulators. This has renewed interest in research on the <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">World Model</span> that focuses on understanding and modeling the physical world in a brain-like manner <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">dawid2023introduction</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">garrido2024learning</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">mendonca2023structured</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2024world</span>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Unlike Large Language Models (LLMs) that map descriptions of the physical world to a latent space and perform reasoning by predicting the text sequence according to the probability, research on human cognitive science illustrates a different mechanism. The human brain comprises multiple mutually-functional areas, of which the important components include the temporal and occipital lobes for perception, and the frontal cortex for reasoning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Churchland1988PerspectivesOC</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Saxe2009BrainRF</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Hobeika2016GeneralAS</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Grzes2001DoesPO</span>)</cite>.
Notably, perception is the primary mechanism through which information about the physical world is acquired, and then effective reasoning is inherently dependent on accurate perception. However, in LLMs, the physical world is only “perceived” through natural language, i.e., concepts and words in the semantic space, which denotes an indirect representation and abstraction of the physical world. A recent study in Nature shows language is primarily a tool for communication rather than thought <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">fedorenko2024language</span>)</cite>, so reasoning the physical-world problem with only language is limited. To enable LLMs with better reasoning capability in the real world, perception is highly demanded. Recent research on Vison Language Models (VLMs) builds the connection between visual perception and languages <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang2024vision</span>)</cite>, yet the vision is only one of the various perceptual modalities. Many aspects of the physical world are still not perceived by existing LLMs.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S1.F1.1.g1" src="ICLR%202025%20Template/brain_final.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Inspired by human cognitive science, we augment LLMs with physical world perception from IoT data. Furthermore, by retrieving pertinent knowledge about IoT tasks, we enhance the reasoning capabilities of LLMs in executing real-world applications.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We draw inspiration from how humans understand the physical world: perception to acquire information and reasoning with relevant domain knowledge. Firstly, humans perceive the world via a multitude of sensory organs, such as eyes for sight and ears for hearing. To empower machine perception, Internet of Things (IoT) sensors are developed. Since the first IoT sensor was designed for Coke machines to count the number of bottles in the 1980s <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">madakam2015internet</span>)</cite>, IoT sensors become the “sensory organs” of machines, modeling the physical world for machine automation.
Secondly, humans understand the world via the perception data with domain knowledge gained from experience and education. Similarly, LLMs can learn domain knowledge of both the physical world and sensors from the context to have stronger reasoning capabilities by in-context learning. In this manner, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, we believe perception data with pertinent knowledge can enable LLMs to address complex problems with IoT-enabled perception in the real world. In this work, we aim to explore the following questions: (1) What types of real-world tasks can LLMs perform via the IoT perception of the physical world? (2) How can we enhance the LLM capability to deal with real-world tasks? (3) Do LLMs truly understand perception data and apply knowledge to realize real-world tasks?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Previous studies have primarily shown the viability of using LLMs for IoT task reasoning <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2024penetrative</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2024hargpt</span></cite>, but we find that these studies are not carefully scrutinized. (1) These studies only focus on specific tasks, such as R-peak identification and action recognition. The choices of tasks are not comprehensive, and thus they lack a benchmark to evaluate the performances of the methods. (2) They directly input raw IoT data into LLMs for reasoning, but LLMs are not good at dense numerical data and calculation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou2024larger</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gruver2024large</span>)</cite>.
(3) They only evaluate their effectiveness on close-source LLMs, and lack a comprehensive study of benchmarking open-source LLMs with different parameter size.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To bridge this gap and answer the questions we proposed, we conduct an in-depth investigation of how to utilize LLMs to perform various tasks in the physical world using IoT data. Firstly, we explore whether LLMs can solve IoT classification and regression problems by setting a new benchmark with five classic IoT tasks with different data and levels of difficulties, including human activity recognition, industrial anomaly detection, heartbeat anomaly detection, WiFi-based human sensing, and indoor localization. The benchmark covers scenarios of daily life, industrial applications, and medical care, which will be detailed in the experiments.
Secondly, we enhance LLMs’ reasoning capabilities with IoT data through three novel steps and consolidate three steps into IoT-LLM, a unified framework for IoT task reasoning.
It is composed of three steps tailored for IoT reasoning: designing an LLM-friendly data format, activating knowledge by chain-of-thought prompting, and automatic IoT-oriented Retrieval-Augmented Generation (RAG) based on LLMs’ in-context learning capability.
Thirdly, to determine whether LLMs truly understand and then solve the task, we have LLMs generate analytical processes and analyze the reasonableness of the analytics. The analysis generated by IoT-LLM indicates that LLMs can provide a reasonable process of solving simple tasks, but their efficacy diminishes in more specialized domains like heartbeat anomaly detection. This performance disparity is attributable to the complexity of data and limited domain-specific knowledge inherent in LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We systematically study how Large Language Models (LLMs) can address real-world problems by perceiving the physical world via IoT sensor data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a unified framework to address IoT-related real-world problems, which enhances the capability of LLMs through three steps: IoT data simplification and enrichment, IoT-oriented knowledge retrieval, and prompt configuration. To the best of our knowledge, this is the first unified framework for IoT-related tasks in the physical world.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We establish the first benchmark for IoT task reasoning, including five real-world tasks with various types of IoT data. We benchmark both open-source and close-source LLMs with different parameter size. Empirical results show that our IoT-LLM significantly improves the performances of all base LLMs on IoT tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">ML/DL methods in IoT tasks.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">The Internet of Things (IoT) sensors gather diverse data from the real world, such as tri-axial acceleration, electrocardiogram readings, WiFi signals, and pressure <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">sehrawat2019smart</span>)</cite>. These data have empowered various human sensing tasks, including Human Activity Recognition (HAR) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Lara2013ASO</span>)</cite>, health monitoring like heartbeat and respiration anomaly detection <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Mousavi2018InterAI</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Aytekin2022COVID19DF</span>)</cite>, and industrial applications such as machine operational state monitoring <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Kong2023IntegratedGM</span>)</cite>. Currently, these IoT data are primarily processed using traditional machine learning techniques, such as Support Vector Machines (SVM) and K-Nearest Neighbors (KNN) Algorithm <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">alam2016analysis</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Luo2021WiFibasedIL</span>)</cite>, or deep learning methods <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Li2021TwoStreamCA</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">njima2019deep</span>)</cite>. These approaches build black-box predictors for specific tasks, yet each predictor only supports one task, and the task cannot be addressed with reasoning analysis, which motivates us to explore LLM for IoT tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">LLMs in IoT tasks.</h5>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Existing literature on Large Language Models (LLMs) in IoT mainly regards LLM as a user interface or as coordinators in smart machines <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2023chatiot</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">cui2023llmind</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">du2023space</span>)</cite>. However, in these studies, LLMs function as intermediaries and do not directly interpret IoT data to perform real-world tasks. Recent studies, such as Penetrative AI <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu-etal-2024-penetrative</span>)</cite> and HarGPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2024hargpt</span>)</cite>, have begun integrating IoT data into LLMs for specific tasks, leveraging their inherent knowledge bases. Despite these advancements, the exploration of LLMs processing IoT data remains nascent. Penetrative AI converts IoT data into textual and numerical formats for basic tasks like R-peak identification in ECG data, heavily relying on manually crafted expert knowledge, which limits automation and scalability. Similarly, HarGPT processes raw IMU data to recognize human activities using a chain of thought technique but is restricted to this specific data type and task, not demonstrating the broader applicability of LLMs. While these studies provide initial insights into using LLMs in the IoT domain, they do not offer a comprehensive framework that fully exploits LLM capabilities or systematically explores the interaction between LLMs and the physical world through IoT devices, which is the primary focus of our work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S3.F2.1.g1" src="ICLR%202025%20Template/overall_pipeline_final.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>In our framework, IoT data is initially preprocessed to create a data description. Next, relevant IoT domain knowledge and task-specific demonstrations are retrieved. These elements are then combined into a prompt, which is input into a LLM to generate the final output.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we define the problem for IoT task reasoning with LLM and introduce our research methodology.
The formulated research problem is how to leverage LLM and in-context learning for task reasoning for IoT data, termed as <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">IoT task reasoning</span>, e.g., using accelerators data for activity recognition or machine sensor for anomaly detection. The prompt for LLM should include two parts: data, as a way to perceive the physical world, and the task description, such as “Is it a Normal heartbeat (N) or Premature ventricular contraction beat (V)?”, serves as the query. To evaluate the performance of IoT reasoning task, we build a new benchmark including 5 real-world tasks with different IoT data types and difficulty levels, encompassing both classification and regression problems.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">At first, we employ LLMs to execute IoT tasks in a basic setting, similar to the existing approaches <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2024hargpt</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2024penetrative</span>)</cite>, where the prompt provided to the LLMs includes only raw IoT data and the associated query. However, the performance of LLMs remains suboptimal. As shown by the baseline results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.T2" title="Table 2 ‣ 4.1.1 IoT tasks. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, even GPT-4 only achieves an accuracy of 43% for 3-way activity recognition and 50% for machine diagnosis based on their approach. These preliminary results akin to near-random guessing suggest a lack of comprehension of IoT data and tasks by this naive prompting way.
Upon analyzing the characteristics of IoT data and real-world tasks, we identify that the challenges stem from the abstraction of dense numeric data and the lack of domain knowledge within LLMs. To address these challenges, we propose a unified framework (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S3.F2" title="Figure 2 ‣ 3 Methodology ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>) consisting of three key stages: (1) IoT data simplification and enrichment, (2) IoT-oriented knowledge augmentation, and (3) prompt configuration. Each stage addresses specific difficulties encountered by LLMs for IoT task reasoning, and we introduce each stage one by one.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>IoT data simplification and enrichment</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Unlike textual human tasks that have been learned by LLMs, IoT data for IoT task reasoning presents unique challenges that hinder LLMs’ comprehension. Firstly, IoT data encompasses a diverse range of types and forms, many of which are complex time-series data (e.g., electrocardiogram readings) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Goldberger2000</span>)</cite> or multi-variant data (e.g., WiFi CSI) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">yang2024mm</span>)</cite>. LLMs often struggle with accurately interpreting dense numerical data, especially when it involves long-sequence time-series data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Zhang2024LargeLM</span>)</cite>. Secondly, IoT data is typically composed of raw numerical values. This data often lacks essential textual annotations, such as units of measurement and metadata about the data collection process, which are critical for LLMs to interpret effectively in real-world applications. In summary, raw IoT data requires (1) appropriate simplification and (2) information enrichment. Previous studies <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2024penetrative</span>)</cite> have employed down-sampling techniques for time-series data but they only achieve coarse-grained simplification at a length level without enhancing the informational content of the IoT data. In contrast, we not only simplify IoT data at the token level but also enrich the IoT data by providing additional information to facilitate better understanding by LLMs (as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2.F10" title="Figure 10 ‣ Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">10</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2" title="Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">B</span></a>). In this way, we transform complex raw IoT data into an LLM-friendly format for IoT task reasoning.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">IoT data simplification.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">To achieve effective simplification, it is crucial to understand why LLMs struggle with dense numeric data. Firstly, according to recent research <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">gruver2024large</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">Spathis2023TheFS</span>)</cite>, tokenization methods, such as Byte Pair Encoding (BPE) often fragment numbers into tokens that do not align with their digits, resulting in inconsistent tokenization of floating-point numbers and complicating arithmetic operations. Therefore, in addition to down-sampling and keeping fixed precision (e.g., two digits of precision) to efficiently manage context length, we propose to insert spaces between digits to ensure distinct tokenization of each digit and use a comma (“,”) to separate each time step in a time series. Secondly, the complexity of long-sequence IoT data poses significant challenges for LLMs in analysis. To assist LLMs in processing this data, we extract essential statistical features, e.g., mean, variance, and FFT mean, utilizing external tools, such as Python scripts. We find that these fundamental features are strong enough for IoT task reasoning in classic IoT tasks.
By doing so, we not only simplify IoT data at both length and token levels but also transform it into a format that is more suitable for tokenization and processing by LLMs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">IoT data enrichment.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">As previously noted, IoT data alone is insufficient for LLMs to effectively perform real-world tasks. To address this, we enrich the data by incorporating contextual information about the physical world. Specifically, we provide a comprehensive overview of IoT data collection and the integration of physical information. For instance, in human activity recognition (HAR) tasks where we employ inertial measurement unit (IMU) data including triaxial acceleration and angular velocity from accelerometers and gyroscopes, we meticulously outline the data collection process, incorporating the metadata such as sampling frequency (e.g., 10 Hz), device placement on the body, and units of measurement (e.g., gravitational acceleration and radians per second). This approach enables LLMs to not only align the three-axis IMU data with the corresponding three-dimensional spatial orientations of the human body but also to understand the physical significance of these numerical values, thereby enhancing the comprehension of LLMs for the task in the physical world.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>IoT-oriented Knowledge Retrieval Augmentation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In IoT task reasoning, the knowledge of LLMs to perform IoT tasks is significant. For example, detecting abnormal heartbeats from electrocardiogram (ECG) data requires interpreting ECG signals and associating them with specific heartbeat states (e.g., normal, premature ventricular contraction), necessitating specialized domain knowledge. Although previous research <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xu2024penetrative</span>)</cite> proposes to include specific expert knowledge for specific tasks, the augmentation is task-specific and added manually, which is time-consuming and not scalable. To address this, we enable LLMs with IoT knowledge in an automatic fashion. Inspired by the in-context learning capability of LLMs, we also retrieve task-specific demonstrations, such as question-answer pairs, to guide LLMs in effectively utilizing IoT data for analyzing IoT tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We first construct an IoT domain knowledge base and a demonstration knowledge base, which will be utilized for retrieving domain knowledge and task-specific demonstrations. To ensure comprehensive coverage of knowledge about IoT data and tasks within the IoT domain knowledge base, we gather relevant documents (e.g., Wikipedia articles, research papers) through web searches encompassing the following themes: (1) IoT data domain knowledge, (2) IoT task domain knowledge, and (3) expert insights on leveraging IoT data for task execution. For the demonstration knowledge base, we create task-specific demonstrations (i.e., question-answer pairs) authored by human or AI models (e.g., ChatGPT).
We then employ an embedding model (e.g., text-embedding-ada-002<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/embeddings" title="">https://platform.openai.com/docs/guides/embeddings</a></span></span></span> by OpenAI) to embed texts into vectors and store the text chunks and corresponding embeddings as key-value pairs, which allows for efficient and scalable search capabilities. To improve the quality of retrieved contents, we also store metadata (e.g., IoT data type for IoT domain knowledge base and task type for demonstration knowledge base) alongside the vector embeddings within the vector database. This approach allows for advanced post-processing techniques, such as metadata filtering <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Poliakov2024MultiMetaRAGIR</span>)</cite>, to refine search results and improve task-specific retrieval accuracy.
Secondly, we retrieve relevant knowledge using both IoT data description and task description as query. We adopt a hybrid search method, which means utilizing both keyword-based retrievers and embedding-based retrievers to harness their unique strengths, ensuring the consistent retrieval of highly relevant and context-rich information. Finally, after applying a re-ranking technique to recalibrate the similarity between the query and retrieved texts using ranker models (e.g. bge-reranker-base<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/BAAI/bge-reranker-base" title="">https://huggingface.co/BAAI/bge-reranker-base</a></span></span></span>), we filter out the top-m most relevant pieces, thus obtaining pertinent knowledge, encompassing documents with specific domain knowledge and task demonstrations relevant to the task at hand.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Prompt Configuration</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In addition to augmenting LLMs’ knowledge by providing external documents in the context utilizing the in-context learning capability of LLMs, we further invoke LLMs’ internal knowledge by carefully configuring the prompt. Recent studies demonstrate that LLMs possess strong role-playing capabilities <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">park2023generative</span>)</cite>. To leverage this, we assign specific roles to LLMs for particular tasks. For instance, we have LLMs assume the role of a professional doctor when performing heartbeat anomaly detection, thereby activating their internal domain knowledge. What’s more, since LLMs’ reasoning capability can be improved a lot by decomposing the whole problem into several parts <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Wei2022ChainOT</span>)</cite>, we decompose the reasoning procedure into two steps, prompting LLMs to analyze the IoT data and task first, and then provide the final answer based on this analysis. By doing so, we can also evaluate the extent to which the LLM understands IoT data and its capability to perform IoT tasks through the generated analysis. In the end, we employ a prompt template (refer to Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2.F8" title="Figure 8 ‣ Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2" title="Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">B</span></a>) to structure the content discussed previously. The ultimate prompt is crafted based on the template and subsequently fed into a downstream LLM. The LLM then produces the final output, encompassing both analysis and answer to the specified task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>A benchmark on IoT task reasoning</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>IoT tasks.</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">To comprehensively assess the capability boundaries of LLMs for IoT task reasoning, we develop a new benchmark comprising five real-world tasks with diverse IoT data types and difficulty levels: (1) Human Activity Recognition (HAR) using Inertial Measurement Unit (IMU) data, (2) Industrial anomaly detection using metrics such as temperature, cooling power, and cooling efficiency, (3) Heartbeat anomaly detection using Electrocardiogram (ECG) data, (4) Human sensing using WiFi Channel State Information (CSI), and (5) Indoor localization based on WiFi signal strength. It is important to note that we don’t need to construct a knowledge base for each task especially, instead, we just need to construct two knowledge bases (i.e., one IoT domain knowledge base and
one demonstration knowledge base), each of which contains all the domain/demonstration knowledge about the total five tasks. During the retrieval phase, we can easily fetch pertinent knowledge precisely corresponding to the task utilizing metadata (e.g., IoT data type and task type) stored within the bases. For demonstrations, we utilize the one-shot setting, which means we retrieve one example for each category in classification tasks.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S4.T1.2.1">Performance of LLMs on WiFi-based Indoor Localization task.</span> Since this is a regression task, we choose the Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and standard deviation (STD) of the RMSE as the main performance metrics.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.3" style="width:423.4pt;height:167.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.5pt,9.3pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.3.1">
<tr class="ltx_tr" id="S4.T1.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="6" id="S4.T1.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.1.2.1">Model</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.1">Llama2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.2">Mistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.3">Claude-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.4">Gemini-pro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.5">GPT-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.2.6">GPT-4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.1" rowspan="3"><span class="ltx_text" id="S4.T1.3.1.3.1.1"><span class="ltx_text" id="S4.T1.3.1.3.1.1.1"></span> <span class="ltx_text" id="S4.T1.3.1.3.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.3.1.3.1.1.2.1">
<span class="ltx_tr" id="S4.T1.3.1.3.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.1.3.1.1.2.1.1.1">Base-</span></span>
<span class="ltx_tr" id="S4.T1.3.1.3.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.1.3.1.1.2.1.2.1">line</span></span>
</span></span> <span class="ltx_text" id="S4.T1.3.1.3.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.3.2">
<span class="ltx_text" id="S4.T1.3.1.3.2.1"></span> <span class="ltx_text" id="S4.T1.3.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.3.1.3.2.2.1">
<span class="ltx_tr" id="S4.T1.3.1.3.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.1.3.2.2.1.1.1">RMSE (m)</span></span>
</span></span><span class="ltx_text" id="S4.T1.3.1.3.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.3">0.374</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.4">11.570</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.5">0.829</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.6">2.318</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.7">2.598</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.3.8">0.741</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.1.4.1">MAE (m)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.2">0.313</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.3">9.347</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.4">0.696</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.5">1.814</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.6">1.937</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.4.7">0.581</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.1.5.1">STD</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.2">0.903</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.3">6.856</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.4">1.607</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.5">5.999</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.6">6.715</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.5.7">1.502</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.1" rowspan="3"><span class="ltx_text" id="S4.T1.3.1.6.1.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.6.2">RMSE (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.3">0.355</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.4">9.995</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.5">0.404</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.6.6.1">0.313</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.7">0.719</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.6.8">0.402</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.1.7.1">MAE (m)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.2">0.295</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.3">7.980</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.4">0.341</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.7.5.1">0.265</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.6">0.592</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.7.7">0.341</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.1.8.1">STD</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.2">0.852</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.3">11.146</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.8.4.1">0.706</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.5">0.763</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.6">1.765</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.1.8.7">0.697</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.3.1.9.1" rowspan="2"><span class="ltx_text" id="S4.T1.3.1.9.1.1"><span class="ltx_text" id="S4.T1.3.1.9.1.1.1"></span> <span class="ltx_text" id="S4.T1.3.1.9.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.3.1.9.1.1.2.1">
<span class="ltx_tr" id="S4.T1.3.1.9.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.1.9.1.1.2.1.1.1">Impro-</span></span>
<span class="ltx_tr" id="S4.T1.3.1.9.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.1.9.1.1.2.1.2.1">vement</span></span>
</span></span> <span class="ltx_text" id="S4.T1.3.1.9.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.1.9.2">RMSE (m)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.3.1.1" style="color:#09A400;">+5.1%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.4.1.1" style="color:#09A400;">+13.6%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.5.1.1" style="color:#09A400;">+51.3%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.6.1.1" style="color:#09A400;">+86.5%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.7" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.7.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.7.1.1" style="color:#09A400;">+72.3%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.1.9.8" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.9.8.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.9.8.1.1" style="color:#09A400;">+45.7%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.1.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.3.1.10.1">MAE (m)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.2.1.1" style="color:#09A400;">+5.8%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.3.1.1" style="color:#09A400;">+14.6%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.4.1.1" style="color:#09A400;">+51.0%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.5.1.1" style="color:#09A400;">+85.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.6.1.1" style="color:#09A400;">+69.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.1.10.7" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T1.3.1.10.7.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.1.10.7.1.1" style="color:#09A400;">+41.3%</span></span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S4.T2.8.1">Overall performance of LLMs on IoT tasks</span>. <span class="ltx_text ltx_font_bold" id="S4.T2.9.2">HAR-2cls</span> stands for classifying walking and standing activities. <span class="ltx_text ltx_font_bold" id="S4.T2.10.3">HAR-3cls</span> stands for classifying lying, walking upstairs, and transitioning from lying to sitting activities. <span class="ltx_text ltx_font_bold" id="S4.T2.11.4">Heartbeat</span> stands for classifying normal and abnormal heartbeats. <span class="ltx_text ltx_font_bold" id="S4.T2.12.5">Machine</span> stands for determining whether the coolers work properly or not. <span class="ltx_text ltx_font_bold" id="S4.T2.13.6">Occupancy</span> stands for detecting the presence of a person in a room.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1.1" style="width:374.7pt;height:324.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.8pt,18.0pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.1">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.1.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T2.1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">IoT tasks</span> (Accuracy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1.1">HAR-2cls</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.2.1">HAR-3cls</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.3.1">Heartbeat</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.4.1">Machine</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.5.1">Occupancy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.3.1.1">Llama2-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3.3">50.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3.4">32.8%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3.5">50.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3.6">35.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3.7">48.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4.2">57.2%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4.3">38.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4.4">54.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4.5">56.4%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4.6">82.5%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.5.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.2.1.1" style="color:#09A400;">+14.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.3.1.1" style="color:#09A400;">+15.9%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.4.1.1" style="color:#09A400;">+9.0%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.5.1.1" style="color:#09A400;">+61.1%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.5.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.6.1.1" style="color:#09A400;">+70.5%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.6.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.6.1.1">Mistral-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.6.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6.3">61.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6.4">26.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6.5">44.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6.6">31.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6.7">50.0%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.7.2">84.9%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.7.3">42.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.7.4">60.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.7.5">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.1.7.5.1">92.1</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.7.6">61.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.8.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.8.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.2.1.1" style="color:#09A400;">+38.0%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.8.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.3.1.1" style="color:#09A400;">+64.2%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.8.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.4.1.1" style="color:#09A400;">+37.5%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.8.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.5.1.1" style="color:#09A400;">+192.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.8.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.8.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.6.1.1" style="color:#09A400;">+22.2%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.9.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.9.1.1">Claude-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.9.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.9.3">98.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.9.4">80.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.9.5">52.4%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.9.6">51.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.9.7">50.0%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.10.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.10.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.10.2.1">100.0</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.10.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.10.3.1">95.3</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.10.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.10.4.1">81.0</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.10.5">86.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.10.6">82.5%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.11.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.11.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.11.2.1.1" style="color:#09A400;">+1.7%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.11.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.11.3.1.1" style="color:#09A400;">+19.0%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.11.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.11.4.1.1" style="color:#09A400;">+54.6%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.11.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.11.5.1.1" style="color:#09A400;">+69.2%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.11.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.11.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.11.6.1.1" style="color:#09A400;">+65.0%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.12.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.12.1.1">Gemini-pro</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.12.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.12.3">39.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.12.4">34.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.12.5">52.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.12.6">49.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.12.7">55.9%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.13.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.13.2">88.4%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.13.3">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.1.13.3.1">82.8</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.13.4">51.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.13.5">70.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.13.6">66.2%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.14.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.14.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.14.2.1.1" style="color:#09A400;">+124.9%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.14.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.14.3.1.1" style="color:#09A400;">+143.5%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.14.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.14.4.1.1" style="color:#CC0000;background-color:#E2E2E2;">-1.0%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.14.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.14.5.1.1" style="color:#09A400;">+43.1%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.14.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.14.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.14.6.1.1" style="color:#09A400;">+18.4%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.15.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.15.1.1">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.15.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.15.3">91.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.15.4">33.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.15.5">35.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.15.6">51.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.15.7">50.0%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.16.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.16.2">92.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.16.3">45.8%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.16.4">51.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.16.5">61.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.16.6">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.16.6.1">92.1</span>%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.17">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.17.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.17.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.17.2.1.1" style="color:#09A400;">+0.7%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.17.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.17.3.1.1" style="color:#09A400;">+37.5%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.17.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.17.4.1.1" style="color:#09A400;">+44.5%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.17.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.17.5.1.1" style="color:#09A400;">+19.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.17.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.17.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.17.6.1.1" style="color:#09A400;">+84.2%</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.18.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.1.18.1.1">GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.18.2">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.18.3">77.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.18.4">43.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.18.5">54.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.18.6">49.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.18.7">43.7%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.19.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.19.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.19.2.1">100.0</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.19.3">87.8%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.19.4">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.1.19.4.1">69.8</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.19.5">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.19.5.1">92.4</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.19.6">
<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.1.1.1.19.6.1">86.6</span>%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1.20">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.1.1.20.1" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.1.1" style="background-color:#E2E2E2;">Improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.1.20.2" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.2.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.20.2.1.1" style="color:#09A400;">+29.4%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.1.20.3" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.3.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.20.3.1.1" style="color:#09A400;">+102.8%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.1.20.4" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.4.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.20.4.1.1" style="color:#09A400;">+29.3%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.1.20.5" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.5.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.20.5.1.1" style="color:#09A400;">+86.7%</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.1.20.6" style="background-color:#E2E2E2;"><span class="ltx_text" id="S4.T2.1.1.1.20.6.1" style="background-color:#E2E2E2;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.20.6.1.1" style="color:#09A400;">+98.2%</span></span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>IoT datasets.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In our benchmark, we choose public IoT datasets on the five tasks to ensure fairness. Since some datasets are too challenging for LLMs with many classes, we simplify some datasets by only using a subset, which is also employed in previous works <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2024hargpt</span>)</cite>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Human Activity Recognition.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px1.p1.1">We employ the Smartphone-Based Recognition of Human Activities and Postural Transitions Dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">misc_smartphone-based_recognition_of_human_activities_and_postural_transitions_341</span>)</cite>. This dataset comprises raw IMU data, specifically 3-axial linear acceleration, and 3-axial angular velocity, captured at a sampling rate of 50Hz by the smartphone’s accelerometer and gyroscope. The data encompasses twelve distinct activities. To reduce both the sequence length and data complexity, we down-sample the data to 10Hz. Given the challenges associated with multi-class classification for LLM, instead of utilizing all twelve activity categories, we conduct a binary classification task involving the WALKING and STANDING labels, and a ternary classification task with the LYING, WALKING UPSTAIRS, and LIE TO SIT labels.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Industrial anomaly detection.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px2.p1.1">We employ the Condition Monitoring of Hydraulic Systems Dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">misc_condition_monitoring_of_hydraulic_systems_447</span>)</cite>, which facilitates the assessment of a hydraulic test rig’s condition using multi-sensor data, including temperature, cooling power, and efficiency factor series, all experimentally derived from the rig. The dataset categorizes cooler conditions into three severity grades: (1) close to failure; (2) reduced efficiency; and (3) full efficiency. For simplicity, we focus on a binary classification task using only “close to failure”and “full efficiency”categories.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Heartbeat anomaly detection.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px3.p1.1">We employ the MIT-BIH Arrhythmia Database <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">Goldberger2000</span>)</cite>. This dataset comprises ECG recordings from 48 subjects, each sampled at 360Hz, and categorizes heartbeats into several types, including Normal beat (N), Atrial premature beat (A), and Premature ventricular contraction (V), among others. To reduce the difficulty of the task, we down-sample the signals to 72Hz and focus on a binary classification task using only the Normal beat (N) and Premature ventricular contraction (V) categories.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px4">
<h5 class="ltx_title ltx_title_paragraph">Human sensing task.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px4.p1.1">We utilize a dataset collected using a TP-Link TL-WDR4300 WiFi router operating at 5 GHz with a 40 MHz bandwidth <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">zhuravchak2022human</span>)</cite>. The dataset specifically captures the absence of human presence across three different rooms. Each room’s environment is carefully monitored to record Channel State Information (CSI) that reflects the presence or absence of occupants, providing a robust basis for occupancy detection tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px5">
<h5 class="ltx_title ltx_title_paragraph">Indoor localization task.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px5.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px5.p1.1">We utilize a dataset collected in a laboratory environment using an IoT system developed in  <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">huang2022varifi</span>)</cite>. The dataset consists of RSSI signals, the basis for determining human positions within the space. By collecting RSS fingerprints at various reference points, a signal radio map is constructed using a modified Gaussian Process Regression (GPR) method. This approach allows us to estimate the RSS distribution at any given location, providing a reliable means of localizing human presence in the environment.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>LLM Baselines.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In the conducted experiments, we utilize a combination of proprietary and open-source LLMs, including gpt-3.5-turbo, gpt-4-turbo, claude-3-5-sonnet, gemini-pro, Mistral-7B<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3" title="">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a></span></span></span>, and LLama2-7B<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/togethercomputer/LLaMA-2-7B-32K" title="">https://huggingface.co/togethercomputer/LLaMA-2-7B-32K</a></span></span></span>. This diverse selection of models enables a comprehensive evaluation of the LLMs’ capabilities in executing IoT tasks and provides insights into their respective strengths and limitations in real-world applications. The project code will be made available after publication.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S4.F3.1.g1" src="ICLR%202025%20Template/case1_1.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Response example of LLM for human activity recognition (HAR). For more comprehensive examples, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A1" title="Appendix A Response examples of LLMs for IoT tasks ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>results and analysis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the efficacy of our proposed framework in enhancing the capabilities of IoT task reasoning for LLMs, we use HarGPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">ji2024hargpt</span>)</cite> as the baseline, of which the prompts only contain raw IoT data and corresponding task descriptions, without any data preprocessing, domain knowledge, and demonstrations. The overall performance of LLMs on IoT tasks is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.T1" title="Table 1 ‣ 4.1.1 IoT tasks. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.T2" title="Table 2 ‣ 4.1.1 IoT tasks. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. The results show that our proposed method consistently boosts the performance of all the LLMs to complete IoT tasks in real-world scenarios. Notably, advanced LLMs such as Claude-3.5, Gemini-pro, and GPT-4 have demonstrated significant performance improvements, evolving from near-random guessing to effectively solving certain tasks. After analyzing the overall performance and outputs of LLMs in IoT task reasoning, we can answer the questions we proposed in the introduction now. Here is a summary of our arguments regarding the IoT task reasoning with LLMs.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">LLMs excel in various IoT tasks but struggle with complex data challenge.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">Based on the experimental results, we observe that when provided with perception data (i.e., IoT data collected by sensors) and external knowledge, advanced LLMs like GPT-4 and Claude-3.5 can effectively perform various IoT tasks in the physical world, particularly excelling in HAR using IMU data. However, LLMs’ performance is limited by their intrinsic lack of domain-specific knowledge and difficulty in comprehending numerical data. For instance, in the task of heartbeat anomaly detection, even provided with external knowledge, LLMs perform sub-optimally. This is because the time-series nature of ECG data presents significant challenges for LLMs due to its numerical density and length. Although we have mitigated some of these challenges by simplifying the data, this approach only addresses the issue at the data level without fundamentally resolving it at the model level. Additionally, LLMs inherently lack the extensive medical knowledge required for comprehensive analysis. While retrieved knowledge can suffice for simpler tasks, more complex problems may necessitate further model fine-tuning to incorporate deeper and broader medical expertise.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">LLMs are excellent learners in IoT task reasoning.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">Without domain-specific knowledge and relevant demonstrations, LLMs face significant challenges in performing IoT tasks, often resorting to near-random guessing, especially in tasks such as heartbeat anomaly detection. This indicates that real-world tasks remain challenging for LLMs to execute directly. However, LLMs are excellent learners, and their capabilities can be significantly enhanced through data simplification &amp; enrichment and knowledge retrieval augmentation. Specifically, the LLama2-7B, Mistral-7B, Claude-3.5, Gemini-pro, GPT-3.5, and GPT-4 models exhibit average performance improvements of 30%, 62%, 44%, 69%, 43%, and 65% respectively across various tasks, underscoring the effectiveness of our methodology.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">LLMs can act as experts, not just classifiers or predictors.</h5>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">In our study, we prompt LLMs to generate both an analysis of the task and the final answer. Based on this analysis, we demonstrate that LLMs can fully comprehend preprocessed IoT data and effectively utilize the provided knowledge to perform IoT tasks. Unlike traditional DL/ML methods, which are trained end-to-end to produce only the final answer, LLMs offer more explainable results. Specifically, LLMs not only provide the final answer but also the reasoning behind it, akin to expert suggestions in daily life. For instance, when tasked with human activity recognition (as illustrated in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.F3" title="Figure 3 ‣ 4.1.3 LLM Baselines. ‣ 4.1 A benchmark on IoT task reasoning ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>), the LLM delivers a detailed step-by-step analysis before presenting the final answer.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>ablation study</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To evaluate the impact of different components within our framework, we performed an ablation study using GPT-4 on HAR and industrial anomaly detection tasks. We tested the following configurations: (1) IoT data simplification and enrichment, (2) addition of retrieved domain knowledge based on (1), (3) inclusion of retrieved demonstrations based on (2), and (4) the full configuration, which incorporates role descriptions and chain-of-thought techniques as outlined in the Prompt Configuration stage. The results, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#S4.T3" title="Table 3 ‣ 4.3 ablation study ‣ 4 Experiments ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>, reveal that for straightforward tasks such as classifying walking and standing activities, IoT data simplification and enrichment and domain knowledge retrieval are sufficient. However, for more complex tasks, the inclusion of additional modules significantly boosts performance. Overall, our findings indicate that each module in our framework progressively enhances the ability of LLMs to perform IoT-related tasks using IoT data.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study of different modules within our framework on three tasks.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1.1" style="width:357.1pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.8pt,6.3pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1.1">Method</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1">IoT tasks</span> (Accuracy<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1.1">HAR-2cls</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.2.1">HAR-3cls</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.3.1">Machine</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3.1">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.3.2">77.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.3.3">43.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.3.4">49.5%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.4.1">+ <span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.4.1.1">IoT data simplification and enrichment</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.4.2">96.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.4.3">47.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.4.4">62.7%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.5.1">+ <span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.5.1.1">retrieved domain knowledge</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5.2">100.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5.3">78.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5.4">78.0%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.6.1">+ <span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.6.1.1">retrieved demonstrations</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.6.2">100.0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.6.3">86.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.6.4">83.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.1.1.7.1">Full setting</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.1.7.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.7.2.1">100.0%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.1.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.7.3.1">87.8%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.1.7.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.7.4.1">92.4%</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">LLMs often struggle with tasks requiring an understanding of physical laws. To address this, we propose IoT-LLM, a framework that integrates IoT sensor data with LLMs to enhance their perception and reasoning abilities in the physical world. Evaluated on tasks like human activity recognition and industrial anomaly detection, IoT-LLM improves LLM performance by approximately 65%, though challenges remain in specialized domains. This approach systematically enhances LLM capabilities for real-world applications by leveraging IoT data.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Limitations.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">While LLMs can manage low-dimensional time-series data, they face significant challenges with higher-dimensional data, such as audio and 3D point cloud data, due to their extensive length and complexity. Integrating such data into the context of LLMs is both difficult and impractical. Instead, directly fine-tuning LLMs with data specific to these modalities may be a more effective approach. This strategy could be explored in future research to extend the capabilities of LLMs to handle more complex data types within the IoT domain.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Response examples of LLMs for IoT tasks</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we present examples of responses generated by LLM for various applications, including industrial anomaly detection, heartbeat anomaly detection, WiFi-based human sensing, and indoor localization. To emphasize the output of the LLMs, we do not provide detailed prompts; instead, we simply display the raw IoT sensor data and the corresponding user queries.</p>
</div>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A1.F4.1.g1" src="ICLR%202025%20Template/case3.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Response example of LLM for industrial anomaly detection.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A1.F5.1.g1" src="ICLR%202025%20Template/case2.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Response example of LLM for heartbeat anomaly detection.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A1.F6.1.g1" src="ICLR%202025%20Template/case_sensing.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Response example of LLM for WiFi-based human sensing.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A1.F7.1.g1" src="ICLR%202025%20Template/case_local.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Response example of LLM for WiFi-based indoor localization.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompt template</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In the Prompt Configuration stage within our framework, we systematically organize IoT data description, task description,
retrieved pertinent knowledge (including IoT domain knowledge and task-specific demonstrations), and role description to generate the final prompt according to the prompt template, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2.F8" title="Figure 8 ‣ Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a>. For example, based on the final prompt template, we obtain the final prompt (as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.02429v2#A2.F9" title="Figure 9 ‣ Appendix B Prompt template ‣ IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>) for heartbeat anomaly detection.</p>
</div>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A2.F8.1.g1" src="ICLR%202025%20Template/prompt_template.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Final prompt template.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A2.F9.1.g1" src="ICLR%202025%20Template/prompt4ecg.png"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Final prompt for heartbeat anomaly detection. Note that role description is generated automatically by AI models (e.g., ChatGPT).</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering" id="A2.F10.g1" src="ICLR%202025%20Template/data_preprocess.png"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>During IoT data simplification and enrichment stage, raw IoT data is transformed into IoT data description, which is easier to understand by LLMs. Raw IoT data is enriched with descriptive metadata, including natural language expressions of implicit physical information like units. Specialized tokenization techniques and extraction of temporal or frequency domain features further enhance LLMs’ understanding of numerical and time-series data. These improvements make IoT data more accessible and interpretable for LLMs, facilitating its use in real-world applications.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 03:29:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
