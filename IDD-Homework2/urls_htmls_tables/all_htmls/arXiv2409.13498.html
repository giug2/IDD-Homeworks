<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging</title>
<!--Generated on Fri Sep 20 13:24:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Hyperspectral Imaging,  Deep Learning,  Material Classification,  Pixel-level Classification,  Real-time Object Detection
" lang="en" name="keywords"/>
<base href="/html/2409.13498v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S1" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S2" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Hyperspectral Imaging Setup</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS1" title="In 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_italic">Imaging Spectroscopy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS2" title="In 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">Camera</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS2.SSS1" title="In 3.2 Camera ‚Ä£ 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Conveyor</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS3" title="In 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span><span class="ltx_text ltx_font_italic">Illumination</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS4" title="In 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span><span class="ltx_text ltx_font_italic">Acquired Data</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.SS1" title="In 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text ltx_font_italic">Dataset description</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.SS2" title="In 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">Acquisition Pipeline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.SS3" title="In 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span><span class="ltx_text ltx_font_italic">Ground truth mask generation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span><span class="ltx_text ltx_font_smallcaps">Model Architecture &amp; Training</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.SS1" title="In 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_italic">Architecture</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.SS2" title="In 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_italic">Data Pre-Processing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.SS3" title="In 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span><span class="ltx_text ltx_font_italic">Model Training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.SS4" title="In 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span><span class="ltx_text ltx_font_italic">Post-Processing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span><span class="ltx_text ltx_font_smallcaps">Results &amp; Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S7" title="In A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span><span class="ltx_text ltx_font_smallcaps">Conclusion &amp; Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Savvas¬†Sifnaios, George¬†Arvanitakis, Fotios¬†K.¬†Konstantinidis, Georgios¬†Tsimiklis, Angelos¬†Amditis, Panayiotis¬†Frangos
</span><span class="ltx_author_notes">S. Sifnaios, G. Arvanitakis, F. K. Konstantinidis, G. Tsimiklis and A. Amditis are with Institute of Communication and Computer Systems, National Technical University of Athens,9 Iroon. Polytechniou Str., Zografou Athens GR-157 73, Greece.
S. Sifnaios and P. Frangos are with School of Electrical and Computing Engineering, National Technical University of Athens,9 Iroon. Polytechniou Str., Zografou Athens GR-157 73, Greece.
G. Arvanitakis is with Technology Innovation Institute, Yas Island, Abu Dhabi,United Arab Emirates
S. Sifnaios is the corresponding author (savvas.sifnaios@iccs.gr)Manuscript received August 23, 2024; revised XX, 202X.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Recent advancements in computer vision, particularly in detection, segmentation, and classification, have significantly impacted various domains. However, these advancements are tied to RGB-based systems, which are insufficient for applications in industries like waste sorting, pharmaceuticals, and defense, where advanced object characterization beyond shape or color is necessary. Hyperspectral (HS) imaging, capturing both spectral and spatial information, addresses these limitations and offers advantages over conventional technologies such as X-ray fluorescence and Raman spectroscopy, particularly in terms of speed, cost, and safety.</p>
<p class="ltx_p" id="id2.id2">This study evaluates the potential of combining HS imaging with deep learning for material characterization. The research involves: i) designing an experimental setup with HS camera, conveyor, and controlled lighting; ii) generating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with semi-automated mask generation and Raman spectroscopy-based labeling; and iii) developing a deep learning model trained on HS images for pixel-level material classification. The model achieved 99.94% classification accuracy, demonstrating robustness in color, size, and shape invariance, and effectively handling material overlap. Limitations, such as challenges with black objects, are also discussed. Extending computer vision beyond RGB to HS imaging proves feasible, overcoming major limitations of traditional methods and showing strong potential for future applications.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Hyperspectral Imaging, Deep Learning, Material Classification, Pixel-level Classification, Real-time Object Detection

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The field of material classification has evolved significantly over the past few decades, transitioning from traditional techniques to the application of deep learning methodologies. Traditional methods, such as thresholding, edge detection, and classical machine learning algorithms (e.g., k-nearest neighbors, support vector machines), rely heavily on manually crafted features and heuristic rules. These methods are often effective for simple tasks but struggle with complex and fine-grained material differentiation due to their inherent limitations in capturing shape and colour variations .
</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Image classification and segmentation have undergone a paradigm shift with the introduction of deep learning, especially convolutional neural networks (CNNs). Material classification tasks have improved in accuracy and robustness through deep learning models that automatically learn hierarchical features from raw data. However, despite these advancements, challenges remain when using conventional RGB images, especially in scenarios were the color or the shape of the object is not that informative regarding its material.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">RGB images solely rely on spatial features‚Äîsuch as edges, textures, and colours visible to the human eye. This reliance often leads to misclassification when objects of different classes have similar appearances, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>. For instance, in the sorting industry, accurately distinguishing between materials with subtle colour differences or complex shapes is crucial for efficiency and quality control. In the pharmaceutical industry, ensuring the purity and correct identification of compounds necessitates fine segmentation capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib1" title="">1</a>]</cite>. Similarly, in agriculture, RGB imaging often falls short in detecting diseases in crops that present with subtle spectral variations not visible in the RGB spectrum <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib2" title="">2</a>]</cite>. In defense applications, accurate material classification can aid in the detection and identification of hazardous substances, ensuring safety and operational efficiency.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S1.F1.g1" src="extracted/5862626/imgs2/RGB_FAIL.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of RGB-based object misclassification. This image depicts pastry cakes that reassemble apples leading the RGB-based model to mistakenly classify the samples as apples.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Hyperspectral imaging has emerged as a promising solution to these challenges. Unlike traditional RGB imaging, hyperspectral cameras capture a wide spectrum of light beyond the visible range, providing detailed spectral information for each pixel in an image. This richness in spectral data allows for more precise material classification and segmentation, overcoming the limitations of traditional methods.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Despite its potential and the direction of mobile manufactures‚Äô to add more spectrum bands on their phone‚Äôs cameras <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib5" title="">5</a>]</cite>, the research on computer vision beyond RGB imaging is very limited mainly because of the <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">lack of publicly available datasets</span> and the need for <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">specialized hardware</span> that up to this day is expensive. In this paper, we propose P1CH (Pixel-wise 1D Convolutional Hyperspectral) Classifier, a lightweight convolutional neural network designed to perform image segmentation and classification using hyperspectral images. The spectral information captured by the employed hyperspectral camera ranges from 900nm up to 1700nm. The proposed approach leverages the spectral richness of hyperspectral data to enhance the accuracy of material classification, addressing the limitations of traditional methods.
This advancement opens new possibilities for applications in various industries, providing a practical solution to the longstanding problem of precise material classification and segmentation.
The contributions of this work are summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Dataset:</span> The generation and release of a dataset, containing hyperspectral images and their corresponding labeled masks, of plastic samples (HDPE, PET, PP, PS), with the aim to avail research community to investigate other models, applications, insights in the field of computer vision via hyperspectral imaging.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">P1CH:</span> The design and implementation of a lightweight yet accurate (99.94% accuracy) deep learning approach, for pixel-level material classification.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Insights:</span> Extensive performance analysis highlighting the capacity (accuracy, color invariance, shape invariance) as well as the limitations (e.g. black plastics) of the computer vision with hyperspectral imaging.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Cost:</span> Taking advantage of the deep learning capabilities, cost-efficient alternatives for calibration and normalization of Hyperspectral images are proposed tailored for deep learning models.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To this end, Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S2" title="2 Related Work ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> describes the related work, while Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3" title="3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">3</span></a> provides a high-level description of the system setup and the hardware equipment used in this work. The creation of hyperpsectral dataset, the spectral preprocessing methodologies along with the semi-automated AI-assisted mask generation are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4" title="4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">4</span></a>. Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5" title="5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">5</span></a> introduces the architecture of the proposed model, the preparation of the training instances. The training process process, along with the selected hyperparameters and the initial results are also discussed in that section. In addition, the experimental results on various scenarios, i.e. <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">mixed materials</span>, <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">shredded samples</span> and <span class="ltx_text ltx_font_italic" id="S1.p7.1.3">overlapping objects</span>, and the performance of the proposed system are discussed in Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6" title="6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>. This section, also, explores the limitations of the hyperspectral model in the case of dark-coloured or black samples. Finally, in Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S7" title="7 Conclusion &amp; Future Work ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a> a discussion about the main conclusions of this study and the future steps are provided.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Material classification has historically relied on several approaches that cut across many domains, including electro-mechanical and chemical analysis techniques. Raman and Near-Infrared spectroscopy, coupled with a multivariate analysis, have widely been used to identify the materials‚Äô composition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib8" title="">8</a>]</cite>. In another work, the utilisation of X-Ray Diffraction (XRD), Energy Dispersive X-ray Spectroscopy (EDS) and Atomic Absorption Spectroscopy (AAS) techinques have been proposed, because their high sensitivity and accuracy in detecting microstructural features and hence identifying the element composition of a sample <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib9" title="">9</a>]</cite>. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib10" title="">10</a>]</cite> highlighted the use of Differential Scanning Calorimetry (DSC) and Thermogravimetric Analysis (TGA) in accurately determining the thermal properties of various materials . Zhang and Shao <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib11" title="">11</a>]</cite> emphasized the role of optical microscopy in material.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">In an attempt to further increase the performance and robustness of material classification pipelines, various studies explored the potential of deep learning. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib12" title="">12</a>]</cite> a deep convolutional neural network (CNN) was introduced for classifying 60-GHz radar data with an accuracy of 97%. Deep neural networks have, also, been utilised to analyse surface haptic data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib13" title="">13</a>]</cite>, achieving a precision of 94% and recall of 92%. Moreover, CNNs have been proposed for the case of visual RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib17" title="">17</a>]</cite> in various use cases, e.g. commercial waste, steel products, etc. In another work, Konstantinids et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib18" title="">18</a>]</cite> proposed a multi-modal deep classifier based on ResNet-18 that jointly extracts information from RGB and multispectral cameras to classify plastic polymers, as well as wood by products with an object accuracy of 96%.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">While the cited work above demonstrates impressive classification performance, these methods are often slow and computationally intensive. Hyperspectral imaging has become a powerful means for in situ material classification, since they offer the advantage of real-time and efficient analysis. Shaikh and Th√∂rnberg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib19" title="">19</a>]</cite> investigated the impact of water vapor on polymer identification by means of short-wave infrared hyperspectral imaging yielding an accuracy rate around 88%. According to Shaban <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib20" title="">20</a>]</cite>, hyperspectral imaging has been used to determine different characteristics pertaining to concrete without disturbing its structure at sites, where it would not be possible to take samples back to laboratories for analysis; the typical accuracies are above 90%. With about 93% accuracy Capobianco et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib21" title="">21</a>]</cite> have characterized ancient roman wall paintings using Hyperspectral Imaging as well as aid in artwork authentication (Polak et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib22" title="">22</a>]</cite>), achieving a precision of 95%.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">The combination of deep learning and hyperspectral imaging has shown great promise in improving the material classification capability in Earth Observation applications. Notable examples include models, e.g. Xception-based, CNN-based systems, and R-VCANet achieving accuracies up to 99%, with precision and recall values around 94% and 93%, and F1-scores reaching 91% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib27" title="">27</a>]</cite>. Venkatesan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib28" title="">28</a>]</cite> applied deep recurrent neural networks in medical hyperspectral images for feature recognition achieving a precision rate of about 96%. Xiong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib29" title="">29</a>]</cite> developed material tracking methods for hyperspectral videos using deep learning, achieving an F1-score of 94%. Medus et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib30" title="">30</a>]</cite> applied CNNs to classify hyperspectral images in industrial food packaging, reporting an accuracy of 99%. Okada et al. followed a patch-wise approach for the identification of 5 different mineral types, using a VGG16-based CNN to classify the acquired HS images they achieved high accuracy over 90% <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib31" title="">31</a>]</cite>. Extreme Learning combined with Stacked Autoencoders for feature extraction has also been applied on HS images, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib32" title="">32</a>]</cite>, for the detection of plastic films within cotton feed stock, creating a pixel-level classification map with accuracy up to 95%. Zhu et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib33" title="">33</a>]</cite>, also attempted to classify different cotton seeds. Their work encompasses a multivariate analysis for manual extraction of the 10 most informative features, which were subsequently fed to CNN for the classification task, resulting in a prediction accuracy of 88%. Finally, Artzai et al. worked with HS images with 76 bands of non ferrous metals, and utilised them to train a CNN-based U-Net like network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib34" title="">34</a>]</cite>. This approach analysed the HS images as a whole and created a classification map with an accuracy equal to 95%.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">The aforementioned studies with hyperspectral classification using deep learning have yielded remarkable results. However, it is important to note most of the already limited approaches that exist on the field commonly either deal with HS images in batches or utilise 2D Convolutional Neural Networks (CNNs). In order to achieve this, they use 2D convolutional kernels that inevitably lead to loss of accuracy at object borders. Notably, for 2D CNNs convolutions are designed to capture spatial context within each individual frame but this may blur boundaries or misclassify pixels on the boundary between neighboring objects that have different spectral signatures. This means that spectral continuity across edges is not incorporated by the 2D convolution operation causing edge artifacts and reducing classification accuracy here. These limitations highlight the need for more refined techniques, that analyse HS images on a pixel level and can preserve boundary integrity while leveraging the rich spectral data provided by hyperspectral imaging.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Hyperspectral Imaging Setup</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section provides a detailed description of the physical infrastructure of the cyber-physical system where the dataset was created using materials that were conveyed under the vision and hyperspectral sensors.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">Imaging Spectroscopy</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Hyperspectral imaging combines conventional imaging and spectroscopic methodologies with the goal to simultaneously obtain spatial and spectral information from various wavelengths of the electromagnetic spectrum for every individual pixel in an image of a scene, with the objective of locating objects, classifying materials, or detecting processes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib35" title="">35</a>]</cite>. Pushbroom sensors capture spectral information across a swath as the sensor moves, line scan sensors capture data one line at a time, while whiskbroom sensors, scan point-by-point to build an image, and snapshot sensors capture the entire spectral image in a single exposure.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Camera</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The SPECIM FX17 line scan camera, following push broom technology, utilises a matrix detector and an imaging spectrograph to capture spectral data efficiently. Light enters through high-performance optics and an entrance slit, forming a line image that the spectrograph disperses into a spectrum (900 - 1700 nm, across 224 bands). This setup allows each axis of the detector to record spatial position and spectral information simultaneously. It ensures measurement stability despite sample or camera movement, requires less illumination power while achieving higher intensity, and is significantly more efficient than filter-based cameras, yielding a purer spectrum.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Conveyor</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Once the line scan camera is selected for capturing, the system comprises a conveyor belt that facilitates the horizontal movement of objects at an adjustable speed, in order to achieve synchronised sampling frequency of the hyperspectal sensor and objects‚Äô movements. The camera was placed 0.73 cm from the conveyor, while the illumination source was placed 0.5 meters vertically above the conveyor belt, at a 45-degree angle, and 0.3 meters horizontally from the center of the camera.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span class="ltx_text ltx_font_italic" id="S3.SS3.1.1">Illumination</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The delpoyed custom-made LDL-222X42CIR Full-Spectrum bar light manufactured by CCS Inc., offers better performance for Imaging spectroscopy challenges in contrast with Led lights¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib36" title="">36</a>]</cite>. Specifically, it includes four different halogen bulbs that emit light in distinct regions of the spectrum and a power rating of 87W. This permits it to span the complete range of wavelengths from 400nm to 2400nm. This light source contains a dispersion layer that uniformly distributes the output light in multiple directions, resulting in more consistent illumination of the scene and the objects within it.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span class="ltx_text ltx_font_italic" id="S3.SS4.1.1">Acquired Data</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.6">The HS camera employed in this work is ample of achieving a maximum sampling rate of 400 FPS, with each scanned line being of shape <math alttext="640\times 224" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">640</mn><mo id="S3.SS4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><cn id="S3.SS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1.2">640</cn><cn id="S3.SS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">640\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">640 √ó 224</annotation></semantics></math>. Moreover, camera provides the option of applying on-chip spatial and/or spectral binning, hence reducing the respective dimensions by factors of <math alttext="\times 2" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml"></mi><mo id="S3.SS4.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.2.m2.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><times id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">absent</csymbol><cn id="S3.SS4.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\times 2</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">√ó 2</annotation></semantics></math>, <math alttext="\times 4" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mrow id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml"></mi><mo id="S3.SS4.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.3.m3.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><times id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">absent</csymbol><cn id="S3.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS4.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\times 4</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">√ó 4</annotation></semantics></math>, or <math alttext="\times 8" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><mrow id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml"></mi><mo id="S3.SS4.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.4.m4.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><times id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1"></times><csymbol cd="latexml" id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">absent</csymbol><cn id="S3.SS4.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.SS4.p1.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\times 8</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">√ó 8</annotation></semantics></math>. In this work, no spectral or spatial binning was applied, thus the acquired data were of shape <math alttext="n_{rows}\times 640\times 224" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml"><msub id="S3.SS4.p1.5.m5.1.1.2" xref="S3.SS4.p1.5.m5.1.1.2.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2.2" xref="S3.SS4.p1.5.m5.1.1.2.2.cmml">n</mi><mrow id="S3.SS4.p1.5.m5.1.1.2.3" xref="S3.SS4.p1.5.m5.1.1.2.3.cmml"><mi id="S3.SS4.p1.5.m5.1.1.2.3.2" xref="S3.SS4.p1.5.m5.1.1.2.3.2.cmml">r</mi><mo id="S3.SS4.p1.5.m5.1.1.2.3.1" xref="S3.SS4.p1.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.5.m5.1.1.2.3.3" xref="S3.SS4.p1.5.m5.1.1.2.3.3.cmml">o</mi><mo id="S3.SS4.p1.5.m5.1.1.2.3.1a" xref="S3.SS4.p1.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.5.m5.1.1.2.3.4" xref="S3.SS4.p1.5.m5.1.1.2.3.4.cmml">w</mi><mo id="S3.SS4.p1.5.m5.1.1.2.3.1b" xref="S3.SS4.p1.5.m5.1.1.2.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.5.m5.1.1.2.3.5" xref="S3.SS4.p1.5.m5.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.SS4.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.5.m5.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.5.m5.1.1.3" xref="S3.SS4.p1.5.m5.1.1.3.cmml">640</mn><mo id="S3.SS4.p1.5.m5.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p1.5.m5.1.1.1.cmml">√ó</mo><mn id="S3.SS4.p1.5.m5.1.1.4" xref="S3.SS4.p1.5.m5.1.1.4.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1"><times id="S3.SS4.p1.5.m5.1.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1.1"></times><apply id="S3.SS4.p1.5.m5.1.1.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.1.2.1.cmml" xref="S3.SS4.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.1.2.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2.2">ùëõ</ci><apply id="S3.SS4.p1.5.m5.1.1.2.3.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3"><times id="S3.SS4.p1.5.m5.1.1.2.3.1.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3.1"></times><ci id="S3.SS4.p1.5.m5.1.1.2.3.2.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3.2">ùëü</ci><ci id="S3.SS4.p1.5.m5.1.1.2.3.3.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3.3">ùëú</ci><ci id="S3.SS4.p1.5.m5.1.1.2.3.4.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3.4">ùë§</ci><ci id="S3.SS4.p1.5.m5.1.1.2.3.5.cmml" xref="S3.SS4.p1.5.m5.1.1.2.3.5">ùë†</ci></apply></apply><cn id="S3.SS4.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS4.p1.5.m5.1.1.3">640</cn><cn id="S3.SS4.p1.5.m5.1.1.4.cmml" type="integer" xref="S3.SS4.p1.5.m5.1.1.4">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">n_{rows}\times 640\times 224</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">italic_n start_POSTSUBSCRIPT italic_r italic_o italic_w italic_s end_POSTSUBSCRIPT √ó 640 √ó 224</annotation></semantics></math>, where <math alttext="n_{rows}" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><msub id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml"><mi id="S3.SS4.p1.6.m6.1.1.2" xref="S3.SS4.p1.6.m6.1.1.2.cmml">n</mi><mrow id="S3.SS4.p1.6.m6.1.1.3" xref="S3.SS4.p1.6.m6.1.1.3.cmml"><mi id="S3.SS4.p1.6.m6.1.1.3.2" xref="S3.SS4.p1.6.m6.1.1.3.2.cmml">r</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.3" xref="S3.SS4.p1.6.m6.1.1.3.3.cmml">o</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1a" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.4" xref="S3.SS4.p1.6.m6.1.1.3.4.cmml">w</mi><mo id="S3.SS4.p1.6.m6.1.1.3.1b" xref="S3.SS4.p1.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS4.p1.6.m6.1.1.3.5" xref="S3.SS4.p1.6.m6.1.1.3.5.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.1.2.cmml" xref="S3.SS4.p1.6.m6.1.1.2">ùëõ</ci><apply id="S3.SS4.p1.6.m6.1.1.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3"><times id="S3.SS4.p1.6.m6.1.1.3.1.cmml" xref="S3.SS4.p1.6.m6.1.1.3.1"></times><ci id="S3.SS4.p1.6.m6.1.1.3.2.cmml" xref="S3.SS4.p1.6.m6.1.1.3.2">ùëü</ci><ci id="S3.SS4.p1.6.m6.1.1.3.3.cmml" xref="S3.SS4.p1.6.m6.1.1.3.3">ùëú</ci><ci id="S3.SS4.p1.6.m6.1.1.3.4.cmml" xref="S3.SS4.p1.6.m6.1.1.3.4">ùë§</ci><ci id="S3.SS4.p1.6.m6.1.1.3.5.cmml" xref="S3.SS4.p1.6.m6.1.1.3.5">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">n_{rows}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">italic_n start_POSTSUBSCRIPT italic_r italic_o italic_w italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is depended on the selected sampling rate and the acquisition duration. Finally, it is worth mentioning that the pixel size of the selected camera is 0.9375mm, thus allowing for very precise and accurate segmentation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Dataset</span>
</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S4.F2.g1" src="extracted/5862626/imgs2/train_set.png" width="281"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The false colour, contrast stretched HS images used for the generation of the training set. </figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Following the aforementioned hyperspectral imaging setup, this section presents the dataset used in the experimental part of this work. At first, a comprehensive description of the dataset is provided, highlighting the different material classes involved in the dataset and summarizing the number of images, objects, and pixels for each category. The following subsections address the processes involved in dataset preparation, including the normalization of the images to ensure numerical consistency. The generation of ground truth masks is subsequently discussed, pointing out their necessity for the proposed supervised learning approach. Moreover, the reasoning behind the utilization of Raman spectroscopy throughout labeling process is explained. Finally, the generation of the training batches and their format is outlined.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Dataset description</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For the generation of the train and test dataset an ensemble of plastic samples of different material categories were collected consisting of <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">HDPE</span>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">PET</span>, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">PP</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">PS</span> objects.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Train Set:</span> In detail, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">169</span> objects were selected to represent the classes. The physical objects were separated according to their material classes and subsequently placed randomly on the conveyor belt, described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S3.SS2" title="3.2 Camera ‚Ä£ 3 Hyperspectral Imaging Setup ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">3.2</span></a>. For the augmentation of the training set by every sample was captured twice with slightly different light conditions. A comprehensive analysis of the dataset composition is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.T1" title="TABLE I ‚Ä£ 4.1 Dataset description ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of Images, Objects, and Pixels per Category of the Training set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Material</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Images</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">Objects</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Pixels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">HDPE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.3">40</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.4">794,142</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1">PET</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2.3">24</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2.4">810,483</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.3.1">PP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3.3">45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3.4">831,340</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.4.1">PS</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.4.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.4.3">60</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.4.4">801,936</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2">At this point, it is important to mention that a <math alttext="5^{th}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><msup id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">5</mn><mrow id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml">t</mi><mo id="S4.SS1.p3.1.m1.1.1.3.1" xref="S4.SS1.p3.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">superscript</csymbol><cn id="S4.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.2">5</cn><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><times id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3.1"></times><ci id="S4.SS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2">ùë°</ci><ci id="S4.SS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">5^{th}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">5 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> class was introduced to the dataset. Namely, this class is <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.2.1">Background</span> and it represents the set of pixels depicting the conveyor belt‚Äôs surface. Since the conveyor belt is apparent in every HS image, no matter the type of plastic, initially the number of <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.2.2">Background</span> pixels was <math alttext="\times 10" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml"></mi><mo id="S4.SS1.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p3.2.m2.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><times id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></times><csymbol cd="latexml" id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">absent</csymbol><cn id="S4.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.p3.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\times 10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">√ó 10</annotation></semantics></math> higher than any other class, potentially leading the model to be biased in favor of this class. To handle this, a random subset of <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.2.3">Background</span> pixels was selected, ensuring its size will be equal to the maximum number of pixels of the plastic classes, i.e. 831,340 pixels.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Test Set:</span> Regarding the test set, was prepared from different objects than the train set, but that belong to the same material categories. Additionally the dataset was selected in order to capture four main cases i) <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.2">unmodified objects</span>, similar to the train set ii) <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.3">shredded objects</span>, the shape do not capture any information iii) Mixed-overlapping materials, where a material is into another material, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.T3" title="TABLE III ‚Ä£ 4.1 Dataset description ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">III</span></a> presents the dimensions of the test set.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T3.fig1" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">TABLE II: </span>Test set without black samples.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.fig1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.fig1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.1.1.1.1.1">Material</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.fig1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.1.1.1.2.1">Objects</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.fig1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.1.1.1.3.1">Pixel</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.fig1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig1.1.2.1.1">HDPE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.2.1.2">29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.2.1.3">124,109</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig1.1.3.2.1">PET</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.3.2.2">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.3.2.3">100,445</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig1.1.4.3.1">PP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.4.3.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig1.1.4.3.3">59,682</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig1.1.5.4.1">PS</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.fig1.1.5.4.2">29</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.fig1.1.5.4.3">116,704</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T3.fig2" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">TABLE III: </span>Black samples test subset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.fig2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.fig2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.fig2.1.1.1.1.1">Material</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.fig2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.fig2.1.1.1.2.1">Objects</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.fig2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.fig2.1.1.1.3.1">Pixels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.fig2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig2.1.2.1.1">HDPE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.2.1.2">19</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.2.1.3">31,649</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig2.1.3.2.1">PET</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.3.2.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.3.2.3">0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig2.1.4.3.1">PP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.4.3.2">14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.fig2.1.4.3.3">25,280</td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.fig2.1.5.4.1">PS</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.fig2.1.5.4.2">5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.fig2.1.5.4.3">5,035</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Furthermore, regarding the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib38" title="">38</a>]</cite>, HS cameras (up to 1700nm) cannot capture information regarding plastics that are coloured black. In order to examine that an extra black plastic dataset was collected and presented in the Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.T3" title="TABLE III ‚Ä£ 4.1 Dataset description ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">III</span></a></p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="97" id="S4.F3.g1" src="extracted/5862626/imgs/raman.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Raman spectra for the 4 material classes. For the HDPE spectrum, the peaks at 1058 cm<sup class="ltx_sup" id="S4.F3.17.1">-1</sup>, 1123 cm<sup class="ltx_sup" id="S4.F3.18.2">-1</sup>, 1291 cm<sup class="ltx_sup" id="S4.F3.19.3">-1</sup>, 1437 cm<sup class="ltx_sup" id="S4.F3.20.4">-1</sup>, and the range from 2843-2876 cm<sup class="ltx_sup" id="S4.F3.21.5">-1</sup> are characteristic of high-density polyethylene (HDPE). These peaks correspond to the vibrations of the molecular structure of HDPE, specifically indicating the various stretching and bending vibrations of the C-H bonds. Accordingly, for the rest of the spectra, it is pointed out that two characteristic peaks at 1607 cm<sup class="ltx_sup" id="S4.F3.22.6">-1</sup> and 1721 cm<sup class="ltx_sup" id="S4.F3.23.7">-1</sup> were detected in the top-right plot and correspond to the vibrations of the phenyl group in the polyester. Additionally, the peaks in the range of 1100-1200 cm<sup class="ltx_sup" id="S4.F3.24.8">-1</sup> indicate the stretching vibrations of the C-O group. The peaks observed at 970 cm<sup class="ltx_sup" id="S4.F3.25.9">-1</sup>, 1034 cm<sup class="ltx_sup" id="S4.F3.26.10">-1</sup>, 1360 cm<sup class="ltx_sup" id="S4.F3.27.11">-1</sup>, 1453 cm<sup class="ltx_sup" id="S4.F3.28.12">-1</sup>, and 2946 cm<sup class="ltx_sup" id="S4.F3.29.13">-1</sup> in the bottom-left plot are associated with the vibrations of the methyl group (CH<sub class="ltx_sub" id="S4.F3.30.14">3</sub>) in polypropylene, while the intense peak observed at 1010 cm<sup class="ltx_sup" id="S4.F3.31.15">-1</sup>, in the bottom-right plot, along with the peak at 1598 cm<sup class="ltx_sup" id="S4.F3.32.16">-1</sup>, suggest the presence of polystyrene.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">Acquisition Pipeline</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">This section describes the process of converting the captured pixel values from <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.2.1">16-bit</span> <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.2.2">unsigned</span> integers, in the range of <math alttext="[0,4095]" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.2"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml"><mo id="S4.SS2.p1.1.m1.2.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0</mn><mo id="S4.SS2.p1.1.m1.2.3.2.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">4095</mn><mo id="S4.SS2.p1.1.m1.2.3.2.3" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><interval closure="closed" id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">0</cn><cn id="S4.SS2.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS2.p1.1.m1.2.2">4095</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">[0,4095]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.2d">[ 0 , 4095 ]</annotation></semantics></math> to compatible for deep learning and ready-to-used <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.2.3">32-bit</span> floats pixel values in the range of <math alttext="[0.,1.]" class="ltx_math_unparsed" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1b"><mo id="S4.SS2.p1.2.m2.1.1" stretchy="false">[</mo><mn id="S4.SS2.p1.2.m2.1.2">0</mn><mo id="S4.SS2.p1.2.m2.1.3" lspace="0em" rspace="0.167em">.</mo><mo id="S4.SS2.p1.2.m2.1.4">,</mo><mn id="S4.SS2.p1.2.m2.1.5">1</mn><mo id="S4.SS2.p1.2.m2.1.6" lspace="0em" rspace="0.167em">.</mo><mo id="S4.SS2.p1.2.m2.1.7" stretchy="false">]</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">[0.,1.]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">[ 0 . , 1 . ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Throughout the literature it is strongly suggested a) to use the <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">spectral calibration matrix</span>, that is specifically provided by the camera manufacturer, and post the HS image‚Äôs multiplication with spectral calibration matrix, b) to <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.2">normalize</span> the calibrated data using <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.3">White</span> and <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.4">Black</span> reference.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">White reference:</span> A white reflection target is required with the key property of reflecting the incident radiation uniformly across all wavelengths. In this manner, the real maximum absolute pixel value a HS camera can capture, given the illumination source, is calculated and ultimately creates the white reference. Although should be taken into account that a white reflection target is often not commercially available and its cost grows exponentially with respect to its dimensions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Black reference:</span> On the other hand, the black reference is utilised to model the sensors‚Äô electronic noise, caused by the electrons‚Äô random movement due to the sensors‚Äô temperature. In this work, the black reference <math alttext="I_{\text{black}}(\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.2" xref="S4.SS2.p4.1.m1.1.2.cmml"><msub id="S4.SS2.p4.1.m1.1.2.2" xref="S4.SS2.p4.1.m1.1.2.2.cmml"><mi id="S4.SS2.p4.1.m1.1.2.2.2" xref="S4.SS2.p4.1.m1.1.2.2.2.cmml">I</mi><mtext id="S4.SS2.p4.1.m1.1.2.2.3" xref="S4.SS2.p4.1.m1.1.2.2.3a.cmml">black</mtext></msub><mo id="S4.SS2.p4.1.m1.1.2.1" xref="S4.SS2.p4.1.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p4.1.m1.1.2.3.2" xref="S4.SS2.p4.1.m1.1.2.cmml"><mo id="S4.SS2.p4.1.m1.1.2.3.2.1" stretchy="false" xref="S4.SS2.p4.1.m1.1.2.cmml">(</mo><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">Œª</mi><mo id="S4.SS2.p4.1.m1.1.2.3.2.2" stretchy="false" xref="S4.SS2.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.2"><times id="S4.SS2.p4.1.m1.1.2.1.cmml" xref="S4.SS2.p4.1.m1.1.2.1"></times><apply id="S4.SS2.p4.1.m1.1.2.2.cmml" xref="S4.SS2.p4.1.m1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.2.2.1.cmml" xref="S4.SS2.p4.1.m1.1.2.2">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.2.2.2.cmml" xref="S4.SS2.p4.1.m1.1.2.2.2">ùêº</ci><ci id="S4.SS2.p4.1.m1.1.2.2.3a.cmml" xref="S4.SS2.p4.1.m1.1.2.2.3"><mtext id="S4.SS2.p4.1.m1.1.2.2.3.cmml" mathsize="70%" xref="S4.SS2.p4.1.m1.1.2.2.3">black</mtext></ci></apply><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">ùúÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">I_{\text{black}}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª )</annotation></semantics></math> was acquired, by closing the shutter of the camera and capturing 1000 lines, which were then averaged to create the black reference. For the maximum absolute pixel value the HS camera can capture, the assumption was made that it can be approximated by calculating the maximum pixel value within the train set.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">By modeling that noise, one can subtract the black reference both from the HS image and the white reference, to acquire the noise-corrected version of both images. Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.E1" title="In 4.2 Acquisition Pipeline ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">1</span></a> provides a mathematical formulation of the black-white reference normalization:</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{\text{norm}}(x,y,\lambda)=\frac{I(x,y,\lambda)-I_{\text{black}}(\lambda)}{I%
_{\text{white}}(\lambda)-I_{\text{black}}(\lambda)}~{}~{}," class="ltx_Math" display="block" id="S4.E1.m1.10"><semantics id="S4.E1.m1.10a"><mrow id="S4.E1.m1.10.10.1" xref="S4.E1.m1.10.10.1.1.cmml"><mrow id="S4.E1.m1.10.10.1.1" xref="S4.E1.m1.10.10.1.1.cmml"><mrow id="S4.E1.m1.10.10.1.1.2" xref="S4.E1.m1.10.10.1.1.2.cmml"><msub id="S4.E1.m1.10.10.1.1.2.2" xref="S4.E1.m1.10.10.1.1.2.2.cmml"><mi id="S4.E1.m1.10.10.1.1.2.2.2" xref="S4.E1.m1.10.10.1.1.2.2.2.cmml">I</mi><mtext id="S4.E1.m1.10.10.1.1.2.2.3" xref="S4.E1.m1.10.10.1.1.2.2.3a.cmml">norm</mtext></msub><mo id="S4.E1.m1.10.10.1.1.2.1" xref="S4.E1.m1.10.10.1.1.2.1.cmml">‚Å¢</mo><mrow id="S4.E1.m1.10.10.1.1.2.3.2" xref="S4.E1.m1.10.10.1.1.2.3.1.cmml"><mo id="S4.E1.m1.10.10.1.1.2.3.2.1" stretchy="false" xref="S4.E1.m1.10.10.1.1.2.3.1.cmml">(</mo><mi id="S4.E1.m1.7.7" xref="S4.E1.m1.7.7.cmml">x</mi><mo id="S4.E1.m1.10.10.1.1.2.3.2.2" xref="S4.E1.m1.10.10.1.1.2.3.1.cmml">,</mo><mi id="S4.E1.m1.8.8" xref="S4.E1.m1.8.8.cmml">y</mi><mo id="S4.E1.m1.10.10.1.1.2.3.2.3" xref="S4.E1.m1.10.10.1.1.2.3.1.cmml">,</mo><mi id="S4.E1.m1.9.9" xref="S4.E1.m1.9.9.cmml">Œª</mi><mo id="S4.E1.m1.10.10.1.1.2.3.2.4" stretchy="false" xref="S4.E1.m1.10.10.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.10.10.1.1.1" xref="S4.E1.m1.10.10.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.6.6" xref="S4.E1.m1.6.6.cmml"><mrow id="S4.E1.m1.4.4.4" xref="S4.E1.m1.4.4.4.cmml"><mrow id="S4.E1.m1.4.4.4.6" xref="S4.E1.m1.4.4.4.6.cmml"><mi id="S4.E1.m1.4.4.4.6.2" xref="S4.E1.m1.4.4.4.6.2.cmml">I</mi><mo id="S4.E1.m1.4.4.4.6.1" xref="S4.E1.m1.4.4.4.6.1.cmml">‚Å¢</mo><mrow id="S4.E1.m1.4.4.4.6.3.2" xref="S4.E1.m1.4.4.4.6.3.1.cmml"><mo id="S4.E1.m1.4.4.4.6.3.2.1" stretchy="false" xref="S4.E1.m1.4.4.4.6.3.1.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">x</mi><mo id="S4.E1.m1.4.4.4.6.3.2.2" xref="S4.E1.m1.4.4.4.6.3.1.cmml">,</mo><mi id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml">y</mi><mo id="S4.E1.m1.4.4.4.6.3.2.3" xref="S4.E1.m1.4.4.4.6.3.1.cmml">,</mo><mi id="S4.E1.m1.3.3.3.3" xref="S4.E1.m1.3.3.3.3.cmml">Œª</mi><mo id="S4.E1.m1.4.4.4.6.3.2.4" stretchy="false" xref="S4.E1.m1.4.4.4.6.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.4.5" xref="S4.E1.m1.4.4.4.5.cmml">‚àí</mo><mrow id="S4.E1.m1.4.4.4.7" xref="S4.E1.m1.4.4.4.7.cmml"><msub id="S4.E1.m1.4.4.4.7.2" xref="S4.E1.m1.4.4.4.7.2.cmml"><mi id="S4.E1.m1.4.4.4.7.2.2" xref="S4.E1.m1.4.4.4.7.2.2.cmml">I</mi><mtext id="S4.E1.m1.4.4.4.7.2.3" xref="S4.E1.m1.4.4.4.7.2.3a.cmml">black</mtext></msub><mo id="S4.E1.m1.4.4.4.7.1" xref="S4.E1.m1.4.4.4.7.1.cmml">‚Å¢</mo><mrow id="S4.E1.m1.4.4.4.7.3.2" xref="S4.E1.m1.4.4.4.7.cmml"><mo id="S4.E1.m1.4.4.4.7.3.2.1" stretchy="false" xref="S4.E1.m1.4.4.4.7.cmml">(</mo><mi id="S4.E1.m1.4.4.4.4" xref="S4.E1.m1.4.4.4.4.cmml">Œª</mi><mo id="S4.E1.m1.4.4.4.7.3.2.2" stretchy="false" xref="S4.E1.m1.4.4.4.7.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E1.m1.6.6.6" xref="S4.E1.m1.6.6.6.cmml"><mrow id="S4.E1.m1.6.6.6.4" xref="S4.E1.m1.6.6.6.4.cmml"><msub id="S4.E1.m1.6.6.6.4.2" xref="S4.E1.m1.6.6.6.4.2.cmml"><mi id="S4.E1.m1.6.6.6.4.2.2" xref="S4.E1.m1.6.6.6.4.2.2.cmml">I</mi><mtext id="S4.E1.m1.6.6.6.4.2.3" xref="S4.E1.m1.6.6.6.4.2.3a.cmml">white</mtext></msub><mo id="S4.E1.m1.6.6.6.4.1" xref="S4.E1.m1.6.6.6.4.1.cmml">‚Å¢</mo><mrow id="S4.E1.m1.6.6.6.4.3.2" xref="S4.E1.m1.6.6.6.4.cmml"><mo id="S4.E1.m1.6.6.6.4.3.2.1" stretchy="false" xref="S4.E1.m1.6.6.6.4.cmml">(</mo><mi id="S4.E1.m1.5.5.5.1" xref="S4.E1.m1.5.5.5.1.cmml">Œª</mi><mo id="S4.E1.m1.6.6.6.4.3.2.2" stretchy="false" xref="S4.E1.m1.6.6.6.4.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.6.6.6.3" xref="S4.E1.m1.6.6.6.3.cmml">‚àí</mo><mrow id="S4.E1.m1.6.6.6.5" xref="S4.E1.m1.6.6.6.5.cmml"><msub id="S4.E1.m1.6.6.6.5.2" xref="S4.E1.m1.6.6.6.5.2.cmml"><mi id="S4.E1.m1.6.6.6.5.2.2" xref="S4.E1.m1.6.6.6.5.2.2.cmml">I</mi><mtext id="S4.E1.m1.6.6.6.5.2.3" xref="S4.E1.m1.6.6.6.5.2.3a.cmml">black</mtext></msub><mo id="S4.E1.m1.6.6.6.5.1" xref="S4.E1.m1.6.6.6.5.1.cmml">‚Å¢</mo><mrow id="S4.E1.m1.6.6.6.5.3.2" xref="S4.E1.m1.6.6.6.5.cmml"><mo id="S4.E1.m1.6.6.6.5.3.2.1" stretchy="false" xref="S4.E1.m1.6.6.6.5.cmml">(</mo><mi id="S4.E1.m1.6.6.6.2" xref="S4.E1.m1.6.6.6.2.cmml">Œª</mi><mo id="S4.E1.m1.6.6.6.5.3.2.2" stretchy="false" xref="S4.E1.m1.6.6.6.5.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S4.E1.m1.10.10.1.2" lspace="0.660em" xref="S4.E1.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.10b"><apply id="S4.E1.m1.10.10.1.1.cmml" xref="S4.E1.m1.10.10.1"><eq id="S4.E1.m1.10.10.1.1.1.cmml" xref="S4.E1.m1.10.10.1.1.1"></eq><apply id="S4.E1.m1.10.10.1.1.2.cmml" xref="S4.E1.m1.10.10.1.1.2"><times id="S4.E1.m1.10.10.1.1.2.1.cmml" xref="S4.E1.m1.10.10.1.1.2.1"></times><apply id="S4.E1.m1.10.10.1.1.2.2.cmml" xref="S4.E1.m1.10.10.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.10.10.1.1.2.2.1.cmml" xref="S4.E1.m1.10.10.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.10.10.1.1.2.2.2.cmml" xref="S4.E1.m1.10.10.1.1.2.2.2">ùêº</ci><ci id="S4.E1.m1.10.10.1.1.2.2.3a.cmml" xref="S4.E1.m1.10.10.1.1.2.2.3"><mtext id="S4.E1.m1.10.10.1.1.2.2.3.cmml" mathsize="70%" xref="S4.E1.m1.10.10.1.1.2.2.3">norm</mtext></ci></apply><vector id="S4.E1.m1.10.10.1.1.2.3.1.cmml" xref="S4.E1.m1.10.10.1.1.2.3.2"><ci id="S4.E1.m1.7.7.cmml" xref="S4.E1.m1.7.7">ùë•</ci><ci id="S4.E1.m1.8.8.cmml" xref="S4.E1.m1.8.8">ùë¶</ci><ci id="S4.E1.m1.9.9.cmml" xref="S4.E1.m1.9.9">ùúÜ</ci></vector></apply><apply id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6"><divide id="S4.E1.m1.6.6.7.cmml" xref="S4.E1.m1.6.6"></divide><apply id="S4.E1.m1.4.4.4.cmml" xref="S4.E1.m1.4.4.4"><minus id="S4.E1.m1.4.4.4.5.cmml" xref="S4.E1.m1.4.4.4.5"></minus><apply id="S4.E1.m1.4.4.4.6.cmml" xref="S4.E1.m1.4.4.4.6"><times id="S4.E1.m1.4.4.4.6.1.cmml" xref="S4.E1.m1.4.4.4.6.1"></times><ci id="S4.E1.m1.4.4.4.6.2.cmml" xref="S4.E1.m1.4.4.4.6.2">ùêº</ci><vector id="S4.E1.m1.4.4.4.6.3.1.cmml" xref="S4.E1.m1.4.4.4.6.3.2"><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">ùë•</ci><ci id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2">ùë¶</ci><ci id="S4.E1.m1.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3">ùúÜ</ci></vector></apply><apply id="S4.E1.m1.4.4.4.7.cmml" xref="S4.E1.m1.4.4.4.7"><times id="S4.E1.m1.4.4.4.7.1.cmml" xref="S4.E1.m1.4.4.4.7.1"></times><apply id="S4.E1.m1.4.4.4.7.2.cmml" xref="S4.E1.m1.4.4.4.7.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.7.2.1.cmml" xref="S4.E1.m1.4.4.4.7.2">subscript</csymbol><ci id="S4.E1.m1.4.4.4.7.2.2.cmml" xref="S4.E1.m1.4.4.4.7.2.2">ùêº</ci><ci id="S4.E1.m1.4.4.4.7.2.3a.cmml" xref="S4.E1.m1.4.4.4.7.2.3"><mtext id="S4.E1.m1.4.4.4.7.2.3.cmml" mathsize="70%" xref="S4.E1.m1.4.4.4.7.2.3">black</mtext></ci></apply><ci id="S4.E1.m1.4.4.4.4.cmml" xref="S4.E1.m1.4.4.4.4">ùúÜ</ci></apply></apply><apply id="S4.E1.m1.6.6.6.cmml" xref="S4.E1.m1.6.6.6"><minus id="S4.E1.m1.6.6.6.3.cmml" xref="S4.E1.m1.6.6.6.3"></minus><apply id="S4.E1.m1.6.6.6.4.cmml" xref="S4.E1.m1.6.6.6.4"><times id="S4.E1.m1.6.6.6.4.1.cmml" xref="S4.E1.m1.6.6.6.4.1"></times><apply id="S4.E1.m1.6.6.6.4.2.cmml" xref="S4.E1.m1.6.6.6.4.2"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.4.2.1.cmml" xref="S4.E1.m1.6.6.6.4.2">subscript</csymbol><ci id="S4.E1.m1.6.6.6.4.2.2.cmml" xref="S4.E1.m1.6.6.6.4.2.2">ùêº</ci><ci id="S4.E1.m1.6.6.6.4.2.3a.cmml" xref="S4.E1.m1.6.6.6.4.2.3"><mtext id="S4.E1.m1.6.6.6.4.2.3.cmml" mathsize="70%" xref="S4.E1.m1.6.6.6.4.2.3">white</mtext></ci></apply><ci id="S4.E1.m1.5.5.5.1.cmml" xref="S4.E1.m1.5.5.5.1">ùúÜ</ci></apply><apply id="S4.E1.m1.6.6.6.5.cmml" xref="S4.E1.m1.6.6.6.5"><times id="S4.E1.m1.6.6.6.5.1.cmml" xref="S4.E1.m1.6.6.6.5.1"></times><apply id="S4.E1.m1.6.6.6.5.2.cmml" xref="S4.E1.m1.6.6.6.5.2"><csymbol cd="ambiguous" id="S4.E1.m1.6.6.6.5.2.1.cmml" xref="S4.E1.m1.6.6.6.5.2">subscript</csymbol><ci id="S4.E1.m1.6.6.6.5.2.2.cmml" xref="S4.E1.m1.6.6.6.5.2.2">ùêº</ci><ci id="S4.E1.m1.6.6.6.5.2.3a.cmml" xref="S4.E1.m1.6.6.6.5.2.3"><mtext id="S4.E1.m1.6.6.6.5.2.3.cmml" mathsize="70%" xref="S4.E1.m1.6.6.6.5.2.3">black</mtext></ci></apply><ci id="S4.E1.m1.6.6.6.2.cmml" xref="S4.E1.m1.6.6.6.2">ùúÜ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.10c">I_{\text{norm}}(x,y,\lambda)=\frac{I(x,y,\lambda)-I_{\text{black}}(\lambda)}{I%
_{\text{white}}(\lambda)-I_{\text{black}}(\lambda)}~{}~{},</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.10d">italic_I start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_x , italic_y , italic_Œª ) = divide start_ARG italic_I ( italic_x , italic_y , italic_Œª ) - italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª ) end_ARG start_ARG italic_I start_POSTSUBSCRIPT white end_POSTSUBSCRIPT ( italic_Œª ) - italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.10">where <math alttext="I_{\text{norm}}(x,y,\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p7.1.m1.3"><semantics id="S4.SS2.p7.1.m1.3a"><mrow id="S4.SS2.p7.1.m1.3.4" xref="S4.SS2.p7.1.m1.3.4.cmml"><msub id="S4.SS2.p7.1.m1.3.4.2" xref="S4.SS2.p7.1.m1.3.4.2.cmml"><mi id="S4.SS2.p7.1.m1.3.4.2.2" xref="S4.SS2.p7.1.m1.3.4.2.2.cmml">I</mi><mtext id="S4.SS2.p7.1.m1.3.4.2.3" xref="S4.SS2.p7.1.m1.3.4.2.3a.cmml">norm</mtext></msub><mo id="S4.SS2.p7.1.m1.3.4.1" xref="S4.SS2.p7.1.m1.3.4.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p7.1.m1.3.4.3.2" xref="S4.SS2.p7.1.m1.3.4.3.1.cmml"><mo id="S4.SS2.p7.1.m1.3.4.3.2.1" stretchy="false" xref="S4.SS2.p7.1.m1.3.4.3.1.cmml">(</mo><mi id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p7.1.m1.3.4.3.2.2" xref="S4.SS2.p7.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p7.1.m1.2.2" xref="S4.SS2.p7.1.m1.2.2.cmml">y</mi><mo id="S4.SS2.p7.1.m1.3.4.3.2.3" xref="S4.SS2.p7.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p7.1.m1.3.3" xref="S4.SS2.p7.1.m1.3.3.cmml">Œª</mi><mo id="S4.SS2.p7.1.m1.3.4.3.2.4" stretchy="false" xref="S4.SS2.p7.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.3b"><apply id="S4.SS2.p7.1.m1.3.4.cmml" xref="S4.SS2.p7.1.m1.3.4"><times id="S4.SS2.p7.1.m1.3.4.1.cmml" xref="S4.SS2.p7.1.m1.3.4.1"></times><apply id="S4.SS2.p7.1.m1.3.4.2.cmml" xref="S4.SS2.p7.1.m1.3.4.2"><csymbol cd="ambiguous" id="S4.SS2.p7.1.m1.3.4.2.1.cmml" xref="S4.SS2.p7.1.m1.3.4.2">subscript</csymbol><ci id="S4.SS2.p7.1.m1.3.4.2.2.cmml" xref="S4.SS2.p7.1.m1.3.4.2.2">ùêº</ci><ci id="S4.SS2.p7.1.m1.3.4.2.3a.cmml" xref="S4.SS2.p7.1.m1.3.4.2.3"><mtext id="S4.SS2.p7.1.m1.3.4.2.3.cmml" mathsize="70%" xref="S4.SS2.p7.1.m1.3.4.2.3">norm</mtext></ci></apply><vector id="S4.SS2.p7.1.m1.3.4.3.1.cmml" xref="S4.SS2.p7.1.m1.3.4.3.2"><ci id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1">ùë•</ci><ci id="S4.SS2.p7.1.m1.2.2.cmml" xref="S4.SS2.p7.1.m1.2.2">ùë¶</ci><ci id="S4.SS2.p7.1.m1.3.3.cmml" xref="S4.SS2.p7.1.m1.3.3">ùúÜ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.3c">I_{\text{norm}}(x,y,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.1.m1.3d">italic_I start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_x , italic_y , italic_Œª )</annotation></semantics></math> is the normalized hyperspectral image at pixel <math alttext="(x,y)" class="ltx_Math" display="inline" id="S4.SS2.p7.2.m2.2"><semantics id="S4.SS2.p7.2.m2.2a"><mrow id="S4.SS2.p7.2.m2.2.3.2" xref="S4.SS2.p7.2.m2.2.3.1.cmml"><mo id="S4.SS2.p7.2.m2.2.3.2.1" stretchy="false" xref="S4.SS2.p7.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS2.p7.2.m2.1.1" xref="S4.SS2.p7.2.m2.1.1.cmml">x</mi><mo id="S4.SS2.p7.2.m2.2.3.2.2" xref="S4.SS2.p7.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS2.p7.2.m2.2.2" xref="S4.SS2.p7.2.m2.2.2.cmml">y</mi><mo id="S4.SS2.p7.2.m2.2.3.2.3" stretchy="false" xref="S4.SS2.p7.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.2.m2.2b"><interval closure="open" id="S4.SS2.p7.2.m2.2.3.1.cmml" xref="S4.SS2.p7.2.m2.2.3.2"><ci id="S4.SS2.p7.2.m2.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1">ùë•</ci><ci id="S4.SS2.p7.2.m2.2.2.cmml" xref="S4.SS2.p7.2.m2.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.2.m2.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.2.m2.2d">( italic_x , italic_y )</annotation></semantics></math> and wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p7.3.m3.1"><semantics id="S4.SS2.p7.3.m3.1a"><mi id="S4.SS2.p7.3.m3.1.1" xref="S4.SS2.p7.3.m3.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.3.m3.1b"><ci id="S4.SS2.p7.3.m3.1.1.cmml" xref="S4.SS2.p7.3.m3.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.3.m3.1d">italic_Œª</annotation></semantics></math>, <math alttext="I(x,y,\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p7.4.m4.3"><semantics id="S4.SS2.p7.4.m4.3a"><mrow id="S4.SS2.p7.4.m4.3.4" xref="S4.SS2.p7.4.m4.3.4.cmml"><mi id="S4.SS2.p7.4.m4.3.4.2" xref="S4.SS2.p7.4.m4.3.4.2.cmml">I</mi><mo id="S4.SS2.p7.4.m4.3.4.1" xref="S4.SS2.p7.4.m4.3.4.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p7.4.m4.3.4.3.2" xref="S4.SS2.p7.4.m4.3.4.3.1.cmml"><mo id="S4.SS2.p7.4.m4.3.4.3.2.1" stretchy="false" xref="S4.SS2.p7.4.m4.3.4.3.1.cmml">(</mo><mi id="S4.SS2.p7.4.m4.1.1" xref="S4.SS2.p7.4.m4.1.1.cmml">x</mi><mo id="S4.SS2.p7.4.m4.3.4.3.2.2" xref="S4.SS2.p7.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p7.4.m4.2.2" xref="S4.SS2.p7.4.m4.2.2.cmml">y</mi><mo id="S4.SS2.p7.4.m4.3.4.3.2.3" xref="S4.SS2.p7.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p7.4.m4.3.3" xref="S4.SS2.p7.4.m4.3.3.cmml">Œª</mi><mo id="S4.SS2.p7.4.m4.3.4.3.2.4" stretchy="false" xref="S4.SS2.p7.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.4.m4.3b"><apply id="S4.SS2.p7.4.m4.3.4.cmml" xref="S4.SS2.p7.4.m4.3.4"><times id="S4.SS2.p7.4.m4.3.4.1.cmml" xref="S4.SS2.p7.4.m4.3.4.1"></times><ci id="S4.SS2.p7.4.m4.3.4.2.cmml" xref="S4.SS2.p7.4.m4.3.4.2">ùêº</ci><vector id="S4.SS2.p7.4.m4.3.4.3.1.cmml" xref="S4.SS2.p7.4.m4.3.4.3.2"><ci id="S4.SS2.p7.4.m4.1.1.cmml" xref="S4.SS2.p7.4.m4.1.1">ùë•</ci><ci id="S4.SS2.p7.4.m4.2.2.cmml" xref="S4.SS2.p7.4.m4.2.2">ùë¶</ci><ci id="S4.SS2.p7.4.m4.3.3.cmml" xref="S4.SS2.p7.4.m4.3.3">ùúÜ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.4.m4.3c">I(x,y,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.4.m4.3d">italic_I ( italic_x , italic_y , italic_Œª )</annotation></semantics></math> is the raw hyperspectral image at pixel <math alttext="(x,y)" class="ltx_Math" display="inline" id="S4.SS2.p7.5.m5.2"><semantics id="S4.SS2.p7.5.m5.2a"><mrow id="S4.SS2.p7.5.m5.2.3.2" xref="S4.SS2.p7.5.m5.2.3.1.cmml"><mo id="S4.SS2.p7.5.m5.2.3.2.1" stretchy="false" xref="S4.SS2.p7.5.m5.2.3.1.cmml">(</mo><mi id="S4.SS2.p7.5.m5.1.1" xref="S4.SS2.p7.5.m5.1.1.cmml">x</mi><mo id="S4.SS2.p7.5.m5.2.3.2.2" xref="S4.SS2.p7.5.m5.2.3.1.cmml">,</mo><mi id="S4.SS2.p7.5.m5.2.2" xref="S4.SS2.p7.5.m5.2.2.cmml">y</mi><mo id="S4.SS2.p7.5.m5.2.3.2.3" stretchy="false" xref="S4.SS2.p7.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.5.m5.2b"><interval closure="open" id="S4.SS2.p7.5.m5.2.3.1.cmml" xref="S4.SS2.p7.5.m5.2.3.2"><ci id="S4.SS2.p7.5.m5.1.1.cmml" xref="S4.SS2.p7.5.m5.1.1">ùë•</ci><ci id="S4.SS2.p7.5.m5.2.2.cmml" xref="S4.SS2.p7.5.m5.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.5.m5.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.5.m5.2d">( italic_x , italic_y )</annotation></semantics></math> and wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p7.6.m6.1"><semantics id="S4.SS2.p7.6.m6.1a"><mi id="S4.SS2.p7.6.m6.1.1" xref="S4.SS2.p7.6.m6.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.6.m6.1b"><ci id="S4.SS2.p7.6.m6.1.1.cmml" xref="S4.SS2.p7.6.m6.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.6.m6.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.6.m6.1d">italic_Œª</annotation></semantics></math>,
<math alttext="I_{\text{black}}(\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p7.7.m7.1"><semantics id="S4.SS2.p7.7.m7.1a"><mrow id="S4.SS2.p7.7.m7.1.2" xref="S4.SS2.p7.7.m7.1.2.cmml"><msub id="S4.SS2.p7.7.m7.1.2.2" xref="S4.SS2.p7.7.m7.1.2.2.cmml"><mi id="S4.SS2.p7.7.m7.1.2.2.2" xref="S4.SS2.p7.7.m7.1.2.2.2.cmml">I</mi><mtext id="S4.SS2.p7.7.m7.1.2.2.3" xref="S4.SS2.p7.7.m7.1.2.2.3a.cmml">black</mtext></msub><mo id="S4.SS2.p7.7.m7.1.2.1" xref="S4.SS2.p7.7.m7.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p7.7.m7.1.2.3.2" xref="S4.SS2.p7.7.m7.1.2.cmml"><mo id="S4.SS2.p7.7.m7.1.2.3.2.1" stretchy="false" xref="S4.SS2.p7.7.m7.1.2.cmml">(</mo><mi id="S4.SS2.p7.7.m7.1.1" xref="S4.SS2.p7.7.m7.1.1.cmml">Œª</mi><mo id="S4.SS2.p7.7.m7.1.2.3.2.2" stretchy="false" xref="S4.SS2.p7.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.7.m7.1b"><apply id="S4.SS2.p7.7.m7.1.2.cmml" xref="S4.SS2.p7.7.m7.1.2"><times id="S4.SS2.p7.7.m7.1.2.1.cmml" xref="S4.SS2.p7.7.m7.1.2.1"></times><apply id="S4.SS2.p7.7.m7.1.2.2.cmml" xref="S4.SS2.p7.7.m7.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p7.7.m7.1.2.2.1.cmml" xref="S4.SS2.p7.7.m7.1.2.2">subscript</csymbol><ci id="S4.SS2.p7.7.m7.1.2.2.2.cmml" xref="S4.SS2.p7.7.m7.1.2.2.2">ùêº</ci><ci id="S4.SS2.p7.7.m7.1.2.2.3a.cmml" xref="S4.SS2.p7.7.m7.1.2.2.3"><mtext id="S4.SS2.p7.7.m7.1.2.2.3.cmml" mathsize="70%" xref="S4.SS2.p7.7.m7.1.2.2.3">black</mtext></ci></apply><ci id="S4.SS2.p7.7.m7.1.1.cmml" xref="S4.SS2.p7.7.m7.1.1">ùúÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.7.m7.1c">I_{\text{black}}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.7.m7.1d">italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª )</annotation></semantics></math> is the black reference image at wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p7.8.m8.1"><semantics id="S4.SS2.p7.8.m8.1a"><mi id="S4.SS2.p7.8.m8.1.1" xref="S4.SS2.p7.8.m8.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.8.m8.1b"><ci id="S4.SS2.p7.8.m8.1.1.cmml" xref="S4.SS2.p7.8.m8.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.8.m8.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.8.m8.1d">italic_Œª</annotation></semantics></math>,
<math alttext="I_{\text{white}}(\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p7.9.m9.1"><semantics id="S4.SS2.p7.9.m9.1a"><mrow id="S4.SS2.p7.9.m9.1.2" xref="S4.SS2.p7.9.m9.1.2.cmml"><msub id="S4.SS2.p7.9.m9.1.2.2" xref="S4.SS2.p7.9.m9.1.2.2.cmml"><mi id="S4.SS2.p7.9.m9.1.2.2.2" xref="S4.SS2.p7.9.m9.1.2.2.2.cmml">I</mi><mtext id="S4.SS2.p7.9.m9.1.2.2.3" xref="S4.SS2.p7.9.m9.1.2.2.3a.cmml">white</mtext></msub><mo id="S4.SS2.p7.9.m9.1.2.1" xref="S4.SS2.p7.9.m9.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p7.9.m9.1.2.3.2" xref="S4.SS2.p7.9.m9.1.2.cmml"><mo id="S4.SS2.p7.9.m9.1.2.3.2.1" stretchy="false" xref="S4.SS2.p7.9.m9.1.2.cmml">(</mo><mi id="S4.SS2.p7.9.m9.1.1" xref="S4.SS2.p7.9.m9.1.1.cmml">Œª</mi><mo id="S4.SS2.p7.9.m9.1.2.3.2.2" stretchy="false" xref="S4.SS2.p7.9.m9.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.9.m9.1b"><apply id="S4.SS2.p7.9.m9.1.2.cmml" xref="S4.SS2.p7.9.m9.1.2"><times id="S4.SS2.p7.9.m9.1.2.1.cmml" xref="S4.SS2.p7.9.m9.1.2.1"></times><apply id="S4.SS2.p7.9.m9.1.2.2.cmml" xref="S4.SS2.p7.9.m9.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p7.9.m9.1.2.2.1.cmml" xref="S4.SS2.p7.9.m9.1.2.2">subscript</csymbol><ci id="S4.SS2.p7.9.m9.1.2.2.2.cmml" xref="S4.SS2.p7.9.m9.1.2.2.2">ùêº</ci><ci id="S4.SS2.p7.9.m9.1.2.2.3a.cmml" xref="S4.SS2.p7.9.m9.1.2.2.3"><mtext id="S4.SS2.p7.9.m9.1.2.2.3.cmml" mathsize="70%" xref="S4.SS2.p7.9.m9.1.2.2.3">white</mtext></ci></apply><ci id="S4.SS2.p7.9.m9.1.1.cmml" xref="S4.SS2.p7.9.m9.1.1">ùúÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.9.m9.1c">I_{\text{white}}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.9.m9.1d">italic_I start_POSTSUBSCRIPT white end_POSTSUBSCRIPT ( italic_Œª )</annotation></semantics></math> is the white reference image at wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p7.10.m10.1"><semantics id="S4.SS2.p7.10.m10.1a"><mi id="S4.SS2.p7.10.m10.1.1" xref="S4.SS2.p7.10.m10.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.10.m10.1b"><ci id="S4.SS2.p7.10.m10.1.1.cmml" xref="S4.SS2.p7.10.m10.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.10.m10.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p7.10.m10.1d">italic_Œª</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">The black reference <math alttext="I_{\text{black}}(\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p8.1.m1.1"><semantics id="S4.SS2.p8.1.m1.1a"><mrow id="S4.SS2.p8.1.m1.1.2" xref="S4.SS2.p8.1.m1.1.2.cmml"><msub id="S4.SS2.p8.1.m1.1.2.2" xref="S4.SS2.p8.1.m1.1.2.2.cmml"><mi id="S4.SS2.p8.1.m1.1.2.2.2" xref="S4.SS2.p8.1.m1.1.2.2.2.cmml">I</mi><mtext id="S4.SS2.p8.1.m1.1.2.2.3" xref="S4.SS2.p8.1.m1.1.2.2.3a.cmml">black</mtext></msub><mo id="S4.SS2.p8.1.m1.1.2.1" xref="S4.SS2.p8.1.m1.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p8.1.m1.1.2.3.2" xref="S4.SS2.p8.1.m1.1.2.cmml"><mo id="S4.SS2.p8.1.m1.1.2.3.2.1" stretchy="false" xref="S4.SS2.p8.1.m1.1.2.cmml">(</mo><mi id="S4.SS2.p8.1.m1.1.1" xref="S4.SS2.p8.1.m1.1.1.cmml">Œª</mi><mo id="S4.SS2.p8.1.m1.1.2.3.2.2" stretchy="false" xref="S4.SS2.p8.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p8.1.m1.1b"><apply id="S4.SS2.p8.1.m1.1.2.cmml" xref="S4.SS2.p8.1.m1.1.2"><times id="S4.SS2.p8.1.m1.1.2.1.cmml" xref="S4.SS2.p8.1.m1.1.2.1"></times><apply id="S4.SS2.p8.1.m1.1.2.2.cmml" xref="S4.SS2.p8.1.m1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p8.1.m1.1.2.2.1.cmml" xref="S4.SS2.p8.1.m1.1.2.2">subscript</csymbol><ci id="S4.SS2.p8.1.m1.1.2.2.2.cmml" xref="S4.SS2.p8.1.m1.1.2.2.2">ùêº</ci><ci id="S4.SS2.p8.1.m1.1.2.2.3a.cmml" xref="S4.SS2.p8.1.m1.1.2.2.3"><mtext id="S4.SS2.p8.1.m1.1.2.2.3.cmml" mathsize="70%" xref="S4.SS2.p8.1.m1.1.2.2.3">black</mtext></ci></apply><ci id="S4.SS2.p8.1.m1.1.1.cmml" xref="S4.SS2.p8.1.m1.1.1">ùúÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p8.1.m1.1c">I_{\text{black}}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p8.1.m1.1d">italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª )</annotation></semantics></math> was acquired, by closing the shutter of the camera and capturing 1000 lines, which were then averaged to create the black reference. For the maximum absolute pixel value the HS camera can capture, the assumption was made that it can be approximated by calculating the maximum pixel value within the train set. Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.E2" title="In 4.2 Acquisition Pipeline ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> provides an approximation of Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.E1" title="In 4.2 Acquisition Pipeline ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">1</span></a> that does not require an expensive white reflection target:</p>
</div>
<div class="ltx_para" id="S4.SS2.p9">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{\text{norm}}(x,y,\lambda)=\frac{I(x,y,\lambda)-I_{\text{black}}(\lambda)}{%
\text{M}}~{}~{}," class="ltx_Math" display="block" id="S4.E2.m1.8"><semantics id="S4.E2.m1.8a"><mrow id="S4.E2.m1.8.8.1" xref="S4.E2.m1.8.8.1.1.cmml"><mrow id="S4.E2.m1.8.8.1.1" xref="S4.E2.m1.8.8.1.1.cmml"><mrow id="S4.E2.m1.8.8.1.1.2" xref="S4.E2.m1.8.8.1.1.2.cmml"><msub id="S4.E2.m1.8.8.1.1.2.2" xref="S4.E2.m1.8.8.1.1.2.2.cmml"><mi id="S4.E2.m1.8.8.1.1.2.2.2" xref="S4.E2.m1.8.8.1.1.2.2.2.cmml">I</mi><mtext id="S4.E2.m1.8.8.1.1.2.2.3" xref="S4.E2.m1.8.8.1.1.2.2.3a.cmml">norm</mtext></msub><mo id="S4.E2.m1.8.8.1.1.2.1" xref="S4.E2.m1.8.8.1.1.2.1.cmml">‚Å¢</mo><mrow id="S4.E2.m1.8.8.1.1.2.3.2" xref="S4.E2.m1.8.8.1.1.2.3.1.cmml"><mo id="S4.E2.m1.8.8.1.1.2.3.2.1" stretchy="false" xref="S4.E2.m1.8.8.1.1.2.3.1.cmml">(</mo><mi id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml">x</mi><mo id="S4.E2.m1.8.8.1.1.2.3.2.2" xref="S4.E2.m1.8.8.1.1.2.3.1.cmml">,</mo><mi id="S4.E2.m1.6.6" xref="S4.E2.m1.6.6.cmml">y</mi><mo id="S4.E2.m1.8.8.1.1.2.3.2.3" xref="S4.E2.m1.8.8.1.1.2.3.1.cmml">,</mo><mi id="S4.E2.m1.7.7" xref="S4.E2.m1.7.7.cmml">Œª</mi><mo id="S4.E2.m1.8.8.1.1.2.3.2.4" stretchy="false" xref="S4.E2.m1.8.8.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.8.8.1.1.1" xref="S4.E2.m1.8.8.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml"><mrow id="S4.E2.m1.4.4.4" xref="S4.E2.m1.4.4.4.cmml"><mrow id="S4.E2.m1.4.4.4.6" xref="S4.E2.m1.4.4.4.6.cmml"><mi id="S4.E2.m1.4.4.4.6.2" xref="S4.E2.m1.4.4.4.6.2.cmml">I</mi><mo id="S4.E2.m1.4.4.4.6.1" xref="S4.E2.m1.4.4.4.6.1.cmml">‚Å¢</mo><mrow id="S4.E2.m1.4.4.4.6.3.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml"><mo id="S4.E2.m1.4.4.4.6.3.2.1" stretchy="false" xref="S4.E2.m1.4.4.4.6.3.1.cmml">(</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">x</mi><mo id="S4.E2.m1.4.4.4.6.3.2.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml">,</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">y</mi><mo id="S4.E2.m1.4.4.4.6.3.2.3" xref="S4.E2.m1.4.4.4.6.3.1.cmml">,</mo><mi id="S4.E2.m1.3.3.3.3" xref="S4.E2.m1.3.3.3.3.cmml">Œª</mi><mo id="S4.E2.m1.4.4.4.6.3.2.4" stretchy="false" xref="S4.E2.m1.4.4.4.6.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.4.4.4.5" xref="S4.E2.m1.4.4.4.5.cmml">‚àí</mo><mrow id="S4.E2.m1.4.4.4.7" xref="S4.E2.m1.4.4.4.7.cmml"><msub id="S4.E2.m1.4.4.4.7.2" xref="S4.E2.m1.4.4.4.7.2.cmml"><mi id="S4.E2.m1.4.4.4.7.2.2" xref="S4.E2.m1.4.4.4.7.2.2.cmml">I</mi><mtext id="S4.E2.m1.4.4.4.7.2.3" xref="S4.E2.m1.4.4.4.7.2.3a.cmml">black</mtext></msub><mo id="S4.E2.m1.4.4.4.7.1" xref="S4.E2.m1.4.4.4.7.1.cmml">‚Å¢</mo><mrow id="S4.E2.m1.4.4.4.7.3.2" xref="S4.E2.m1.4.4.4.7.cmml"><mo id="S4.E2.m1.4.4.4.7.3.2.1" stretchy="false" xref="S4.E2.m1.4.4.4.7.cmml">(</mo><mi id="S4.E2.m1.4.4.4.4" xref="S4.E2.m1.4.4.4.4.cmml">Œª</mi><mo id="S4.E2.m1.4.4.4.7.3.2.2" stretchy="false" xref="S4.E2.m1.4.4.4.7.cmml">)</mo></mrow></mrow></mrow><mtext id="S4.E2.m1.4.4.6" xref="S4.E2.m1.4.4.6a.cmml">M</mtext></mfrac></mrow><mo id="S4.E2.m1.8.8.1.2" lspace="0.660em" xref="S4.E2.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.8b"><apply id="S4.E2.m1.8.8.1.1.cmml" xref="S4.E2.m1.8.8.1"><eq id="S4.E2.m1.8.8.1.1.1.cmml" xref="S4.E2.m1.8.8.1.1.1"></eq><apply id="S4.E2.m1.8.8.1.1.2.cmml" xref="S4.E2.m1.8.8.1.1.2"><times id="S4.E2.m1.8.8.1.1.2.1.cmml" xref="S4.E2.m1.8.8.1.1.2.1"></times><apply id="S4.E2.m1.8.8.1.1.2.2.cmml" xref="S4.E2.m1.8.8.1.1.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.8.8.1.1.2.2.1.cmml" xref="S4.E2.m1.8.8.1.1.2.2">subscript</csymbol><ci id="S4.E2.m1.8.8.1.1.2.2.2.cmml" xref="S4.E2.m1.8.8.1.1.2.2.2">ùêº</ci><ci id="S4.E2.m1.8.8.1.1.2.2.3a.cmml" xref="S4.E2.m1.8.8.1.1.2.2.3"><mtext id="S4.E2.m1.8.8.1.1.2.2.3.cmml" mathsize="70%" xref="S4.E2.m1.8.8.1.1.2.2.3">norm</mtext></ci></apply><vector id="S4.E2.m1.8.8.1.1.2.3.1.cmml" xref="S4.E2.m1.8.8.1.1.2.3.2"><ci id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5">ùë•</ci><ci id="S4.E2.m1.6.6.cmml" xref="S4.E2.m1.6.6">ùë¶</ci><ci id="S4.E2.m1.7.7.cmml" xref="S4.E2.m1.7.7">ùúÜ</ci></vector></apply><apply id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4"><divide id="S4.E2.m1.4.4.5.cmml" xref="S4.E2.m1.4.4"></divide><apply id="S4.E2.m1.4.4.4.cmml" xref="S4.E2.m1.4.4.4"><minus id="S4.E2.m1.4.4.4.5.cmml" xref="S4.E2.m1.4.4.4.5"></minus><apply id="S4.E2.m1.4.4.4.6.cmml" xref="S4.E2.m1.4.4.4.6"><times id="S4.E2.m1.4.4.4.6.1.cmml" xref="S4.E2.m1.4.4.4.6.1"></times><ci id="S4.E2.m1.4.4.4.6.2.cmml" xref="S4.E2.m1.4.4.4.6.2">ùêº</ci><vector id="S4.E2.m1.4.4.4.6.3.1.cmml" xref="S4.E2.m1.4.4.4.6.3.2"><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">ùë•</ci><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">ùë¶</ci><ci id="S4.E2.m1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.3.3">ùúÜ</ci></vector></apply><apply id="S4.E2.m1.4.4.4.7.cmml" xref="S4.E2.m1.4.4.4.7"><times id="S4.E2.m1.4.4.4.7.1.cmml" xref="S4.E2.m1.4.4.4.7.1"></times><apply id="S4.E2.m1.4.4.4.7.2.cmml" xref="S4.E2.m1.4.4.4.7.2"><csymbol cd="ambiguous" id="S4.E2.m1.4.4.4.7.2.1.cmml" xref="S4.E2.m1.4.4.4.7.2">subscript</csymbol><ci id="S4.E2.m1.4.4.4.7.2.2.cmml" xref="S4.E2.m1.4.4.4.7.2.2">ùêº</ci><ci id="S4.E2.m1.4.4.4.7.2.3a.cmml" xref="S4.E2.m1.4.4.4.7.2.3"><mtext id="S4.E2.m1.4.4.4.7.2.3.cmml" mathsize="70%" xref="S4.E2.m1.4.4.4.7.2.3">black</mtext></ci></apply><ci id="S4.E2.m1.4.4.4.4.cmml" xref="S4.E2.m1.4.4.4.4">ùúÜ</ci></apply></apply><ci id="S4.E2.m1.4.4.6a.cmml" xref="S4.E2.m1.4.4.6"><mtext id="S4.E2.m1.4.4.6.cmml" xref="S4.E2.m1.4.4.6">M</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.8c">I_{\text{norm}}(x,y,\lambda)=\frac{I(x,y,\lambda)-I_{\text{black}}(\lambda)}{%
\text{M}}~{}~{},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.8d">italic_I start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_x , italic_y , italic_Œª ) = divide start_ARG italic_I ( italic_x , italic_y , italic_Œª ) - italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª ) end_ARG start_ARG M end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.9">where <math alttext="I_{\text{norm}}(x,y,\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p10.1.m1.3"><semantics id="S4.SS2.p10.1.m1.3a"><mrow id="S4.SS2.p10.1.m1.3.4" xref="S4.SS2.p10.1.m1.3.4.cmml"><msub id="S4.SS2.p10.1.m1.3.4.2" xref="S4.SS2.p10.1.m1.3.4.2.cmml"><mi id="S4.SS2.p10.1.m1.3.4.2.2" xref="S4.SS2.p10.1.m1.3.4.2.2.cmml">I</mi><mtext id="S4.SS2.p10.1.m1.3.4.2.3" xref="S4.SS2.p10.1.m1.3.4.2.3a.cmml">norm</mtext></msub><mo id="S4.SS2.p10.1.m1.3.4.1" xref="S4.SS2.p10.1.m1.3.4.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p10.1.m1.3.4.3.2" xref="S4.SS2.p10.1.m1.3.4.3.1.cmml"><mo id="S4.SS2.p10.1.m1.3.4.3.2.1" stretchy="false" xref="S4.SS2.p10.1.m1.3.4.3.1.cmml">(</mo><mi id="S4.SS2.p10.1.m1.1.1" xref="S4.SS2.p10.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p10.1.m1.3.4.3.2.2" xref="S4.SS2.p10.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p10.1.m1.2.2" xref="S4.SS2.p10.1.m1.2.2.cmml">y</mi><mo id="S4.SS2.p10.1.m1.3.4.3.2.3" xref="S4.SS2.p10.1.m1.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p10.1.m1.3.3" xref="S4.SS2.p10.1.m1.3.3.cmml">Œª</mi><mo id="S4.SS2.p10.1.m1.3.4.3.2.4" stretchy="false" xref="S4.SS2.p10.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.1.m1.3b"><apply id="S4.SS2.p10.1.m1.3.4.cmml" xref="S4.SS2.p10.1.m1.3.4"><times id="S4.SS2.p10.1.m1.3.4.1.cmml" xref="S4.SS2.p10.1.m1.3.4.1"></times><apply id="S4.SS2.p10.1.m1.3.4.2.cmml" xref="S4.SS2.p10.1.m1.3.4.2"><csymbol cd="ambiguous" id="S4.SS2.p10.1.m1.3.4.2.1.cmml" xref="S4.SS2.p10.1.m1.3.4.2">subscript</csymbol><ci id="S4.SS2.p10.1.m1.3.4.2.2.cmml" xref="S4.SS2.p10.1.m1.3.4.2.2">ùêº</ci><ci id="S4.SS2.p10.1.m1.3.4.2.3a.cmml" xref="S4.SS2.p10.1.m1.3.4.2.3"><mtext id="S4.SS2.p10.1.m1.3.4.2.3.cmml" mathsize="70%" xref="S4.SS2.p10.1.m1.3.4.2.3">norm</mtext></ci></apply><vector id="S4.SS2.p10.1.m1.3.4.3.1.cmml" xref="S4.SS2.p10.1.m1.3.4.3.2"><ci id="S4.SS2.p10.1.m1.1.1.cmml" xref="S4.SS2.p10.1.m1.1.1">ùë•</ci><ci id="S4.SS2.p10.1.m1.2.2.cmml" xref="S4.SS2.p10.1.m1.2.2">ùë¶</ci><ci id="S4.SS2.p10.1.m1.3.3.cmml" xref="S4.SS2.p10.1.m1.3.3">ùúÜ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.1.m1.3c">I_{\text{norm}}(x,y,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.1.m1.3d">italic_I start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_x , italic_y , italic_Œª )</annotation></semantics></math> is the normalized hyperspectral image at pixel <math alttext="(x,y)" class="ltx_Math" display="inline" id="S4.SS2.p10.2.m2.2"><semantics id="S4.SS2.p10.2.m2.2a"><mrow id="S4.SS2.p10.2.m2.2.3.2" xref="S4.SS2.p10.2.m2.2.3.1.cmml"><mo id="S4.SS2.p10.2.m2.2.3.2.1" stretchy="false" xref="S4.SS2.p10.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS2.p10.2.m2.1.1" xref="S4.SS2.p10.2.m2.1.1.cmml">x</mi><mo id="S4.SS2.p10.2.m2.2.3.2.2" xref="S4.SS2.p10.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS2.p10.2.m2.2.2" xref="S4.SS2.p10.2.m2.2.2.cmml">y</mi><mo id="S4.SS2.p10.2.m2.2.3.2.3" stretchy="false" xref="S4.SS2.p10.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.2.m2.2b"><interval closure="open" id="S4.SS2.p10.2.m2.2.3.1.cmml" xref="S4.SS2.p10.2.m2.2.3.2"><ci id="S4.SS2.p10.2.m2.1.1.cmml" xref="S4.SS2.p10.2.m2.1.1">ùë•</ci><ci id="S4.SS2.p10.2.m2.2.2.cmml" xref="S4.SS2.p10.2.m2.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.2.m2.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.2.m2.2d">( italic_x , italic_y )</annotation></semantics></math> and wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p10.3.m3.1"><semantics id="S4.SS2.p10.3.m3.1a"><mi id="S4.SS2.p10.3.m3.1.1" xref="S4.SS2.p10.3.m3.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.3.m3.1b"><ci id="S4.SS2.p10.3.m3.1.1.cmml" xref="S4.SS2.p10.3.m3.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.3.m3.1d">italic_Œª</annotation></semantics></math>,
<math alttext="I(x,y,\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p10.4.m4.3"><semantics id="S4.SS2.p10.4.m4.3a"><mrow id="S4.SS2.p10.4.m4.3.4" xref="S4.SS2.p10.4.m4.3.4.cmml"><mi id="S4.SS2.p10.4.m4.3.4.2" xref="S4.SS2.p10.4.m4.3.4.2.cmml">I</mi><mo id="S4.SS2.p10.4.m4.3.4.1" xref="S4.SS2.p10.4.m4.3.4.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p10.4.m4.3.4.3.2" xref="S4.SS2.p10.4.m4.3.4.3.1.cmml"><mo id="S4.SS2.p10.4.m4.3.4.3.2.1" stretchy="false" xref="S4.SS2.p10.4.m4.3.4.3.1.cmml">(</mo><mi id="S4.SS2.p10.4.m4.1.1" xref="S4.SS2.p10.4.m4.1.1.cmml">x</mi><mo id="S4.SS2.p10.4.m4.3.4.3.2.2" xref="S4.SS2.p10.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p10.4.m4.2.2" xref="S4.SS2.p10.4.m4.2.2.cmml">y</mi><mo id="S4.SS2.p10.4.m4.3.4.3.2.3" xref="S4.SS2.p10.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS2.p10.4.m4.3.3" xref="S4.SS2.p10.4.m4.3.3.cmml">Œª</mi><mo id="S4.SS2.p10.4.m4.3.4.3.2.4" stretchy="false" xref="S4.SS2.p10.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.4.m4.3b"><apply id="S4.SS2.p10.4.m4.3.4.cmml" xref="S4.SS2.p10.4.m4.3.4"><times id="S4.SS2.p10.4.m4.3.4.1.cmml" xref="S4.SS2.p10.4.m4.3.4.1"></times><ci id="S4.SS2.p10.4.m4.3.4.2.cmml" xref="S4.SS2.p10.4.m4.3.4.2">ùêº</ci><vector id="S4.SS2.p10.4.m4.3.4.3.1.cmml" xref="S4.SS2.p10.4.m4.3.4.3.2"><ci id="S4.SS2.p10.4.m4.1.1.cmml" xref="S4.SS2.p10.4.m4.1.1">ùë•</ci><ci id="S4.SS2.p10.4.m4.2.2.cmml" xref="S4.SS2.p10.4.m4.2.2">ùë¶</ci><ci id="S4.SS2.p10.4.m4.3.3.cmml" xref="S4.SS2.p10.4.m4.3.3">ùúÜ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.4.m4.3c">I(x,y,\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.4.m4.3d">italic_I ( italic_x , italic_y , italic_Œª )</annotation></semantics></math> is the raw hyperspectral image at pixel <math alttext="(x,y)" class="ltx_Math" display="inline" id="S4.SS2.p10.5.m5.2"><semantics id="S4.SS2.p10.5.m5.2a"><mrow id="S4.SS2.p10.5.m5.2.3.2" xref="S4.SS2.p10.5.m5.2.3.1.cmml"><mo id="S4.SS2.p10.5.m5.2.3.2.1" stretchy="false" xref="S4.SS2.p10.5.m5.2.3.1.cmml">(</mo><mi id="S4.SS2.p10.5.m5.1.1" xref="S4.SS2.p10.5.m5.1.1.cmml">x</mi><mo id="S4.SS2.p10.5.m5.2.3.2.2" xref="S4.SS2.p10.5.m5.2.3.1.cmml">,</mo><mi id="S4.SS2.p10.5.m5.2.2" xref="S4.SS2.p10.5.m5.2.2.cmml">y</mi><mo id="S4.SS2.p10.5.m5.2.3.2.3" stretchy="false" xref="S4.SS2.p10.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.5.m5.2b"><interval closure="open" id="S4.SS2.p10.5.m5.2.3.1.cmml" xref="S4.SS2.p10.5.m5.2.3.2"><ci id="S4.SS2.p10.5.m5.1.1.cmml" xref="S4.SS2.p10.5.m5.1.1">ùë•</ci><ci id="S4.SS2.p10.5.m5.2.2.cmml" xref="S4.SS2.p10.5.m5.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.5.m5.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.5.m5.2d">( italic_x , italic_y )</annotation></semantics></math> and wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p10.6.m6.1"><semantics id="S4.SS2.p10.6.m6.1a"><mi id="S4.SS2.p10.6.m6.1.1" xref="S4.SS2.p10.6.m6.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.6.m6.1b"><ci id="S4.SS2.p10.6.m6.1.1.cmml" xref="S4.SS2.p10.6.m6.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.6.m6.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.6.m6.1d">italic_Œª</annotation></semantics></math>,
<math alttext="I_{\text{black}}(\lambda)" class="ltx_Math" display="inline" id="S4.SS2.p10.7.m7.1"><semantics id="S4.SS2.p10.7.m7.1a"><mrow id="S4.SS2.p10.7.m7.1.2" xref="S4.SS2.p10.7.m7.1.2.cmml"><msub id="S4.SS2.p10.7.m7.1.2.2" xref="S4.SS2.p10.7.m7.1.2.2.cmml"><mi id="S4.SS2.p10.7.m7.1.2.2.2" xref="S4.SS2.p10.7.m7.1.2.2.2.cmml">I</mi><mtext id="S4.SS2.p10.7.m7.1.2.2.3" xref="S4.SS2.p10.7.m7.1.2.2.3a.cmml">black</mtext></msub><mo id="S4.SS2.p10.7.m7.1.2.1" xref="S4.SS2.p10.7.m7.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS2.p10.7.m7.1.2.3.2" xref="S4.SS2.p10.7.m7.1.2.cmml"><mo id="S4.SS2.p10.7.m7.1.2.3.2.1" stretchy="false" xref="S4.SS2.p10.7.m7.1.2.cmml">(</mo><mi id="S4.SS2.p10.7.m7.1.1" xref="S4.SS2.p10.7.m7.1.1.cmml">Œª</mi><mo id="S4.SS2.p10.7.m7.1.2.3.2.2" stretchy="false" xref="S4.SS2.p10.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.7.m7.1b"><apply id="S4.SS2.p10.7.m7.1.2.cmml" xref="S4.SS2.p10.7.m7.1.2"><times id="S4.SS2.p10.7.m7.1.2.1.cmml" xref="S4.SS2.p10.7.m7.1.2.1"></times><apply id="S4.SS2.p10.7.m7.1.2.2.cmml" xref="S4.SS2.p10.7.m7.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p10.7.m7.1.2.2.1.cmml" xref="S4.SS2.p10.7.m7.1.2.2">subscript</csymbol><ci id="S4.SS2.p10.7.m7.1.2.2.2.cmml" xref="S4.SS2.p10.7.m7.1.2.2.2">ùêº</ci><ci id="S4.SS2.p10.7.m7.1.2.2.3a.cmml" xref="S4.SS2.p10.7.m7.1.2.2.3"><mtext id="S4.SS2.p10.7.m7.1.2.2.3.cmml" mathsize="70%" xref="S4.SS2.p10.7.m7.1.2.2.3">black</mtext></ci></apply><ci id="S4.SS2.p10.7.m7.1.1.cmml" xref="S4.SS2.p10.7.m7.1.1">ùúÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.7.m7.1c">I_{\text{black}}(\lambda)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.7.m7.1d">italic_I start_POSTSUBSCRIPT black end_POSTSUBSCRIPT ( italic_Œª )</annotation></semantics></math> is the black reference image at wavelength <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.SS2.p10.8.m8.1"><semantics id="S4.SS2.p10.8.m8.1a"><mi id="S4.SS2.p10.8.m8.1.1" xref="S4.SS2.p10.8.m8.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.8.m8.1b"><ci id="S4.SS2.p10.8.m8.1.1.cmml" xref="S4.SS2.p10.8.m8.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.8.m8.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.8.m8.1d">italic_Œª</annotation></semantics></math>,
<math alttext="M" class="ltx_Math" display="inline" id="S4.SS2.p10.9.m9.1"><semantics id="S4.SS2.p10.9.m9.1a"><mi id="S4.SS2.p10.9.m9.1.1" xref="S4.SS2.p10.9.m9.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p10.9.m9.1b"><ci id="S4.SS2.p10.9.m9.1.1.cmml" xref="S4.SS2.p10.9.m9.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p10.9.m9.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p10.9.m9.1d">italic_M</annotation></semantics></math> is the dataset‚Äôs maximum pixel value.</p>
</div>
<div class="ltx_para" id="S4.SS2.p11">
<p class="ltx_p" id="S4.SS2.p11.1">It should be noted that in HS image processing applications it is highly recommended to use the aforementioned steps for <span class="ltx_text ltx_font_italic" id="S4.SS2.p11.1.1">spectral calibration</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p11.1.2">normalization</span> which often include expensive equipment and dependencies on the hardware manufacturer. However, as shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6" title="6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>, even if we totally ignore those steps, the learning performance of the proposed deep learning algorithm is not affected.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmremarkx1.1.1.1">Remark</span></span><span class="ltx_text ltx_font_bold" id="Thmremarkx1.2.2"> </span>(Calibration on Training)<span class="ltx_text ltx_font_bold" id="Thmremarkx1.3.3">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx1.p1">
<p class="ltx_p" id="Thmremarkx1.p1.1"><span class="ltx_text ltx_font_italic" id="Thmremarkx1.p1.1.1">The spectral calibration and the normalization as operations are a sequence of matrix multiplications, it seems by our experiments, that this transformation can be learned directly in the training process.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Ground truth mask generation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As in every supervised learning application, a set of ground truth labels is needed to ensure successful training of the neural network model. To this end, an AI-assisted methodology was deployed for the generation of the binary masks, presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.F2" title="Figure 2 ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">2</span></a>, which later on will be utilised as training labels.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSSx1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Semi-Automated Segmentation</h4>
<div class="ltx_para" id="S4.SS3.SSSx1.p1">
<p class="ltx_p" id="S4.SS3.SSSx1.p1.1">The first step in the proposed methodology, is the creation of a false-colour RGB version of the HS image. To this end, the Standard Deviation of each channel in the original image was calculated, as a measure of its contrast. The three channels with the highest contrast were selected and sorted in ascending order of wavelength, for each image, in order to create the false-colour RGB image. An adaptive histogram stretching algorithm was, also, applied to the respective RGB versions in order to further increase the contrast and make the objects‚Äô edges as sharp as possible without altering the spatial content of each image.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSSx1.p2">
<p class="ltx_p" id="S4.SS3.SSSx1.p2.1">The false-colour, histogram stretched images were subsequently utilised for the generation of segmentation masks. A ViT model, namely SAM (Segment Anything) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib39" title="">39</a>]</cite>, for the semi-automated mask generation task. In detail, positive (pixel to be included in the mask) and negative (pixel to be excluded from the mask) points were given as prompts to the model in order to generate a first estimation of the mask. The predicted mask was then visually inspected and refined, when needed, aiming for precision maximization at the boundaries of the object. This procedure was repeated for every image and every object depicted within an image of the dataset and the final results can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.F2" title="Figure 2 ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S4.F4.g1" src="extracted/5862626/imgs/architecture.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The high-level architecture of the proposed Pixel-wise 1D Convolutional Hyperspectral (P1CH) Classifier.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSSx2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Labeling - Raman</h4>
<div class="ltx_para" id="S4.SS3.SSSx2.p1">
<p class="ltx_p" id="S4.SS3.SSSx2.p1.1">The final step of the ground truth generation is to assign a class to each of the aforementioned masks. To this end, Raman Spectroscopy was employed.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSSx2.p2">
<p class="ltx_p" id="S4.SS3.SSSx2.p2.1">Raman spectroscopy is a powerful analytical technique used to observe vibrational, rotational, and other low-frequency modes in a system. It relies on inelastic scattering, or Raman scattering, of monochromatic light, typically from a laser. When light interacts with molecules, vibrations, or other excitations in the system occur shifting up or down the energy of the laser, creating peaks in the acquired spectrum and hence providing a fingerprint by which molecules can be identified <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib41" title="">41</a>]</cite>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.F3" title="Figure 3 ‚Ä£ 4.1 Dataset description ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">3</span></a>, an example of the spectrum for each of the 4 plastic types is presented, where the red marked peaks indicate the existence of each polymer in under examination the plastic sample.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSSx2.p3">
<p class="ltx_p" id="S4.SS3.SSSx2.p3.1">In this manner, each sample annotated in the previous step was individually scanned with the Raman equipment and it spectrum was analysed in order to identify the indicative, for each class, peaks. The results of the Raman Spectroscopy analysis, were used as the class of each of the aforementioned masks.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Model Architecture &amp; Training</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_italic" id="S5.SS1.1.1">Architecture</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The proposed hyperspectral image classification model uses a <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">1D CNN architecture</span> to capture and process spectral information. The architecture uses a) 2 convolutional layers, b) 2 residual blocks, and c) 2 fully connected layers to accurately classify hyperspectral data. The architecture of the proposed Pixel-wise 1D Convolutional Hyperspectral Classifier is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.F4" title="Figure 4 ‚Ä£ Semi-Automated Segmentation ‚Ä£ 4.3 Ground truth mask generation ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Convolutions:</span> The input pixel of shape <math alttext="(1\times 224)" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml"><mo id="S5.SS1.p2.1.m1.1.1.1.2" stretchy="false" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.p2.1.m1.1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml"><mn id="S5.SS1.p2.1.m1.1.1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.1.1.2.cmml">1</mn><mo id="S5.SS1.p2.1.m1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p2.1.m1.1.1.1.1.1.cmml">√ó</mo><mn id="S5.SS1.p2.1.m1.1.1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.1.1.3.cmml">224</mn></mrow><mo id="S5.SS1.p2.1.m1.1.1.1.3" stretchy="false" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1.1.1"></times><cn id="S5.SS1.p2.1.m1.1.1.1.1.2.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.1.1.2">1</cn><cn id="S5.SS1.p2.1.m1.1.1.1.1.3.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">(1\times 224)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">( 1 √ó 224 )</annotation></semantics></math> is passed through an initial convolutional layer with 16 size-3 filters and padding to preserve dimensions. Afterwards, a 32-filter, 3-size convolutional layer with padding follows. After each convolutional layer, a ReLU activation function introduces non-linearity and a max-pooling layer with a kernel size of 2 and stride of 2 reduces data dimensionality.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Residuals:</span> The model uses two residual blocks for feature extraction and learning. The first residual block receives the output from the second convolutional layer and processes it through two convolutional layers with 64 filters each, maintaining a kernel size of 3. Batch normalization is also applied after each convolutional layer. This output is added to the block‚Äôs input - through the utilisation of skip connection - and passed though a ReLU activation function. The second residual block follows a similar structure, but with 128 filters in each convolutional layer.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Fully Connected</span>: The processed features are flattened and passed through a fully connected layer with 512 neurons, followed by a dropout layer. A second fully connected layer follows with its number of neurons set equal to the number of classes. Finally a soft-max layer provides the final classification probabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">Data Pre-Processing</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">As mentioned in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Architecture</span>, the model expects a 1-dimensional vector as input. In this work, the input vectors are the individual pixels of the dataset‚Äôs images. Up to this point, however, the dataset consists of HS images, hence it is necessary to convert the images into a set of pixels.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To this end, a data handling pipeline was implemented to efficiently utilise the large volume of data encoded within HS images. In detail, a memory-mapped array uses the operating system‚Äôs virtual memory capabilities to map a disk file directly into the address space of the application, allowing for efficient, random access to large datasets without loading the entire file into memory. In this manner, by employing memory-mapped arrays out-of-core processing is achieved, which significantly reduces the memory footprint and improves the performance of data loading operations.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">By exploiting the capabilities of memory-mapped arrays, the entire set of images, along with their respective ground truth masks are flattened across the spatial dimensions, thus creating the desired 1-dimensional vectors is 224 features each. The feature vectors are, subsequently, randomly shuffled and split in two subsets; the train and the validation set with ratios 90% and 10% respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span class="ltx_text ltx_font_italic" id="S5.SS3.1.1">Model Training</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">With the data preprocessing pipeline established, the focus can now shift on the model training phase, where the calibrated, and pre-processed data is being used to train the Pixel-wise 1D Convolutional Hyperspectral Classifier.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">A dataloader was defined for each of the two sub-sets, each with a <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">batch size</span> equal to 640. The samples in the train set are shuffled in the beginning of each epoch, thus ensuring slightly different data distribution on batch-level in every iteration. The selected optimizer is Adam <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib42" title="">42</a>]</cite>, and the initial learning rate was set to <math alttext="0.001" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn id="S5.SS3.p2.1.m1.1.1.cmml" type="float" xref="S5.SS3.p2.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">0.001</annotation></semantics></math>. To prevent the model being stuck to a local minima of the loss function, a learning rate (LR) scheduler was also implemented. The LR scheduler utilised in this work was <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">Cosine Annealing with Warmup</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib44" title="">44</a>]</cite>. The mathematical formulation of this scheduler is described in the following equation :</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.SS3.p3.1" style="width:181.7pt;height:22.6pt;vertical-align:-1.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.9pt,0.9pt) scale(0.92,0.92) ;">
<p class="ltx_p" id="S5.SS3.p3.1.1"><math alttext="\eta_{t}=\begin{cases}\eta_{\text{init}}\frac{t}{T_{\text{w}}}&amp;\text{if }t\leq
T%
_{\text{w}}\\
\eta_{\text{min}}+\frac{1}{2}(\eta_{\text{max}}-\eta_{\text{min}})\left(1+\cos%
\left(\frac{(t-T_{\text{w}})\pi}{T-T_{\text{w}}}\right)\right)&amp;\text{if }t&gt;T_{%
\text{w}}~{}~{},\end{cases}" class="ltx_Math" display="inline" id="S5.SS3.p3.1.1.m1.4"><semantics id="S5.SS3.p3.1.1.m1.4a"><mrow id="S5.SS3.p3.1.1.m1.4.5" xref="S5.SS3.p3.1.1.m1.4.5.cmml"><msub id="S5.SS3.p3.1.1.m1.4.5.2" xref="S5.SS3.p3.1.1.m1.4.5.2.cmml"><mi id="S5.SS3.p3.1.1.m1.4.5.2.2" xref="S5.SS3.p3.1.1.m1.4.5.2.2.cmml">Œ∑</mi><mi id="S5.SS3.p3.1.1.m1.4.5.2.3" xref="S5.SS3.p3.1.1.m1.4.5.2.3.cmml">t</mi></msub><mo id="S5.SS3.p3.1.1.m1.4.5.1" xref="S5.SS3.p3.1.1.m1.4.5.1.cmml">=</mo><mrow id="S5.SS3.p3.1.1.m1.4.4" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mo id="S5.SS3.p3.1.1.m1.4.4.5" xref="S5.SS3.p3.1.1.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" id="S5.SS3.p3.1.1.m1.4.4.4" rowspacing="0pt" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mtr id="S5.SS3.p3.1.1.m1.4.4.4a" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.SS3.p3.1.1.m1.4.4.4b" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.2" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3a.cmml">init</mtext></msub><mo id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.1" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.1.cmml">‚Å¢</mo><mfrac id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.2" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.2.cmml">t</mi><msub id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.2" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.2.cmml">T</mi><mtext id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3a.cmml">w</mtext></msub></mfrac></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.SS3.p3.1.1.m1.4.4.4c" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.cmml"><mtext id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2a.cmml">if¬†</mtext><mo id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.1" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.1.cmml">‚Å¢</mo><mi id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.3" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.3.cmml">t</mi></mrow><mo id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.1" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.1.cmml">‚â§</mo><msub id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.2" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.2.cmml">T</mi><mtext id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3a.cmml">w</mtext></msub></mrow></mtd></mtr><mtr id="S5.SS3.p3.1.1.m1.4.4.4d" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.SS3.p3.1.1.m1.4.4.4e" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.cmml"><msub id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3a.cmml">min</mtext></msub><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.5" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.5.cmml">+</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.cmml"><mfrac id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.cmml"><mn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.2.cmml">1</mn><mn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.3.cmml">2</mn></mfrac><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3.cmml">‚Å¢</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml"><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.2" stretchy="false" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml">(</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml"><msub id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3a.cmml">max</mtext></msub><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.1.cmml">‚àí</mo><msub id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3a.cmml">min</mtext></msub></mrow><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.3" stretchy="false" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml">)</mo></mrow><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3a" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3.cmml">‚Å¢</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml"><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml">(</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml"><mn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.2.cmml">1</mn><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.1.cmml">+</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.2.cmml">cos</mi><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2a" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml">‚Å°</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml"><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml">(</mo><mfrac id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml"><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml">t</mi><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.2.cmml">T</mi><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3a.cmml">w</mtext></msub></mrow><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.3.cmml">œÄ</mi></mrow><mrow id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.2.cmml">T</mi><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.1" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.1.cmml">‚àí</mo><msub id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.cmml"><mi id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.2.cmml">T</mi><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3a.cmml">w</mtext></msub></mrow></mfrac><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2.1.2" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.3" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml">)</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.SS3.p3.1.1.m1.4.4.4f" xref="S5.SS3.p3.1.1.m1.4.5.3.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.cmml"><mtext id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2a.cmml">if¬†</mtext><mo id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.1" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.1.cmml">‚Å¢</mo><mi id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.3" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.3.cmml">t</mi></mrow><mo id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.1" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.1.cmml">&gt;</mo><msub id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.cmml"><mi id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.2" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.2.cmml">T</mi><mtext id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3a.cmml">w</mtext></msub></mrow><mo id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.2" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.1.m1.4b"><apply id="S5.SS3.p3.1.1.m1.4.5.cmml" xref="S5.SS3.p3.1.1.m1.4.5"><eq id="S5.SS3.p3.1.1.m1.4.5.1.cmml" xref="S5.SS3.p3.1.1.m1.4.5.1"></eq><apply id="S5.SS3.p3.1.1.m1.4.5.2.cmml" xref="S5.SS3.p3.1.1.m1.4.5.2"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.4.5.2.1.cmml" xref="S5.SS3.p3.1.1.m1.4.5.2">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.4.5.2.2.cmml" xref="S5.SS3.p3.1.1.m1.4.5.2.2">ùúÇ</ci><ci id="S5.SS3.p3.1.1.m1.4.5.2.3.cmml" xref="S5.SS3.p3.1.1.m1.4.5.2.3">ùë°</ci></apply><apply id="S5.SS3.p3.1.1.m1.4.5.3.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4"><csymbol cd="latexml" id="S5.SS3.p3.1.1.m1.4.5.3.1.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4.5">cases</csymbol><apply id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1"><times id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.1"></times><apply id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.2">ùúÇ</ci><ci id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3a.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3"><mtext id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.2.3">init</mtext></ci></apply><apply id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3"><divide id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3"></divide><ci id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.2">ùë°</ci><apply id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.2">ùëá</ci><ci id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3"><mtext id="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3.cmml" mathsize="50%" xref="S5.SS3.p3.1.1.m1.1.1.1.1.1.1.3.3.3">w</mtext></ci></apply></apply></apply><apply id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1"><leq id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.1.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.1"></leq><apply id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2"><times id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.1.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.1"></times><ci id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2a.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2"><mtext id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.2">if¬†</mtext></ci><ci id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.3.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.2.3">ùë°</ci></apply><apply id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.2">ùëá</ci><ci id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3"><mtext id="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.2.2.2.2.2.1.3.3">w</mtext></ci></apply></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1"><plus id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.5.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.5"></plus><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.2">ùúÇ</ci><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3a.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3"><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.6.3">min</mtext></ci></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4"><times id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.3"></times><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4"><divide id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4"></divide><cn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.2.cmml" type="integer" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.2">1</cn><cn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.3.cmml" type="integer" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.4.3">2</cn></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1"><minus id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.1"></minus><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.2">ùúÇ</ci><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3a.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3"><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.2.3">max</mtext></ci></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.2">ùúÇ</ci><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3"><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.3.1.1.1.3.3">min</mtext></ci></apply></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1"><plus id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.1"></plus><cn id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.2.cmml" type="integer" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.2">1</cn><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.4.2.1.1.3.2"><cos id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.2"></cos><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1"><divide id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1"></divide><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1"><times id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.2"></times><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1"><minus id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.1"></minus><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.2">ùë°</ci><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.2">ùëá</ci><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3"><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3.cmml" mathsize="50%" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.1.1.1.3.3">w</mtext></ci></apply></apply><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.1.3">ùúã</ci></apply><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3"><minus id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.1"></minus><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.2">ùëá</ci><apply id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.1.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.2.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.2">ùëá</ci><ci id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3"><mtext id="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3.cmml" mathsize="50%" xref="S5.SS3.p3.1.1.m1.3.3.3.3.1.1.1.3.3.3">w</mtext></ci></apply></apply></apply></apply></apply></apply></apply><apply id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1"><gt id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.1"></gt><apply id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2"><times id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.1"></times><ci id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2a.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2"><mtext id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.2">if¬†</mtext></ci><ci id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.3.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.2.3">ùë°</ci></apply><apply id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.1.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3">subscript</csymbol><ci id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.2.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.2">ùëá</ci><ci id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3a.cmml" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3"><mtext id="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3.cmml" mathsize="70%" xref="S5.SS3.p3.1.1.m1.4.4.4.4.2.1.1.1.3.3">w</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.1.m1.4c">\eta_{t}=\begin{cases}\eta_{\text{init}}\frac{t}{T_{\text{w}}}&amp;\text{if }t\leq
T%
_{\text{w}}\\
\eta_{\text{min}}+\frac{1}{2}(\eta_{\text{max}}-\eta_{\text{min}})\left(1+\cos%
\left(\frac{(t-T_{\text{w}})\pi}{T-T_{\text{w}}}\right)\right)&amp;\text{if }t&gt;T_{%
\text{w}}~{}~{},\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.1.m1.4d">italic_Œ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { start_ROW start_CELL italic_Œ∑ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT divide start_ARG italic_t end_ARG start_ARG italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT end_ARG end_CELL start_CELL if italic_t ‚â§ italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_Œ∑ start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( italic_Œ∑ start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_Œ∑ start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ) ( 1 + roman_cos ( divide start_ARG ( italic_t - italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT ) italic_œÄ end_ARG start_ARG italic_T - italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT end_ARG ) ) end_CELL start_CELL if italic_t &gt; italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT , end_CELL end_ROW</annotation></semantics></math></p>
</span></div>
<p class="ltx_p" id="S5.SS3.p3.8">where <math alttext="\eta_{t}" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m1.1"><semantics id="S5.SS3.p3.2.m1.1a"><msub id="S5.SS3.p3.2.m1.1.1" xref="S5.SS3.p3.2.m1.1.1.cmml"><mi id="S5.SS3.p3.2.m1.1.1.2" xref="S5.SS3.p3.2.m1.1.1.2.cmml">Œ∑</mi><mi id="S5.SS3.p3.2.m1.1.1.3" xref="S5.SS3.p3.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m1.1b"><apply id="S5.SS3.p3.2.m1.1.1.cmml" xref="S5.SS3.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m1.1.1.1.cmml" xref="S5.SS3.p3.2.m1.1.1">subscript</csymbol><ci id="S5.SS3.p3.2.m1.1.1.2.cmml" xref="S5.SS3.p3.2.m1.1.1.2">ùúÇ</ci><ci id="S5.SS3.p3.2.m1.1.1.3.cmml" xref="S5.SS3.p3.2.m1.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m1.1c">\eta_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m1.1d">italic_Œ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the learning rate at epoch <math alttext="t" class="ltx_Math" display="inline" id="S5.SS3.p3.3.m2.1"><semantics id="S5.SS3.p3.3.m2.1a"><mi id="S5.SS3.p3.3.m2.1.1" xref="S5.SS3.p3.3.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m2.1b"><ci id="S5.SS3.p3.3.m2.1.1.cmml" xref="S5.SS3.p3.3.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.3.m2.1d">italic_t</annotation></semantics></math>.
<math alttext="\eta_{\text{init}}" class="ltx_Math" display="inline" id="S5.SS3.p3.4.m3.1"><semantics id="S5.SS3.p3.4.m3.1a"><msub id="S5.SS3.p3.4.m3.1.1" xref="S5.SS3.p3.4.m3.1.1.cmml"><mi id="S5.SS3.p3.4.m3.1.1.2" xref="S5.SS3.p3.4.m3.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.4.m3.1.1.3" xref="S5.SS3.p3.4.m3.1.1.3a.cmml">init</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m3.1b"><apply id="S5.SS3.p3.4.m3.1.1.cmml" xref="S5.SS3.p3.4.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.4.m3.1.1.1.cmml" xref="S5.SS3.p3.4.m3.1.1">subscript</csymbol><ci id="S5.SS3.p3.4.m3.1.1.2.cmml" xref="S5.SS3.p3.4.m3.1.1.2">ùúÇ</ci><ci id="S5.SS3.p3.4.m3.1.1.3a.cmml" xref="S5.SS3.p3.4.m3.1.1.3"><mtext id="S5.SS3.p3.4.m3.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p3.4.m3.1.1.3">init</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m3.1c">\eta_{\text{init}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.4.m3.1d">italic_Œ∑ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT</annotation></semantics></math> is the initial learning rate,
<math alttext="\eta_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS3.p3.5.m4.1"><semantics id="S5.SS3.p3.5.m4.1a"><msub id="S5.SS3.p3.5.m4.1.1" xref="S5.SS3.p3.5.m4.1.1.cmml"><mi id="S5.SS3.p3.5.m4.1.1.2" xref="S5.SS3.p3.5.m4.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.5.m4.1.1.3" xref="S5.SS3.p3.5.m4.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.5.m4.1b"><apply id="S5.SS3.p3.5.m4.1.1.cmml" xref="S5.SS3.p3.5.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.5.m4.1.1.1.cmml" xref="S5.SS3.p3.5.m4.1.1">subscript</csymbol><ci id="S5.SS3.p3.5.m4.1.1.2.cmml" xref="S5.SS3.p3.5.m4.1.1.2">ùúÇ</ci><ci id="S5.SS3.p3.5.m4.1.1.3a.cmml" xref="S5.SS3.p3.5.m4.1.1.3"><mtext id="S5.SS3.p3.5.m4.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p3.5.m4.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.5.m4.1c">\eta_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.5.m4.1d">italic_Œ∑ start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> is the maximum learning rate,
<math alttext="\eta_{\text{min}}" class="ltx_Math" display="inline" id="S5.SS3.p3.6.m5.1"><semantics id="S5.SS3.p3.6.m5.1a"><msub id="S5.SS3.p3.6.m5.1.1" xref="S5.SS3.p3.6.m5.1.1.cmml"><mi id="S5.SS3.p3.6.m5.1.1.2" xref="S5.SS3.p3.6.m5.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p3.6.m5.1.1.3" xref="S5.SS3.p3.6.m5.1.1.3a.cmml">min</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.6.m5.1b"><apply id="S5.SS3.p3.6.m5.1.1.cmml" xref="S5.SS3.p3.6.m5.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.6.m5.1.1.1.cmml" xref="S5.SS3.p3.6.m5.1.1">subscript</csymbol><ci id="S5.SS3.p3.6.m5.1.1.2.cmml" xref="S5.SS3.p3.6.m5.1.1.2">ùúÇ</ci><ci id="S5.SS3.p3.6.m5.1.1.3a.cmml" xref="S5.SS3.p3.6.m5.1.1.3"><mtext id="S5.SS3.p3.6.m5.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p3.6.m5.1.1.3">min</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.6.m5.1c">\eta_{\text{min}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.6.m5.1d">italic_Œ∑ start_POSTSUBSCRIPT min end_POSTSUBSCRIPT</annotation></semantics></math> is the minimum learning rate,
<math alttext="T_{\text{w}}" class="ltx_Math" display="inline" id="S5.SS3.p3.7.m6.1"><semantics id="S5.SS3.p3.7.m6.1a"><msub id="S5.SS3.p3.7.m6.1.1" xref="S5.SS3.p3.7.m6.1.1.cmml"><mi id="S5.SS3.p3.7.m6.1.1.2" xref="S5.SS3.p3.7.m6.1.1.2.cmml">T</mi><mtext id="S5.SS3.p3.7.m6.1.1.3" xref="S5.SS3.p3.7.m6.1.1.3a.cmml">w</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.7.m6.1b"><apply id="S5.SS3.p3.7.m6.1.1.cmml" xref="S5.SS3.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.7.m6.1.1.1.cmml" xref="S5.SS3.p3.7.m6.1.1">subscript</csymbol><ci id="S5.SS3.p3.7.m6.1.1.2.cmml" xref="S5.SS3.p3.7.m6.1.1.2">ùëá</ci><ci id="S5.SS3.p3.7.m6.1.1.3a.cmml" xref="S5.SS3.p3.7.m6.1.1.3"><mtext id="S5.SS3.p3.7.m6.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p3.7.m6.1.1.3">w</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.7.m6.1c">T_{\text{w}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.7.m6.1d">italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT</annotation></semantics></math> is the number of warmup epochs,
<math alttext="T" class="ltx_Math" display="inline" id="S5.SS3.p3.8.m7.1"><semantics id="S5.SS3.p3.8.m7.1a"><mi id="S5.SS3.p3.8.m7.1.1" xref="S5.SS3.p3.8.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.8.m7.1b"><ci id="S5.SS3.p3.8.m7.1.1.cmml" xref="S5.SS3.p3.8.m7.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.8.m7.1c">T</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.8.m7.1d">italic_T</annotation></semantics></math> is the total number of epochs.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Since the model‚Äôs aim is to classify each of the pixel in a HS image to their respective class, the <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.1">Cross-Entropy</span> Loss Function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib45" title="">45</a>]</cite> was selected to be minimized throughout the training process as depicted in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.E3" title="In 5.3 Model Training ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{CE}}=-\sum_{i=1}^{N}\sum_{c=1}^{C}y_{ic}\log(\hat{y}_{ic})~%
{}~{}," class="ltx_Math" display="block" id="S5.E3.m1.2"><semantics id="S5.E3.m1.2a"><mrow id="S5.E3.m1.2.2.1" xref="S5.E3.m1.2.2.1.1.cmml"><mrow id="S5.E3.m1.2.2.1.1" xref="S5.E3.m1.2.2.1.1.cmml"><msub id="S5.E3.m1.2.2.1.1.3" xref="S5.E3.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E3.m1.2.2.1.1.3.2" xref="S5.E3.m1.2.2.1.1.3.2.cmml">‚Ñí</mi><mtext id="S5.E3.m1.2.2.1.1.3.3" xref="S5.E3.m1.2.2.1.1.3.3a.cmml">CE</mtext></msub><mo id="S5.E3.m1.2.2.1.1.2" xref="S5.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S5.E3.m1.2.2.1.1.1" xref="S5.E3.m1.2.2.1.1.1.cmml"><mo id="S5.E3.m1.2.2.1.1.1a" xref="S5.E3.m1.2.2.1.1.1.cmml">‚àí</mo><mrow id="S5.E3.m1.2.2.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.cmml"><munderover id="S5.E3.m1.2.2.1.1.1.1.2" xref="S5.E3.m1.2.2.1.1.1.1.2.cmml"><mo id="S5.E3.m1.2.2.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S5.E3.m1.2.2.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.2.2.3" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.2.2.3.2" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E3.m1.2.2.1.1.1.1.2.2.3.1" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E3.m1.2.2.1.1.1.1.2.2.3.3" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E3.m1.2.2.1.1.1.1.2.3" xref="S5.E3.m1.2.2.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S5.E3.m1.2.2.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.cmml"><munderover id="S5.E3.m1.2.2.1.1.1.1.1.2" xref="S5.E3.m1.2.2.1.1.1.1.1.2.cmml"><mo id="S5.E3.m1.2.2.1.1.1.1.1.2.2.2" movablelimits="false" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.2" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.2.cmml">c</mi><mo id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.1" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.3" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E3.m1.2.2.1.1.1.1.1.2.3" xref="S5.E3.m1.2.2.1.1.1.1.1.2.3.cmml">C</mi></munderover><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S5.E3.m1.2.2.1.1.1.1.1.1.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.3.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.2.cmml">i</mi><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.3.cmml">c</mi></mrow></msub><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.2" lspace="0.167em" xref="S5.E3.m1.2.2.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml">log</mi><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1a" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">‚Å°</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml">c</mi></mrow></msub><mo id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3" rspace="0.660em" stretchy="false" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S5.E3.m1.2.2.1.2" xref="S5.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.2b"><apply id="S5.E3.m1.2.2.1.1.cmml" xref="S5.E3.m1.2.2.1"><eq id="S5.E3.m1.2.2.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.2"></eq><apply id="S5.E3.m1.2.2.1.1.3.cmml" xref="S5.E3.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.3.1.cmml" xref="S5.E3.m1.2.2.1.1.3">subscript</csymbol><ci id="S5.E3.m1.2.2.1.1.3.2.cmml" xref="S5.E3.m1.2.2.1.1.3.2">‚Ñí</ci><ci id="S5.E3.m1.2.2.1.1.3.3a.cmml" xref="S5.E3.m1.2.2.1.1.3.3"><mtext id="S5.E3.m1.2.2.1.1.3.3.cmml" mathsize="70%" xref="S5.E3.m1.2.2.1.1.3.3">CE</mtext></ci></apply><apply id="S5.E3.m1.2.2.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1"><minus id="S5.E3.m1.2.2.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1"></minus><apply id="S5.E3.m1.2.2.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1"><apply id="S5.E3.m1.2.2.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.2.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S5.E3.m1.2.2.1.1.1.1.2.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S5.E3.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S5.E3.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3"><eq id="S5.E3.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S5.E3.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.2">ùëñ</ci><cn id="S5.E3.m1.2.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E3.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E3.m1.2.2.1.1.1.1.2.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="S5.E3.m1.2.2.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1"><apply id="S5.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2">superscript</csymbol><apply id="S5.E3.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.1.2.2.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S5.E3.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.2"></sum><apply id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3"><eq id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.2">ùëê</ci><cn id="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E3.m1.2.2.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E3.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.2.3">ùê∂</ci></apply><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1"><times id="S5.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.2">ùë¶</ci><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3"><times id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.1"></times><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.2">ùëñ</ci><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.3.3">ùëê</ci></apply></apply><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1"><log id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"></log><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2"><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.2">ùë¶</ci></apply><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3"><times id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.2">ùëñ</ci><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.3">ùëê</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.2c">\mathcal{L}_{\text{CE}}=-\sum_{i=1}^{N}\sum_{c=1}^{C}y_{ic}\log(\hat{y}_{ic})~%
{}~{},</annotation><annotation encoding="application/x-llamapun" id="S5.E3.m1.2d">caligraphic_L start_POSTSUBSCRIPT CE end_POSTSUBSCRIPT = - ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚àë start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_i italic_c end_POSTSUBSCRIPT roman_log ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_c end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS3.p5.8">where <math alttext="N" class="ltx_Math" display="inline" id="S5.SS3.p5.1.m1.1"><semantics id="S5.SS3.p5.1.m1.1a"><mi id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><ci id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.1.m1.1d">italic_N</annotation></semantics></math> is the number of samples,
<math alttext="C" class="ltx_Math" display="inline" id="S5.SS3.p5.2.m2.1"><semantics id="S5.SS3.p5.2.m2.1a"><mi id="S5.SS3.p5.2.m2.1.1" xref="S5.SS3.p5.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.2.m2.1b"><ci id="S5.SS3.p5.2.m2.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.2.m2.1d">italic_C</annotation></semantics></math> is the number of classes.
<math alttext="y_{ic}" class="ltx_Math" display="inline" id="S5.SS3.p5.3.m3.1"><semantics id="S5.SS3.p5.3.m3.1a"><msub id="S5.SS3.p5.3.m3.1.1" xref="S5.SS3.p5.3.m3.1.1.cmml"><mi id="S5.SS3.p5.3.m3.1.1.2" xref="S5.SS3.p5.3.m3.1.1.2.cmml">y</mi><mrow id="S5.SS3.p5.3.m3.1.1.3" xref="S5.SS3.p5.3.m3.1.1.3.cmml"><mi id="S5.SS3.p5.3.m3.1.1.3.2" xref="S5.SS3.p5.3.m3.1.1.3.2.cmml">i</mi><mo id="S5.SS3.p5.3.m3.1.1.3.1" xref="S5.SS3.p5.3.m3.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS3.p5.3.m3.1.1.3.3" xref="S5.SS3.p5.3.m3.1.1.3.3.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.3.m3.1b"><apply id="S5.SS3.p5.3.m3.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.3.m3.1.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p5.3.m3.1.1.2.cmml" xref="S5.SS3.p5.3.m3.1.1.2">ùë¶</ci><apply id="S5.SS3.p5.3.m3.1.1.3.cmml" xref="S5.SS3.p5.3.m3.1.1.3"><times id="S5.SS3.p5.3.m3.1.1.3.1.cmml" xref="S5.SS3.p5.3.m3.1.1.3.1"></times><ci id="S5.SS3.p5.3.m3.1.1.3.2.cmml" xref="S5.SS3.p5.3.m3.1.1.3.2">ùëñ</ci><ci id="S5.SS3.p5.3.m3.1.1.3.3.cmml" xref="S5.SS3.p5.3.m3.1.1.3.3">ùëê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.3.m3.1c">y_{ic}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_i italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is the ground truth label (1 if sample <math alttext="i" class="ltx_Math" display="inline" id="S5.SS3.p5.4.m4.1"><semantics id="S5.SS3.p5.4.m4.1a"><mi id="S5.SS3.p5.4.m4.1.1" xref="S5.SS3.p5.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.4.m4.1b"><ci id="S5.SS3.p5.4.m4.1.1.cmml" xref="S5.SS3.p5.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.4.m4.1d">italic_i</annotation></semantics></math> belongs to class <math alttext="c" class="ltx_Math" display="inline" id="S5.SS3.p5.5.m5.1"><semantics id="S5.SS3.p5.5.m5.1a"><mi id="S5.SS3.p5.5.m5.1.1" xref="S5.SS3.p5.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.5.m5.1b"><ci id="S5.SS3.p5.5.m5.1.1.cmml" xref="S5.SS3.p5.5.m5.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.5.m5.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.5.m5.1d">italic_c</annotation></semantics></math>, otherwise 0),
<math alttext="\hat{y}_{ic}" class="ltx_Math" display="inline" id="S5.SS3.p5.6.m6.1"><semantics id="S5.SS3.p5.6.m6.1a"><msub id="S5.SS3.p5.6.m6.1.1" xref="S5.SS3.p5.6.m6.1.1.cmml"><mover accent="true" id="S5.SS3.p5.6.m6.1.1.2" xref="S5.SS3.p5.6.m6.1.1.2.cmml"><mi id="S5.SS3.p5.6.m6.1.1.2.2" xref="S5.SS3.p5.6.m6.1.1.2.2.cmml">y</mi><mo id="S5.SS3.p5.6.m6.1.1.2.1" xref="S5.SS3.p5.6.m6.1.1.2.1.cmml">^</mo></mover><mrow id="S5.SS3.p5.6.m6.1.1.3" xref="S5.SS3.p5.6.m6.1.1.3.cmml"><mi id="S5.SS3.p5.6.m6.1.1.3.2" xref="S5.SS3.p5.6.m6.1.1.3.2.cmml">i</mi><mo id="S5.SS3.p5.6.m6.1.1.3.1" xref="S5.SS3.p5.6.m6.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS3.p5.6.m6.1.1.3.3" xref="S5.SS3.p5.6.m6.1.1.3.3.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.6.m6.1b"><apply id="S5.SS3.p5.6.m6.1.1.cmml" xref="S5.SS3.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.6.m6.1.1.1.cmml" xref="S5.SS3.p5.6.m6.1.1">subscript</csymbol><apply id="S5.SS3.p5.6.m6.1.1.2.cmml" xref="S5.SS3.p5.6.m6.1.1.2"><ci id="S5.SS3.p5.6.m6.1.1.2.1.cmml" xref="S5.SS3.p5.6.m6.1.1.2.1">^</ci><ci id="S5.SS3.p5.6.m6.1.1.2.2.cmml" xref="S5.SS3.p5.6.m6.1.1.2.2">ùë¶</ci></apply><apply id="S5.SS3.p5.6.m6.1.1.3.cmml" xref="S5.SS3.p5.6.m6.1.1.3"><times id="S5.SS3.p5.6.m6.1.1.3.1.cmml" xref="S5.SS3.p5.6.m6.1.1.3.1"></times><ci id="S5.SS3.p5.6.m6.1.1.3.2.cmml" xref="S5.SS3.p5.6.m6.1.1.3.2">ùëñ</ci><ci id="S5.SS3.p5.6.m6.1.1.3.3.cmml" xref="S5.SS3.p5.6.m6.1.1.3.3">ùëê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.6.m6.1c">\hat{y}_{ic}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.6.m6.1d">over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_c end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted probability that sample <math alttext="i" class="ltx_Math" display="inline" id="S5.SS3.p5.7.m7.1"><semantics id="S5.SS3.p5.7.m7.1a"><mi id="S5.SS3.p5.7.m7.1.1" xref="S5.SS3.p5.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.7.m7.1b"><ci id="S5.SS3.p5.7.m7.1.1.cmml" xref="S5.SS3.p5.7.m7.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.7.m7.1d">italic_i</annotation></semantics></math> belongs to class <math alttext="c" class="ltx_Math" display="inline" id="S5.SS3.p5.8.m8.1"><semantics id="S5.SS3.p5.8.m8.1a"><mi id="S5.SS3.p5.8.m8.1.1" xref="S5.SS3.p5.8.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.8.m8.1b"><ci id="S5.SS3.p5.8.m8.1.1.cmml" xref="S5.SS3.p5.8.m8.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.8.m8.1c">c</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.8.m8.1d">italic_c</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p6">
<p class="ltx_p" id="S5.SS3.p6.7">The number of training epochs <math alttext="T" class="ltx_Math" display="inline" id="S5.SS3.p6.1.m1.1"><semantics id="S5.SS3.p6.1.m1.1a"><mi id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><ci id="S5.SS3.p6.1.m1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.1.m1.1d">italic_T</annotation></semantics></math> was set to 50, while a callback was also employed, in order to save to the model‚Äôs checkpoint with the best accuracy score in the validation set, thus ensuring that after the training process is finished the final model will be the one with the best performance in the validation set. Moreover, it is noted that the initial and maximum learning rate <math alttext="\eta_{\text{init}}" class="ltx_Math" display="inline" id="S5.SS3.p6.2.m2.1"><semantics id="S5.SS3.p6.2.m2.1a"><msub id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml"><mi id="S5.SS3.p6.2.m2.1.1.2" xref="S5.SS3.p6.2.m2.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p6.2.m2.1.1.3" xref="S5.SS3.p6.2.m2.1.1.3a.cmml">init</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><apply id="S5.SS3.p6.2.m2.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p6.2.m2.1.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p6.2.m2.1.1.2.cmml" xref="S5.SS3.p6.2.m2.1.1.2">ùúÇ</ci><ci id="S5.SS3.p6.2.m2.1.1.3a.cmml" xref="S5.SS3.p6.2.m2.1.1.3"><mtext id="S5.SS3.p6.2.m2.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p6.2.m2.1.1.3">init</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">\eta_{\text{init}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.2.m2.1d">italic_Œ∑ start_POSTSUBSCRIPT init end_POSTSUBSCRIPT</annotation></semantics></math> , <math alttext="\eta_{\text{max}}" class="ltx_Math" display="inline" id="S5.SS3.p6.3.m3.1"><semantics id="S5.SS3.p6.3.m3.1a"><msub id="S5.SS3.p6.3.m3.1.1" xref="S5.SS3.p6.3.m3.1.1.cmml"><mi id="S5.SS3.p6.3.m3.1.1.2" xref="S5.SS3.p6.3.m3.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p6.3.m3.1.1.3" xref="S5.SS3.p6.3.m3.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.3.m3.1b"><apply id="S5.SS3.p6.3.m3.1.1.cmml" xref="S5.SS3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p6.3.m3.1.1.1.cmml" xref="S5.SS3.p6.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p6.3.m3.1.1.2.cmml" xref="S5.SS3.p6.3.m3.1.1.2">ùúÇ</ci><ci id="S5.SS3.p6.3.m3.1.1.3a.cmml" xref="S5.SS3.p6.3.m3.1.1.3"><mtext id="S5.SS3.p6.3.m3.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p6.3.m3.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.3.m3.1c">\eta_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.3.m3.1d">italic_Œ∑ start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> respectively were set to <math alttext="0.001" class="ltx_Math" display="inline" id="S5.SS3.p6.4.m4.1"><semantics id="S5.SS3.p6.4.m4.1a"><mn id="S5.SS3.p6.4.m4.1.1" xref="S5.SS3.p6.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.4.m4.1b"><cn id="S5.SS3.p6.4.m4.1.1.cmml" type="float" xref="S5.SS3.p6.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.4.m4.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.4.m4.1d">0.001</annotation></semantics></math>, <math alttext="\eta_{\text{min}}" class="ltx_Math" display="inline" id="S5.SS3.p6.5.m5.1"><semantics id="S5.SS3.p6.5.m5.1a"><msub id="S5.SS3.p6.5.m5.1.1" xref="S5.SS3.p6.5.m5.1.1.cmml"><mi id="S5.SS3.p6.5.m5.1.1.2" xref="S5.SS3.p6.5.m5.1.1.2.cmml">Œ∑</mi><mtext id="S5.SS3.p6.5.m5.1.1.3" xref="S5.SS3.p6.5.m5.1.1.3a.cmml">min</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.5.m5.1b"><apply id="S5.SS3.p6.5.m5.1.1.cmml" xref="S5.SS3.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS3.p6.5.m5.1.1.1.cmml" xref="S5.SS3.p6.5.m5.1.1">subscript</csymbol><ci id="S5.SS3.p6.5.m5.1.1.2.cmml" xref="S5.SS3.p6.5.m5.1.1.2">ùúÇ</ci><ci id="S5.SS3.p6.5.m5.1.1.3a.cmml" xref="S5.SS3.p6.5.m5.1.1.3"><mtext id="S5.SS3.p6.5.m5.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p6.5.m5.1.1.3">min</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.5.m5.1c">\eta_{\text{min}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.5.m5.1d">italic_Œ∑ start_POSTSUBSCRIPT min end_POSTSUBSCRIPT</annotation></semantics></math> was set to <math alttext="0.0001" class="ltx_Math" display="inline" id="S5.SS3.p6.6.m6.1"><semantics id="S5.SS3.p6.6.m6.1a"><mn id="S5.SS3.p6.6.m6.1.1" xref="S5.SS3.p6.6.m6.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.6.m6.1b"><cn id="S5.SS3.p6.6.m6.1.1.cmml" type="float" xref="S5.SS3.p6.6.m6.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.6.m6.1c">0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.6.m6.1d">0.0001</annotation></semantics></math>, while warmup <math alttext="T_{\text{w}}" class="ltx_Math" display="inline" id="S5.SS3.p6.7.m7.1"><semantics id="S5.SS3.p6.7.m7.1a"><msub id="S5.SS3.p6.7.m7.1.1" xref="S5.SS3.p6.7.m7.1.1.cmml"><mi id="S5.SS3.p6.7.m7.1.1.2" xref="S5.SS3.p6.7.m7.1.1.2.cmml">T</mi><mtext id="S5.SS3.p6.7.m7.1.1.3" xref="S5.SS3.p6.7.m7.1.1.3a.cmml">w</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.7.m7.1b"><apply id="S5.SS3.p6.7.m7.1.1.cmml" xref="S5.SS3.p6.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS3.p6.7.m7.1.1.1.cmml" xref="S5.SS3.p6.7.m7.1.1">subscript</csymbol><ci id="S5.SS3.p6.7.m7.1.1.2.cmml" xref="S5.SS3.p6.7.m7.1.1.2">ùëá</ci><ci id="S5.SS3.p6.7.m7.1.1.3a.cmml" xref="S5.SS3.p6.7.m7.1.1.3"><mtext id="S5.SS3.p6.7.m7.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p6.7.m7.1.1.3">w</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.7.m7.1c">T_{\text{w}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p6.7.m7.1d">italic_T start_POSTSUBSCRIPT w end_POSTSUBSCRIPT</annotation></semantics></math> was set to 10 epochs.</p>
</div>
<div class="ltx_para" id="S5.SS3.p7">
<p class="ltx_p" id="S5.SS3.p7.1">Post training, the model was able to achieve an accuracy score of <span class="ltx_text ltx_font_bold" id="S5.SS3.p7.1.1">99.58</span>% in the train set, with the last epoch‚Äôs loss being equal to <span class="ltx_text ltx_font_bold" id="S5.SS3.p7.1.2">0.0157</span>, as seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.F5" title="Figure 5 ‚Ä£ 5.3 Model Training ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S5.F5.g1" src="extracted/5862626/imgs/training_plots.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Evolution of model‚Äôs loss (top), and accuracy score (bottom) during training.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p8">
<p class="ltx_p" id="S5.SS3.p8.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.F6" title="Figure 6 ‚Ä£ 5.3 Model Training ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>, one may notice that the model achieved peak performance in the validation set at epoch 35, with an accuracy score of <span class="ltx_text ltx_font_bold" id="S5.SS3.p8.1.1">98.05</span>%.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S5.F6.g1" src="extracted/5862626/imgs/Validation_plot.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The model‚Äôs performance in the validation set, during training, with the best accuracy score achieved in epoch 35.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span class="ltx_text ltx_font_italic" id="S5.SS4.1.1">Post-Processing</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The proposed model is capable of processing <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">330</span> lines of shape <math alttext="640\times 224" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mn id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">640</mn><mo id="S5.SS4.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><times id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1"></times><cn id="S5.SS4.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1.2">640</cn><cn id="S5.SS4.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">640\times 224</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">640 √ó 224</annotation></semantics></math> per second. In order to take advantage by the spatial domain and to acquire the final classification map, each of the predicted lines is accumulated in a 3D buffer until the whole set of lines post-processed together.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">Median filter:</span> a kernel size 5, is applied to the reconstructed classification map. In this manner, misclassifications that reassemble the <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.2">Salt &amp; Pepper</span> noise are correct by substituting them with the median value of the surrounding <math alttext="5\times 5" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><mrow id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml"><mn id="S5.SS4.p2.1.m1.1.1.2" xref="S5.SS4.p2.1.m1.1.1.2.cmml">5</mn><mo id="S5.SS4.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS4.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S5.SS4.p2.1.m1.1.1.3" xref="S5.SS4.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><apply id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1"><times id="S5.SS4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1.1"></times><cn id="S5.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.2">5</cn><cn id="S5.SS4.p2.1.m1.1.1.3.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">5\times 5</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">5 √ó 5</annotation></semantics></math> region, as described in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.E4" title="In 5.4 Post-Processing ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I^{\prime}(x,y)=\text{median}\{I(i,j)\mid(i,j)\in W(x,y)\}~{}~{}," class="ltx_Math" display="block" id="S5.E4.m1.9"><semantics id="S5.E4.m1.9a"><mrow id="S5.E4.m1.9.9.1" xref="S5.E4.m1.9.9.1.1.cmml"><mrow id="S5.E4.m1.9.9.1.1" xref="S5.E4.m1.9.9.1.1.cmml"><mrow id="S5.E4.m1.9.9.1.1.4" xref="S5.E4.m1.9.9.1.1.4.cmml"><msup id="S5.E4.m1.9.9.1.1.4.2" xref="S5.E4.m1.9.9.1.1.4.2.cmml"><mi id="S5.E4.m1.9.9.1.1.4.2.2" xref="S5.E4.m1.9.9.1.1.4.2.2.cmml">I</mi><mo id="S5.E4.m1.9.9.1.1.4.2.3" xref="S5.E4.m1.9.9.1.1.4.2.3.cmml">‚Ä≤</mo></msup><mo id="S5.E4.m1.9.9.1.1.4.1" xref="S5.E4.m1.9.9.1.1.4.1.cmml">‚Å¢</mo><mrow id="S5.E4.m1.9.9.1.1.4.3.2" xref="S5.E4.m1.9.9.1.1.4.3.1.cmml"><mo id="S5.E4.m1.9.9.1.1.4.3.2.1" stretchy="false" xref="S5.E4.m1.9.9.1.1.4.3.1.cmml">(</mo><mi id="S5.E4.m1.1.1" xref="S5.E4.m1.1.1.cmml">x</mi><mo id="S5.E4.m1.9.9.1.1.4.3.2.2" xref="S5.E4.m1.9.9.1.1.4.3.1.cmml">,</mo><mi id="S5.E4.m1.2.2" xref="S5.E4.m1.2.2.cmml">y</mi><mo id="S5.E4.m1.9.9.1.1.4.3.2.3" stretchy="false" xref="S5.E4.m1.9.9.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo id="S5.E4.m1.9.9.1.1.3" xref="S5.E4.m1.9.9.1.1.3.cmml">=</mo><mrow id="S5.E4.m1.9.9.1.1.2" xref="S5.E4.m1.9.9.1.1.2.cmml"><mtext id="S5.E4.m1.9.9.1.1.2.4" xref="S5.E4.m1.9.9.1.1.2.4a.cmml">median</mtext><mo id="S5.E4.m1.9.9.1.1.2.3" xref="S5.E4.m1.9.9.1.1.2.3.cmml">‚Å¢</mo><mrow id="S5.E4.m1.9.9.1.1.2.2.2" xref="S5.E4.m1.9.9.1.1.2.2.3.cmml"><mo id="S5.E4.m1.9.9.1.1.2.2.2.3" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.3.1.cmml">{</mo><mrow id="S5.E4.m1.9.9.1.1.1.1.1.1" xref="S5.E4.m1.9.9.1.1.1.1.1.1.cmml"><mi id="S5.E4.m1.9.9.1.1.1.1.1.1.2" xref="S5.E4.m1.9.9.1.1.1.1.1.1.2.cmml">I</mi><mo id="S5.E4.m1.9.9.1.1.1.1.1.1.1" xref="S5.E4.m1.9.9.1.1.1.1.1.1.1.cmml">‚Å¢</mo><mrow id="S5.E4.m1.9.9.1.1.1.1.1.1.3.2" xref="S5.E4.m1.9.9.1.1.1.1.1.1.3.1.cmml"><mo id="S5.E4.m1.9.9.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S5.E4.m1.9.9.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S5.E4.m1.3.3" xref="S5.E4.m1.3.3.cmml">i</mi><mo id="S5.E4.m1.9.9.1.1.1.1.1.1.3.2.2" xref="S5.E4.m1.9.9.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S5.E4.m1.4.4" xref="S5.E4.m1.4.4.cmml">j</mi><mo id="S5.E4.m1.9.9.1.1.1.1.1.1.3.2.3" stretchy="false" xref="S5.E4.m1.9.9.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo fence="true" id="S5.E4.m1.9.9.1.1.2.2.2.4" lspace="0em" rspace="0em" xref="S5.E4.m1.9.9.1.1.2.2.3.1.cmml">‚à£</mo><mrow id="S5.E4.m1.9.9.1.1.2.2.2.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.cmml"><mrow id="S5.E4.m1.9.9.1.1.2.2.2.2.2.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.2.1.cmml"><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.2.2.1" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.2.2.2.1.cmml">(</mo><mi id="S5.E4.m1.5.5" xref="S5.E4.m1.5.5.cmml">i</mi><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.2.2.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.2.1.cmml">,</mo><mi id="S5.E4.m1.6.6" xref="S5.E4.m1.6.6.cmml">j</mi><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.2.2.3" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.2.2.2.1.cmml">)</mo></mrow><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.1" xref="S5.E4.m1.9.9.1.1.2.2.2.2.1.cmml">‚àà</mo><mrow id="S5.E4.m1.9.9.1.1.2.2.2.2.3" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.cmml"><mi id="S5.E4.m1.9.9.1.1.2.2.2.2.3.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.2.cmml">W</mi><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.3.1" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.1.cmml">‚Å¢</mo><mrow id="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.1.cmml"><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.2.1" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.1.cmml">(</mo><mi id="S5.E4.m1.7.7" xref="S5.E4.m1.7.7.cmml">x</mi><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.2.2" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.1.cmml">,</mo><mi id="S5.E4.m1.8.8" xref="S5.E4.m1.8.8.cmml">y</mi><mo id="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.2.3" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E4.m1.9.9.1.1.2.2.2.5" rspace="0.660em" stretchy="false" xref="S5.E4.m1.9.9.1.1.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S5.E4.m1.9.9.1.2" xref="S5.E4.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.9b"><apply id="S5.E4.m1.9.9.1.1.cmml" xref="S5.E4.m1.9.9.1"><eq id="S5.E4.m1.9.9.1.1.3.cmml" xref="S5.E4.m1.9.9.1.1.3"></eq><apply id="S5.E4.m1.9.9.1.1.4.cmml" xref="S5.E4.m1.9.9.1.1.4"><times id="S5.E4.m1.9.9.1.1.4.1.cmml" xref="S5.E4.m1.9.9.1.1.4.1"></times><apply id="S5.E4.m1.9.9.1.1.4.2.cmml" xref="S5.E4.m1.9.9.1.1.4.2"><csymbol cd="ambiguous" id="S5.E4.m1.9.9.1.1.4.2.1.cmml" xref="S5.E4.m1.9.9.1.1.4.2">superscript</csymbol><ci id="S5.E4.m1.9.9.1.1.4.2.2.cmml" xref="S5.E4.m1.9.9.1.1.4.2.2">ùêº</ci><ci id="S5.E4.m1.9.9.1.1.4.2.3.cmml" xref="S5.E4.m1.9.9.1.1.4.2.3">‚Ä≤</ci></apply><interval closure="open" id="S5.E4.m1.9.9.1.1.4.3.1.cmml" xref="S5.E4.m1.9.9.1.1.4.3.2"><ci id="S5.E4.m1.1.1.cmml" xref="S5.E4.m1.1.1">ùë•</ci><ci id="S5.E4.m1.2.2.cmml" xref="S5.E4.m1.2.2">ùë¶</ci></interval></apply><apply id="S5.E4.m1.9.9.1.1.2.cmml" xref="S5.E4.m1.9.9.1.1.2"><times id="S5.E4.m1.9.9.1.1.2.3.cmml" xref="S5.E4.m1.9.9.1.1.2.3"></times><ci id="S5.E4.m1.9.9.1.1.2.4a.cmml" xref="S5.E4.m1.9.9.1.1.2.4"><mtext id="S5.E4.m1.9.9.1.1.2.4.cmml" xref="S5.E4.m1.9.9.1.1.2.4">median</mtext></ci><apply id="S5.E4.m1.9.9.1.1.2.2.3.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2"><csymbol cd="latexml" id="S5.E4.m1.9.9.1.1.2.2.3.1.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.3">conditional-set</csymbol><apply id="S5.E4.m1.9.9.1.1.1.1.1.1.cmml" xref="S5.E4.m1.9.9.1.1.1.1.1.1"><times id="S5.E4.m1.9.9.1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.9.9.1.1.1.1.1.1.1"></times><ci id="S5.E4.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S5.E4.m1.9.9.1.1.1.1.1.1.2">ùêº</ci><interval closure="open" id="S5.E4.m1.9.9.1.1.1.1.1.1.3.1.cmml" xref="S5.E4.m1.9.9.1.1.1.1.1.1.3.2"><ci id="S5.E4.m1.3.3.cmml" xref="S5.E4.m1.3.3">ùëñ</ci><ci id="S5.E4.m1.4.4.cmml" xref="S5.E4.m1.4.4">ùëó</ci></interval></apply><apply id="S5.E4.m1.9.9.1.1.2.2.2.2.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2"><in id="S5.E4.m1.9.9.1.1.2.2.2.2.1.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.1"></in><interval closure="open" id="S5.E4.m1.9.9.1.1.2.2.2.2.2.1.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.2.2"><ci id="S5.E4.m1.5.5.cmml" xref="S5.E4.m1.5.5">ùëñ</ci><ci id="S5.E4.m1.6.6.cmml" xref="S5.E4.m1.6.6">ùëó</ci></interval><apply id="S5.E4.m1.9.9.1.1.2.2.2.2.3.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3"><times id="S5.E4.m1.9.9.1.1.2.2.2.2.3.1.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.1"></times><ci id="S5.E4.m1.9.9.1.1.2.2.2.2.3.2.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.2">ùëä</ci><interval closure="open" id="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.1.cmml" xref="S5.E4.m1.9.9.1.1.2.2.2.2.3.3.2"><ci id="S5.E4.m1.7.7.cmml" xref="S5.E4.m1.7.7">ùë•</ci><ci id="S5.E4.m1.8.8.cmml" xref="S5.E4.m1.8.8">ùë¶</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.9c">I^{\prime}(x,y)=\text{median}\{I(i,j)\mid(i,j)\in W(x,y)\}~{}~{},</annotation><annotation encoding="application/x-llamapun" id="S5.E4.m1.9d">italic_I start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x , italic_y ) = median { italic_I ( italic_i , italic_j ) ‚à£ ( italic_i , italic_j ) ‚àà italic_W ( italic_x , italic_y ) } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS4.p3.4">where <math alttext="I(x,y)" class="ltx_Math" display="inline" id="S5.SS4.p3.1.m1.2"><semantics id="S5.SS4.p3.1.m1.2a"><mrow id="S5.SS4.p3.1.m1.2.3" xref="S5.SS4.p3.1.m1.2.3.cmml"><mi id="S5.SS4.p3.1.m1.2.3.2" xref="S5.SS4.p3.1.m1.2.3.2.cmml">I</mi><mo id="S5.SS4.p3.1.m1.2.3.1" xref="S5.SS4.p3.1.m1.2.3.1.cmml">‚Å¢</mo><mrow id="S5.SS4.p3.1.m1.2.3.3.2" xref="S5.SS4.p3.1.m1.2.3.3.1.cmml"><mo id="S5.SS4.p3.1.m1.2.3.3.2.1" stretchy="false" xref="S5.SS4.p3.1.m1.2.3.3.1.cmml">(</mo><mi id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">x</mi><mo id="S5.SS4.p3.1.m1.2.3.3.2.2" xref="S5.SS4.p3.1.m1.2.3.3.1.cmml">,</mo><mi id="S5.SS4.p3.1.m1.2.2" xref="S5.SS4.p3.1.m1.2.2.cmml">y</mi><mo id="S5.SS4.p3.1.m1.2.3.3.2.3" stretchy="false" xref="S5.SS4.p3.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.2b"><apply id="S5.SS4.p3.1.m1.2.3.cmml" xref="S5.SS4.p3.1.m1.2.3"><times id="S5.SS4.p3.1.m1.2.3.1.cmml" xref="S5.SS4.p3.1.m1.2.3.1"></times><ci id="S5.SS4.p3.1.m1.2.3.2.cmml" xref="S5.SS4.p3.1.m1.2.3.2">ùêº</ci><interval closure="open" id="S5.SS4.p3.1.m1.2.3.3.1.cmml" xref="S5.SS4.p3.1.m1.2.3.3.2"><ci id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">ùë•</ci><ci id="S5.SS4.p3.1.m1.2.2.cmml" xref="S5.SS4.p3.1.m1.2.2">ùë¶</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.2c">I(x,y)</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.1.m1.2d">italic_I ( italic_x , italic_y )</annotation></semantics></math> is the original image, <math alttext="I^{\prime}(x,y)" class="ltx_Math" display="inline" id="S5.SS4.p3.2.m2.2"><semantics id="S5.SS4.p3.2.m2.2a"><mrow id="S5.SS4.p3.2.m2.2.3" xref="S5.SS4.p3.2.m2.2.3.cmml"><msup id="S5.SS4.p3.2.m2.2.3.2" xref="S5.SS4.p3.2.m2.2.3.2.cmml"><mi id="S5.SS4.p3.2.m2.2.3.2.2" xref="S5.SS4.p3.2.m2.2.3.2.2.cmml">I</mi><mo id="S5.SS4.p3.2.m2.2.3.2.3" xref="S5.SS4.p3.2.m2.2.3.2.3.cmml">‚Ä≤</mo></msup><mo id="S5.SS4.p3.2.m2.2.3.1" xref="S5.SS4.p3.2.m2.2.3.1.cmml">‚Å¢</mo><mrow id="S5.SS4.p3.2.m2.2.3.3.2" xref="S5.SS4.p3.2.m2.2.3.3.1.cmml"><mo id="S5.SS4.p3.2.m2.2.3.3.2.1" stretchy="false" xref="S5.SS4.p3.2.m2.2.3.3.1.cmml">(</mo><mi id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">x</mi><mo id="S5.SS4.p3.2.m2.2.3.3.2.2" xref="S5.SS4.p3.2.m2.2.3.3.1.cmml">,</mo><mi id="S5.SS4.p3.2.m2.2.2" xref="S5.SS4.p3.2.m2.2.2.cmml">y</mi><mo id="S5.SS4.p3.2.m2.2.3.3.2.3" stretchy="false" xref="S5.SS4.p3.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.2b"><apply id="S5.SS4.p3.2.m2.2.3.cmml" xref="S5.SS4.p3.2.m2.2.3"><times id="S5.SS4.p3.2.m2.2.3.1.cmml" xref="S5.SS4.p3.2.m2.2.3.1"></times><apply id="S5.SS4.p3.2.m2.2.3.2.cmml" xref="S5.SS4.p3.2.m2.2.3.2"><csymbol cd="ambiguous" id="S5.SS4.p3.2.m2.2.3.2.1.cmml" xref="S5.SS4.p3.2.m2.2.3.2">superscript</csymbol><ci id="S5.SS4.p3.2.m2.2.3.2.2.cmml" xref="S5.SS4.p3.2.m2.2.3.2.2">ùêº</ci><ci id="S5.SS4.p3.2.m2.2.3.2.3.cmml" xref="S5.SS4.p3.2.m2.2.3.2.3">‚Ä≤</ci></apply><interval closure="open" id="S5.SS4.p3.2.m2.2.3.3.1.cmml" xref="S5.SS4.p3.2.m2.2.3.3.2"><ci id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">ùë•</ci><ci id="S5.SS4.p3.2.m2.2.2.cmml" xref="S5.SS4.p3.2.m2.2.2">ùë¶</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.2c">I^{\prime}(x,y)</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.2.m2.2d">italic_I start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x , italic_y )</annotation></semantics></math> is the filtered image, and<math alttext="W(x,y)" class="ltx_Math" display="inline" id="S5.SS4.p3.3.m3.2"><semantics id="S5.SS4.p3.3.m3.2a"><mrow id="S5.SS4.p3.3.m3.2.3" xref="S5.SS4.p3.3.m3.2.3.cmml"><mi id="S5.SS4.p3.3.m3.2.3.2" xref="S5.SS4.p3.3.m3.2.3.2.cmml">W</mi><mo id="S5.SS4.p3.3.m3.2.3.1" xref="S5.SS4.p3.3.m3.2.3.1.cmml">‚Å¢</mo><mrow id="S5.SS4.p3.3.m3.2.3.3.2" xref="S5.SS4.p3.3.m3.2.3.3.1.cmml"><mo id="S5.SS4.p3.3.m3.2.3.3.2.1" stretchy="false" xref="S5.SS4.p3.3.m3.2.3.3.1.cmml">(</mo><mi id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml">x</mi><mo id="S5.SS4.p3.3.m3.2.3.3.2.2" xref="S5.SS4.p3.3.m3.2.3.3.1.cmml">,</mo><mi id="S5.SS4.p3.3.m3.2.2" xref="S5.SS4.p3.3.m3.2.2.cmml">y</mi><mo id="S5.SS4.p3.3.m3.2.3.3.2.3" stretchy="false" xref="S5.SS4.p3.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.2b"><apply id="S5.SS4.p3.3.m3.2.3.cmml" xref="S5.SS4.p3.3.m3.2.3"><times id="S5.SS4.p3.3.m3.2.3.1.cmml" xref="S5.SS4.p3.3.m3.2.3.1"></times><ci id="S5.SS4.p3.3.m3.2.3.2.cmml" xref="S5.SS4.p3.3.m3.2.3.2">ùëä</ci><interval closure="open" id="S5.SS4.p3.3.m3.2.3.3.1.cmml" xref="S5.SS4.p3.3.m3.2.3.3.2"><ci id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1">ùë•</ci><ci id="S5.SS4.p3.3.m3.2.2.cmml" xref="S5.SS4.p3.3.m3.2.2">ùë¶</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.2c">W(x,y)</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.3.m3.2d">italic_W ( italic_x , italic_y )</annotation></semantics></math> is the neighborhood window centered at <math alttext="(x,y)" class="ltx_Math" display="inline" id="S5.SS4.p3.4.m4.2"><semantics id="S5.SS4.p3.4.m4.2a"><mrow id="S5.SS4.p3.4.m4.2.3.2" xref="S5.SS4.p3.4.m4.2.3.1.cmml"><mo id="S5.SS4.p3.4.m4.2.3.2.1" stretchy="false" xref="S5.SS4.p3.4.m4.2.3.1.cmml">(</mo><mi id="S5.SS4.p3.4.m4.1.1" xref="S5.SS4.p3.4.m4.1.1.cmml">x</mi><mo id="S5.SS4.p3.4.m4.2.3.2.2" xref="S5.SS4.p3.4.m4.2.3.1.cmml">,</mo><mi id="S5.SS4.p3.4.m4.2.2" xref="S5.SS4.p3.4.m4.2.2.cmml">y</mi><mo id="S5.SS4.p3.4.m4.2.3.2.3" stretchy="false" xref="S5.SS4.p3.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.4.m4.2b"><interval closure="open" id="S5.SS4.p3.4.m4.2.3.1.cmml" xref="S5.SS4.p3.4.m4.2.3.2"><ci id="S5.SS4.p3.4.m4.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1">ùë•</ci><ci id="S5.SS4.p3.4.m4.2.2.cmml" xref="S5.SS4.p3.4.m4.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.4.m4.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p3.4.m4.2d">( italic_x , italic_y )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p4.1.1">Morphological opening and closing:</span> the application of such filters enhances the overall quality of the segmentation by removing small noise artifacts and refining object boundaries. The opening filter effectively eliminates small, isolated regions of misclassified pixels, while the closing filter fills in small gaps and smoothens the contours of classified regions, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#bib.bib47" title="">47</a>]</cite>. Morphological filters presented respectively in Equations <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.E5" title="In 5.4 Post-Processing ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S5.E6" title="In 5.4 Post-Processing ‚Ä£ 5 Model Architecture &amp; Training ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">6</span></a></p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<table class="ltx_equation ltx_eqn_table" id="S5.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I\circ B=(I\ominus B)\oplus B~{}~{}," class="ltx_Math" display="block" id="S5.E5.m1.1"><semantics id="S5.E5.m1.1a"><mrow id="S5.E5.m1.1.1.1" xref="S5.E5.m1.1.1.1.1.cmml"><mrow id="S5.E5.m1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.cmml"><mrow id="S5.E5.m1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.1.1.3.2" xref="S5.E5.m1.1.1.1.1.3.2.cmml">I</mi><mo id="S5.E5.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S5.E5.m1.1.1.1.1.3.1.cmml">‚àò</mo><mi id="S5.E5.m1.1.1.1.1.3.3" xref="S5.E5.m1.1.1.1.1.3.3.cmml">B</mi></mrow><mo id="S5.E5.m1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E5.m1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.cmml"><mrow id="S5.E5.m1.1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E5.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E5.m1.1.1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E5.m1.1.1.1.1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mo id="S5.E5.m1.1.1.1.1.1.1.1.1.1" xref="S5.E5.m1.1.1.1.1.1.1.1.1.1.cmml">‚äñ</mo><mi id="S5.E5.m1.1.1.1.1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.1.1.1.1.3.cmml">B</mi></mrow><mo id="S5.E5.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E5.m1.1.1.1.1.1.2" xref="S5.E5.m1.1.1.1.1.1.2.cmml">‚äï</mo><mi id="S5.E5.m1.1.1.1.1.1.3" xref="S5.E5.m1.1.1.1.1.1.3.cmml">B</mi></mrow></mrow><mo id="S5.E5.m1.1.1.1.2" lspace="0.660em" xref="S5.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E5.m1.1b"><apply id="S5.E5.m1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1"><eq id="S5.E5.m1.1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1.2"></eq><apply id="S5.E5.m1.1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.1.3"><compose id="S5.E5.m1.1.1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.1.1.3.1"></compose><ci id="S5.E5.m1.1.1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.1.1.3.2">ùêº</ci><ci id="S5.E5.m1.1.1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.1.1.3.3">ùêµ</ci></apply><apply id="S5.E5.m1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E5.m1.1.1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1.1.2">direct-sum</csymbol><apply id="S5.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E5.m1.1.1.1.1.1.1.1.1.1">symmetric-difference</csymbol><ci id="S5.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E5.m1.1.1.1.1.1.1.1.1.2">ùêº</ci><ci id="S5.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.1.1.1.1.1.3">ùêµ</ci></apply><ci id="S5.E5.m1.1.1.1.1.1.3.cmml" xref="S5.E5.m1.1.1.1.1.1.3">ùêµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E5.m1.1c">I\circ B=(I\ominus B)\oplus B~{}~{},</annotation><annotation encoding="application/x-llamapun" id="S5.E5.m1.1d">italic_I ‚àò italic_B = ( italic_I ‚äñ italic_B ) ‚äï italic_B ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS4.p5.4">where <math alttext="I" class="ltx_Math" display="inline" id="S5.SS4.p5.1.m1.1"><semantics id="S5.SS4.p5.1.m1.1a"><mi id="S5.SS4.p5.1.m1.1.1" xref="S5.SS4.p5.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.1.m1.1b"><ci id="S5.SS4.p5.1.m1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.1.m1.1d">italic_I</annotation></semantics></math> is the original image, <math alttext="B" class="ltx_Math" display="inline" id="S5.SS4.p5.2.m2.1"><semantics id="S5.SS4.p5.2.m2.1a"><mi id="S5.SS4.p5.2.m2.1.1" xref="S5.SS4.p5.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.2.m2.1b"><ci id="S5.SS4.p5.2.m2.1.1.cmml" xref="S5.SS4.p5.2.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.2.m2.1d">italic_B</annotation></semantics></math> is the structuring element, <math alttext="\ominus" class="ltx_Math" display="inline" id="S5.SS4.p5.3.m3.1"><semantics id="S5.SS4.p5.3.m3.1a"><mo id="S5.SS4.p5.3.m3.1.1" xref="S5.SS4.p5.3.m3.1.1.cmml">‚äñ</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.3.m3.1b"><csymbol cd="latexml" id="S5.SS4.p5.3.m3.1.1.cmml" xref="S5.SS4.p5.3.m3.1.1">symmetric-difference</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.3.m3.1c">\ominus</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.3.m3.1d">‚äñ</annotation></semantics></math> denotes the erosion operation, <math alttext="\oplus" class="ltx_Math" display="inline" id="S5.SS4.p5.4.m4.1"><semantics id="S5.SS4.p5.4.m4.1a"><mo id="S5.SS4.p5.4.m4.1.1" xref="S5.SS4.p5.4.m4.1.1.cmml">‚äï</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.4.m4.1b"><csymbol cd="latexml" id="S5.SS4.p5.4.m4.1.1.cmml" xref="S5.SS4.p5.4.m4.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.4.m4.1c">\oplus</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p5.4.m4.1d">‚äï</annotation></semantics></math> denotes the dilation operation.</p>
</div>
<div class="ltx_para" id="S5.SS4.p6">
<table class="ltx_equation ltx_eqn_table" id="S5.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I\bullet B=(I\oplus B)\ominus B~{}~{}," class="ltx_Math" display="block" id="S5.E6.m1.1"><semantics id="S5.E6.m1.1a"><mrow id="S5.E6.m1.1.1.1" xref="S5.E6.m1.1.1.1.1.cmml"><mrow id="S5.E6.m1.1.1.1.1" xref="S5.E6.m1.1.1.1.1.cmml"><mrow id="S5.E6.m1.1.1.1.1.3" xref="S5.E6.m1.1.1.1.1.3.cmml"><mi id="S5.E6.m1.1.1.1.1.3.2" xref="S5.E6.m1.1.1.1.1.3.2.cmml">I</mi><mo id="S5.E6.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S5.E6.m1.1.1.1.1.3.1.cmml">‚àô</mo><mi id="S5.E6.m1.1.1.1.1.3.3" xref="S5.E6.m1.1.1.1.1.3.3.cmml">B</mi></mrow><mo id="S5.E6.m1.1.1.1.1.2" xref="S5.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E6.m1.1.1.1.1.1" xref="S5.E6.m1.1.1.1.1.1.cmml"><mrow id="S5.E6.m1.1.1.1.1.1.1.1" xref="S5.E6.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E6.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E6.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E6.m1.1.1.1.1.1.1.1.1" xref="S5.E6.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E6.m1.1.1.1.1.1.1.1.1.2" xref="S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mo id="S5.E6.m1.1.1.1.1.1.1.1.1.1" xref="S5.E6.m1.1.1.1.1.1.1.1.1.1.cmml">‚äï</mo><mi id="S5.E6.m1.1.1.1.1.1.1.1.1.3" xref="S5.E6.m1.1.1.1.1.1.1.1.1.3.cmml">B</mi></mrow><mo id="S5.E6.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E6.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S5.E6.m1.1.1.1.1.1.2" xref="S5.E6.m1.1.1.1.1.1.2.cmml">‚äñ</mo><mi id="S5.E6.m1.1.1.1.1.1.3" xref="S5.E6.m1.1.1.1.1.1.3.cmml">B</mi></mrow></mrow><mo id="S5.E6.m1.1.1.1.2" lspace="0.660em" xref="S5.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E6.m1.1b"><apply id="S5.E6.m1.1.1.1.1.cmml" xref="S5.E6.m1.1.1.1"><eq id="S5.E6.m1.1.1.1.1.2.cmml" xref="S5.E6.m1.1.1.1.1.2"></eq><apply id="S5.E6.m1.1.1.1.1.3.cmml" xref="S5.E6.m1.1.1.1.1.3"><ci id="S5.E6.m1.1.1.1.1.3.1.cmml" xref="S5.E6.m1.1.1.1.1.3.1">‚àô</ci><ci id="S5.E6.m1.1.1.1.1.3.2.cmml" xref="S5.E6.m1.1.1.1.1.3.2">ùêº</ci><ci id="S5.E6.m1.1.1.1.1.3.3.cmml" xref="S5.E6.m1.1.1.1.1.3.3">ùêµ</ci></apply><apply id="S5.E6.m1.1.1.1.1.1.cmml" xref="S5.E6.m1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E6.m1.1.1.1.1.1.2.cmml" xref="S5.E6.m1.1.1.1.1.1.2">symmetric-difference</csymbol><apply id="S5.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E6.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E6.m1.1.1.1.1.1.1.1.1.1">direct-sum</csymbol><ci id="S5.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E6.m1.1.1.1.1.1.1.1.1.2">ùêº</ci><ci id="S5.E6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E6.m1.1.1.1.1.1.1.1.1.3">ùêµ</ci></apply><ci id="S5.E6.m1.1.1.1.1.1.3.cmml" xref="S5.E6.m1.1.1.1.1.1.3">ùêµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E6.m1.1c">I\bullet B=(I\oplus B)\ominus B~{}~{},</annotation><annotation encoding="application/x-llamapun" id="S5.E6.m1.1d">italic_I ‚àô italic_B = ( italic_I ‚äï italic_B ) ‚äñ italic_B ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS4.p6.4">where <math alttext="I" class="ltx_Math" display="inline" id="S5.SS4.p6.1.m1.1"><semantics id="S5.SS4.p6.1.m1.1a"><mi id="S5.SS4.p6.1.m1.1.1" xref="S5.SS4.p6.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p6.1.m1.1b"><ci id="S5.SS4.p6.1.m1.1.1.cmml" xref="S5.SS4.p6.1.m1.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p6.1.m1.1c">I</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p6.1.m1.1d">italic_I</annotation></semantics></math> is the original image,
<math alttext="B" class="ltx_Math" display="inline" id="S5.SS4.p6.2.m2.1"><semantics id="S5.SS4.p6.2.m2.1a"><mi id="S5.SS4.p6.2.m2.1.1" xref="S5.SS4.p6.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p6.2.m2.1b"><ci id="S5.SS4.p6.2.m2.1.1.cmml" xref="S5.SS4.p6.2.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p6.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p6.2.m2.1d">italic_B</annotation></semantics></math> is the structuring element,
<math alttext="\oplus" class="ltx_Math" display="inline" id="S5.SS4.p6.3.m3.1"><semantics id="S5.SS4.p6.3.m3.1a"><mo id="S5.SS4.p6.3.m3.1.1" xref="S5.SS4.p6.3.m3.1.1.cmml">‚äï</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p6.3.m3.1b"><csymbol cd="latexml" id="S5.SS4.p6.3.m3.1.1.cmml" xref="S5.SS4.p6.3.m3.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p6.3.m3.1c">\oplus</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p6.3.m3.1d">‚äï</annotation></semantics></math> denotes the dilation operation,
<math alttext="\ominus" class="ltx_Math" display="inline" id="S5.SS4.p6.4.m4.1"><semantics id="S5.SS4.p6.4.m4.1a"><mo id="S5.SS4.p6.4.m4.1.1" xref="S5.SS4.p6.4.m4.1.1.cmml">‚äñ</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p6.4.m4.1b"><csymbol cd="latexml" id="S5.SS4.p6.4.m4.1.1.cmml" xref="S5.SS4.p6.4.m4.1.1">symmetric-difference</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p6.4.m4.1c">\ominus</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p6.4.m4.1d">‚äñ</annotation></semantics></math> denotes the erosion operation.</p>
</div>
<div class="ltx_para" id="S5.SS4.p7">
<p class="ltx_p" id="S5.SS4.p7.1">Together, these morphological operations improve the structural integrity of the classification map, leading to more accurate and visually coherent segmentation results.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Results &amp; Discussion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section the results of the proposed work on pixel-level material classification of hyperspectral images are presented, the structure of the sections is as follows a) overall performance, b) capacity to analyse randomly shredded objects, c) ability to distinguish mixed overlapping materials and d) limitations are discussed.</p>
</div>
<section class="ltx_subsection" id="S6.SSx1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Overall Performance</h3>
<div class="ltx_para" id="S6.SSx1.p1">
<p class="ltx_p" id="S6.SSx1.p1.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4" title="4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">4</span></a> the test-set presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S4.T3" title="TABLE III ‚Ä£ 4.1 Dataset description ‚Ä£ 4 Dataset ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">III</span></a> includes images of all the HDPE, PET, PP, PS classes, which however have not been utilised in the training subset. In this manner, guarantee is provided that no same object or pixel is simultaneously evident in both train and test data.</p>
</div>
<div class="ltx_para" id="S6.SSx1.p2">
<p class="ltx_p" id="S6.SSx1.p2.1">The hyperspectral images of this dataset are used by P1CH classifier in order to predict the material class of each pixel. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F7" title="Figure 7 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a> shows the original image as well as the ground truth and the prediction of each pixel.</p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="366" id="S6.F7.g1" src="extracted/5862626/imgs2/results1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The false-colour version of the HS images, the ground truth mask, as well as the generated classification map from the proposed model.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx1.p3">
<p class="ltx_p" id="S6.SSx1.p3.1">To quantify the model‚Äôs performance, the accuracy score was calculated for the whole test set in pixel level thus providing a detailed evaluation on the model‚Äôs classification capacity. The overall accuracy achieved by the model is <span class="ltx_text ltx_font_bold" id="S6.SSx1.p3.1.1">97.44</span>%. The confusion matrix is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F8" title="Figure 8 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">8</span></a>, summarizing the classification performance among different material classes.</p>
</div>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S6.F8.g1" src="extracted/5862626/imgs/Confusion_matrix.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Confusion Matrix describing models performance in classifying materials on pixel-level.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx1.p4">
<p class="ltx_p" id="S6.SSx1.p4.1">The cell values in the confusion matrix are row-wise normalized, i.e. normalized with respect to the total number of samples in each class.
By observing the confusion matrix of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F8" title="Figure 8 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">8</span></a> is easy to notice that less than <span class="ltx_text ltx_font_italic" id="S6.SSx1.p4.1.1">1</span>% of the total pixels of each class is falsely classified as another material and most of the errors are misclassifications between materials and the Background.</p>
</div>
<div class="ltx_para" id="S6.SSx1.p5">
<p class="ltx_p" id="S6.SSx1.p5.1">It can be calculated that <span class="ltx_text ltx_font_italic" id="S6.SSx1.p5.1.1">97.45</span>% of the error comes in the borders between objects and the background. After more careful inspection is not easy to conclude that the error is actually on the predictions of P1CH and not on the ground truth masks. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F9" title="Figure 9 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">9</span></a> presents two zoomed-in crops of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F7" title="Figure 7 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a>, where it is clearly shown that in both cases the P1CH has the capacity to generate much smoother and precise masks for the respective object while simultaneously predicting correctly the material class. Comparing, also, the zoomed crops one can realize the mistake made on the top right side of the PP sample‚Äôs mask, as well as at the PET bottle‚Äôs spout. In both cases, P1CH was capable of predicting a more precise than the semi-automatically generated mask.</p>
</div>
<figure class="ltx_figure" id="S6.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S6.F9.g1" src="extracted/5862626/imgs2/zoom.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Two zoomed crops of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F7" title="Figure 7 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a>. In the first row the PP sample originally located on the bottom right-side of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F7" title="Figure 7 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a> is depicted with the GT and Predicted masks. In the second row the PET sample originally located on the left-side of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F7" title="Figure 7 ‚Ä£ Overall Performance ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">7</span></a> is presented.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx1.p6">
<p class="ltx_p" id="S6.SSx1.p6.1">Therefore, given the aforementioned observations if the misclassifications on the borders are excluded from error calculations, the updated error rate is <span class="ltx_text ltx_font_italic" id="S6.SSx1.p6.1.1">0.0653</span>% and hence the total pixel-wise accuracy of P1CH classifier is <span class="ltx_text ltx_font_bold" id="S6.SSx1.p6.1.2">99.94</span>%!</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Shredded materials</h3>
<div class="ltx_para" id="S6.SSx2.p1">
<p class="ltx_p" id="S6.SSx2.p1.1">Beyond overall accuracy metrics, evaluating the model‚Äôs effectiveness in classifying randomly shaped plastic samples is critical, as these shapes present significant challenges for RGB computer vision models and the same time is needed in various real-life applications. In this manner, an ensemble of plastic samples was collected and shredded in small, irregular pieces, and then placed on the conveyor belt. P1CH classifier demonstrates remarkable capabilities in classifying on pixel-level the different material classes, clearly proving its superior performance compared to traditional RGB-based instance segmentation algorithms.</p>
</div>
<div class="ltx_para" id="S6.SSx2.p2">
<p class="ltx_p" id="S6.SSx2.p2.1">Traditional RGB models often struggle with such high variability due to their reliance on surface-level features like colour, texture and shape, which are limited in the case of small shredded samples. On the contrary, the proposed hyperspectral imaging approach is trained to purely exploit the rich spectral content of each pixel in the HS image, hence successfully tackling the challenge of small and irregularly shaped objects. Another key advantage of the proposed model is its resilience to noise and artifacts commonly found in RGB images. While traditional models can be easily misled by variations in lighting and surface texture, the spectral information utilised by the proposed model provides a more stable basis for classification.</p>
</div>
<div class="ltx_para" id="S6.SSx2.p3">
<p class="ltx_p" id="S6.SSx2.p3.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F10" title="Figure 10 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">10</span></a> the HS and the respective RGB images of the shredded plastic samples of HDPE, PET, PP and PS are depicted. Along with the aforementioned images, the Ground Truth and the Predicted masks are presented in the same figure. The achieved accuracy in this specific image is <span class="ltx_text ltx_font_italic" id="S6.SSx2.p3.1.1">98.9</span>%, with all the misclassifications falling under the borders case described in the previous section.</p>
</div>
<figure class="ltx_figure" id="S6.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S6.F10.g1" src="extracted/5862626/imgs2/results_p2_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>From left to right: The false-colour HS image, the RGB equivalent image, the Ground Truth mask, as well as the generated classification map, by the P1CH Classifier, in a challenging, cluttered scene, where the objects are small with irregular shapes and similar textures.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx2.p4">
<p class="ltx_p" id="S6.SSx2.p4.1">Visual comparison between the ground truth and the predicted mask indicates a high level of agreement in the classification of the plastic fragments. Each class‚ÄîHDPE, PET, PP, and PS‚Äîis distinctly identified and accurately located in the predicted mask. In detail, even though the samples have similar colours regardless of their class, being either white or transparent, the model was able to correctly classify all the samples to their respective category, as indicated in the ground truth mask. This result underscores the robustness of P1CH is <span class="ltx_text ltx_font_bold" id="S6.SSx2.p4.1.1">invariant to colour and to size</span> of the objects by solely relying on the spectral content of each individual pixel.</p>
</div>
<div class="ltx_para" id="S6.SSx2.p5">
<p class="ltx_p" id="S6.SSx2.p5.1">Even though the points just presented are important, the most impressive observation from this experiment is the model‚Äôs ability to detect objects that were not even visually identified during the labeling process of the HS images. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F11" title="Figure 11 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">11</span></a> specifies two regions in the HS image, denoted with red and green boxes respectively, in which there are two small shards of PET samples. Inspecting the zoomed-in crops presenting in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F11" title="Figure 11 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">11</span></a>. It is almost impossible to detect those objects, yet the proposed HS approach clearly identified these two objects as PET. This outcome is only possible through the analytical processing of the spectral information of pixels in those regions, resulting in very precise masks and ultimately correctly classified samples.</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="275" id="S6.F11.g1" src="extracted/5862626/imgs/amazing.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Zoomed-in crops of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F10" title="Figure 10 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">10</span></a>, highlighting two small PET shards that were mistakenly omitted from the labelling process (denoted with red and green boxes), and yet were detected by the model.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="232" id="S6.F12.g1" src="extracted/5862626/imgs2/results_p1.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>From left to right: The false-colour HS image, the RGB equivalent image, the Ground Truth mask, the classification maps generated by an RGB-based model using the RGB equivalent images,as well as the generated classification map, by the P1CH Classifier, in the scenario of mixed or overlapping materials.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SSx3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Mixed overlapping materials</h3>
<div class="ltx_para" id="S6.SSx3.p1">
<p class="ltx_p" id="S6.SSx3.p1.1">In addition to the challenge posed by small irregularly shaped objects, conventional RGB-based detection models often fail to accurately identify distinct samples that are either attached to one another or overlapping. In this subsection, an analysis is conducted to evaluate the model‚Äôs capacity to precisely segment samples of different material class, while accurately classifying at the same time each individual pixel to its respective class.
To this end, two experiments were executed, and in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F12" title="Figure 12 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">12</span></a> the HS and RGB images and the respective masks of the utilised objects are depicted. To further highlight the proposed model‚Äôs ability to accurately classify materials and segment their instances within an image, a fine-tuned on the specific classes RGB-based instance segmentation model was also employed to segment the RGB images. The predictions of the RGB-based model are depicted in the 4th column of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F12" title="Figure 12 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SSx3.p2">
<p class="ltx_p" id="S6.SSx3.p2.1">In the first experiment, a common, commercially available, light blue shampoo bottle was selected. From a visual point of view, as seen in the RGB image, the body of the shampoo container and its lid appear almost identical making it impossible, even for a human, to realize that those two parts are different types of plastic, i.e. HDPE and PP. From the first row in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F12" title="Figure 12 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">12</span></a>, one can easily observe that the RGB-based model was able to identify the contour of greater complex of objects, but not the individual parts, i.e. body and lid. Moreover, the RGB-based model failed to predict the class of both samples, assigning them to the PET class. Finally, due to shadows and non uniform illumination of the scene, the RGB generated mask lacks in precision as it includes pixels of the background too. On the contrary, the proposed HS approach, not only generated highly precise masks for both the body and the lid, but also is capable of classifying those two parts in their respective class. The overall accuracy of the prediction for this specific test case is <span class="ltx_text ltx_font_italic" id="S6.SSx3.p2.1.1">98.8</span>%, with disagreements in a pixel‚Äôs class being evident only between background and not the material classes.</p>
</div>
<div class="ltx_para" id="S6.SSx3.p3">
<p class="ltx_p" id="S6.SSx3.p3.1">In the second experiment, the case of overlapping materials with very similar (white) colour was examined. To this end, a white PS flat surface was selected, on top which a white PP (top right) and a white HDPE (bottom left) lid were placed. The respective masks and images are depicted in the second row of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F12" title="Figure 12 ‚Ä£ Shredded materials ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">12</span></a>. Looking at the RGB image, no distinctive textures can be detect for the two lids, while some minor changes in texture may be identified between the lids and the PS surface. Therefore, given the uniformity in colour and the very low variance in texture, as anticipated, the RGB-based model fail to detect all three objects apparent in the scene. Once again, the generated mask contains the greater complex of samples with no regard to the two lids. Moreover, the class prediction of the RGB-based is inaccurate since it considers the PS surface as PP. In contrast to the RGB case, the proposed HS model effectively utilises the rich spectral signature encoded in each pixel being able to precisely segment all three components of the image, while classyfing materials on pixel-level with <span class="ltx_text ltx_font_italic" id="S6.SSx3.p3.1.1">99.54</span>% accuarcy.</p>
</div>
<div class="ltx_para" id="S6.SSx3.p4">
<p class="ltx_p" id="S6.SSx3.p4.1">The results presented above underscore a pivotal advancement in material classification and segmentation capabilities. The most remarkable achievement demonstrated here is the proposed HS model‚Äôs ability to accurately identify and classify distinct, overlapping samples with complex boundaries. This marks a significant breakthrough, as conventional RGB-based models consistently fail under these conditions, misidentifying materials and producing imprecise masks. The superior performance of P1CH classifier achieving up to <span class="ltx_text ltx_font_bold" id="S6.SSx3.p4.1.1">99.54</span>% accuracy even in the presence of overlapping objects, illustrates a transformative improvement in hyperspectral imaging applications, paving the way for more sophisticated and reliable material detection and sorting systems in real-world environments.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SSx4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">Limitations</h3>
<div class="ltx_para" id="S6.SSx4.p1">
<p class="ltx_p" id="S6.SSx4.p1.1">Despite the impressive results acquired in the previous experiments, this work intendeds to also underline the limitations of the HS imaging in classifying materials. To this end, the last experiment involves the analysis of dark-coloured, irregularly shaped samples. For the needs of this experiment, black and dark-coloured HDPE, PP and PS objects were cut in random small fractions and placed on top of the conveyor belt. The respective images and masks are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F13" title="Figure 13 ‚Ä£ Limitations ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">13</span></a>. This subsection delves into the acquired results of the analysis of dark samples, discussing also the reasons why the model performs poorly in that case.</p>
</div>
<div class="ltx_para" id="S6.SSx4.p2">
<p class="ltx_p" id="S6.SSx4.p2.1">Examining the predictions, in the last column of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F13" title="Figure 13 ‚Ä£ Limitations ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">13</span></a> it is concluded that the model performs very poorly in the case of black plastics. In detail, only PP samples were are correctly classified, although lacking in mask precision. The PS samples were not completely undetected, while some HDPE samples were identified by the model but once again misclassified as PS or PP. The performance of the proposed model in terms of accuracy without taking into consideration the background is equal to merely <span class="ltx_text ltx_font_italic" id="S6.SSx4.p2.1.1">39.69</span>%.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="185" id="S6.F13.g1" src="extracted/5862626/imgs2/blackPlastics.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>From left to right: The false-colour HS image, the RGB equivalent image, the Ground Truth mask, as well as the generated classification map, by the P1CH Classifier, in the case of black plastics.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx4.p3">
<p class="ltx_p" id="S6.SSx4.p3.1">The confusion matrix presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F14" title="Figure 14 ‚Ä£ Limitations ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">14</span></a> specifically summarizes the models performance in the case of black samples. In detail, it is confirmed that all the PS pixels were classified as background, i.e. they were not detected by the model at all. Moreover, from this confusion matrix one can see that indeed the PP samples were accurately classified, with the models precision in that case dropping to <span class="ltx_text ltx_font_italic" id="S6.SSx4.p3.1.1">75</span>% since HDPE samples are also misclassifies as PP. Finally, for the HDPE case, only <span class="ltx_text ltx_font_italic" id="S6.SSx4.p3.1.2">4.01</span>% of the total samples were correctly classified, with the model‚Äôs recall for the specific case being equal to <span class="ltx_text ltx_font_italic" id="S6.SSx4.p3.1.3">46</span>%.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S6.F14.g1" src="extracted/5862626/imgs/cfm_blacks.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Normalized Confusion Matrix for the predictions of the model in the case of black and dark-coloured samples.</figcaption>
</figure>
<div class="ltx_para" id="S6.SSx4.p4">
<p class="ltx_p" id="S6.SSx4.p4.1">At this point, it of paramount importance to explain the reasons that lead in this drop in the model‚Äôs performance. Specifically, this phenomenon should mainly be attributed to the nature of dark-coloured materials, rather than being considered a model‚Äôs deficiency. In detail, the observed dark or even black colour of an object is the result of complete or almost complete absorption of the incident radiation. In this manner, the reflected radiation, which is captured by the HS cameras‚Äô sensor, is of very low intensity; hence resulting in a very weak digital signal. Therefore, given the low amplitude of the captured signal, the PSNR is consequently also low, and so is the variability of the spectrum. All the above, result in very similar -almost identical-, noisy features of the model‚Äôs input vector, thus rendering the model incapable of properly analyzing each pixel‚Äôs spectrum and ultimately correctly classifying them in their respective classes. An example of the spectrum of a black and a white plastic of the same material are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13498v1#S6.F15" title="Figure 15 ‚Ä£ Limitations ‚Ä£ 6 Results &amp; Discussion ‚Ä£ A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging"><span class="ltx_text ltx_ref_tag">15</span></a>. Can be easily observed that the spectrum of the black-colored object is much more noisy with <math alttext="10" class="ltx_Math" display="inline" id="S6.SSx4.p4.1.m1.1"><semantics id="S6.SSx4.p4.1.m1.1a"><mn id="S6.SSx4.p4.1.m1.1.1" xref="S6.SSx4.p4.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SSx4.p4.1.m1.1b"><cn id="S6.SSx4.p4.1.m1.1.1.cmml" type="integer" xref="S6.SSx4.p4.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SSx4.p4.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S6.SSx4.p4.1.m1.1d">10</annotation></semantics></math> times lower intensity.</p>
</div>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S6.F15.g1" src="extracted/5862626/imgs2/blackRaman.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>A comparison between the spectra acquired from a black (left) and a white (right) PS sample. A significant drop in the signal‚Äôs amplitude can be observed in the case of black PS. Also one can notice that most of the indicative peaks for PS are non-existent in the case of black PS. </figcaption>
</figure>
<div class="ltx_para" id="S6.SSx4.p5">
<p class="ltx_p" id="S6.SSx4.p5.1">Although, it should be highlighted that black plastics of any material class were totally absent from the training set. The exploration of techniques that would probably lead to mitigate the pure performance of computer vision with HS imaging on black objects is beyond the scope of this paper, but opens and intriguing research direction on the intersection between material science and artificial intelligence.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion &amp; Future Work</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper a lightweight <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">1D Convolutional Neural Hyperspectral Classification</span>, system, P1CH, is proposed. The developed model generates highly detailed and precise classification maps. Unlike conventional RGB methods, the proposed algorithm utilises the spectral information encoded in hyperspectral image‚Äôs pixel, allowing to detect and correctly classify objects in challenging scenarios.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">For the training and validation purposes two sets of hyperspectral images were generated. Both of the dataset splits contain images of <span class="ltx_text ltx_font_italic" id="S7.p2.1.1">HDPE, PET, PP</span> and <span class="ltx_text ltx_font_italic" id="S7.p2.1.2">PS</span> samples. Each of the images in the training set, contain explicitly objects of one class, while the images in the test set aim to reproduce challenging conditions, hence depicting very small irregularly shaped objects, overlapping objects, and mixed materials with similar to identical texture. A simplified, cost-efficient spectral calibration and normalization technique is, also, proposed in this work that do not requires specialized hardware. The model was validated on the aforementioned test set and it achieved an overall accuracy of <span class="ltx_text ltx_font_bold" id="S7.p2.1.3">99.54</span>%. Moreover, to explore the limitations of the P1CH Classifier the scenario of classifying black samples was explored, where it was noticed that the model struggled to properly classify the samples.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">On the one hand, this work presents the capabilities of computer vision with Hyperspectral imaging. On the other hand raised limitations and problems that are absolutely worth systematic research effort. Further investigation on the problem of black or dark-coloured samples. Study the capabilities on more extensive spectral range. Moreover, the model‚Äôs robustness in various illumination conditions is set to be further examined. Furthermore, dimensionality reduction and compression problems have to be examined, as well as the effects such operations might have on the learning performance. Finally, the utilisation of the spatial information encoded in hyperspectral images, shall also be examined in future studies.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research was financially supported by the European Union‚Äôs Horizon Europe research and innovation program under grant agreement No 101138789 (project W2W).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M.¬†J. Khan, H.¬†S. Khan, A.¬†Yousaf, K.¬†Khurshid, and A.¬†Abbas, ‚ÄúModern trends in hyperspectral image analysis: A review,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Ieee Access</em>, vol.¬†6, pp. 14‚Äâ118‚Äì14‚Äâ129, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B.¬†Lu, P.¬†D. Dao, J.¬†Liu, Y.¬†He, and J.¬†Shang, ‚ÄúRecent advances of hyperspectral imaging technology and applications in agriculture,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Remote Sensing</em>, vol.¬†12, no.¬†16, p. 2659, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.¬†D√≠az, R.¬†Guerra, P.¬†Horstrand, E.¬†Martel, S.¬†L√≥pez, J.¬†F. L√≥pez, and R.¬†Sarmiento, ‚ÄúReal-time hyperspectral image compression onto embedded gpus,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, vol.¬†12, no.¬†8, pp. 2792‚Äì2809, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
N.¬†Sharma, M.¬†S. Waseem, S.¬†Mirzaei, and M.¬†Hefeeda, ‚ÄúMobispectral: Hyperspectral imaging on mobile devices,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 29th Annual International Conference on Mobile Computing and Networking</em>, 2023, pp. 1‚Äì15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
O.¬†Lim, S.¬†Mancini, and M.¬†Dalla¬†Mura, ‚ÄúFeasibility of a real-time embedded hyperspectral compressive sensing imaging system,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Sensors</em>, vol.¬†22, no.¬†24, p. 9793, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B.¬†Przestrzelski, E.¬†Reddy, and S.¬†Lord, ‚ÄúIntegrating experiential with technical: How materials science modules can help redefine the traditional engineering canon,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ASEE Annual Conference</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K.¬†Jang, T.¬†Heo, and S.¬†Jeong, ‚ÄúClassification option for korean traditional paper based on type of raw materials, using near-infrared spectroscopy and multivariate statistical methods,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">BioResources</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D.¬†P. Penumuru, S.¬†Muthuswamy, and P.¬†Karumbu, ‚ÄúIdentification and classification of materials using machine vision and machine learning in the context of industry 4.0,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Journal of Intelligent Manufacturing</em>, vol.¬†31, pp. 1229‚Äì1241, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.¬†Chowdhury, E.¬†Kautz, B.¬†Yener, and D.¬†Lewis, ‚ÄúImage driven machine learning methods for microstructure recognition,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Computational Materials Science</em>, vol. 123, pp. 176‚Äì187, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L.¬†Zhang and Z.¬†Li, ‚ÄúMachine learning for materials classifications from images,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Journal of Physics: Conference Series</em>, vol. 2369, 2022, p. 012081.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L.¬†Zhang and S.¬†Shao, ‚ÄúImage-based machine learning for materials science,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Journal of Applied Physics</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.¬†Wei√ü and A.¬†Santra, ‚ÄúMaterial classification using 60-ghz radar and deep convolutional neural network,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Radar Conference</em>, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/abstract/document/9079136/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H.¬†Zheng, L.¬†Fang, M.¬†Ji, M.¬†Strese, and Y.¬†√ñzer, ‚ÄúDeep learning for surface material classification using haptic and visual information,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Automation Science and Engineering</em>, 2016. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/abstract/document/7530831/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
P.¬†Zoumpoulis, F.¬†K. Konstantinidis, G.¬†Tsimiklis, and A.¬†Amditis, ‚ÄúSmart bins for enhanced resource recovery and sustainable urban waste practices in smart cities: A systematic literature review,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Cities</em>, vol. 152, p. 105150, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúAdvancing urban waste management using industry 5.0 principles: A novel smart bin,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2024 IEEE International Workshop on Metrology for Industry 4.0 &amp; IoT (MetroInd4. 0 &amp; IoT)</em>.¬†¬†¬†IEEE, 2024, pp. i‚Äìvi.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
O.¬†Adedeji and Z.¬†Wang, ‚ÄúIntelligent waste classification system using deep learning convolutional neural network,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Procedia Manufacturing</em>, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S2351978919307231</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S.¬†Azimi, D.¬†Britz, M.¬†Engstler, M.¬†Fritz, and F.¬†M√ºcklich, ‚ÄúAdvanced steel microstructural classification by deep learning methods,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Scientific Reports</em>, 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nature.com/articles/s41598-018-20037-5</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F.¬†K. Konstantinidis, S.¬†Sifnaios, G.¬†Arvanitakis, G.¬†Tsimiklis, S.¬†G. Mouroutsos, A.¬†Amditis, and A.¬†Gasteratos, ‚ÄúMulti-modal sorting in plastic and wood waste streams,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Resources, Conservation and Recycling</em>, vol. 199, p. 107244, 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0921344923003786</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M.¬†Shaikh and B.¬†Th√∂rnberg, ‚ÄúImpact of water vapour on polymer classification using in situ short-wave infrared hyperspectral imaging,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Journal of Spectral Imaging</em>, 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.impopen.com/download.php?code=I11_a5</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.¬†Shaban, ‚ÄúDetermination of concrete properties using hyperspectral imaging technology: A review,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Science Journal of Physics</em>, 2013. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sjpub.org/sjcea/sjcea-102.pdf</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G.¬†Capobianco, F.¬†Prestileo, and S.¬†Serranti, ‚ÄúHyperspectral imaging-based approach for the in-situ characterization of ancient roman wall paintings,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Image Processing Theory, Tools and Applications</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.¬†Polak, T.¬†Kelman, P.¬†Murray, and S.¬†Marshall, ‚ÄúHyperspectral imaging combined with data classification techniques as an aid for artwork authentication,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Journal of Cultural Heritage</em>, 2017. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S1296207417301218</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J.¬†Banumathi and A.¬†Muthumari, ‚ÄúAn intelligent deep learning based xception model for hyperspectral image analysis and classification,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Computational Materials</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†Li, W.¬†Song, L.¬†Fang, and Y.¬†Chen, ‚ÄúDeep learning for hyperspectral image classification: An overview,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
L.¬†Windrim, A.¬†Melkumyan, and R.¬†Murphy, ‚ÄúPretraining for hyperspectral convolutional neural network classification,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.¬†Chen, K.¬†Zhu, L.¬†Zhu, X.¬†He, and P.¬†Ghamisi, ‚ÄúAutomatic design of convolutional neural network for hyperspectral image classification,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/abstract/document/8703410/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
B.¬†Pan, Z.¬†Shi, and X.¬†Xu, ‚ÄúR-vcanet: A new deep-learning-based hyperspectral image classification method,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2017. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://levir.buaa.edu.cn/publications/RVACNet-R2.pdf</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R.¬†Venkatesan and S.¬†Prabu, ‚ÄúHyperspectral image features classification using deep learning recurrent neural networks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of Medical Systems</em>, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://link.springer.com/article/10.1007/s10916-019-1347-9</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
F.¬†Xiong, J.¬†Zhou, and Y.¬†Qian, ‚ÄúMaterial based object tracking in hyperspectral videos,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IEEE Transactions on Image Processing</em>, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/pdf/1812.04179</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L.¬†Medus, M.¬†Saban, and J.¬†Franc√©s-V√≠llora, ‚ÄúHyperspectral image classification using cnn: Application to industrial food packaging,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Food Control</em>, 2021. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0956713521001006</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
N.¬†Okada, Y.¬†Maekawa, N.¬†Owada, K.¬†Haga, A.¬†Shibayama, and Y.¬†Kawamura, ‚ÄúAutomated identification of mineral types and grain size using hyperspectral imaging and deep learning for mineral processing,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Minerals</em>, vol.¬†10, no.¬†9, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.mdpi.com/2075-163X/10/9/809</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C.¬†Ni, Z.¬†Li, X.¬†Zhang, X.¬†Sun, Y.¬†Huang, L.¬†Zhao, T.¬†Zhu, and D.¬†Wang, ‚ÄúOnline sorting of the film on cotton based on deep learning and hyperspectral imaging,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IEEE Access</em>, vol.¬†8, pp. 93‚Äâ028‚Äì93‚Äâ038, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.¬†Zhu, L.¬†Zhou, P.¬†Gao, Y.¬†Bao, Y.¬†He, and L.¬†Feng, ‚ÄúNear-infrared hyperspectral imaging combined with deep learning to identify cotton seed varieties,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Molecules</em>, vol.¬†24, no.¬†18, 2019. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.mdpi.com/1420-3049/24/18/3268</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A.¬†Picon, P.¬†Galan, A.¬†Bereciartua-Perez, and L.¬†B. del Valle, ‚ÄúHyperspectral dataset and deep learning methods for waste from electric and electronic equipment identification (weee),‚Äù 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2407.04505</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D.¬†J. Brady, <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Optical imaging and spectroscopy</em>.¬†¬†¬†John Wiley &amp; Sons, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S.¬†Sifnaios, I.¬†Zorzos, G.¬†Arvanitakis, F.¬†K. Konstantinidis, G.¬†Tsimiklis, and A.¬†Amditis, ‚ÄúExploration and mitigation of the impact of lighting conditions on multi-spectral image classification,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2023 IEEE International Conference on Imaging Systems and Techniques (IST)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
O.¬†Tamin, E.¬†Moung, J.¬†Dargham, F.¬†Yahya, and S.¬†Omatu, ‚ÄúA review of hyperspectral imaging-based plastic waste detection state-of-the-arts,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">International Journal of Electrical and Computer Engineering</em>, vol.¬†13, pp. 3407‚Äì3419, 06 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M.¬†Lindell, ‚ÄúPixel classification of hyperspectral images,‚Äù Ph.D. dissertation, Uppsala University, 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://uu.diva-portal.org/smash/get/diva2:1216370/FULLTEXT01.pdf</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A.¬†Kirillov, E.¬†Mintun, N.¬†Ravi, H.¬†Mao, C.¬†Rolland, L.¬†Gustafson, T.¬†Xiao, S.¬†Whitehead, A.¬†C. Berg, W.-Y. Lo <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">et¬†al.</em>, ‚ÄúSegment anything,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib39.2.2">arXiv preprint arXiv:2304.02643</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
E.¬†Smith and G.¬†Dent, <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Modern Raman Spectroscopy: A Practical Approach</em>.¬†¬†¬†John Wiley &amp; Sons, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
D.¬†A. Long, ‚ÄúThe raman effect: a unified treatment of the theory of raman scattering by molecules,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">John Wiley &amp; Sons</em>, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma and J.¬†Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
I.¬†Loshchilov and F.¬†Hutter, ‚ÄúSgdr: Stochastic gradient descent with warm restarts,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:1608.03983</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúDecoupled weight decay regularization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
I.¬†Goodfellow, Y.¬†Bengio, and A.¬†Courville, <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Deep Learning</em>.¬†¬†¬†MIT Press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J.¬†Serra, ‚ÄúImage analysis and mathematical morphology,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Academic Press, Inc.</em>, 1982.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
F.¬†De¬†la Torre and J.¬†Serra, ‚ÄúMorphological filtering,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">SPIE Proceedings Vol. 0534, Image Processing</em>, 1984.

</span>
</li>
</ul>
</section>
<figure class="ltx_float biography" id="tab1">
<table class="ltx_tabular" id="tab1.1">
<tr class="ltx_tr" id="tab1.1.1">
<td class="ltx_td" id="tab1.1.1.1">
<span class="ltx_inline-block" id="tab1.1.1.1.1">
<span class="ltx_p" id="tab1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab1.1.1.1.1.1.1">Savvas Sifnaios</span>  is a Researcher and Data Scientist at the I-SENSE Group of Institute of Communication and Computer Systems (ICCS) since February 2022. He has received a Diploma degree from the School of Electrical &amp; Computer Engineering of National Technical University of Athens (NTUA) in 2022. Moreover, he is currently pursuing a PhD in the School of Electrical &amp; Computer Engineering of NTUA. Throughout his involvement in EU-funded Horizon Projects, Savvas has developed AI-powered Computer Vision solutions for the classification of objects within various waste streams. His primary focus is the design and implementation of efficient algorithms for the analysis of Hypespectral (HS) Images with deep learning, the development intuitive interfaces for controlling HS cameras, as well as the integration of such technology into real-time sorting robotic systems. Since 2022 he is acting as supporting Project Manager, publishing conference papers, contributing in journal papers, while also actively participating in the preparation of Horizon Europe grand proposals.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab2">
<table class="ltx_tabular" id="tab2.1">
<tr class="ltx_tr" id="tab2.1.1">
<td class="ltx_td" id="tab2.1.1.1">
<span class="ltx_inline-block" id="tab2.1.1.1.1">
<span class="ltx_p" id="tab2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab2.1.1.1.1.1.1">Dr. George Arvanitakis</span>  is a Researcher in the field of Artificial intelligence, with a PhD in applied mathematics on Networks from ParisTech (Eurecom). He worked as a senior researcher in Huawei‚Äôs Maths and Algorithms lab in Paris and in AI research center of the Technology Innovation Institute in Abu Dhabi. He has also served as a postdoc researcher at Laboratoire Informatique Grenoble and the National Observatory of Athens.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab3">
<table class="ltx_tabular" id="tab3.1">
<tr class="ltx_tr" id="tab3.1.1">
<td class="ltx_td" id="tab3.1.1.1">
<span class="ltx_inline-block" id="tab3.1.1.1.1">
<span class="ltx_p" id="tab3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab3.1.1.1.1.1.1">Dr. Fotios Konstantinidis</span>  serves as the Division leader of Industry 5.0 &amp; Smart Manufacturing at the I-SENSE group of the Institute of Communication and Computer Systems (ICCS). He holds a Ph.D. in Smart Manufacturing from the Department of Production and Management Engineering. Additionally, he holds a Diploma in Electrical and Computer Engineering and a Master‚Äôs degree in Mechatronics and Systems Automation from the Democritus University of Thrace. With a team of researchers and project managers at various career stages, he oversees the realisation of industrial machinery from concept to launch where these systems utilize cutting-edge technologies such as hyperspectral &amp; visual imaging, delta robots, air nozzles, X-ray sensors, and pretreatment units. His primary focus areas include robotic-based waste segregation for bioplastic, construction and demolition, metal, and wood wastes, as well as robotic-based quality characterization for mining and quality food and agriculture operations. Fotios has worked as an Industry 4.0 Technology Analyst , analyzing the plants‚Äô maturity level and proposing I4.0 strategies for Fortune 500 companies, while also creating I4.0 training material. Prior to this, he had worked in the Next-Generation Access networks in Telecom Industry. Throughout his career, he have also been involved in organising workshops, delivered presentations at conferences/workshops, published journal papers and contributed in Horizon Europe grand proposals.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab4">
<table class="ltx_tabular" id="tab4.1">
<tr class="ltx_tr" id="tab4.1.1">
<td class="ltx_td" id="tab4.1.1.1">
<span class="ltx_inline-block" id="tab4.1.1.1.1">
<span class="ltx_p" id="tab4.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab4.1.1.1.1.1.1">Georgios Tsimiklis</span>  is a Researcher and Scientific Project Manager at the I-SENSE Group of Institute of Communication and Computer Systems (ICCS) since May 2018. He has received a Diploma degree from the Electrical &amp; Computer Engineering Department of Patras University in 2010, and a MSc in Entrepreneurship and Innovation Management from the Royal Institute of Technology (KTH) in 2012. He has an extensive experience in the design and management of large scale IT projects gained in the field of Airline Passenger Service Systems, in France (2012-2015) and Japan (2015-2018).Since 2018 he is participating and managing digital research projects in the area of Earth Observation, Environmental Monitoring, Circular Economy, Immersive Technologies and Smart Manufacturing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab5">
<table class="ltx_tabular" id="tab5.1">
<tr class="ltx_tr" id="tab5.1.1">
<td class="ltx_td" id="tab5.1.1.1">
<span class="ltx_inline-block" id="tab5.1.1.1.1">
<span class="ltx_p" id="tab5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab5.1.1.1.1.1.1">Dr. Angelos Amditis</span>  is the Research and Development Director of ICCS and member of its BoD, as well as the founder and Director of the I-SENSE Research Group. He is the ERTICO-ITS Europe Chairman since 2018. He also serves as a member of the Executive Board of ETP-ALICE (co-Chair of the TG3: Systems &amp; Technologies for Interconnected Logistics) and as the Vice President (and founder) of ITS Hellas. He is the Chairman of the Steering Committee of the Hellenic Port Community System (HPCS) of the Piraeus Container Terminal S.A. (PCT) and Member of the Supervisory Board of the Hellenic Institute of Logistics Management (ILME). He has been the President and Vice President of Athens Urban Transport Organisation (2019-2023) and former member of the BoD of Athens Water Supply and Sewerage Company (2019-2023). He is also the founder and today the Honorary President of EUROXR. During his long career, he has been active in the fields of ITS, Smart Mobility, Applied Smart Systems, Logistics, CCAM, ICT for environmental and security applications, risk &amp; crisis management, sensors and information fusion, data management, 5G Technology, citizen science, circular economy, platforms, Industry 4.0, AI, Robotics, UAVs, AR/VR, and many more. While within the last 30 years, he has participated in numerous research projects, in most of them as the scientific responsible and/or coordinator, while he has been extensively contributing as the writer in peer reviewed journal articles and book chapters, as well as numerous conference papers.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="tab6">
<table class="ltx_tabular" id="tab6.1">
<tr class="ltx_tr" id="tab6.1.1">
<td class="ltx_td" id="tab6.1.1.1">
<span class="ltx_inline-block" id="tab6.1.1.1.1">
<span class="ltx_p" id="tab6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="tab6.1.1.1.1.1.1">Prof. Panayiotis Frangos</span> 
was born in Thessaloniki, Greece, in 1959. He received the Bachelor Degree from the NTUA, Greece, in 1983, and the Master‚Äôs and PhD. Degrees from the Moore School of Electrical Engineering, University of Pennsylvania, Philadelphia, USA, in 1985 and 1986 respectively, all in Electrical Engineering. Since 1989, after his military service in Greece, he has been with the
Faculty of the Department of Electrical and Computer Engineering, NTUA, first as a
Lecturer (1989 - 92), and subsequently as an Assistant Professor (1992 - 96),
Associate Professor (1996 - 2000), and Full Professor (2000 - today).His areas of research include : Propagation of electromagnetic waves over terrain,
radar systems, direct and inverse synthetic aperture radar signal processing techniques,
scattering from fractal surfaces, high frequency scattering techniques, inverse
scattering, nonlinear propagation of electromagnetic (EM) waves in optical fibers,
finite - element techniques, near EM field calculations, and imaging of radar targets.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 13:24:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
