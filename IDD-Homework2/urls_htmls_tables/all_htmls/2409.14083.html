<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information</title>
<!--Generated on Sat Sep 21 09:31:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14083v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S1" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Robust Multimodal RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS1" title="In 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS2" title="In 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multimodal RAG Benefit LVLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS3" title="In 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Irrelevant Harms Model Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS4" title="In 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Robust RAG Training Framework</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS4.SSS1" title="In 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Construction of Positive and Negative Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS4.SSS2" title="In 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Data Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS4.SSS3" title="In 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>RAG Instruction-Tuning</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS1" title="In 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS2" title="In 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS3" title="In 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Implementation details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS4" title="In 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS4.SSS0.Px1" title="In 3.4 Experimental Results ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Compare to Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS4.SSS0.Px2" title="In 3.4 Experimental Results ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Compare to ICL models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS4.SSS0.Px3" title="In 3.4 Experimental Results ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Robustness</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS5" title="In 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS5.SSS0.Px1" title="In 3.5 Ablation Study ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Size of the Database</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS5.SSS0.Px2" title="In 3.5 Ablation Study ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Data Filter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.SS5.SSS0.Px3" title="In 3.5 Ablation Study ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Efficiency Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S4" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S4.SS1" title="In 4 Related Work ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Large Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S4.SS2" title="In 4 Related Work ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Retrieval-Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S5" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S6" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1" title="In SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS1" title="In Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2" title="In Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Data Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px1" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">POPE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px2" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">MMStar</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px3" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Vizwiz-VQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px4" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">MS-COCO</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px5" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">Vizwiz-Caption</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px6" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">CIFAR-10</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS0.Px7" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title">EmoSet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2.SSS1" title="In A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3" title="In Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Additional Ablation Study and Experiment Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3.SSS1" title="In A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.1 </span>Sensitivity to the Number of Examplars</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3.SSS2" title="In A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.2 </span>Effect of Training Data Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3.SSS3" title="In A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.3 </span>Effect of Different Retrieved Content</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3.SSS4" title="In A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.4 </span>Detail Results of Captioning Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS3.SSS5" title="In A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.5 </span>Effect of Irrelevant Content</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS4" title="In Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Case Study</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text" id="id1.1" style="position:relative; bottom:-12.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="42" id="id1.1.g1" src="extracted/5869924/images/surf-llava.png" width="42"/></span> SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiashuo Sun<sup class="ltx_sup" id="id2.1.id1">1</sup>,
Jihai Zhang<sup class="ltx_sup" id="id3.2.id2">2</sup>,
Yucheng Zhou<sup class="ltx_sup" id="id4.3.id3">3</sup>,
<span class="ltx_text ltx_font_bold" id="id5.4.id4">
Zhaochen Su<sup class="ltx_sup" id="id5.4.id4.1"><span class="ltx_text ltx_font_medium" id="id5.4.id4.1.1">4</span></sup>,
Xiaoye Qu<sup class="ltx_sup" id="id5.4.id4.2"><span class="ltx_text ltx_font_medium" id="id5.4.id4.2.1">5</span></sup><sup class="ltx_sup" id="id5.4.id4.3">†</sup>,
Yu Cheng<sup class="ltx_sup" id="id5.4.id4.4"><span class="ltx_text ltx_font_medium" id="id5.4.id4.4.1">2</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.5.id5">1</sup>
Xiamen University,
<sup class="ltx_sup" id="id7.6.id6">2</sup>
The Chinese University of Hong Kong
<br class="ltx_break"/><sup class="ltx_sup" id="id8.7.id7">3</sup>
SKL-IOTSC, CIS, University of Macau, <sup class="ltx_sup" id="id9.8.id8">4</sup>
Soochow University 
<br class="ltx_break"/><sup class="ltx_sup" id="id10.9.id9">5</sup>
Shanghai AI Laboratory
</span><span class="ltx_author_notes">Work done during internship at Shanghai AI Laboratory.<span class="ltx_text ltx_font_bold" id="id11.10.id1">Both are corresponding authors.</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs’ Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to <span class="ltx_text ltx_font_bold" id="id12.id1.1">S</span>electively <span class="ltx_text ltx_font_bold" id="id12.id1.2">U</span>tilize <span class="ltx_text ltx_font_bold" id="id12.id1.3">R</span>etrieved In<span class="ltx_text ltx_font_bold" id="id12.id1.4">f</span>ormation (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs’ ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/GasolSun36/SURf" title="">https://github.com/GasolSun36/SURf</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1" style="position:relative; bottom:-12.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="42" id="p1.1.1.1.g1" src="extracted/5869924/images/surf-llava.png" width="42"/></span><span class="ltx_text ltx_font_bold" id="p1.1.1.2"> SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Jiashuo Sun<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.1.1">1</span></sup><span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Work done during internship at Shanghai AI Laboratory.</span></span></span>,
Jihai Zhang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.3.1">2</span></sup>,
Yucheng Zhou<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.4.1">3</span></sup>,

Zhaochen Su<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.5.1">4</span></sup>,
Xiaoye Qu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.6"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.6.1">5</span></sup><sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.7">†</sup>,
Yu Cheng<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.8"><span class="ltx_text ltx_font_medium" id="p1.1.2.1.1.1.1.1.1.8.1">2</span></sup><span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Both are corresponding authors.</span></span></span></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1">1</sup>
Xiamen University,
<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.2">2</sup>
The Chinese University of Hong Kong</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">3</sup>
SKL-IOTSC, CIS, University of Macau, <sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.2">4</sup>
Soochow University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">5</sup>
Shanghai AI Laboratory</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Vision-Language Models (LVLMs) have become crucial at the intersection of computer vision and natural language processing (NLP), empowering various applications by generating contextually relevant textual descriptions from visual inputs <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib28" title="">2023b</a>); gpt (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib1" title="">2023</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib11" title="">2023</a>); Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib5" title="">2023</a>); Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib51" title="">2023</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib58" title="">2023</a>); Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib12" title="">2024</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib43" title="">2024</a>)</cite>. These models capture and translate complex visual patterns into coherent linguistic representations. The development of LVLMs is driven by continuous improvements in model architecture, training methodologies, and data diversity <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib46" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib45" title="">a</a>); Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib52" title="">2023</a>); Qu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib32" title="">2024b</a>)</cite>, resulting in better performance and broader applicability.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1010" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of multimodal RAG. RAG can introduce misleading content, causing LVLMs to generate incorrect responses. SURf can selectively utilize information from images and descriptions, e.g., the first image-caption pair.).</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The illustration of Multimodal RAG for VQA, Captioning and Classification Tasks. Providing images similar to the test images along with their descriptions as references can help LVLMs answer questions more accurately.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although LVLMs excel in visual language representation, they struggle with image generalization and understanding <cite class="ltx_cite ltx_citemacro_cite">Qu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib33" title="">2024c</a>)</cite>. Similarly, LLMs face these challenges in the NLP domain but can mitigate them by incorporating additional knowledge or references through Retrieval-Augmented Generation (RAG), ensuring high trustworthiness <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib16" title="">2020</a>); Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib3" title="">2023</a>); Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib47" title="">2023</a>)</cite>. However, in LVLMs, the full potential of RAG remains under-explored. Firstly, many previous multimodal RAG-related works have only focused on the text modality <cite class="ltx_cite ltx_citemacro_cite">Ramos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib37" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib35" title="">a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib36" title="">b</a>)</cite>, without fully utilizing the LVLMs’ understanding of visual content. Secondly, the few works that integrate multimodal references are often limited to specific tasks like image captioning, ignoring the broader potential of applying RAG technology <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib49" title="">2023b</a>); Yasunaga et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib50" title="">2023</a>)</cite>.
Finally, a significant issue overlooked by existing research is the potential irrelevance or even disruptive nature of retrieved content in practical applications.
Under this circumstance, vanilla LVLMs fail to dynamically select retrieval content, but treat them indiscriminately, leading to a performance decline <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib26" title="">2023b</a>); Qu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib31" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we propose a self-refinement framework that enables LVLMs to selectively utilize the retrieved information from both image and text sources while effectively enhancing the model’s robustness against irrelevant or misleading content. Specifically, we identify the visual questions that are wrongly answered by LVLM and use image-caption pairs to prompt the LVLMs to generate responses. Secondly, we assess the contribution of the introduced image-caption pairs by invoking external evaluation tools, thereby constructing a training dataset with positive and negative samples. Subsequently, we build a RAG instruction dataset to further train the LVLMs, allowing them to better benefit from RAG tasks and improve their robustness against irrelevant retrieval content. It is worth noting that we only reconstruct data from the SFT phase of the LVLMs without using any additional new datasets. We extensively evaluate our method across seven datasets and benchmarks in three different tasks: VQA, image captioning, and image classification. The experimental results demonstrate that our approach can further enhance the RAG capabilities of existing LVLMs and significantly improve their robustness in generating responses when faced with irrelevant images or content.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our contributions are summarized as follows: (1) We empirically demonstrate that integrating Multimodal RAG with LVLMs can improve model performance, while also revealing that current LVLMs are highly sensitive to irrelevant and misleading retrieval information, which presents a significant challenge. (2) We design a lightweight and cost-effective self-refinement framework specifically aimed at teaching LVLMs to selectively utilize relevant information. (3) Through extensive experiments and evaluations, we show that our approach enhances the models’ ability to effectively utilize retrieval information, making them more robust against irrelevant and misleading references.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S1.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of our training framework. First, we collect questions that LVLMs initially answered incorrectly. Next, we retrieve the Top-N image-caption pairs from the corpus, allowing the LVLM to reattempt the questions. We then evaluate the answers to see if they have improved (positive) or worsened (negative). After that, we filter for the highest-quality training data and use it for instruction tuning to train the LVLMs.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Robust Multimodal RAG</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Preliminaries</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The RAG consists of two main components: a retriever and a generator. The retriever fetches relevant information from a large document collection, and the generator uses the retrieved document to produce the final output. We can represent the functioning of the RAG in LVLMs with the following formulas:</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.11">Given an input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">italic_x</annotation></semantics></math> (e.g., a question <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">italic_q</annotation></semantics></math> or instruction with a feature vector of an image <math alttext="\text{i}_{\text{test}}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mtext id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2a.cmml">i</mtext><mtext id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3a.cmml">test</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2a.cmml" xref="S2.SS1.p2.3.m3.1.1.2"><mtext id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">i</mtext></ci><ci id="S2.SS1.p2.3.m3.1.1.3a.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><mtext id="S2.SS1.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS1.p2.3.m3.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\text{i}_{\text{test}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">i start_POSTSUBSCRIPT test end_POSTSUBSCRIPT</annotation></semantics></math>), the retriever fetches <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">italic_k</annotation></semantics></math> relevant images <math alttext="\{i_{1},i_{2},\ldots,i_{k}\}" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.4"><semantics id="S2.SS1.p2.5.m5.4a"><mrow id="S2.SS1.p2.5.m5.4.4.3" xref="S2.SS1.p2.5.m5.4.4.4.cmml"><mo id="S2.SS1.p2.5.m5.4.4.3.4" stretchy="false" xref="S2.SS1.p2.5.m5.4.4.4.cmml">{</mo><msub id="S2.SS1.p2.5.m5.2.2.1.1" xref="S2.SS1.p2.5.m5.2.2.1.1.cmml"><mi id="S2.SS1.p2.5.m5.2.2.1.1.2" xref="S2.SS1.p2.5.m5.2.2.1.1.2.cmml">i</mi><mn id="S2.SS1.p2.5.m5.2.2.1.1.3" xref="S2.SS1.p2.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p2.5.m5.4.4.3.5" xref="S2.SS1.p2.5.m5.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.5.m5.3.3.2.2" xref="S2.SS1.p2.5.m5.3.3.2.2.cmml"><mi id="S2.SS1.p2.5.m5.3.3.2.2.2" xref="S2.SS1.p2.5.m5.3.3.2.2.2.cmml">i</mi><mn id="S2.SS1.p2.5.m5.3.3.2.2.3" xref="S2.SS1.p2.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p2.5.m5.4.4.3.6" xref="S2.SS1.p2.5.m5.4.4.4.cmml">,</mo><mi id="S2.SS1.p2.5.m5.1.1" mathvariant="normal" xref="S2.SS1.p2.5.m5.1.1.cmml">…</mi><mo id="S2.SS1.p2.5.m5.4.4.3.7" xref="S2.SS1.p2.5.m5.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.5.m5.4.4.3.3" xref="S2.SS1.p2.5.m5.4.4.3.3.cmml"><mi id="S2.SS1.p2.5.m5.4.4.3.3.2" xref="S2.SS1.p2.5.m5.4.4.3.3.2.cmml">i</mi><mi id="S2.SS1.p2.5.m5.4.4.3.3.3" xref="S2.SS1.p2.5.m5.4.4.3.3.3.cmml">k</mi></msub><mo id="S2.SS1.p2.5.m5.4.4.3.8" stretchy="false" xref="S2.SS1.p2.5.m5.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.4b"><set id="S2.SS1.p2.5.m5.4.4.4.cmml" xref="S2.SS1.p2.5.m5.4.4.3"><apply id="S2.SS1.p2.5.m5.2.2.1.1.cmml" xref="S2.SS1.p2.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.2.2.1.1.1.cmml" xref="S2.SS1.p2.5.m5.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m5.2.2.1.1.2.cmml" xref="S2.SS1.p2.5.m5.2.2.1.1.2">𝑖</ci><cn id="S2.SS1.p2.5.m5.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.p2.5.m5.3.3.2.2.cmml" xref="S2.SS1.p2.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.3.3.2.2.1.cmml" xref="S2.SS1.p2.5.m5.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p2.5.m5.3.3.2.2.2.cmml" xref="S2.SS1.p2.5.m5.3.3.2.2.2">𝑖</ci><cn id="S2.SS1.p2.5.m5.3.3.2.2.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">…</ci><apply id="S2.SS1.p2.5.m5.4.4.3.3.cmml" xref="S2.SS1.p2.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.4.4.3.3.1.cmml" xref="S2.SS1.p2.5.m5.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p2.5.m5.4.4.3.3.2.cmml" xref="S2.SS1.p2.5.m5.4.4.3.3.2">𝑖</ci><ci id="S2.SS1.p2.5.m5.4.4.3.3.3.cmml" xref="S2.SS1.p2.5.m5.4.4.3.3.3">𝑘</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.4c">\{i_{1},i_{2},\ldots,i_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.4d">{ italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> from an image set of image-caption collection <math alttext="\mathbb{D}" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">𝔻</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">𝔻</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">\mathbb{D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">blackboard_D</annotation></semantics></math>.
The probability distribution of the retriever can be represented as <math alttext="\bar{P}(i\mid x)" class="ltx_Math" display="inline" id="S2.SS1.p2.7.m7.1"><semantics id="S2.SS1.p2.7.m7.1a"><mrow id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mover accent="true" id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml"><mi id="S2.SS1.p2.7.m7.1.1.3.2" xref="S2.SS1.p2.7.m7.1.1.3.2.cmml">P</mi><mo id="S2.SS1.p2.7.m7.1.1.3.1" xref="S2.SS1.p2.7.m7.1.1.3.1.cmml">¯</mo></mover><mo id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml">⁢</mo><mrow id="S2.SS1.p2.7.m7.1.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.1.1.cmml"><mo id="S2.SS1.p2.7.m7.1.1.1.1.2" stretchy="false" xref="S2.SS1.p2.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p2.7.m7.1.1.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.7.m7.1.1.1.1.1.2" xref="S2.SS1.p2.7.m7.1.1.1.1.1.2.cmml">i</mi><mo id="S2.SS1.p2.7.m7.1.1.1.1.1.1" xref="S2.SS1.p2.7.m7.1.1.1.1.1.1.cmml">∣</mo><mi id="S2.SS1.p2.7.m7.1.1.1.1.1.3" xref="S2.SS1.p2.7.m7.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.SS1.p2.7.m7.1.1.1.1.3" stretchy="false" xref="S2.SS1.p2.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><times id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2"></times><apply id="S2.SS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.3"><ci id="S2.SS1.p2.7.m7.1.1.3.1.cmml" xref="S2.SS1.p2.7.m7.1.1.3.1">¯</ci><ci id="S2.SS1.p2.7.m7.1.1.3.2.cmml" xref="S2.SS1.p2.7.m7.1.1.3.2">𝑃</ci></apply><apply id="S2.SS1.p2.7.m7.1.1.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.7.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p2.7.m7.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.1.1.1.2">𝑖</ci><ci id="S2.SS1.p2.7.m7.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">\bar{P}(i\mid x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m7.1d">over¯ start_ARG italic_P end_ARG ( italic_i ∣ italic_x )</annotation></semantics></math>.
The generator uses the retrieved images <math alttext="\{i_{1},i_{2},\ldots,i_{k}\}" class="ltx_Math" display="inline" id="S2.SS1.p2.8.m8.4"><semantics id="S2.SS1.p2.8.m8.4a"><mrow id="S2.SS1.p2.8.m8.4.4.3" xref="S2.SS1.p2.8.m8.4.4.4.cmml"><mo id="S2.SS1.p2.8.m8.4.4.3.4" stretchy="false" xref="S2.SS1.p2.8.m8.4.4.4.cmml">{</mo><msub id="S2.SS1.p2.8.m8.2.2.1.1" xref="S2.SS1.p2.8.m8.2.2.1.1.cmml"><mi id="S2.SS1.p2.8.m8.2.2.1.1.2" xref="S2.SS1.p2.8.m8.2.2.1.1.2.cmml">i</mi><mn id="S2.SS1.p2.8.m8.2.2.1.1.3" xref="S2.SS1.p2.8.m8.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p2.8.m8.4.4.3.5" xref="S2.SS1.p2.8.m8.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.8.m8.3.3.2.2" xref="S2.SS1.p2.8.m8.3.3.2.2.cmml"><mi id="S2.SS1.p2.8.m8.3.3.2.2.2" xref="S2.SS1.p2.8.m8.3.3.2.2.2.cmml">i</mi><mn id="S2.SS1.p2.8.m8.3.3.2.2.3" xref="S2.SS1.p2.8.m8.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p2.8.m8.4.4.3.6" xref="S2.SS1.p2.8.m8.4.4.4.cmml">,</mo><mi id="S2.SS1.p2.8.m8.1.1" mathvariant="normal" xref="S2.SS1.p2.8.m8.1.1.cmml">…</mi><mo id="S2.SS1.p2.8.m8.4.4.3.7" xref="S2.SS1.p2.8.m8.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.8.m8.4.4.3.3" xref="S2.SS1.p2.8.m8.4.4.3.3.cmml"><mi id="S2.SS1.p2.8.m8.4.4.3.3.2" xref="S2.SS1.p2.8.m8.4.4.3.3.2.cmml">i</mi><mi id="S2.SS1.p2.8.m8.4.4.3.3.3" xref="S2.SS1.p2.8.m8.4.4.3.3.3.cmml">k</mi></msub><mo id="S2.SS1.p2.8.m8.4.4.3.8" stretchy="false" xref="S2.SS1.p2.8.m8.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.4b"><set id="S2.SS1.p2.8.m8.4.4.4.cmml" xref="S2.SS1.p2.8.m8.4.4.3"><apply id="S2.SS1.p2.8.m8.2.2.1.1.cmml" xref="S2.SS1.p2.8.m8.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.8.m8.2.2.1.1.1.cmml" xref="S2.SS1.p2.8.m8.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p2.8.m8.2.2.1.1.2.cmml" xref="S2.SS1.p2.8.m8.2.2.1.1.2">𝑖</ci><cn id="S2.SS1.p2.8.m8.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.p2.8.m8.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.p2.8.m8.3.3.2.2.cmml" xref="S2.SS1.p2.8.m8.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.8.m8.3.3.2.2.1.cmml" xref="S2.SS1.p2.8.m8.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p2.8.m8.3.3.2.2.2.cmml" xref="S2.SS1.p2.8.m8.3.3.2.2.2">𝑖</ci><cn id="S2.SS1.p2.8.m8.3.3.2.2.3.cmml" type="integer" xref="S2.SS1.p2.8.m8.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1">…</ci><apply id="S2.SS1.p2.8.m8.4.4.3.3.cmml" xref="S2.SS1.p2.8.m8.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.8.m8.4.4.3.3.1.cmml" xref="S2.SS1.p2.8.m8.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p2.8.m8.4.4.3.3.2.cmml" xref="S2.SS1.p2.8.m8.4.4.3.3.2">𝑖</ci><ci id="S2.SS1.p2.8.m8.4.4.3.3.3.cmml" xref="S2.SS1.p2.8.m8.4.4.3.3.3">𝑘</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.4c">\{i_{1},i_{2},\ldots,i_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.8.m8.4d">{ italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math>, the corresponding captions <math alttext="\{c_{1},c_{2},\ldots,c_{m}\}" class="ltx_Math" display="inline" id="S2.SS1.p2.9.m9.4"><semantics id="S2.SS1.p2.9.m9.4a"><mrow id="S2.SS1.p2.9.m9.4.4.3" xref="S2.SS1.p2.9.m9.4.4.4.cmml"><mo id="S2.SS1.p2.9.m9.4.4.3.4" stretchy="false" xref="S2.SS1.p2.9.m9.4.4.4.cmml">{</mo><msub id="S2.SS1.p2.9.m9.2.2.1.1" xref="S2.SS1.p2.9.m9.2.2.1.1.cmml"><mi id="S2.SS1.p2.9.m9.2.2.1.1.2" xref="S2.SS1.p2.9.m9.2.2.1.1.2.cmml">c</mi><mn id="S2.SS1.p2.9.m9.2.2.1.1.3" xref="S2.SS1.p2.9.m9.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p2.9.m9.4.4.3.5" xref="S2.SS1.p2.9.m9.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.9.m9.3.3.2.2" xref="S2.SS1.p2.9.m9.3.3.2.2.cmml"><mi id="S2.SS1.p2.9.m9.3.3.2.2.2" xref="S2.SS1.p2.9.m9.3.3.2.2.2.cmml">c</mi><mn id="S2.SS1.p2.9.m9.3.3.2.2.3" xref="S2.SS1.p2.9.m9.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p2.9.m9.4.4.3.6" xref="S2.SS1.p2.9.m9.4.4.4.cmml">,</mo><mi id="S2.SS1.p2.9.m9.1.1" mathvariant="normal" xref="S2.SS1.p2.9.m9.1.1.cmml">…</mi><mo id="S2.SS1.p2.9.m9.4.4.3.7" xref="S2.SS1.p2.9.m9.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.9.m9.4.4.3.3" xref="S2.SS1.p2.9.m9.4.4.3.3.cmml"><mi id="S2.SS1.p2.9.m9.4.4.3.3.2" xref="S2.SS1.p2.9.m9.4.4.3.3.2.cmml">c</mi><mi id="S2.SS1.p2.9.m9.4.4.3.3.3" xref="S2.SS1.p2.9.m9.4.4.3.3.3.cmml">m</mi></msub><mo id="S2.SS1.p2.9.m9.4.4.3.8" stretchy="false" xref="S2.SS1.p2.9.m9.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m9.4b"><set id="S2.SS1.p2.9.m9.4.4.4.cmml" xref="S2.SS1.p2.9.m9.4.4.3"><apply id="S2.SS1.p2.9.m9.2.2.1.1.cmml" xref="S2.SS1.p2.9.m9.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.9.m9.2.2.1.1.1.cmml" xref="S2.SS1.p2.9.m9.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p2.9.m9.2.2.1.1.2.cmml" xref="S2.SS1.p2.9.m9.2.2.1.1.2">𝑐</ci><cn id="S2.SS1.p2.9.m9.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.p2.9.m9.2.2.1.1.3">1</cn></apply><apply id="S2.SS1.p2.9.m9.3.3.2.2.cmml" xref="S2.SS1.p2.9.m9.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.9.m9.3.3.2.2.1.cmml" xref="S2.SS1.p2.9.m9.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p2.9.m9.3.3.2.2.2.cmml" xref="S2.SS1.p2.9.m9.3.3.2.2.2">𝑐</ci><cn id="S2.SS1.p2.9.m9.3.3.2.2.3.cmml" type="integer" xref="S2.SS1.p2.9.m9.3.3.2.2.3">2</cn></apply><ci id="S2.SS1.p2.9.m9.1.1.cmml" xref="S2.SS1.p2.9.m9.1.1">…</ci><apply id="S2.SS1.p2.9.m9.4.4.3.3.cmml" xref="S2.SS1.p2.9.m9.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.9.m9.4.4.3.3.1.cmml" xref="S2.SS1.p2.9.m9.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p2.9.m9.4.4.3.3.2.cmml" xref="S2.SS1.p2.9.m9.4.4.3.3.2">𝑐</ci><ci id="S2.SS1.p2.9.m9.4.4.3.3.3.cmml" xref="S2.SS1.p2.9.m9.4.4.3.3.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m9.4c">\{c_{1},c_{2},\ldots,c_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.9.m9.4d">{ italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_c start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math> and the input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p2.10.m10.1"><semantics id="S2.SS1.p2.10.m10.1a"><mi id="S2.SS1.p2.10.m10.1.1" xref="S2.SS1.p2.10.m10.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.10.m10.1b"><ci id="S2.SS1.p2.10.m10.1.1.cmml" xref="S2.SS1.p2.10.m10.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.10.m10.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.10.m10.1d">italic_x</annotation></semantics></math> to generate the output <math alttext="y" class="ltx_Math" display="inline" id="S2.SS1.p2.11.m11.1"><semantics id="S2.SS1.p2.11.m11.1a"><mi id="S2.SS1.p2.11.m11.1.1" xref="S2.SS1.p2.11.m11.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.11.m11.1b"><ci id="S2.SS1.p2.11.m11.1.1.cmml" xref="S2.SS1.p2.11.m11.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.11.m11.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.11.m11.1d">italic_y</annotation></semantics></math> (e.g., an answer, image caption, or classification label). The conditional probability distribution of the generator can be represented as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P(y\mid x,\{[i_{1},c_{1}],[i_{2},c_{2}]\ldots,[i_{k},c_{k}]\}" class="ltx_math_unparsed" display="inline" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1b"><mi id="S2.E1.m1.1.2">P</mi><mrow id="S2.E1.m1.1.3"><mo id="S2.E1.m1.1.3.1" stretchy="false">(</mo><mi id="S2.E1.m1.1.3.2">y</mi><mo id="S2.E1.m1.1.3.3" lspace="0em" rspace="0.167em">∣</mo><mi id="S2.E1.m1.1.1">x</mi><mo id="S2.E1.m1.1.3.4">,</mo><mrow id="S2.E1.m1.1.3.5"><mo id="S2.E1.m1.1.3.5.1" stretchy="false">{</mo><mrow id="S2.E1.m1.1.3.5.2"><mo id="S2.E1.m1.1.3.5.2.1" stretchy="false">[</mo><msub id="S2.E1.m1.1.3.5.2.2"><mi id="S2.E1.m1.1.3.5.2.2.2">i</mi><mn id="S2.E1.m1.1.3.5.2.2.3">1</mn></msub><mo id="S2.E1.m1.1.3.5.2.3">,</mo><msub id="S2.E1.m1.1.3.5.2.4"><mi id="S2.E1.m1.1.3.5.2.4.2">c</mi><mn id="S2.E1.m1.1.3.5.2.4.3">1</mn></msub><mo id="S2.E1.m1.1.3.5.2.5" stretchy="false">]</mo></mrow><mo id="S2.E1.m1.1.3.5.3">,</mo><mrow id="S2.E1.m1.1.3.5.4"><mo id="S2.E1.m1.1.3.5.4.1" stretchy="false">[</mo><msub id="S2.E1.m1.1.3.5.4.2"><mi id="S2.E1.m1.1.3.5.4.2.2">i</mi><mn id="S2.E1.m1.1.3.5.4.2.3">2</mn></msub><mo id="S2.E1.m1.1.3.5.4.3">,</mo><msub id="S2.E1.m1.1.3.5.4.4"><mi id="S2.E1.m1.1.3.5.4.4.2">c</mi><mn id="S2.E1.m1.1.3.5.4.4.3">2</mn></msub><mo id="S2.E1.m1.1.3.5.4.5" stretchy="false">]</mo></mrow><mi id="S2.E1.m1.1.3.5.5" mathvariant="normal">…</mi><mo id="S2.E1.m1.1.3.5.6">,</mo><mrow id="S2.E1.m1.1.3.5.7"><mo id="S2.E1.m1.1.3.5.7.1" stretchy="false">[</mo><msub id="S2.E1.m1.1.3.5.7.2"><mi id="S2.E1.m1.1.3.5.7.2.2">i</mi><mi id="S2.E1.m1.1.3.5.7.2.3">k</mi></msub><mo id="S2.E1.m1.1.3.5.7.3">,</mo><msub id="S2.E1.m1.1.3.5.7.4"><mi id="S2.E1.m1.1.3.5.7.4.2">c</mi><mi id="S2.E1.m1.1.3.5.7.4.3">k</mi></msub><mo id="S2.E1.m1.1.3.5.7.5" stretchy="false">]</mo></mrow><mo id="S2.E1.m1.1.3.5.8" stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle P(y\mid x,\{[i_{1},c_{1}],[i_{2},c_{2}]\ldots,[i_{k},c_{k}]\}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_P ( italic_y ∣ italic_x , { [ italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] , [ italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] … , [ italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.2">The final output of the LVLM with RAG is based on the joint probability of the input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">italic_x</annotation></semantics></math> and the set of retrieved image-caption pairs <math alttext="\{[i_{1},c_{1}],[i_{2},c_{2}],\ldots,[i_{k},c_{k}]\}" class="ltx_Math" display="inline" id="S2.SS1.p3.2.m2.4"><semantics id="S2.SS1.p3.2.m2.4a"><mrow id="S2.SS1.p3.2.m2.4.4.3" xref="S2.SS1.p3.2.m2.4.4.4.cmml"><mo id="S2.SS1.p3.2.m2.4.4.3.4" stretchy="false" xref="S2.SS1.p3.2.m2.4.4.4.cmml">{</mo><mrow id="S2.SS1.p3.2.m2.2.2.1.1.2" xref="S2.SS1.p3.2.m2.2.2.1.1.3.cmml"><mo id="S2.SS1.p3.2.m2.2.2.1.1.2.3" stretchy="false" xref="S2.SS1.p3.2.m2.2.2.1.1.3.cmml">[</mo><msub id="S2.SS1.p3.2.m2.2.2.1.1.1.1" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml"><mi id="S2.SS1.p3.2.m2.2.2.1.1.1.1.2" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.2.cmml">i</mi><mn id="S2.SS1.p3.2.m2.2.2.1.1.1.1.3" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p3.2.m2.2.2.1.1.2.4" xref="S2.SS1.p3.2.m2.2.2.1.1.3.cmml">,</mo><msub id="S2.SS1.p3.2.m2.2.2.1.1.2.2" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2.cmml"><mi id="S2.SS1.p3.2.m2.2.2.1.1.2.2.2" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2.2.cmml">c</mi><mn id="S2.SS1.p3.2.m2.2.2.1.1.2.2.3" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2.3.cmml">1</mn></msub><mo id="S2.SS1.p3.2.m2.2.2.1.1.2.5" stretchy="false" xref="S2.SS1.p3.2.m2.2.2.1.1.3.cmml">]</mo></mrow><mo id="S2.SS1.p3.2.m2.4.4.3.5" xref="S2.SS1.p3.2.m2.4.4.4.cmml">,</mo><mrow id="S2.SS1.p3.2.m2.3.3.2.2.2" xref="S2.SS1.p3.2.m2.3.3.2.2.3.cmml"><mo id="S2.SS1.p3.2.m2.3.3.2.2.2.3" stretchy="false" xref="S2.SS1.p3.2.m2.3.3.2.2.3.cmml">[</mo><msub id="S2.SS1.p3.2.m2.3.3.2.2.1.1" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.3.3.2.2.1.1.2" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1.2.cmml">i</mi><mn id="S2.SS1.p3.2.m2.3.3.2.2.1.1.3" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1.3.cmml">2</mn></msub><mo id="S2.SS1.p3.2.m2.3.3.2.2.2.4" xref="S2.SS1.p3.2.m2.3.3.2.2.3.cmml">,</mo><msub id="S2.SS1.p3.2.m2.3.3.2.2.2.2" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2.cmml"><mi id="S2.SS1.p3.2.m2.3.3.2.2.2.2.2" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2.2.cmml">c</mi><mn id="S2.SS1.p3.2.m2.3.3.2.2.2.2.3" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p3.2.m2.3.3.2.2.2.5" stretchy="false" xref="S2.SS1.p3.2.m2.3.3.2.2.3.cmml">]</mo></mrow><mo id="S2.SS1.p3.2.m2.4.4.3.6" xref="S2.SS1.p3.2.m2.4.4.4.cmml">,</mo><mi id="S2.SS1.p3.2.m2.1.1" mathvariant="normal" xref="S2.SS1.p3.2.m2.1.1.cmml">…</mi><mo id="S2.SS1.p3.2.m2.4.4.3.7" xref="S2.SS1.p3.2.m2.4.4.4.cmml">,</mo><mrow id="S2.SS1.p3.2.m2.4.4.3.3.2" xref="S2.SS1.p3.2.m2.4.4.3.3.3.cmml"><mo id="S2.SS1.p3.2.m2.4.4.3.3.2.3" stretchy="false" xref="S2.SS1.p3.2.m2.4.4.3.3.3.cmml">[</mo><msub id="S2.SS1.p3.2.m2.4.4.3.3.1.1" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1.cmml"><mi id="S2.SS1.p3.2.m2.4.4.3.3.1.1.2" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1.2.cmml">i</mi><mi id="S2.SS1.p3.2.m2.4.4.3.3.1.1.3" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1.3.cmml">k</mi></msub><mo id="S2.SS1.p3.2.m2.4.4.3.3.2.4" xref="S2.SS1.p3.2.m2.4.4.3.3.3.cmml">,</mo><msub id="S2.SS1.p3.2.m2.4.4.3.3.2.2" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2.cmml"><mi id="S2.SS1.p3.2.m2.4.4.3.3.2.2.2" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2.2.cmml">c</mi><mi id="S2.SS1.p3.2.m2.4.4.3.3.2.2.3" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2.3.cmml">k</mi></msub><mo id="S2.SS1.p3.2.m2.4.4.3.3.2.5" stretchy="false" xref="S2.SS1.p3.2.m2.4.4.3.3.3.cmml">]</mo></mrow><mo id="S2.SS1.p3.2.m2.4.4.3.8" stretchy="false" xref="S2.SS1.p3.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.4b"><set id="S2.SS1.p3.2.m2.4.4.4.cmml" xref="S2.SS1.p3.2.m2.4.4.3"><interval closure="closed" id="S2.SS1.p3.2.m2.2.2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.2"><apply id="S2.SS1.p3.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.2.2.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.2.2.1.1.1.1.2.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.2">𝑖</ci><cn id="S2.SS1.p3.2.m2.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.SS1.p3.2.m2.2.2.1.1.1.1.3">1</cn></apply><apply id="S2.SS1.p3.2.m2.2.2.1.1.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.2.2.1.1.2.2.1.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2">subscript</csymbol><ci id="S2.SS1.p3.2.m2.2.2.1.1.2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2.2">𝑐</ci><cn id="S2.SS1.p3.2.m2.2.2.1.1.2.2.3.cmml" type="integer" xref="S2.SS1.p3.2.m2.2.2.1.1.2.2.3">1</cn></apply></interval><interval closure="closed" id="S2.SS1.p3.2.m2.3.3.2.2.3.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.2"><apply id="S2.SS1.p3.2.m2.3.3.2.2.1.1.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.2.2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.2.2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1.2">𝑖</ci><cn id="S2.SS1.p3.2.m2.3.3.2.2.1.1.3.cmml" type="integer" xref="S2.SS1.p3.2.m2.3.3.2.2.1.1.3">2</cn></apply><apply id="S2.SS1.p3.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.2.2.2.2.1.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.2.2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2.2">𝑐</ci><cn id="S2.SS1.p3.2.m2.3.3.2.2.2.2.3.cmml" type="integer" xref="S2.SS1.p3.2.m2.3.3.2.2.2.2.3">2</cn></apply></interval><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">…</ci><interval closure="closed" id="S2.SS1.p3.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.2"><apply id="S2.SS1.p3.2.m2.4.4.3.3.1.1.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.4.4.3.3.1.1.1.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.4.4.3.3.1.1.2.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1.2">𝑖</ci><ci id="S2.SS1.p3.2.m2.4.4.3.3.1.1.3.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.1.1.3">𝑘</ci></apply><apply id="S2.SS1.p3.2.m2.4.4.3.3.2.2.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.4.4.3.3.2.2.1.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p3.2.m2.4.4.3.3.2.2.2.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2.2">𝑐</ci><ci id="S2.SS1.p3.2.m2.4.4.3.3.2.2.3.cmml" xref="S2.SS1.p3.2.m2.4.4.3.3.2.2.3">𝑘</ci></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.4c">\{[i_{1},c_{1}],[i_{2},c_{2}],\ldots,[i_{k},c_{k}]\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.2.m2.4d">{ [ italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] , [ italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] , … , [ italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] }</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P(y\mid x)=\sum_{j=1}^{k}P(y\mid x,r_{j})\bar{P}(i_{j}\mid x)" class="ltx_Math" display="inline" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.cmml"><mi id="S2.E2.m1.2.2.1.3" xref="S2.E2.m1.2.2.1.3.cmml">P</mi><mo id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.cmml"><mo id="S2.E2.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.2.cmml">y</mi><mo id="S2.E2.m1.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.cmml">∣</mo><mi id="S2.E2.m1.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.E2.m1.2.2.1.1.1.3" stretchy="false" xref="S2.E2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.4" xref="S2.E2.m1.4.4.4.cmml">=</mo><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><mstyle displaystyle="true" id="S2.E2.m1.4.4.3.3" xref="S2.E2.m1.4.4.3.3.cmml"><munderover id="S2.E2.m1.4.4.3.3a" xref="S2.E2.m1.4.4.3.3.cmml"><mo id="S2.E2.m1.4.4.3.3.2.2" movablelimits="false" xref="S2.E2.m1.4.4.3.3.2.2.cmml">∑</mo><mrow id="S2.E2.m1.4.4.3.3.2.3" xref="S2.E2.m1.4.4.3.3.2.3.cmml"><mi id="S2.E2.m1.4.4.3.3.2.3.2" xref="S2.E2.m1.4.4.3.3.2.3.2.cmml">j</mi><mo id="S2.E2.m1.4.4.3.3.2.3.1" xref="S2.E2.m1.4.4.3.3.2.3.1.cmml">=</mo><mn id="S2.E2.m1.4.4.3.3.2.3.3" xref="S2.E2.m1.4.4.3.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.4.4.3.3.3" xref="S2.E2.m1.4.4.3.3.3.cmml">k</mi></munderover></mstyle><mrow id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><mi id="S2.E2.m1.4.4.3.2.4" xref="S2.E2.m1.4.4.3.2.4.cmml">P</mi><mo id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.2.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.2.1.1.1.2" stretchy="false" xref="S2.E2.m1.3.3.2.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.2.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.2.1.1.1.1.3" xref="S2.E2.m1.3.3.2.1.1.1.1.3.cmml">y</mi><mo id="S2.E2.m1.3.3.2.1.1.1.1.2" xref="S2.E2.m1.3.3.2.1.1.1.1.2.cmml">∣</mo><mrow id="S2.E2.m1.3.3.2.1.1.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo id="S2.E2.m1.3.3.2.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.2.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.3.cmml">j</mi></msub></mrow></mrow><mo id="S2.E2.m1.3.3.2.1.1.1.3" stretchy="false" xref="S2.E2.m1.3.3.2.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E2.m1.4.4.3.2.3a" xref="S2.E2.m1.4.4.3.2.3.cmml">⁢</mo><mover accent="true" id="S2.E2.m1.4.4.3.2.5" xref="S2.E2.m1.4.4.3.2.5.cmml"><mi id="S2.E2.m1.4.4.3.2.5.2" xref="S2.E2.m1.4.4.3.2.5.2.cmml">P</mi><mo id="S2.E2.m1.4.4.3.2.5.1" xref="S2.E2.m1.4.4.3.2.5.1.cmml">¯</mo></mover><mo id="S2.E2.m1.4.4.3.2.3b" xref="S2.E2.m1.4.4.3.2.3.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.3.2.2.1" xref="S2.E2.m1.4.4.3.2.2.1.1.cmml"><mo id="S2.E2.m1.4.4.3.2.2.1.2" stretchy="false" xref="S2.E2.m1.4.4.3.2.2.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.3.2.2.1.1" xref="S2.E2.m1.4.4.3.2.2.1.1.cmml"><msub id="S2.E2.m1.4.4.3.2.2.1.1.2" xref="S2.E2.m1.4.4.3.2.2.1.1.2.cmml"><mi id="S2.E2.m1.4.4.3.2.2.1.1.2.2" xref="S2.E2.m1.4.4.3.2.2.1.1.2.2.cmml">i</mi><mi id="S2.E2.m1.4.4.3.2.2.1.1.2.3" xref="S2.E2.m1.4.4.3.2.2.1.1.2.3.cmml">j</mi></msub><mo id="S2.E2.m1.4.4.3.2.2.1.1.1" xref="S2.E2.m1.4.4.3.2.2.1.1.1.cmml">∣</mo><mi id="S2.E2.m1.4.4.3.2.2.1.1.3" xref="S2.E2.m1.4.4.3.2.2.1.1.3.cmml">x</mi></mrow><mo id="S2.E2.m1.4.4.3.2.2.1.3" stretchy="false" xref="S2.E2.m1.4.4.3.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.4.cmml" xref="S2.E2.m1.4.4.4"></eq><apply id="S2.E2.m1.2.2.1.cmml" xref="S2.E2.m1.2.2.1"><times id="S2.E2.m1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.2"></times><ci id="S2.E2.m1.2.2.1.3.cmml" xref="S2.E2.m1.2.2.1.3">𝑃</ci><apply id="S2.E2.m1.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1">conditional</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.2">𝑦</ci><ci id="S2.E2.m1.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><apply id="S2.E2.m1.4.4.3.3.cmml" xref="S2.E2.m1.4.4.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.3.1.cmml" xref="S2.E2.m1.4.4.3.3">superscript</csymbol><apply id="S2.E2.m1.4.4.3.3.2.cmml" xref="S2.E2.m1.4.4.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.3.2.1.cmml" xref="S2.E2.m1.4.4.3.3">subscript</csymbol><sum id="S2.E2.m1.4.4.3.3.2.2.cmml" xref="S2.E2.m1.4.4.3.3.2.2"></sum><apply id="S2.E2.m1.4.4.3.3.2.3.cmml" xref="S2.E2.m1.4.4.3.3.2.3"><eq id="S2.E2.m1.4.4.3.3.2.3.1.cmml" xref="S2.E2.m1.4.4.3.3.2.3.1"></eq><ci id="S2.E2.m1.4.4.3.3.2.3.2.cmml" xref="S2.E2.m1.4.4.3.3.2.3.2">𝑗</ci><cn id="S2.E2.m1.4.4.3.3.2.3.3.cmml" type="integer" xref="S2.E2.m1.4.4.3.3.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.4.4.3.3.3.cmml" xref="S2.E2.m1.4.4.3.3.3">𝑘</ci></apply><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><times id="S2.E2.m1.4.4.3.2.3.cmml" xref="S2.E2.m1.4.4.3.2.3"></times><ci id="S2.E2.m1.4.4.3.2.4.cmml" xref="S2.E2.m1.4.4.3.2.4">𝑃</ci><apply id="S2.E2.m1.3.3.2.1.1.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.3.3.2.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.2">conditional</csymbol><ci id="S2.E2.m1.3.3.2.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.3">𝑦</ci><list id="S2.E2.m1.3.3.2.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci><apply id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.2">𝑟</ci><ci id="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.1.3">𝑗</ci></apply></list></apply><apply id="S2.E2.m1.4.4.3.2.5.cmml" xref="S2.E2.m1.4.4.3.2.5"><ci id="S2.E2.m1.4.4.3.2.5.1.cmml" xref="S2.E2.m1.4.4.3.2.5.1">¯</ci><ci id="S2.E2.m1.4.4.3.2.5.2.cmml" xref="S2.E2.m1.4.4.3.2.5.2">𝑃</ci></apply><apply id="S2.E2.m1.4.4.3.2.2.1.1.cmml" xref="S2.E2.m1.4.4.3.2.2.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.3.2.2.1.1.1.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.1">conditional</csymbol><apply id="S2.E2.m1.4.4.3.2.2.1.1.2.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.2.1.1.2.1.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.2">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2.1.1.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.2.2">𝑖</ci><ci id="S2.E2.m1.4.4.3.2.2.1.1.2.3.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.2.3">𝑗</ci></apply><ci id="S2.E2.m1.4.4.3.2.2.1.1.3.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\displaystyle P(y\mid x)=\sum_{j=1}^{k}P(y\mid x,r_{j})\bar{P}(i_{j}\mid x)</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">italic_P ( italic_y ∣ italic_x ) = ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_P ( italic_y ∣ italic_x , italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) over¯ start_ARG italic_P end_ARG ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∣ italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p3.5">where <math alttext="r_{i}=[i_{j},c_{j}]" class="ltx_Math" display="inline" id="S2.SS1.p3.3.m1.2"><semantics id="S2.SS1.p3.3.m1.2a"><mrow id="S2.SS1.p3.3.m1.2.2" xref="S2.SS1.p3.3.m1.2.2.cmml"><msub id="S2.SS1.p3.3.m1.2.2.4" xref="S2.SS1.p3.3.m1.2.2.4.cmml"><mi id="S2.SS1.p3.3.m1.2.2.4.2" xref="S2.SS1.p3.3.m1.2.2.4.2.cmml">r</mi><mi id="S2.SS1.p3.3.m1.2.2.4.3" xref="S2.SS1.p3.3.m1.2.2.4.3.cmml">i</mi></msub><mo id="S2.SS1.p3.3.m1.2.2.3" xref="S2.SS1.p3.3.m1.2.2.3.cmml">=</mo><mrow id="S2.SS1.p3.3.m1.2.2.2.2" xref="S2.SS1.p3.3.m1.2.2.2.3.cmml"><mo id="S2.SS1.p3.3.m1.2.2.2.2.3" stretchy="false" xref="S2.SS1.p3.3.m1.2.2.2.3.cmml">[</mo><msub id="S2.SS1.p3.3.m1.1.1.1.1.1" xref="S2.SS1.p3.3.m1.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.3.m1.1.1.1.1.1.2" xref="S2.SS1.p3.3.m1.1.1.1.1.1.2.cmml">i</mi><mi id="S2.SS1.p3.3.m1.1.1.1.1.1.3" xref="S2.SS1.p3.3.m1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.SS1.p3.3.m1.2.2.2.2.4" xref="S2.SS1.p3.3.m1.2.2.2.3.cmml">,</mo><msub id="S2.SS1.p3.3.m1.2.2.2.2.2" xref="S2.SS1.p3.3.m1.2.2.2.2.2.cmml"><mi id="S2.SS1.p3.3.m1.2.2.2.2.2.2" xref="S2.SS1.p3.3.m1.2.2.2.2.2.2.cmml">c</mi><mi id="S2.SS1.p3.3.m1.2.2.2.2.2.3" xref="S2.SS1.p3.3.m1.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S2.SS1.p3.3.m1.2.2.2.2.5" stretchy="false" xref="S2.SS1.p3.3.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m1.2b"><apply id="S2.SS1.p3.3.m1.2.2.cmml" xref="S2.SS1.p3.3.m1.2.2"><eq id="S2.SS1.p3.3.m1.2.2.3.cmml" xref="S2.SS1.p3.3.m1.2.2.3"></eq><apply id="S2.SS1.p3.3.m1.2.2.4.cmml" xref="S2.SS1.p3.3.m1.2.2.4"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.2.2.4.1.cmml" xref="S2.SS1.p3.3.m1.2.2.4">subscript</csymbol><ci id="S2.SS1.p3.3.m1.2.2.4.2.cmml" xref="S2.SS1.p3.3.m1.2.2.4.2">𝑟</ci><ci id="S2.SS1.p3.3.m1.2.2.4.3.cmml" xref="S2.SS1.p3.3.m1.2.2.4.3">𝑖</ci></apply><interval closure="closed" id="S2.SS1.p3.3.m1.2.2.2.3.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2"><apply id="S2.SS1.p3.3.m1.1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.3.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.1.2">𝑖</ci><ci id="S2.SS1.p3.3.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S2.SS1.p3.3.m1.2.2.2.2.2.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.2.2.2.2.2.1.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.3.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.2.2">𝑐</ci><ci id="S2.SS1.p3.3.m1.2.2.2.2.2.3.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.2.3">𝑗</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m1.2c">r_{i}=[i_{j},c_{j}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.3.m1.2d">italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ]</annotation></semantics></math> represents the retrieved image-caption pair, with <math alttext="c_{j}" class="ltx_Math" display="inline" id="S2.SS1.p3.4.m2.1"><semantics id="S2.SS1.p3.4.m2.1a"><msub id="S2.SS1.p3.4.m2.1.1" xref="S2.SS1.p3.4.m2.1.1.cmml"><mi id="S2.SS1.p3.4.m2.1.1.2" xref="S2.SS1.p3.4.m2.1.1.2.cmml">c</mi><mi id="S2.SS1.p3.4.m2.1.1.3" xref="S2.SS1.p3.4.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m2.1b"><apply id="S2.SS1.p3.4.m2.1.1.cmml" xref="S2.SS1.p3.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m2.1.1.1.cmml" xref="S2.SS1.p3.4.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.4.m2.1.1.2.cmml" xref="S2.SS1.p3.4.m2.1.1.2">𝑐</ci><ci id="S2.SS1.p3.4.m2.1.1.3.cmml" xref="S2.SS1.p3.4.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m2.1c">c_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.4.m2.1d">italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> being the caption corresponding to the retrieved image <math alttext="i_{j}" class="ltx_Math" display="inline" id="S2.SS1.p3.5.m3.1"><semantics id="S2.SS1.p3.5.m3.1a"><msub id="S2.SS1.p3.5.m3.1.1" xref="S2.SS1.p3.5.m3.1.1.cmml"><mi id="S2.SS1.p3.5.m3.1.1.2" xref="S2.SS1.p3.5.m3.1.1.2.cmml">i</mi><mi id="S2.SS1.p3.5.m3.1.1.3" xref="S2.SS1.p3.5.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m3.1b"><apply id="S2.SS1.p3.5.m3.1.1.cmml" xref="S2.SS1.p3.5.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m3.1.1.1.cmml" xref="S2.SS1.p3.5.m3.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m3.1.1.2.cmml" xref="S2.SS1.p3.5.m3.1.1.2">𝑖</ci><ci id="S2.SS1.p3.5.m3.1.1.3.cmml" xref="S2.SS1.p3.5.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m3.1c">i_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.5.m3.1d">italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="550" id="S2.F4.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance of the base model (LLaVA-1.5-7B) without using RAG (Base), RAG with irrelevant content (Irrelevant), and RAG on POPE-popular, MS-COCO, and CIFAR-10.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multimodal RAG Benefit LVLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">RAG has been proven to improve model performance on downstream tasks while maintaining a high level of trustworthiness in the field of NLP <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib16" title="">2020</a>); Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib3" title="">2023</a>); Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib47" title="">2023</a>); Jin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib14" title="">2024</a>)</cite>. However, in LVLMs, the full potential of RAG remains under-explored.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.5">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">2</span></a>, when addressing tasks such as VQA, captioning, and classification, we can enhance the model performance by retrieving relevant images and their corresponding descriptions to provide a pattern mapping for the input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_x</annotation></semantics></math>. The collection of pattern state is denoted as <math alttext="\mathbb{M}=\{M_{0},M_{1},\cdots,M_{n}\}" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.4"><semantics id="S2.SS2.p2.2.m2.4a"><mrow id="S2.SS2.p2.2.m2.4.4" xref="S2.SS2.p2.2.m2.4.4.cmml"><mi id="S2.SS2.p2.2.m2.4.4.5" xref="S2.SS2.p2.2.m2.4.4.5.cmml">𝕄</mi><mo id="S2.SS2.p2.2.m2.4.4.4" xref="S2.SS2.p2.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS2.p2.2.m2.4.4.3.3" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml"><mo id="S2.SS2.p2.2.m2.4.4.3.3.4" stretchy="false" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.SS2.p2.2.m2.2.2.1.1.1" xref="S2.SS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS2.p2.2.m2.2.2.1.1.1.2" xref="S2.SS2.p2.2.m2.2.2.1.1.1.2.cmml">M</mi><mn id="S2.SS2.p2.2.m2.2.2.1.1.1.3" xref="S2.SS2.p2.2.m2.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS2.p2.2.m2.4.4.3.3.5" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS2.p2.2.m2.3.3.2.2.2" xref="S2.SS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS2.p2.2.m2.3.3.2.2.2.2" xref="S2.SS2.p2.2.m2.3.3.2.2.2.2.cmml">M</mi><mn id="S2.SS2.p2.2.m2.3.3.2.2.2.3" xref="S2.SS2.p2.2.m2.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS2.p2.2.m2.4.4.3.3.6" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><mi id="S2.SS2.p2.2.m2.1.1" mathvariant="normal" xref="S2.SS2.p2.2.m2.1.1.cmml">⋯</mi><mo id="S2.SS2.p2.2.m2.4.4.3.3.7" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS2.p2.2.m2.4.4.3.3.3" xref="S2.SS2.p2.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS2.p2.2.m2.4.4.3.3.3.2" xref="S2.SS2.p2.2.m2.4.4.3.3.3.2.cmml">M</mi><mi id="S2.SS2.p2.2.m2.4.4.3.3.3.3" xref="S2.SS2.p2.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S2.SS2.p2.2.m2.4.4.3.3.8" stretchy="false" xref="S2.SS2.p2.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.4b"><apply id="S2.SS2.p2.2.m2.4.4.cmml" xref="S2.SS2.p2.2.m2.4.4"><eq id="S2.SS2.p2.2.m2.4.4.4.cmml" xref="S2.SS2.p2.2.m2.4.4.4"></eq><ci id="S2.SS2.p2.2.m2.4.4.5.cmml" xref="S2.SS2.p2.2.m2.4.4.5">𝕄</ci><set id="S2.SS2.p2.2.m2.4.4.3.4.cmml" xref="S2.SS2.p2.2.m2.4.4.3.3"><apply id="S2.SS2.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS2.p2.2.m2.2.2.1.1.1.2">𝑀</ci><cn id="S2.SS2.p2.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS2.p2.2.m2.2.2.1.1.1.3">0</cn></apply><apply id="S2.SS2.p2.2.m2.3.3.2.2.2.cmml" xref="S2.SS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS2.p2.2.m2.3.3.2.2.2.2">𝑀</ci><cn id="S2.SS2.p2.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS2.p2.2.m2.3.3.2.2.2.3">1</cn></apply><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">⋯</ci><apply id="S2.SS2.p2.2.m2.4.4.3.3.3.cmml" xref="S2.SS2.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS2.p2.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.p2.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS2.p2.2.m2.4.4.3.3.3.2">𝑀</ci><ci id="S2.SS2.p2.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS2.p2.2.m2.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.4c">\mathbb{M}=\{M_{0},M_{1},\cdots,M_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.4d">blackboard_M = { italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>, and
<math alttext="M_{i}\sim([I,T]\in\mathbb{M})" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.3"><semantics id="S2.SS2.p2.3.m3.3a"><mrow id="S2.SS2.p2.3.m3.3.3" xref="S2.SS2.p2.3.m3.3.3.cmml"><msub id="S2.SS2.p2.3.m3.3.3.3" xref="S2.SS2.p2.3.m3.3.3.3.cmml"><mi id="S2.SS2.p2.3.m3.3.3.3.2" xref="S2.SS2.p2.3.m3.3.3.3.2.cmml">M</mi><mi id="S2.SS2.p2.3.m3.3.3.3.3" xref="S2.SS2.p2.3.m3.3.3.3.3.cmml">i</mi></msub><mo id="S2.SS2.p2.3.m3.3.3.2" xref="S2.SS2.p2.3.m3.3.3.2.cmml">∼</mo><mrow id="S2.SS2.p2.3.m3.3.3.1.1" xref="S2.SS2.p2.3.m3.3.3.1.1.1.cmml"><mo id="S2.SS2.p2.3.m3.3.3.1.1.2" stretchy="false" xref="S2.SS2.p2.3.m3.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS2.p2.3.m3.3.3.1.1.1" xref="S2.SS2.p2.3.m3.3.3.1.1.1.cmml"><mrow id="S2.SS2.p2.3.m3.3.3.1.1.1.2.2" xref="S2.SS2.p2.3.m3.3.3.1.1.1.2.1.cmml"><mo id="S2.SS2.p2.3.m3.3.3.1.1.1.2.2.1" stretchy="false" xref="S2.SS2.p2.3.m3.3.3.1.1.1.2.1.cmml">[</mo><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">I</mi><mo id="S2.SS2.p2.3.m3.3.3.1.1.1.2.2.2" xref="S2.SS2.p2.3.m3.3.3.1.1.1.2.1.cmml">,</mo><mi id="S2.SS2.p2.3.m3.2.2" xref="S2.SS2.p2.3.m3.2.2.cmml">T</mi><mo id="S2.SS2.p2.3.m3.3.3.1.1.1.2.2.3" stretchy="false" xref="S2.SS2.p2.3.m3.3.3.1.1.1.2.1.cmml">]</mo></mrow><mo id="S2.SS2.p2.3.m3.3.3.1.1.1.1" xref="S2.SS2.p2.3.m3.3.3.1.1.1.1.cmml">∈</mo><mi id="S2.SS2.p2.3.m3.3.3.1.1.1.3" xref="S2.SS2.p2.3.m3.3.3.1.1.1.3.cmml">𝕄</mi></mrow><mo id="S2.SS2.p2.3.m3.3.3.1.1.3" stretchy="false" xref="S2.SS2.p2.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.3b"><apply id="S2.SS2.p2.3.m3.3.3.cmml" xref="S2.SS2.p2.3.m3.3.3"><csymbol cd="latexml" id="S2.SS2.p2.3.m3.3.3.2.cmml" xref="S2.SS2.p2.3.m3.3.3.2">similar-to</csymbol><apply id="S2.SS2.p2.3.m3.3.3.3.cmml" xref="S2.SS2.p2.3.m3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.3.3.3.1.cmml" xref="S2.SS2.p2.3.m3.3.3.3">subscript</csymbol><ci id="S2.SS2.p2.3.m3.3.3.3.2.cmml" xref="S2.SS2.p2.3.m3.3.3.3.2">𝑀</ci><ci id="S2.SS2.p2.3.m3.3.3.3.3.cmml" xref="S2.SS2.p2.3.m3.3.3.3.3">𝑖</ci></apply><apply id="S2.SS2.p2.3.m3.3.3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.3.3.1.1"><in id="S2.SS2.p2.3.m3.3.3.1.1.1.1.cmml" xref="S2.SS2.p2.3.m3.3.3.1.1.1.1"></in><interval closure="closed" id="S2.SS2.p2.3.m3.3.3.1.1.1.2.1.cmml" xref="S2.SS2.p2.3.m3.3.3.1.1.1.2.2"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">𝐼</ci><ci id="S2.SS2.p2.3.m3.2.2.cmml" xref="S2.SS2.p2.3.m3.2.2">𝑇</ci></interval><ci id="S2.SS2.p2.3.m3.3.3.1.1.1.3.cmml" xref="S2.SS2.p2.3.m3.3.3.1.1.1.3">𝕄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.3c">M_{i}\sim([I,T]\in\mathbb{M})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.3d">italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∼ ( [ italic_I , italic_T ] ∈ blackboard_M )</annotation></semantics></math>
, where <math alttext="I" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m4.1"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">I</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.4.m4.1d">italic_I</annotation></semantics></math> and <math alttext="T" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m5.1"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.5.m5.1d">italic_T</annotation></semantics></math> denote the image and description, respectively. Next, our goal is to learn this mapping:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx3">
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle f:x\rightarrow f(x|M_{i_{1}},M_{i_{2}},\ldots,M_{i_{k}})" class="ltx_Math" display="inline" id="S2.E3.m1.2"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mi id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml">f</mi><mo id="S2.E3.m1.2.2.2" lspace="0.278em" rspace="0.278em" xref="S2.E3.m1.2.2.2.cmml">:</mo><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.cmml"><mi id="S2.E3.m1.2.2.1.3" xref="S2.E3.m1.2.2.1.3.cmml">x</mi><mo id="S2.E3.m1.2.2.1.2" stretchy="false" xref="S2.E3.m1.2.2.1.2.cmml">→</mo><mrow id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.3" xref="S2.E3.m1.2.2.1.1.3.cmml">f</mi><mo id="S2.E3.m1.2.2.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.2.2.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml"><mo id="S2.E3.m1.2.2.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.5" xref="S2.E3.m1.2.2.1.1.1.1.1.5.cmml">x</mi><mo fence="false" id="S2.E3.m1.2.2.1.1.1.1.1.4" xref="S2.E3.m1.2.2.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E3.m1.2.2.1.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.4.cmml"><msub id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">M</mi><msub id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mn id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></msub><mo id="S2.E3.m1.2.2.1.1.1.1.1.3.3.4" xref="S2.E3.m1.2.2.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.2.cmml">M</mi><msub id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mn id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.3.cmml">2</mn></msub></msub><mo id="S2.E3.m1.2.2.1.1.1.1.1.3.3.5" xref="S2.E3.m1.2.2.1.1.1.1.1.3.4.cmml">,</mo><mi id="S2.E3.m1.1.1" mathvariant="normal" xref="S2.E3.m1.1.1.cmml">…</mi><mo id="S2.E3.m1.2.2.1.1.1.1.1.3.3.6" xref="S2.E3.m1.2.2.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.2" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.2.cmml">M</mi><msub id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.2" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mi id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.3.cmml">k</mi></msub></msub></mrow></mrow><mo id="S2.E3.m1.2.2.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><ci id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2">:</ci><ci id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3">𝑓</ci><apply id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.2.2.1"><ci id="S2.E3.m1.2.2.1.2.cmml" xref="S2.E3.m1.2.2.1.2">→</ci><ci id="S2.E3.m1.2.2.1.3.cmml" xref="S2.E3.m1.2.2.1.3">𝑥</ci><apply id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1.1"><times id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.2"></times><ci id="S2.E3.m1.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.3">𝑓</ci><apply id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.1.4.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.4">conditional</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.5.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.5">𝑥</ci><list id="S2.E3.m1.2.2.1.1.1.1.1.3.4.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3"><apply id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.2">𝑀</ci><apply id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.2">𝑖</ci><cn id="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.E3.m1.2.2.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.2">𝑀</ci><apply id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.2">𝑖</ci><cn id="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S2.E3.m1.2.2.1.1.1.1.1.2.2.2.3.3">2</cn></apply></apply><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">…</ci><apply id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.2">𝑀</ci><apply id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.2">𝑖</ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3.3.3.3.3">𝑘</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\displaystyle f:x\rightarrow f(x|M_{i_{1}},M_{i_{2}},\ldots,M_{i_{k}})</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.2d">italic_f : italic_x → italic_f ( italic_x | italic_M start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , … , italic_M start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">To better understand the impact of RAG on model performance, we conducted experiments comparing direct inference with RAG-enhanced inference across three datasets. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.F4" title="Figure 4 ‣ 2.1 Preliminaries ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the performance differences. It can be observed that retrieving and incorporating additional multimodal information (both image and text) significantly improves the model’s performance in tasks across VQA, Image Captioning, and Classification.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Irrelevant Harms Model Performance</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.2">Typically, the retrieval process <math alttext="\bar{P}(i_{j}\mid x)" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mover accent="true" id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS3.p1.1.m1.1.1.3.2" xref="S2.SS3.p1.1.m1.1.1.3.2.cmml">P</mi><mo id="S2.SS3.p1.1.m1.1.1.3.1" xref="S2.SS3.p1.1.m1.1.1.3.1.cmml">¯</mo></mover><mo id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.SS3.p1.1.m1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml"><mo id="S2.SS3.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.1.m1.1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml"><msub id="S2.SS3.p1.1.m1.1.1.1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.cmml"><mi id="S2.SS3.p1.1.m1.1.1.1.1.1.2.2" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.2.cmml">i</mi><mi id="S2.SS3.p1.1.m1.1.1.1.1.1.2.3" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S2.SS3.p1.1.m1.1.1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml">∣</mo><mi id="S2.SS3.p1.1.m1.1.1.1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.SS3.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2"></times><apply id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3"><ci id="S2.SS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3.1">¯</ci><ci id="S2.SS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.2">𝑃</ci></apply><apply id="S2.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.2">𝑖</ci><ci id="S2.SS3.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.3">𝑗</ci></apply><ci id="S2.SS3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\bar{P}(i_{j}\mid x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">over¯ start_ARG italic_P end_ARG ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∣ italic_x )</annotation></semantics></math> or <math alttext="\bar{P}(c_{j}\mid x)" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mrow id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mover accent="true" id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml"><mi id="S2.SS3.p1.2.m2.1.1.3.2" xref="S2.SS3.p1.2.m2.1.1.3.2.cmml">P</mi><mo id="S2.SS3.p1.2.m2.1.1.3.1" xref="S2.SS3.p1.2.m2.1.1.3.1.cmml">¯</mo></mover><mo id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">⁢</mo><mrow id="S2.SS3.p1.2.m2.1.1.1.1" xref="S2.SS3.p1.2.m2.1.1.1.1.1.cmml"><mo id="S2.SS3.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S2.SS3.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.2.m2.1.1.1.1.1" xref="S2.SS3.p1.2.m2.1.1.1.1.1.cmml"><msub id="S2.SS3.p1.2.m2.1.1.1.1.1.2" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2.cmml"><mi id="S2.SS3.p1.2.m2.1.1.1.1.1.2.2" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml">c</mi><mi id="S2.SS3.p1.2.m2.1.1.1.1.1.2.3" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S2.SS3.p1.2.m2.1.1.1.1.1.1" xref="S2.SS3.p1.2.m2.1.1.1.1.1.1.cmml">∣</mo><mi id="S2.SS3.p1.2.m2.1.1.1.1.1.3" xref="S2.SS3.p1.2.m2.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.SS3.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S2.SS3.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><times id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2"></times><apply id="S2.SS3.p1.2.m2.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.3"><ci id="S2.SS3.p1.2.m2.1.1.3.1.cmml" xref="S2.SS3.p1.2.m2.1.1.3.1">¯</ci><ci id="S2.SS3.p1.2.m2.1.1.3.2.cmml" xref="S2.SS3.p1.2.m2.1.1.3.2">𝑃</ci></apply><apply id="S2.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p1.2.m2.1.1.1.1.1.2.1.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2.2">𝑐</ci><ci id="S2.SS3.p1.2.m2.1.1.1.1.1.2.3.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.2.3">𝑗</ci></apply><ci id="S2.SS3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\bar{P}(c_{j}\mid x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">over¯ start_ARG italic_P end_ARG ( italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∣ italic_x )</annotation></semantics></math> is typically implemented by computing image-to-image or image-to-text similarity in CLIP embedding space <cite class="ltx_cite ltx_citemacro_cite">Ramos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib37" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib35" title="">a</a>)</cite>. However, this retrieval process is not always reliable, leading to the inclusion of irrelevant or misleading references. For example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">1</span></a>, the similarity scores returned the <span class="ltx_text" id="S2.SS3.p1.2.1">Top-2</span> images most similar to the test image. Nevertheless, these two images contribute differently to the original question. The latter image misleads the model and causes incorrect responses.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.F4" title="Figure 4 ‣ 2.1 Preliminaries ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates the impact of irrelevant information on RAG. It can be seen that the performance of RAG is even worse than without introducing any additional information, which indicates the negative impact of irrelevant or disturbing information on current LVLMs. We believe that the RAG of current LVLMs still has significant potential. If we can teach the model to selectively utilize the retrieved information and ignore the irrelevant or misleading ones, the performance of RAG in LVLMs will be further improved, potentially approaching the results shown by the gray bars.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Robust RAG Training Framework</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Since RAG has great potential to help improve the accuracy of model generation, and regardless of how the retriever is optimized, achieving perfect retrieval recall is unattainable <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib34" title="">2021</a>); Cherti et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib9" title="">2023</a>)</cite>. Therefore, we choose to optimize <math alttext="P(y\mid x,r_{i})," class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.2"><semantics id="S2.SS4.p1.1.m1.2a"><mrow id="S2.SS4.p1.1.m1.2.2.1" xref="S2.SS4.p1.1.m1.2.2.1.1.cmml"><mrow id="S2.SS4.p1.1.m1.2.2.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS4.p1.1.m1.2.2.1.1.3" xref="S2.SS4.p1.1.m1.2.2.1.1.3.cmml">P</mi><mo id="S2.SS4.p1.1.m1.2.2.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S2.SS4.p1.1.m1.2.2.1.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.cmml"><mo id="S2.SS4.p1.1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.3" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.3.cmml">y</mi><mo id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.2.cmml">∣</mo><mrow id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml">,</mo><msub id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub></mrow></mrow><mo id="S2.SS4.p1.1.m1.2.2.1.1.1.1.3" stretchy="false" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS4.p1.1.m1.2.2.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.2b"><apply id="S2.SS4.p1.1.m1.2.2.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1"><times id="S2.SS4.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.2"></times><ci id="S2.SS4.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.3">𝑃</ci><apply id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.2">conditional</csymbol><ci id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.3">𝑦</ci><list id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1"><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">𝑥</ci><apply id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.2">𝑟</ci><ci id="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.2c">P(y\mid x,r_{i}),</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.2d">italic_P ( italic_y ∣ italic_x , italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math> through teaching the model to learn to selectively utilize the retrieved information. We propose a self-refinement framework that enables LVLMs to selectively refer to relevant information from both image and text sources while effectively enhancing the model’s robustness against irrelevant or misleading content.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.2" style="width:433.6pt;height:186.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.6pt,33.2pt) scale(0.736355344575616,0.736355344575616) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T1.2.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S2.T1.2.2.3.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.3.1.2.1">VQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T1.2.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.3.1.3.1">Captioning</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T1.2.2.3.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.3.1.4.1">Classification</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.2.2.3.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.3.1.5.1">Avg.</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S2.T1.2.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.4">POPE (R)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.5">POPE (P)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.6">POPE (A)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.7">MMstar</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.1.1.1.1"><math alttext="\text{Vizwiz}^{V}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml"><mtext id="S2.T1.1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.1.m1.1.1.2a.cmml">Vizwiz</mtext><mi id="S2.T1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.m1.1.1.3.cmml">V</mi></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S2.T1.1.1.1.1.m1.1.1.2a.cmml" xref="S2.T1.1.1.1.1.m1.1.1.2"><mtext id="S2.T1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.1.m1.1.1.2">Vizwiz</mtext></ci><ci id="S2.T1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\text{Vizwiz}^{V}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">Vizwiz start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.8">MS-COCO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.2"><math alttext="\text{Vizwiz}^{C}" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.m1.1"><semantics id="S2.T1.2.2.2.2.m1.1a"><msup id="S2.T1.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.m1.1.1.cmml"><mtext id="S2.T1.2.2.2.2.m1.1.1.2" xref="S2.T1.2.2.2.2.m1.1.1.2a.cmml">Vizwiz</mtext><mi id="S2.T1.2.2.2.2.m1.1.1.3" xref="S2.T1.2.2.2.2.m1.1.1.3.cmml">C</mi></msup><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><apply id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.2.2.2.2.m1.1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">superscript</csymbol><ci id="S2.T1.2.2.2.2.m1.1.1.2a.cmml" xref="S2.T1.2.2.2.2.m1.1.1.2"><mtext id="S2.T1.2.2.2.2.m1.1.1.2.cmml" xref="S2.T1.2.2.2.2.m1.1.1.2">Vizwiz</mtext></ci><ci id="S2.T1.2.2.2.2.m1.1.1.3.cmml" xref="S2.T1.2.2.2.2.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\text{Vizwiz}^{C}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.m1.1d">Vizwiz start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.9">CIFAR-10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T1.2.2.2.10">EmoSet</th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="11" id="S2.T1.2.2.4.2.1" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.T1.2.2.4.2.1.1" style="background-color:#E6E6E6;">7B Parameter Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.2.2.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.5.1.1">Zero-shot</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.2">87.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.3">86.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.4">84.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.5">30.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.6">50.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.7">198.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.8">134.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.9">81.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.10">52.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.5.1.11">89.48</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.6.2.1">Vanilla-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.2">87.9</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.3">86.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.4">83.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.5">32.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.6">48.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.7">178.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.8">169.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.9">79.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.10">50.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.6.2.11">90.63</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.7.3.1">Rerank-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.2">88.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.3">86.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.4">83.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.5">31.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.6">49.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.7">210.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.8">164.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.9">80.9</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.10">50.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.7.3.11">93.81</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.8.4.1">Filter-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.2">88.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.3">86.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.8.4.4.1">83.9</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.5">31.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.6">51.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.7">231.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.8">172.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.9">82.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.10">51.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.8.4.11">97.71</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.9.5.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.1.1">SURf</span></th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.2.1">89.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.3.1">87.9</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.4">83.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.5.1">33.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.6.1">54.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.7"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.7.1">238.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.8"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.8.1">177.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.9"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.9.1">83.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.10"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.10.1">53.1</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.9.5.11"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.9.5.11.1">100.17</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.10.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="11" id="S2.T1.2.2.10.6.1" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.T1.2.2.10.6.1.1" style="background-color:#E6E6E6;">13B Parameter Model</span></th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.11.7.1">Zero-shot</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.2">87.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.3">86.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.4">84.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.5">32.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.6">53.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.7">210.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.8">150.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.9">82.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.10">56.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.11.7.11">93.71</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.12.8.1">Vanilla-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.2">88.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.3">86.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.4">83.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.5">33.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.6">50.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.7">218.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.8">160.9</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.9">80.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.10">55.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.12.8.11">95.23</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.13.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.13.9.1">Rerank-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.2">88.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.3">86.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.4">83.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.5">33.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.6">50.9</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.7">223.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.8">162.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.9">82.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.10">56.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.13.9.11">96.22</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.14.10.1">Filter-RAG</th>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.2">88.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.3">86.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.4">83.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.5">33.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.6">51.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.7">226.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.8">164.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.9">83.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.10">56.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.2.2.14.10.11">97.20</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2.15.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.2.2.15.11.1"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.1.1">SURf</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.2.1">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.3"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.3.1">87.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.4"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.4.1">84.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.5"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.5.1">34.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.6"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.6.1">54.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.7"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.7.1">250.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.8"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.8.1">177.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.9"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.9.1">85.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.10"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.10.1">58.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.2.2.15.11.11"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.15.11.11.1">102.50</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of our model on 7B and 13B parameters using four methods across seven tasks. In POPE, (R), (P), and (A) stand for Random, Popular, and Adversarial subsets, respectively (applies to all tables below.). <math alttext="\text{Vizwiz}^{V}" class="ltx_Math" display="inline" id="S2.T1.5.m1.1"><semantics id="S2.T1.5.m1.1b"><msup id="S2.T1.5.m1.1.1" xref="S2.T1.5.m1.1.1.cmml"><mtext id="S2.T1.5.m1.1.1.2" xref="S2.T1.5.m1.1.1.2a.cmml">Vizwiz</mtext><mi id="S2.T1.5.m1.1.1.3" xref="S2.T1.5.m1.1.1.3.cmml">V</mi></msup><annotation-xml encoding="MathML-Content" id="S2.T1.5.m1.1c"><apply id="S2.T1.5.m1.1.1.cmml" xref="S2.T1.5.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.5.m1.1.1.1.cmml" xref="S2.T1.5.m1.1.1">superscript</csymbol><ci id="S2.T1.5.m1.1.1.2a.cmml" xref="S2.T1.5.m1.1.1.2"><mtext id="S2.T1.5.m1.1.1.2.cmml" xref="S2.T1.5.m1.1.1.2">Vizwiz</mtext></ci><ci id="S2.T1.5.m1.1.1.3.cmml" xref="S2.T1.5.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.m1.1d">\text{Vizwiz}^{V}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.m1.1e">Vizwiz start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\text{Vizwiz}^{C}" class="ltx_Math" display="inline" id="S2.T1.6.m2.1"><semantics id="S2.T1.6.m2.1b"><msup id="S2.T1.6.m2.1.1" xref="S2.T1.6.m2.1.1.cmml"><mtext id="S2.T1.6.m2.1.1.2" xref="S2.T1.6.m2.1.1.2a.cmml">Vizwiz</mtext><mi id="S2.T1.6.m2.1.1.3" xref="S2.T1.6.m2.1.1.3.cmml">C</mi></msup><annotation-xml encoding="MathML-Content" id="S2.T1.6.m2.1c"><apply id="S2.T1.6.m2.1.1.cmml" xref="S2.T1.6.m2.1.1"><csymbol cd="ambiguous" id="S2.T1.6.m2.1.1.1.cmml" xref="S2.T1.6.m2.1.1">superscript</csymbol><ci id="S2.T1.6.m2.1.1.2a.cmml" xref="S2.T1.6.m2.1.1.2"><mtext id="S2.T1.6.m2.1.1.2.cmml" xref="S2.T1.6.m2.1.1.2">Vizwiz</mtext></ci><ci id="S2.T1.6.m2.1.1.3.cmml" xref="S2.T1.6.m2.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.m2.1d">\text{Vizwiz}^{C}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.m2.1e">Vizwiz start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> represents VQA and captioning based on Vizwiz. The best performance in the table is highlighted in <span class="ltx_text ltx_font_bold" id="S2.T1.8.1">bold</span>.</figcaption>
</figure>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T2.1" style="width:433.6pt;height:89.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.9pt,18.6pt) scale(0.704626362226795,0.704626362226795) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T2.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.1.1.2.1.2" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.2.1.2.1">Para.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.1.1.2.1.3" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.2.1.3.1">Shots</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T2.1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.4.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T2.1.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.5.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T2.1.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.6.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.1.1.2.1.7"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.7.1">MS-COCO</span></th>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row" id="S2.T2.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.3">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.4">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.5">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.6">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.7">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.8">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T2.1.1.1.1">CIDEr <math alttext="\uparrow" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T2.1.1.3.1.1">Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib2" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.2">9B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.3">4-shots</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.9">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3.1.10">93.1</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.1.4.2.1">OpenFlamingo <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib4" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.2">9B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.3">4-shots</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.4">48.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.5">48.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.6">49.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.7">49.0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.8">48.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.9">48.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.4.2.10">89.0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.1.5.3.1">Otter <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib19" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.2">9B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.3">4-shots</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.4">82.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.5">81.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.6">74.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.7">73.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.8">69.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.9">69.4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3.10">92.2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T2.1.1.6.4.1">MMICL <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib55" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.2">12.1B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.3">4-shots</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.4">87.3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.5">86.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.6">82.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.7">82.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.8">81.0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.9">80.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.6.4.10">95.7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T2.1.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.1.1">SURf</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.2">7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.3">2-shots</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.4.1">89.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.5"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.5.1">89.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.6"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.6.1">87.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.7.1">87.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.8"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.8.1">83.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.9.1">83.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.1.1.7.5.10"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.7.5.10.1">101.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of our 7B model compared to four ICL models on the three POPE subsets (VQA) and MS-COCO (captioning). The results of the ICL models are directly from the original paper.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Construction of Positive and Negative Examples</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.2">Introducing both relevant and irrelevant content during training can enhance the model’s ability to distinguish and select relevant information <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib26" title="">2023b</a>)</cite>. Therefore, at this stage, we construct positive and negative examples (denoted as <math alttext="\mathbb{C}_{\text{pos}}" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.1.m1.1"><semantics id="S2.SS4.SSS1.p1.1.m1.1a"><msub id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS4.SSS1.p1.1.m1.1.1.2" xref="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml">ℂ</mi><mtext id="S2.SS4.SSS1.p1.1.m1.1.1.3" xref="S2.SS4.SSS1.p1.1.m1.1.1.3a.cmml">pos</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.1b"><apply id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.2">ℂ</ci><ci id="S2.SS4.SSS1.p1.1.m1.1.1.3a.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3"><mtext id="S2.SS4.SSS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS4.SSS1.p1.1.m1.1.1.3">pos</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.1c">\mathbb{C}_{\text{pos}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.1.m1.1d">blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbb{C}_{\text{neg}}" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p1.2.m2.1"><semantics id="S2.SS4.SSS1.p1.2.m2.1a"><msub id="S2.SS4.SSS1.p1.2.m2.1.1" xref="S2.SS4.SSS1.p1.2.m2.1.1.cmml"><mi id="S2.SS4.SSS1.p1.2.m2.1.1.2" xref="S2.SS4.SSS1.p1.2.m2.1.1.2.cmml">ℂ</mi><mtext id="S2.SS4.SSS1.p1.2.m2.1.1.3" xref="S2.SS4.SSS1.p1.2.m2.1.1.3a.cmml">neg</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.2.m2.1b"><apply id="S2.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS1.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS1.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1.2">ℂ</ci><ci id="S2.SS4.SSS1.p1.2.m2.1.1.3a.cmml" xref="S2.SS4.SSS1.p1.2.m2.1.1.3"><mtext id="S2.SS4.SSS1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS4.SSS1.p1.2.m2.1.1.3">neg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.2.m2.1c">\mathbb{C}_{\text{neg}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p1.2.m2.1d">blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT</annotation></semantics></math>) for subsequent robust training.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p2">
<p class="ltx_p" id="S2.SS4.SSS1.p2.1">We hypothesize that if the model initially answers a question incorrectly but can answer correctly after including an example (both image and description), that example contains useful information (positive). Otherwise, the example is considered misleading or irrelevant (negative). Specifically, we first collect the data used by the LVLM during the SFT stage and use a fixed-parameter LVLM to answer questions based on images, recording incorrect examples. Then, we perform retrieval from the image-caption corpus to obtain the <span class="ltx_text" id="S2.SS4.SSS1.p2.1.1">Top-N</span> images and their corresponding descriptions. These are then appended to the test image and question, allowing the LVLM to answer the question again. We use specific evaluation tools to determine whether the answer has improved, remained unchanged, or worsened. Image-caption pairs that successfully improve the answer are considered positive examples of the current question, while those that do not cause any change or worsen the answer are considered negative examples of the current question.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p3">
<p class="ltx_p" id="S2.SS4.SSS1.p3.1">Notably, the data we construct is sourced from the examples in the existing LVLM training data used during the instruction fine-tuning stage, requiring no new external data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Data Filtering</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">Due to the token length limitation in LVLMs, we need to further filter the positive and negative examples obtained in the previous step. We exclude examples from the <span class="ltx_text" id="S2.SS4.SSS2.p1.1.1">Top-N</span> image-caption pairs that contain only positive or negative examples. For positive examples, we select the image with the highest similarity to the test image to ensure the inclusion of highly relevant information and to avoid model training collapse:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx4">
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p_{\text{pos}}\sim\max_{i_{j}\in\mathbb{C}_{\text{pos}}}p_{V}^{%
\theta}(x,i_{j})" class="ltx_Math" display="inline" id="S2.E4.m1.2"><semantics id="S2.E4.m1.2a"><mrow id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml"><msub id="S2.E4.m1.2.2.3" xref="S2.E4.m1.2.2.3.cmml"><mi id="S2.E4.m1.2.2.3.2" xref="S2.E4.m1.2.2.3.2.cmml">p</mi><mtext id="S2.E4.m1.2.2.3.3" xref="S2.E4.m1.2.2.3.3a.cmml">pos</mtext></msub><mo id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml">∼</mo><mrow id="S2.E4.m1.2.2.1" xref="S2.E4.m1.2.2.1.cmml"><mrow id="S2.E4.m1.2.2.1.3" xref="S2.E4.m1.2.2.1.3.cmml"><munder id="S2.E4.m1.2.2.1.3.1" xref="S2.E4.m1.2.2.1.3.1.cmml"><mi id="S2.E4.m1.2.2.1.3.1.2" xref="S2.E4.m1.2.2.1.3.1.2.cmml">max</mi><mrow id="S2.E4.m1.2.2.1.3.1.3" xref="S2.E4.m1.2.2.1.3.1.3.cmml"><msub id="S2.E4.m1.2.2.1.3.1.3.2" xref="S2.E4.m1.2.2.1.3.1.3.2.cmml"><mi id="S2.E4.m1.2.2.1.3.1.3.2.2" xref="S2.E4.m1.2.2.1.3.1.3.2.2.cmml">i</mi><mi id="S2.E4.m1.2.2.1.3.1.3.2.3" xref="S2.E4.m1.2.2.1.3.1.3.2.3.cmml">j</mi></msub><mo id="S2.E4.m1.2.2.1.3.1.3.1" xref="S2.E4.m1.2.2.1.3.1.3.1.cmml">∈</mo><msub id="S2.E4.m1.2.2.1.3.1.3.3" xref="S2.E4.m1.2.2.1.3.1.3.3.cmml"><mi id="S2.E4.m1.2.2.1.3.1.3.3.2" xref="S2.E4.m1.2.2.1.3.1.3.3.2.cmml">ℂ</mi><mtext id="S2.E4.m1.2.2.1.3.1.3.3.3" xref="S2.E4.m1.2.2.1.3.1.3.3.3a.cmml">pos</mtext></msub></mrow></munder><mo id="S2.E4.m1.2.2.1.3a" lspace="0.167em" xref="S2.E4.m1.2.2.1.3.cmml">⁡</mo><msubsup id="S2.E4.m1.2.2.1.3.2" xref="S2.E4.m1.2.2.1.3.2.cmml"><mi id="S2.E4.m1.2.2.1.3.2.2.2" xref="S2.E4.m1.2.2.1.3.2.2.2.cmml">p</mi><mi id="S2.E4.m1.2.2.1.3.2.2.3" xref="S2.E4.m1.2.2.1.3.2.2.3.cmml">V</mi><mi id="S2.E4.m1.2.2.1.3.2.3" xref="S2.E4.m1.2.2.1.3.2.3.cmml">θ</mi></msubsup></mrow><mo id="S2.E4.m1.2.2.1.2" xref="S2.E4.m1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.2.2.1.1.1" xref="S2.E4.m1.2.2.1.1.2.cmml"><mo id="S2.E4.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E4.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">x</mi><mo id="S2.E4.m1.2.2.1.1.1.3" xref="S2.E4.m1.2.2.1.1.2.cmml">,</mo><msub id="S2.E4.m1.2.2.1.1.1.1" xref="S2.E4.m1.2.2.1.1.1.1.cmml"><mi id="S2.E4.m1.2.2.1.1.1.1.2" xref="S2.E4.m1.2.2.1.1.1.1.2.cmml">i</mi><mi id="S2.E4.m1.2.2.1.1.1.1.3" xref="S2.E4.m1.2.2.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.E4.m1.2.2.1.1.1.4" stretchy="false" xref="S2.E4.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.2b"><apply id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2"><csymbol cd="latexml" id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2">similar-to</csymbol><apply id="S2.E4.m1.2.2.3.cmml" xref="S2.E4.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.3.1.cmml" xref="S2.E4.m1.2.2.3">subscript</csymbol><ci id="S2.E4.m1.2.2.3.2.cmml" xref="S2.E4.m1.2.2.3.2">𝑝</ci><ci id="S2.E4.m1.2.2.3.3a.cmml" xref="S2.E4.m1.2.2.3.3"><mtext id="S2.E4.m1.2.2.3.3.cmml" mathsize="70%" xref="S2.E4.m1.2.2.3.3">pos</mtext></ci></apply><apply id="S2.E4.m1.2.2.1.cmml" xref="S2.E4.m1.2.2.1"><times id="S2.E4.m1.2.2.1.2.cmml" xref="S2.E4.m1.2.2.1.2"></times><apply id="S2.E4.m1.2.2.1.3.cmml" xref="S2.E4.m1.2.2.1.3"><apply id="S2.E4.m1.2.2.1.3.1.cmml" xref="S2.E4.m1.2.2.1.3.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.1.1.cmml" xref="S2.E4.m1.2.2.1.3.1">subscript</csymbol><max id="S2.E4.m1.2.2.1.3.1.2.cmml" xref="S2.E4.m1.2.2.1.3.1.2"></max><apply id="S2.E4.m1.2.2.1.3.1.3.cmml" xref="S2.E4.m1.2.2.1.3.1.3"><in id="S2.E4.m1.2.2.1.3.1.3.1.cmml" xref="S2.E4.m1.2.2.1.3.1.3.1"></in><apply id="S2.E4.m1.2.2.1.3.1.3.2.cmml" xref="S2.E4.m1.2.2.1.3.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.1.3.2.1.cmml" xref="S2.E4.m1.2.2.1.3.1.3.2">subscript</csymbol><ci id="S2.E4.m1.2.2.1.3.1.3.2.2.cmml" xref="S2.E4.m1.2.2.1.3.1.3.2.2">𝑖</ci><ci id="S2.E4.m1.2.2.1.3.1.3.2.3.cmml" xref="S2.E4.m1.2.2.1.3.1.3.2.3">𝑗</ci></apply><apply id="S2.E4.m1.2.2.1.3.1.3.3.cmml" xref="S2.E4.m1.2.2.1.3.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.1.3.3.1.cmml" xref="S2.E4.m1.2.2.1.3.1.3.3">subscript</csymbol><ci id="S2.E4.m1.2.2.1.3.1.3.3.2.cmml" xref="S2.E4.m1.2.2.1.3.1.3.3.2">ℂ</ci><ci id="S2.E4.m1.2.2.1.3.1.3.3.3a.cmml" xref="S2.E4.m1.2.2.1.3.1.3.3.3"><mtext id="S2.E4.m1.2.2.1.3.1.3.3.3.cmml" mathsize="50%" xref="S2.E4.m1.2.2.1.3.1.3.3.3">pos</mtext></ci></apply></apply></apply><apply id="S2.E4.m1.2.2.1.3.2.cmml" xref="S2.E4.m1.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.2.1.cmml" xref="S2.E4.m1.2.2.1.3.2">superscript</csymbol><apply id="S2.E4.m1.2.2.1.3.2.2.cmml" xref="S2.E4.m1.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.2.2.1.cmml" xref="S2.E4.m1.2.2.1.3.2">subscript</csymbol><ci id="S2.E4.m1.2.2.1.3.2.2.2.cmml" xref="S2.E4.m1.2.2.1.3.2.2.2">𝑝</ci><ci id="S2.E4.m1.2.2.1.3.2.2.3.cmml" xref="S2.E4.m1.2.2.1.3.2.2.3">𝑉</ci></apply><ci id="S2.E4.m1.2.2.1.3.2.3.cmml" xref="S2.E4.m1.2.2.1.3.2.3">𝜃</ci></apply></apply><interval closure="open" id="S2.E4.m1.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.1.1.1"><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝑥</ci><apply id="S2.E4.m1.2.2.1.1.1.1.cmml" xref="S2.E4.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.2.2.1.1.1.1.2.cmml" xref="S2.E4.m1.2.2.1.1.1.1.2">𝑖</ci><ci id="S2.E4.m1.2.2.1.1.1.1.3.cmml" xref="S2.E4.m1.2.2.1.1.1.1.3">𝑗</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.2c">\displaystyle p_{\text{pos}}\sim\max_{i_{j}\in\mathbb{C}_{\text{pos}}}p_{V}^{%
\theta}(x,i_{j})</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.2d">italic_p start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT ∼ roman_max start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_x , italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">For negative examples, we choose the image with the highest similarity to the test image as hard negatives. These hard negatives are more similar to the positive examples, thus requiring the model to develop higher discriminative capabilities to accurately identify them:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx5">
<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p_{\text{neg}}\sim\max_{i_{j}\in\mathbb{C}_{\text{neg}}}p_{V}^{%
\theta}(x,i_{j})" class="ltx_Math" display="inline" id="S2.E5.m1.2"><semantics id="S2.E5.m1.2a"><mrow id="S2.E5.m1.2.2" xref="S2.E5.m1.2.2.cmml"><msub id="S2.E5.m1.2.2.3" xref="S2.E5.m1.2.2.3.cmml"><mi id="S2.E5.m1.2.2.3.2" xref="S2.E5.m1.2.2.3.2.cmml">p</mi><mtext id="S2.E5.m1.2.2.3.3" xref="S2.E5.m1.2.2.3.3a.cmml">neg</mtext></msub><mo id="S2.E5.m1.2.2.2" xref="S2.E5.m1.2.2.2.cmml">∼</mo><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.cmml"><mrow id="S2.E5.m1.2.2.1.3" xref="S2.E5.m1.2.2.1.3.cmml"><munder id="S2.E5.m1.2.2.1.3.1" xref="S2.E5.m1.2.2.1.3.1.cmml"><mi id="S2.E5.m1.2.2.1.3.1.2" xref="S2.E5.m1.2.2.1.3.1.2.cmml">max</mi><mrow id="S2.E5.m1.2.2.1.3.1.3" xref="S2.E5.m1.2.2.1.3.1.3.cmml"><msub id="S2.E5.m1.2.2.1.3.1.3.2" xref="S2.E5.m1.2.2.1.3.1.3.2.cmml"><mi id="S2.E5.m1.2.2.1.3.1.3.2.2" xref="S2.E5.m1.2.2.1.3.1.3.2.2.cmml">i</mi><mi id="S2.E5.m1.2.2.1.3.1.3.2.3" xref="S2.E5.m1.2.2.1.3.1.3.2.3.cmml">j</mi></msub><mo id="S2.E5.m1.2.2.1.3.1.3.1" xref="S2.E5.m1.2.2.1.3.1.3.1.cmml">∈</mo><msub id="S2.E5.m1.2.2.1.3.1.3.3" xref="S2.E5.m1.2.2.1.3.1.3.3.cmml"><mi id="S2.E5.m1.2.2.1.3.1.3.3.2" xref="S2.E5.m1.2.2.1.3.1.3.3.2.cmml">ℂ</mi><mtext id="S2.E5.m1.2.2.1.3.1.3.3.3" xref="S2.E5.m1.2.2.1.3.1.3.3.3a.cmml">neg</mtext></msub></mrow></munder><mo id="S2.E5.m1.2.2.1.3a" lspace="0.167em" xref="S2.E5.m1.2.2.1.3.cmml">⁡</mo><msubsup id="S2.E5.m1.2.2.1.3.2" xref="S2.E5.m1.2.2.1.3.2.cmml"><mi id="S2.E5.m1.2.2.1.3.2.2.2" xref="S2.E5.m1.2.2.1.3.2.2.2.cmml">p</mi><mi id="S2.E5.m1.2.2.1.3.2.2.3" xref="S2.E5.m1.2.2.1.3.2.2.3.cmml">V</mi><mi id="S2.E5.m1.2.2.1.3.2.3" xref="S2.E5.m1.2.2.1.3.2.3.cmml">θ</mi></msubsup></mrow><mo id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.2.2.1.1.1" xref="S2.E5.m1.2.2.1.1.2.cmml"><mo id="S2.E5.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E5.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml">x</mi><mo id="S2.E5.m1.2.2.1.1.1.3" xref="S2.E5.m1.2.2.1.1.2.cmml">,</mo><msub id="S2.E5.m1.2.2.1.1.1.1" xref="S2.E5.m1.2.2.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.1.1.1.1.2" xref="S2.E5.m1.2.2.1.1.1.1.2.cmml">i</mi><mi id="S2.E5.m1.2.2.1.1.1.1.3" xref="S2.E5.m1.2.2.1.1.1.1.3.cmml">j</mi></msub><mo id="S2.E5.m1.2.2.1.1.1.4" stretchy="false" xref="S2.E5.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.2b"><apply id="S2.E5.m1.2.2.cmml" xref="S2.E5.m1.2.2"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.cmml" xref="S2.E5.m1.2.2.2">similar-to</csymbol><apply id="S2.E5.m1.2.2.3.cmml" xref="S2.E5.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.3.1.cmml" xref="S2.E5.m1.2.2.3">subscript</csymbol><ci id="S2.E5.m1.2.2.3.2.cmml" xref="S2.E5.m1.2.2.3.2">𝑝</ci><ci id="S2.E5.m1.2.2.3.3a.cmml" xref="S2.E5.m1.2.2.3.3"><mtext id="S2.E5.m1.2.2.3.3.cmml" mathsize="70%" xref="S2.E5.m1.2.2.3.3">neg</mtext></ci></apply><apply id="S2.E5.m1.2.2.1.cmml" xref="S2.E5.m1.2.2.1"><times id="S2.E5.m1.2.2.1.2.cmml" xref="S2.E5.m1.2.2.1.2"></times><apply id="S2.E5.m1.2.2.1.3.cmml" xref="S2.E5.m1.2.2.1.3"><apply id="S2.E5.m1.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.1.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.3.1.1.cmml" xref="S2.E5.m1.2.2.1.3.1">subscript</csymbol><max id="S2.E5.m1.2.2.1.3.1.2.cmml" xref="S2.E5.m1.2.2.1.3.1.2"></max><apply id="S2.E5.m1.2.2.1.3.1.3.cmml" xref="S2.E5.m1.2.2.1.3.1.3"><in id="S2.E5.m1.2.2.1.3.1.3.1.cmml" xref="S2.E5.m1.2.2.1.3.1.3.1"></in><apply id="S2.E5.m1.2.2.1.3.1.3.2.cmml" xref="S2.E5.m1.2.2.1.3.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.3.1.3.2.1.cmml" xref="S2.E5.m1.2.2.1.3.1.3.2">subscript</csymbol><ci id="S2.E5.m1.2.2.1.3.1.3.2.2.cmml" xref="S2.E5.m1.2.2.1.3.1.3.2.2">𝑖</ci><ci id="S2.E5.m1.2.2.1.3.1.3.2.3.cmml" xref="S2.E5.m1.2.2.1.3.1.3.2.3">𝑗</ci></apply><apply id="S2.E5.m1.2.2.1.3.1.3.3.cmml" xref="S2.E5.m1.2.2.1.3.1.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.3.1.3.3.1.cmml" xref="S2.E5.m1.2.2.1.3.1.3.3">subscript</csymbol><ci id="S2.E5.m1.2.2.1.3.1.3.3.2.cmml" xref="S2.E5.m1.2.2.1.3.1.3.3.2">ℂ</ci><ci id="S2.E5.m1.2.2.1.3.1.3.3.3a.cmml" xref="S2.E5.m1.2.2.1.3.1.3.3.3"><mtext id="S2.E5.m1.2.2.1.3.1.3.3.3.cmml" mathsize="50%" xref="S2.E5.m1.2.2.1.3.1.3.3.3">neg</mtext></ci></apply></apply></apply><apply id="S2.E5.m1.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.3.2.1.cmml" xref="S2.E5.m1.2.2.1.3.2">superscript</csymbol><apply id="S2.E5.m1.2.2.1.3.2.2.cmml" xref="S2.E5.m1.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.3.2.2.1.cmml" xref="S2.E5.m1.2.2.1.3.2">subscript</csymbol><ci id="S2.E5.m1.2.2.1.3.2.2.2.cmml" xref="S2.E5.m1.2.2.1.3.2.2.2">𝑝</ci><ci id="S2.E5.m1.2.2.1.3.2.2.3.cmml" xref="S2.E5.m1.2.2.1.3.2.2.3">𝑉</ci></apply><ci id="S2.E5.m1.2.2.1.3.2.3.cmml" xref="S2.E5.m1.2.2.1.3.2.3">𝜃</ci></apply></apply><interval closure="open" id="S2.E5.m1.2.2.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1"><ci id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1">𝑥</ci><apply id="S2.E5.m1.2.2.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.1.1.1.1.2">𝑖</ci><ci id="S2.E5.m1.2.2.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.1.1.1.1.3">𝑗</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.2c">\displaystyle p_{\text{neg}}\sim\max_{i_{j}\in\mathbb{C}_{\text{neg}}}p_{V}^{%
\theta}(x,i_{j})</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.2d">italic_p start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT ∼ roman_max start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_x , italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>RAG Instruction-Tuning</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1">Using the high-quality positive and negative example pairs generated through the above process, we fine-tune the existing model with RAG instructions. The retrieved images and their corresponding descriptions are concatenated sequentially before the test image, enclosed by special characters <span class="ltx_text ltx_font_typewriter" id="S2.SS4.SSS3.p1.1.1">&lt;Retrieval&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS4.SSS3.p1.1.2">&lt;/Retrieval&gt;</span>. This ensures that the model can effectively distinguish between retrieved-context and the actual test input, enhancing its ability to leverage relevant information while minimizing the impact of irrelevant or misleading data.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS3.p2">
<p class="ltx_p" id="S2.SS4.SSS3.p2.1">The algorithm of our method is shown in the Appendix Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#alg1" title="Algorithm 1 ‣ A.1 Algorithm ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T3.1" style="width:433.6pt;height:216.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.1pt,26.9pt) scale(0.80044607746292,0.80044607746292) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T3.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.2.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.3.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.4.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S2.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.5.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.6.1">CIFAR-10</span></th>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S2.T3.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.2">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.3">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.4">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.5">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.6">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.7">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.8">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.9">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.10">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.11">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.12">SPICE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S2.T3.1.1.2.2.13">Acc.</th>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T3.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.3.3.1.1">Zero-shot</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.2">87.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.3">86.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.4">86.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.5">84.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.6">84.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.7">83.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.8">22.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.9">28.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.10">50.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.11">75.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.12">22.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T3.1.1.3.3.13">81.5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.1.1.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T3.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.1.1">Vanilla-RAG</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.2.1">87.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.3.1">86.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.4"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.4.1">86.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.5"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.5.1">85.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.6"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.6.1">83.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.7"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.7.1">82.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.8"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.8.1">24.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.9"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.9.1">28.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.10"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.10.1">51.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.11"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.11.1">79.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.12"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.12.1">22.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.4.1.13"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.4.1.13.1">79.7</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.5.2.1">w/ 1k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.2">87.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.3">86.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.4">85.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.5">84.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.6">82.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.7">81.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.8">22.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.9">27.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.10">50.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.11">75.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.12">21.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2.13">79.5</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.6.3.1">w/ 5k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.2">87.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.3">86.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.4">85.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.5">84.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.6">82.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.7">81.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.8">22.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.9">27.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.10">50.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.11">75.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.12">21.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.6.3.13">77.1</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.7.4.1">w/ 10k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.2">87.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.3">86.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.4">85.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.5">83.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.6">82.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.7">81.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.8">22.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.9">27.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.10">50.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.11">75.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.12">21.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.7.4.13">76.7</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.8.5.1">w/ 100k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.2">87.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.3">85.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.4">84.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.5">83.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.6">82.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.7">81.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.8">22.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.9">27.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.10">50.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.11">74.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.12">21.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.8.5.13">76.4</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.9.6.1">w/ 1,000k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.2">86.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.3">85.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.4">84.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.5">83.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.6">81.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.7">80.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.8">22.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.9">27.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.10">49.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.11">73.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.12">21.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.9.6.13">75.6</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T3.1.1.10.7.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.2.1">89.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.3.1">89.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.4"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.4.1">87.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.5"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.5.1">87.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.6"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.6.1">83.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.7"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.7.1">83.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.8"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.8.1">27.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.9"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.9.1">29.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.10"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.10.1">55.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.11"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.11.1">101.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.12"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.12.1">24.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.10.7.13"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.10.7.13.1">83.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.11.8.1">w/ 1k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.2">88.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.3">88.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.4">87.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.5">87.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.6">83.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.7">83.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.8">26.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.9">29.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.10">54.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.11">97.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.12">23.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.11.8.13">83.1</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.12.9.1">w/ 5k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.2">89.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.3">88.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.4">87.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.5">87.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.6">83.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.7">83.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.8">26.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.9">29.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.10">53.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.11">97.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.12">23.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.12.9.13">82.4</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.13.10.1">w/ 10k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.2">89.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.3">88.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.4">87.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.5">87.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.6">83.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.7">83.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.8">26.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.9">29.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.10">54.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.11">97.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.12">23.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.13.10.13">83.4</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.1.14.11.1">w/ 100k</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.2">89.1</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.3">88.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.4">87.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.5">87.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.6">83.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.7">83.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.8">26.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.9">29.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.10">53.9</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.11">96.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.12">23.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.14.11.13">80.5</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.15.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T3.1.1.15.12.1">w/ 1,000k</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.2">89.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.3">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.4">87.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.5">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.6">83.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.7">83.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.8">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.9">29.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.10">54.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.11">98.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.12">23.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T3.1.1.15.12.13">80.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison of our model and vanilla-RAG on three tasks when introducing irrelevant image-caption pairs. "1k to 1,000k" indicates the range of similarity between the introduced images and the test images, with larger values indicating less relevance.</figcaption>
</figure>
<figure class="ltx_table" id="S2.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T4.1" style="width:433.6pt;height:87.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.1pt,10.8pt) scale(0.80044607746292,0.80044607746292) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T4.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T4.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T4.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.2.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T4.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.3.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T4.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.4.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S2.T4.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.5.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T4.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S2.T4.1.1.1.1.6.1">CIFAR-10</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T4.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row" id="S2.T4.1.1.2.1.1"></th>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.2">Acc.</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.3">F1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.4">Acc.</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.5">F1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.6">Acc.</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.7">F1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.8">B@4</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.9">METEOR</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.10">ROUGE-L</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.11">CIDEr</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.12">SPICE</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.2.1.13">Acc.</td>
</tr>
<tr class="ltx_tr" id="S2.T4.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T4.1.1.3.2.1">Vanilla-RAG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.2">87.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.3">86.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.4">86.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.5">85.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.6">83.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.7">82.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.8">24.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.9">28.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.10">52.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.11">89.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.12">22.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.3.2.13">79.5</th>
</tr>
<tr class="ltx_tr" id="S2.T4.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T4.1.1.4.3.1">w/ Switch</th>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.2">87.2</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.3">86.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.4">85.7</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.5">84.6</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.6">82.4</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.7">82.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.8">22.2</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.9">28.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.10">50.8</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.11">75.1</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.12">22.0</td>
<td class="ltx_td ltx_align_center" id="S2.T4.1.1.4.3.13">78.4</td>
</tr>
<tr class="ltx_tr" id="S2.T4.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T4.1.1.5.4.1">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.2">89.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.3">89.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.4">87.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.5">87.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.6">83.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.7">83.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.8">27.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.9">29.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.10">55.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.11">101.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.12">24.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T4.1.1.5.4.13">83.5</th>
</tr>
<tr class="ltx_tr" id="S2.T4.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T4.1.1.6.5.1">w/ Switch</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.2">89.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.3">89.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.4">87.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.5">87.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.6">83.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.7">83.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.8">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.9">29.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.10">54.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.11">97.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.12">23.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T4.1.1.6.5.13">83.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison of our model and vanilla-RAG on three tasks in the random switching of retrieved content setting.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We evaluated our model using seven datasets across three distinct tasks: VQA: POPE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib21" title="">2023c</a>)</cite>, MMStar <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib8" title="">2024</a>)</cite>, Vizwiz-VQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib6" title="">2022</a>)</cite>, Image Captioning: MS-COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib25" title="">2014</a>)</cite>, Vizwiz-Caption <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib13" title="">2020</a>)</cite>, Image Classification: CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib17" title="">2009</a>)</cite>, EmoSet <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib48" title="">2023a</a>)</cite>. For more detailed information and metrics can be found in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.SS2" title="A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baselines</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We compared four methods among LlaVA-1.5-7B and LLaVA-1.5-13B <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib28" title="">2023b</a>)</cite>:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Zero-shot</span> Directly prompting LVLMs to generate responses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Vanilla-RAG</span> Concatenating the Top-N image-caption pairs from the database, which have the highest CLIP score similarity to the test image before the questions and images for the LVLMs to respond.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Rerank-RAG</span> Building on Vanilla-RAG, we prompt the LLM to generate a caption for the test image. We then calculate the BERT-Score between this caption and the descriptions of the retrieved images, ranking the image-caption pairs with higher relevance scores at the top.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Filter-RAG</span> Enhancing Rerank-RAG by removing any image-caption pairs with a similarity score less than <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_S</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">Additionally, we compared four In-Context Learning (ICL) models: Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib2" title="">2022</a>)</cite>, OpenFlamingo <cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib4" title="">2023</a>)</cite>, Otter <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib19" title="">2023a</a>)</cite>, and MMICL <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib55" title="">2023</a>)</cite>. For all approaches, we used greedy decoding as the decoding strategy.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation details</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We collected 60,000 initial incorrect responses from LVLMs and generated 10,000 samples with positive and negative sample pairs. After filtering, we refined this to 2,000 samples for the final training data. We use LLaVA-1.5 as the LVLM backbone of our model SURf-7B and SURf-13B and use CLIP (ViT-L with a resolution of 336*336) <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib34" title="">2021</a>)</cite> as the vision encoder. Our 7B and 13B models are further trained from the instruction-finetuned LLaVA-1.5-7B and LLaVA-1.5-13B models following previous works <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib23" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib24" title="">2023a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib20" title="">2023b</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib29" title="">2023c</a>)</cite> since LLaVA is the most popular used LVLMs. We use 8 A100-80G to training 1 hour for 2 epochs. For the VQA and image captioning task, we use exact match and Bert-Score <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib54" title="">2020</a>)</cite> as the evaluation tool respectively, mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.SS4.SSS2" title="2.4.2 Data Filtering ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">2.4.2</span></a>. We use ShareGPT4v-PT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib7" title="">2023</a>)</cite> as our database for RAG, which includes approximately 1,246k image-caption pairs with an average caption length of 826. For the retrieval system, we use FAISS <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib15" title="">2021</a>)</cite> with flat indexes to pre-index the computed embeddings of all images in the database.</p>
</div>
<figure class="ltx_figure" id="S3.F6.sf1">
</figure>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="398" id="S3.F6.g1" src="x5.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Efficiency analysis of our model compared to four methods. We report the running times per sample.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Experimental Results</h3>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Compare to Baselines</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T1" title="Table 1 ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">1</span></a> presents the comparison of our model, trained with our method, against four other methods. On the VQA task, our model significantly outperforms previous methods, with a VQA accuracy improvement of approximately 3.7% for the 7B model compared to zero-shot and 2.3% compared to Filter-RAG, achieving state-of-the-art results. Furthermore, on the captioning task, the improvement of our model is even more pronounced (detailed results can be found in the Appendix).</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">In contrast, for the classification task, vanilla-RAG may perform worse than direct inference. However, our training method enables the model to selectively refer to the retrieved content, resulting in a final performance that significantly surpasses zero-shot. For the 13B model, the improvement in captioning is even more significant, with an approximate 34.1% increase compared to zero-shot. Additionally, the table illustrates that simple methods, such as reranking and filtering, cannot effectively address the problem of irrelevant content introduced by retrieval.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Compare to ICL models</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">The experiments in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T2" title="Table 2 ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">2</span></a> compare our model with various ICL models, as ICL models are very similar to ours at the input level. Despite having fewer parameters and exemplars (For the ICL models, more shots correspond to better performance, we used their 4-shot results for comparison since they only reported results for 4-shot or 32-shot scenarios.) in the prompts compared to the other models, our model achieves the best performance on both the POPE and MS-COCO datasets. Specifically, it improves the average accuracy by 3.4% and the F1 score by 3.8% on POPE compared to the second-best model. This demonstrates that our model can effectively utilize the retrieved content to enhance the performance of downstream tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Robustness</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T3" title="Table 3 ‣ 2.4.3 RAG Instruction-Tuning ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T4" title="Table 4 ‣ 2.4.3 RAG Instruction-Tuning ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">4</span></a> present the results of our robustness tests. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T3" title="Table 3 ‣ 2.4.3 RAG Instruction-Tuning ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">3</span></a>, we maintain the image-caption pair with the highest CLIP similarity score among the retrieved content to ensure effective information. We also introduce image-caption pairs from the Top-K (from 1k to 1,000k) positions as forced irrelevant information. The results show that the performance of vanilla-RAG significantly declines on the three datasets as more irrelevant image-caption pairs are introduced. In contrast, our model’s performance remains very stable. Notably, the model’s performance when introducing 100k and 1000k irrelevant image-caption pairs is better than when introducing 1k pairs. This improvement is because, after training with hard negative samples, our model can easily distinguish content unrelated to the test image and question, thereby focusing more on other relevant information in the retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S2.T4" title="Table 4 ‣ 2.4.3 RAG Instruction-Tuning ‣ 2.4 Robust RAG Training Framework ‣ 2 Robust Multimodal RAG ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">4</span></a> shows that our model remains robust even after randomly shuffling the examples, whereas vanilla-RAG exhibits a significant decline in performance. This demonstrates that training the model with our proposed framework enables it to selectively extract relevant information from the retrieved content, making it less sensitive to the order of the examples.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Ablation Study</h3>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Size of the Database</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">We conducted experiments using different databases as retrieval sources, with results shown in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:database</span>. It can be seen that using COCO-2017 (approximately 118k image-caption pairs) and ShareGPT-4V (approximately 1,246k image-caption pairs) results in notable differences in model performance for VQA and classification tasks, while the metrics for the captioning task show minimal differences. The reason is that VQA and classification tasks are more challenging for the model compared to captioning, requiring a larger retrieval source to provide more diverse reference image-caption pairs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Data Filter</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:filter</span> presents the results with and without using the data filtering step. It can be seen that the performance of the model trained without data filtering is significantly worse compared to when data filtering is used. This highlights the importance of filtering positive and negative samples and training with hard negative sampling in our training framework.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS5.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Efficiency Analysis</h5>
<div class="ltx_para" id="S3.SS5.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS5.SSS0.Px3.p1.1">We compared the efficiency of our method with four other methods, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#S3.F6" title="Figure 6 ‣ 3.3 Implementation details ‣ 3 Experiment ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">6</span></a>. We calculate the average running time for 1,000 samples in image captioning tasks with the max token length set to 256. It can be observed that our method increases the time by approximately 1.3 seconds per sample compared to the zero-shot approach. This increase is primarily due to the time required to convert the image to an embedding, retrieval time, and the additional overhead introduced by the increased length of the prompt. However, this slight increase in time is acceptable considering the performance improvement.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS5.SSS0.Px3.p2.1">In contrast, Rerank-RAG and Filter-RAG are slower because they require additional prompts for the LVLMs to generate captions for the current image, which are then used for text similarity comparisons.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Large Vision-Language Models</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Large vision-language models (LVLMs) have greatly benefited from advancements in large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib44" title="">2023</a>); Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib10" title="">2023</a>); Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib39" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib42" title="">c</a>)</cite>, which integrate a vision encoder with a language model backbone. Leveraging the success of LLMs through pre-training and instruction tuning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib28" title="">2023b</a>); Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib51" title="">2023</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib58" title="">2023</a>); Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib5" title="">2023</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib11" title="">2023</a>)</cite>, LVLMs like LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib28" title="">2023b</a>)</cite> employ GPT-4V<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://openai.com/research/gpt-4v-system-card</span></span></span> to generate diverse instruction datasets, thereby enhancing their capacity to understand images and follow human instructions. Despite these successes, current MLLMs still face significant challenges with hallucinations <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib40" title="">2022</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib22" title="">2023d</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib27" title="">2023a</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib45" title="">2024a</a>); Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib56" title="">2024</a>); Leng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib18" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib53" title="">2024</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib57" title="">2024</a>)</cite>. These issues often result from misalignment between the vision and language components, leading to neglecting image details and generating incorrect content. Our work aims to improve LVLMs’ ability to selectively reference retrieved information and enhance robustness against misleading content.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Retrieval-Augmented Generation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Retrieval-augmented generation (RAG) has become a powerful approach in natural language processing, combining the strengths of retrieval-based methods and generative models <cite class="ltx_cite ltx_citemacro_cite">Merth et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib30" title="">2024</a>); Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib3" title="">2023</a>); Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib47" title="">2023</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib26" title="">2023b</a>); Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib41" title="">2024b</a>)</cite>. In the NLP domain, RAG aims to select the most relevant documents from a large corpus using techniques such as BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson and Zaragoza (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib38" title="">2009</a>)</cite> and neural retrievers like DPR <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib16" title="">2020</a>)</cite>. However, the challenge in the multimodal domain is considerably higher, as the retrieval dimension encompasses images along with text. Previous works <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib49" title="">2023b</a>); Ramos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib37" title="">2023c</a>); Yasunaga et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib50" title="">2023</a>); Ramos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib35" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib36" title="">b</a>)</cite> have shown that retrieving similar images based on a test image and using their corresponding captions can enhance model performance on captioning tasks. Nevertheless, these methods often fail to address how to manage irrelevant image-caption pairs, which can decrease model accuracy. Our work focuses on improving LVLMs’ ability to selectively reference pertinent retrieved information and increase robustness against misleading content, thereby enhancing performance across various downstream tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper introduces a robust self-refinement multimodal RAG training framework designed for LVLMs. Our approach incorporates retrieval information for initially incorrect answers, filtering in beneficial positive examples and excluding detrimental negative ones. We implement a hard-negative sampling strategy to preserve the training data of the highest quality and employ RAG-based instruction fine-tuning. Experimental results across seven datasets spanning three different tasks show that our method significantly enhances the capability of LVLMs to effectively utilize multimodal retrieval information, while also improving their resilience against misleading content.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our method mainly has three limitations:</p>
</div>
<div class="ltx_para" id="S6.p2">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">Our retrieval approach heavily depends on large-scale, high-quality data sources. While using only the training data as the data source is a feasible solution, the performance is slightly inferior compared to large-scale data sources in complex tasks. Future work should explore how to leverage small sample data sources for inference through retrieval.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">Despite our method having been extensively evaluated on tasks such as Visual Question Answering (VQA), image captioning, and image classification, its generalization to other visual tasks, such as image generation and image segmentation, remains unexplored. Future work should investigate the adaptability of our framework to a broader range of tasks.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">Given that the retrieval process currently supports a maximum of three image-caption pairs due to lengthy descriptions, future optimizations could include using shorter captions, employing methods to compress descriptions, or increasing the maximum input tokens for LVLMs. These improvements would enable more image-caption pairs to be included, enhancing the accuracy of downstream tasks of LVLMs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">gpt (2023)</span>
<span class="ltx_bibblock">
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:263218031" title="">Gpt-4v(ision) system card</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" title="">Flamingo: a visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awadalla et al. (2023)</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.01390" title="">Openflamingo: An open-source framework for training large autoregressive vision-language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">CoRR</em>, abs/2308.01390.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2308.12966</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Chongyan Chen, Samreen Anjum, and Danna Gurari. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR52688.2022.01851" title="">Grounding answers for visual questions asked by visually impaired people</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>, pages 19076–19085. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.12793" title="">Sharegpt4v: Improving large multi-modal models with better captions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2311.12793.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. 2024.

</span>
<span class="ltx_bibblock">Are we on the right way for evaluating large vision-language models?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2403.20330</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherti et al. (2023)</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR52729.2023.00276" title="">Reproducible scaling laws for contrastive language-image learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023</em>, pages 2818–2829. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.06500" title="">Instructblip: Towards general-purpose vision-language models with instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Preprint</em>, arXiv:2305.06500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2024)</span>
<span class="ltx_bibblock">
Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, et al. 2024.

</span>
<span class="ltx_bibblock">Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2403.01777</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2020)</span>
<span class="ltx_bibblock">
Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-58520-4_25" title="">Captioning images taken by people who are blind</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVII</em>, volume 12362 of <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">Lecture Notes in Computer Science</em>, pages 417–434. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2024)</span>
<span class="ltx_bibblock">
Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. 2024.

</span>
<span class="ltx_bibblock">The impact of reasoning step length on large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2401.04925</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2021)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TBDATA.2019.2921572" title="">Billion-scale similarity search with gpus</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Trans. Big Data</em>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.EMNLP-MAIN.550" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 6769–6781. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky (2009)</span>
<span class="ltx_bibblock">
Alex Krizhevsky. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:18268744" title="">Learning multiple layers of features from tiny images</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leng et al. (2023)</span>
<span class="ltx_bibblock">
Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Mitigating object hallucinations in large vision-language models through visual contrastive decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2311.16922</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.03726" title="">Otter: A multi-modal model with in-context instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2305.03726.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023b.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2306.00890</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=xozJw0kZXF" title="">Evaluating object hallucination in large vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023d)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2305.10355</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. 2024.

</span>
<span class="ltx_bibblock">Moe-llava: Mixture of experts for large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2401.15947</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023a)</span>
<span class="ltx_bibblock">
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023a.

</span>
<span class="ltx_bibblock">Video-llava: Learning united visual representation by alignment before projection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2311.10122</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-319-10602-1_48" title="">Microsoft COCO: common objects in context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V</em>, volume 8693 of <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">Lecture Notes in Computer Science</em>, pages 740–755. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023b)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01352" title="">RA-DIT: retrieval-augmented dual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023a.

</span>
<span class="ltx_bibblock">Mitigating hallucination in large multi-modal models via robust instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.03744" title="">Improved baselines with visual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CoRR</em>, abs/2310.03744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. 2023c.

</span>
<span class="ltx_bibblock">Llava-plus: Learning to use tools for creating multimodal agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv:2311.05437</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merth et al. (2024)</span>
<span class="ltx_bibblock">
Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.06910" title="">Superposition prompting: Improving and accelerating retrieval-augmented generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:2404.06910.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2024a)</span>
<span class="ltx_bibblock">
Xiaoye Qu, Qiyuan Chen, Wei Wei, Jishuo Sun, and Jianfeng Dong. 2024a.

</span>
<span class="ltx_bibblock">Alleviating hallucination in large vision-language models with active retrieval augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2408.00555</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2024b)</span>
<span class="ltx_bibblock">
Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, and Yu Cheng. 2024b.

</span>
<span class="ltx_bibblock">Mitigating multilingual hallucination in large vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2408.00550</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2024c)</span>
<span class="ltx_bibblock">
Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. 2024c.

</span>
<span class="ltx_bibblock">Look, compare, decide: Alleviating hallucination in large vision-language models via multi-view multi-path reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2408.17150</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://proceedings.mlr.press/v139/radford21a.html" title="">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et al. (2023a)</span>
<span class="ltx_bibblock">
Rita Ramos, Desmond Elliott, and Bruno Martins. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.EACL-MAIN.266" title="">Retrieval-augmented image captioning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, pages 3648–3663. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et al. (2023b)</span>
<span class="ltx_bibblock">
Rita Ramos, Bruno Martins, and Desmond Elliott. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.104" title="">Lmcap: Few-shot multilingual image captioning by retrieval augmented language model prompting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 1635–1651. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et al. (2023c)</span>
<span class="ltx_bibblock">
Rita Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/CVPR52729.2023.00278" title="">Smallcap: Lightweight image captioning prompted with retrieval augmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023</em>, pages 2840–2849. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
Stephen E. Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1561/1500000019" title="">The probabilistic relevance framework: BM25 and beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Found. Trends Inf. Retr.</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024a)</span>
<span class="ltx_bibblock">
Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al. 2024a.

</span>
<span class="ltx_bibblock">Living in the moment: Can large language models grasp co-temporal reasoning?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2406.09072</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2022)</span>
<span class="ltx_bibblock">
Zhaochen Su, Zecheng Tang, Xinyan Guan, Juntao Li, Lijun Wu, and Min Zhang. 2022.

</span>
<span class="ltx_bibblock">Improving temporal generalization of pre-trained language models with lexical semantic change.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2210.17127</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024b)</span>
<span class="ltx_bibblock">
Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng. 2024b.

</span>
<span class="ltx_bibblock">Conflictbank: A benchmark for evaluating the influence of knowledge conflicts in llm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2408.12076</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024c)</span>
<span class="ltx_bibblock">
Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. 2024c.

</span>
<span class="ltx_bibblock">Timo: Towards better temporal reasoning for language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2406.14192</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. 2024.

</span>
<span class="ltx_bibblock">Visual agents as fast and slow thinkers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2408.08862</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. 2024a.

</span>
<span class="ltx_bibblock">Vigc: Visual instruction generation and correction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 5309–5317.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024b.

</span>
<span class="ltx_bibblock">Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Multimedia Modeling</em>, pages 32–45. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.04408" title="">RECOMP: improving retrieval-augmented lms with compression and selective augmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/2310.04408.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023a)</span>
<span class="ltx_bibblock">
Jingyuan Yang, Qirui Huang, Tingting Ding, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV51070.2023.01864" title="">Emoset: A large-scale visual emotion dataset with rich attributes</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</em>, pages 20326–20337. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023b)</span>
<span class="ltx_bibblock">
Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.793" title="">Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 11844–11857. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al. (2023)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/yasunaga23a.html" title="">Retrieval-augmented multimodal language modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">Proceedings of Machine Learning Research</em>, pages 39755–39769. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.04257" title="">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">CoRR</em>, abs/2311.04257.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. 2023.

</span>
<span class="ltx_bibblock">Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2312.00849</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, and Bryan Hooi. 2024.

</span>
<span class="ltx_bibblock">Avoiding feature suppression in contrastive learning: Learning what has not been learned before.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2402.11816</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">Bertscore: Evaluating text generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.07915" title="">MMICL: empowering vision-language model with multi-modal in-context learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2309.07915.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. 2024.

</span>
<span class="ltx_bibblock">Mitigating object hallucination in large vision-language models via classifier-free guidance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2402.08680</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Yucheng Zhou, Xiang Li, Qianning Wang, and Jianbing Shen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.findings-acl.940" title="">Visual in-context learning for large vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024</em>, pages 15890–15902. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.10592" title="">Minigpt-4: Enhancing vision-language understanding with advanced large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">CoRR</em>, abs/2304.10592.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Algorithm</h3>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Robust RAG Training Framework</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>Input question <math alttext="q" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_q</annotation></semantics></math> and image <math alttext="i_{\text{test}}" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><msub id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">i</mi><mtext id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3a.cmml">test</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">𝑖</ci><ci id="alg1.l1.m2.1.1.3a.cmml" xref="alg1.l1.m2.1.1.3"><mtext id="alg1.l1.m2.1.1.3.cmml" mathsize="70%" xref="alg1.l1.m2.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">i_{\text{test}}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_i start_POSTSUBSCRIPT test end_POSTSUBSCRIPT</annotation></semantics></math>, Image-Caption collection <math alttext="\mathbb{D}" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">𝔻</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">𝔻</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\mathbb{D}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">blackboard_D</annotation></semantics></math>, Evaluate Tools <math alttext="T" class="ltx_Math" display="inline" id="alg1.l1.m4.1"><semantics id="alg1.l1.m4.1a"><mi id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m4.1d">italic_T</annotation></semantics></math>, Vision Encoder <math alttext="p_{V}^{\theta}" class="ltx_Math" display="inline" id="alg1.l1.m5.1"><semantics id="alg1.l1.m5.1a"><msubsup id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml"><mi id="alg1.l1.m5.1.1.2.2" xref="alg1.l1.m5.1.1.2.2.cmml">p</mi><mi id="alg1.l1.m5.1.1.2.3" xref="alg1.l1.m5.1.1.2.3.cmml">V</mi><mi id="alg1.l1.m5.1.1.3" xref="alg1.l1.m5.1.1.3.cmml">θ</mi></msubsup><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><apply id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1"><csymbol cd="ambiguous" id="alg1.l1.m5.1.1.1.cmml" xref="alg1.l1.m5.1.1">superscript</csymbol><apply id="alg1.l1.m5.1.1.2.cmml" xref="alg1.l1.m5.1.1"><csymbol cd="ambiguous" id="alg1.l1.m5.1.1.2.1.cmml" xref="alg1.l1.m5.1.1">subscript</csymbol><ci id="alg1.l1.m5.1.1.2.2.cmml" xref="alg1.l1.m5.1.1.2.2">𝑝</ci><ci id="alg1.l1.m5.1.1.2.3.cmml" xref="alg1.l1.m5.1.1.2.3">𝑉</ci></apply><ci id="alg1.l1.m5.1.1.3.cmml" xref="alg1.l1.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">p_{V}^{\theta}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m5.1d">italic_p start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT</annotation></semantics></math>, LVLMs <math alttext="M_{\theta}" class="ltx_Math" display="inline" id="alg1.l1.m6.1"><semantics id="alg1.l1.m6.1a"><msub id="alg1.l1.m6.1.1" xref="alg1.l1.m6.1.1.cmml"><mi id="alg1.l1.m6.1.1.2" xref="alg1.l1.m6.1.1.2.cmml">M</mi><mi id="alg1.l1.m6.1.1.3" xref="alg1.l1.m6.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m6.1b"><apply id="alg1.l1.m6.1.1.cmml" xref="alg1.l1.m6.1.1"><csymbol cd="ambiguous" id="alg1.l1.m6.1.1.1.cmml" xref="alg1.l1.m6.1.1">subscript</csymbol><ci id="alg1.l1.m6.1.1.2.cmml" xref="alg1.l1.m6.1.1.2">𝑀</ci><ci id="alg1.l1.m6.1.1.3.cmml" xref="alg1.l1.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m6.1c">M_{\theta}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m6.1d">italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, SFT data collection <math alttext="C" class="ltx_Math" display="inline" id="alg1.l1.m7.1"><semantics id="alg1.l1.m7.1a"><mi id="alg1.l1.m7.1.1" xref="alg1.l1.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m7.1b"><ci id="alg1.l1.m7.1.1.cmml" xref="alg1.l1.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m7.1c">C</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m7.1d">italic_C</annotation></semantics></math>, Training data set <math alttext="S" class="ltx_Math" display="inline" id="alg1.l1.m8.1"><semantics id="alg1.l1.m8.1a"><mi id="alg1.l1.m8.1.1" xref="alg1.l1.m8.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m8.1b"><ci id="alg1.l1.m8.1.1.cmml" xref="alg1.l1.m8.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m8.1c">S</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m8.1d">italic_S</annotation></semantics></math>, Positive set <math alttext="\mathbb{C}_{\text{pos}}" class="ltx_Math" display="inline" id="alg1.l1.m9.1"><semantics id="alg1.l1.m9.1a"><msub id="alg1.l1.m9.1.1" xref="alg1.l1.m9.1.1.cmml"><mi id="alg1.l1.m9.1.1.2" xref="alg1.l1.m9.1.1.2.cmml">ℂ</mi><mtext id="alg1.l1.m9.1.1.3" xref="alg1.l1.m9.1.1.3a.cmml">pos</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m9.1b"><apply id="alg1.l1.m9.1.1.cmml" xref="alg1.l1.m9.1.1"><csymbol cd="ambiguous" id="alg1.l1.m9.1.1.1.cmml" xref="alg1.l1.m9.1.1">subscript</csymbol><ci id="alg1.l1.m9.1.1.2.cmml" xref="alg1.l1.m9.1.1.2">ℂ</ci><ci id="alg1.l1.m9.1.1.3a.cmml" xref="alg1.l1.m9.1.1.3"><mtext id="alg1.l1.m9.1.1.3.cmml" mathsize="70%" xref="alg1.l1.m9.1.1.3">pos</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m9.1c">\mathbb{C}_{\text{pos}}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m9.1d">blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT</annotation></semantics></math>, Negative set <math alttext="\mathbb{C}_{\text{neg}}" class="ltx_Math" display="inline" id="alg1.l1.m10.1"><semantics id="alg1.l1.m10.1a"><msub id="alg1.l1.m10.1.1" xref="alg1.l1.m10.1.1.cmml"><mi id="alg1.l1.m10.1.1.2" xref="alg1.l1.m10.1.1.2.cmml">ℂ</mi><mtext id="alg1.l1.m10.1.1.3" xref="alg1.l1.m10.1.1.3a.cmml">neg</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m10.1b"><apply id="alg1.l1.m10.1.1.cmml" xref="alg1.l1.m10.1.1"><csymbol cd="ambiguous" id="alg1.l1.m10.1.1.1.cmml" xref="alg1.l1.m10.1.1">subscript</csymbol><ci id="alg1.l1.m10.1.1.2.cmml" xref="alg1.l1.m10.1.1.2">ℂ</ci><ci id="alg1.l1.m10.1.1.3a.cmml" xref="alg1.l1.m10.1.1.3"><mtext id="alg1.l1.m10.1.1.3.cmml" mathsize="70%" xref="alg1.l1.m10.1.1.3">neg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m10.1c">\mathbb{C}_{\text{neg}}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m10.1d">blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><math alttext="S\leftarrow[]" class="ltx_Math" display="inline" id="alg1.l2.m1.1"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">S</mi><mo id="alg1.l2.m1.1.1.1" stretchy="false" xref="alg1.l2.m1.1.1.1.cmml">←</mo><mrow id="alg1.l2.m1.1.1.3.2" xref="alg1.l2.m1.1.1.cmml"><mo id="alg1.l2.m1.1.1.3.2.1" stretchy="false" xref="alg1.l2.m1.1.1.3.1.cmml">[</mo><mo id="alg1.l2.m1.1.1.3.2.2" stretchy="false" xref="alg1.l2.m1.1.1.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><ci id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1">←</ci><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝑆</ci><list id="alg1.l2.m1.1.1.3.1.cmml" xref="alg1.l2.m1.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">S\leftarrow[]</annotation><annotation encoding="application/x-llamapun" id="alg1.l2.m1.1d">italic_S ← [ ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><span class="ltx_text ltx_font_bold" id="alg1.l3.1">for</span> each instruction <math alttext="x" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_x</annotation></semantics></math> in <math alttext="C" class="ltx_Math" display="inline" id="alg1.l3.m2.1"><semantics id="alg1.l3.m2.1a"><mi id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m2.1d">italic_C</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l3.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>     response <math alttext="\leftarrow M_{\theta}(x)" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.2" xref="alg1.l4.m1.1.2.cmml"><mi id="alg1.l4.m1.1.2.2" xref="alg1.l4.m1.1.2.2.cmml"></mi><mo id="alg1.l4.m1.1.2.1" stretchy="false" xref="alg1.l4.m1.1.2.1.cmml">←</mo><mrow id="alg1.l4.m1.1.2.3" xref="alg1.l4.m1.1.2.3.cmml"><msub id="alg1.l4.m1.1.2.3.2" xref="alg1.l4.m1.1.2.3.2.cmml"><mi id="alg1.l4.m1.1.2.3.2.2" xref="alg1.l4.m1.1.2.3.2.2.cmml">M</mi><mi id="alg1.l4.m1.1.2.3.2.3" xref="alg1.l4.m1.1.2.3.2.3.cmml">θ</mi></msub><mo id="alg1.l4.m1.1.2.3.1" xref="alg1.l4.m1.1.2.3.1.cmml">⁢</mo><mrow id="alg1.l4.m1.1.2.3.3.2" xref="alg1.l4.m1.1.2.3.cmml"><mo id="alg1.l4.m1.1.2.3.3.2.1" stretchy="false" xref="alg1.l4.m1.1.2.3.cmml">(</mo><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">x</mi><mo id="alg1.l4.m1.1.2.3.3.2.2" stretchy="false" xref="alg1.l4.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.2.cmml" xref="alg1.l4.m1.1.2"><ci id="alg1.l4.m1.1.2.1.cmml" xref="alg1.l4.m1.1.2.1">←</ci><csymbol cd="latexml" id="alg1.l4.m1.1.2.2.cmml" xref="alg1.l4.m1.1.2.2">absent</csymbol><apply id="alg1.l4.m1.1.2.3.cmml" xref="alg1.l4.m1.1.2.3"><times id="alg1.l4.m1.1.2.3.1.cmml" xref="alg1.l4.m1.1.2.3.1"></times><apply id="alg1.l4.m1.1.2.3.2.cmml" xref="alg1.l4.m1.1.2.3.2"><csymbol cd="ambiguous" id="alg1.l4.m1.1.2.3.2.1.cmml" xref="alg1.l4.m1.1.2.3.2">subscript</csymbol><ci id="alg1.l4.m1.1.2.3.2.2.cmml" xref="alg1.l4.m1.1.2.3.2.2">𝑀</ci><ci id="alg1.l4.m1.1.2.3.2.3.cmml" xref="alg1.l4.m1.1.2.3.2.3">𝜃</ci></apply><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\leftarrow M_{\theta}(x)</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">← italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     state <math alttext="\leftarrow T" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml"></mi><mo id="alg1.l5.m1.1.1.1" stretchy="false" xref="alg1.l5.m1.1.1.1.cmml">←</mo><mi id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">←</ci><csymbol cd="latexml" id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">absent</csymbol><ci id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">\leftarrow T</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">← italic_T</annotation></semantics></math>(response)

</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l6.1">if</span> not state <span class="ltx_text ltx_font_bold" id="alg1.l6.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>         <math alttext="S\leftarrow S\cup\{x\}" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.2" xref="alg1.l7.m1.1.2.cmml"><mi id="alg1.l7.m1.1.2.2" xref="alg1.l7.m1.1.2.2.cmml">S</mi><mo id="alg1.l7.m1.1.2.1" stretchy="false" xref="alg1.l7.m1.1.2.1.cmml">←</mo><mrow id="alg1.l7.m1.1.2.3" xref="alg1.l7.m1.1.2.3.cmml"><mi id="alg1.l7.m1.1.2.3.2" xref="alg1.l7.m1.1.2.3.2.cmml">S</mi><mo id="alg1.l7.m1.1.2.3.1" xref="alg1.l7.m1.1.2.3.1.cmml">∪</mo><mrow id="alg1.l7.m1.1.2.3.3.2" xref="alg1.l7.m1.1.2.3.3.1.cmml"><mo id="alg1.l7.m1.1.2.3.3.2.1" stretchy="false" xref="alg1.l7.m1.1.2.3.3.1.cmml">{</mo><mi id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">x</mi><mo id="alg1.l7.m1.1.2.3.3.2.2" stretchy="false" xref="alg1.l7.m1.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.2.cmml" xref="alg1.l7.m1.1.2"><ci id="alg1.l7.m1.1.2.1.cmml" xref="alg1.l7.m1.1.2.1">←</ci><ci id="alg1.l7.m1.1.2.2.cmml" xref="alg1.l7.m1.1.2.2">𝑆</ci><apply id="alg1.l7.m1.1.2.3.cmml" xref="alg1.l7.m1.1.2.3"><union id="alg1.l7.m1.1.2.3.1.cmml" xref="alg1.l7.m1.1.2.3.1"></union><ci id="alg1.l7.m1.1.2.3.2.cmml" xref="alg1.l7.m1.1.2.3.2">𝑆</ci><set id="alg1.l7.m1.1.2.3.3.1.cmml" xref="alg1.l7.m1.1.2.3.3.2"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">𝑥</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">S\leftarrow S\cup\{x\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">italic_S ← italic_S ∪ { italic_x }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l8.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l8.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span><span class="ltx_text ltx_font_bold" id="alg1.l9.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l9.2">for</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span><span class="ltx_text ltx_font_bold" id="alg1.l10.1">for</span> each instruction <math alttext="x" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_x</annotation></semantics></math> in <math alttext="S" class="ltx_Math" display="inline" id="alg1.l10.m2.1"><semantics id="alg1.l10.m2.1a"><mi id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><ci id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m2.1d">italic_S</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l10.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>     <math alttext="[i,c]\leftarrow" class="ltx_Math" display="inline" id="alg1.l11.m1.2"><semantics id="alg1.l11.m1.2a"><mrow id="alg1.l11.m1.2.3" xref="alg1.l11.m1.2.3.cmml"><mrow id="alg1.l11.m1.2.3.2.2" xref="alg1.l11.m1.2.3.2.1.cmml"><mo id="alg1.l11.m1.2.3.2.2.1" stretchy="false" xref="alg1.l11.m1.2.3.2.1.cmml">[</mo><mi id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">i</mi><mo id="alg1.l11.m1.2.3.2.2.2" xref="alg1.l11.m1.2.3.2.1.cmml">,</mo><mi id="alg1.l11.m1.2.2" xref="alg1.l11.m1.2.2.cmml">c</mi><mo id="alg1.l11.m1.2.3.2.2.3" stretchy="false" xref="alg1.l11.m1.2.3.2.1.cmml">]</mo></mrow><mo id="alg1.l11.m1.2.3.1" stretchy="false" xref="alg1.l11.m1.2.3.1.cmml">←</mo><mi id="alg1.l11.m1.2.3.3" xref="alg1.l11.m1.2.3.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.2b"><apply id="alg1.l11.m1.2.3.cmml" xref="alg1.l11.m1.2.3"><ci id="alg1.l11.m1.2.3.1.cmml" xref="alg1.l11.m1.2.3.1">←</ci><interval closure="closed" id="alg1.l11.m1.2.3.2.1.cmml" xref="alg1.l11.m1.2.3.2.2"><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">𝑖</ci><ci id="alg1.l11.m1.2.2.cmml" xref="alg1.l11.m1.2.2">𝑐</ci></interval><csymbol cd="latexml" id="alg1.l11.m1.2.3.3.cmml" xref="alg1.l11.m1.2.3.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.2c">[i,c]\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.2d">[ italic_i , italic_c ] ←</annotation></semantics></math> Retrieval from <math alttext="\mathbb{D}" class="ltx_Math" display="inline" id="alg1.l11.m2.1"><semantics id="alg1.l11.m2.1a"><mi id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml">𝔻</mi><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><ci id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">𝔻</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">\mathbb{D}</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m2.1d">blackboard_D</annotation></semantics></math> with query <math alttext="i_{\text{test}}" class="ltx_Math" display="inline" id="alg1.l11.m3.1"><semantics id="alg1.l11.m3.1a"><msub id="alg1.l11.m3.1.1" xref="alg1.l11.m3.1.1.cmml"><mi id="alg1.l11.m3.1.1.2" xref="alg1.l11.m3.1.1.2.cmml">i</mi><mtext id="alg1.l11.m3.1.1.3" xref="alg1.l11.m3.1.1.3a.cmml">test</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l11.m3.1b"><apply id="alg1.l11.m3.1.1.cmml" xref="alg1.l11.m3.1.1"><csymbol cd="ambiguous" id="alg1.l11.m3.1.1.1.cmml" xref="alg1.l11.m3.1.1">subscript</csymbol><ci id="alg1.l11.m3.1.1.2.cmml" xref="alg1.l11.m3.1.1.2">𝑖</ci><ci id="alg1.l11.m3.1.1.3a.cmml" xref="alg1.l11.m3.1.1.3"><mtext id="alg1.l11.m3.1.1.3.cmml" mathsize="70%" xref="alg1.l11.m3.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m3.1c">i_{\text{test}}</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m3.1d">italic_i start_POSTSUBSCRIPT test end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>     response <math alttext="\leftarrow M_{\theta}(x,[i,c])" class="ltx_Math" display="inline" id="alg1.l12.m1.4"><semantics id="alg1.l12.m1.4a"><mrow id="alg1.l12.m1.4.4" xref="alg1.l12.m1.4.4.cmml"><mi id="alg1.l12.m1.4.4.3" xref="alg1.l12.m1.4.4.3.cmml"></mi><mo id="alg1.l12.m1.4.4.2" stretchy="false" xref="alg1.l12.m1.4.4.2.cmml">←</mo><mrow id="alg1.l12.m1.4.4.1" xref="alg1.l12.m1.4.4.1.cmml"><msub id="alg1.l12.m1.4.4.1.3" xref="alg1.l12.m1.4.4.1.3.cmml"><mi id="alg1.l12.m1.4.4.1.3.2" xref="alg1.l12.m1.4.4.1.3.2.cmml">M</mi><mi id="alg1.l12.m1.4.4.1.3.3" xref="alg1.l12.m1.4.4.1.3.3.cmml">θ</mi></msub><mo id="alg1.l12.m1.4.4.1.2" xref="alg1.l12.m1.4.4.1.2.cmml">⁢</mo><mrow id="alg1.l12.m1.4.4.1.1.1" xref="alg1.l12.m1.4.4.1.1.2.cmml"><mo id="alg1.l12.m1.4.4.1.1.1.2" stretchy="false" xref="alg1.l12.m1.4.4.1.1.2.cmml">(</mo><mi id="alg1.l12.m1.3.3" xref="alg1.l12.m1.3.3.cmml">x</mi><mo id="alg1.l12.m1.4.4.1.1.1.3" xref="alg1.l12.m1.4.4.1.1.2.cmml">,</mo><mrow id="alg1.l12.m1.4.4.1.1.1.1.2" xref="alg1.l12.m1.4.4.1.1.1.1.1.cmml"><mo id="alg1.l12.m1.4.4.1.1.1.1.2.1" stretchy="false" xref="alg1.l12.m1.4.4.1.1.1.1.1.cmml">[</mo><mi id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml">i</mi><mo id="alg1.l12.m1.4.4.1.1.1.1.2.2" xref="alg1.l12.m1.4.4.1.1.1.1.1.cmml">,</mo><mi id="alg1.l12.m1.2.2" xref="alg1.l12.m1.2.2.cmml">c</mi><mo id="alg1.l12.m1.4.4.1.1.1.1.2.3" stretchy="false" xref="alg1.l12.m1.4.4.1.1.1.1.1.cmml">]</mo></mrow><mo id="alg1.l12.m1.4.4.1.1.1.4" stretchy="false" xref="alg1.l12.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.4b"><apply id="alg1.l12.m1.4.4.cmml" xref="alg1.l12.m1.4.4"><ci id="alg1.l12.m1.4.4.2.cmml" xref="alg1.l12.m1.4.4.2">←</ci><csymbol cd="latexml" id="alg1.l12.m1.4.4.3.cmml" xref="alg1.l12.m1.4.4.3">absent</csymbol><apply id="alg1.l12.m1.4.4.1.cmml" xref="alg1.l12.m1.4.4.1"><times id="alg1.l12.m1.4.4.1.2.cmml" xref="alg1.l12.m1.4.4.1.2"></times><apply id="alg1.l12.m1.4.4.1.3.cmml" xref="alg1.l12.m1.4.4.1.3"><csymbol cd="ambiguous" id="alg1.l12.m1.4.4.1.3.1.cmml" xref="alg1.l12.m1.4.4.1.3">subscript</csymbol><ci id="alg1.l12.m1.4.4.1.3.2.cmml" xref="alg1.l12.m1.4.4.1.3.2">𝑀</ci><ci id="alg1.l12.m1.4.4.1.3.3.cmml" xref="alg1.l12.m1.4.4.1.3.3">𝜃</ci></apply><interval closure="open" id="alg1.l12.m1.4.4.1.1.2.cmml" xref="alg1.l12.m1.4.4.1.1.1"><ci id="alg1.l12.m1.3.3.cmml" xref="alg1.l12.m1.3.3">𝑥</ci><interval closure="closed" id="alg1.l12.m1.4.4.1.1.1.1.1.cmml" xref="alg1.l12.m1.4.4.1.1.1.1.2"><ci id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1">𝑖</ci><ci id="alg1.l12.m1.2.2.cmml" xref="alg1.l12.m1.2.2">𝑐</ci></interval></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.4c">\leftarrow M_{\theta}(x,[i,c])</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m1.4d">← italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x , [ italic_i , italic_c ] )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>     state <math alttext="\leftarrow T" class="ltx_Math" display="inline" id="alg1.l13.m1.1"><semantics id="alg1.l13.m1.1a"><mrow id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml"><mi id="alg1.l13.m1.1.1.2" xref="alg1.l13.m1.1.1.2.cmml"></mi><mo id="alg1.l13.m1.1.1.1" stretchy="false" xref="alg1.l13.m1.1.1.1.cmml">←</mo><mi id="alg1.l13.m1.1.1.3" xref="alg1.l13.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><apply id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"><ci id="alg1.l13.m1.1.1.1.cmml" xref="alg1.l13.m1.1.1.1">←</ci><csymbol cd="latexml" id="alg1.l13.m1.1.1.2.cmml" xref="alg1.l13.m1.1.1.2">absent</csymbol><ci id="alg1.l13.m1.1.1.3.cmml" xref="alg1.l13.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">\leftarrow T</annotation><annotation encoding="application/x-llamapun" id="alg1.l13.m1.1d">← italic_T</annotation></semantics></math>(response)

</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l14.1">if</span> state <span class="ltx_text ltx_font_bold" id="alg1.l14.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>         <math alttext="C_{\text{pos}}\leftarrow\mathbb{C}_{\text{pos}}\cup\{(x," class="ltx_math_unparsed" display="inline" id="alg1.l15.m1.1"><semantics id="alg1.l15.m1.1a"><mrow id="alg1.l15.m1.1b"><msub id="alg1.l15.m1.1.1"><mi id="alg1.l15.m1.1.1.2">C</mi><mtext id="alg1.l15.m1.1.1.3">pos</mtext></msub><mo id="alg1.l15.m1.1.2" stretchy="false">←</mo><msub id="alg1.l15.m1.1.3"><mi id="alg1.l15.m1.1.3.2">ℂ</mi><mtext id="alg1.l15.m1.1.3.3">pos</mtext></msub><mo id="alg1.l15.m1.1.4">∪</mo><mrow id="alg1.l15.m1.1.5"><mo id="alg1.l15.m1.1.5.1" stretchy="false">{</mo><mrow id="alg1.l15.m1.1.5.2"><mo id="alg1.l15.m1.1.5.2.1" stretchy="false">(</mo><mi id="alg1.l15.m1.1.5.2.2">x</mi><mo id="alg1.l15.m1.1.5.2.3">,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">C_{\text{pos}}\leftarrow\mathbb{C}_{\text{pos}}\cup\{(x,</annotation><annotation encoding="application/x-llamapun" id="alg1.l15.m1.1d">italic_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT ← blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT ∪ { ( italic_x ,</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span>              <math alttext="[\max_{i_{j}\in\mathbb{C}_{\text{pos}}}p_{V}^{\theta}(x,i_{j}),c_{j}])\}" class="ltx_math_unparsed" display="inline" id="alg1.l16.m1.1"><semantics id="alg1.l16.m1.1a"><mrow id="alg1.l16.m1.1b"><mrow id="alg1.l16.m1.1.2"><mrow id="alg1.l16.m1.1.2.1"><mo id="alg1.l16.m1.1.2.1.1" stretchy="false">[</mo><msub id="alg1.l16.m1.1.2.1.2"><mi id="alg1.l16.m1.1.2.1.2.2">max</mi><mrow id="alg1.l16.m1.1.2.1.2.3"><msub id="alg1.l16.m1.1.2.1.2.3.2"><mi id="alg1.l16.m1.1.2.1.2.3.2.2">i</mi><mi id="alg1.l16.m1.1.2.1.2.3.2.3">j</mi></msub><mo id="alg1.l16.m1.1.2.1.2.3.1">∈</mo><msub id="alg1.l16.m1.1.2.1.2.3.3"><mi id="alg1.l16.m1.1.2.1.2.3.3.2">ℂ</mi><mtext id="alg1.l16.m1.1.2.1.2.3.3.3">pos</mtext></msub></mrow></msub><msubsup id="alg1.l16.m1.1.2.1.3"><mi id="alg1.l16.m1.1.2.1.3.2.2">p</mi><mi id="alg1.l16.m1.1.2.1.3.2.3">V</mi><mi id="alg1.l16.m1.1.2.1.3.3">θ</mi></msubsup><mrow id="alg1.l16.m1.1.2.1.4"><mo id="alg1.l16.m1.1.2.1.4.1" stretchy="false">(</mo><mi id="alg1.l16.m1.1.1">x</mi><mo id="alg1.l16.m1.1.2.1.4.2">,</mo><msub id="alg1.l16.m1.1.2.1.4.3"><mi id="alg1.l16.m1.1.2.1.4.3.2">i</mi><mi id="alg1.l16.m1.1.2.1.4.3.3">j</mi></msub><mo id="alg1.l16.m1.1.2.1.4.4" stretchy="false">)</mo></mrow><mo id="alg1.l16.m1.1.2.1.5">,</mo><msub id="alg1.l16.m1.1.2.1.6"><mi id="alg1.l16.m1.1.2.1.6.2">c</mi><mi id="alg1.l16.m1.1.2.1.6.3">j</mi></msub><mo id="alg1.l16.m1.1.2.1.7" stretchy="false">]</mo></mrow><mo id="alg1.l16.m1.1.2.2" stretchy="false">)</mo></mrow><mo id="alg1.l16.m1.1.3" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="alg1.l16.m1.1c">[\max_{i_{j}\in\mathbb{C}_{\text{pos}}}p_{V}^{\theta}(x,i_{j}),c_{j}])\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l16.m1.1d">[ roman_max start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_x , italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l17.1">else</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span>         <math alttext="C_{\text{neg}}\leftarrow\mathbb{C}_{\text{neg}}\cup\{(x," class="ltx_math_unparsed" display="inline" id="alg1.l18.m1.1"><semantics id="alg1.l18.m1.1a"><mrow id="alg1.l18.m1.1b"><msub id="alg1.l18.m1.1.1"><mi id="alg1.l18.m1.1.1.2">C</mi><mtext id="alg1.l18.m1.1.1.3">neg</mtext></msub><mo id="alg1.l18.m1.1.2" stretchy="false">←</mo><msub id="alg1.l18.m1.1.3"><mi id="alg1.l18.m1.1.3.2">ℂ</mi><mtext id="alg1.l18.m1.1.3.3">neg</mtext></msub><mo id="alg1.l18.m1.1.4">∪</mo><mrow id="alg1.l18.m1.1.5"><mo id="alg1.l18.m1.1.5.1" stretchy="false">{</mo><mrow id="alg1.l18.m1.1.5.2"><mo id="alg1.l18.m1.1.5.2.1" stretchy="false">(</mo><mi id="alg1.l18.m1.1.5.2.2">x</mi><mo id="alg1.l18.m1.1.5.2.3">,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">C_{\text{neg}}\leftarrow\mathbb{C}_{\text{neg}}\cup\{(x,</annotation><annotation encoding="application/x-llamapun" id="alg1.l18.m1.1d">italic_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT ← blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT ∪ { ( italic_x ,</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span>              <math alttext="[\max_{i_{j}\in\mathbb{C}_{\text{neg}}}p_{V}^{\theta}(x,i_{j}),c_{j}])\}" class="ltx_math_unparsed" display="inline" id="alg1.l19.m1.1"><semantics id="alg1.l19.m1.1a"><mrow id="alg1.l19.m1.1b"><mrow id="alg1.l19.m1.1.2"><mrow id="alg1.l19.m1.1.2.1"><mo id="alg1.l19.m1.1.2.1.1" stretchy="false">[</mo><msub id="alg1.l19.m1.1.2.1.2"><mi id="alg1.l19.m1.1.2.1.2.2">max</mi><mrow id="alg1.l19.m1.1.2.1.2.3"><msub id="alg1.l19.m1.1.2.1.2.3.2"><mi id="alg1.l19.m1.1.2.1.2.3.2.2">i</mi><mi id="alg1.l19.m1.1.2.1.2.3.2.3">j</mi></msub><mo id="alg1.l19.m1.1.2.1.2.3.1">∈</mo><msub id="alg1.l19.m1.1.2.1.2.3.3"><mi id="alg1.l19.m1.1.2.1.2.3.3.2">ℂ</mi><mtext id="alg1.l19.m1.1.2.1.2.3.3.3">neg</mtext></msub></mrow></msub><msubsup id="alg1.l19.m1.1.2.1.3"><mi id="alg1.l19.m1.1.2.1.3.2.2">p</mi><mi id="alg1.l19.m1.1.2.1.3.2.3">V</mi><mi id="alg1.l19.m1.1.2.1.3.3">θ</mi></msubsup><mrow id="alg1.l19.m1.1.2.1.4"><mo id="alg1.l19.m1.1.2.1.4.1" stretchy="false">(</mo><mi id="alg1.l19.m1.1.1">x</mi><mo id="alg1.l19.m1.1.2.1.4.2">,</mo><msub id="alg1.l19.m1.1.2.1.4.3"><mi id="alg1.l19.m1.1.2.1.4.3.2">i</mi><mi id="alg1.l19.m1.1.2.1.4.3.3">j</mi></msub><mo id="alg1.l19.m1.1.2.1.4.4" stretchy="false">)</mo></mrow><mo id="alg1.l19.m1.1.2.1.5">,</mo><msub id="alg1.l19.m1.1.2.1.6"><mi id="alg1.l19.m1.1.2.1.6.2">c</mi><mi id="alg1.l19.m1.1.2.1.6.3">j</mi></msub><mo id="alg1.l19.m1.1.2.1.7" stretchy="false">]</mo></mrow><mo id="alg1.l19.m1.1.2.2" stretchy="false">)</mo></mrow><mo id="alg1.l19.m1.1.3" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="alg1.l19.m1.1c">[\max_{i_{j}\in\mathbb{C}_{\text{neg}}}p_{V}^{\theta}(x,i_{j}),c_{j}])\}</annotation><annotation encoding="application/x-llamapun" id="alg1.l19.m1.1d">[ roman_max start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT ( italic_x , italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) }</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l20.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l20.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline">21:</span><span class="ltx_text ltx_font_bold" id="alg1.l21.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l21.2">for</span>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline">22:</span><math alttext="S\leftarrow[\mathbb{C}_{\text{pos}},\mathbb{C}_{\text{neg}}]" class="ltx_Math" display="inline" id="alg1.l22.m1.2"><semantics id="alg1.l22.m1.2a"><mrow id="alg1.l22.m1.2.2" xref="alg1.l22.m1.2.2.cmml"><mi id="alg1.l22.m1.2.2.4" xref="alg1.l22.m1.2.2.4.cmml">S</mi><mo id="alg1.l22.m1.2.2.3" stretchy="false" xref="alg1.l22.m1.2.2.3.cmml">←</mo><mrow id="alg1.l22.m1.2.2.2.2" xref="alg1.l22.m1.2.2.2.3.cmml"><mo id="alg1.l22.m1.2.2.2.2.3" stretchy="false" xref="alg1.l22.m1.2.2.2.3.cmml">[</mo><msub id="alg1.l22.m1.1.1.1.1.1" xref="alg1.l22.m1.1.1.1.1.1.cmml"><mi id="alg1.l22.m1.1.1.1.1.1.2" xref="alg1.l22.m1.1.1.1.1.1.2.cmml">ℂ</mi><mtext id="alg1.l22.m1.1.1.1.1.1.3" xref="alg1.l22.m1.1.1.1.1.1.3a.cmml">pos</mtext></msub><mo id="alg1.l22.m1.2.2.2.2.4" xref="alg1.l22.m1.2.2.2.3.cmml">,</mo><msub id="alg1.l22.m1.2.2.2.2.2" xref="alg1.l22.m1.2.2.2.2.2.cmml"><mi id="alg1.l22.m1.2.2.2.2.2.2" xref="alg1.l22.m1.2.2.2.2.2.2.cmml">ℂ</mi><mtext id="alg1.l22.m1.2.2.2.2.2.3" xref="alg1.l22.m1.2.2.2.2.2.3a.cmml">neg</mtext></msub><mo id="alg1.l22.m1.2.2.2.2.5" stretchy="false" xref="alg1.l22.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.2b"><apply id="alg1.l22.m1.2.2.cmml" xref="alg1.l22.m1.2.2"><ci id="alg1.l22.m1.2.2.3.cmml" xref="alg1.l22.m1.2.2.3">←</ci><ci id="alg1.l22.m1.2.2.4.cmml" xref="alg1.l22.m1.2.2.4">𝑆</ci><interval closure="closed" id="alg1.l22.m1.2.2.2.3.cmml" xref="alg1.l22.m1.2.2.2.2"><apply id="alg1.l22.m1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l22.m1.1.1.1.1.1.2.cmml" xref="alg1.l22.m1.1.1.1.1.1.2">ℂ</ci><ci id="alg1.l22.m1.1.1.1.1.1.3a.cmml" xref="alg1.l22.m1.1.1.1.1.1.3"><mtext id="alg1.l22.m1.1.1.1.1.1.3.cmml" mathsize="70%" xref="alg1.l22.m1.1.1.1.1.1.3">pos</mtext></ci></apply><apply id="alg1.l22.m1.2.2.2.2.2.cmml" xref="alg1.l22.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l22.m1.2.2.2.2.2.1.cmml" xref="alg1.l22.m1.2.2.2.2.2">subscript</csymbol><ci id="alg1.l22.m1.2.2.2.2.2.2.cmml" xref="alg1.l22.m1.2.2.2.2.2.2">ℂ</ci><ci id="alg1.l22.m1.2.2.2.2.2.3a.cmml" xref="alg1.l22.m1.2.2.2.2.2.3"><mtext id="alg1.l22.m1.2.2.2.2.2.3.cmml" mathsize="70%" xref="alg1.l22.m1.2.2.2.2.2.3">neg</mtext></ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.2c">S\leftarrow[\mathbb{C}_{\text{pos}},\mathbb{C}_{\text{neg}}]</annotation><annotation encoding="application/x-llamapun" id="alg1.l22.m1.2d">italic_S ← [ blackboard_C start_POSTSUBSCRIPT pos end_POSTSUBSCRIPT , blackboard_C start_POSTSUBSCRIPT neg end_POSTSUBSCRIPT ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l23">
<span class="ltx_tag ltx_tag_listingline">23:</span><span class="ltx_text ltx_font_bold" id="alg1.l23.1">while</span> <math alttext="M_{\theta}" class="ltx_Math" display="inline" id="alg1.l23.m1.1"><semantics id="alg1.l23.m1.1a"><msub id="alg1.l23.m1.1.1" xref="alg1.l23.m1.1.1.cmml"><mi id="alg1.l23.m1.1.1.2" xref="alg1.l23.m1.1.1.2.cmml">M</mi><mi id="alg1.l23.m1.1.1.3" xref="alg1.l23.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l23.m1.1b"><apply id="alg1.l23.m1.1.1.cmml" xref="alg1.l23.m1.1.1"><csymbol cd="ambiguous" id="alg1.l23.m1.1.1.1.cmml" xref="alg1.l23.m1.1.1">subscript</csymbol><ci id="alg1.l23.m1.1.1.2.cmml" xref="alg1.l23.m1.1.1.2">𝑀</ci><ci id="alg1.l23.m1.1.1.3.cmml" xref="alg1.l23.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l23.m1.1c">M_{\theta}</annotation><annotation encoding="application/x-llamapun" id="alg1.l23.m1.1d">italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> has not converged <span class="ltx_text ltx_font_bold" id="alg1.l23.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l24">
<span class="ltx_tag ltx_tag_listingline">24:</span>     Update parameters of <math alttext="M_{\theta}" class="ltx_Math" display="inline" id="alg1.l24.m1.1"><semantics id="alg1.l24.m1.1a"><msub id="alg1.l24.m1.1.1" xref="alg1.l24.m1.1.1.cmml"><mi id="alg1.l24.m1.1.1.2" xref="alg1.l24.m1.1.1.2.cmml">M</mi><mi id="alg1.l24.m1.1.1.3" xref="alg1.l24.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l24.m1.1b"><apply id="alg1.l24.m1.1.1.cmml" xref="alg1.l24.m1.1.1"><csymbol cd="ambiguous" id="alg1.l24.m1.1.1.1.cmml" xref="alg1.l24.m1.1.1">subscript</csymbol><ci id="alg1.l24.m1.1.1.2.cmml" xref="alg1.l24.m1.1.1.2">𝑀</ci><ci id="alg1.l24.m1.1.1.3.cmml" xref="alg1.l24.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l24.m1.1c">M_{\theta}</annotation><annotation encoding="application/x-llamapun" id="alg1.l24.m1.1d">italic_M start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> on <math alttext="S" class="ltx_Math" display="inline" id="alg1.l24.m2.1"><semantics id="alg1.l24.m2.1a"><mi id="alg1.l24.m2.1.1" xref="alg1.l24.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l24.m2.1b"><ci id="alg1.l24.m2.1.1.cmml" xref="alg1.l24.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l24.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="alg1.l24.m2.1d">italic_S</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l25">
<span class="ltx_tag ltx_tag_listingline">25:</span><span class="ltx_text ltx_font_bold" id="alg1.l25.1">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l25.2">while</span>
</div>
</div>
</figure>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T5.1" style="width:212.5pt;height:120.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.1pt,11.9pt) scale(0.834238105288724,0.834238105288724) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1.1">Dataset/Benchmark</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.2.1">Answer Type</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.3.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.1.1.2.1.1">POPE</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.1.1.2.1.2">Yes/No</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.1.1.2.1.3">9,000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.3.2.1">MMStar</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.3.2.2">Multiple Choice</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.3.2.3">1,500</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.4.3.1">Vizwiz-VQA</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.4.3.2">Single word or Phrase</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.4.3.3">8,000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.5.4.1">MS-COCO</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.5.4.2">Text</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.5.4.3">5,000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.6.5.1">Vizwiz-Caption</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.6.5.2">Text</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.6.5.3">7,750</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.7.6.1">CIFAR-10</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.7.6.2">Class Name</td>
<td class="ltx_td ltx_align_left" id="A1.T5.1.1.7.6.3">10,000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T5.1.1.8.7.1">EmoSet</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T5.1.1.8.7.2">Class Name</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T5.1.1.8.7.3">800*</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>The statistics of the datasets used in this paper. * denotes we randomly selected 800
samples from EmoSet to constitute the test set.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Data Analysis</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this section, we introduce the datasets used in our experiments. The statistics of these datasets are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.T5" title="Table 5 ‣ A.1 Algorithm ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">POPE</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">POPE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib21" title="">2023c</a>)</cite> offers a method to assess object hallucination in LVLMs by querying if specific objects exist in images. The queries are balanced between existent and non-existent objects (50% each). There are three sampling settings: random, popular, and adversarial. The evaluation pivots on two key metrics: Accuracy and the F1 score.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">MMStar</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">MMStar <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib8" title="">2024</a>)</cite> is an advanced benchmark designed to evaluate the capabilities of LVLMs across multiple dimensions. The benchmark includes 1,500 meticulously selected challenge samples. These samples are initially chosen from existing benchmarks using an automated pipeline, followed by a rigorous human review to ensure high quality.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Vizwiz-VQA</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.1">Vizwiz-VQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib6" title="">2022</a>)</cite> is the task of returning the answer to a question about an image. It has 8,000 test samples with the unique label "Unanswerable."</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">MS-COCO</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px4.p1.1">The MS-COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib25" title="">2014</a>)</cite> dataset is a large-scale dataset for object detection, segmentation, key-point detection, and captioning. We use this dataset only for the image captioning task.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.1" style="width:433.6pt;height:173.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.1pt,21.6pt) scale(0.80044607746292,0.80044607746292) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T6.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T6.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.2.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T6.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.3.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T6.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.4.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A1.T6.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.5.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.6.1">CIFAR-10</span></th>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T6.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.2">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.3">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.4">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.5">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.6">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.7">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.8">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.9">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.10">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.11">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.12">SPICE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.1.2.2.13">Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T6.1.1.3.1.1">Zero-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.2">87.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.3">86.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.4">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.5">84.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.6">84.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.7">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.8">22.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.9">28.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.10">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.11">85.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.12">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.1.13">81.5</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.4.2">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T6.1.1.4.2.1"></th>
<td class="ltx_td ltx_align_center" colspan="12" id="A1.T6.1.1.4.2.2" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.T6.1.1.4.2.2.1" style="background-color:#E6E6E6;">1-shot</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.1.1.5.3.1">Vanilla-RAG</th>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.2">87.7</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.3">86.3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.4">85.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.5">84.1</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.6">82.8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.7">81.9</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.8">22.1</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.9">27.8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.10">50.7</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.11">76.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.12">21.9</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.5.3.13">79.8</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.1.1.6.4.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.6.4.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.2">89.6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.3">89.1</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.4">87.8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.5">87.4</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.6">83.3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.7">83.7</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.8">26.6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.9">29.4</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.10">54.0</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.11">96.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.12">23.7</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.4.13">82.4</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.7.5">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T6.1.1.7.5.1"></th>
<td class="ltx_td ltx_align_center" colspan="12" id="A1.T6.1.1.7.5.2" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.T6.1.1.7.5.2.1" style="background-color:#E6E6E6;">2-shot</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.1.1.8.6.1">Vanilla-RAG</th>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.2">87.9</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.3">86.5</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.4">86.3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.5">85.0</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.6">83.3</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.7">82.5</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.8">24.9</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.9">28.5</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.10">52.8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.11">89.7</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.12">22.6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.6.13">79.5</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.1.1.9.7.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.2.1">89.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.3.1">89.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.4.1">87.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.5.1">87.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.6"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.6.1">83.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.7"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.7.1">83.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.8"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.8.1">27.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.9"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.9.1">29.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.10"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.10.1">55.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.11"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.11.1">101.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.12"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.12.1">24.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.9.7.13"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.9.7.13.1">83.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.10.8">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T6.1.1.10.8.1"></th>
<td class="ltx_td ltx_align_center" colspan="12" id="A1.T6.1.1.10.8.2" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.T6.1.1.10.8.2.1" style="background-color:#E6E6E6;">3-shot</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T6.1.1.11.9.1">Vanilla-RAG</th>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.2">87.5</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.3">86.0</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.4">85.5</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.5">84.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.6">82.6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.7">81.6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.8">23.0</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.9">28.1</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.10">51.4</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.11">79.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.12">22.2</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.11.9.13">79.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T6.1.1.12.10.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.12.10.1.1">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.2">89.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.3">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.4">87.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.5">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.6">83.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.7">83.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.8">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.9">29.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.10">54.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.11">98.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.12">23.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.12.10.13">82.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Number of exemplars.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T7.1" style="width:433.6pt;height:101pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.7pt,12.9pt) scale(0.795550895588669,0.795550895588669) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1.1">
<td class="ltx_td ltx_border_tt" id="A1.T7.1.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.2.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.3.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.4.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A1.T7.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.5.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T7.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.6.1">CIFAR-10</span></th>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.2.2">
<td class="ltx_td" id="A1.T7.1.1.2.2.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.2">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.3">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.4">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.5">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.6">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.7">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.8">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.9">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.10">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.11">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.12">SPICE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T7.1.1.2.2.13">Acc.</th>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.1">Vanilla-RAG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.2">87.9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.3">86.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.4">86.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.5">85.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.6">83.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.7">82.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.8">24.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.9">28.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.10">51.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.11">79.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.12">22.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.3.3.13">79.7</th>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.1">1k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.2">86.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.3">84.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.4">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.5">84.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.6">83.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.7">81.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.8">25.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.9">26.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.10">51.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.11">98.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.12">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.4.4.13">79.2</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.5.5">
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.1">2k</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.2.1">89.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.3.1">89.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.4.1">87.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.5.1">87.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.6"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.6.1">83.6</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.7"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.7.1">83.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.8"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.8.1">27.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.9"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.9.1">29.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.10"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.10.1">55.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.11"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.11.1">101.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.12"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.12.1">24.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.5.5.13"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.5.5.13.1">83.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.6.6">
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.1">3k</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.2">88.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.3">87.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.4">87.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.5">86.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.6">83.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.7">82.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.8">27.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.9">29.4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.10">53.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.11">99.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.12">23.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.6.6.13">81.5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.1">4k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.2">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.3">87.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.4">87.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.5">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.6">82.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.7">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.8">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.9">28.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.10">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.11">96.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.12">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.1.7.7.13">79.6</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Effect of training data size.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Vizwiz-Caption</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px5.p1.1">VizWiz-Caption <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib13" title="">2020</a>)</cite> is a specialized dataset for evaluating and improving image captioning systems, particularly for visually impaired users. It consists of images taken by visually impaired individuals using their smartphones, accompanied by human-generated captions.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">CIFAR-10</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px6.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px6.p1.1">CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib17" title="">2009</a>)</cite> is a well-known benchmark dataset primarily used for evaluating image classification algorithms. The dataset is split into 50,000 training images and 10,000 test images, divided into ten different classes: airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">EmoSet</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px7.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px7.p1.1">EmoSet <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib48" title="">2023a</a>)</cite> comprises 3.3 million images in total, with 118,102 of these images carefully labeled by human annotators, making it five times larger than the largest existing dataset. We randomly sampled 100 instances from each class to serve as our test set.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Metrics</h4>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">Unless otherwise specified, we use exact match as the evaluation metric for VQA and classification tasks. For captioning tasks, we use BLEU-4, ROUGE-L, CIDEr, METEOR, and SPICE as evaluation metrics<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We use the official COCO evaluation toolkit.</span></span></span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional Ablation Study and Experiment Analysis</h3>
<section class="ltx_subsubsection" id="A1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>Sensitivity to the Number of Examplars</h4>
<div class="ltx_para" id="A1.SS3.SSS1.p1">
<p class="ltx_p" id="A1.SS3.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.T6" title="Table 6 ‣ MS-COCO ‣ A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of our model with different numbers of examples. Due to the long captions of ShareGPT-4V, only three examples can fit within a 4096 context window. Our method demonstrates robustness with 1, 2, and 3 examples, indicating adaptability to various numbers of examples. However, the performance peaks with 2 examples and declines with 1 and 3 examples. The decline with 1 example may be due to insufficient information, while 3 examples may introduce excessive irrelevant information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.2 </span>Effect of Training Data Size</h4>
<div class="ltx_para" id="A1.SS3.SSS2.p1">
<p class="ltx_p" id="A1.SS3.SSS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.T7" title="Table 7 ‣ MS-COCO ‣ A.2 Data Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">7</span></a> shows the experiments on the amount of training data. Using only 2k data points, our model is already able to utilize RAG and achieve the best performance fully. Although the performance with 3k and 4k data points is slightly worse than with 2k, the results still surpass those of vanilla-RAG. This indicates that our framework can sufficiently leverage its capabilities using just 2k samples self-generated by the model.</p>
</div>
<figure class="ltx_table" id="A1.T8">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T8.1" style="width:433.6pt;height:70.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.2pt,10.3pt) scale(0.771469433812738,0.771469433812738) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T8.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T8.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.2.1">POPE (R)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T8.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.3.1">POPE (P)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T8.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.4.1">POPE (A)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A1.T8.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.5.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.6.1">CIFAR-10</span></th>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T8.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.2">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.3">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.4">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.5">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.6">Acc.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.7">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.8">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.9">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.10">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.11">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.12">SPICE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T8.1.1.2.2.13">Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T8.1.1.3.1.1">Vanilla-RAG</th>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.2"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.3"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.4"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.5"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.6"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.7"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.8"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.9"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.10"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.11"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.12"></td>
<td class="ltx_td ltx_border_t" id="A1.T8.1.1.3.1.13"></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T8.1.1.4.2.1">w/ image-caption</th>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.2">87.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.3">86.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.4">86.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.5">85.0</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.6">83.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.7">82.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.8">24.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.9">28.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.10">51.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.11">79.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.12">22.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.2.13">79.7</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T8.1.1.5.3.1">w/ caption</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.2">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.3">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.4">85.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.5">84.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.6">83.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.7">82.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.8">24.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.9">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.10">51.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.11">80.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.12">22.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3.13">79.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance of Vanilla-RAG on downstream tasks with different retrieval content.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T9">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T9.1" style="width:433.6pt;height:192.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.7pt,30.0pt) scale(0.762054145797213,0.762054145797213) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T9.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A1.T9.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.2.1">MS-COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5" id="A1.T9.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.3.1">Vizwiz-Caption</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.4.1">Avg.</span></th>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column" id="A1.T9.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.2">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.3">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.4">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.5">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T9.1.1.2.2.6">SPICE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.7">B@4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.8">METEOR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.9">ROUGE-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.1.2.2.10">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A1.T9.1.1.2.2.11">SPICE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="A1.T9.1.1.3.1.1" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.T9.1.1.3.1.1.1" style="background-color:#E6E6E6;">7B Parameter Model</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.4.2">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.4.2.1">Zero-shot</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.2">22.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.3">28.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.4">50.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.5">75.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.4.2.6">22.1</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.7">15.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.8">19.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.9">40.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.10">47.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.4.2.11">11.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2.12">33.51</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.5.3">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.5.3.1">Vanilla-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.2">24.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.3">28.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.4">51.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.5">79.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.5.3.6">22.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.7">21.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.8">21.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.9">45.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.10">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.5.3.11">14.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3.12">37.60</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.6.4">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.6.4.1">Rerank-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.2">24.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.3">28.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.4">52.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.5">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.6.4.6">22.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.7">20.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.8">20.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.9">44.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.10">64.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.6.4.11">13.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4.12">37.42</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.7.5">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.7.5.1">Filter-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.2">26.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.3">29.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.4">54.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.5">97.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.7.5.6">23.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.7">21.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.8">21.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.9">45.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.10">68.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.7.5.11">14.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.7.5.12">40.31</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.8.6">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.8.6.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.1.1">Ours</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.2.1">27.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.3.1">29.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.4.1">55.1</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.5.1">101.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.8.6.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.6.1">24.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.7"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.7.1">22.4</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.8"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.8.1">22.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.9"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.9.1">46.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.10"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.10.1">71.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.8.6.11"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.11.1">15.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.8.6.12"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.6.12.1">43.57</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.9.7">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="A1.T9.1.1.9.7.1" style="background-color:#E6E6E6;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.T9.1.1.9.7.1.1" style="background-color:#E6E6E6;">13B Parameter Model</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.10.8">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.10.8.1">Zero-shot</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.2">22.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.3">28.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.4">51.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.5">85.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.10.8.6">22.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.7">18.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.8">19.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.9">42.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.10">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.10.8.11">12.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.8.12">36.52</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.11.9">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.11.9.1">Vanilla-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.2">24.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.3">28.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.4">52.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.5">89.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.11.9.6">22.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.7">21.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.8">21.3</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.9">45.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.10">59.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.11.9.11">13.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.9.12">37.94</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.12.10">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.12.10.1">Rerank-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.2">25.1</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.3">28.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.4">53.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.5">93.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.12.10.6">22.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.7">21.8</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.8">21.2</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.9">45.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.10">60.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.12.10.11">13.4</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.10.12">38.52</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.13.11">
<td class="ltx_td ltx_align_left" id="A1.T9.1.1.13.11.1">Filter-RAG</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.2">25.5</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.3">28.7</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.4">53.9</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.5">95.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.13.11.6">23.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.7">22.0</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.8"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.13.11.8.1">21.4</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.9">45.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.10">61.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.13.11.11">13.6</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.13.11.12">39.08</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.14.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T9.1.1.14.12.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.1.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.2.1">30.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.3.1">29.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.4.1">56.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.5.1">111.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T9.1.1.14.12.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.6.1">23.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.7"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.7.1">24.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.8">21.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.9"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.9.1">46.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.10"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.10.1">71.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T9.1.1.14.12.11"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.11.1">14.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.14.12.12"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.14.12.12.1">44.83</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Full results of 7B and 13B Robust-LlaVA on MS-COCO and Vizwiz-Caption. The best performance in the table is highlighted in <span class="ltx_text ltx_font_bold" id="A1.T9.3.1">bold</span>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.3 </span>Effect of Different Retrieved Content</h4>
<div class="ltx_para" id="A1.SS3.SSS3.p1">
<p class="ltx_p" id="A1.SS3.SSS3.p1.1">In this section, we explore the performance differences when using image-caption pairs versus using only captions for retrieval across three tasks, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.T8" title="Table 8 ‣ A.3.2 Effect of Training Data Size ‣ A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">8</span></a>. For VQA and classification tasks, using both image and caption yields the best results, as the additional information from the image is beneficial for tasks that require a strong understanding of the image. However, for the captioning task, using only captions performs better since this task requires the model to generate a relevant response based solely on the retrieved descriptions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.4 </span>Detail Results of Captioning Tasks</h4>
<div class="ltx_para" id="A1.SS3.SSS4.p1">
<p class="ltx_p" id="A1.SS3.SSS4.p1.1">In the main table, the metrics for the captioning task are the sum of BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE. We present the detailed results of our 7B and 13B models on MS-COCO and Vizwiz-Caption in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.T9" title="Table 9 ‣ A.3.2 Effect of Training Data Size ‣ A.3 Additional Ablation Study and Experiment Analysis ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.5 </span>Effect of Irrelevant Content</h4>
<div class="ltx_para" id="A1.SS3.SSS5.p1">
<p class="ltx_p" id="A1.SS3.SSS5.p1.1">We also tested Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib5" title="">2023</a>)</cite> and mPLUG-Owl2 <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#bib.bib51" title="">2023</a>)</cite> under three settings (Base, Irrelevant, and RAG) across three tasks. The results are shown in Figures <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:qwen</span> and <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:mplug</span>. It can be observed that irrelevant content has a significant impact on the current LVLMs.</p>
</div>
<figure class="ltx_figure" id="A1.F8.sf1">
</figure>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Case Study</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We present four examples comparing our method with zero-shot and vanilla-RAG, as shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.F8" title="Figure 8 ‣ A.4 Case Study ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.F9" title="Figure 9 ‣ A.4 Case Study ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.F10" title="Figure 10 ‣ A.4 Case Study ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">10</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.14083v1#A1.F11" title="Figure 11 ‣ A.4 Case Study ‣ Appendix A Appendix ‣ SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="628" id="A1.F8.g1" src="x6.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Case for comparing our method with zero-shot and vanilla-RAG.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="628" id="A1.F9.g1" src="x7.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Case for comparing our method with zero-shot and vanilla-RAG.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="628" id="A1.F10.g1" src="x8.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Case for comparing our method with zero-shot and vanilla-RAG.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="628" id="A1.F11.g1" src="x9.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Case for comparing our method with zero-shot and vanilla-RAG.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 09:31:08 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
