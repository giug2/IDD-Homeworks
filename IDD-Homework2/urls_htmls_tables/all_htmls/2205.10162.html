<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.10162] FedAdapter: Efficient Federated Learning for Modern NLP</title><meta property="og:description" content="Transformer-based pre-trained models have revolutionized NLP for superior performance and generality.
Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedAdapter: Efficient Federated Learning for Modern NLP">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedAdapter: Efficient Federated Learning for Modern NLP">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.10162">

<!--Generated on Mon Mar 11 13:45:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning,  Natural Language Processing,  Communication efficiency">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">FedAdapter: Efficient Federated Learning for Modern NLP</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dongqi Cai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">Beiyou Shenzhen Institute</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yaozong Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Beiyou Shenzhen Institute</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shangguang Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Beiyou Shenzhen Institute</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Xiaozhu Lin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">University of Virginia</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mengwei Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">Beiyou Shenzhen Institute</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id2.2" class="ltx_p">Transformer-based pre-trained models have revolutionized NLP for superior performance and generality.
Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the de-facto approach (i.e., FedNLP).
However, our measurements show that FedNLP is prohibitively slow due to the large model sizes and the resultant high network/computation cost.
Towards practical FedNLP, we identify as the key building blocks <span id="id2.2.1" class="ltx_text ltx_font_italic">adapters</span>, small bottleneck modules inserted at a variety of model layers.
A key challenge is to properly configure the depth and width of adapters, to which the training speed and efficiency is highly sensitive.
No silver-bullet configuration exists:
the optimal choice varies across downstream NLP tasks,
desired model accuracy,
and mobile resources.
To automate adapter configuration,
we propose <span id="id2.2.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> <span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
<span id="footnotex1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> is available at <a target="_blank" href="https://github.com/UbiquitousLearning/AdaFL" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_italic">https://github.com/UbiquitousLearning/AdaFL</a>.
In the conference version (MobiCom’23) of this work, the system is named to <span id="footnotex1.2" class="ltx_text ltx_font_typewriter">AdaFL</span> instead of <span id="footnotex1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span>.
</span></span></span>, a framework that enhances the existing FedNLP with two novel designs.
First, <span id="id2.2.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> progressively upgrades the adapter configuration throughout a training session;
the principle is to quickly learn shallow knowledge by only training fewer and smaller adapters at the model’s top layers, and incrementally learn deep knowledge by incorporating deeper and larger adapters.
Second, <span id="id2.2.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> continuously profiles future adapter configurations by allocating participant devices to trial groups.
Extensive experiments show that <span id="id2.2.5" class="ltx_text ltx_font_typewriter">FedAdapter</span> can reduce FedNLP’s model convergence delay to no more than several hours, which is up to 155.5<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> faster compared to vanilla FedNLP and 48<math id="id2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation></semantics></math> faster compared to strong baselines.</p>
</div>
<div class="ltx_keywords">Federated Learning, Natural Language Processing, Communication efficiency
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2205.10162/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="219" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">FedNLP and its role in modern NLP</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the recent rise of transformers and its variants <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib78" title="" class="ltx_ref">vaswani2017attention, </a>; <a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>; <a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>; <a href="#bib.bib33" title="" class="ltx_ref">hou2020dynabert, </a>; <a href="#bib.bib52" title="" class="ltx_ref">liu2020fastbert, </a>; <a href="#bib.bib74" title="" class="ltx_ref">sun2020mobilebert, </a>; <a href="#bib.bib91" title="" class="ltx_ref">zafrir2019q8bert, </a>; <a href="#bib.bib9" title="" class="ltx_ref">bai2020binarybert, </a>)</cite>,
modern NLP models show compelling use cases on mobile devices.
Examples include sentiment analysis, QA, and auto completion <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib92" title="" class="ltx_ref">zhang2018deep, </a>; <a href="#bib.bib68" title="" class="ltx_ref">shao2019transformer, </a>; <a href="#bib.bib77" title="" class="ltx_ref">van2019does, </a>; <a href="#bib.bib42" title="" class="ltx_ref">kim2021code, </a>; <a href="#bib.bib76" title="" class="ltx_ref">svyatkovskiy2020intellicode, </a>)</cite>.
Through careful engineering <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>; <a href="#bib.bib33" title="" class="ltx_ref">hou2020dynabert, </a>; <a href="#bib.bib52" title="" class="ltx_ref">liu2020fastbert, </a>; <a href="#bib.bib74" title="" class="ltx_ref">sun2020mobilebert, </a>; <a href="#bib.bib9" title="" class="ltx_ref">bai2020binarybert, </a>)</cite>, inference with the NLP models is demonstrated to be affordable on mobile devices.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Much success of modern NLP comes from its training workflow as illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
(1) The pre-training phase initializes a model
on large text corpora.
The training is self-supervised and time-consuming, often taking hundreds if not thousands of GPU days <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>; <a href="#bib.bib14" title="" class="ltx_ref">brown2020language, </a>)</cite>.
Pre-training teaches the model a language’s inherent structure, e.g. word distribution.
(2) The fine-tuning phase further adapts a pre-trained model for a specific NLP task targeting a specific domain, e.g. to classify sentiments (a task) of user emails (a domain) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite>.
Fine-tuning is indispensable to modern NLP training;
only through it, the model maps the generic language understanding to the outputs for rich NLP tasks.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedNLP</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">The two NLP training phases require data of disparate natures.
While pre-training is typically done on public text corpora such as Wikipedia articles,
fine-tuning requires domain-specific samples such as user reviews, messages, or emails.
In mobile computing, these samples are generated by end users continually, distributed over mobile devices, and in many cases considered privacy sensitive.
</p>
</div>
<div id="S1.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p2.1" class="ltx_p">To fine-tune models on private, distributed data,
federated learning is the de-facto approach <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>; <a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>)</cite>.
In a training session targeting a specific NLP task and domain, a cloud service selects multiple mobile devices to participate in training.
A device trains a local copy of the model with its private data and sends the model updates to the cloud.
Having aggregated model updates from multiple devices, the cloud sends an updated model to the devices.
The training procedure repeats many rounds (typically hundreds or thousands <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib72" title="" class="ltx_ref">stremmel2021pretraining, </a>; <a href="#bib.bib54" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>) until the model accuracy reaches a desired level.</p>
</div>
<div id="S1.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p3.1" class="ltx_p">As such, this paper focuses on NLP model fine-tuning in a federated setting, a core NLP process in mobile computing.
Such a process is often referred to as FedNLP <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>, for which <math id="S1.SS0.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S1.SS0.SSS0.Px1.p3.1.m1.1a"><mi mathvariant="normal" id="S1.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S1.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S1.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px1.p3.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px1.p3.1.m1.1c">\S</annotation></semantics></math><a href="#S2.SS2" title="2.2. Federated Learning ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> will present a detailed system model.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedNLP overhead</h4>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">Despite the established FedNLP algorithm <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>,
it was unclear if FedNLP is practical on today’s mobile platforms.
This paper’s first contribution is a thorough characterization of FedNLP on a suite of benchmarks.
Our results in <math id="S1.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S1.SS0.SSS0.Px2.p1.1.m1.1a"><mi mathvariant="normal" id="S1.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px2.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S2.SS3" title="2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> show prohibitive overheads in twofold:
(1) Communication.
In each round, participating devices upload local gradients and then download the updated model,
each transferring hundreds of MBs of data.
(2) Mobile computation.
Even on an mobile device with GPU, its local computation takes up to several hundred seconds per round.
As a result, a fine-tuning session can take as long as a few days.
While federated learning has been known for high overhead in general,
FedNLP is particularly expensive, primarily because of the large sizes of transformer-based models and NLP task complexity.
As a comparison, FedNLP’s delay is at least
one order of magnitude
higher than typical federated training delays reported in literature.
</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adapters and their configuration</h4>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p1.1" class="ltx_p">Our primary goal is to reduce FedNLP’s training delay to reach a target accuracy, i.e. time to accuracy.
We first identify <span id="S1.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">adapters</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">houlsby2019parameter, </a>)</cite> as key building blocks for NLP models.
As small modules injected between adjacent transformer layers, adapters become the only tunable modules in a pre-trained model, freezing the remaining model parameters (often <math id="S1.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S1.SS0.SSS0.Px3.p1.1.m1.1a"><mo id="S1.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px3.p1.1.m1.1b"><gt id="S1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px3.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px3.p1.1.m1.1c">&gt;</annotation></semantics></math>99%) which therefore incur no communication or compute overhead.
While adapters have been proposed for parameter-efficient learning in general <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">houlsby2019parameter, </a>)</cite>,
we are the first to identify their significance for FedNLP and investigate the system implications.</p>
</div>
<div id="S1.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p2.2" class="ltx_p">Although adapters significantly reduce tunable parameters and hence the overhead,
they do not automatically result in optimal training delays.
The challenge is a large configuration space of adapters:
to which layer the adapters are injected (depth) and the capacities of individual adapters (width).
The adapter configuration has a strong impact on training overhead as well as the model convergence delay.
For instance, adding fixed-size adapters to all layers could see up to 2.29<math id="S1.SS0.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px3.p2.1.m1.1a"><mo id="S1.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S1.SS0.SSS0.Px3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px3.p2.1.m1.1b"><times id="S1.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px3.p2.1.m1.1c">\times</annotation></semantics></math> longer training delay as compared to adding the same adapters to fewer hand-picked layers (<math id="S1.SS0.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S1.SS0.SSS0.Px3.p2.2.m2.1a"><mi mathvariant="normal" id="S1.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S1.SS0.SSS0.Px3.p2.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px3.p2.2.m2.1b"><ci id="S1.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S1.SS0.SSS0.Px3.p2.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px3.p2.2.m2.1c">\S</annotation></semantics></math><a href="#S3.SS2" title="3.2. The Configuration Challenge ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
There is no silver-bullet configuration that results in the fastest convergence under each condition;
rather, the optimal configuration depends on
the specific NLP tasks, the target accuracy, and mobile resources such as network bandwidth and local execution speed.
The choice is also dynamic:
even within the same training session, the favorable configuration drifts over time, depending on the model’s learning progress.
Picking a non-optimal configuration could slow down model convergence by up to 4.7x and even underperform training with no adapters at all.
</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Our system: <span id="S1.SS0.SSS0.Px4.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>
</h4>

<div id="S1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p1.1" class="ltx_p">We therefore present a system called <span id="S1.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>,
which speeds up FedNLP with two key designs.
</p>
</div>
<div id="S1.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p2.1" class="ltx_p">First, <span id="S1.SS0.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> augments the cloud controller with dynamic adapter configuration.
The key ideas are twofold.
(1) <span id="S1.SS0.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_italic">Progressive training</span>.
<span id="S1.SS0.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> launches a training session with only small adapters inserted at the model’s top layers (i.e. close to the model output), which essentially learns shallow knowledge at a low training cost.
Only as the model accuracy starts to plateau, <span id="S1.SS0.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> adds bottom-layer adapters to training and increases their widths,
which learns deep knowledge at increasingly higher training costs.
This resonates with how humans learn knowledge in an incremental fashion and modern learning theories <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib11" title="" class="ltx_ref">bengio2009curriculum, </a>)</cite>.
(2) <span id="S1.SS0.SSS0.Px4.p2.1.5" class="ltx_text ltx_font_italic">Sideline trials</span>.
In addition to training the model with a current configuration,
<span id="S1.SS0.SSS0.Px4.p2.1.6" class="ltx_text ltx_font_typewriter">FedAdapter</span> probes <span id="S1.SS0.SSS0.Px4.p2.1.7" class="ltx_text ltx_font_italic">what</span> the next configuration will be and <span id="S1.SS0.SSS0.Px4.p2.1.8" class="ltx_text ltx_font_italic">when</span> to switch to it,
for which <span id="S1.SS0.SSS0.Px4.p2.1.9" class="ltx_text ltx_font_typewriter">FedAdapter</span> continuously profiles multiple candidate configurations.
To do so,
from all the participating devices in each round, <span id="S1.SS0.SSS0.Px4.p2.1.10" class="ltx_text ltx_font_typewriter">FedAdapter</span> allocates multiple trial groups, requests them to train the model with different adapter configurations, and compares their learning progresses.
If a trial group shows a much higher convergence rate than others as well as the current configuration under training, <span id="S1.SS0.SSS0.Px4.p2.1.11" class="ltx_text ltx_font_typewriter">FedAdapter</span> will commit to the configuration of this group.
While configuration trial distracts some devices from training with the current configuration,
it actually speeds up model convergence.
This is because the model convergence rate often sees diminishing returns as the population of participant devices grows <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>,
and our insight is that the surplus devices can better benefit model convergence by profiling next configurations.</p>
</div>
<div id="S1.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p3.1" class="ltx_p">Second, <span id="S1.SS0.SSS0.Px4.p3.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> enhances devices with cross-round activation cache.
We exploit an observation: by design, a device trains the same set of adapters in repeated runs until the configuration switches;
this set of adapters always spans a continuous range of top layers while the bottom layers remain frozen.
Exploiting such an opportunity, for a given configuration a device only executes a forward pass <span id="S1.SS0.SSS0.Px4.p3.1.2" class="ltx_text ltx_font_italic">once</span> through the bottom layers, caches the output activations,
and reuses the cached output as the input to the top layers which will undergo forward and backward passes.
Caching thus eschews training for the bottom layers, reducing the total training cost by up to one order of magnitude.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S1.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px5.p1.9" class="ltx_p">We implement <span id="S1.SS0.SSS0.Px5.p1.9.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> atop FedNLP <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>, a popular FL framework.
We test the resultant implementation on NVIDIA TX2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">tx2, </a>)</cite>/Nano <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">nano, </a>)</cite> and RaspberryPi 4B <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">rpi4b, </a>)</cite>, three development boards with resources similar to mainstream mobile devices.
On a diverse set of 4 NLP datasets, <span id="S1.SS0.SSS0.Px5.p1.9.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> reduces the training (model convergence) delay from 31.1–124.3 hours to 0.2–4.5 hours (up to 155.5<math id="S1.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.1.m1.1a"><mo id="S1.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.1.m1.1b"><times id="S1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.1.m1.1c">\times</annotation></semantics></math> reduction) as against a vanilla fine-tuning approach.
During a training session, <span id="S1.SS0.SSS0.Px5.p1.9.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> reduces the network traffic by 126.7<math id="S1.SS0.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.2.m2.1a"><mo id="S1.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S1.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.2.m2.1b"><times id="S1.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.2.m2.1c">\times</annotation></semantics></math> and per-device energy consumption by 18.4<math id="S1.SS0.SSS0.Px5.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.3.m3.1a"><mo id="S1.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S1.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.3.m3.1b"><times id="S1.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.3.m3.1c">\times</annotation></semantics></math> on average.
Compared to more advanced methods such as model quantization <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib86" title="" class="ltx_ref">wu2018error, </a>)</cite>, layer freezing <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">guo2019spottune, </a>; <a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>, and their combination, <span id="S1.SS0.SSS0.Px5.p1.9.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> still brings 4<math id="S1.SS0.SSS0.Px5.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.4.m4.1a"><mo id="S1.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S1.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.4.m4.1b"><times id="S1.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.4.m4.1c">\times</annotation></semantics></math>–48<math id="S1.SS0.SSS0.Px5.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.5.m5.1a"><mo id="S1.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S1.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.5.m5.1b"><times id="S1.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.5.m5.1c">\times</annotation></semantics></math> speedup.
Our key designs contribute to the results significantly:
compared to a hand-picked configuration which requires exhaustive offline search, <span id="S1.SS0.SSS0.Px5.p1.9.5" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s online configurator reduces the model convergence delay by 4.6<math id="S1.SS0.SSS0.Px5.p1.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.6.m6.1a"><mo id="S1.SS0.SSS0.Px5.p1.6.m6.1.1" xref="S1.SS0.SSS0.Px5.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.6.m6.1b"><times id="S1.SS0.SSS0.Px5.p1.6.m6.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.6.m6.1c">\times</annotation></semantics></math>;
<span id="S1.SS0.SSS0.Px5.p1.9.6" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s caching reduces the delay by 3.3<math id="S1.SS0.SSS0.Px5.p1.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.7.m7.1a"><mo id="S1.SS0.SSS0.Px5.p1.7.m7.1.1" xref="S1.SS0.SSS0.Px5.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.7.m7.1b"><times id="S1.SS0.SSS0.Px5.p1.7.m7.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.7.m7.1c">\times</annotation></semantics></math>.
<span id="S1.SS0.SSS0.Px5.p1.9.7" class="ltx_text ltx_font_typewriter">FedAdapter</span> is also resource efficient: it reduces the network traffic by 126.7<math id="S1.SS0.SSS0.Px5.p1.8.m8.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.8.m8.1a"><mo id="S1.SS0.SSS0.Px5.p1.8.m8.1.1" xref="S1.SS0.SSS0.Px5.p1.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.8.m8.1b"><times id="S1.SS0.SSS0.Px5.p1.8.m8.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.8.m8.1c">\times</annotation></semantics></math> and per-device energy consumption by 18.4<math id="S1.SS0.SSS0.Px5.p1.9.m9.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.SS0.SSS0.Px5.p1.9.m9.1a"><mo id="S1.SS0.SSS0.Px5.p1.9.m9.1.1" xref="S1.SS0.SSS0.Px5.p1.9.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px5.p1.9.m9.1b"><times id="S1.SS0.SSS0.Px5.p1.9.m9.1.1.cmml" xref="S1.SS0.SSS0.Px5.p1.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px5.p1.9.m9.1c">\times</annotation></semantics></math> on average.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Contributions</h4>

<div id="S1.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px6.p1.1" class="ltx_p">We have made the following contributions.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We carry out the first FedNLP measurement on actual embedded hardware and demonstrate its slow convergence.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We identify adapters as a building block for FedNLP and the major challenge of adapter configuration.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We design an FL framework <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> that automatically configures adapters on the fly for fast training and optimizes for mobile resource usage.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We demonstrate <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s effectiveness through extensive experiments.
For the first time, <span id="S1.I1.i4.p1.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> makes FedNLP practical for commodity mobile devices.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Motivations</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>NLP Training Workflow</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The modern NLP training typically consists of two stages: pre-training and fine-tuning.
During pre-training, a model is trained on large text datasets, e.g., OSCAR corpora <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">AbadjiOrtizSuarezRomaryetal.2021, </a>)</cite> with more than 370 billion words.
Those datasets are obtained from public domains, e.g., Wikipedia, Twitter, etc.
A pre-trained language model captures the linguistic structure that is ubiquitous and independent of downstream tasks.
The pre-training is usually performed in a self-supervised manner and therefore requires no data labels <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">erhan2010does, </a>)</cite>.
It needs huge compute resources (a mid/large GPU cluster) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib14" title="" class="ltx_ref">brown2020language, </a>; <a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite>, typically done by big companies such as Google.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The fine-tuning adapts the pre-trained model to various, concrete “downstream” language tasks such as text classification, sequence tagging, text generation, and question answering.
This often entails modifying the whole or only top layers of the pre-trained model, and additional training passes to adjust the model weights.
Fine-tuning requires <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">labeled</span> samples for the given task, and is done in a supervised fashion.
The downstream tasks are abundant and keep emerging with time, e.g., new domains, topics or data distributions.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The state-of-the-art NLP models that follow the pre-training workflow are transformer-based, e.g., BERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite> and its variants <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>; <a href="#bib.bib33" title="" class="ltx_ref">hou2020dynabert, </a>; <a href="#bib.bib52" title="" class="ltx_ref">liu2020fastbert, </a>; <a href="#bib.bib74" title="" class="ltx_ref">sun2020mobilebert, </a>; <a href="#bib.bib91" title="" class="ltx_ref">zafrir2019q8bert, </a>; <a href="#bib.bib9" title="" class="ltx_ref">bai2020binarybert, </a>)</cite>.
Those models are composed of many transformer blocks, where each block extensively uses attention mechanisms <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib78" title="" class="ltx_ref">vaswani2017attention, </a>)</cite>.
We refer readers to recent surveys <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib41" title="" class="ltx_ref">khadhraoui2022survey, </a>; <a href="#bib.bib25" title="" class="ltx_ref">han2022survey, </a>; <a href="#bib.bib26" title="" class="ltx_ref">han2021transformer, </a>)</cite> for how those models work internally.
This work specifically targets the fine-tuning stage of transformer-based models for its pivotal role in modern NLP services.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The fine-tuning is often performed on data generated by applications in user devices, e.g., input methods <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib89" title="" class="ltx_ref">xu2018deeptype, </a>)</cite>, emails <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib46" title="" class="ltx_ref">lee2020catbert, </a>; <a href="#bib.bib73" title="" class="ltx_ref">sun2019fine, </a>)</cite>, and instant messaging <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib71" title="" class="ltx_ref">soldevilla2021natural, </a>)</cite>.
Those data is private by nature and cannot be collected arbitrarily to respect user’s privacy concern and legal regulation like GDPR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib79" title="" class="ltx_ref">voigt2017eu, </a>)</cite>.
Federated learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">kairouz2021advances, </a>)</cite> addresses this need by enabling many devices to collaboratively train a shared model without giving away their data.
The key idea is to decentralize the training over devices and only ask them to share model updates instead of raw data.
For this reason, the benchmark for NLP training in a federated setting is emerging <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">System model</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">A pre-trained transformer-based language model is given as input.
After that, our task is to fine-tune the model in a federated environment for an unbounded number of unforeseen tasks may emerge.
For each task, the fine-tuning initiator (or <span id="S2.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">developer</span>) specifies how the last output layer shall be revised (e.g., number of classes).</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p2.1" class="ltx_p">The fine-tuning mostly follows an FL common practice <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>)</cite>.
In each round, a cloud service (or <span id="S2.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">aggregator</span>) selects a fixed number of mobile devices (or <span id="S2.SS2.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_italic">clients</span>) as participants.
The model is fine-tuned on each client and the model updates will be uploaded/aggregated on the cloud.
The aggregation is often lightweight and the cloud resources can be flexibly scaled out, so its time cost could be neglected.
The fine-tuning speed is mainly bottlenecked by the on-device training and the network transmission.
The most likely network for FedNLP is WiFi, which is highly unstable and constrained from a hundred Kbps to a few Mbps <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib88" title="" class="ltx_ref">xu2021cloud, </a>)</cite>.
Note that FL typically executes in synchronous manner, so the more constrained hardware/network often bottlenecks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib64" title="" class="ltx_ref">reisizadeh2020straggler, </a>)</cite>.
We do not consider the device memory to be an obstacle because:
(1) Modern mobile devices with many GBs of DRAM can support fine-tuning tasks for BERT (BS=8) according to our experiments (<math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><mi mathvariant="normal" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">\S</annotation></semantics></math><a href="#S6.SS3" title="6.3. Client Resource Cost ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>);
(2) Memory inefficiency can be compensated with acceptable training overhead through advanced memory optimizations such as rematerialization <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib15" title="" class="ltx_ref">chen2016training, </a>)</cite> and paging <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib56" title="" class="ltx_ref">peng2020capuchin, </a>)</cite>.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p3.1" class="ltx_p">The key metric concerned in this work is time-to-accuracy, a widely adopted metric <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">coleman2019analysis, </a>)</cite> that indicates the training time taken to reach a target accuracy.
This is more practical and comprehensive than a single “time to full convergence” because the accuracy improvement per time slice dramatically decreases when the model approaches full convergence.
For instance, when FL fine-tuning on task <span id="S2.SS2.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_typewriter">AGNEWS</span>, it takes only 5.2 hours to obtain 80% accuracy but another 25.9 hours to 90%.
In our system model, the target accuracy could be preset by the developer so the cloud service will train multiple rounds using FL until the target is met.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p4.1" class="ltx_p">We impose no constraint on client selection <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>; <a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>)</cite> or training data sampling <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib48" title="" class="ltx_ref">li2021sample, </a>)</cite> strategies, making it compatible with a mass of recent FL system literature.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Preliminary Measurements</h3>

<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="281" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F2.1.2.2" class="ltx_text" style="font-size:90%;">CV vs. NLP models</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x3.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="315" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F2.2.2.2" class="ltx_text" style="font-size:90%;">FL convegence time</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F2.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="" id="S2.F2.3.g1" class="ltx_graphics ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F2.3.2.2" class="ltx_text" style="font-size:90%;">Breakdown of FedNLP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F2.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x5.png" id="S2.F2.4.g1" class="ltx_graphics ltx_img_landscape" width="507" height="334" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.4.1.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S2.F2.4.2.2" class="ltx_text" style="font-size:90%;">Layer freezing in FedNLP</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.6.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S2.F2.7.2" class="ltx_text" style="font-size:90%;">The preliminary measurement results of FedNLP.
(a) A glance at the complexity of NLP models and traditional CNNs;
(b) End-to-end convergence time of CV and NLP models under FL settings (CV1: “Densenet-121 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib36" title="" class="ltx_ref">huang2017densely, </a>)</cite> + CelebA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib53" title="" class="ltx_ref">liu2015deep, </a>)</cite>”; CV2: “Resnet56 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib30" title="" class="ltx_ref">he2016deep, </a>)</cite> + Cifar-100 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib43" title="" class="ltx_ref">krizhevsky2009learning, </a>)</cite>”; NLP: “BERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite> + Semeval <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib31" title="" class="ltx_ref">hendrickx2019semeval, </a>)</cite>”).
(c) Training time breakdown of FedNLP tasks on different hardware. Model: BERT; batch size: 4.
(d) The performance of layer freezing. Model: DistilBERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>)</cite>; Dataset: ONTONOTES <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib60" title="" class="ltx_ref">pradhan2013towards, </a>)</cite>; batch size: 4.</span></figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We perform preliminary experiments that highlight the motivations to improve FedNLP fine-tuning performance and provide implications for the design of <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.2" class="ltx_p"><span id="S2.SS3.p2.2.1" class="ltx_text ltx_font_bold">Observation-1: Transformer-based NLP models are highly costly.</span>
As illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, transformer-based NLP models are much more expensive than the classic vision models in general in consideration of parameter numbers and computing complexity.
BERT large has 330M trainable weights and takes 250,000 PFLOPs to train, which is 6<math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mo id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><times id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\times</annotation></semantics></math>/23<math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><mo id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><times id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">\times</annotation></semantics></math> higher than ResNet-152, respectively.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Observation-2: FedNLP task is extremely slow.</span>
Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the end-to-end training time towards full convergence for typical NLP and CV tasks under federated setting.
As observed, it takes up to 210.74–359.7 hours to train on <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">SEMEVAL</span> dataset, which is 1.53–10.53<math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mo id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><times id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\times</annotation></semantics></math> longer than training ResNet-56 on CIFAR-100.
Note that dataset <span id="S2.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">SEMEVAL</span> used for classification tasks has 19 labels, while <span id="S2.SS3.p3.1.4" class="ltx_text ltx_font_typewriter">CIFAR-100</span> has 100 labels.
It’s also worth mentioning that the above CV tasks are launched from scratch while the NLP tasks are fine-tuned atop a well pre-trained model.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Observation-3: network transmission dominates the training delay on high-end devices.</span>
The training time spent towards model convergence is dominated by two parts: on-device training and network transmission.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows such breakdown on three kinds of hardware (Jetson TX2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">tx2, </a>)</cite>, Jetson Nano <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">nano, </a>)</cite>, and Raspberry Pi 4B <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">rpi4b, </a>)</cite>) that span a wide spectrum of hardware capacity and 1MB/s network bandwidth (both uplink and downlink).
It shows that for a high-end edge device like Jetson TX2, the network transmission delay is the major bottleneck (about 94.22%) of FedNLP tasks.
On a relatively wimpy device, both two parts contribute nontrivially to the total training time.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">Observation-4: existing techniques are inadequate for FedNLP.</span>
A common approach to reducing the training cost in NLP fine-tuning is freezing a few bottom transformer blocks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">guo2019spottune, </a>; <a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>.
It literally reduces the training computations by early stopping the backward propagation and the network cost by only sending the trainable parameters.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the tradeoffs between the convergence time and training accuracy loss by tuning the number of frozen transformer blocks on DistilBERT and <span id="S2.SS3.p5.1.2" class="ltx_text ltx_font_typewriter">ONTONOTES</span>.
Unfortunately, we observed that the profits of such an approach are modest.
For instance, to guarantee an acceptable accuracy loss (e.g., <math id="S2.SS3.p5.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S2.SS3.p5.1.m1.1a"><mo id="S2.SS3.p5.1.m1.1.1" xref="S2.SS3.p5.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p5.1.m1.1b"><leq id="S2.SS3.p5.1.m1.1.1.cmml" xref="S2.SS3.p5.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p5.1.m1.1c">\leq</annotation></semantics></math>1%), 2 out of 6 transformer layers can be frozen at most and only 33.3% (2/6) network traffic can be saved.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Implications</span>
FedNLP is slow due to the considerable amount of time spent on data transmission and local training.
Simply freezing part of the model brings only modest improvement.
While cellular network capacity keeps upgrading, their costly nature hinders adoption in federated tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib16" title="" class="ltx_ref">cheng20225g, </a>)</cite>.
To enable practical FedNLP with a tolerable convergence delay (e.g., a few hours), the model structure and training paradigm need to be re-architected.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Design</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Plugable Adapters</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2205.10162/assets/x6.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="217" height="48" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">The structure of adapters used.</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Transformer adapters</span>
For efficient FedNLP, we retrofit adapters – a recently proposed technique for both CV and NLP tasks to achieve parameter efficiency in machine learning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">houlsby2019parameter, </a>)</cite>.
The initial goal of adapters is to reduce the tunable parameters especially in continuous learning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib19" title="" class="ltx_ref">delange2021continual, </a>)</cite> scenario where unlimited number of new tasks might emerge.
However, it has been seldomly used to tackle system challenges like network cost and convergence speed.
As far as we know, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> is the first to apply adapters to federated NLP tasks and demonstrate its efficiency under a system context.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">The key idea of adapter is to freeze the whole original model but insert a few small modules into different locations inside it.
Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the architecture of our adapters and how it’s applied to the transformer.
The adapter approach inserts small modules (adapters) between transformer layers.
The adapter layer generally uses a down-projection with <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\boldsymbol{W}_{\text{down }}\in\mathbb{R}^{n\times m}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msub id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">𝑾</mi><mtext id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3a.cmml">down </mtext></msub><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.3.2" xref="S3.SS1.p2.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.3.3.1" xref="S3.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.3.cmml">m</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><in id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></in><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2.2">𝑾</ci><ci id="S3.SS1.p2.1.m1.1.1.2.3a.cmml" xref="S3.SS1.p2.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p2.1.m1.1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.1.1.2.3">down </mtext></ci></apply><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3"><times id="S3.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.2">𝑛</ci><ci id="S3.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\boldsymbol{W}_{\text{down }}\in\mathbb{R}^{n\times m}</annotation></semantics></math> to project the input <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">h</annotation></semantics></math> to a lower-dimensional space specified by bottleneck dimension <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">m</annotation></semantics></math>, followed by a nonlinear activation function <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="f(\cdot)" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.2" xref="S3.SS1.p2.4.m4.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.2.2" xref="S3.SS1.p2.4.m4.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.2.1" xref="S3.SS1.p2.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.1.2.3.2" xref="S3.SS1.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.1.2.3.2.1" xref="S3.SS1.p2.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p2.4.m4.1.2.3.2.2" xref="S3.SS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.2.cmml" xref="S3.SS1.p2.4.m4.1.2"><times id="S3.SS1.p2.4.m4.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.2.1"></times><ci id="S3.SS1.p2.4.m4.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.2.2">𝑓</ci><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">f(\cdot)</annotation></semantics></math>, and a up-projection with <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\boldsymbol{W}_{\text{up }}\in\mathbb{R}^{m\times n}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><msub id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2.2" xref="S3.SS1.p2.5.m5.1.1.2.2.cmml">𝑾</mi><mtext id="S3.SS1.p2.5.m5.1.1.2.3" xref="S3.SS1.p2.5.m5.1.1.2.3a.cmml">up </mtext></msub><mo id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.3.2" xref="S3.SS1.p2.5.m5.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.5.m5.1.1.3.3.1" xref="S3.SS1.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.5.m5.1.1.3.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><in id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"></in><apply id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.2.1.cmml" xref="S3.SS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2.2">𝑾</ci><ci id="S3.SS1.p2.5.m5.1.1.2.3a.cmml" xref="S3.SS1.p2.5.m5.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p2.5.m5.1.1.2.3.cmml" xref="S3.SS1.p2.5.m5.1.1.2.3">up </mtext></ci></apply><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3"><times id="S3.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.2">𝑚</ci><ci id="S3.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\boldsymbol{W}_{\text{up }}\in\mathbb{R}^{m\times n}</annotation></semantics></math>.
These adapters are surrounded by a residual connection, leading to a final form as:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="\boldsymbol{h}\leftarrow\boldsymbol{h}+f\left(\boldsymbol{h}\boldsymbol{W}_{\text{down }}\right)\boldsymbol{W}_{\text{up }}." display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.3.cmml">𝒉</mi><mo stretchy="false" id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml">←</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml">𝒉</mi><mo id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.2.cmml">𝒉</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝑾</mi><mtext id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3a.cmml">down </mtext></msub></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.1.2a" xref="S3.Ex1.m1.1.1.1.1.1.1.2.cmml">​</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.1.4.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.4.2" xref="S3.Ex1.m1.1.1.1.1.1.1.4.2.cmml">𝑾</mi><mtext id="S3.Ex1.m1.1.1.1.1.1.1.4.3" xref="S3.Ex1.m1.1.1.1.1.1.1.4.3a.cmml">up </mtext></msub></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex1.m1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><ci id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2">←</ci><ci id="S3.Ex1.m1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3">𝒉</ci><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><plus id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></plus><ci id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3">𝒉</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.2"></times><ci id="S3.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.3">𝑓</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.2">𝒉</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.2">𝑾</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.3.3">down </mtext></ci></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.4.2">𝑾</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.4.3a.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.4.3"><mtext mathsize="70%" id="S3.Ex1.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.4.3">up </mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\boldsymbol{h}\leftarrow\boldsymbol{h}+f\left(\boldsymbol{h}\boldsymbol{W}_{\text{down }}\right)\boldsymbol{W}_{\text{up }}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We follow prior work <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib57" title="" class="ltx_ref">pfeiffer2021adapterfusion, </a>)</cite>, a state-of-the-art adapter variant to only insert one adapter module after the second sub-layer, i.e., the feed-forward network ”add &amp; layer norm” sublayer.
The output of the adapter is then passed directly into the next transformer layer.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">The rationales behind adapters</span>
Why is adapter able to achieve comparable accuracy with much fewer parameters than freezing the bottom transformer layers without revising the model structure?
We reason it with two insights from our experiments<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
The experiments refer to Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3. Preliminary Measurements ‣ 2. Background and Motivations ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which shows layer freezing exhibits moderated improvement.
</span></span></span>
and related literature <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib57" title="" class="ltx_ref">pfeiffer2021adapterfusion, </a>; <a href="#bib.bib66" title="" class="ltx_ref">ruckle2020adapterdrop, </a>)</cite>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">First, adapters allow modifying a model’s hidden state at a low cost.
By keeping the whole original model as it is, adapters can maximally preserve the knowledge learned from the pre-training dataset.
The pluggable adapters are only used to encode task-specific representations in intermediate layers of the shared model.
While in fine-tuning scenario, the downstream tasks mostly share low-level feature representation with the pre-training task, it’s still beneficial to adjust the low and middle-level feature extractor.
Second, we observe that using adapters stabilizes the convergence process, while fine-tuning on the full model easily goes to overfitting.
Though the overfitting can be remedied by carefully tuning the hyper-parameters, it also requires non-trivial efforts for each separated fine-tuning task.
</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.3" class="ltx_p"><span id="S3.SS1.p6.3.1" class="ltx_text ltx_font_bold">Network cost analysis</span>
The trainable parameter number per adapter is <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="2mn+n+m" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mrow id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml"><mn id="S3.SS1.p6.1.m1.1.1.2.2" xref="S3.SS1.p6.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.2.1" xref="S3.SS1.p6.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.2.3" xref="S3.SS1.p6.1.m1.1.1.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.2.1a" xref="S3.SS1.p6.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.2.4" xref="S3.SS1.p6.1.m1.1.1.2.4.cmml">n</mi></mrow><mo id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">+</mo><mi id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">n</mi><mo id="S3.SS1.p6.1.m1.1.1.1a" xref="S3.SS1.p6.1.m1.1.1.1.cmml">+</mo><mi id="S3.SS1.p6.1.m1.1.1.4" xref="S3.SS1.p6.1.m1.1.1.4.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><plus id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></plus><apply id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2"><times id="S3.SS1.p6.1.m1.1.1.2.1.cmml" xref="S3.SS1.p6.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS1.p6.1.m1.1.1.2.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2.2">2</cn><ci id="S3.SS1.p6.1.m1.1.1.2.3.cmml" xref="S3.SS1.p6.1.m1.1.1.2.3">𝑚</ci><ci id="S3.SS1.p6.1.m1.1.1.2.4.cmml" xref="S3.SS1.p6.1.m1.1.1.2.4">𝑛</ci></apply><ci id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3">𝑛</ci><ci id="S3.SS1.p6.1.m1.1.1.4.cmml" xref="S3.SS1.p6.1.m1.1.1.4">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">2mn+n+m</annotation></semantics></math>.
Clients only send those parameters and last-layer classifier parameters after on-device training to the aggregator.
Therefore the network transmission per round is reduced to
<math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="D\times(2mn+n+m)+n\times\#labels" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mrow id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.1.3" xref="S3.SS1.p6.2.m2.1.1.1.3.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.2.m2.1.1.1.2" xref="S3.SS1.p6.2.m2.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p6.2.m2.1.1.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p6.2.m2.1.1.1.1.1.2" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p6.2.m2.1.1.1.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.cmml"><mrow id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.cmml"><mn id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.2" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.3" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1a" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.4" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.4.cmml">n</mi></mrow><mo id="S3.SS1.p6.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS1.p6.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.3.cmml">n</mi><mo id="S3.SS1.p6.2.m2.1.1.1.1.1.1.1a" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS1.p6.2.m2.1.1.1.1.1.1.4" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.4.cmml">m</mi></mrow><mo stretchy="false" id="S3.SS1.p6.2.m2.1.1.1.1.1.3" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">+</mo><mrow id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml"><mrow id="S3.SS1.p6.2.m2.1.1.3.2" xref="S3.SS1.p6.2.m2.1.1.3.2.cmml"><mi id="S3.SS1.p6.2.m2.1.1.3.2.2" xref="S3.SS1.p6.2.m2.1.1.3.2.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.2.m2.1.1.3.2.1" xref="S3.SS1.p6.2.m2.1.1.3.2.1.cmml">×</mo><mi mathvariant="normal" id="S3.SS1.p6.2.m2.1.1.3.2.3" xref="S3.SS1.p6.2.m2.1.1.3.2.3.cmml">#</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.3" xref="S3.SS1.p6.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1a" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.4" xref="S3.SS1.p6.2.m2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1b" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.5" xref="S3.SS1.p6.2.m2.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1c" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.6" xref="S3.SS1.p6.2.m2.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1d" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.7" xref="S3.SS1.p6.2.m2.1.1.3.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.2.m2.1.1.3.1e" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p6.2.m2.1.1.3.8" xref="S3.SS1.p6.2.m2.1.1.3.8.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><plus id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2"></plus><apply id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1"><times id="S3.SS1.p6.2.m2.1.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.1.2"></times><ci id="S3.SS1.p6.2.m2.1.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.1.3">𝐷</ci><apply id="S3.SS1.p6.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1"><plus id="S3.SS1.p6.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.1"></plus><apply id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2"><times id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.1"></times><cn type="integer" id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.2">2</cn><ci id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.3">𝑚</ci><ci id="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.4.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.2.4">𝑛</ci></apply><ci id="S3.SS1.p6.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.3">𝑛</ci><ci id="S3.SS1.p6.2.m2.1.1.1.1.1.1.4.cmml" xref="S3.SS1.p6.2.m2.1.1.1.1.1.1.4">𝑚</ci></apply></apply><apply id="S3.SS1.p6.2.m2.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3"><times id="S3.SS1.p6.2.m2.1.1.3.1.cmml" xref="S3.SS1.p6.2.m2.1.1.3.1"></times><apply id="S3.SS1.p6.2.m2.1.1.3.2.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2"><times id="S3.SS1.p6.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2.1"></times><ci id="S3.SS1.p6.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2.2">𝑛</ci><ci id="S3.SS1.p6.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2.3">#</ci></apply><ci id="S3.SS1.p6.2.m2.1.1.3.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3.3">𝑙</ci><ci id="S3.SS1.p6.2.m2.1.1.3.4.cmml" xref="S3.SS1.p6.2.m2.1.1.3.4">𝑎</ci><ci id="S3.SS1.p6.2.m2.1.1.3.5.cmml" xref="S3.SS1.p6.2.m2.1.1.3.5">𝑏</ci><ci id="S3.SS1.p6.2.m2.1.1.3.6.cmml" xref="S3.SS1.p6.2.m2.1.1.3.6">𝑒</ci><ci id="S3.SS1.p6.2.m2.1.1.3.7.cmml" xref="S3.SS1.p6.2.m2.1.1.3.7">𝑙</ci><ci id="S3.SS1.p6.2.m2.1.1.3.8.cmml" xref="S3.SS1.p6.2.m2.1.1.3.8">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">D\times(2mn+n+m)+n\times\#labels</annotation></semantics></math>,
where <math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mi id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><ci id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">D</annotation></semantics></math> is the total number of transformer blocks of the NLP model.
As shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
compared to fine-tuning the whole BERT model, the network saving could be more than 99%.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.2" class="ltx_p"><span id="S3.SS1.p7.2.1" class="ltx_text ltx_font_bold">Compute cost analysis</span>
The computation FLOPs of each adapter in forward pass is <math id="S3.SS1.p7.1.m1.1" class="ltx_Math" alttext="2\times m\times n\times seqlen" display="inline"><semantics id="S3.SS1.p7.1.m1.1a"><mrow id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mrow id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml"><mn id="S3.SS1.p7.1.m1.1.1.2.2" xref="S3.SS1.p7.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p7.1.m1.1.1.2.1" xref="S3.SS1.p7.1.m1.1.1.2.1.cmml">×</mo><mi id="S3.SS1.p7.1.m1.1.1.2.3" xref="S3.SS1.p7.1.m1.1.1.2.3.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p7.1.m1.1.1.2.1a" xref="S3.SS1.p7.1.m1.1.1.2.1.cmml">×</mo><mi id="S3.SS1.p7.1.m1.1.1.2.4" xref="S3.SS1.p7.1.m1.1.1.2.4.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p7.1.m1.1.1.2.1b" xref="S3.SS1.p7.1.m1.1.1.2.1.cmml">×</mo><mi id="S3.SS1.p7.1.m1.1.1.2.5" xref="S3.SS1.p7.1.m1.1.1.2.5.cmml">s</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p7.1.m1.1.1.1" xref="S3.SS1.p7.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.1.m1.1.1.1a" xref="S3.SS1.p7.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.1.m1.1.1.4" xref="S3.SS1.p7.1.m1.1.1.4.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.1.m1.1.1.1b" xref="S3.SS1.p7.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.1.m1.1.1.5" xref="S3.SS1.p7.1.m1.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.1.m1.1.1.1c" xref="S3.SS1.p7.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.1.m1.1.1.6" xref="S3.SS1.p7.1.m1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.1.m1.1.1.1d" xref="S3.SS1.p7.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.1.m1.1.1.7" xref="S3.SS1.p7.1.m1.1.1.7.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><times id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1.1"></times><apply id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2"><times id="S3.SS1.p7.1.m1.1.1.2.1.cmml" xref="S3.SS1.p7.1.m1.1.1.2.1"></times><cn type="integer" id="S3.SS1.p7.1.m1.1.1.2.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2.2">2</cn><ci id="S3.SS1.p7.1.m1.1.1.2.3.cmml" xref="S3.SS1.p7.1.m1.1.1.2.3">𝑚</ci><ci id="S3.SS1.p7.1.m1.1.1.2.4.cmml" xref="S3.SS1.p7.1.m1.1.1.2.4">𝑛</ci><ci id="S3.SS1.p7.1.m1.1.1.2.5.cmml" xref="S3.SS1.p7.1.m1.1.1.2.5">𝑠</ci></apply><ci id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3">𝑒</ci><ci id="S3.SS1.p7.1.m1.1.1.4.cmml" xref="S3.SS1.p7.1.m1.1.1.4">𝑞</ci><ci id="S3.SS1.p7.1.m1.1.1.5.cmml" xref="S3.SS1.p7.1.m1.1.1.5">𝑙</ci><ci id="S3.SS1.p7.1.m1.1.1.6.cmml" xref="S3.SS1.p7.1.m1.1.1.6">𝑒</ci><ci id="S3.SS1.p7.1.m1.1.1.7.cmml" xref="S3.SS1.p7.1.m1.1.1.7">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">2\times m\times n\times seqlen</annotation></semantics></math> (normalized to single data sample), where <math id="S3.SS1.p7.2.m2.1" class="ltx_Math" alttext="seqlen" display="inline"><semantics id="S3.SS1.p7.2.m2.1a"><mrow id="S3.SS1.p7.2.m2.1.1" xref="S3.SS1.p7.2.m2.1.1.cmml"><mi id="S3.SS1.p7.2.m2.1.1.2" xref="S3.SS1.p7.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.2.m2.1.1.1" xref="S3.SS1.p7.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.2.m2.1.1.3" xref="S3.SS1.p7.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.2.m2.1.1.1a" xref="S3.SS1.p7.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.2.m2.1.1.4" xref="S3.SS1.p7.2.m2.1.1.4.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.2.m2.1.1.1b" xref="S3.SS1.p7.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.2.m2.1.1.5" xref="S3.SS1.p7.2.m2.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.2.m2.1.1.1c" xref="S3.SS1.p7.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.2.m2.1.1.6" xref="S3.SS1.p7.2.m2.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p7.2.m2.1.1.1d" xref="S3.SS1.p7.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS1.p7.2.m2.1.1.7" xref="S3.SS1.p7.2.m2.1.1.7.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.2.m2.1b"><apply id="S3.SS1.p7.2.m2.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1"><times id="S3.SS1.p7.2.m2.1.1.1.cmml" xref="S3.SS1.p7.2.m2.1.1.1"></times><ci id="S3.SS1.p7.2.m2.1.1.2.cmml" xref="S3.SS1.p7.2.m2.1.1.2">𝑠</ci><ci id="S3.SS1.p7.2.m2.1.1.3.cmml" xref="S3.SS1.p7.2.m2.1.1.3">𝑒</ci><ci id="S3.SS1.p7.2.m2.1.1.4.cmml" xref="S3.SS1.p7.2.m2.1.1.4">𝑞</ci><ci id="S3.SS1.p7.2.m2.1.1.5.cmml" xref="S3.SS1.p7.2.m2.1.1.5">𝑙</ci><ci id="S3.SS1.p7.2.m2.1.1.6.cmml" xref="S3.SS1.p7.2.m2.1.1.6">𝑒</ci><ci id="S3.SS1.p7.2.m2.1.1.7.cmml" xref="S3.SS1.p7.2.m2.1.1.7">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.2.m2.1c">seqlen</annotation></semantics></math> is the sequence length (default 256 in BERT).
This incurred overhead is trivial compared to the original model complexity, e.g., less than 1% on BERT.
On the other hand, since all other parameters are fixed during training, calculating the gradients of those fixed weights can be avoided in backward propagation. Table <a href="#S3.T1" title="Table 1 ‣ 3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows adapter brings around 40% training time reduction.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.5.1" class="ltx_tr">
<th id="S3.T1.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<th id="S3.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.4.5.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Method</span></th>
<th id="S3.T1.4.5.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T1.4.5.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.5.1.3.1.1" class="ltx_tr">
<td id="S3.T1.4.5.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.4.5.1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Training Time</span></td>
</tr>
</table>
</th>
<th id="S3.T1.4.5.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S3.T1.4.5.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.5.1.4.1.1" class="ltx_tr">
<td id="S3.T1.4.5.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.4.5.1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Updated Paras.</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">BERT</span></th>
<th id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">Full Fine-tuning</span></th>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.1.1.4.1" class="ltx_text" style="font-size:80%;">1.86 sec</span></td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S3.T1.1.1.1.1" class="ltx_text" style="font-size:80%;">110.01 x </span><math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><msup id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.m1.1.1.2.cmml">10</mn><mn mathsize="80%" id="S3.T1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.m1.1.1.2">10</cn><cn type="integer" id="S3.T1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">10^{6}</annotation></semantics></math>
</td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">Adapter</span></th>
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">1.14 sec</span></td>
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S3.T1.2.2.1.1" class="ltx_text" style="font-size:80%;">0.61 x </span><math id="S3.T1.2.2.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S3.T1.2.2.1.m1.1a"><msup id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T1.2.2.1.m1.1.1.2" xref="S3.T1.2.2.1.m1.1.1.2.cmml">10</mn><mn mathsize="80%" id="S3.T1.2.2.1.m1.1.1.3" xref="S3.T1.2.2.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><apply id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T1.2.2.1.m1.1.1.2.cmml" xref="S3.T1.2.2.1.m1.1.1.2">10</cn><cn type="integer" id="S3.T1.2.2.1.m1.1.1.3.cmml" xref="S3.T1.2.2.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">10^{6}</annotation></semantics></math>
</td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<th id="S3.T1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.3.3.2.1" class="ltx_text" style="font-size:80%;">DistilBERT</span></th>
<th id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.3.3.3.1" class="ltx_text" style="font-size:80%;">Full Fine-tuning</span></th>
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T1.3.3.4.1" class="ltx_text" style="font-size:80%;">0.91 sec</span></td>
<td id="S3.T1.3.3.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S3.T1.3.3.1.1" class="ltx_text" style="font-size:80%;">67 x </span><math id="S3.T1.3.3.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S3.T1.3.3.1.m1.1a"><msup id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T1.3.3.1.m1.1.1.2" xref="S3.T1.3.3.1.m1.1.1.2.cmml">10</mn><mn mathsize="80%" id="S3.T1.3.3.1.m1.1.1.3" xref="S3.T1.3.3.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><apply id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T1.3.3.1.m1.1.1.2.cmml" xref="S3.T1.3.3.1.m1.1.1.2">10</cn><cn type="integer" id="S3.T1.3.3.1.m1.1.1.3.cmml" xref="S3.T1.3.3.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">10^{6}</annotation></semantics></math>
</td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.4.4.2.1" class="ltx_text" style="font-size:80%;">Adapter</span></th>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.4.4.3.1" class="ltx_text" style="font-size:80%;">0.56 sec</span></td>
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">
<span id="S3.T1.4.4.1.1" class="ltx_text" style="font-size:80%;">0.32 x </span><math id="S3.T1.4.4.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S3.T1.4.4.1.m1.1a"><msup id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml"><mn mathsize="80%" id="S3.T1.4.4.1.m1.1.1.2" xref="S3.T1.4.4.1.m1.1.1.2.cmml">10</mn><mn mathsize="80%" id="S3.T1.4.4.1.m1.1.1.3" xref="S3.T1.4.4.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><apply id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T1.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.1.m1.1.1.2">10</cn><cn type="integer" id="S3.T1.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">10^{6}</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.8.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>. </span><span id="S3.T1.9.2" class="ltx_text" style="font-size:113%;">Computation and communication cost of inserting adapters into each transformer block (width=32) and full-model tuning on Jetson TX2.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>The Configuration Challenge</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A unique challenge raised by adapters is its sensitivity to the configurations (explained below).
Different configurations result in a variety of convergence delays, up to 4.7<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation></semantics></math> gap.
Choosing an “optimal” configuration towards fast convergence is fundamentally challenging for the following reasons.
</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Large adapter configuration space</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">There are two critical parameters of adapters to be determined: depth and width.
(1) Similar to the idea of layer freezing, adapters are not necessarily inserted into each transformer block.
Reducing the number of adapters inserted into the top blocks (namely <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">tuning depth</span>) can effectively reduce the network cost and on-device training time.
(2) Apart from the depth, the bottleneck size (<span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">tuning width</span>), i.e., the target projection dimension of input needs to be carefully set as well.
A small width might not suffice to encode the latent features for fine-tuning tasks and thus incurs high accuracy degradation.
Yet, a too wide adapter incurs high resource costs and therefore slows down the training (i.e., increased time to accuracy).
Overall, the candidate depth spans from 0 to the number of transformer layers even if we only consider inserting adapters at top K consecutive layers, i.e., 12 in BERT, and the valid widths range from 8 to 64 according to our experiments<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The experiments refer to Table <a href="#S3.T2" title="Table 2 ‣ Decisions must be online ‣ 3.2. The Configuration Challenge ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which shows that the optimal adapter width ranges from 8 to 64 on four datasets.</span></span></span>.
That results in hundreds of different alternative configurations.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">Another dimension of design space is that the configuration can be switched across FL rounds during a training session.
<math id="S3.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS2.SSS0.Px1.p2.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS3" title="3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> elaborates how such switching could be realized with the knowledge learned by the old adapter configuration well preserved.
But within a round, clients better use the same configuration to facilitate the model aggregation.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Decisions must be online</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Making a good decision offline is difficult without pre-knowledge about the training dataset – a common setup in fine-tuning scenarios.
Even with the same task, with the data distribution drifting over time, the resultant model structure could differ tremendously <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib33" title="" class="ltx_ref">hou2020dynabert, </a>)</cite>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<td id="S3.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">
</span><span id="S3.T2.2.1.1.1.2" class="ltx_text" style="font-size:80%;">Model</span>
</td>
<td id="S3.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T2.2.1.1.2.1" class="ltx_text" style="font-size:80%;">Datasets</span></td>
<td id="S3.T2.2.1.1.3" class="ltx_td ltx_align_left ltx_border_t" colspan="5">
<span id="S3.T2.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.1.1.3.1.1" class="ltx_p"><span id="S3.T2.2.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Optimal adapter configuration </span><span id="S3.T2.2.1.1.3.1.1.2" class="ltx_text ltx_font_bold" style="font-size:80%;">(depth, width)</span><span id="S3.T2.2.1.1.3.1.1.3" class="ltx_text" style="font-size:80%;"></span></span>
<span id="S3.T2.2.1.1.3.1.2" class="ltx_p ltx_align_left"><span id="S3.T2.2.1.1.3.1.2.1" class="ltx_text" style="font-size:80%;">towards different target accuracy</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r" rowspan="4"><span id="S3.T2.2.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">BERT</span></td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">99%</span></td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95%</span></td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90%</span></td>
<td id="S3.T2.2.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80%</span></td>
<td id="S3.T2.2.2.2.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70%</span></td>
</tr>
<tr id="S3.T2.2.3.3" class="ltx_tr">
<td id="S3.T2.2.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20news</span></td>
<td id="S3.T2.2.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.3.3.2.1" class="ltx_text" style="font-size:80%;">(2,64)</span></td>
<td id="S3.T2.2.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.3.3.3.1" class="ltx_text" style="font-size:80%;">(2,32)</span></td>
<td id="S3.T2.2.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.3.3.4.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
<td id="S3.T2.2.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.3.3.5.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
<td id="S3.T2.2.3.3.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.3.3.6.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
</tr>
<tr id="S3.T2.2.4.4" class="ltx_tr">
<td id="S3.T2.2.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.4.4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">agnews</span></td>
<td id="S3.T2.2.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.4.4.2.1" class="ltx_text" style="font-size:80%;">(3,16)</span></td>
<td id="S3.T2.2.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.4.4.3.1" class="ltx_text" style="font-size:80%;">(2,16)</span></td>
<td id="S3.T2.2.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.4.4.4.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
<td id="S3.T2.2.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.4.4.5.1" class="ltx_text" style="font-size:80%;">(0,8)</span></td>
<td id="S3.T2.2.4.4.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.4.4.6.1" class="ltx_text" style="font-size:80%;">(0,8)</span></td>
</tr>
<tr id="S3.T2.2.5.5" class="ltx_tr">
<td id="S3.T2.2.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">semeval</span></td>
<td id="S3.T2.2.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.5.5.2.1" class="ltx_text" style="font-size:80%;">(10,8)</span></td>
<td id="S3.T2.2.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.5.5.3.1" class="ltx_text" style="font-size:80%;">(6,8)</span></td>
<td id="S3.T2.2.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.5.5.4.1" class="ltx_text" style="font-size:80%;">(6,8)</span></td>
<td id="S3.T2.2.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.2.5.5.5.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
<td id="S3.T2.2.5.5.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.2.5.5.6.1" class="ltx_text" style="font-size:80%;">(2,8)</span></td>
</tr>
<tr id="S3.T2.2.6.6" class="ltx_tr">
<td id="S3.T2.2.6.6.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S3.T2.2.6.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ontonotes</span></td>
<td id="S3.T2.2.6.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.6.6.3.1" class="ltx_text" style="font-size:80%;">(12, 32)</span></td>
<td id="S3.T2.2.6.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.6.6.4.1" class="ltx_text" style="font-size:80%;">(12, 32)</span></td>
<td id="S3.T2.2.6.6.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.6.6.5.1" class="ltx_text" style="font-size:80%;">(10, 32)</span></td>
<td id="S3.T2.2.6.6.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.6.6.6.1" class="ltx_text" style="font-size:80%;">(0, 16)</span></td>
<td id="S3.T2.2.6.6.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S3.T2.2.6.6.7.1" class="ltx_text" style="font-size:80%;">(0, 16)</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>. </span><span id="S3.T2.6.2" class="ltx_text" style="font-size:113%;">The optimal adapter configuration (i.e., best time-to-accuracy) for different target accuracy (ratio to the full convergence) and different datasets.
</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">No silver bullet configuration</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">A key observation we made from extensive experiments is that <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">there is no silver-bullet configuration for FedNLP tasks</span>.
Rather, the optimal configuration depends on many factors:
the specific NLP tasks, the target accuracy, and client resources such as network bandwidth and local execution speed.
Even with a given pre-trained model, there are many factors that affect which configuration shall be picked for the fastest convergence.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px3.p2.2" class="ltx_p"><math id="S3.SS2.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.1.m1.1a"><mo id="S3.SS2.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS2.SSS0.Px3.p2.2.1" class="ltx_text ltx_font_italic">Targeted accuracy.</span>
Within a training session, different target accuracy favors different configurations.
As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ No silver bullet configuration ‣ 3.2. The Configuration Challenge ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a), to achieve the best accuracy possible, using an adapter with depth 6 and width 16 is the best option.
If 80% relative accuracy is satisfactory, the adapter with depth 2 and width 8 is 2<math id="S3.SS2.SSS0.Px3.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.SSS0.Px3.p2.2.m2.1a"><mo id="S3.SS2.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p2.2.m2.1b"><times id="S3.SS2.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p2.2.m2.1c">\times</annotation></semantics></math> faster than the previous configuration.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x7.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="922" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F4.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">SEMEVAL</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x8.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="451" height="309" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F4.2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">20NEWS</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S3.F4.5.2" class="ltx_text" style="font-size:90%;">Across different target accuracy and FedNLP tasks, the optimal adapter configuration (depth, width) varies. Tested with BERT and Jetson TX2.</span></figcaption>
</figure>
<div id="S3.SS2.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px3.p3.1" class="ltx_p"><math id="S3.SS2.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.SSS0.Px3.p3.1.m1.1a"><mo id="S3.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p3.1.m1.1b"><ci id="S3.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS2.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_italic">Targeted NLP tasks.</span>
Across different FedNLP tasks, the optimal adapter configuration varies.
As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ No silver bullet configuration ‣ 3.2. The Configuration Challenge ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, using depth 2 and width 8 leads to fast convergence to 80% accuracy on <span id="S3.SS2.SSS0.Px3.p3.1.2" class="ltx_text ltx_font_typewriter">20NEWS</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib45" title="" class="ltx_ref">lang1995newsweeder, </a>)</cite>. However, on <span id="S3.SS2.SSS0.Px3.p3.1.3" class="ltx_text ltx_font_typewriter">SEMEVAL</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib31" title="" class="ltx_ref">hendrickx2019semeval, </a>)</cite>, the same configuration results in 10% lower convergence accuracy as compared to a more complex configuration.</p>
</div>
<div id="S3.SS2.SSS0.Px3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px3.p4.2" class="ltx_p"><math id="S3.SS2.SSS0.Px3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.SSS0.Px3.p4.1.m1.1a"><mo id="S3.SS2.SSS0.Px3.p4.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p4.1.m1.1b"><ci id="S3.SS2.SSS0.Px3.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS2.SSS0.Px3.p4.2.1" class="ltx_text ltx_font_italic">Client resources.</span>
Local clients’ training speed and network capacity also make a difference in adapter selection.
This is due to the disparate impacts of adapter depth/width on the computation and communication reduction.
For instance, a larger tuning depth linearly amplifies the communication and computation cost, while a larger tuning width linearly increases the communication cost but only adds negligible computation cost according to the analysis in <math id="S3.SS2.SSS0.Px3.p4.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS2.SSS0.Px3.p4.2.m2.1a"><mi mathvariant="normal" id="S3.SS2.SSS0.Px3.p4.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p4.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p4.2.m2.1b"><ci id="S3.SS2.SSS0.Px3.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p4.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p4.2.m2.1c">\S</annotation></semantics></math><a href="#S3.SS1" title="3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Why prior work is inadequate</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">A closely related technique is neural architecture search (NAS) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib21" title="" class="ltx_ref">elsken2019neural, </a>)</cite>, which automatically looks for the best model structures but with a totally different design goal.
Essentially, NAS sacrifices the time for good training accuracy, e.g., days to train a single model in a centralized manner <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib84" title="" class="ltx_ref">wei2022npenas, </a>)</cite>.
Instead, we pursue fast time-to-accuracy, which is a more practical and affordable setting for FedNLP developers.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>The Online Configurator</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We build a configurator that automatically adjusts the tuning depth and width throughout a training session.
The goal is fast model convergence: achieving the target model accuracy in the shortest time.
Our key ideas are twofold:
</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">Progressive training.</span>
The first key idea is, to begin with a shallow tuning configuration (i.e., small depth and width) to quickly boost the model accuracy.
When it encounters a “choke point” where more rounds of training no longer provide enough accuracy profit, it “upgrades” to a more complex configuration, i.e., either deeper or wider.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Such upgrading mechanism is inspired by curriculum learning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib11" title="" class="ltx_ref">bengio2009curriculum, </a>)</cite>, a learning strategy that trains a model beginning from easier data samples to harder ones.
Instead of altering the training samples, we propose to alter the model structure.
In the beginning, a simpler adapter configuration can learn fast.
This is because, by focusing on fewer compact trainable parameters closer to the model output, the model can rapidly learn the coarse-grained domain-specific knowledge for the downstream tasks, such as new class labels <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib65" title="" class="ltx_ref">ro2021autolr, </a>)</cite>.
For simple downstream tasks, fine-tuning without re-learning deep features is enough to obtain satisfactory model accuracy, e.g., depth 2 and width 64 for 20NEWS dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib45" title="" class="ltx_ref">lang1995newsweeder, </a>)</cite>.
As the training proceeds, the model encounters a “choke point” where the learning curve becomes gentle.
It demands deeper or wider adapters to learn new features.
The experiment results in Table <a href="#S3.T2" title="Table 2 ‣ Decisions must be online ‣ 3.2. The Configuration Challenge ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> attests to our claim that a higher target accuracy favors deeper and wider adapters.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.2" class="ltx_p"><math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mo id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS3.p4.2.1" class="ltx_text ltx_font_italic">Identifying timing and direction to upgrade configuration through sideline trials.</span>
The learning curve is fundamentally challenging to be estimated or predicted ahead of time.
How can a system possibly know the timing and to which direction to upgrade?
In this work, we propose an intuitive approach based on the concept of sideline trials.
Its key idea is to ask extra participant clients to attempt different configurations, and make a decision on whether and where to upgrade based on the tested accuracy of different directions.
In federated settings, such “extra clients” are common because the client-level parallelism of existing FL algorithms is notoriously low.
That is, limited by the learning theory <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib40" title="" class="ltx_ref">keskar2016large, </a>)</cite>, a small number of clients (i.e., 5 for <span id="S3.SS3.p4.2.2" class="ltx_text ltx_font_typewriter">20NEWS</span>) is enough to saturate the convergence performance (both accuracy and speed) and allocating more clients gives a negligible return.
As will be shown in <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi mathvariant="normal" id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\S</annotation></semantics></math><a href="#S6.SS2" title="6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>, using those extra clients for trial is much more beneficial than asking them to participate in training.</p>
</div>
<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.71" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.71.72" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.72.1.1.1" class="ltx_text" style="font-size:80%;">1</span></span>
<span id="algorithm1.71.72.2" class="ltx_text" style="font-size:80%;">



</span>
</div>
<div id="algorithm1.1.1" class="ltx_listingline">
<span id="algorithm1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">input :</span><span id="algorithm1.1.1.2" class="ltx_text" style="font-size:80%;"> 
Target accuracy, </span><math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="acc" display="inline"><semantics id="algorithm1.1.1.m1.1a"><mrow id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.1.1.m1.1.1.2" xref="algorithm1.1.1.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.1.1.m1.1.1.3" xref="algorithm1.1.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="algorithm1.1.1.m1.1.1.1a" xref="algorithm1.1.1.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.1.1.m1.1.1.4" xref="algorithm1.1.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1"><times id="algorithm1.1.1.m1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1"></times><ci id="algorithm1.1.1.m1.1.1.2.cmml" xref="algorithm1.1.1.m1.1.1.2">𝑎</ci><ci id="algorithm1.1.1.m1.1.1.3.cmml" xref="algorithm1.1.1.m1.1.1.3">𝑐</ci><ci id="algorithm1.1.1.m1.1.1.4.cmml" xref="algorithm1.1.1.m1.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">acc</annotation></semantics></math><span id="algorithm1.1.1.3" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span id="algorithm1.2.2.1" class="ltx_text" style="font-size:80%;">
Trial interval, </span><math id="algorithm1.2.2.m1.1" class="ltx_Math" alttext="trial\_intvl" display="inline"><semantics id="algorithm1.2.2.m1.1a"><mrow id="algorithm1.2.2.m1.1.1" xref="algorithm1.2.2.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.2" xref="algorithm1.2.2.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.3" xref="algorithm1.2.2.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1a" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.4" xref="algorithm1.2.2.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1b" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.5" xref="algorithm1.2.2.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1c" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.6" xref="algorithm1.2.2.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1d" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" mathvariant="normal" id="algorithm1.2.2.m1.1.1.7" xref="algorithm1.2.2.m1.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1e" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.8" xref="algorithm1.2.2.m1.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1f" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.9" xref="algorithm1.2.2.m1.1.1.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1g" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.10" xref="algorithm1.2.2.m1.1.1.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1h" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.11" xref="algorithm1.2.2.m1.1.1.11.cmml">v</mi><mo lspace="0em" rspace="0em" id="algorithm1.2.2.m1.1.1.1i" xref="algorithm1.2.2.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.2.2.m1.1.1.12" xref="algorithm1.2.2.m1.1.1.12.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m1.1b"><apply id="algorithm1.2.2.m1.1.1.cmml" xref="algorithm1.2.2.m1.1.1"><times id="algorithm1.2.2.m1.1.1.1.cmml" xref="algorithm1.2.2.m1.1.1.1"></times><ci id="algorithm1.2.2.m1.1.1.2.cmml" xref="algorithm1.2.2.m1.1.1.2">𝑡</ci><ci id="algorithm1.2.2.m1.1.1.3.cmml" xref="algorithm1.2.2.m1.1.1.3">𝑟</ci><ci id="algorithm1.2.2.m1.1.1.4.cmml" xref="algorithm1.2.2.m1.1.1.4">𝑖</ci><ci id="algorithm1.2.2.m1.1.1.5.cmml" xref="algorithm1.2.2.m1.1.1.5">𝑎</ci><ci id="algorithm1.2.2.m1.1.1.6.cmml" xref="algorithm1.2.2.m1.1.1.6">𝑙</ci><ci id="algorithm1.2.2.m1.1.1.7.cmml" xref="algorithm1.2.2.m1.1.1.7">_</ci><ci id="algorithm1.2.2.m1.1.1.8.cmml" xref="algorithm1.2.2.m1.1.1.8">𝑖</ci><ci id="algorithm1.2.2.m1.1.1.9.cmml" xref="algorithm1.2.2.m1.1.1.9">𝑛</ci><ci id="algorithm1.2.2.m1.1.1.10.cmml" xref="algorithm1.2.2.m1.1.1.10">𝑡</ci><ci id="algorithm1.2.2.m1.1.1.11.cmml" xref="algorithm1.2.2.m1.1.1.11">𝑣</ci><ci id="algorithm1.2.2.m1.1.1.12.cmml" xref="algorithm1.2.2.m1.1.1.12">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m1.1c">trial\_intvl</annotation></semantics></math><span id="algorithm1.2.2.2" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span id="algorithm1.4.4.1" class="ltx_text" style="font-size:80%;">
Start-up depth and width, </span><math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><msub id="algorithm1.3.3.m1.1.1" xref="algorithm1.3.3.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.3.3.m1.1.1.2" xref="algorithm1.3.3.m1.1.1.2.cmml">D</mi><mn mathsize="80%" id="algorithm1.3.3.m1.1.1.3" xref="algorithm1.3.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.1.cmml" xref="algorithm1.3.3.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1">subscript</csymbol><ci id="algorithm1.3.3.m1.1.1.2.cmml" xref="algorithm1.3.3.m1.1.1.2">𝐷</ci><cn type="integer" id="algorithm1.3.3.m1.1.1.3.cmml" xref="algorithm1.3.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">D_{0}</annotation></semantics></math><span id="algorithm1.4.4.2" class="ltx_text" style="font-size:80%;"> and </span><math id="algorithm1.4.4.m2.1" class="ltx_Math" alttext="W_{0}" display="inline"><semantics id="algorithm1.4.4.m2.1a"><msub id="algorithm1.4.4.m2.1.1" xref="algorithm1.4.4.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.4.4.m2.1.1.2" xref="algorithm1.4.4.m2.1.1.2.cmml">W</mi><mn mathsize="80%" id="algorithm1.4.4.m2.1.1.3" xref="algorithm1.4.4.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m2.1b"><apply id="algorithm1.4.4.m2.1.1.cmml" xref="algorithm1.4.4.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.4.4.m2.1.1.1.cmml" xref="algorithm1.4.4.m2.1.1">subscript</csymbol><ci id="algorithm1.4.4.m2.1.1.2.cmml" xref="algorithm1.4.4.m2.1.1.2">𝑊</ci><cn type="integer" id="algorithm1.4.4.m2.1.1.3.cmml" xref="algorithm1.4.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m2.1c">W_{0}</annotation></semantics></math><span id="algorithm1.4.4.3" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.6.6" class="ltx_listingline">
<span id="algorithm1.6.6.1" class="ltx_text" style="font-size:80%;">
Step of depth and width, </span><math id="algorithm1.5.5.m1.1" class="ltx_Math" alttext="S_{d}" display="inline"><semantics id="algorithm1.5.5.m1.1a"><msub id="algorithm1.5.5.m1.1.1" xref="algorithm1.5.5.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.5.5.m1.1.1.2" xref="algorithm1.5.5.m1.1.1.2.cmml">S</mi><mi mathsize="80%" id="algorithm1.5.5.m1.1.1.3" xref="algorithm1.5.5.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.1b"><apply id="algorithm1.5.5.m1.1.1.cmml" xref="algorithm1.5.5.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1">subscript</csymbol><ci id="algorithm1.5.5.m1.1.1.2.cmml" xref="algorithm1.5.5.m1.1.1.2">𝑆</ci><ci id="algorithm1.5.5.m1.1.1.3.cmml" xref="algorithm1.5.5.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.1c">S_{d}</annotation></semantics></math><span id="algorithm1.6.6.2" class="ltx_text" style="font-size:80%;"> and </span><math id="algorithm1.6.6.m2.1" class="ltx_Math" alttext="S_{w}" display="inline"><semantics id="algorithm1.6.6.m2.1a"><msub id="algorithm1.6.6.m2.1.1" xref="algorithm1.6.6.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.6.6.m2.1.1.2" xref="algorithm1.6.6.m2.1.1.2.cmml">S</mi><mi mathsize="80%" id="algorithm1.6.6.m2.1.1.3" xref="algorithm1.6.6.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m2.1b"><apply id="algorithm1.6.6.m2.1.1.cmml" xref="algorithm1.6.6.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.6.6.m2.1.1.1.cmml" xref="algorithm1.6.6.m2.1.1">subscript</csymbol><ci id="algorithm1.6.6.m2.1.1.2.cmml" xref="algorithm1.6.6.m2.1.1.2">𝑆</ci><ci id="algorithm1.6.6.m2.1.1.3.cmml" xref="algorithm1.6.6.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m2.1c">S_{w}</annotation></semantics></math><span id="algorithm1.6.6.3" class="ltx_text" style="font-size:80%;">.
</span>
</div>
<div id="algorithm1.71.73" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.73.1.1.1" class="ltx_text" style="font-size:80%;">2</span></span><span id="algorithm1.71.73.2" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="algorithm1.7.7" class="ltx_listingline">
<span id="algorithm1.7.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">output :</span><span id="algorithm1.7.7.2" class="ltx_text" style="font-size:80%;"> 
Fine-tuned adapter weights, </span><math id="algorithm1.7.7.m1.1" class="ltx_Math" alttext="\Theta_{i}" display="inline"><semantics id="algorithm1.7.7.m1.1a"><msub id="algorithm1.7.7.m1.1.1" xref="algorithm1.7.7.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.7.7.m1.1.1.2" xref="algorithm1.7.7.m1.1.1.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.7.7.m1.1.1.3" xref="algorithm1.7.7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m1.1b"><apply id="algorithm1.7.7.m1.1.1.cmml" xref="algorithm1.7.7.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.1.1.1.cmml" xref="algorithm1.7.7.m1.1.1">subscript</csymbol><ci id="algorithm1.7.7.m1.1.1.2.cmml" xref="algorithm1.7.7.m1.1.1.2">Θ</ci><ci id="algorithm1.7.7.m1.1.1.3.cmml" xref="algorithm1.7.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m1.1c">\Theta_{i}</annotation></semantics></math><span id="algorithm1.7.7.3" class="ltx_text" style="font-size:80%;"> (i=1,2…).
</span>
</div>
<div id="algorithm1.71.74" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.74.1.1.1" class="ltx_text" style="font-size:80%;">3</span></span><span id="algorithm1.71.74.2" class="ltx_text" style="font-size:80%;">
</span>
</div>
<div id="algorithm1.71.75" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.75.1.1.1" class="ltx_text" style="font-size:80%;">4</span></span><span id="algorithm1.71.75.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Function</span><span id="algorithm1.71.75.3" class="ltx_text" style="font-size:80%;"> </span><em id="algorithm1.71.75.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Cloud_controller():</em><span id="algorithm1.71.75.5" class="ltx_text ltx_font_bold" style="font-size:80%;"> </span>
</div>
<div id="algorithm1.12.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.12.12.1.1.1" class="ltx_text" style="font-size:80%;">5</span></span><span id="algorithm1.12.12.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.12.12.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.12.12.4" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.8.8.m1.1" class="ltx_Math" alttext="Trial_{0}" display="inline"><semantics id="algorithm1.8.8.m1.1a"><mrow id="algorithm1.8.8.m1.1.1" xref="algorithm1.8.8.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.8.8.m1.1.1.2" xref="algorithm1.8.8.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.1.1.1" xref="algorithm1.8.8.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.8.8.m1.1.1.3" xref="algorithm1.8.8.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.1.1.1a" xref="algorithm1.8.8.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.8.8.m1.1.1.4" xref="algorithm1.8.8.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.1.1.1b" xref="algorithm1.8.8.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.8.8.m1.1.1.5" xref="algorithm1.8.8.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.1.1.1c" xref="algorithm1.8.8.m1.1.1.1.cmml">​</mo><msub id="algorithm1.8.8.m1.1.1.6" xref="algorithm1.8.8.m1.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.8.8.m1.1.1.6.2" xref="algorithm1.8.8.m1.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.8.8.m1.1.1.6.3" xref="algorithm1.8.8.m1.1.1.6.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.1b"><apply id="algorithm1.8.8.m1.1.1.cmml" xref="algorithm1.8.8.m1.1.1"><times id="algorithm1.8.8.m1.1.1.1.cmml" xref="algorithm1.8.8.m1.1.1.1"></times><ci id="algorithm1.8.8.m1.1.1.2.cmml" xref="algorithm1.8.8.m1.1.1.2">𝑇</ci><ci id="algorithm1.8.8.m1.1.1.3.cmml" xref="algorithm1.8.8.m1.1.1.3">𝑟</ci><ci id="algorithm1.8.8.m1.1.1.4.cmml" xref="algorithm1.8.8.m1.1.1.4">𝑖</ci><ci id="algorithm1.8.8.m1.1.1.5.cmml" xref="algorithm1.8.8.m1.1.1.5">𝑎</ci><apply id="algorithm1.8.8.m1.1.1.6.cmml" xref="algorithm1.8.8.m1.1.1.6"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.1.1.6.1.cmml" xref="algorithm1.8.8.m1.1.1.6">subscript</csymbol><ci id="algorithm1.8.8.m1.1.1.6.2.cmml" xref="algorithm1.8.8.m1.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.8.8.m1.1.1.6.3.cmml" xref="algorithm1.8.8.m1.1.1.6.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.1c">Trial_{0}</annotation></semantics></math><span id="algorithm1.12.12.5" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.9.9.m2.1" class="ltx_Math" alttext="Trial_{1}" display="inline"><semantics id="algorithm1.9.9.m2.1a"><mrow id="algorithm1.9.9.m2.1.1" xref="algorithm1.9.9.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.9.9.m2.1.1.2" xref="algorithm1.9.9.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.9.9.m2.1.1.1" xref="algorithm1.9.9.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.9.9.m2.1.1.3" xref="algorithm1.9.9.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.9.9.m2.1.1.1a" xref="algorithm1.9.9.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.9.9.m2.1.1.4" xref="algorithm1.9.9.m2.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.9.9.m2.1.1.1b" xref="algorithm1.9.9.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.9.9.m2.1.1.5" xref="algorithm1.9.9.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.9.9.m2.1.1.1c" xref="algorithm1.9.9.m2.1.1.1.cmml">​</mo><msub id="algorithm1.9.9.m2.1.1.6" xref="algorithm1.9.9.m2.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.9.9.m2.1.1.6.2" xref="algorithm1.9.9.m2.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.9.9.m2.1.1.6.3" xref="algorithm1.9.9.m2.1.1.6.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m2.1b"><apply id="algorithm1.9.9.m2.1.1.cmml" xref="algorithm1.9.9.m2.1.1"><times id="algorithm1.9.9.m2.1.1.1.cmml" xref="algorithm1.9.9.m2.1.1.1"></times><ci id="algorithm1.9.9.m2.1.1.2.cmml" xref="algorithm1.9.9.m2.1.1.2">𝑇</ci><ci id="algorithm1.9.9.m2.1.1.3.cmml" xref="algorithm1.9.9.m2.1.1.3">𝑟</ci><ci id="algorithm1.9.9.m2.1.1.4.cmml" xref="algorithm1.9.9.m2.1.1.4">𝑖</ci><ci id="algorithm1.9.9.m2.1.1.5.cmml" xref="algorithm1.9.9.m2.1.1.5">𝑎</ci><apply id="algorithm1.9.9.m2.1.1.6.cmml" xref="algorithm1.9.9.m2.1.1.6"><csymbol cd="ambiguous" id="algorithm1.9.9.m2.1.1.6.1.cmml" xref="algorithm1.9.9.m2.1.1.6">subscript</csymbol><ci id="algorithm1.9.9.m2.1.1.6.2.cmml" xref="algorithm1.9.9.m2.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.9.9.m2.1.1.6.3.cmml" xref="algorithm1.9.9.m2.1.1.6.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m2.1c">Trial_{1}</annotation></semantics></math><span id="algorithm1.12.12.6" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.10.10.m3.1" class="ltx_Math" alttext="Trial_{2}" display="inline"><semantics id="algorithm1.10.10.m3.1a"><mrow id="algorithm1.10.10.m3.1.1" xref="algorithm1.10.10.m3.1.1.cmml"><mi mathsize="80%" id="algorithm1.10.10.m3.1.1.2" xref="algorithm1.10.10.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.10.10.m3.1.1.1" xref="algorithm1.10.10.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.10.10.m3.1.1.3" xref="algorithm1.10.10.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.10.10.m3.1.1.1a" xref="algorithm1.10.10.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.10.10.m3.1.1.4" xref="algorithm1.10.10.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.10.10.m3.1.1.1b" xref="algorithm1.10.10.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.10.10.m3.1.1.5" xref="algorithm1.10.10.m3.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.10.10.m3.1.1.1c" xref="algorithm1.10.10.m3.1.1.1.cmml">​</mo><msub id="algorithm1.10.10.m3.1.1.6" xref="algorithm1.10.10.m3.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.10.10.m3.1.1.6.2" xref="algorithm1.10.10.m3.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.10.10.m3.1.1.6.3" xref="algorithm1.10.10.m3.1.1.6.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m3.1b"><apply id="algorithm1.10.10.m3.1.1.cmml" xref="algorithm1.10.10.m3.1.1"><times id="algorithm1.10.10.m3.1.1.1.cmml" xref="algorithm1.10.10.m3.1.1.1"></times><ci id="algorithm1.10.10.m3.1.1.2.cmml" xref="algorithm1.10.10.m3.1.1.2">𝑇</ci><ci id="algorithm1.10.10.m3.1.1.3.cmml" xref="algorithm1.10.10.m3.1.1.3">𝑟</ci><ci id="algorithm1.10.10.m3.1.1.4.cmml" xref="algorithm1.10.10.m3.1.1.4">𝑖</ci><ci id="algorithm1.10.10.m3.1.1.5.cmml" xref="algorithm1.10.10.m3.1.1.5">𝑎</ci><apply id="algorithm1.10.10.m3.1.1.6.cmml" xref="algorithm1.10.10.m3.1.1.6"><csymbol cd="ambiguous" id="algorithm1.10.10.m3.1.1.6.1.cmml" xref="algorithm1.10.10.m3.1.1.6">subscript</csymbol><ci id="algorithm1.10.10.m3.1.1.6.2.cmml" xref="algorithm1.10.10.m3.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.10.10.m3.1.1.6.3.cmml" xref="algorithm1.10.10.m3.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m3.1c">Trial_{2}</annotation></semantics></math><span id="algorithm1.12.12.7" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.11.11.m4.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.11.11.m4.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.11.11.m4.1.1" xref="algorithm1.11.11.m4.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m4.1b"><ci id="algorithm1.11.11.m4.1.1.cmml" xref="algorithm1.11.11.m4.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m4.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.12.12.8" class="ltx_text" style="font-size:80%;"> selects </span><math id="algorithm1.12.12.m5.1" class="ltx_Math" alttext="3N" display="inline"><semantics id="algorithm1.12.12.m5.1a"><mrow id="algorithm1.12.12.m5.1.1" xref="algorithm1.12.12.m5.1.1.cmml"><mn mathsize="80%" id="algorithm1.12.12.m5.1.1.2" xref="algorithm1.12.12.m5.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="algorithm1.12.12.m5.1.1.1" xref="algorithm1.12.12.m5.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.12.12.m5.1.1.3" xref="algorithm1.12.12.m5.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m5.1b"><apply id="algorithm1.12.12.m5.1.1.cmml" xref="algorithm1.12.12.m5.1.1"><times id="algorithm1.12.12.m5.1.1.1.cmml" xref="algorithm1.12.12.m5.1.1.1"></times><cn type="integer" id="algorithm1.12.12.m5.1.1.2.cmml" xref="algorithm1.12.12.m5.1.1.2">3</cn><ci id="algorithm1.12.12.m5.1.1.3.cmml" xref="algorithm1.12.12.m5.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m5.1c">3N</annotation></semantics></math><span id="algorithm1.12.12.9" class="ltx_text" style="font-size:80%;"> clients;</span>
</div>
<div id="algorithm1.71.76" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.76.1.1.1" class="ltx_text" style="font-size:80%;">6</span></span><span id="algorithm1.71.76.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.76.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.76.4" class="ltx_text" style="font-size:80%;">
Iteration i=0;</span>
</div>
<div id="algorithm1.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.14.14.1.1.1" class="ltx_text" style="font-size:80%;">7</span></span><span id="algorithm1.14.14.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.14.14.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.14.14.4" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.13.13.m1.1" class="ltx_Math" alttext="T_{trial}" display="inline"><semantics id="algorithm1.13.13.m1.1a"><msub id="algorithm1.13.13.m1.1.1" xref="algorithm1.13.13.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.2" xref="algorithm1.13.13.m1.1.1.2.cmml">T</mi><mrow id="algorithm1.13.13.m1.1.1.3" xref="algorithm1.13.13.m1.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.3.2" xref="algorithm1.13.13.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.13.13.m1.1.1.3.1" xref="algorithm1.13.13.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.3.3" xref="algorithm1.13.13.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.13.13.m1.1.1.3.1a" xref="algorithm1.13.13.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.3.4" xref="algorithm1.13.13.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.13.13.m1.1.1.3.1b" xref="algorithm1.13.13.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.3.5" xref="algorithm1.13.13.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.13.13.m1.1.1.3.1c" xref="algorithm1.13.13.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.13.13.m1.1.1.3.6" xref="algorithm1.13.13.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m1.1b"><apply id="algorithm1.13.13.m1.1.1.cmml" xref="algorithm1.13.13.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m1.1.1.1.cmml" xref="algorithm1.13.13.m1.1.1">subscript</csymbol><ci id="algorithm1.13.13.m1.1.1.2.cmml" xref="algorithm1.13.13.m1.1.1.2">𝑇</ci><apply id="algorithm1.13.13.m1.1.1.3.cmml" xref="algorithm1.13.13.m1.1.1.3"><times id="algorithm1.13.13.m1.1.1.3.1.cmml" xref="algorithm1.13.13.m1.1.1.3.1"></times><ci id="algorithm1.13.13.m1.1.1.3.2.cmml" xref="algorithm1.13.13.m1.1.1.3.2">𝑡</ci><ci id="algorithm1.13.13.m1.1.1.3.3.cmml" xref="algorithm1.13.13.m1.1.1.3.3">𝑟</ci><ci id="algorithm1.13.13.m1.1.1.3.4.cmml" xref="algorithm1.13.13.m1.1.1.3.4">𝑖</ci><ci id="algorithm1.13.13.m1.1.1.3.5.cmml" xref="algorithm1.13.13.m1.1.1.3.5">𝑎</ci><ci id="algorithm1.13.13.m1.1.1.3.6.cmml" xref="algorithm1.13.13.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m1.1c">T_{trial}</annotation></semantics></math><span id="algorithm1.14.14.5" class="ltx_text" style="font-size:80%;">=</span><math id="algorithm1.14.14.m2.1" class="ltx_Math" alttext="T_{now}" display="inline"><semantics id="algorithm1.14.14.m2.1a"><msub id="algorithm1.14.14.m2.1.1" xref="algorithm1.14.14.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.14.14.m2.1.1.2" xref="algorithm1.14.14.m2.1.1.2.cmml">T</mi><mrow id="algorithm1.14.14.m2.1.1.3" xref="algorithm1.14.14.m2.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.14.14.m2.1.1.3.2" xref="algorithm1.14.14.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="algorithm1.14.14.m2.1.1.3.1" xref="algorithm1.14.14.m2.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.14.14.m2.1.1.3.3" xref="algorithm1.14.14.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="algorithm1.14.14.m2.1.1.3.1a" xref="algorithm1.14.14.m2.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.14.14.m2.1.1.3.4" xref="algorithm1.14.14.m2.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m2.1b"><apply id="algorithm1.14.14.m2.1.1.cmml" xref="algorithm1.14.14.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.14.14.m2.1.1.1.cmml" xref="algorithm1.14.14.m2.1.1">subscript</csymbol><ci id="algorithm1.14.14.m2.1.1.2.cmml" xref="algorithm1.14.14.m2.1.1.2">𝑇</ci><apply id="algorithm1.14.14.m2.1.1.3.cmml" xref="algorithm1.14.14.m2.1.1.3"><times id="algorithm1.14.14.m2.1.1.3.1.cmml" xref="algorithm1.14.14.m2.1.1.3.1"></times><ci id="algorithm1.14.14.m2.1.1.3.2.cmml" xref="algorithm1.14.14.m2.1.1.3.2">𝑛</ci><ci id="algorithm1.14.14.m2.1.1.3.3.cmml" xref="algorithm1.14.14.m2.1.1.3.3">𝑜</ci><ci id="algorithm1.14.14.m2.1.1.3.4.cmml" xref="algorithm1.14.14.m2.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m2.1c">T_{now}</annotation></semantics></math><span id="algorithm1.14.14.6" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.71.77" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.77.1.1.1" class="ltx_text" style="font-size:80%;">8</span></span><span id="algorithm1.71.77.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.77.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.77.4" class="ltx_text" style="font-size:80%;">
Dispatch(0); // Init model and trigger client training</span>
</div>
<div id="algorithm1.16.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.16.16.3.1.1" class="ltx_text" style="font-size:80%;">9</span></span><span id="algorithm1.16.16.4" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.16.16.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.16.16.6" class="ltx_text" style="font-size:80%;">
</span><span id="algorithm1.16.16.7" class="ltx_text ltx_font_bold" style="font-size:80%;">while</span><span id="algorithm1.16.16.8" class="ltx_text" style="font-size:80%;"> </span><em id="algorithm1.16.16.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Eval() <math id="algorithm1.15.15.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="algorithm1.15.15.1.m1.1a"><mo id="algorithm1.15.15.1.m1.1.1" xref="algorithm1.15.15.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.1.m1.1b"><lt id="algorithm1.15.15.1.m1.1.1.cmml" xref="algorithm1.15.15.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.1.m1.1c">&lt;</annotation></semantics></math> <math id="algorithm1.16.16.2.m2.1" class="ltx_Math" alttext="acc" display="inline"><semantics id="algorithm1.16.16.2.m2.1a"><mrow id="algorithm1.16.16.2.m2.1.1" xref="algorithm1.16.16.2.m2.1.1.cmml"><mi id="algorithm1.16.16.2.m2.1.1.2" xref="algorithm1.16.16.2.m2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.2.m2.1.1.1" xref="algorithm1.16.16.2.m2.1.1.1.cmml">​</mo><mi id="algorithm1.16.16.2.m2.1.1.3" xref="algorithm1.16.16.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="algorithm1.16.16.2.m2.1.1.1a" xref="algorithm1.16.16.2.m2.1.1.1.cmml">​</mo><mi id="algorithm1.16.16.2.m2.1.1.4" xref="algorithm1.16.16.2.m2.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.2.m2.1b"><apply id="algorithm1.16.16.2.m2.1.1.cmml" xref="algorithm1.16.16.2.m2.1.1"><times id="algorithm1.16.16.2.m2.1.1.1.cmml" xref="algorithm1.16.16.2.m2.1.1.1"></times><ci id="algorithm1.16.16.2.m2.1.1.2.cmml" xref="algorithm1.16.16.2.m2.1.1.2">𝑎</ci><ci id="algorithm1.16.16.2.m2.1.1.3.cmml" xref="algorithm1.16.16.2.m2.1.1.3">𝑐</ci><ci id="algorithm1.16.16.2.m2.1.1.4.cmml" xref="algorithm1.16.16.2.m2.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.2.m2.1c">acc</annotation></semantics></math></em><span id="algorithm1.16.16.9" class="ltx_text" style="font-size:80%;"> </span><span id="algorithm1.16.16.10" class="ltx_text ltx_font_bold" style="font-size:80%;">do</span>
</div>
<div id="algorithm1.71.78" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.78.1.1.1" class="ltx_text" style="font-size:80%;">10</span></span><span id="algorithm1.71.78.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.78.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.78.4" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.78.5" class="ltx_text" style="font-size:80%;">
i++;</span>
</div>
<div id="algorithm1.19.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.19.19.1.1.1" class="ltx_text" style="font-size:80%;">11</span></span><span id="algorithm1.19.19.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.19.19.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.19.19.4" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.19.19.5" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.17.17.m1.1" class="ltx_Math" alttext="\Theta^{k}_{i}(n)" display="inline"><semantics id="algorithm1.17.17.m1.1a"><mrow id="algorithm1.17.17.m1.1.2" xref="algorithm1.17.17.m1.1.2.cmml"><msubsup id="algorithm1.17.17.m1.1.2.2" xref="algorithm1.17.17.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.17.17.m1.1.2.2.2.2" xref="algorithm1.17.17.m1.1.2.2.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.17.17.m1.1.2.2.3" xref="algorithm1.17.17.m1.1.2.2.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.17.17.m1.1.2.2.2.3" xref="algorithm1.17.17.m1.1.2.2.2.3.cmml">k</mi></msubsup><mo lspace="0em" rspace="0em" id="algorithm1.17.17.m1.1.2.1" xref="algorithm1.17.17.m1.1.2.1.cmml">​</mo><mrow id="algorithm1.17.17.m1.1.2.3.2" xref="algorithm1.17.17.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="algorithm1.17.17.m1.1.2.3.2.1" xref="algorithm1.17.17.m1.1.2.cmml">(</mo><mi mathsize="80%" id="algorithm1.17.17.m1.1.1" xref="algorithm1.17.17.m1.1.1.cmml">n</mi><mo maxsize="80%" minsize="80%" id="algorithm1.17.17.m1.1.2.3.2.2" xref="algorithm1.17.17.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.1b"><apply id="algorithm1.17.17.m1.1.2.cmml" xref="algorithm1.17.17.m1.1.2"><times id="algorithm1.17.17.m1.1.2.1.cmml" xref="algorithm1.17.17.m1.1.2.1"></times><apply id="algorithm1.17.17.m1.1.2.2.cmml" xref="algorithm1.17.17.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.2.2.1.cmml" xref="algorithm1.17.17.m1.1.2.2">subscript</csymbol><apply id="algorithm1.17.17.m1.1.2.2.2.cmml" xref="algorithm1.17.17.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.2.2.2.1.cmml" xref="algorithm1.17.17.m1.1.2.2">superscript</csymbol><ci id="algorithm1.17.17.m1.1.2.2.2.2.cmml" xref="algorithm1.17.17.m1.1.2.2.2.2">Θ</ci><ci id="algorithm1.17.17.m1.1.2.2.2.3.cmml" xref="algorithm1.17.17.m1.1.2.2.2.3">𝑘</ci></apply><ci id="algorithm1.17.17.m1.1.2.2.3.cmml" xref="algorithm1.17.17.m1.1.2.2.3">𝑖</ci></apply><ci id="algorithm1.17.17.m1.1.1.cmml" xref="algorithm1.17.17.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.1c">\Theta^{k}_{i}(n)</annotation></semantics></math><span id="algorithm1.19.19.6" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.18.18.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.18.18.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.18.18.m2.1.1" xref="algorithm1.18.18.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m2.1b"><ci id="algorithm1.18.18.m2.1.1.cmml" xref="algorithm1.18.18.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.19.19.7" class="ltx_text" style="font-size:80%;"> Receive updated adapters from </span><math id="algorithm1.19.19.m3.1" class="ltx_Math" alttext="Trial_{k}" display="inline"><semantics id="algorithm1.19.19.m3.1a"><mrow id="algorithm1.19.19.m3.1.1" xref="algorithm1.19.19.m3.1.1.cmml"><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.2" xref="algorithm1.19.19.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.19.19.m3.1.1.1" xref="algorithm1.19.19.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.3" xref="algorithm1.19.19.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.19.19.m3.1.1.1a" xref="algorithm1.19.19.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.4" xref="algorithm1.19.19.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.19.19.m3.1.1.1b" xref="algorithm1.19.19.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.5" xref="algorithm1.19.19.m3.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.19.19.m3.1.1.1c" xref="algorithm1.19.19.m3.1.1.1.cmml">​</mo><msub id="algorithm1.19.19.m3.1.1.6" xref="algorithm1.19.19.m3.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.6.2" xref="algorithm1.19.19.m3.1.1.6.2.cmml">l</mi><mi mathsize="80%" id="algorithm1.19.19.m3.1.1.6.3" xref="algorithm1.19.19.m3.1.1.6.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.m3.1b"><apply id="algorithm1.19.19.m3.1.1.cmml" xref="algorithm1.19.19.m3.1.1"><times id="algorithm1.19.19.m3.1.1.1.cmml" xref="algorithm1.19.19.m3.1.1.1"></times><ci id="algorithm1.19.19.m3.1.1.2.cmml" xref="algorithm1.19.19.m3.1.1.2">𝑇</ci><ci id="algorithm1.19.19.m3.1.1.3.cmml" xref="algorithm1.19.19.m3.1.1.3">𝑟</ci><ci id="algorithm1.19.19.m3.1.1.4.cmml" xref="algorithm1.19.19.m3.1.1.4">𝑖</ci><ci id="algorithm1.19.19.m3.1.1.5.cmml" xref="algorithm1.19.19.m3.1.1.5">𝑎</ci><apply id="algorithm1.19.19.m3.1.1.6.cmml" xref="algorithm1.19.19.m3.1.1.6"><csymbol cd="ambiguous" id="algorithm1.19.19.m3.1.1.6.1.cmml" xref="algorithm1.19.19.m3.1.1.6">subscript</csymbol><ci id="algorithm1.19.19.m3.1.1.6.2.cmml" xref="algorithm1.19.19.m3.1.1.6.2">𝑙</ci><ci id="algorithm1.19.19.m3.1.1.6.3.cmml" xref="algorithm1.19.19.m3.1.1.6.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.m3.1c">Trial_{k}</annotation></semantics></math><span id="algorithm1.19.19.8" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.22.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.22.22.1.1.1" class="ltx_text" style="font-size:80%;">12</span></span><span id="algorithm1.22.22.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.22.22.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.22.22.4" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.22.22.5" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.20.20.m1.1" class="ltx_Math" alttext="\Theta^{k}_{i}" display="inline"><semantics id="algorithm1.20.20.m1.1a"><msubsup id="algorithm1.20.20.m1.1.1" xref="algorithm1.20.20.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.20.20.m1.1.1.2.2" xref="algorithm1.20.20.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.20.20.m1.1.1.3" xref="algorithm1.20.20.m1.1.1.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.20.20.m1.1.1.2.3" xref="algorithm1.20.20.m1.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m1.1b"><apply id="algorithm1.20.20.m1.1.1.cmml" xref="algorithm1.20.20.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.1.1.cmml" xref="algorithm1.20.20.m1.1.1">subscript</csymbol><apply id="algorithm1.20.20.m1.1.1.2.cmml" xref="algorithm1.20.20.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.1.2.1.cmml" xref="algorithm1.20.20.m1.1.1">superscript</csymbol><ci id="algorithm1.20.20.m1.1.1.2.2.cmml" xref="algorithm1.20.20.m1.1.1.2.2">Θ</ci><ci id="algorithm1.20.20.m1.1.1.2.3.cmml" xref="algorithm1.20.20.m1.1.1.2.3">𝑘</ci></apply><ci id="algorithm1.20.20.m1.1.1.3.cmml" xref="algorithm1.20.20.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m1.1c">\Theta^{k}_{i}</annotation></semantics></math><span id="algorithm1.22.22.6" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.21.21.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.21.21.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.21.21.m2.1.1" xref="algorithm1.21.21.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m2.1b"><ci id="algorithm1.21.21.m2.1.1.cmml" xref="algorithm1.21.21.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.22.22.7" class="ltx_text" style="font-size:80%;"> Fedavg(</span><math id="algorithm1.22.22.m3.1" class="ltx_Math" alttext="\Theta^{k}_{i}(n)" display="inline"><semantics id="algorithm1.22.22.m3.1a"><mrow id="algorithm1.22.22.m3.1.2" xref="algorithm1.22.22.m3.1.2.cmml"><msubsup id="algorithm1.22.22.m3.1.2.2" xref="algorithm1.22.22.m3.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.22.22.m3.1.2.2.2.2" xref="algorithm1.22.22.m3.1.2.2.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.22.22.m3.1.2.2.3" xref="algorithm1.22.22.m3.1.2.2.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.22.22.m3.1.2.2.2.3" xref="algorithm1.22.22.m3.1.2.2.2.3.cmml">k</mi></msubsup><mo lspace="0em" rspace="0em" id="algorithm1.22.22.m3.1.2.1" xref="algorithm1.22.22.m3.1.2.1.cmml">​</mo><mrow id="algorithm1.22.22.m3.1.2.3.2" xref="algorithm1.22.22.m3.1.2.cmml"><mo maxsize="80%" minsize="80%" id="algorithm1.22.22.m3.1.2.3.2.1" xref="algorithm1.22.22.m3.1.2.cmml">(</mo><mi mathsize="80%" id="algorithm1.22.22.m3.1.1" xref="algorithm1.22.22.m3.1.1.cmml">n</mi><mo maxsize="80%" minsize="80%" id="algorithm1.22.22.m3.1.2.3.2.2" xref="algorithm1.22.22.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.22.22.m3.1b"><apply id="algorithm1.22.22.m3.1.2.cmml" xref="algorithm1.22.22.m3.1.2"><times id="algorithm1.22.22.m3.1.2.1.cmml" xref="algorithm1.22.22.m3.1.2.1"></times><apply id="algorithm1.22.22.m3.1.2.2.cmml" xref="algorithm1.22.22.m3.1.2.2"><csymbol cd="ambiguous" id="algorithm1.22.22.m3.1.2.2.1.cmml" xref="algorithm1.22.22.m3.1.2.2">subscript</csymbol><apply id="algorithm1.22.22.m3.1.2.2.2.cmml" xref="algorithm1.22.22.m3.1.2.2"><csymbol cd="ambiguous" id="algorithm1.22.22.m3.1.2.2.2.1.cmml" xref="algorithm1.22.22.m3.1.2.2">superscript</csymbol><ci id="algorithm1.22.22.m3.1.2.2.2.2.cmml" xref="algorithm1.22.22.m3.1.2.2.2.2">Θ</ci><ci id="algorithm1.22.22.m3.1.2.2.2.3.cmml" xref="algorithm1.22.22.m3.1.2.2.2.3">𝑘</ci></apply><ci id="algorithm1.22.22.m3.1.2.2.3.cmml" xref="algorithm1.22.22.m3.1.2.2.3">𝑖</ci></apply><ci id="algorithm1.22.22.m3.1.1.cmml" xref="algorithm1.22.22.m3.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.22.22.m3.1c">\Theta^{k}_{i}(n)</annotation></semantics></math><span id="algorithm1.22.22.8" class="ltx_text" style="font-size:80%;">);</span>
</div>
<div id="algorithm1.27.27" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.27.27.1.1.1" class="ltx_text" style="font-size:80%;">13</span></span><span id="algorithm1.27.27.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.27.27.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.27.27.4" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.27.27.5" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.23.23.m1.1" class="ltx_Math" alttext="Trial_{0}" display="inline"><semantics id="algorithm1.23.23.m1.1a"><mrow id="algorithm1.23.23.m1.1.1" xref="algorithm1.23.23.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.23.23.m1.1.1.2" xref="algorithm1.23.23.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.23.23.m1.1.1.1" xref="algorithm1.23.23.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.23.23.m1.1.1.3" xref="algorithm1.23.23.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.23.23.m1.1.1.1a" xref="algorithm1.23.23.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.23.23.m1.1.1.4" xref="algorithm1.23.23.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.23.23.m1.1.1.1b" xref="algorithm1.23.23.m1.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.23.23.m1.1.1.5" xref="algorithm1.23.23.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.23.23.m1.1.1.1c" xref="algorithm1.23.23.m1.1.1.1.cmml">​</mo><msub id="algorithm1.23.23.m1.1.1.6" xref="algorithm1.23.23.m1.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.23.23.m1.1.1.6.2" xref="algorithm1.23.23.m1.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.23.23.m1.1.1.6.3" xref="algorithm1.23.23.m1.1.1.6.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m1.1b"><apply id="algorithm1.23.23.m1.1.1.cmml" xref="algorithm1.23.23.m1.1.1"><times id="algorithm1.23.23.m1.1.1.1.cmml" xref="algorithm1.23.23.m1.1.1.1"></times><ci id="algorithm1.23.23.m1.1.1.2.cmml" xref="algorithm1.23.23.m1.1.1.2">𝑇</ci><ci id="algorithm1.23.23.m1.1.1.3.cmml" xref="algorithm1.23.23.m1.1.1.3">𝑟</ci><ci id="algorithm1.23.23.m1.1.1.4.cmml" xref="algorithm1.23.23.m1.1.1.4">𝑖</ci><ci id="algorithm1.23.23.m1.1.1.5.cmml" xref="algorithm1.23.23.m1.1.1.5">𝑎</ci><apply id="algorithm1.23.23.m1.1.1.6.cmml" xref="algorithm1.23.23.m1.1.1.6"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.1.1.6.1.cmml" xref="algorithm1.23.23.m1.1.1.6">subscript</csymbol><ci id="algorithm1.23.23.m1.1.1.6.2.cmml" xref="algorithm1.23.23.m1.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.23.23.m1.1.1.6.3.cmml" xref="algorithm1.23.23.m1.1.1.6.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m1.1c">Trial_{0}</annotation></semantics></math><span id="algorithm1.27.27.6" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.24.24.m2.1" class="ltx_Math" alttext="Trial_{1}" display="inline"><semantics id="algorithm1.24.24.m2.1a"><mrow id="algorithm1.24.24.m2.1.1" xref="algorithm1.24.24.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.24.24.m2.1.1.2" xref="algorithm1.24.24.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.24.24.m2.1.1.1" xref="algorithm1.24.24.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.24.24.m2.1.1.3" xref="algorithm1.24.24.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.24.24.m2.1.1.1a" xref="algorithm1.24.24.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.24.24.m2.1.1.4" xref="algorithm1.24.24.m2.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.24.24.m2.1.1.1b" xref="algorithm1.24.24.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.24.24.m2.1.1.5" xref="algorithm1.24.24.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.24.24.m2.1.1.1c" xref="algorithm1.24.24.m2.1.1.1.cmml">​</mo><msub id="algorithm1.24.24.m2.1.1.6" xref="algorithm1.24.24.m2.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.24.24.m2.1.1.6.2" xref="algorithm1.24.24.m2.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.24.24.m2.1.1.6.3" xref="algorithm1.24.24.m2.1.1.6.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m2.1b"><apply id="algorithm1.24.24.m2.1.1.cmml" xref="algorithm1.24.24.m2.1.1"><times id="algorithm1.24.24.m2.1.1.1.cmml" xref="algorithm1.24.24.m2.1.1.1"></times><ci id="algorithm1.24.24.m2.1.1.2.cmml" xref="algorithm1.24.24.m2.1.1.2">𝑇</ci><ci id="algorithm1.24.24.m2.1.1.3.cmml" xref="algorithm1.24.24.m2.1.1.3">𝑟</ci><ci id="algorithm1.24.24.m2.1.1.4.cmml" xref="algorithm1.24.24.m2.1.1.4">𝑖</ci><ci id="algorithm1.24.24.m2.1.1.5.cmml" xref="algorithm1.24.24.m2.1.1.5">𝑎</ci><apply id="algorithm1.24.24.m2.1.1.6.cmml" xref="algorithm1.24.24.m2.1.1.6"><csymbol cd="ambiguous" id="algorithm1.24.24.m2.1.1.6.1.cmml" xref="algorithm1.24.24.m2.1.1.6">subscript</csymbol><ci id="algorithm1.24.24.m2.1.1.6.2.cmml" xref="algorithm1.24.24.m2.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.24.24.m2.1.1.6.3.cmml" xref="algorithm1.24.24.m2.1.1.6.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m2.1c">Trial_{1}</annotation></semantics></math><span id="algorithm1.27.27.7" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.25.25.m3.1" class="ltx_Math" alttext="Trial_{2}" display="inline"><semantics id="algorithm1.25.25.m3.1a"><mrow id="algorithm1.25.25.m3.1.1" xref="algorithm1.25.25.m3.1.1.cmml"><mi mathsize="80%" id="algorithm1.25.25.m3.1.1.2" xref="algorithm1.25.25.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.25.25.m3.1.1.1" xref="algorithm1.25.25.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.25.25.m3.1.1.3" xref="algorithm1.25.25.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.25.25.m3.1.1.1a" xref="algorithm1.25.25.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.25.25.m3.1.1.4" xref="algorithm1.25.25.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.25.25.m3.1.1.1b" xref="algorithm1.25.25.m3.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.25.25.m3.1.1.5" xref="algorithm1.25.25.m3.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.25.25.m3.1.1.1c" xref="algorithm1.25.25.m3.1.1.1.cmml">​</mo><msub id="algorithm1.25.25.m3.1.1.6" xref="algorithm1.25.25.m3.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.25.25.m3.1.1.6.2" xref="algorithm1.25.25.m3.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.25.25.m3.1.1.6.3" xref="algorithm1.25.25.m3.1.1.6.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.25.25.m3.1b"><apply id="algorithm1.25.25.m3.1.1.cmml" xref="algorithm1.25.25.m3.1.1"><times id="algorithm1.25.25.m3.1.1.1.cmml" xref="algorithm1.25.25.m3.1.1.1"></times><ci id="algorithm1.25.25.m3.1.1.2.cmml" xref="algorithm1.25.25.m3.1.1.2">𝑇</ci><ci id="algorithm1.25.25.m3.1.1.3.cmml" xref="algorithm1.25.25.m3.1.1.3">𝑟</ci><ci id="algorithm1.25.25.m3.1.1.4.cmml" xref="algorithm1.25.25.m3.1.1.4">𝑖</ci><ci id="algorithm1.25.25.m3.1.1.5.cmml" xref="algorithm1.25.25.m3.1.1.5">𝑎</ci><apply id="algorithm1.25.25.m3.1.1.6.cmml" xref="algorithm1.25.25.m3.1.1.6"><csymbol cd="ambiguous" id="algorithm1.25.25.m3.1.1.6.1.cmml" xref="algorithm1.25.25.m3.1.1.6">subscript</csymbol><ci id="algorithm1.25.25.m3.1.1.6.2.cmml" xref="algorithm1.25.25.m3.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.25.25.m3.1.1.6.3.cmml" xref="algorithm1.25.25.m3.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.25.25.m3.1c">Trial_{2}</annotation></semantics></math><span id="algorithm1.27.27.8" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.26.26.m4.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.26.26.m4.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.26.26.m4.1.1" xref="algorithm1.26.26.m4.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.26.26.m4.1b"><ci id="algorithm1.26.26.m4.1.1.cmml" xref="algorithm1.26.26.m4.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.26.26.m4.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.27.27.9" class="ltx_text" style="font-size:80%;"> selects </span><math id="algorithm1.27.27.m5.1" class="ltx_Math" alttext="3N" display="inline"><semantics id="algorithm1.27.27.m5.1a"><mrow id="algorithm1.27.27.m5.1.1" xref="algorithm1.27.27.m5.1.1.cmml"><mn mathsize="80%" id="algorithm1.27.27.m5.1.1.2" xref="algorithm1.27.27.m5.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="algorithm1.27.27.m5.1.1.1" xref="algorithm1.27.27.m5.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.27.27.m5.1.1.3" xref="algorithm1.27.27.m5.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.27.27.m5.1b"><apply id="algorithm1.27.27.m5.1.1.cmml" xref="algorithm1.27.27.m5.1.1"><times id="algorithm1.27.27.m5.1.1.1.cmml" xref="algorithm1.27.27.m5.1.1.1"></times><cn type="integer" id="algorithm1.27.27.m5.1.1.2.cmml" xref="algorithm1.27.27.m5.1.1.2">3</cn><ci id="algorithm1.27.27.m5.1.1.3.cmml" xref="algorithm1.27.27.m5.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.27.27.m5.1c">3N</annotation></semantics></math><span id="algorithm1.27.27.10" class="ltx_text" style="font-size:80%;"> new clients;</span>
</div>
<div id="algorithm1.30.30" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.30.30.4.1.1" class="ltx_text" style="font-size:80%;">14</span></span><span id="algorithm1.30.30.5" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.30.30.6" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.30.30.7" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.30.30.8" class="ltx_text" style="font-size:80%;">
</span><span id="algorithm1.30.30.9" class="ltx_text ltx_font_bold" style="font-size:80%;">if</span><span id="algorithm1.30.30.10" class="ltx_text" style="font-size:80%;"> </span><em id="algorithm1.30.30.3" class="ltx_emph ltx_font_italic" style="font-size:80%;"><math id="algorithm1.28.28.1.m1.1" class="ltx_Math" alttext="T_{now}" display="inline"><semantics id="algorithm1.28.28.1.m1.1a"><msub id="algorithm1.28.28.1.m1.1.1" xref="algorithm1.28.28.1.m1.1.1.cmml"><mi id="algorithm1.28.28.1.m1.1.1.2" xref="algorithm1.28.28.1.m1.1.1.2.cmml">T</mi><mrow id="algorithm1.28.28.1.m1.1.1.3" xref="algorithm1.28.28.1.m1.1.1.3.cmml"><mi id="algorithm1.28.28.1.m1.1.1.3.2" xref="algorithm1.28.28.1.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="algorithm1.28.28.1.m1.1.1.3.1" xref="algorithm1.28.28.1.m1.1.1.3.1.cmml">​</mo><mi id="algorithm1.28.28.1.m1.1.1.3.3" xref="algorithm1.28.28.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="algorithm1.28.28.1.m1.1.1.3.1a" xref="algorithm1.28.28.1.m1.1.1.3.1.cmml">​</mo><mi id="algorithm1.28.28.1.m1.1.1.3.4" xref="algorithm1.28.28.1.m1.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.28.28.1.m1.1b"><apply id="algorithm1.28.28.1.m1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.28.28.1.m1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1">subscript</csymbol><ci id="algorithm1.28.28.1.m1.1.1.2.cmml" xref="algorithm1.28.28.1.m1.1.1.2">𝑇</ci><apply id="algorithm1.28.28.1.m1.1.1.3.cmml" xref="algorithm1.28.28.1.m1.1.1.3"><times id="algorithm1.28.28.1.m1.1.1.3.1.cmml" xref="algorithm1.28.28.1.m1.1.1.3.1"></times><ci id="algorithm1.28.28.1.m1.1.1.3.2.cmml" xref="algorithm1.28.28.1.m1.1.1.3.2">𝑛</ci><ci id="algorithm1.28.28.1.m1.1.1.3.3.cmml" xref="algorithm1.28.28.1.m1.1.1.3.3">𝑜</ci><ci id="algorithm1.28.28.1.m1.1.1.3.4.cmml" xref="algorithm1.28.28.1.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.28.28.1.m1.1c">T_{now}</annotation></semantics></math> - <math id="algorithm1.29.29.2.m2.1" class="ltx_Math" alttext="T_{trial}" display="inline"><semantics id="algorithm1.29.29.2.m2.1a"><msub id="algorithm1.29.29.2.m2.1.1" xref="algorithm1.29.29.2.m2.1.1.cmml"><mi id="algorithm1.29.29.2.m2.1.1.2" xref="algorithm1.29.29.2.m2.1.1.2.cmml">T</mi><mrow id="algorithm1.29.29.2.m2.1.1.3" xref="algorithm1.29.29.2.m2.1.1.3.cmml"><mi id="algorithm1.29.29.2.m2.1.1.3.2" xref="algorithm1.29.29.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.29.29.2.m2.1.1.3.1" xref="algorithm1.29.29.2.m2.1.1.3.1.cmml">​</mo><mi id="algorithm1.29.29.2.m2.1.1.3.3" xref="algorithm1.29.29.2.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.29.29.2.m2.1.1.3.1a" xref="algorithm1.29.29.2.m2.1.1.3.1.cmml">​</mo><mi id="algorithm1.29.29.2.m2.1.1.3.4" xref="algorithm1.29.29.2.m2.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.29.29.2.m2.1.1.3.1b" xref="algorithm1.29.29.2.m2.1.1.3.1.cmml">​</mo><mi id="algorithm1.29.29.2.m2.1.1.3.5" xref="algorithm1.29.29.2.m2.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.29.29.2.m2.1.1.3.1c" xref="algorithm1.29.29.2.m2.1.1.3.1.cmml">​</mo><mi id="algorithm1.29.29.2.m2.1.1.3.6" xref="algorithm1.29.29.2.m2.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.29.29.2.m2.1b"><apply id="algorithm1.29.29.2.m2.1.1.cmml" xref="algorithm1.29.29.2.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.29.29.2.m2.1.1.1.cmml" xref="algorithm1.29.29.2.m2.1.1">subscript</csymbol><ci id="algorithm1.29.29.2.m2.1.1.2.cmml" xref="algorithm1.29.29.2.m2.1.1.2">𝑇</ci><apply id="algorithm1.29.29.2.m2.1.1.3.cmml" xref="algorithm1.29.29.2.m2.1.1.3"><times id="algorithm1.29.29.2.m2.1.1.3.1.cmml" xref="algorithm1.29.29.2.m2.1.1.3.1"></times><ci id="algorithm1.29.29.2.m2.1.1.3.2.cmml" xref="algorithm1.29.29.2.m2.1.1.3.2">𝑡</ci><ci id="algorithm1.29.29.2.m2.1.1.3.3.cmml" xref="algorithm1.29.29.2.m2.1.1.3.3">𝑟</ci><ci id="algorithm1.29.29.2.m2.1.1.3.4.cmml" xref="algorithm1.29.29.2.m2.1.1.3.4">𝑖</ci><ci id="algorithm1.29.29.2.m2.1.1.3.5.cmml" xref="algorithm1.29.29.2.m2.1.1.3.5">𝑎</ci><ci id="algorithm1.29.29.2.m2.1.1.3.6.cmml" xref="algorithm1.29.29.2.m2.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.29.29.2.m2.1c">T_{trial}</annotation></semantics></math> ¿ <math id="algorithm1.30.30.3.m3.1" class="ltx_Math" alttext="trial\_intvl" display="inline"><semantics id="algorithm1.30.30.3.m3.1a"><mrow id="algorithm1.30.30.3.m3.1.1" xref="algorithm1.30.30.3.m3.1.1.cmml"><mi id="algorithm1.30.30.3.m3.1.1.2" xref="algorithm1.30.30.3.m3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.3" xref="algorithm1.30.30.3.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1a" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.4" xref="algorithm1.30.30.3.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1b" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.5" xref="algorithm1.30.30.3.m3.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1c" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.6" xref="algorithm1.30.30.3.m3.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1d" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="algorithm1.30.30.3.m3.1.1.7" xref="algorithm1.30.30.3.m3.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1e" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.8" xref="algorithm1.30.30.3.m3.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1f" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.9" xref="algorithm1.30.30.3.m3.1.1.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1g" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.10" xref="algorithm1.30.30.3.m3.1.1.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1h" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.11" xref="algorithm1.30.30.3.m3.1.1.11.cmml">v</mi><mo lspace="0em" rspace="0em" id="algorithm1.30.30.3.m3.1.1.1i" xref="algorithm1.30.30.3.m3.1.1.1.cmml">​</mo><mi id="algorithm1.30.30.3.m3.1.1.12" xref="algorithm1.30.30.3.m3.1.1.12.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.30.30.3.m3.1b"><apply id="algorithm1.30.30.3.m3.1.1.cmml" xref="algorithm1.30.30.3.m3.1.1"><times id="algorithm1.30.30.3.m3.1.1.1.cmml" xref="algorithm1.30.30.3.m3.1.1.1"></times><ci id="algorithm1.30.30.3.m3.1.1.2.cmml" xref="algorithm1.30.30.3.m3.1.1.2">𝑡</ci><ci id="algorithm1.30.30.3.m3.1.1.3.cmml" xref="algorithm1.30.30.3.m3.1.1.3">𝑟</ci><ci id="algorithm1.30.30.3.m3.1.1.4.cmml" xref="algorithm1.30.30.3.m3.1.1.4">𝑖</ci><ci id="algorithm1.30.30.3.m3.1.1.5.cmml" xref="algorithm1.30.30.3.m3.1.1.5">𝑎</ci><ci id="algorithm1.30.30.3.m3.1.1.6.cmml" xref="algorithm1.30.30.3.m3.1.1.6">𝑙</ci><ci id="algorithm1.30.30.3.m3.1.1.7.cmml" xref="algorithm1.30.30.3.m3.1.1.7">_</ci><ci id="algorithm1.30.30.3.m3.1.1.8.cmml" xref="algorithm1.30.30.3.m3.1.1.8">𝑖</ci><ci id="algorithm1.30.30.3.m3.1.1.9.cmml" xref="algorithm1.30.30.3.m3.1.1.9">𝑛</ci><ci id="algorithm1.30.30.3.m3.1.1.10.cmml" xref="algorithm1.30.30.3.m3.1.1.10">𝑡</ci><ci id="algorithm1.30.30.3.m3.1.1.11.cmml" xref="algorithm1.30.30.3.m3.1.1.11">𝑣</ci><ci id="algorithm1.30.30.3.m3.1.1.12.cmml" xref="algorithm1.30.30.3.m3.1.1.12">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.30.30.3.m3.1c">trial\_intvl</annotation></semantics></math></em><span id="algorithm1.30.30.11" class="ltx_text" style="font-size:80%;"> </span><span id="algorithm1.30.30.12" class="ltx_text ltx_font_bold" style="font-size:80%;">then</span>
</div>
<div id="algorithm1.31.31" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.31.31.1.1.1" class="ltx_text" style="font-size:80%;">15</span></span><span id="algorithm1.31.31.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.31.31.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.31.31.4" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.31.31.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.31.31.6" class="ltx_text" style="font-size:80%;">Compare accuracy under different </span><math id="algorithm1.31.31.m1.1" class="ltx_Math" alttext="\Theta^{k}_{i}" display="inline"><semantics id="algorithm1.31.31.m1.1a"><msubsup id="algorithm1.31.31.m1.1.1" xref="algorithm1.31.31.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.31.31.m1.1.1.2.2" xref="algorithm1.31.31.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.31.31.m1.1.1.3" xref="algorithm1.31.31.m1.1.1.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.31.31.m1.1.1.2.3" xref="algorithm1.31.31.m1.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.31.31.m1.1b"><apply id="algorithm1.31.31.m1.1.1.cmml" xref="algorithm1.31.31.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.31.31.m1.1.1.1.cmml" xref="algorithm1.31.31.m1.1.1">subscript</csymbol><apply id="algorithm1.31.31.m1.1.1.2.cmml" xref="algorithm1.31.31.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.31.31.m1.1.1.2.1.cmml" xref="algorithm1.31.31.m1.1.1">superscript</csymbol><ci id="algorithm1.31.31.m1.1.1.2.2.cmml" xref="algorithm1.31.31.m1.1.1.2.2">Θ</ci><ci id="algorithm1.31.31.m1.1.1.2.3.cmml" xref="algorithm1.31.31.m1.1.1.2.3">𝑘</ci></apply><ci id="algorithm1.31.31.m1.1.1.3.cmml" xref="algorithm1.31.31.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.31.31.m1.1c">\Theta^{k}_{i}</annotation></semantics></math><span id="algorithm1.31.31.7" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.35.35" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.35.35.1.1.1" class="ltx_text" style="font-size:80%;">16</span></span><span id="algorithm1.35.35.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.35.35.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.35.35.4" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.35.35.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.35.35.6" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.32.32.m1.1" class="ltx_Math" alttext="\Theta_{i}" display="inline"><semantics id="algorithm1.32.32.m1.1a"><msub id="algorithm1.32.32.m1.1.1" xref="algorithm1.32.32.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.32.32.m1.1.1.2" xref="algorithm1.32.32.m1.1.1.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.32.32.m1.1.1.3" xref="algorithm1.32.32.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.32.32.m1.1b"><apply id="algorithm1.32.32.m1.1.1.cmml" xref="algorithm1.32.32.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.32.32.m1.1.1.1.cmml" xref="algorithm1.32.32.m1.1.1">subscript</csymbol><ci id="algorithm1.32.32.m1.1.1.2.cmml" xref="algorithm1.32.32.m1.1.1.2">Θ</ci><ci id="algorithm1.32.32.m1.1.1.3.cmml" xref="algorithm1.32.32.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.32.32.m1.1c">\Theta_{i}</annotation></semantics></math><span id="algorithm1.35.35.7" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.33.33.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.33.33.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.33.33.m2.1.1" xref="algorithm1.33.33.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.33.33.m2.1b"><ci id="algorithm1.33.33.m2.1.1.cmml" xref="algorithm1.33.33.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.33.33.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.35.35.8" class="ltx_text" style="font-size:80%;"> Winner track;
</span><math id="algorithm1.34.34.m3.2" class="ltx_Math" alttext="D_{i},W_{i}" display="inline"><semantics id="algorithm1.34.34.m3.2a"><mrow id="algorithm1.34.34.m3.2.2.2" xref="algorithm1.34.34.m3.2.2.3.cmml"><msub id="algorithm1.34.34.m3.1.1.1.1" xref="algorithm1.34.34.m3.1.1.1.1.cmml"><mi mathsize="80%" id="algorithm1.34.34.m3.1.1.1.1.2" xref="algorithm1.34.34.m3.1.1.1.1.2.cmml">D</mi><mi mathsize="80%" id="algorithm1.34.34.m3.1.1.1.1.3" xref="algorithm1.34.34.m3.1.1.1.1.3.cmml">i</mi></msub><mo mathsize="80%" id="algorithm1.34.34.m3.2.2.2.3" xref="algorithm1.34.34.m3.2.2.3.cmml">,</mo><msub id="algorithm1.34.34.m3.2.2.2.2" xref="algorithm1.34.34.m3.2.2.2.2.cmml"><mi mathsize="80%" id="algorithm1.34.34.m3.2.2.2.2.2" xref="algorithm1.34.34.m3.2.2.2.2.2.cmml">W</mi><mi mathsize="80%" id="algorithm1.34.34.m3.2.2.2.2.3" xref="algorithm1.34.34.m3.2.2.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.34.34.m3.2b"><list id="algorithm1.34.34.m3.2.2.3.cmml" xref="algorithm1.34.34.m3.2.2.2"><apply id="algorithm1.34.34.m3.1.1.1.1.cmml" xref="algorithm1.34.34.m3.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.34.34.m3.1.1.1.1.1.cmml" xref="algorithm1.34.34.m3.1.1.1.1">subscript</csymbol><ci id="algorithm1.34.34.m3.1.1.1.1.2.cmml" xref="algorithm1.34.34.m3.1.1.1.1.2">𝐷</ci><ci id="algorithm1.34.34.m3.1.1.1.1.3.cmml" xref="algorithm1.34.34.m3.1.1.1.1.3">𝑖</ci></apply><apply id="algorithm1.34.34.m3.2.2.2.2.cmml" xref="algorithm1.34.34.m3.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.34.34.m3.2.2.2.2.1.cmml" xref="algorithm1.34.34.m3.2.2.2.2">subscript</csymbol><ci id="algorithm1.34.34.m3.2.2.2.2.2.cmml" xref="algorithm1.34.34.m3.2.2.2.2.2">𝑊</ci><ci id="algorithm1.34.34.m3.2.2.2.2.3.cmml" xref="algorithm1.34.34.m3.2.2.2.2.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.34.34.m3.2c">D_{i},W_{i}</annotation></semantics></math><span id="algorithm1.35.35.9" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.35.35.m4.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.35.35.m4.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.35.35.m4.1.1" xref="algorithm1.35.35.m4.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.35.35.m4.1b"><ci id="algorithm1.35.35.m4.1.1.cmml" xref="algorithm1.35.35.m4.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.35.35.m4.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.35.35.10" class="ltx_text" style="font-size:80%;"> Winner setting;</span>
</div>
<div id="algorithm1.37.37" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.37.37.1.1.1" class="ltx_text" style="font-size:80%;">17</span></span><span id="algorithm1.37.37.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.37.37.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.37.37.4" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.37.37.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.37.37.6" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.36.36.m1.1" class="ltx_Math" alttext="T_{trial}" display="inline"><semantics id="algorithm1.36.36.m1.1a"><msub id="algorithm1.36.36.m1.1.1" xref="algorithm1.36.36.m1.1.1.cmml"><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.2" xref="algorithm1.36.36.m1.1.1.2.cmml">T</mi><mrow id="algorithm1.36.36.m1.1.1.3" xref="algorithm1.36.36.m1.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.3.2" xref="algorithm1.36.36.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="algorithm1.36.36.m1.1.1.3.1" xref="algorithm1.36.36.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.3.3" xref="algorithm1.36.36.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.36.36.m1.1.1.3.1a" xref="algorithm1.36.36.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.3.4" xref="algorithm1.36.36.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.36.36.m1.1.1.3.1b" xref="algorithm1.36.36.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.3.5" xref="algorithm1.36.36.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.36.36.m1.1.1.3.1c" xref="algorithm1.36.36.m1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.36.36.m1.1.1.3.6" xref="algorithm1.36.36.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.36.36.m1.1b"><apply id="algorithm1.36.36.m1.1.1.cmml" xref="algorithm1.36.36.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.36.36.m1.1.1.1.cmml" xref="algorithm1.36.36.m1.1.1">subscript</csymbol><ci id="algorithm1.36.36.m1.1.1.2.cmml" xref="algorithm1.36.36.m1.1.1.2">𝑇</ci><apply id="algorithm1.36.36.m1.1.1.3.cmml" xref="algorithm1.36.36.m1.1.1.3"><times id="algorithm1.36.36.m1.1.1.3.1.cmml" xref="algorithm1.36.36.m1.1.1.3.1"></times><ci id="algorithm1.36.36.m1.1.1.3.2.cmml" xref="algorithm1.36.36.m1.1.1.3.2">𝑡</ci><ci id="algorithm1.36.36.m1.1.1.3.3.cmml" xref="algorithm1.36.36.m1.1.1.3.3">𝑟</ci><ci id="algorithm1.36.36.m1.1.1.3.4.cmml" xref="algorithm1.36.36.m1.1.1.3.4">𝑖</ci><ci id="algorithm1.36.36.m1.1.1.3.5.cmml" xref="algorithm1.36.36.m1.1.1.3.5">𝑎</ci><ci id="algorithm1.36.36.m1.1.1.3.6.cmml" xref="algorithm1.36.36.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.36.36.m1.1c">T_{trial}</annotation></semantics></math><span id="algorithm1.37.37.7" class="ltx_text" style="font-size:80%;">=</span><math id="algorithm1.37.37.m2.1" class="ltx_Math" alttext="T_{now}" display="inline"><semantics id="algorithm1.37.37.m2.1a"><msub id="algorithm1.37.37.m2.1.1" xref="algorithm1.37.37.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.37.37.m2.1.1.2" xref="algorithm1.37.37.m2.1.1.2.cmml">T</mi><mrow id="algorithm1.37.37.m2.1.1.3" xref="algorithm1.37.37.m2.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.37.37.m2.1.1.3.2" xref="algorithm1.37.37.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="algorithm1.37.37.m2.1.1.3.1" xref="algorithm1.37.37.m2.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.37.37.m2.1.1.3.3" xref="algorithm1.37.37.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="algorithm1.37.37.m2.1.1.3.1a" xref="algorithm1.37.37.m2.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.37.37.m2.1.1.3.4" xref="algorithm1.37.37.m2.1.1.3.4.cmml">w</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="algorithm1.37.37.m2.1b"><apply id="algorithm1.37.37.m2.1.1.cmml" xref="algorithm1.37.37.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.37.37.m2.1.1.1.cmml" xref="algorithm1.37.37.m2.1.1">subscript</csymbol><ci id="algorithm1.37.37.m2.1.1.2.cmml" xref="algorithm1.37.37.m2.1.1.2">𝑇</ci><apply id="algorithm1.37.37.m2.1.1.3.cmml" xref="algorithm1.37.37.m2.1.1.3"><times id="algorithm1.37.37.m2.1.1.3.1.cmml" xref="algorithm1.37.37.m2.1.1.3.1"></times><ci id="algorithm1.37.37.m2.1.1.3.2.cmml" xref="algorithm1.37.37.m2.1.1.3.2">𝑛</ci><ci id="algorithm1.37.37.m2.1.1.3.3.cmml" xref="algorithm1.37.37.m2.1.1.3.3">𝑜</ci><ci id="algorithm1.37.37.m2.1.1.3.4.cmml" xref="algorithm1.37.37.m2.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.37.37.m2.1c">T_{now}</annotation></semantics></math><span id="algorithm1.37.37.8" class="ltx_text" style="font-size:80%;">;</span>
</div>
<div id="algorithm1.71.79" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.79.1.1.1" class="ltx_text" style="font-size:80%;">18</span></span><span id="algorithm1.71.79.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.79.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.79.4" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.79.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.79.6" class="ltx_text" style="font-size:80%;">
Dispatch(i) // Inherit winner track;</span>
</div>
<div id="algorithm1.71.80" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.80.1.1.1" class="ltx_text" style="font-size:80%;">19</span></span><span id="algorithm1.71.80.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.80.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.80.4" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.80.5" class="ltx_text ltx_font_bold" style="font-size:80%;">else</span>
</div>
<div id="algorithm1.39.39" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.39.39.1.1.1" class="ltx_text" style="font-size:80%;">20</span></span><span id="algorithm1.39.39.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.39.39.3" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.39.39.4" class="ltx_text" style="font-size:80%;">     </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.39.39.5" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.39.39.6" class="ltx_text" style="font-size:80%;">
Send the aggregated model </span><math id="algorithm1.38.38.m1.1" class="ltx_Math" alttext="\Theta^{k}_{i}" display="inline"><semantics id="algorithm1.38.38.m1.1a"><msubsup id="algorithm1.38.38.m1.1.1" xref="algorithm1.38.38.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.38.38.m1.1.1.2.2" xref="algorithm1.38.38.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.38.38.m1.1.1.3" xref="algorithm1.38.38.m1.1.1.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.38.38.m1.1.1.2.3" xref="algorithm1.38.38.m1.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.38.38.m1.1b"><apply id="algorithm1.38.38.m1.1.1.cmml" xref="algorithm1.38.38.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.38.38.m1.1.1.1.cmml" xref="algorithm1.38.38.m1.1.1">subscript</csymbol><apply id="algorithm1.38.38.m1.1.1.2.cmml" xref="algorithm1.38.38.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.38.38.m1.1.1.2.1.cmml" xref="algorithm1.38.38.m1.1.1">superscript</csymbol><ci id="algorithm1.38.38.m1.1.1.2.2.cmml" xref="algorithm1.38.38.m1.1.1.2.2">Θ</ci><ci id="algorithm1.38.38.m1.1.1.2.3.cmml" xref="algorithm1.38.38.m1.1.1.2.3">𝑘</ci></apply><ci id="algorithm1.38.38.m1.1.1.3.cmml" xref="algorithm1.38.38.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.38.38.m1.1c">\Theta^{k}_{i}</annotation></semantics></math><span id="algorithm1.39.39.7" class="ltx_text" style="font-size:80%;"> to </span><math id="algorithm1.39.39.m2.1" class="ltx_Math" alttext="Trial_{k}" display="inline"><semantics id="algorithm1.39.39.m2.1a"><mrow id="algorithm1.39.39.m2.1.1" xref="algorithm1.39.39.m2.1.1.cmml"><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.2" xref="algorithm1.39.39.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.39.39.m2.1.1.1" xref="algorithm1.39.39.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.3" xref="algorithm1.39.39.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.39.39.m2.1.1.1a" xref="algorithm1.39.39.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.4" xref="algorithm1.39.39.m2.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.39.39.m2.1.1.1b" xref="algorithm1.39.39.m2.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.5" xref="algorithm1.39.39.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.39.39.m2.1.1.1c" xref="algorithm1.39.39.m2.1.1.1.cmml">​</mo><msub id="algorithm1.39.39.m2.1.1.6" xref="algorithm1.39.39.m2.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.6.2" xref="algorithm1.39.39.m2.1.1.6.2.cmml">l</mi><mi mathsize="80%" id="algorithm1.39.39.m2.1.1.6.3" xref="algorithm1.39.39.m2.1.1.6.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.39.39.m2.1b"><apply id="algorithm1.39.39.m2.1.1.cmml" xref="algorithm1.39.39.m2.1.1"><times id="algorithm1.39.39.m2.1.1.1.cmml" xref="algorithm1.39.39.m2.1.1.1"></times><ci id="algorithm1.39.39.m2.1.1.2.cmml" xref="algorithm1.39.39.m2.1.1.2">𝑇</ci><ci id="algorithm1.39.39.m2.1.1.3.cmml" xref="algorithm1.39.39.m2.1.1.3">𝑟</ci><ci id="algorithm1.39.39.m2.1.1.4.cmml" xref="algorithm1.39.39.m2.1.1.4">𝑖</ci><ci id="algorithm1.39.39.m2.1.1.5.cmml" xref="algorithm1.39.39.m2.1.1.5">𝑎</ci><apply id="algorithm1.39.39.m2.1.1.6.cmml" xref="algorithm1.39.39.m2.1.1.6"><csymbol cd="ambiguous" id="algorithm1.39.39.m2.1.1.6.1.cmml" xref="algorithm1.39.39.m2.1.1.6">subscript</csymbol><ci id="algorithm1.39.39.m2.1.1.6.2.cmml" xref="algorithm1.39.39.m2.1.1.6.2">𝑙</ci><ci id="algorithm1.39.39.m2.1.1.6.3.cmml" xref="algorithm1.39.39.m2.1.1.6.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.39.39.m2.1c">Trial_{k}</annotation></semantics></math><span id="algorithm1.39.39.8" class="ltx_text" style="font-size:80%;">.
</span>
</div>
<div id="algorithm1.71.81" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.81.1.1.1" class="ltx_text" style="font-size:80%;">21</span></span><span id="algorithm1.71.81.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.81.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.81.4" class="ltx_text" style="font-size:80%;">Exit training.
</span>
</div>
<div id="algorithm1.71.82" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.82.1.1.1" class="ltx_text" style="font-size:80%;">22</span></span><span id="algorithm1.71.82.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Function</span><span id="algorithm1.71.82.3" class="ltx_text" style="font-size:80%;"> </span><em id="algorithm1.71.82.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Client_training(i,k):</em><span id="algorithm1.71.82.5" class="ltx_text ltx_font_bold" style="font-size:80%;"> </span>
</div>
<div id="algorithm1.41.41" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.41.41.1.1.1" class="ltx_text" style="font-size:80%;">23</span></span><span id="algorithm1.41.41.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.41.41.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.41.41.4" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.40.40.m1.1" class="ltx_Math" alttext="\Theta_{i}^{k}" display="inline"><semantics id="algorithm1.40.40.m1.1a"><msubsup id="algorithm1.40.40.m1.1.1" xref="algorithm1.40.40.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.40.40.m1.1.1.2.2" xref="algorithm1.40.40.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.40.40.m1.1.1.2.3" xref="algorithm1.40.40.m1.1.1.2.3.cmml">i</mi><mi mathsize="80%" id="algorithm1.40.40.m1.1.1.3" xref="algorithm1.40.40.m1.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.40.40.m1.1b"><apply id="algorithm1.40.40.m1.1.1.cmml" xref="algorithm1.40.40.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.40.40.m1.1.1.1.cmml" xref="algorithm1.40.40.m1.1.1">superscript</csymbol><apply id="algorithm1.40.40.m1.1.1.2.cmml" xref="algorithm1.40.40.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.40.40.m1.1.1.2.1.cmml" xref="algorithm1.40.40.m1.1.1">subscript</csymbol><ci id="algorithm1.40.40.m1.1.1.2.2.cmml" xref="algorithm1.40.40.m1.1.1.2.2">Θ</ci><ci id="algorithm1.40.40.m1.1.1.2.3.cmml" xref="algorithm1.40.40.m1.1.1.2.3">𝑖</ci></apply><ci id="algorithm1.40.40.m1.1.1.3.cmml" xref="algorithm1.40.40.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.40.40.m1.1c">\Theta_{i}^{k}</annotation></semantics></math><span id="algorithm1.41.41.5" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.41.41.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.41.41.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.41.41.m2.1.1" xref="algorithm1.41.41.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.41.41.m2.1b"><ci id="algorithm1.41.41.m2.1.1.cmml" xref="algorithm1.41.41.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.41.41.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.41.41.6" class="ltx_text" style="font-size:80%;"> Receive global adapter from cloud;</span>
</div>
<div id="algorithm1.43.43" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.43.43.1.1.1" class="ltx_text" style="font-size:80%;">24</span></span><span id="algorithm1.43.43.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.43.43.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.43.43.4" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.42.42.m1.1" class="ltx_Math" alttext="\Theta_{i+1}^{k}(n)" display="inline"><semantics id="algorithm1.42.42.m1.1a"><mrow id="algorithm1.42.42.m1.1.2" xref="algorithm1.42.42.m1.1.2.cmml"><msubsup id="algorithm1.42.42.m1.1.2.2" xref="algorithm1.42.42.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.42.42.m1.1.2.2.2.2" xref="algorithm1.42.42.m1.1.2.2.2.2.cmml">Θ</mi><mrow id="algorithm1.42.42.m1.1.2.2.2.3" xref="algorithm1.42.42.m1.1.2.2.2.3.cmml"><mi mathsize="80%" id="algorithm1.42.42.m1.1.2.2.2.3.2" xref="algorithm1.42.42.m1.1.2.2.2.3.2.cmml">i</mi><mo mathsize="80%" id="algorithm1.42.42.m1.1.2.2.2.3.1" xref="algorithm1.42.42.m1.1.2.2.2.3.1.cmml">+</mo><mn mathsize="80%" id="algorithm1.42.42.m1.1.2.2.2.3.3" xref="algorithm1.42.42.m1.1.2.2.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="algorithm1.42.42.m1.1.2.2.3" xref="algorithm1.42.42.m1.1.2.2.3.cmml">k</mi></msubsup><mo lspace="0em" rspace="0em" id="algorithm1.42.42.m1.1.2.1" xref="algorithm1.42.42.m1.1.2.1.cmml">​</mo><mrow id="algorithm1.42.42.m1.1.2.3.2" xref="algorithm1.42.42.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="algorithm1.42.42.m1.1.2.3.2.1" xref="algorithm1.42.42.m1.1.2.cmml">(</mo><mi mathsize="80%" id="algorithm1.42.42.m1.1.1" xref="algorithm1.42.42.m1.1.1.cmml">n</mi><mo maxsize="80%" minsize="80%" id="algorithm1.42.42.m1.1.2.3.2.2" xref="algorithm1.42.42.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.42.42.m1.1b"><apply id="algorithm1.42.42.m1.1.2.cmml" xref="algorithm1.42.42.m1.1.2"><times id="algorithm1.42.42.m1.1.2.1.cmml" xref="algorithm1.42.42.m1.1.2.1"></times><apply id="algorithm1.42.42.m1.1.2.2.cmml" xref="algorithm1.42.42.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.42.42.m1.1.2.2.1.cmml" xref="algorithm1.42.42.m1.1.2.2">superscript</csymbol><apply id="algorithm1.42.42.m1.1.2.2.2.cmml" xref="algorithm1.42.42.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.42.42.m1.1.2.2.2.1.cmml" xref="algorithm1.42.42.m1.1.2.2">subscript</csymbol><ci id="algorithm1.42.42.m1.1.2.2.2.2.cmml" xref="algorithm1.42.42.m1.1.2.2.2.2">Θ</ci><apply id="algorithm1.42.42.m1.1.2.2.2.3.cmml" xref="algorithm1.42.42.m1.1.2.2.2.3"><plus id="algorithm1.42.42.m1.1.2.2.2.3.1.cmml" xref="algorithm1.42.42.m1.1.2.2.2.3.1"></plus><ci id="algorithm1.42.42.m1.1.2.2.2.3.2.cmml" xref="algorithm1.42.42.m1.1.2.2.2.3.2">𝑖</ci><cn type="integer" id="algorithm1.42.42.m1.1.2.2.2.3.3.cmml" xref="algorithm1.42.42.m1.1.2.2.2.3.3">1</cn></apply></apply><ci id="algorithm1.42.42.m1.1.2.2.3.cmml" xref="algorithm1.42.42.m1.1.2.2.3">𝑘</ci></apply><ci id="algorithm1.42.42.m1.1.1.cmml" xref="algorithm1.42.42.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.42.42.m1.1c">\Theta_{i+1}^{k}(n)</annotation></semantics></math><span id="algorithm1.43.43.5" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.43.43.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.43.43.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.43.43.m2.1.1" xref="algorithm1.43.43.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.43.43.m2.1b"><ci id="algorithm1.43.43.m2.1.1.cmml" xref="algorithm1.43.43.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.43.43.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.43.43.6" class="ltx_text" style="font-size:80%;"> Train and update local adapter; </span>
</div>
<div id="algorithm1.44.44" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.44.44.1.1.1" class="ltx_text" style="font-size:80%;">25</span></span><span id="algorithm1.44.44.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.44.44.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.44.44.4" class="ltx_text" style="font-size:80%;">
Send updated adapter </span><math id="algorithm1.44.44.m1.1" class="ltx_Math" alttext="\Theta_{i+1}^{k}(n)" display="inline"><semantics id="algorithm1.44.44.m1.1a"><mrow id="algorithm1.44.44.m1.1.2" xref="algorithm1.44.44.m1.1.2.cmml"><msubsup id="algorithm1.44.44.m1.1.2.2" xref="algorithm1.44.44.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.44.44.m1.1.2.2.2.2" xref="algorithm1.44.44.m1.1.2.2.2.2.cmml">Θ</mi><mrow id="algorithm1.44.44.m1.1.2.2.2.3" xref="algorithm1.44.44.m1.1.2.2.2.3.cmml"><mi mathsize="80%" id="algorithm1.44.44.m1.1.2.2.2.3.2" xref="algorithm1.44.44.m1.1.2.2.2.3.2.cmml">i</mi><mo mathsize="80%" id="algorithm1.44.44.m1.1.2.2.2.3.1" xref="algorithm1.44.44.m1.1.2.2.2.3.1.cmml">+</mo><mn mathsize="80%" id="algorithm1.44.44.m1.1.2.2.2.3.3" xref="algorithm1.44.44.m1.1.2.2.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="algorithm1.44.44.m1.1.2.2.3" xref="algorithm1.44.44.m1.1.2.2.3.cmml">k</mi></msubsup><mo lspace="0em" rspace="0em" id="algorithm1.44.44.m1.1.2.1" xref="algorithm1.44.44.m1.1.2.1.cmml">​</mo><mrow id="algorithm1.44.44.m1.1.2.3.2" xref="algorithm1.44.44.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="algorithm1.44.44.m1.1.2.3.2.1" xref="algorithm1.44.44.m1.1.2.cmml">(</mo><mi mathsize="80%" id="algorithm1.44.44.m1.1.1" xref="algorithm1.44.44.m1.1.1.cmml">n</mi><mo maxsize="80%" minsize="80%" id="algorithm1.44.44.m1.1.2.3.2.2" xref="algorithm1.44.44.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.44.44.m1.1b"><apply id="algorithm1.44.44.m1.1.2.cmml" xref="algorithm1.44.44.m1.1.2"><times id="algorithm1.44.44.m1.1.2.1.cmml" xref="algorithm1.44.44.m1.1.2.1"></times><apply id="algorithm1.44.44.m1.1.2.2.cmml" xref="algorithm1.44.44.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.44.44.m1.1.2.2.1.cmml" xref="algorithm1.44.44.m1.1.2.2">superscript</csymbol><apply id="algorithm1.44.44.m1.1.2.2.2.cmml" xref="algorithm1.44.44.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.44.44.m1.1.2.2.2.1.cmml" xref="algorithm1.44.44.m1.1.2.2">subscript</csymbol><ci id="algorithm1.44.44.m1.1.2.2.2.2.cmml" xref="algorithm1.44.44.m1.1.2.2.2.2">Θ</ci><apply id="algorithm1.44.44.m1.1.2.2.2.3.cmml" xref="algorithm1.44.44.m1.1.2.2.2.3"><plus id="algorithm1.44.44.m1.1.2.2.2.3.1.cmml" xref="algorithm1.44.44.m1.1.2.2.2.3.1"></plus><ci id="algorithm1.44.44.m1.1.2.2.2.3.2.cmml" xref="algorithm1.44.44.m1.1.2.2.2.3.2">𝑖</ci><cn type="integer" id="algorithm1.44.44.m1.1.2.2.2.3.3.cmml" xref="algorithm1.44.44.m1.1.2.2.2.3.3">1</cn></apply></apply><ci id="algorithm1.44.44.m1.1.2.2.3.cmml" xref="algorithm1.44.44.m1.1.2.2.3">𝑘</ci></apply><ci id="algorithm1.44.44.m1.1.1.cmml" xref="algorithm1.44.44.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.44.44.m1.1c">\Theta_{i+1}^{k}(n)</annotation></semantics></math><span id="algorithm1.44.44.5" class="ltx_text" style="font-size:80%;"> to cloud.
</span>
</div>
<div id="algorithm1.71.83" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.83.1.1.1" class="ltx_text" style="font-size:80%;">26</span></span><span id="algorithm1.71.83.2" class="ltx_text ltx_font_bold" style="font-size:80%;">Function</span><span id="algorithm1.71.83.3" class="ltx_text" style="font-size:80%;"> </span><em id="algorithm1.71.83.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Dispatch(i):</em><span id="algorithm1.71.83.5" class="ltx_text ltx_font_bold" style="font-size:80%;"> </span>
</div>
<div id="algorithm1.50.50" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.50.50.1.1.1" class="ltx_text" style="font-size:80%;">27</span></span><span id="algorithm1.50.50.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.50.50.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.50.50.4" class="ltx_text" style="font-size:80%;">
</span><math id="algorithm1.45.45.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="algorithm1.45.45.m1.1a"><mi mathsize="80%" id="algorithm1.45.45.m1.1.1" xref="algorithm1.45.45.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="algorithm1.45.45.m1.1b"><ci id="algorithm1.45.45.m1.1.1.cmml" xref="algorithm1.45.45.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.45.45.m1.1c">F</annotation></semantics></math><span id="algorithm1.50.50.5" class="ltx_text" style="font-size:80%;">(</span><math id="algorithm1.46.46.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="algorithm1.46.46.m2.1a"><mi mathsize="80%" id="algorithm1.46.46.m2.1.1" xref="algorithm1.46.46.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="algorithm1.46.46.m2.1b"><ci id="algorithm1.46.46.m2.1.1.cmml" xref="algorithm1.46.46.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.46.46.m2.1c">D</annotation></semantics></math><span id="algorithm1.50.50.6" class="ltx_text" style="font-size:80%;">,</span><math id="algorithm1.47.47.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="algorithm1.47.47.m3.1a"><mi mathsize="80%" id="algorithm1.47.47.m3.1.1" xref="algorithm1.47.47.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="algorithm1.47.47.m3.1b"><ci id="algorithm1.47.47.m3.1.1.cmml" xref="algorithm1.47.47.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.47.47.m3.1c">W</annotation></semantics></math><span id="algorithm1.50.50.7" class="ltx_text" style="font-size:80%;">): Initial/Inherit winner track </span><math id="algorithm1.48.48.m4.1" class="ltx_Math" alttext="\Theta_{i}" display="inline"><semantics id="algorithm1.48.48.m4.1a"><msub id="algorithm1.48.48.m4.1.1" xref="algorithm1.48.48.m4.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.48.48.m4.1.1.2" xref="algorithm1.48.48.m4.1.1.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.48.48.m4.1.1.3" xref="algorithm1.48.48.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.48.48.m4.1b"><apply id="algorithm1.48.48.m4.1.1.cmml" xref="algorithm1.48.48.m4.1.1"><csymbol cd="ambiguous" id="algorithm1.48.48.m4.1.1.1.cmml" xref="algorithm1.48.48.m4.1.1">subscript</csymbol><ci id="algorithm1.48.48.m4.1.1.2.cmml" xref="algorithm1.48.48.m4.1.1.2">Θ</ci><ci id="algorithm1.48.48.m4.1.1.3.cmml" xref="algorithm1.48.48.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.48.48.m4.1c">\Theta_{i}</annotation></semantics></math><span id="algorithm1.50.50.8" class="ltx_text" style="font-size:80%;"> with </span><math id="algorithm1.49.49.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="algorithm1.49.49.m5.1a"><mi mathsize="80%" id="algorithm1.49.49.m5.1.1" xref="algorithm1.49.49.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="algorithm1.49.49.m5.1b"><ci id="algorithm1.49.49.m5.1.1.cmml" xref="algorithm1.49.49.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.49.49.m5.1c">D</annotation></semantics></math><span id="algorithm1.50.50.9" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.50.50.m6.1" class="ltx_Math" alttext="W" display="inline"><semantics id="algorithm1.50.50.m6.1a"><mi mathsize="80%" id="algorithm1.50.50.m6.1.1" xref="algorithm1.50.50.m6.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="algorithm1.50.50.m6.1b"><ci id="algorithm1.50.50.m6.1.1.cmml" xref="algorithm1.50.50.m6.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.50.50.m6.1c">W</annotation></semantics></math><span id="algorithm1.50.50.10" class="ltx_text" style="font-size:80%;">.
</span>
</div>
<div id="algorithm1.65.65" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.65.65.1.1.1" class="ltx_text" style="font-size:80%;">28</span></span><span id="algorithm1.65.65.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.65.65.3" class="ltx_text" style="font-size:80%;">   </span><math id="algorithm1.51.51.m1.1" class="ltx_Math" alttext="\Theta_{i}^{0}" display="inline"><semantics id="algorithm1.51.51.m1.1a"><msubsup id="algorithm1.51.51.m1.1.1" xref="algorithm1.51.51.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.51.51.m1.1.1.2.2" xref="algorithm1.51.51.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.51.51.m1.1.1.2.3" xref="algorithm1.51.51.m1.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.51.51.m1.1.1.3" xref="algorithm1.51.51.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.51.51.m1.1b"><apply id="algorithm1.51.51.m1.1.1.cmml" xref="algorithm1.51.51.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.51.51.m1.1.1.1.cmml" xref="algorithm1.51.51.m1.1.1">superscript</csymbol><apply id="algorithm1.51.51.m1.1.1.2.cmml" xref="algorithm1.51.51.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.51.51.m1.1.1.2.1.cmml" xref="algorithm1.51.51.m1.1.1">subscript</csymbol><ci id="algorithm1.51.51.m1.1.1.2.2.cmml" xref="algorithm1.51.51.m1.1.1.2.2">Θ</ci><ci id="algorithm1.51.51.m1.1.1.2.3.cmml" xref="algorithm1.51.51.m1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.51.51.m1.1.1.3.cmml" xref="algorithm1.51.51.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.51.51.m1.1c">\Theta_{i}^{0}</annotation></semantics></math><span id="algorithm1.65.65.4" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.52.52.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.52.52.m2.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.52.52.m2.1.1" xref="algorithm1.52.52.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.52.52.m2.1b"><ci id="algorithm1.52.52.m2.1.1.cmml" xref="algorithm1.52.52.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.52.52.m2.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.65.65.5" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.53.53.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="algorithm1.53.53.m3.1a"><mi mathsize="80%" id="algorithm1.53.53.m3.1.1" xref="algorithm1.53.53.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="algorithm1.53.53.m3.1b"><ci id="algorithm1.53.53.m3.1.1.cmml" xref="algorithm1.53.53.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.53.53.m3.1c">F</annotation></semantics></math><span id="algorithm1.65.65.6" class="ltx_text" style="font-size:80%;">(</span><math id="algorithm1.54.54.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="algorithm1.54.54.m4.1a"><msub id="algorithm1.54.54.m4.1.1" xref="algorithm1.54.54.m4.1.1.cmml"><mi mathsize="80%" id="algorithm1.54.54.m4.1.1.2" xref="algorithm1.54.54.m4.1.1.2.cmml">D</mi><mi mathsize="80%" id="algorithm1.54.54.m4.1.1.3" xref="algorithm1.54.54.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.54.54.m4.1b"><apply id="algorithm1.54.54.m4.1.1.cmml" xref="algorithm1.54.54.m4.1.1"><csymbol cd="ambiguous" id="algorithm1.54.54.m4.1.1.1.cmml" xref="algorithm1.54.54.m4.1.1">subscript</csymbol><ci id="algorithm1.54.54.m4.1.1.2.cmml" xref="algorithm1.54.54.m4.1.1.2">𝐷</ci><ci id="algorithm1.54.54.m4.1.1.3.cmml" xref="algorithm1.54.54.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.54.54.m4.1c">D_{i}</annotation></semantics></math><span id="algorithm1.65.65.7" class="ltx_text" style="font-size:80%;">,</span><math id="algorithm1.55.55.m5.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="algorithm1.55.55.m5.1a"><msub id="algorithm1.55.55.m5.1.1" xref="algorithm1.55.55.m5.1.1.cmml"><mi mathsize="80%" id="algorithm1.55.55.m5.1.1.2" xref="algorithm1.55.55.m5.1.1.2.cmml">W</mi><mi mathsize="80%" id="algorithm1.55.55.m5.1.1.3" xref="algorithm1.55.55.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.55.55.m5.1b"><apply id="algorithm1.55.55.m5.1.1.cmml" xref="algorithm1.55.55.m5.1.1"><csymbol cd="ambiguous" id="algorithm1.55.55.m5.1.1.1.cmml" xref="algorithm1.55.55.m5.1.1">subscript</csymbol><ci id="algorithm1.55.55.m5.1.1.2.cmml" xref="algorithm1.55.55.m5.1.1.2">𝑊</ci><ci id="algorithm1.55.55.m5.1.1.3.cmml" xref="algorithm1.55.55.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.55.55.m5.1c">W_{i}</annotation></semantics></math><span id="algorithm1.65.65.8" class="ltx_text" style="font-size:80%;">);
</span><math id="algorithm1.56.56.m6.1" class="ltx_Math" alttext="\Theta_{i}^{1}" display="inline"><semantics id="algorithm1.56.56.m6.1a"><msubsup id="algorithm1.56.56.m6.1.1" xref="algorithm1.56.56.m6.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.56.56.m6.1.1.2.2" xref="algorithm1.56.56.m6.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.56.56.m6.1.1.2.3" xref="algorithm1.56.56.m6.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.56.56.m6.1.1.3" xref="algorithm1.56.56.m6.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.56.56.m6.1b"><apply id="algorithm1.56.56.m6.1.1.cmml" xref="algorithm1.56.56.m6.1.1"><csymbol cd="ambiguous" id="algorithm1.56.56.m6.1.1.1.cmml" xref="algorithm1.56.56.m6.1.1">superscript</csymbol><apply id="algorithm1.56.56.m6.1.1.2.cmml" xref="algorithm1.56.56.m6.1.1"><csymbol cd="ambiguous" id="algorithm1.56.56.m6.1.1.2.1.cmml" xref="algorithm1.56.56.m6.1.1">subscript</csymbol><ci id="algorithm1.56.56.m6.1.1.2.2.cmml" xref="algorithm1.56.56.m6.1.1.2.2">Θ</ci><ci id="algorithm1.56.56.m6.1.1.2.3.cmml" xref="algorithm1.56.56.m6.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.56.56.m6.1.1.3.cmml" xref="algorithm1.56.56.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.56.56.m6.1c">\Theta_{i}^{1}</annotation></semantics></math><span id="algorithm1.65.65.9" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.57.57.m7.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.57.57.m7.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.57.57.m7.1.1" xref="algorithm1.57.57.m7.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.57.57.m7.1b"><ci id="algorithm1.57.57.m7.1.1.cmml" xref="algorithm1.57.57.m7.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.57.57.m7.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.65.65.10" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.58.58.m8.1" class="ltx_Math" alttext="F" display="inline"><semantics id="algorithm1.58.58.m8.1a"><mi mathsize="80%" id="algorithm1.58.58.m8.1.1" xref="algorithm1.58.58.m8.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="algorithm1.58.58.m8.1b"><ci id="algorithm1.58.58.m8.1.1.cmml" xref="algorithm1.58.58.m8.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.58.58.m8.1c">F</annotation></semantics></math><span id="algorithm1.65.65.11" class="ltx_text" style="font-size:80%;">(</span><math id="algorithm1.59.59.m9.1" class="ltx_Math" alttext="D_{i}+S_{d}" display="inline"><semantics id="algorithm1.59.59.m9.1a"><mrow id="algorithm1.59.59.m9.1.1" xref="algorithm1.59.59.m9.1.1.cmml"><msub id="algorithm1.59.59.m9.1.1.2" xref="algorithm1.59.59.m9.1.1.2.cmml"><mi mathsize="80%" id="algorithm1.59.59.m9.1.1.2.2" xref="algorithm1.59.59.m9.1.1.2.2.cmml">D</mi><mi mathsize="80%" id="algorithm1.59.59.m9.1.1.2.3" xref="algorithm1.59.59.m9.1.1.2.3.cmml">i</mi></msub><mo mathsize="80%" id="algorithm1.59.59.m9.1.1.1" xref="algorithm1.59.59.m9.1.1.1.cmml">+</mo><msub id="algorithm1.59.59.m9.1.1.3" xref="algorithm1.59.59.m9.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.59.59.m9.1.1.3.2" xref="algorithm1.59.59.m9.1.1.3.2.cmml">S</mi><mi mathsize="80%" id="algorithm1.59.59.m9.1.1.3.3" xref="algorithm1.59.59.m9.1.1.3.3.cmml">d</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.59.59.m9.1b"><apply id="algorithm1.59.59.m9.1.1.cmml" xref="algorithm1.59.59.m9.1.1"><plus id="algorithm1.59.59.m9.1.1.1.cmml" xref="algorithm1.59.59.m9.1.1.1"></plus><apply id="algorithm1.59.59.m9.1.1.2.cmml" xref="algorithm1.59.59.m9.1.1.2"><csymbol cd="ambiguous" id="algorithm1.59.59.m9.1.1.2.1.cmml" xref="algorithm1.59.59.m9.1.1.2">subscript</csymbol><ci id="algorithm1.59.59.m9.1.1.2.2.cmml" xref="algorithm1.59.59.m9.1.1.2.2">𝐷</ci><ci id="algorithm1.59.59.m9.1.1.2.3.cmml" xref="algorithm1.59.59.m9.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.59.59.m9.1.1.3.cmml" xref="algorithm1.59.59.m9.1.1.3"><csymbol cd="ambiguous" id="algorithm1.59.59.m9.1.1.3.1.cmml" xref="algorithm1.59.59.m9.1.1.3">subscript</csymbol><ci id="algorithm1.59.59.m9.1.1.3.2.cmml" xref="algorithm1.59.59.m9.1.1.3.2">𝑆</ci><ci id="algorithm1.59.59.m9.1.1.3.3.cmml" xref="algorithm1.59.59.m9.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.59.59.m9.1c">D_{i}+S_{d}</annotation></semantics></math><span id="algorithm1.65.65.12" class="ltx_text" style="font-size:80%;">,</span><math id="algorithm1.60.60.m10.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="algorithm1.60.60.m10.1a"><msub id="algorithm1.60.60.m10.1.1" xref="algorithm1.60.60.m10.1.1.cmml"><mi mathsize="80%" id="algorithm1.60.60.m10.1.1.2" xref="algorithm1.60.60.m10.1.1.2.cmml">W</mi><mi mathsize="80%" id="algorithm1.60.60.m10.1.1.3" xref="algorithm1.60.60.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.60.60.m10.1b"><apply id="algorithm1.60.60.m10.1.1.cmml" xref="algorithm1.60.60.m10.1.1"><csymbol cd="ambiguous" id="algorithm1.60.60.m10.1.1.1.cmml" xref="algorithm1.60.60.m10.1.1">subscript</csymbol><ci id="algorithm1.60.60.m10.1.1.2.cmml" xref="algorithm1.60.60.m10.1.1.2">𝑊</ci><ci id="algorithm1.60.60.m10.1.1.3.cmml" xref="algorithm1.60.60.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.60.60.m10.1c">W_{i}</annotation></semantics></math><span id="algorithm1.65.65.13" class="ltx_text" style="font-size:80%;">);
</span><math id="algorithm1.61.61.m11.1" class="ltx_Math" alttext="\Theta_{i}^{2}" display="inline"><semantics id="algorithm1.61.61.m11.1a"><msubsup id="algorithm1.61.61.m11.1.1" xref="algorithm1.61.61.m11.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.61.61.m11.1.1.2.2" xref="algorithm1.61.61.m11.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.61.61.m11.1.1.2.3" xref="algorithm1.61.61.m11.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.61.61.m11.1.1.3" xref="algorithm1.61.61.m11.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.61.61.m11.1b"><apply id="algorithm1.61.61.m11.1.1.cmml" xref="algorithm1.61.61.m11.1.1"><csymbol cd="ambiguous" id="algorithm1.61.61.m11.1.1.1.cmml" xref="algorithm1.61.61.m11.1.1">superscript</csymbol><apply id="algorithm1.61.61.m11.1.1.2.cmml" xref="algorithm1.61.61.m11.1.1"><csymbol cd="ambiguous" id="algorithm1.61.61.m11.1.1.2.1.cmml" xref="algorithm1.61.61.m11.1.1">subscript</csymbol><ci id="algorithm1.61.61.m11.1.1.2.2.cmml" xref="algorithm1.61.61.m11.1.1.2.2">Θ</ci><ci id="algorithm1.61.61.m11.1.1.2.3.cmml" xref="algorithm1.61.61.m11.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.61.61.m11.1.1.3.cmml" xref="algorithm1.61.61.m11.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.61.61.m11.1c">\Theta_{i}^{2}</annotation></semantics></math><span id="algorithm1.65.65.14" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.62.62.m12.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="algorithm1.62.62.m12.1a"><mo mathsize="80%" stretchy="false" id="algorithm1.62.62.m12.1.1" xref="algorithm1.62.62.m12.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="algorithm1.62.62.m12.1b"><ci id="algorithm1.62.62.m12.1.1.cmml" xref="algorithm1.62.62.m12.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.62.62.m12.1c">\leftarrow</annotation></semantics></math><span id="algorithm1.65.65.15" class="ltx_text" style="font-size:80%;"> </span><math id="algorithm1.63.63.m13.1" class="ltx_Math" alttext="F" display="inline"><semantics id="algorithm1.63.63.m13.1a"><mi mathsize="80%" id="algorithm1.63.63.m13.1.1" xref="algorithm1.63.63.m13.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="algorithm1.63.63.m13.1b"><ci id="algorithm1.63.63.m13.1.1.cmml" xref="algorithm1.63.63.m13.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.63.63.m13.1c">F</annotation></semantics></math><span id="algorithm1.65.65.16" class="ltx_text" style="font-size:80%;">(</span><math id="algorithm1.64.64.m14.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="algorithm1.64.64.m14.1a"><msub id="algorithm1.64.64.m14.1.1" xref="algorithm1.64.64.m14.1.1.cmml"><mi mathsize="80%" id="algorithm1.64.64.m14.1.1.2" xref="algorithm1.64.64.m14.1.1.2.cmml">D</mi><mi mathsize="80%" id="algorithm1.64.64.m14.1.1.3" xref="algorithm1.64.64.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.64.64.m14.1b"><apply id="algorithm1.64.64.m14.1.1.cmml" xref="algorithm1.64.64.m14.1.1"><csymbol cd="ambiguous" id="algorithm1.64.64.m14.1.1.1.cmml" xref="algorithm1.64.64.m14.1.1">subscript</csymbol><ci id="algorithm1.64.64.m14.1.1.2.cmml" xref="algorithm1.64.64.m14.1.1.2">𝐷</ci><ci id="algorithm1.64.64.m14.1.1.3.cmml" xref="algorithm1.64.64.m14.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.64.64.m14.1c">D_{i}</annotation></semantics></math><span id="algorithm1.65.65.17" class="ltx_text" style="font-size:80%;">,</span><math id="algorithm1.65.65.m15.1" class="ltx_Math" alttext="W_{i}+S_{w}" display="inline"><semantics id="algorithm1.65.65.m15.1a"><mrow id="algorithm1.65.65.m15.1.1" xref="algorithm1.65.65.m15.1.1.cmml"><msub id="algorithm1.65.65.m15.1.1.2" xref="algorithm1.65.65.m15.1.1.2.cmml"><mi mathsize="80%" id="algorithm1.65.65.m15.1.1.2.2" xref="algorithm1.65.65.m15.1.1.2.2.cmml">W</mi><mi mathsize="80%" id="algorithm1.65.65.m15.1.1.2.3" xref="algorithm1.65.65.m15.1.1.2.3.cmml">i</mi></msub><mo mathsize="80%" id="algorithm1.65.65.m15.1.1.1" xref="algorithm1.65.65.m15.1.1.1.cmml">+</mo><msub id="algorithm1.65.65.m15.1.1.3" xref="algorithm1.65.65.m15.1.1.3.cmml"><mi mathsize="80%" id="algorithm1.65.65.m15.1.1.3.2" xref="algorithm1.65.65.m15.1.1.3.2.cmml">S</mi><mi mathsize="80%" id="algorithm1.65.65.m15.1.1.3.3" xref="algorithm1.65.65.m15.1.1.3.3.cmml">w</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.65.65.m15.1b"><apply id="algorithm1.65.65.m15.1.1.cmml" xref="algorithm1.65.65.m15.1.1"><plus id="algorithm1.65.65.m15.1.1.1.cmml" xref="algorithm1.65.65.m15.1.1.1"></plus><apply id="algorithm1.65.65.m15.1.1.2.cmml" xref="algorithm1.65.65.m15.1.1.2"><csymbol cd="ambiguous" id="algorithm1.65.65.m15.1.1.2.1.cmml" xref="algorithm1.65.65.m15.1.1.2">subscript</csymbol><ci id="algorithm1.65.65.m15.1.1.2.2.cmml" xref="algorithm1.65.65.m15.1.1.2.2">𝑊</ci><ci id="algorithm1.65.65.m15.1.1.2.3.cmml" xref="algorithm1.65.65.m15.1.1.2.3">𝑖</ci></apply><apply id="algorithm1.65.65.m15.1.1.3.cmml" xref="algorithm1.65.65.m15.1.1.3"><csymbol cd="ambiguous" id="algorithm1.65.65.m15.1.1.3.1.cmml" xref="algorithm1.65.65.m15.1.1.3">subscript</csymbol><ci id="algorithm1.65.65.m15.1.1.3.2.cmml" xref="algorithm1.65.65.m15.1.1.3.2">𝑆</ci><ci id="algorithm1.65.65.m15.1.1.3.3.cmml" xref="algorithm1.65.65.m15.1.1.3.3">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.65.65.m15.1c">W_{i}+S_{w}</annotation></semantics></math><span id="algorithm1.65.65.18" class="ltx_text" style="font-size:80%;">);</span>
</div>
<div id="algorithm1.71.71" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.71.1.1.1" class="ltx_text" style="font-size:80%;">29</span></span><span id="algorithm1.71.71.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.71.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.71.4" class="ltx_text" style="font-size:80%;">
Sends </span><math id="algorithm1.66.66.m1.1" class="ltx_Math" alttext="\Theta_{i}^{0}" display="inline"><semantics id="algorithm1.66.66.m1.1a"><msubsup id="algorithm1.66.66.m1.1.1" xref="algorithm1.66.66.m1.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.66.66.m1.1.1.2.2" xref="algorithm1.66.66.m1.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.66.66.m1.1.1.2.3" xref="algorithm1.66.66.m1.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.66.66.m1.1.1.3" xref="algorithm1.66.66.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.66.66.m1.1b"><apply id="algorithm1.66.66.m1.1.1.cmml" xref="algorithm1.66.66.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.66.66.m1.1.1.1.cmml" xref="algorithm1.66.66.m1.1.1">superscript</csymbol><apply id="algorithm1.66.66.m1.1.1.2.cmml" xref="algorithm1.66.66.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.66.66.m1.1.1.2.1.cmml" xref="algorithm1.66.66.m1.1.1">subscript</csymbol><ci id="algorithm1.66.66.m1.1.1.2.2.cmml" xref="algorithm1.66.66.m1.1.1.2.2">Θ</ci><ci id="algorithm1.66.66.m1.1.1.2.3.cmml" xref="algorithm1.66.66.m1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.66.66.m1.1.1.3.cmml" xref="algorithm1.66.66.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.66.66.m1.1c">\Theta_{i}^{0}</annotation></semantics></math><span id="algorithm1.71.71.5" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.67.67.m2.1" class="ltx_Math" alttext="\Theta_{i}^{1}" display="inline"><semantics id="algorithm1.67.67.m2.1a"><msubsup id="algorithm1.67.67.m2.1.1" xref="algorithm1.67.67.m2.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.67.67.m2.1.1.2.2" xref="algorithm1.67.67.m2.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.67.67.m2.1.1.2.3" xref="algorithm1.67.67.m2.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.67.67.m2.1.1.3" xref="algorithm1.67.67.m2.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.67.67.m2.1b"><apply id="algorithm1.67.67.m2.1.1.cmml" xref="algorithm1.67.67.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.67.67.m2.1.1.1.cmml" xref="algorithm1.67.67.m2.1.1">superscript</csymbol><apply id="algorithm1.67.67.m2.1.1.2.cmml" xref="algorithm1.67.67.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.67.67.m2.1.1.2.1.cmml" xref="algorithm1.67.67.m2.1.1">subscript</csymbol><ci id="algorithm1.67.67.m2.1.1.2.2.cmml" xref="algorithm1.67.67.m2.1.1.2.2">Θ</ci><ci id="algorithm1.67.67.m2.1.1.2.3.cmml" xref="algorithm1.67.67.m2.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.67.67.m2.1.1.3.cmml" xref="algorithm1.67.67.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.67.67.m2.1c">\Theta_{i}^{1}</annotation></semantics></math><span id="algorithm1.71.71.6" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.68.68.m3.1" class="ltx_Math" alttext="\Theta_{i}^{2}" display="inline"><semantics id="algorithm1.68.68.m3.1a"><msubsup id="algorithm1.68.68.m3.1.1" xref="algorithm1.68.68.m3.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="algorithm1.68.68.m3.1.1.2.2" xref="algorithm1.68.68.m3.1.1.2.2.cmml">Θ</mi><mi mathsize="80%" id="algorithm1.68.68.m3.1.1.2.3" xref="algorithm1.68.68.m3.1.1.2.3.cmml">i</mi><mn mathsize="80%" id="algorithm1.68.68.m3.1.1.3" xref="algorithm1.68.68.m3.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.68.68.m3.1b"><apply id="algorithm1.68.68.m3.1.1.cmml" xref="algorithm1.68.68.m3.1.1"><csymbol cd="ambiguous" id="algorithm1.68.68.m3.1.1.1.cmml" xref="algorithm1.68.68.m3.1.1">superscript</csymbol><apply id="algorithm1.68.68.m3.1.1.2.cmml" xref="algorithm1.68.68.m3.1.1"><csymbol cd="ambiguous" id="algorithm1.68.68.m3.1.1.2.1.cmml" xref="algorithm1.68.68.m3.1.1">subscript</csymbol><ci id="algorithm1.68.68.m3.1.1.2.2.cmml" xref="algorithm1.68.68.m3.1.1.2.2">Θ</ci><ci id="algorithm1.68.68.m3.1.1.2.3.cmml" xref="algorithm1.68.68.m3.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.68.68.m3.1.1.3.cmml" xref="algorithm1.68.68.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.68.68.m3.1c">\Theta_{i}^{2}</annotation></semantics></math><span id="algorithm1.71.71.7" class="ltx_text" style="font-size:80%;"> to </span><math id="algorithm1.69.69.m4.1" class="ltx_Math" alttext="Trial_{0}" display="inline"><semantics id="algorithm1.69.69.m4.1a"><mrow id="algorithm1.69.69.m4.1.1" xref="algorithm1.69.69.m4.1.1.cmml"><mi mathsize="80%" id="algorithm1.69.69.m4.1.1.2" xref="algorithm1.69.69.m4.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.69.69.m4.1.1.1" xref="algorithm1.69.69.m4.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.69.69.m4.1.1.3" xref="algorithm1.69.69.m4.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.69.69.m4.1.1.1a" xref="algorithm1.69.69.m4.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.69.69.m4.1.1.4" xref="algorithm1.69.69.m4.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.69.69.m4.1.1.1b" xref="algorithm1.69.69.m4.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.69.69.m4.1.1.5" xref="algorithm1.69.69.m4.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.69.69.m4.1.1.1c" xref="algorithm1.69.69.m4.1.1.1.cmml">​</mo><msub id="algorithm1.69.69.m4.1.1.6" xref="algorithm1.69.69.m4.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.69.69.m4.1.1.6.2" xref="algorithm1.69.69.m4.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.69.69.m4.1.1.6.3" xref="algorithm1.69.69.m4.1.1.6.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.69.69.m4.1b"><apply id="algorithm1.69.69.m4.1.1.cmml" xref="algorithm1.69.69.m4.1.1"><times id="algorithm1.69.69.m4.1.1.1.cmml" xref="algorithm1.69.69.m4.1.1.1"></times><ci id="algorithm1.69.69.m4.1.1.2.cmml" xref="algorithm1.69.69.m4.1.1.2">𝑇</ci><ci id="algorithm1.69.69.m4.1.1.3.cmml" xref="algorithm1.69.69.m4.1.1.3">𝑟</ci><ci id="algorithm1.69.69.m4.1.1.4.cmml" xref="algorithm1.69.69.m4.1.1.4">𝑖</ci><ci id="algorithm1.69.69.m4.1.1.5.cmml" xref="algorithm1.69.69.m4.1.1.5">𝑎</ci><apply id="algorithm1.69.69.m4.1.1.6.cmml" xref="algorithm1.69.69.m4.1.1.6"><csymbol cd="ambiguous" id="algorithm1.69.69.m4.1.1.6.1.cmml" xref="algorithm1.69.69.m4.1.1.6">subscript</csymbol><ci id="algorithm1.69.69.m4.1.1.6.2.cmml" xref="algorithm1.69.69.m4.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.69.69.m4.1.1.6.3.cmml" xref="algorithm1.69.69.m4.1.1.6.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.69.69.m4.1c">Trial_{0}</annotation></semantics></math><span id="algorithm1.71.71.8" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.70.70.m5.1" class="ltx_Math" alttext="Trial_{1}" display="inline"><semantics id="algorithm1.70.70.m5.1a"><mrow id="algorithm1.70.70.m5.1.1" xref="algorithm1.70.70.m5.1.1.cmml"><mi mathsize="80%" id="algorithm1.70.70.m5.1.1.2" xref="algorithm1.70.70.m5.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.70.70.m5.1.1.1" xref="algorithm1.70.70.m5.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.70.70.m5.1.1.3" xref="algorithm1.70.70.m5.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.70.70.m5.1.1.1a" xref="algorithm1.70.70.m5.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.70.70.m5.1.1.4" xref="algorithm1.70.70.m5.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.70.70.m5.1.1.1b" xref="algorithm1.70.70.m5.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.70.70.m5.1.1.5" xref="algorithm1.70.70.m5.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.70.70.m5.1.1.1c" xref="algorithm1.70.70.m5.1.1.1.cmml">​</mo><msub id="algorithm1.70.70.m5.1.1.6" xref="algorithm1.70.70.m5.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.70.70.m5.1.1.6.2" xref="algorithm1.70.70.m5.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.70.70.m5.1.1.6.3" xref="algorithm1.70.70.m5.1.1.6.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.70.70.m5.1b"><apply id="algorithm1.70.70.m5.1.1.cmml" xref="algorithm1.70.70.m5.1.1"><times id="algorithm1.70.70.m5.1.1.1.cmml" xref="algorithm1.70.70.m5.1.1.1"></times><ci id="algorithm1.70.70.m5.1.1.2.cmml" xref="algorithm1.70.70.m5.1.1.2">𝑇</ci><ci id="algorithm1.70.70.m5.1.1.3.cmml" xref="algorithm1.70.70.m5.1.1.3">𝑟</ci><ci id="algorithm1.70.70.m5.1.1.4.cmml" xref="algorithm1.70.70.m5.1.1.4">𝑖</ci><ci id="algorithm1.70.70.m5.1.1.5.cmml" xref="algorithm1.70.70.m5.1.1.5">𝑎</ci><apply id="algorithm1.70.70.m5.1.1.6.cmml" xref="algorithm1.70.70.m5.1.1.6"><csymbol cd="ambiguous" id="algorithm1.70.70.m5.1.1.6.1.cmml" xref="algorithm1.70.70.m5.1.1.6">subscript</csymbol><ci id="algorithm1.70.70.m5.1.1.6.2.cmml" xref="algorithm1.70.70.m5.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.70.70.m5.1.1.6.3.cmml" xref="algorithm1.70.70.m5.1.1.6.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.70.70.m5.1c">Trial_{1}</annotation></semantics></math><span id="algorithm1.71.71.9" class="ltx_text" style="font-size:80%;">, </span><math id="algorithm1.71.71.m6.1" class="ltx_Math" alttext="Trial_{2}" display="inline"><semantics id="algorithm1.71.71.m6.1a"><mrow id="algorithm1.71.71.m6.1.1" xref="algorithm1.71.71.m6.1.1.cmml"><mi mathsize="80%" id="algorithm1.71.71.m6.1.1.2" xref="algorithm1.71.71.m6.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="algorithm1.71.71.m6.1.1.1" xref="algorithm1.71.71.m6.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.71.71.m6.1.1.3" xref="algorithm1.71.71.m6.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="algorithm1.71.71.m6.1.1.1a" xref="algorithm1.71.71.m6.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.71.71.m6.1.1.4" xref="algorithm1.71.71.m6.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="algorithm1.71.71.m6.1.1.1b" xref="algorithm1.71.71.m6.1.1.1.cmml">​</mo><mi mathsize="80%" id="algorithm1.71.71.m6.1.1.5" xref="algorithm1.71.71.m6.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="algorithm1.71.71.m6.1.1.1c" xref="algorithm1.71.71.m6.1.1.1.cmml">​</mo><msub id="algorithm1.71.71.m6.1.1.6" xref="algorithm1.71.71.m6.1.1.6.cmml"><mi mathsize="80%" id="algorithm1.71.71.m6.1.1.6.2" xref="algorithm1.71.71.m6.1.1.6.2.cmml">l</mi><mn mathsize="80%" id="algorithm1.71.71.m6.1.1.6.3" xref="algorithm1.71.71.m6.1.1.6.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.71.71.m6.1b"><apply id="algorithm1.71.71.m6.1.1.cmml" xref="algorithm1.71.71.m6.1.1"><times id="algorithm1.71.71.m6.1.1.1.cmml" xref="algorithm1.71.71.m6.1.1.1"></times><ci id="algorithm1.71.71.m6.1.1.2.cmml" xref="algorithm1.71.71.m6.1.1.2">𝑇</ci><ci id="algorithm1.71.71.m6.1.1.3.cmml" xref="algorithm1.71.71.m6.1.1.3">𝑟</ci><ci id="algorithm1.71.71.m6.1.1.4.cmml" xref="algorithm1.71.71.m6.1.1.4">𝑖</ci><ci id="algorithm1.71.71.m6.1.1.5.cmml" xref="algorithm1.71.71.m6.1.1.5">𝑎</ci><apply id="algorithm1.71.71.m6.1.1.6.cmml" xref="algorithm1.71.71.m6.1.1.6"><csymbol cd="ambiguous" id="algorithm1.71.71.m6.1.1.6.1.cmml" xref="algorithm1.71.71.m6.1.1.6">subscript</csymbol><ci id="algorithm1.71.71.m6.1.1.6.2.cmml" xref="algorithm1.71.71.m6.1.1.6.2">𝑙</ci><cn type="integer" id="algorithm1.71.71.m6.1.1.6.3.cmml" xref="algorithm1.71.71.m6.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.71.71.m6.1c">Trial_{2}</annotation></semantics></math><span id="algorithm1.71.71.10" class="ltx_text" style="font-size:80%;"> seperately;</span>
</div>
<div id="algorithm1.71.84" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algorithm1.71.84.1.1.1" class="ltx_text" style="font-size:80%;">30</span></span><span id="algorithm1.71.84.2" class="ltx_text" style="font-size:80%;">  </span><span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span><span id="algorithm1.71.84.3" class="ltx_text" style="font-size:80%;">   </span><span id="algorithm1.71.84.4" class="ltx_text" style="font-size:80%;">
Parallel: Client_training(i,k).
</span>
</div>
<div id="algorithm1.71.85" class="ltx_listingline">
<span id="algorithm1.71.85.1" class="ltx_text" style="font-size:80%;">
</span>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.75.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Our Online Configurator</figcaption>
</figure>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Configurator algorithm in detail</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">Algorithm <a href="#algorithm1" title="Algorithm 1 ‣ 3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows how <span id="S3.SS3.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> progressively upgrades the configuration of adapters during a training session.
Unlike the traditional FL scheme where only one global model with a fixed structure undergoes the training, in <span id="S3.SS3.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> the cloud aggregator periodically dispatches the global model to three groups of clients: one is to train with the current configuration, one with a deeper one and the other with a wider one (line 2–5, 23-26).
After a few rounds of parallel training (line 27, 19–22, 7–10, 18), the aggregator server checks the accuracy of three global models and re-starts the process on the model with the highest accuracy (line 12–15).
Note that when the aggregator checks the accuracy, the three global models undergo different numbers of global rounds because the per-round training time and network time depend on the adapter configuration (<math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS1" title="3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Therefore, the training speed of different tuning depth/width is considered in this mechanism.
Except that, the clients and aggregator follow the common FL process in local training (line 19–22) and model aggregation (line 8–9).</p>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p2.5" class="ltx_p">As described in Algorithm <a href="#algorithm1" title="Algorithm 1 ‣ 3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (line 23–27), the models dispatched to different groups are with different model configurations.
Group <math id="S3.SS3.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="Trial_{0}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1a" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.4" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1b" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.5" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1c" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">​</mo><msub id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.2.cmml">l</mi><mn id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1"><times id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1"></times><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2">𝑇</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3">𝑟</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.4.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.4">𝑖</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.5.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.5">𝑎</ci><apply id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.6.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.1.m1.1c">Trial_{0}</annotation></semantics></math> inherits the learned adapter from the previous winner track whereas <math id="S3.SS3.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="Trial_{1}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.2.m2.1a"><mrow id="S3.SS3.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1a" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.4" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1b" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.5" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1c" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.cmml"><mi id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.2" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.2.cmml">l</mi><mn id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.3" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1"><times id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.1"></times><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.2">𝑇</ci><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.3">𝑟</ci><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.4.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.4">𝑖</ci><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.5.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.5">𝑎</ci><apply id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.2.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.3.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m2.1.1.6.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.2.m2.1c">Trial_{1}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="Trial_{2}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.3.m3.1a"><mrow id="S3.SS3.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1a" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.4" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1b" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.5" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1c" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml">​</mo><msub id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.2" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.2.cmml">l</mi><mn id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.3" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1"><times id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.1"></times><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.2">𝑇</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.3">𝑟</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.4.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.4">𝑖</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.5.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.5">𝑎</ci><apply id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.2">𝑙</ci><cn type="integer" id="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.3.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m3.1.1.6.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.3.m3.1c">Trial_{2}</annotation></semantics></math> also inherit the old adapters but add extra depth and width, respectively.
The step sizes of depth and width are pre-defined, e.g., <math id="S3.SS3.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="S_{d}=1" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.4.m4.1a"><mrow id="S3.SS3.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml"><msub id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.2" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.3" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.3.cmml">d</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.3" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1"><eq id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.1"></eq><apply id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.2.3">𝑑</ci></apply><cn type="integer" id="S3.SS3.SSS0.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.4.m4.1c">S_{d}=1</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="S_{w}=8" display="inline"><semantics id="S3.SS3.SSS0.Px1.p2.5.m5.1a"><mrow id="S3.SS3.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml"><msub id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.2" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.2.cmml">S</mi><mi id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.3" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.3.cmml">w</mi></msub><mo id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.1" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.3" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.5.m5.1b"><apply id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1"><eq id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.1"></eq><apply id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.2">𝑆</ci><ci id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.2.3">𝑤</ci></apply><cn type="integer" id="S3.SS3.SSS0.Px1.p2.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.5.m5.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.5.m5.1c">S_{w}=8</annotation></semantics></math> by default, respectively.
The depths and widths are both added on the fly.
All newly added weights are normalized with mean (0.0) and stddev (0.02).
We experiment with two ways to expand the adapter width: vertical and parallel stacking.
Our micro experiments show that vertically stacking will outperform parallel stacking with the same amount of parameters.
So we always use this method.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p3.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">Integration with the existing FL frameworks</span>
<span id="S3.SS3.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> is compatible with how existing FL frameworks manage clients for training efficiency,
a key system component having received high research attention <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>; <a href="#bib.bib48" title="" class="ltx_ref">li2021sample, </a>)</cite>.
This is because the adapters and their configuration scheduler are intentionally designed to be decoupled from which device or data will be involved in per-round training.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p4" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p4.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_bold">Fairness across trial groups</span>
<span id="S3.SS3.SSS0.Px1.p4.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> could make a fair comparison across trial groups under client/data heterogeneity.
<span id="S3.SS3.SSS0.Px1.p4.1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> uses a two-stage client selection: it first follows the used FL method to select clients (might be prioritized and biased), and then “randomly” partitions them into different groups.
It statistically ensures a fair comparison across trial groups.
The same notion applies to data heterogeneity as well.
Moreover, <span id="S3.SS3.SSS0.Px1.p4.1.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> re-selects clients/data for each group per round.
It compares performance across groups every N round.
<span id="S3.SS3.SSS0.Px1.p4.1.5" class="ltx_text ltx_font_typewriter">FedAdapter</span> hence mitigates potential unfairness given a large N (N¿10), per the sampling theory.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p5" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p5.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p5.1.1" class="ltx_text ltx_font_bold">User overhead with sideline trials</span>
The extra cost incurred by the sideline trials is amortized across different clients and shall not impose higher impacts on user experience.
In an end-to-end manner, using trial groups reduces the elapsed training time towards convergence by discovering better adapter configurations, thus reducing the total cost accumulated on all clients as reported in <math id="S3.SS3.SSS0.Px1.p5.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS3.SSS0.Px1.p5.1.m1.1a"><mi mathvariant="normal" id="S3.SS3.SSS0.Px1.p5.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p5.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p5.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p5.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p5.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p5.1.m1.1c">\S</annotation></semantics></math><a href="#S6.SS2" title="6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>.
This design makes <span id="S3.SS3.SSS0.Px1.p5.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> more scalable to available clients: using those extra clients for trial is much more beneficial than asking them to participate in training, either in convergence speed or end-to-end resource cost.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>FedNLP Activation Cache</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">Using adapter (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.p1.1.m1.1a"><mi mathvariant="normal" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS1" title="3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) significantly reduces the network cost in FedNLP tasks, exposing client-side computation as the next major bottleneck for model convergence.
Layer-freezing reduces the training computations by early-stopping the backward propagation, but does not address the computation cost at forward pass.
For instance, with tuning depth as 2 of 12 transformer blocks in BERT, the forward computation takes <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="12/(12+2*2)=75\%" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mrow id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml"><mn id="S4.p1.2.m2.1.1.1.3" xref="S4.p1.2.m2.1.1.1.3.cmml">12</mn><mo id="S4.p1.2.m2.1.1.1.2" xref="S4.p1.2.m2.1.1.1.2.cmml">/</mo><mrow id="S4.p1.2.m2.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p1.2.m2.1.1.1.1.1.2" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p1.2.m2.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml"><mn id="S4.p1.2.m2.1.1.1.1.1.1.2" xref="S4.p1.2.m2.1.1.1.1.1.1.2.cmml">12</mn><mo id="S4.p1.2.m2.1.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p1.2.m2.1.1.1.1.1.1.3" xref="S4.p1.2.m2.1.1.1.1.1.1.3.cmml"><mn id="S4.p1.2.m2.1.1.1.1.1.1.3.2" xref="S4.p1.2.m2.1.1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p1.2.m2.1.1.1.1.1.1.3.1" xref="S4.p1.2.m2.1.1.1.1.1.1.3.1.cmml">∗</mo><mn id="S4.p1.2.m2.1.1.1.1.1.1.3.3" xref="S4.p1.2.m2.1.1.1.1.1.1.3.3.cmml">2</mn></mrow></mrow><mo stretchy="false" id="S4.p1.2.m2.1.1.1.1.1.3" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">=</mo><mrow id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mn id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml">75</mn><mo id="S4.p1.2.m2.1.1.3.1" xref="S4.p1.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><eq id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2"></eq><apply id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"><divide id="S4.p1.2.m2.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1.2"></divide><cn type="integer" id="S4.p1.2.m2.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.3">12</cn><apply id="S4.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1"><plus id="S4.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.1"></plus><cn type="integer" id="S4.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.2">12</cn><apply id="S4.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.3"><times id="S4.p1.2.m2.1.1.1.1.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.3.1"></times><cn type="integer" id="S4.p1.2.m2.1.1.1.1.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.3.2">2</cn><cn type="integer" id="S4.p1.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1.3.3">2</cn></apply></apply></apply><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p1.2.m2.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.3.2">75</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">12/(12+2*2)=75\%</annotation></semantics></math> of the total computation, considering that the backward propagation computation of each layer is approximately 2 times of the forward counterpart <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib61" title="" class="ltx_ref">qi2016paleo, </a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Opportunities</span>
To reduce the client computation, we exploit two unique FedNLP opportunities:
(i) Throughout a training session, a device participates in many FL rounds.
Across rounds, the device executes local training on the same set of local samples.
In typical FL scenarios, it takes tens of thousands of rounds till model convergence <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>)</cite>.
(ii) During on-device training, the weights of the bottom transformer blocks that do not have adapters inserted are fixed.
Moreover, those untouched transformer blocks remain the same across FL rounds till <span id="S4.p2.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> switches the tuning depth (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.p2.1.m1.1a"><mi mathvariant="normal" id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS3" title="3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
In our experiments we observe a tuning depth upgrading interval typically at hundreds to thousands of rounds.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.15" class="ltx_p"><span id="S4.p3.15.1" class="ltx_text ltx_font_bold">The activation cache</span>
Our key idea is to leverage both the static input and the fixed bottom layers so a client’s computed activations can be reused across rounds.
Assuming the cloud aggregator selects a device to train with depth <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="d_{prev}" display="inline"><semantics id="S4.p3.1.m1.1a"><msub id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">d</mi><mrow id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1" xref="S4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.3.3" xref="S4.p3.1.m1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1a" xref="S4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.3.4" xref="S4.p3.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.3.1b" xref="S4.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.3.5" xref="S4.p3.1.m1.1.1.3.5.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">𝑑</ci><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><times id="S4.p3.1.m1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.3.1"></times><ci id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">𝑝</ci><ci id="S4.p3.1.m1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.3.3">𝑟</ci><ci id="S4.p3.1.m1.1.1.3.4.cmml" xref="S4.p3.1.m1.1.1.3.4">𝑒</ci><ci id="S4.p3.1.m1.1.1.3.5.cmml" xref="S4.p3.1.m1.1.1.3.5">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">d_{prev}</annotation></semantics></math> at round <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="r_{prev}" display="inline"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">r</mi><mrow id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml"><mi id="S4.p3.2.m2.1.1.3.2" xref="S4.p3.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1" xref="S4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3.3" xref="S4.p3.2.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1a" xref="S4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3.4" xref="S4.p3.2.m2.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.3.1b" xref="S4.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3.5" xref="S4.p3.2.m2.1.1.3.5.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝑟</ci><apply id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3"><times id="S4.p3.2.m2.1.1.3.1.cmml" xref="S4.p3.2.m2.1.1.3.1"></times><ci id="S4.p3.2.m2.1.1.3.2.cmml" xref="S4.p3.2.m2.1.1.3.2">𝑝</ci><ci id="S4.p3.2.m2.1.1.3.3.cmml" xref="S4.p3.2.m2.1.1.3.3">𝑟</ci><ci id="S4.p3.2.m2.1.1.3.4.cmml" xref="S4.p3.2.m2.1.1.3.4">𝑒</ci><ci id="S4.p3.2.m2.1.1.3.5.cmml" xref="S4.p3.2.m2.1.1.3.5">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">r_{prev}</annotation></semantics></math>.
During the training, the device also extracts the output of the last fixed bottom layer for each input batch, i.e., the output of the <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="(D-d_{prev})_{th}" display="inline"><semantics id="S4.p3.3.m3.1a"><msub id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mrow id="S4.p3.3.m3.1.1.1.1" xref="S4.p3.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.3.m3.1.1.1.1.2" xref="S4.p3.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.3.m3.1.1.1.1.1" xref="S4.p3.3.m3.1.1.1.1.1.cmml"><mi id="S4.p3.3.m3.1.1.1.1.1.2" xref="S4.p3.3.m3.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.3.m3.1.1.1.1.1.1" xref="S4.p3.3.m3.1.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.3.m3.1.1.1.1.1.3" xref="S4.p3.3.m3.1.1.1.1.1.3.cmml"><mi id="S4.p3.3.m3.1.1.1.1.1.3.2" xref="S4.p3.3.m3.1.1.1.1.1.3.2.cmml">d</mi><mrow id="S4.p3.3.m3.1.1.1.1.1.3.3" xref="S4.p3.3.m3.1.1.1.1.1.3.3.cmml"><mi id="S4.p3.3.m3.1.1.1.1.1.3.3.2" xref="S4.p3.3.m3.1.1.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.1.1.1.3.3.1" xref="S4.p3.3.m3.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.1.1.1.3.3.3" xref="S4.p3.3.m3.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.1.1.1.3.3.1a" xref="S4.p3.3.m3.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.1.1.1.3.3.4" xref="S4.p3.3.m3.1.1.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.1.1.1.3.3.1b" xref="S4.p3.3.m3.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.1.1.1.3.3.5" xref="S4.p3.3.m3.1.1.1.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><mo stretchy="false" id="S4.p3.3.m3.1.1.1.1.3" xref="S4.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml"><mi id="S4.p3.3.m3.1.1.3.2" xref="S4.p3.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.3.1" xref="S4.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.3.3" xref="S4.p3.3.m3.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1">subscript</csymbol><apply id="S4.p3.3.m3.1.1.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1.1"><minus id="S4.p3.3.m3.1.1.1.1.1.1.cmml" xref="S4.p3.3.m3.1.1.1.1.1.1"></minus><ci id="S4.p3.3.m3.1.1.1.1.1.2.cmml" xref="S4.p3.3.m3.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.3.m3.1.1.1.1.1.3.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.1.1.3.1.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.3.m3.1.1.1.1.1.3.2.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.2">𝑑</ci><apply id="S4.p3.3.m3.1.1.1.1.1.3.3.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3"><times id="S4.p3.3.m3.1.1.1.1.1.3.3.1.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3.1"></times><ci id="S4.p3.3.m3.1.1.1.1.1.3.3.2.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S4.p3.3.m3.1.1.1.1.1.3.3.3.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3.3">𝑟</ci><ci id="S4.p3.3.m3.1.1.1.1.1.3.3.4.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3.4">𝑒</ci><ci id="S4.p3.3.m3.1.1.1.1.1.3.3.5.cmml" xref="S4.p3.3.m3.1.1.1.1.1.3.3.5">𝑣</ci></apply></apply></apply><apply id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3"><times id="S4.p3.3.m3.1.1.3.1.cmml" xref="S4.p3.3.m3.1.1.3.1"></times><ci id="S4.p3.3.m3.1.1.3.2.cmml" xref="S4.p3.3.m3.1.1.3.2">𝑡</ci><ci id="S4.p3.3.m3.1.1.3.3.cmml" xref="S4.p3.3.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">(D-d_{prev})_{th}</annotation></semantics></math> transformer layer and stores them in local storage.
For the next time device <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p3.4.m4.1a"><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">k</annotation></semantics></math> is selected, it first inquires the cloud aggregator that how deep the model has been tuned since the last time it’s selected, i.e., <math id="S4.p3.5.m5.5" class="ltx_Math" alttext="d^{\prime}=\max(d_{prev},d_{prev+1},\dots,d_{now})" display="inline"><semantics id="S4.p3.5.m5.5a"><mrow id="S4.p3.5.m5.5.5" xref="S4.p3.5.m5.5.5.cmml"><msup id="S4.p3.5.m5.5.5.5" xref="S4.p3.5.m5.5.5.5.cmml"><mi id="S4.p3.5.m5.5.5.5.2" xref="S4.p3.5.m5.5.5.5.2.cmml">d</mi><mo id="S4.p3.5.m5.5.5.5.3" xref="S4.p3.5.m5.5.5.5.3.cmml">′</mo></msup><mo id="S4.p3.5.m5.5.5.4" xref="S4.p3.5.m5.5.5.4.cmml">=</mo><mrow id="S4.p3.5.m5.5.5.3.3" xref="S4.p3.5.m5.5.5.3.4.cmml"><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">max</mi><mo id="S4.p3.5.m5.5.5.3.3a" xref="S4.p3.5.m5.5.5.3.4.cmml">⁡</mo><mrow id="S4.p3.5.m5.5.5.3.3.3" xref="S4.p3.5.m5.5.5.3.4.cmml"><mo stretchy="false" id="S4.p3.5.m5.5.5.3.3.3.4" xref="S4.p3.5.m5.5.5.3.4.cmml">(</mo><msub id="S4.p3.5.m5.3.3.1.1.1.1" xref="S4.p3.5.m5.3.3.1.1.1.1.cmml"><mi id="S4.p3.5.m5.3.3.1.1.1.1.2" xref="S4.p3.5.m5.3.3.1.1.1.1.2.cmml">d</mi><mrow id="S4.p3.5.m5.3.3.1.1.1.1.3" xref="S4.p3.5.m5.3.3.1.1.1.1.3.cmml"><mi id="S4.p3.5.m5.3.3.1.1.1.1.3.2" xref="S4.p3.5.m5.3.3.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.3.3.1.1.1.1.3.1" xref="S4.p3.5.m5.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p3.5.m5.3.3.1.1.1.1.3.3" xref="S4.p3.5.m5.3.3.1.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.3.3.1.1.1.1.3.1a" xref="S4.p3.5.m5.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p3.5.m5.3.3.1.1.1.1.3.4" xref="S4.p3.5.m5.3.3.1.1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.3.3.1.1.1.1.3.1b" xref="S4.p3.5.m5.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p3.5.m5.3.3.1.1.1.1.3.5" xref="S4.p3.5.m5.3.3.1.1.1.1.3.5.cmml">v</mi></mrow></msub><mo id="S4.p3.5.m5.5.5.3.3.3.5" xref="S4.p3.5.m5.5.5.3.4.cmml">,</mo><msub id="S4.p3.5.m5.4.4.2.2.2.2" xref="S4.p3.5.m5.4.4.2.2.2.2.cmml"><mi id="S4.p3.5.m5.4.4.2.2.2.2.2" xref="S4.p3.5.m5.4.4.2.2.2.2.2.cmml">d</mi><mrow id="S4.p3.5.m5.4.4.2.2.2.2.3" xref="S4.p3.5.m5.4.4.2.2.2.2.3.cmml"><mrow id="S4.p3.5.m5.4.4.2.2.2.2.3.2" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.cmml"><mi id="S4.p3.5.m5.4.4.2.2.2.2.3.2.2" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.4.4.2.2.2.2.3.2.1" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.4.4.2.2.2.2.3.2.3" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.4.4.2.2.2.2.3.2.1a" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.4.4.2.2.2.2.3.2.4" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.4.4.2.2.2.2.3.2.1b" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.1.cmml">​</mo><mi id="S4.p3.5.m5.4.4.2.2.2.2.3.2.5" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.5.cmml">v</mi></mrow><mo id="S4.p3.5.m5.4.4.2.2.2.2.3.1" xref="S4.p3.5.m5.4.4.2.2.2.2.3.1.cmml">+</mo><mn id="S4.p3.5.m5.4.4.2.2.2.2.3.3" xref="S4.p3.5.m5.4.4.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S4.p3.5.m5.5.5.3.3.3.6" xref="S4.p3.5.m5.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.p3.5.m5.2.2" xref="S4.p3.5.m5.2.2.cmml">…</mi><mo id="S4.p3.5.m5.5.5.3.3.3.7" xref="S4.p3.5.m5.5.5.3.4.cmml">,</mo><msub id="S4.p3.5.m5.5.5.3.3.3.3" xref="S4.p3.5.m5.5.5.3.3.3.3.cmml"><mi id="S4.p3.5.m5.5.5.3.3.3.3.2" xref="S4.p3.5.m5.5.5.3.3.3.3.2.cmml">d</mi><mrow id="S4.p3.5.m5.5.5.3.3.3.3.3" xref="S4.p3.5.m5.5.5.3.3.3.3.3.cmml"><mi id="S4.p3.5.m5.5.5.3.3.3.3.3.2" xref="S4.p3.5.m5.5.5.3.3.3.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.5.5.3.3.3.3.3.1" xref="S4.p3.5.m5.5.5.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p3.5.m5.5.5.3.3.3.3.3.3" xref="S4.p3.5.m5.5.5.3.3.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p3.5.m5.5.5.3.3.3.3.3.1a" xref="S4.p3.5.m5.5.5.3.3.3.3.3.1.cmml">​</mo><mi id="S4.p3.5.m5.5.5.3.3.3.3.3.4" xref="S4.p3.5.m5.5.5.3.3.3.3.3.4.cmml">w</mi></mrow></msub><mo stretchy="false" id="S4.p3.5.m5.5.5.3.3.3.8" xref="S4.p3.5.m5.5.5.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.5b"><apply id="S4.p3.5.m5.5.5.cmml" xref="S4.p3.5.m5.5.5"><eq id="S4.p3.5.m5.5.5.4.cmml" xref="S4.p3.5.m5.5.5.4"></eq><apply id="S4.p3.5.m5.5.5.5.cmml" xref="S4.p3.5.m5.5.5.5"><csymbol cd="ambiguous" id="S4.p3.5.m5.5.5.5.1.cmml" xref="S4.p3.5.m5.5.5.5">superscript</csymbol><ci id="S4.p3.5.m5.5.5.5.2.cmml" xref="S4.p3.5.m5.5.5.5.2">𝑑</ci><ci id="S4.p3.5.m5.5.5.5.3.cmml" xref="S4.p3.5.m5.5.5.5.3">′</ci></apply><apply id="S4.p3.5.m5.5.5.3.4.cmml" xref="S4.p3.5.m5.5.5.3.3"><max id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1"></max><apply id="S4.p3.5.m5.3.3.1.1.1.1.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.p3.5.m5.3.3.1.1.1.1.1.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1">subscript</csymbol><ci id="S4.p3.5.m5.3.3.1.1.1.1.2.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.2">𝑑</ci><apply id="S4.p3.5.m5.3.3.1.1.1.1.3.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3"><times id="S4.p3.5.m5.3.3.1.1.1.1.3.1.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3.1"></times><ci id="S4.p3.5.m5.3.3.1.1.1.1.3.2.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3.2">𝑝</ci><ci id="S4.p3.5.m5.3.3.1.1.1.1.3.3.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3.3">𝑟</ci><ci id="S4.p3.5.m5.3.3.1.1.1.1.3.4.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3.4">𝑒</ci><ci id="S4.p3.5.m5.3.3.1.1.1.1.3.5.cmml" xref="S4.p3.5.m5.3.3.1.1.1.1.3.5">𝑣</ci></apply></apply><apply id="S4.p3.5.m5.4.4.2.2.2.2.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S4.p3.5.m5.4.4.2.2.2.2.1.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2">subscript</csymbol><ci id="S4.p3.5.m5.4.4.2.2.2.2.2.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.2">𝑑</ci><apply id="S4.p3.5.m5.4.4.2.2.2.2.3.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3"><plus id="S4.p3.5.m5.4.4.2.2.2.2.3.1.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.1"></plus><apply id="S4.p3.5.m5.4.4.2.2.2.2.3.2.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2"><times id="S4.p3.5.m5.4.4.2.2.2.2.3.2.1.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.1"></times><ci id="S4.p3.5.m5.4.4.2.2.2.2.3.2.2.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.2">𝑝</ci><ci id="S4.p3.5.m5.4.4.2.2.2.2.3.2.3.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.3">𝑟</ci><ci id="S4.p3.5.m5.4.4.2.2.2.2.3.2.4.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.4">𝑒</ci><ci id="S4.p3.5.m5.4.4.2.2.2.2.3.2.5.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.2.5">𝑣</ci></apply><cn type="integer" id="S4.p3.5.m5.4.4.2.2.2.2.3.3.cmml" xref="S4.p3.5.m5.4.4.2.2.2.2.3.3">1</cn></apply></apply><ci id="S4.p3.5.m5.2.2.cmml" xref="S4.p3.5.m5.2.2">…</ci><apply id="S4.p3.5.m5.5.5.3.3.3.3.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S4.p3.5.m5.5.5.3.3.3.3.1.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3">subscript</csymbol><ci id="S4.p3.5.m5.5.5.3.3.3.3.2.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.2">𝑑</ci><apply id="S4.p3.5.m5.5.5.3.3.3.3.3.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.3"><times id="S4.p3.5.m5.5.5.3.3.3.3.3.1.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.3.1"></times><ci id="S4.p3.5.m5.5.5.3.3.3.3.3.2.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.3.2">𝑛</ci><ci id="S4.p3.5.m5.5.5.3.3.3.3.3.3.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.3.3">𝑜</ci><ci id="S4.p3.5.m5.5.5.3.3.3.3.3.4.cmml" xref="S4.p3.5.m5.5.5.3.3.3.3.3.4">𝑤</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.5c">d^{\prime}=\max(d_{prev},d_{prev+1},\dots,d_{now})</annotation></semantics></math>.
If <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="d^{\prime}&gt;d_{prev}" display="inline"><semantics id="S4.p3.6.m6.1a"><mrow id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml"><msup id="S4.p3.6.m6.1.1.2" xref="S4.p3.6.m6.1.1.2.cmml"><mi id="S4.p3.6.m6.1.1.2.2" xref="S4.p3.6.m6.1.1.2.2.cmml">d</mi><mo id="S4.p3.6.m6.1.1.2.3" xref="S4.p3.6.m6.1.1.2.3.cmml">′</mo></msup><mo id="S4.p3.6.m6.1.1.1" xref="S4.p3.6.m6.1.1.1.cmml">&gt;</mo><msub id="S4.p3.6.m6.1.1.3" xref="S4.p3.6.m6.1.1.3.cmml"><mi id="S4.p3.6.m6.1.1.3.2" xref="S4.p3.6.m6.1.1.3.2.cmml">d</mi><mrow id="S4.p3.6.m6.1.1.3.3" xref="S4.p3.6.m6.1.1.3.3.cmml"><mi id="S4.p3.6.m6.1.1.3.3.2" xref="S4.p3.6.m6.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.1.1.3.3.1" xref="S4.p3.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.6.m6.1.1.3.3.3" xref="S4.p3.6.m6.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.1.1.3.3.1a" xref="S4.p3.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.6.m6.1.1.3.3.4" xref="S4.p3.6.m6.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.6.m6.1.1.3.3.1b" xref="S4.p3.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.6.m6.1.1.3.3.5" xref="S4.p3.6.m6.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><apply id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1"><gt id="S4.p3.6.m6.1.1.1.cmml" xref="S4.p3.6.m6.1.1.1"></gt><apply id="S4.p3.6.m6.1.1.2.cmml" xref="S4.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.p3.6.m6.1.1.2.1.cmml" xref="S4.p3.6.m6.1.1.2">superscript</csymbol><ci id="S4.p3.6.m6.1.1.2.2.cmml" xref="S4.p3.6.m6.1.1.2.2">𝑑</ci><ci id="S4.p3.6.m6.1.1.2.3.cmml" xref="S4.p3.6.m6.1.1.2.3">′</ci></apply><apply id="S4.p3.6.m6.1.1.3.cmml" xref="S4.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.p3.6.m6.1.1.3.1.cmml" xref="S4.p3.6.m6.1.1.3">subscript</csymbol><ci id="S4.p3.6.m6.1.1.3.2.cmml" xref="S4.p3.6.m6.1.1.3.2">𝑑</ci><apply id="S4.p3.6.m6.1.1.3.3.cmml" xref="S4.p3.6.m6.1.1.3.3"><times id="S4.p3.6.m6.1.1.3.3.1.cmml" xref="S4.p3.6.m6.1.1.3.3.1"></times><ci id="S4.p3.6.m6.1.1.3.3.2.cmml" xref="S4.p3.6.m6.1.1.3.3.2">𝑝</ci><ci id="S4.p3.6.m6.1.1.3.3.3.cmml" xref="S4.p3.6.m6.1.1.3.3.3">𝑟</ci><ci id="S4.p3.6.m6.1.1.3.3.4.cmml" xref="S4.p3.6.m6.1.1.3.3.4">𝑒</ci><ci id="S4.p3.6.m6.1.1.3.3.5.cmml" xref="S4.p3.6.m6.1.1.3.3.5">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">d^{\prime}&gt;d_{prev}</annotation></semantics></math>, it means the model has gone deeper.
The transformer layers between <math id="S4.p3.7.m7.1" class="ltx_Math" alttext="(D-d^{\prime})_{th}" display="inline"><semantics id="S4.p3.7.m7.1a"><msub id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml"><mrow id="S4.p3.7.m7.1.1.1.1" xref="S4.p3.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.7.m7.1.1.1.1.2" xref="S4.p3.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.7.m7.1.1.1.1.1" xref="S4.p3.7.m7.1.1.1.1.1.cmml"><mi id="S4.p3.7.m7.1.1.1.1.1.2" xref="S4.p3.7.m7.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.7.m7.1.1.1.1.1.1" xref="S4.p3.7.m7.1.1.1.1.1.1.cmml">−</mo><msup id="S4.p3.7.m7.1.1.1.1.1.3" xref="S4.p3.7.m7.1.1.1.1.1.3.cmml"><mi id="S4.p3.7.m7.1.1.1.1.1.3.2" xref="S4.p3.7.m7.1.1.1.1.1.3.2.cmml">d</mi><mo id="S4.p3.7.m7.1.1.1.1.1.3.3" xref="S4.p3.7.m7.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S4.p3.7.m7.1.1.1.1.3" xref="S4.p3.7.m7.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.7.m7.1.1.3" xref="S4.p3.7.m7.1.1.3.cmml"><mi id="S4.p3.7.m7.1.1.3.2" xref="S4.p3.7.m7.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.7.m7.1.1.3.1" xref="S4.p3.7.m7.1.1.3.1.cmml">​</mo><mi id="S4.p3.7.m7.1.1.3.3" xref="S4.p3.7.m7.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><apply id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p3.7.m7.1.1.2.cmml" xref="S4.p3.7.m7.1.1">subscript</csymbol><apply id="S4.p3.7.m7.1.1.1.1.1.cmml" xref="S4.p3.7.m7.1.1.1.1"><minus id="S4.p3.7.m7.1.1.1.1.1.1.cmml" xref="S4.p3.7.m7.1.1.1.1.1.1"></minus><ci id="S4.p3.7.m7.1.1.1.1.1.2.cmml" xref="S4.p3.7.m7.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.7.m7.1.1.1.1.1.3.cmml" xref="S4.p3.7.m7.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.7.m7.1.1.1.1.1.3.1.cmml" xref="S4.p3.7.m7.1.1.1.1.1.3">superscript</csymbol><ci id="S4.p3.7.m7.1.1.1.1.1.3.2.cmml" xref="S4.p3.7.m7.1.1.1.1.1.3.2">𝑑</ci><ci id="S4.p3.7.m7.1.1.1.1.1.3.3.cmml" xref="S4.p3.7.m7.1.1.1.1.1.3.3">′</ci></apply></apply><apply id="S4.p3.7.m7.1.1.3.cmml" xref="S4.p3.7.m7.1.1.3"><times id="S4.p3.7.m7.1.1.3.1.cmml" xref="S4.p3.7.m7.1.1.3.1"></times><ci id="S4.p3.7.m7.1.1.3.2.cmml" xref="S4.p3.7.m7.1.1.3.2">𝑡</ci><ci id="S4.p3.7.m7.1.1.3.3.cmml" xref="S4.p3.7.m7.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">(D-d^{\prime})_{th}</annotation></semantics></math> and <math id="S4.p3.8.m8.1" class="ltx_Math" alttext="(D-d_{prev})_{th}" display="inline"><semantics id="S4.p3.8.m8.1a"><msub id="S4.p3.8.m8.1.1" xref="S4.p3.8.m8.1.1.cmml"><mrow id="S4.p3.8.m8.1.1.1.1" xref="S4.p3.8.m8.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.8.m8.1.1.1.1.2" xref="S4.p3.8.m8.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.8.m8.1.1.1.1.1" xref="S4.p3.8.m8.1.1.1.1.1.cmml"><mi id="S4.p3.8.m8.1.1.1.1.1.2" xref="S4.p3.8.m8.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.8.m8.1.1.1.1.1.1" xref="S4.p3.8.m8.1.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.8.m8.1.1.1.1.1.3" xref="S4.p3.8.m8.1.1.1.1.1.3.cmml"><mi id="S4.p3.8.m8.1.1.1.1.1.3.2" xref="S4.p3.8.m8.1.1.1.1.1.3.2.cmml">d</mi><mrow id="S4.p3.8.m8.1.1.1.1.1.3.3" xref="S4.p3.8.m8.1.1.1.1.1.3.3.cmml"><mi id="S4.p3.8.m8.1.1.1.1.1.3.3.2" xref="S4.p3.8.m8.1.1.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.1.1.1.1.1.3.3.1" xref="S4.p3.8.m8.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.8.m8.1.1.1.1.1.3.3.3" xref="S4.p3.8.m8.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.1.1.1.1.1.3.3.1a" xref="S4.p3.8.m8.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.8.m8.1.1.1.1.1.3.3.4" xref="S4.p3.8.m8.1.1.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.1.1.1.1.1.3.3.1b" xref="S4.p3.8.m8.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.8.m8.1.1.1.1.1.3.3.5" xref="S4.p3.8.m8.1.1.1.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><mo stretchy="false" id="S4.p3.8.m8.1.1.1.1.3" xref="S4.p3.8.m8.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.8.m8.1.1.3" xref="S4.p3.8.m8.1.1.3.cmml"><mi id="S4.p3.8.m8.1.1.3.2" xref="S4.p3.8.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.8.m8.1.1.3.1" xref="S4.p3.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.p3.8.m8.1.1.3.3" xref="S4.p3.8.m8.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.8.m8.1b"><apply id="S4.p3.8.m8.1.1.cmml" xref="S4.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p3.8.m8.1.1.2.cmml" xref="S4.p3.8.m8.1.1">subscript</csymbol><apply id="S4.p3.8.m8.1.1.1.1.1.cmml" xref="S4.p3.8.m8.1.1.1.1"><minus id="S4.p3.8.m8.1.1.1.1.1.1.cmml" xref="S4.p3.8.m8.1.1.1.1.1.1"></minus><ci id="S4.p3.8.m8.1.1.1.1.1.2.cmml" xref="S4.p3.8.m8.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.8.m8.1.1.1.1.1.3.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.8.m8.1.1.1.1.1.3.1.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.8.m8.1.1.1.1.1.3.2.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.2">𝑑</ci><apply id="S4.p3.8.m8.1.1.1.1.1.3.3.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3"><times id="S4.p3.8.m8.1.1.1.1.1.3.3.1.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3.1"></times><ci id="S4.p3.8.m8.1.1.1.1.1.3.3.2.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S4.p3.8.m8.1.1.1.1.1.3.3.3.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3.3">𝑟</ci><ci id="S4.p3.8.m8.1.1.1.1.1.3.3.4.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3.4">𝑒</ci><ci id="S4.p3.8.m8.1.1.1.1.1.3.3.5.cmml" xref="S4.p3.8.m8.1.1.1.1.1.3.3.5">𝑣</ci></apply></apply></apply><apply id="S4.p3.8.m8.1.1.3.cmml" xref="S4.p3.8.m8.1.1.3"><times id="S4.p3.8.m8.1.1.3.1.cmml" xref="S4.p3.8.m8.1.1.3.1"></times><ci id="S4.p3.8.m8.1.1.3.2.cmml" xref="S4.p3.8.m8.1.1.3.2">𝑡</ci><ci id="S4.p3.8.m8.1.1.3.3.cmml" xref="S4.p3.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.8.m8.1c">(D-d_{prev})_{th}</annotation></semantics></math> have been updated and the cached activations of <math id="S4.p3.9.m9.1" class="ltx_Math" alttext="(D-d_{prev})_{th}" display="inline"><semantics id="S4.p3.9.m9.1a"><msub id="S4.p3.9.m9.1.1" xref="S4.p3.9.m9.1.1.cmml"><mrow id="S4.p3.9.m9.1.1.1.1" xref="S4.p3.9.m9.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.9.m9.1.1.1.1.2" xref="S4.p3.9.m9.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.9.m9.1.1.1.1.1" xref="S4.p3.9.m9.1.1.1.1.1.cmml"><mi id="S4.p3.9.m9.1.1.1.1.1.2" xref="S4.p3.9.m9.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.9.m9.1.1.1.1.1.1" xref="S4.p3.9.m9.1.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.9.m9.1.1.1.1.1.3" xref="S4.p3.9.m9.1.1.1.1.1.3.cmml"><mi id="S4.p3.9.m9.1.1.1.1.1.3.2" xref="S4.p3.9.m9.1.1.1.1.1.3.2.cmml">d</mi><mrow id="S4.p3.9.m9.1.1.1.1.1.3.3" xref="S4.p3.9.m9.1.1.1.1.1.3.3.cmml"><mi id="S4.p3.9.m9.1.1.1.1.1.3.3.2" xref="S4.p3.9.m9.1.1.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.9.m9.1.1.1.1.1.3.3.1" xref="S4.p3.9.m9.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.9.m9.1.1.1.1.1.3.3.3" xref="S4.p3.9.m9.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.9.m9.1.1.1.1.1.3.3.1a" xref="S4.p3.9.m9.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.9.m9.1.1.1.1.1.3.3.4" xref="S4.p3.9.m9.1.1.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.9.m9.1.1.1.1.1.3.3.1b" xref="S4.p3.9.m9.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.9.m9.1.1.1.1.1.3.3.5" xref="S4.p3.9.m9.1.1.1.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><mo stretchy="false" id="S4.p3.9.m9.1.1.1.1.3" xref="S4.p3.9.m9.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.9.m9.1.1.3" xref="S4.p3.9.m9.1.1.3.cmml"><mi id="S4.p3.9.m9.1.1.3.2" xref="S4.p3.9.m9.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.9.m9.1.1.3.1" xref="S4.p3.9.m9.1.1.3.1.cmml">​</mo><mi id="S4.p3.9.m9.1.1.3.3" xref="S4.p3.9.m9.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.9.m9.1b"><apply id="S4.p3.9.m9.1.1.cmml" xref="S4.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p3.9.m9.1.1.2.cmml" xref="S4.p3.9.m9.1.1">subscript</csymbol><apply id="S4.p3.9.m9.1.1.1.1.1.cmml" xref="S4.p3.9.m9.1.1.1.1"><minus id="S4.p3.9.m9.1.1.1.1.1.1.cmml" xref="S4.p3.9.m9.1.1.1.1.1.1"></minus><ci id="S4.p3.9.m9.1.1.1.1.1.2.cmml" xref="S4.p3.9.m9.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.9.m9.1.1.1.1.1.3.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.9.m9.1.1.1.1.1.3.1.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.9.m9.1.1.1.1.1.3.2.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.2">𝑑</ci><apply id="S4.p3.9.m9.1.1.1.1.1.3.3.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3"><times id="S4.p3.9.m9.1.1.1.1.1.3.3.1.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3.1"></times><ci id="S4.p3.9.m9.1.1.1.1.1.3.3.2.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S4.p3.9.m9.1.1.1.1.1.3.3.3.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3.3">𝑟</ci><ci id="S4.p3.9.m9.1.1.1.1.1.3.3.4.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3.4">𝑒</ci><ci id="S4.p3.9.m9.1.1.1.1.1.3.3.5.cmml" xref="S4.p3.9.m9.1.1.1.1.1.3.3.5">𝑣</ci></apply></apply></apply><apply id="S4.p3.9.m9.1.1.3.cmml" xref="S4.p3.9.m9.1.1.3"><times id="S4.p3.9.m9.1.1.3.1.cmml" xref="S4.p3.9.m9.1.1.3.1"></times><ci id="S4.p3.9.m9.1.1.3.2.cmml" xref="S4.p3.9.m9.1.1.3.2">𝑡</ci><ci id="S4.p3.9.m9.1.1.3.3.cmml" xref="S4.p3.9.m9.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.9.m9.1c">(D-d_{prev})_{th}</annotation></semantics></math> transformer block have “expired”.
The output of the <math id="S4.p3.10.m10.1" class="ltx_Math" alttext="(D-d^{\prime})_{th}" display="inline"><semantics id="S4.p3.10.m10.1a"><msub id="S4.p3.10.m10.1.1" xref="S4.p3.10.m10.1.1.cmml"><mrow id="S4.p3.10.m10.1.1.1.1" xref="S4.p3.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.10.m10.1.1.1.1.2" xref="S4.p3.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.10.m10.1.1.1.1.1" xref="S4.p3.10.m10.1.1.1.1.1.cmml"><mi id="S4.p3.10.m10.1.1.1.1.1.2" xref="S4.p3.10.m10.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.10.m10.1.1.1.1.1.1" xref="S4.p3.10.m10.1.1.1.1.1.1.cmml">−</mo><msup id="S4.p3.10.m10.1.1.1.1.1.3" xref="S4.p3.10.m10.1.1.1.1.1.3.cmml"><mi id="S4.p3.10.m10.1.1.1.1.1.3.2" xref="S4.p3.10.m10.1.1.1.1.1.3.2.cmml">d</mi><mo id="S4.p3.10.m10.1.1.1.1.1.3.3" xref="S4.p3.10.m10.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S4.p3.10.m10.1.1.1.1.3" xref="S4.p3.10.m10.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.10.m10.1.1.3" xref="S4.p3.10.m10.1.1.3.cmml"><mi id="S4.p3.10.m10.1.1.3.2" xref="S4.p3.10.m10.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.10.m10.1.1.3.1" xref="S4.p3.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.p3.10.m10.1.1.3.3" xref="S4.p3.10.m10.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.10.m10.1b"><apply id="S4.p3.10.m10.1.1.cmml" xref="S4.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S4.p3.10.m10.1.1.2.cmml" xref="S4.p3.10.m10.1.1">subscript</csymbol><apply id="S4.p3.10.m10.1.1.1.1.1.cmml" xref="S4.p3.10.m10.1.1.1.1"><minus id="S4.p3.10.m10.1.1.1.1.1.1.cmml" xref="S4.p3.10.m10.1.1.1.1.1.1"></minus><ci id="S4.p3.10.m10.1.1.1.1.1.2.cmml" xref="S4.p3.10.m10.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.10.m10.1.1.1.1.1.3.cmml" xref="S4.p3.10.m10.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.10.m10.1.1.1.1.1.3.1.cmml" xref="S4.p3.10.m10.1.1.1.1.1.3">superscript</csymbol><ci id="S4.p3.10.m10.1.1.1.1.1.3.2.cmml" xref="S4.p3.10.m10.1.1.1.1.1.3.2">𝑑</ci><ci id="S4.p3.10.m10.1.1.1.1.1.3.3.cmml" xref="S4.p3.10.m10.1.1.1.1.1.3.3">′</ci></apply></apply><apply id="S4.p3.10.m10.1.1.3.cmml" xref="S4.p3.10.m10.1.1.3"><times id="S4.p3.10.m10.1.1.3.1.cmml" xref="S4.p3.10.m10.1.1.3.1"></times><ci id="S4.p3.10.m10.1.1.3.2.cmml" xref="S4.p3.10.m10.1.1.3.2">𝑡</ci><ci id="S4.p3.10.m10.1.1.3.3.cmml" xref="S4.p3.10.m10.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.10.m10.1c">(D-d^{\prime})_{th}</annotation></semantics></math> transformer layer needs to be recomputed and recached.
Otherwise, if <math id="S4.p3.11.m11.1" class="ltx_Math" alttext="d^{\prime}\leq d_{prev}" display="inline"><semantics id="S4.p3.11.m11.1a"><mrow id="S4.p3.11.m11.1.1" xref="S4.p3.11.m11.1.1.cmml"><msup id="S4.p3.11.m11.1.1.2" xref="S4.p3.11.m11.1.1.2.cmml"><mi id="S4.p3.11.m11.1.1.2.2" xref="S4.p3.11.m11.1.1.2.2.cmml">d</mi><mo id="S4.p3.11.m11.1.1.2.3" xref="S4.p3.11.m11.1.1.2.3.cmml">′</mo></msup><mo id="S4.p3.11.m11.1.1.1" xref="S4.p3.11.m11.1.1.1.cmml">≤</mo><msub id="S4.p3.11.m11.1.1.3" xref="S4.p3.11.m11.1.1.3.cmml"><mi id="S4.p3.11.m11.1.1.3.2" xref="S4.p3.11.m11.1.1.3.2.cmml">d</mi><mrow id="S4.p3.11.m11.1.1.3.3" xref="S4.p3.11.m11.1.1.3.3.cmml"><mi id="S4.p3.11.m11.1.1.3.3.2" xref="S4.p3.11.m11.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.11.m11.1.1.3.3.1" xref="S4.p3.11.m11.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.11.m11.1.1.3.3.3" xref="S4.p3.11.m11.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.11.m11.1.1.3.3.1a" xref="S4.p3.11.m11.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.11.m11.1.1.3.3.4" xref="S4.p3.11.m11.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.11.m11.1.1.3.3.1b" xref="S4.p3.11.m11.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.11.m11.1.1.3.3.5" xref="S4.p3.11.m11.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.11.m11.1b"><apply id="S4.p3.11.m11.1.1.cmml" xref="S4.p3.11.m11.1.1"><leq id="S4.p3.11.m11.1.1.1.cmml" xref="S4.p3.11.m11.1.1.1"></leq><apply id="S4.p3.11.m11.1.1.2.cmml" xref="S4.p3.11.m11.1.1.2"><csymbol cd="ambiguous" id="S4.p3.11.m11.1.1.2.1.cmml" xref="S4.p3.11.m11.1.1.2">superscript</csymbol><ci id="S4.p3.11.m11.1.1.2.2.cmml" xref="S4.p3.11.m11.1.1.2.2">𝑑</ci><ci id="S4.p3.11.m11.1.1.2.3.cmml" xref="S4.p3.11.m11.1.1.2.3">′</ci></apply><apply id="S4.p3.11.m11.1.1.3.cmml" xref="S4.p3.11.m11.1.1.3"><csymbol cd="ambiguous" id="S4.p3.11.m11.1.1.3.1.cmml" xref="S4.p3.11.m11.1.1.3">subscript</csymbol><ci id="S4.p3.11.m11.1.1.3.2.cmml" xref="S4.p3.11.m11.1.1.3.2">𝑑</ci><apply id="S4.p3.11.m11.1.1.3.3.cmml" xref="S4.p3.11.m11.1.1.3.3"><times id="S4.p3.11.m11.1.1.3.3.1.cmml" xref="S4.p3.11.m11.1.1.3.3.1"></times><ci id="S4.p3.11.m11.1.1.3.3.2.cmml" xref="S4.p3.11.m11.1.1.3.3.2">𝑝</ci><ci id="S4.p3.11.m11.1.1.3.3.3.cmml" xref="S4.p3.11.m11.1.1.3.3.3">𝑟</ci><ci id="S4.p3.11.m11.1.1.3.3.4.cmml" xref="S4.p3.11.m11.1.1.3.3.4">𝑒</ci><ci id="S4.p3.11.m11.1.1.3.3.5.cmml" xref="S4.p3.11.m11.1.1.3.3.5">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.11.m11.1c">d^{\prime}\leq d_{prev}</annotation></semantics></math>, the first <math id="S4.p3.12.m12.1" class="ltx_Math" alttext="d_{split}=D-d_{prev}" display="inline"><semantics id="S4.p3.12.m12.1a"><mrow id="S4.p3.12.m12.1.1" xref="S4.p3.12.m12.1.1.cmml"><msub id="S4.p3.12.m12.1.1.2" xref="S4.p3.12.m12.1.1.2.cmml"><mi id="S4.p3.12.m12.1.1.2.2" xref="S4.p3.12.m12.1.1.2.2.cmml">d</mi><mrow id="S4.p3.12.m12.1.1.2.3" xref="S4.p3.12.m12.1.1.2.3.cmml"><mi id="S4.p3.12.m12.1.1.2.3.2" xref="S4.p3.12.m12.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.2.3.1" xref="S4.p3.12.m12.1.1.2.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.2.3.3" xref="S4.p3.12.m12.1.1.2.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.2.3.1a" xref="S4.p3.12.m12.1.1.2.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.2.3.4" xref="S4.p3.12.m12.1.1.2.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.2.3.1b" xref="S4.p3.12.m12.1.1.2.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.2.3.5" xref="S4.p3.12.m12.1.1.2.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.2.3.1c" xref="S4.p3.12.m12.1.1.2.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.2.3.6" xref="S4.p3.12.m12.1.1.2.3.6.cmml">t</mi></mrow></msub><mo id="S4.p3.12.m12.1.1.1" xref="S4.p3.12.m12.1.1.1.cmml">=</mo><mrow id="S4.p3.12.m12.1.1.3" xref="S4.p3.12.m12.1.1.3.cmml"><mi id="S4.p3.12.m12.1.1.3.2" xref="S4.p3.12.m12.1.1.3.2.cmml">D</mi><mo id="S4.p3.12.m12.1.1.3.1" xref="S4.p3.12.m12.1.1.3.1.cmml">−</mo><msub id="S4.p3.12.m12.1.1.3.3" xref="S4.p3.12.m12.1.1.3.3.cmml"><mi id="S4.p3.12.m12.1.1.3.3.2" xref="S4.p3.12.m12.1.1.3.3.2.cmml">d</mi><mrow id="S4.p3.12.m12.1.1.3.3.3" xref="S4.p3.12.m12.1.1.3.3.3.cmml"><mi id="S4.p3.12.m12.1.1.3.3.3.2" xref="S4.p3.12.m12.1.1.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.3.3.3.1" xref="S4.p3.12.m12.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.3.3.3.3" xref="S4.p3.12.m12.1.1.3.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.3.3.3.1a" xref="S4.p3.12.m12.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.3.3.3.4" xref="S4.p3.12.m12.1.1.3.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.12.m12.1.1.3.3.3.1b" xref="S4.p3.12.m12.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p3.12.m12.1.1.3.3.3.5" xref="S4.p3.12.m12.1.1.3.3.3.5.cmml">v</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.12.m12.1b"><apply id="S4.p3.12.m12.1.1.cmml" xref="S4.p3.12.m12.1.1"><eq id="S4.p3.12.m12.1.1.1.cmml" xref="S4.p3.12.m12.1.1.1"></eq><apply id="S4.p3.12.m12.1.1.2.cmml" xref="S4.p3.12.m12.1.1.2"><csymbol cd="ambiguous" id="S4.p3.12.m12.1.1.2.1.cmml" xref="S4.p3.12.m12.1.1.2">subscript</csymbol><ci id="S4.p3.12.m12.1.1.2.2.cmml" xref="S4.p3.12.m12.1.1.2.2">𝑑</ci><apply id="S4.p3.12.m12.1.1.2.3.cmml" xref="S4.p3.12.m12.1.1.2.3"><times id="S4.p3.12.m12.1.1.2.3.1.cmml" xref="S4.p3.12.m12.1.1.2.3.1"></times><ci id="S4.p3.12.m12.1.1.2.3.2.cmml" xref="S4.p3.12.m12.1.1.2.3.2">𝑠</ci><ci id="S4.p3.12.m12.1.1.2.3.3.cmml" xref="S4.p3.12.m12.1.1.2.3.3">𝑝</ci><ci id="S4.p3.12.m12.1.1.2.3.4.cmml" xref="S4.p3.12.m12.1.1.2.3.4">𝑙</ci><ci id="S4.p3.12.m12.1.1.2.3.5.cmml" xref="S4.p3.12.m12.1.1.2.3.5">𝑖</ci><ci id="S4.p3.12.m12.1.1.2.3.6.cmml" xref="S4.p3.12.m12.1.1.2.3.6">𝑡</ci></apply></apply><apply id="S4.p3.12.m12.1.1.3.cmml" xref="S4.p3.12.m12.1.1.3"><minus id="S4.p3.12.m12.1.1.3.1.cmml" xref="S4.p3.12.m12.1.1.3.1"></minus><ci id="S4.p3.12.m12.1.1.3.2.cmml" xref="S4.p3.12.m12.1.1.3.2">𝐷</ci><apply id="S4.p3.12.m12.1.1.3.3.cmml" xref="S4.p3.12.m12.1.1.3.3"><csymbol cd="ambiguous" id="S4.p3.12.m12.1.1.3.3.1.cmml" xref="S4.p3.12.m12.1.1.3.3">subscript</csymbol><ci id="S4.p3.12.m12.1.1.3.3.2.cmml" xref="S4.p3.12.m12.1.1.3.3.2">𝑑</ci><apply id="S4.p3.12.m12.1.1.3.3.3.cmml" xref="S4.p3.12.m12.1.1.3.3.3"><times id="S4.p3.12.m12.1.1.3.3.3.1.cmml" xref="S4.p3.12.m12.1.1.3.3.3.1"></times><ci id="S4.p3.12.m12.1.1.3.3.3.2.cmml" xref="S4.p3.12.m12.1.1.3.3.3.2">𝑝</ci><ci id="S4.p3.12.m12.1.1.3.3.3.3.cmml" xref="S4.p3.12.m12.1.1.3.3.3.3">𝑟</ci><ci id="S4.p3.12.m12.1.1.3.3.3.4.cmml" xref="S4.p3.12.m12.1.1.3.3.3.4">𝑒</ci><ci id="S4.p3.12.m12.1.1.3.3.3.5.cmml" xref="S4.p3.12.m12.1.1.3.3.3.5">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.12.m12.1c">d_{split}=D-d_{prev}</annotation></semantics></math> transformer layers are not touched since round <math id="S4.p3.13.m13.1" class="ltx_Math" alttext="r_{prev}" display="inline"><semantics id="S4.p3.13.m13.1a"><msub id="S4.p3.13.m13.1.1" xref="S4.p3.13.m13.1.1.cmml"><mi id="S4.p3.13.m13.1.1.2" xref="S4.p3.13.m13.1.1.2.cmml">r</mi><mrow id="S4.p3.13.m13.1.1.3" xref="S4.p3.13.m13.1.1.3.cmml"><mi id="S4.p3.13.m13.1.1.3.2" xref="S4.p3.13.m13.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.13.m13.1.1.3.1" xref="S4.p3.13.m13.1.1.3.1.cmml">​</mo><mi id="S4.p3.13.m13.1.1.3.3" xref="S4.p3.13.m13.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.13.m13.1.1.3.1a" xref="S4.p3.13.m13.1.1.3.1.cmml">​</mo><mi id="S4.p3.13.m13.1.1.3.4" xref="S4.p3.13.m13.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.13.m13.1.1.3.1b" xref="S4.p3.13.m13.1.1.3.1.cmml">​</mo><mi id="S4.p3.13.m13.1.1.3.5" xref="S4.p3.13.m13.1.1.3.5.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.13.m13.1b"><apply id="S4.p3.13.m13.1.1.cmml" xref="S4.p3.13.m13.1.1"><csymbol cd="ambiguous" id="S4.p3.13.m13.1.1.1.cmml" xref="S4.p3.13.m13.1.1">subscript</csymbol><ci id="S4.p3.13.m13.1.1.2.cmml" xref="S4.p3.13.m13.1.1.2">𝑟</ci><apply id="S4.p3.13.m13.1.1.3.cmml" xref="S4.p3.13.m13.1.1.3"><times id="S4.p3.13.m13.1.1.3.1.cmml" xref="S4.p3.13.m13.1.1.3.1"></times><ci id="S4.p3.13.m13.1.1.3.2.cmml" xref="S4.p3.13.m13.1.1.3.2">𝑝</ci><ci id="S4.p3.13.m13.1.1.3.3.cmml" xref="S4.p3.13.m13.1.1.3.3">𝑟</ci><ci id="S4.p3.13.m13.1.1.3.4.cmml" xref="S4.p3.13.m13.1.1.3.4">𝑒</ci><ci id="S4.p3.13.m13.1.1.3.5.cmml" xref="S4.p3.13.m13.1.1.3.5">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.13.m13.1c">r_{prev}</annotation></semantics></math>.
Therefore, it can directly load the output of the <math id="S4.p3.14.m14.1" class="ltx_Math" alttext="(D-d_{prev})_{th}" display="inline"><semantics id="S4.p3.14.m14.1a"><msub id="S4.p3.14.m14.1.1" xref="S4.p3.14.m14.1.1.cmml"><mrow id="S4.p3.14.m14.1.1.1.1" xref="S4.p3.14.m14.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.14.m14.1.1.1.1.2" xref="S4.p3.14.m14.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.14.m14.1.1.1.1.1" xref="S4.p3.14.m14.1.1.1.1.1.cmml"><mi id="S4.p3.14.m14.1.1.1.1.1.2" xref="S4.p3.14.m14.1.1.1.1.1.2.cmml">D</mi><mo id="S4.p3.14.m14.1.1.1.1.1.1" xref="S4.p3.14.m14.1.1.1.1.1.1.cmml">−</mo><msub id="S4.p3.14.m14.1.1.1.1.1.3" xref="S4.p3.14.m14.1.1.1.1.1.3.cmml"><mi id="S4.p3.14.m14.1.1.1.1.1.3.2" xref="S4.p3.14.m14.1.1.1.1.1.3.2.cmml">d</mi><mrow id="S4.p3.14.m14.1.1.1.1.1.3.3" xref="S4.p3.14.m14.1.1.1.1.1.3.3.cmml"><mi id="S4.p3.14.m14.1.1.1.1.1.3.3.2" xref="S4.p3.14.m14.1.1.1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p3.14.m14.1.1.1.1.1.3.3.1" xref="S4.p3.14.m14.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.14.m14.1.1.1.1.1.3.3.3" xref="S4.p3.14.m14.1.1.1.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p3.14.m14.1.1.1.1.1.3.3.1a" xref="S4.p3.14.m14.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.14.m14.1.1.1.1.1.3.3.4" xref="S4.p3.14.m14.1.1.1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p3.14.m14.1.1.1.1.1.3.3.1b" xref="S4.p3.14.m14.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.p3.14.m14.1.1.1.1.1.3.3.5" xref="S4.p3.14.m14.1.1.1.1.1.3.3.5.cmml">v</mi></mrow></msub></mrow><mo stretchy="false" id="S4.p3.14.m14.1.1.1.1.3" xref="S4.p3.14.m14.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.p3.14.m14.1.1.3" xref="S4.p3.14.m14.1.1.3.cmml"><mi id="S4.p3.14.m14.1.1.3.2" xref="S4.p3.14.m14.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p3.14.m14.1.1.3.1" xref="S4.p3.14.m14.1.1.3.1.cmml">​</mo><mi id="S4.p3.14.m14.1.1.3.3" xref="S4.p3.14.m14.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.14.m14.1b"><apply id="S4.p3.14.m14.1.1.cmml" xref="S4.p3.14.m14.1.1"><csymbol cd="ambiguous" id="S4.p3.14.m14.1.1.2.cmml" xref="S4.p3.14.m14.1.1">subscript</csymbol><apply id="S4.p3.14.m14.1.1.1.1.1.cmml" xref="S4.p3.14.m14.1.1.1.1"><minus id="S4.p3.14.m14.1.1.1.1.1.1.cmml" xref="S4.p3.14.m14.1.1.1.1.1.1"></minus><ci id="S4.p3.14.m14.1.1.1.1.1.2.cmml" xref="S4.p3.14.m14.1.1.1.1.1.2">𝐷</ci><apply id="S4.p3.14.m14.1.1.1.1.1.3.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.p3.14.m14.1.1.1.1.1.3.1.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3">subscript</csymbol><ci id="S4.p3.14.m14.1.1.1.1.1.3.2.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.2">𝑑</ci><apply id="S4.p3.14.m14.1.1.1.1.1.3.3.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3"><times id="S4.p3.14.m14.1.1.1.1.1.3.3.1.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3.1"></times><ci id="S4.p3.14.m14.1.1.1.1.1.3.3.2.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S4.p3.14.m14.1.1.1.1.1.3.3.3.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3.3">𝑟</ci><ci id="S4.p3.14.m14.1.1.1.1.1.3.3.4.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3.4">𝑒</ci><ci id="S4.p3.14.m14.1.1.1.1.1.3.3.5.cmml" xref="S4.p3.14.m14.1.1.1.1.1.3.3.5">𝑣</ci></apply></apply></apply><apply id="S4.p3.14.m14.1.1.3.cmml" xref="S4.p3.14.m14.1.1.3"><times id="S4.p3.14.m14.1.1.3.1.cmml" xref="S4.p3.14.m14.1.1.3.1"></times><ci id="S4.p3.14.m14.1.1.3.2.cmml" xref="S4.p3.14.m14.1.1.3.2">𝑡</ci><ci id="S4.p3.14.m14.1.1.3.3.cmml" xref="S4.p3.14.m14.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.14.m14.1c">(D-d_{prev})_{th}</annotation></semantics></math> layer from the cache and feed it into the training process without starting from scratch.
The above process repeats when the device participates in training every time.
The cache expiration incurs re-computations of bottom transformer blocks and compromises its profits.
Fortunately, <span id="S4.p3.15.2" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s design of online configurator (<math id="S4.p3.15.m15.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.p3.15.m15.1a"><mi mathvariant="normal" id="S4.p3.15.m15.1.1" xref="S4.p3.15.m15.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.p3.15.m15.1b"><ci id="S4.p3.15.m15.1.1.cmml" xref="S4.p3.15.m15.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.15.m15.1c">\S</annotation></semantics></math><a href="#S3.SS3" title="3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) orchestrates with the caching technique by monotonously upgrading the tuning depth.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">To be noted, while cache mechanism has been widely exploited to accelerate on-device DNN inference <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">crankshaw2017clipper, </a>; <a href="#bib.bib69" title="" class="ltx_ref">shen2019nexus, </a>; <a href="#bib.bib27" title="" class="ltx_ref">han2016mcdnn, </a>)</cite>, we are the first to sense its opportunity in training tasks (FedNLP in this case) that naturally involves repeated computations on the same data.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.4" class="ltx_p"><span id="S4.p5.4.1" class="ltx_text ltx_font_bold">Computation and storage cost analysis</span>
Using activation caching reduces the computations by <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="d_{split}/D" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><msub id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml"><mi id="S4.p5.1.m1.1.1.2.2" xref="S4.p5.1.m1.1.1.2.2.cmml">d</mi><mrow id="S4.p5.1.m1.1.1.2.3" xref="S4.p5.1.m1.1.1.2.3.cmml"><mi id="S4.p5.1.m1.1.1.2.3.2" xref="S4.p5.1.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.2.3.1" xref="S4.p5.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.p5.1.m1.1.1.2.3.3" xref="S4.p5.1.m1.1.1.2.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.2.3.1a" xref="S4.p5.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.p5.1.m1.1.1.2.3.4" xref="S4.p5.1.m1.1.1.2.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.2.3.1b" xref="S4.p5.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.p5.1.m1.1.1.2.3.5" xref="S4.p5.1.m1.1.1.2.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.2.3.1c" xref="S4.p5.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.p5.1.m1.1.1.2.3.6" xref="S4.p5.1.m1.1.1.2.3.6.cmml">t</mi></mrow></msub><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">/</mo><mi id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><divide id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1"></divide><apply id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.2.1.cmml" xref="S4.p5.1.m1.1.1.2">subscript</csymbol><ci id="S4.p5.1.m1.1.1.2.2.cmml" xref="S4.p5.1.m1.1.1.2.2">𝑑</ci><apply id="S4.p5.1.m1.1.1.2.3.cmml" xref="S4.p5.1.m1.1.1.2.3"><times id="S4.p5.1.m1.1.1.2.3.1.cmml" xref="S4.p5.1.m1.1.1.2.3.1"></times><ci id="S4.p5.1.m1.1.1.2.3.2.cmml" xref="S4.p5.1.m1.1.1.2.3.2">𝑠</ci><ci id="S4.p5.1.m1.1.1.2.3.3.cmml" xref="S4.p5.1.m1.1.1.2.3.3">𝑝</ci><ci id="S4.p5.1.m1.1.1.2.3.4.cmml" xref="S4.p5.1.m1.1.1.2.3.4">𝑙</ci><ci id="S4.p5.1.m1.1.1.2.3.5.cmml" xref="S4.p5.1.m1.1.1.2.3.5">𝑖</ci><ci id="S4.p5.1.m1.1.1.2.3.6.cmml" xref="S4.p5.1.m1.1.1.2.3.6">𝑡</ci></apply></apply><ci id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">d_{split}/D</annotation></semantics></math> at the forward pass.
Yet it also takes extra storage, i.e., <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="seqlen\times n\times BS" display="inline"><semantics id="S4.p5.2.m2.1a"><mrow id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mrow id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml"><mrow id="S4.p5.2.m2.1.1.2.2" xref="S4.p5.2.m2.1.1.2.2.cmml"><mi id="S4.p5.2.m2.1.1.2.2.2" xref="S4.p5.2.m2.1.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.2.2.1" xref="S4.p5.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.2.2.3" xref="S4.p5.2.m2.1.1.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.2.2.1a" xref="S4.p5.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.2.2.4" xref="S4.p5.2.m2.1.1.2.2.4.cmml">q</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.2.2.1b" xref="S4.p5.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.2.2.5" xref="S4.p5.2.m2.1.1.2.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.2.2.1c" xref="S4.p5.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.2.2.6" xref="S4.p5.2.m2.1.1.2.2.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.2.2.1d" xref="S4.p5.2.m2.1.1.2.2.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.2.2.7" xref="S4.p5.2.m2.1.1.2.2.7.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.p5.2.m2.1.1.2.1" xref="S4.p5.2.m2.1.1.2.1.cmml">×</mo><mi id="S4.p5.2.m2.1.1.2.3" xref="S4.p5.2.m2.1.1.2.3.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.p5.2.m2.1.1.2.1a" xref="S4.p5.2.m2.1.1.2.1.cmml">×</mo><mi id="S4.p5.2.m2.1.1.2.4" xref="S4.p5.2.m2.1.1.2.4.cmml">B</mi></mrow><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.1" xref="S4.p5.2.m2.1.1.1.cmml">​</mo><mi id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><times id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1"></times><apply id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2"><times id="S4.p5.2.m2.1.1.2.1.cmml" xref="S4.p5.2.m2.1.1.2.1"></times><apply id="S4.p5.2.m2.1.1.2.2.cmml" xref="S4.p5.2.m2.1.1.2.2"><times id="S4.p5.2.m2.1.1.2.2.1.cmml" xref="S4.p5.2.m2.1.1.2.2.1"></times><ci id="S4.p5.2.m2.1.1.2.2.2.cmml" xref="S4.p5.2.m2.1.1.2.2.2">𝑠</ci><ci id="S4.p5.2.m2.1.1.2.2.3.cmml" xref="S4.p5.2.m2.1.1.2.2.3">𝑒</ci><ci id="S4.p5.2.m2.1.1.2.2.4.cmml" xref="S4.p5.2.m2.1.1.2.2.4">𝑞</ci><ci id="S4.p5.2.m2.1.1.2.2.5.cmml" xref="S4.p5.2.m2.1.1.2.2.5">𝑙</ci><ci id="S4.p5.2.m2.1.1.2.2.6.cmml" xref="S4.p5.2.m2.1.1.2.2.6">𝑒</ci><ci id="S4.p5.2.m2.1.1.2.2.7.cmml" xref="S4.p5.2.m2.1.1.2.2.7">𝑛</ci></apply><ci id="S4.p5.2.m2.1.1.2.3.cmml" xref="S4.p5.2.m2.1.1.2.3">𝑛</ci><ci id="S4.p5.2.m2.1.1.2.4.cmml" xref="S4.p5.2.m2.1.1.2.4">𝐵</ci></apply><ci id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">seqlen\times n\times BS</annotation></semantics></math> per batch,
where <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p5.3.m3.1a"><mi id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><ci id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">n</annotation></semantics></math> is the transformer’s internal feature size (default 768), and <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="BS" display="inline"><semantics id="S4.p5.4.m4.1a"><mrow id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml"><mi id="S4.p5.4.m4.1.1.2" xref="S4.p5.4.m4.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.p5.4.m4.1.1.1" xref="S4.p5.4.m4.1.1.1.cmml">​</mo><mi id="S4.p5.4.m4.1.1.3" xref="S4.p5.4.m4.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><apply id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1"><times id="S4.p5.4.m4.1.1.1.cmml" xref="S4.p5.4.m4.1.1.1"></times><ci id="S4.p5.4.m4.1.1.2.cmml" xref="S4.p5.4.m4.1.1.2">𝐵</ci><ci id="S4.p5.4.m4.1.1.3.cmml" xref="S4.p5.4.m4.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">BS</annotation></semantics></math> is the batch size (default 4).
The cache is reloaded from disk per minibatch, taking no more than tens of ms on embedded flash and incurs less than 2% overhead.
The total cache size is also proportional to the number of batches samples per client (typically dozens).
Assuming 100 training samples, the storage cost is calculated to be around 100MB.
Such cost is no more than 1% of the storage of a modern mobile/embedded device, e.g., tens to hundreds of GBs.
The cache can be cleared once the FL process finishes.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Caching efficiency under millions of clients</span>
Our activation caching might lose efficiency in the face of millions of available clients, each of which is selected in one or very few rounds.
But we expect such scenarios as uncommon from two folds.
For one thing, an FL process typically needs thousands of or even more rounds to reach convergence.
For another thing, in practice, FL employs rigorous client selection strategies, e.g., a training-eligible device must be charged, wifi-connected, and battery-sufficient. According to Google <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>)</cite>, the utmost eligible client number is only 0.1% (10K out of 10M) at midnight and could even be 10<math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p6.1.m1.1a"><mo id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><times id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\times</annotation></semantics></math> smaller at other times. Such observation is also confirmed by another FL literature <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib90" title="" class="ltx_ref">yang2021characterizing, </a>)</cite> that reports a large portion of devices will never participate in the FL process.
The advanced client selection methods <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>)</cite> that favor strong devices make the participant devices even more skewed and so those devices will participate in many rounds of training.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Implementation and Setups</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have fully implemented the <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> prototype atop <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">FedNLP</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite> (the SOTA framework to evaluate FL methods on NLP tasks) and <span id="S5.p1.1.3" class="ltx_text ltx_font_typewriter">Adapterhub</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib58" title="" class="ltx_ref">pfeiffer2020adapterhub, </a>)</cite> (a library that facilitates the integration of pre-trained adapters for different tasks).
As prior work <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>)</cite>, we adopt the parameter server (PS) architecture among the clients and central server.
At the server side, once job is submitted by the developer, the server initializes the pluggable meta adapter to be trained (through the API of <span id="S5.p1.1.4" class="ltx_text ltx_font_typewriter">Adapterhub</span>) into the pre-trained model.
The server also splits the initialized meta adapter into three branches: normal, wider and deeper.
The wider branch will stack a few meta adapters parallel to expand the bottleneck size of adapter in single layer.
The deeper branch will insert the meta adapter into one more deeper layer.
A client selector will sample 3N clients from available devices and shuffle them into 3 groups.
We now employ a random client selector (default in most FL literature) but more advanced selection strategies <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>; <a href="#bib.bib48" title="" class="ltx_ref">li2021sample, </a>)</cite> can be plugged into our implementation as well.
Then, the server sends three branches of adapters to three groups separately via MPI (in standalone mode) or WLAN/Cellular (in distributed mode).
Once receiving the adapters, the clients insert the adapter into their local pre-trained model.
They fine-tune the model with their own private data.
The trained adapters will be collected in the central server and aggregated through <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">FedAvg</span> algorithm <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>.
All clients run in synchronized mode <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">ho2013more, </a>)</cite>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Device</span></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:113.8pt;">
<span id="S5.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.2.1.1" class="ltx_p"><span id="S5.T3.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Processor</span></span>
</span>
</th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:42.7pt;">
<span id="S5.T3.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.1.1.3.1.1" class="ltx_p"><span id="S5.T3.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Per-batch Latency (s)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Jetson TX2 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T3.2.2.1.1.2.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib1" title="" class="ltx_ref">tx2<span id="S5.T3.2.2.1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T3.2.2.1.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:113.8pt;">
<span id="S5.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.2.1.1" class="ltx_p"><span id="S5.T3.2.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">256-core NVIDIA Pascal™ GPU.</span></span>
</span>
</td>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S5.T3.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.1.3.1.1" class="ltx_p"><span id="S5.T3.2.2.1.3.1.1.1" class="ltx_text" style="font-size:80%;">0.88</span></span>
</span>
</td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T3.2.3.2.1.1" class="ltx_text" style="font-size:80%;">Jetson Nano </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T3.2.3.2.1.2.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib2" title="" class="ltx_ref">nano<span id="S5.T3.2.3.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T3.2.3.2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:113.8pt;">
<span id="S5.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.2.1.1" class="ltx_p"><span id="S5.T3.2.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">128-core NVIDIA CUDA® GPU.</span></span>
</span>
</td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S5.T3.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.3.2.3.1.1" class="ltx_p"><span id="S5.T3.2.3.2.3.1.1.1" class="ltx_text" style="font-size:80%;">1.89</span></span>
</span>
</td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T3.2.4.3.1.1" class="ltx_text" style="font-size:80%;">RPI 4B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T3.2.4.3.1.2.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib3" title="" class="ltx_ref">rpi4b<span id="S5.T3.2.4.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T3.2.4.3.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:113.8pt;">
<span id="S5.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.2.1.1" class="ltx_p"><span id="S5.T3.2.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Broadcom BCM2711B0 quad-core A72 64-bit @ 1.5GHz CPU.</span></span>
</span>
</td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:42.7pt;">
<span id="S5.T3.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.4.3.3.1.1" class="ltx_p"><span id="S5.T3.2.4.3.3.1.1.1" class="ltx_text" style="font-size:80%;">18.27</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.5.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>. </span><span id="S5.T3.6.2" class="ltx_text" style="font-size:113%;">Development boards used in experiments.</span></figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Metrics</span>
We mainly report the time-to-accuracy metric.
We divide the dataset of each device for training (80%) and testing (20%).
For clarity, we pay attention to a few typical accuracy targets, e.g., 99%, 95%, 90% of the full convergence accuracy achievable by the baseline that fine-tunes the whole model.
We refer to those accuracy numbers as <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">relative target accuracy</span>.
For example, the 100% relative target accuracy of BERT is 0.8 (accuracy) for <span id="S5.p2.1.3" class="ltx_text ltx_font_typewriter">20NEWS</span>; 0.9 (accuracy) for <span id="S5.p2.1.4" class="ltx_text ltx_font_typewriter">AGNews</span>; 0.8 (accuracy) for <span id="S5.p2.1.5" class="ltx_text ltx_font_typewriter">SEMEVAL</span>; and 0.75 (token-F1) for <span id="S5.p2.1.6" class="ltx_text ltx_font_typewriter">ONTONOTES</span>.
We also report the resource cost in an FL process, including the total energy consumption on data transmitting and training computation on each client; the total amount of network traffic; and the peak memory usage.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Hardware</span>
As prior FL literature <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>; <a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib70" title="" class="ltx_ref">shin2022fedbalancer, </a>)</cite>, our experiments are carried out in an emulation manner on a GPU server with 8x NVIDIA A40.
The on-device training time is obtained on 3 development boards with similar hardware capacity to mainstream mobile devices, i.e., Jetson TX2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">tx2, </a>)</cite>, Jetson Nano <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">nano, </a>)</cite>, and Raspberry Pi 4B <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">rpi4b, </a>)</cite>.
The numbers are then plugged into the emulation framework to calculate the elapsed time.
The default network bandwidth between clients and server is set to 1MB/s, a typical setting for mobile and IoT devices <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib4" title="" class="ltx_ref">wifi-state, </a>; <a href="#bib.bib24" title="" class="ltx_ref">han2016mp, </a>)</cite>.
Note that while home/office WiFi downlink could be faster, the uplink bandwidth is often bound by the
broadband backbone <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib37" title="" class="ltx_ref">huang2013lte, </a>)</cite>.
In <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S5.p3.1.m1.1a"><mi mathvariant="normal" id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\S</annotation></semantics></math><a href="#S6.SS1" title="6.1. End-to-end Performance ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>, we will also quantify the performance of <span id="S5.p3.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> under various hardware and bandwidth settings (100KB/s–10MB/s).</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Models</span>
We use two representative models for FedNLP tasks: BERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite> (default) and its varient DistilBERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>)</cite>.
BERT and DistilBERT are composed of 12 and 6 transformer blocks, respectively.
DistilBERT leverages knowledge distillation during the pre-training phase and reduces the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.
We use BERT for most of our experiments, as all BERT-based variants derive from it.
The pre-trained weights of both models are downloaded directly from Hugging Face <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib85" title="" class="ltx_ref">wolf2019huggingface, </a>)</cite>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.1.1" class="ltx_tr">
<th id="S5.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></th>
<th id="S5.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></th>
<th id="S5.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;"># of Clients</span></th>
<th id="S5.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Labels</span></th>
<th id="S5.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Non-IID</span></th>
<th id="S5.T4.2.1.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.2.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Samples</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2.1" class="ltx_tr">
<td id="S5.T4.2.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.2.1.1.1" class="ltx_text" style="font-size:80%;">TC</span></td>
<td id="S5.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S5.T4.2.2.1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">20NEWS</span><span id="S5.T4.2.2.1.2.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T4.2.2.1.2.3.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib45" title="" class="ltx_ref">lang1995newsweeder<span id="S5.T4.2.2.1.2.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T4.2.2.1.2.5.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S5.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.2.1.3.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="S5.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.2.1.4.1" class="ltx_text" style="font-size:80%;">20</span></td>
<td id="S5.T4.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.2.1.5.1" class="ltx_text" style="font-size:80%;">/</span></td>
<td id="S5.T4.2.2.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t"><span id="S5.T4.2.2.1.6.1" class="ltx_text" style="font-size:80%;">18.8k</span></td>
</tr>
<tr id="S5.T4.2.3.2" class="ltx_tr">
<td id="S5.T4.2.3.2.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.3.2.1.1" class="ltx_text" style="font-size:80%;">TC</span></td>
<td id="S5.T4.2.3.2.2" class="ltx_td ltx_align_center">
<span id="S5.T4.2.3.2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">AGNEWS</span><span id="S5.T4.2.3.2.2.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T4.2.3.2.2.3.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib93" title="" class="ltx_ref">zhang2015character<span id="S5.T4.2.3.2.2.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T4.2.3.2.2.5.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S5.T4.2.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T4.2.3.2.3.1" class="ltx_text" style="font-size:80%;">1,000</span></td>
<td id="S5.T4.2.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T4.2.3.2.4.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S5.T4.2.3.2.5" class="ltx_td ltx_align_center"><span id="S5.T4.2.3.2.5.1" class="ltx_text" style="font-size:80%;">a=10</span></td>
<td id="S5.T4.2.3.2.6" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.2.3.2.6.1" class="ltx_text" style="font-size:80%;">127.6k</span></td>
</tr>
<tr id="S5.T4.2.4.3" class="ltx_tr">
<td id="S5.T4.2.4.3.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.4.3.1.1" class="ltx_text" style="font-size:80%;">TC</span></td>
<td id="S5.T4.2.4.3.2" class="ltx_td ltx_align_center">
<span id="S5.T4.2.4.3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">SEMEVAL</span><span id="S5.T4.2.4.3.2.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T4.2.4.3.2.3.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib31" title="" class="ltx_ref">hendrickx2019semeval<span id="S5.T4.2.4.3.2.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T4.2.4.3.2.5.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S5.T4.2.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T4.2.4.3.3.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="S5.T4.2.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T4.2.4.3.4.1" class="ltx_text" style="font-size:80%;">19</span></td>
<td id="S5.T4.2.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T4.2.4.3.5.1" class="ltx_text" style="font-size:80%;">a=100</span></td>
<td id="S5.T4.2.4.3.6" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T4.2.4.3.6.1" class="ltx_text" style="font-size:80%;">10.7k</span></td>
</tr>
<tr id="S5.T4.2.5.4" class="ltx_tr">
<td id="S5.T4.2.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.2.5.4.1.1" class="ltx_text" style="font-size:80%;">ST</span></td>
<td id="S5.T4.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S5.T4.2.5.4.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">ONTONOTES</span><span id="S5.T4.2.5.4.2.2" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T4.2.5.4.2.3.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib60" title="" class="ltx_ref">pradhan2013towards<span id="S5.T4.2.5.4.2.4.2.1.1" class="ltx_text" style="font-size:80%;">, </span></a><span id="S5.T4.2.5.4.2.5.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S5.T4.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.2.5.4.3.1" class="ltx_text" style="font-size:80%;">600</span></td>
<td id="S5.T4.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.2.5.4.4.1" class="ltx_text" style="font-size:80%;">37</span></td>
<td id="S5.T4.2.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.2.5.4.5.1" class="ltx_text" style="font-size:80%;">a=10</span></td>
<td id="S5.T4.2.5.4.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><span id="S5.T4.2.5.4.6.1" class="ltx_text" style="font-size:80%;">5.5k</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.17.1.1" class="ltx_text" style="font-size:113%;">Table 4</span>. </span><span id="S5.T4.18.2" class="ltx_text" style="font-size:113%;">Datasets and settings used in experiments for <span id="S5.T4.18.2.1" class="ltx_text ltx_font_bold">T</span>ext <span id="S5.T4.18.2.2" class="ltx_text ltx_font_bold">C</span>lassification and <span id="S5.T4.18.2.3" class="ltx_text ltx_font_bold">S</span>equence <span id="S5.T4.18.2.4" class="ltx_text ltx_font_bold">T</span>agging. “a” is a parameter that controls the datasets’ non-IID level <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>.</span></figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Tasks and datasets</span>
We evaluate <span id="S5.p5.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> on 4 classic NLP downstream datasets as shown in Table <a href="#S5.T4" title="Table 4 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
We follow the approach in <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite> to build the non-IID datasets.
(1) <span id="S5.p5.1.3" class="ltx_text ltx_font_typewriter">20NEWS</span> (IID) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib45" title="" class="ltx_ref">lang1995newsweeder, </a>)</cite> dataset is a collection of approximately 20,000 newsgroup documents.
(2) <span id="S5.p5.1.4" class="ltx_text ltx_font_typewriter">AGNEWS</span> (non-IID) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib93" title="" class="ltx_ref">zhang2015character, </a>)</cite> is a collection of 127.6K news articles gathered from more than 2,000 news sources.
(3) <span id="S5.p5.1.5" class="ltx_text ltx_font_typewriter">SEMEVAL</span> (non-IID) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib31" title="" class="ltx_ref">hendrickx2019semeval, </a>)</cite> is a relation classification datasets which assigns predefined relation labels to the entity pairs that occur in texts.
The above 3 datasets are used for text classification (TC) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib73" title="" class="ltx_ref">sun2019fine, </a>)</cite> tasks, where the output is a label in a fixed set of label set (e.g., political, sports, and entertainment).
(4) <span id="S5.p5.1.6" class="ltx_text ltx_font_typewriter">ONTONOTES</span> (non-IID) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib60" title="" class="ltx_ref">pradhan2013towards, </a>)</cite> is a corpus where sentences have annotations for the entity spans and types.
This dataset is for sequence tagging (ST) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">aras2021evaluation, </a>)</cite> task, where the output is a sequence of tags.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.2.3.1" class="ltx_tr">
<th id="S5.T5.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.2.3.1.1.1" class="ltx_text ltx_font_bold">Datasets</span></th>
<td id="S5.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T5.2.3.1.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">20NEWS</span></td>
<td id="S5.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T5.2.3.1.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">AGNEWS</span></td>
<td id="S5.T5.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S5.T5.2.3.1.4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">SEMEVAL</span></td>
<td id="S5.T5.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T5.2.3.1.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ONTONOTES</span></td>
</tr>
<tr id="S5.T5.2.4.2" class="ltx_tr">
<th id="S5.T5.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Relative Accuracy</th>
<td id="S5.T5.2.4.2.2" class="ltx_td ltx_align_center">99%</td>
<td id="S5.T5.2.4.2.3" class="ltx_td ltx_align_center">95%</td>
<td id="S5.T5.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">90%</td>
<td id="S5.T5.2.4.2.5" class="ltx_td ltx_align_center">99%</td>
<td id="S5.T5.2.4.2.6" class="ltx_td ltx_align_center">95%</td>
<td id="S5.T5.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r">90%</td>
<td id="S5.T5.2.4.2.8" class="ltx_td ltx_align_center">99%</td>
<td id="S5.T5.2.4.2.9" class="ltx_td ltx_align_center">95%</td>
<td id="S5.T5.2.4.2.10" class="ltx_td ltx_align_center ltx_border_r">90%</td>
<td id="S5.T5.2.4.2.11" class="ltx_td ltx_align_center">99%</td>
<td id="S5.T5.2.4.2.12" class="ltx_td ltx_align_center">95%</td>
<td id="S5.T5.2.4.2.13" class="ltx_td ltx_align_center">90%</td>
</tr>
<tr id="S5.T5.2.5.3" class="ltx_tr">
<th id="S5.T5.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.2.5.3.1.1" class="ltx_text ltx_font_typewriter">FT</span></th>
<td id="S5.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">44.0</td>
<td id="S5.T5.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">23.4</td>
<td id="S5.T5.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.1</td>
<td id="S5.T5.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">31.1</td>
<td id="S5.T5.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">10.1</td>
<td id="S5.T5.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.2</td>
<td id="S5.T5.2.5.3.8" class="ltx_td ltx_align_center ltx_border_t">124.3</td>
<td id="S5.T5.2.5.3.9" class="ltx_td ltx_align_center ltx_border_t">89.9</td>
<td id="S5.T5.2.5.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.7</td>
<td id="S5.T5.2.5.3.11" class="ltx_td ltx_align_center ltx_border_t">76.1</td>
<td id="S5.T5.2.5.3.12" class="ltx_td ltx_align_center ltx_border_t">55.9</td>
<td id="S5.T5.2.5.3.13" class="ltx_td ltx_align_center ltx_border_t">35.6</td>
</tr>
<tr id="S5.T5.2.6.4" class="ltx_tr">
<th id="S5.T5.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.2.6.4.1.1" class="ltx_text ltx_font_typewriter">FTQ</span></th>
<td id="S5.T5.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">12.7</td>
<td id="S5.T5.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">6.8</td>
<td id="S5.T5.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.8</td>
<td id="S5.T5.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">9.1</td>
<td id="S5.T5.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t">2.6</td>
<td id="S5.T5.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.7</td>
<td id="S5.T5.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t">32.0</td>
<td id="S5.T5.2.6.4.9" class="ltx_td ltx_align_center ltx_border_t">23.1</td>
<td id="S5.T5.2.6.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.9</td>
<td id="S5.T5.2.6.4.11" class="ltx_td ltx_align_center ltx_border_t">21.2</td>
<td id="S5.T5.2.6.4.12" class="ltx_td ltx_align_center ltx_border_t">15.5</td>
<td id="S5.T5.2.6.4.13" class="ltx_td ltx_align_center ltx_border_t">9.9</td>
</tr>
<tr id="S5.T5.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.1.1.1.1" class="ltx_text ltx_font_typewriter">LF<sub id="S5.T5.1.1.1.1.1" class="ltx_sub"><span id="S5.T5.1.1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span></th>
<td id="S5.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_t">18.5</td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">8.1</td>
<td id="S5.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.3</td>
<td id="S5.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">9.6</td>
<td id="S5.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t">1.4</td>
<td id="S5.T5.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.1</td>
<td id="S5.T5.1.1.8" class="ltx_td ltx_align_center ltx_border_t">74.0</td>
<td id="S5.T5.1.1.9" class="ltx_td ltx_align_center ltx_border_t">46.8</td>
<td id="S5.T5.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.2</td>
<td id="S5.T5.1.1.11" class="ltx_td ltx_align_center ltx_border_t">82.5</td>
<td id="S5.T5.1.1.12" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td id="S5.T5.1.1.13" class="ltx_td ltx_align_center ltx_border_t">24.5</td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<th id="S5.T5.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T5.2.2.1.1" class="ltx_text ltx_font_typewriter">LFQ<sub id="S5.T5.2.2.1.1.1" class="ltx_sub"><span id="S5.T5.2.2.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span></th>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_t">5.2</td>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_t">2.5</td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.1</td>
<td id="S5.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_t">1.6</td>
<td id="S5.T5.2.2.6" class="ltx_td ltx_align_center ltx_border_t">0.3</td>
<td id="S5.T5.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.2</td>
<td id="S5.T5.2.2.8" class="ltx_td ltx_align_center ltx_border_t">16.8</td>
<td id="S5.T5.2.2.9" class="ltx_td ltx_align_center ltx_border_t">11.0</td>
<td id="S5.T5.2.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.7</td>
<td id="S5.T5.2.2.11" class="ltx_td ltx_align_center ltx_border_t">23.9</td>
<td id="S5.T5.2.2.12" class="ltx_td ltx_align_center ltx_border_t">12.9</td>
<td id="S5.T5.2.2.13" class="ltx_td ltx_align_center ltx_border_t">7.2</td>
</tr>
<tr id="S5.T5.2.7.5" class="ltx_tr">
<th id="S5.T5.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T5.2.7.5.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">FedAdapter</span></th>
<td id="S5.T5.2.7.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.2.1" class="ltx_text ltx_font_bold">1.3</span></td>
<td id="S5.T5.2.7.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.3.1" class="ltx_text ltx_font_bold">0.4</span></td>
<td id="S5.T5.2.7.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T5.2.7.5.4.1" class="ltx_text ltx_font_bold">0.1</span></td>
<td id="S5.T5.2.7.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.5.1" class="ltx_text ltx_font_bold">0.2</span></td>
<td id="S5.T5.2.7.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.6.1" class="ltx_text ltx_font_bold">0.03</span></td>
<td id="S5.T5.2.7.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T5.2.7.5.7.1" class="ltx_text ltx_font_bold">0.02</span></td>
<td id="S5.T5.2.7.5.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.8.1" class="ltx_text ltx_font_bold">2.3</span></td>
<td id="S5.T5.2.7.5.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.9.1" class="ltx_text ltx_font_bold">1.1</span></td>
<td id="S5.T5.2.7.5.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T5.2.7.5.10.1" class="ltx_text ltx_font_bold">0.6</span></td>
<td id="S5.T5.2.7.5.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.11.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S5.T5.2.7.5.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.12.1" class="ltx_text ltx_font_bold">2.4</span></td>
<td id="S5.T5.2.7.5.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T5.2.7.5.13.1" class="ltx_text ltx_font_bold">1.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="S5.T5.5.2" class="ltx_text" style="font-size:90%;">Elapsed training time taken to reach different relative target accuracy. NLP model: BERT. Unit: Hour.
</span></figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.1.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x9.png" id="S5.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="337" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x10.png" id="S5.F5.2.2.g1" class="ltx_graphics ltx_img_landscape" width="438" height="341" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.3.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x11.png" id="S5.F5.3.3.g1" class="ltx_graphics ltx_img_landscape" width="438" height="341" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.F5.4.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x12.png" id="S5.F5.4.4.g1" class="ltx_graphics ltx_img_landscape" width="438" height="341" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.4.5.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.4.6.2" class="ltx_text" style="font-size:90%;">BERT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.8" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.5.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x13.png" id="S5.F5.5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="336" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.6.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x14.png" id="S5.F5.6.2.g1" class="ltx_graphics ltx_img_landscape" width="438" height="340" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F5.7.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x15.png" id="S5.F5.7.3.g1" class="ltx_graphics ltx_img_landscape" width="438" height="340" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.F5.8.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:104.1pt;">
<img src="/html/2205.10162/assets/x16.png" id="S5.F5.8.4.g1" class="ltx_graphics ltx_img_landscape" width="438" height="340" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.8.5.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.8.6.2" class="ltx_text" style="font-size:90%;">DistilBERT</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.11.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S5.F5.12.2" class="ltx_text" style="font-size:90%;">Time-to-accuracy throughout a training session. <span id="S5.F5.12.2.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> speeds up model convergence significantly.</span></figcaption>
</figure>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.4" class="ltx_p"><span id="S5.p6.4.5" class="ltx_text ltx_font_bold">Baselines</span>
We compare <span id="S5.p6.4.6" class="ltx_text ltx_font_typewriter">FedAdapter</span> to the following alternatives.
(1) <span id="S5.p6.4.7" class="ltx_text ltx_font_typewriter">Vanilla Fine-Tuning</span> (FT) always fine-tunes the whole model on each client.
This is the default fine-tuning methodology used in most NLP literature <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>; <a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>)</cite>.
(2) <span id="S5.p6.4.8" class="ltx_text ltx_font_typewriter">Fine</span>-<span id="S5.p6.4.9" class="ltx_text ltx_font_typewriter">Tuning</span>-<span id="S5.p6.4.10" class="ltx_text ltx_font_typewriter">Quantized</span> (FTQ) quantizes the model parameters from FP32 to lower precision to reduce the network traffic between clients and aggregator.
Quantization is one of the most widely adopted approaches to reduce the communication cost and speedup FL process.
We use a state-of-the-art quantization algorithm <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib86" title="" class="ltx_ref">wu2018error, </a>)</cite> in our FedNLP tasks.
According to the algorithm, we observe the NLP models are quantized to INT4 or INT8 adaptively.
(3) <span id="S5.p6.4.11" class="ltx_text ltx_font_typewriter">LayerFreeze-Oracle</span> (<span id="S5.p6.1.1" class="ltx_text ltx_font_typewriter">LF<sub id="S5.p6.1.1.1" class="ltx_sub"><span id="S5.p6.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span>) freezes a few transformer layers at bottom and only fine-tunes the ones above.
This is widely used to reduce the fine-tuning cost <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">guo2019spottune, </a>; <a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>.
The number of freezed layers is selected per task to achieve the best time-to-accuracy at 99% relative accuracy target.
(4) <span id="S5.p6.4.12" class="ltx_text ltx_font_typewriter">LayerFreeze</span>-<span id="S5.p6.4.13" class="ltx_text ltx_font_typewriter">Quantized</span>-<span id="S5.p6.4.14" class="ltx_text ltx_font_typewriter">Oracle</span> (<span id="S5.p6.2.2" class="ltx_text ltx_font_typewriter">LFQ<sub id="S5.p6.2.2.1" class="ltx_sub"><span id="S5.p6.2.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span>) combines the above quantization and freezing techniques and selects the best setting for each task, i.e., the number of freezed layers and the quantized data precision.
To be noted, <span id="S5.p6.3.3" class="ltx_text ltx_font_typewriter">LF<sub id="S5.p6.3.3.1" class="ltx_sub"><span id="S5.p6.3.3.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span> and <span id="S5.p6.4.4" class="ltx_text ltx_font_typewriter">LFQ<sub id="S5.p6.4.4.1" class="ltx_sub"><span id="S5.p6.4.4.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span> are impractical in reality as they require prior knowledge to obtain an oracle system parameter.
For a fair comparison, all baselines use the same model aggregation algorithm (<span id="S5.p6.4.15" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">mcmahan2017communication, </a>)</cite>) and client sampling (random), which are also the default setting in prior FL literature <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite>.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Hyper-parameters</span>
Unless otherwise stated, <span id="S5.p7.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> and all baselines use the same set of hyper-parameters as <span id="S5.p7.1.3" class="ltx_text ltx_font_typewriter">FedNLP</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite> framework: mini-batch size as 4; local training iteration as 1; learning rate as 0.1; max sequence length as 256 for <span id="S5.p7.1.4" class="ltx_text ltx_font_typewriter">20NEWS</span> and <span id="S5.p7.1.5" class="ltx_text ltx_font_typewriter">ONTONOTES</span>, 64 for <span id="S5.p7.1.6" class="ltx_text ltx_font_typewriter">AGNEWS</span> and <span id="S5.p7.1.7" class="ltx_text ltx_font_typewriter">SEMEVAL</span>.
For the FL configurations at the server side, we follow the prior FedNLP literature to select 15 participants by default for each training round, i.e., 5 clients in each trial group of <span id="S5.p7.1.8" class="ltx_text ltx_font_typewriter">FedAdapter</span>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Evaluation</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>End-to-end Performance</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.6" class="ltx_p"><span id="S6.SS1.p1.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">FedAdapter<span id="S6.SS1.p1.6.1.1" class="ltx_text ltx_font_serif"> reduces model convergence delays significantly, making FedNLP practical.</span></span>
Table <a href="#S5.T5" title="Table 5 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the convergence time and Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the convergence process under the default setting.
To reach 99% relative target accuracy, <span id="S6.SS1.p1.6.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> is 33.8<math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mo id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><times id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">\times</annotation></semantics></math>, 155.5<math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mo id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><times id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">\times</annotation></semantics></math>, 54.0<math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><mo id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><times id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">\times</annotation></semantics></math> and 16.9<math id="S6.SS1.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.4.m4.1a"><mo id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><times id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">\times</annotation></semantics></math> faster than <span id="S6.SS1.p1.6.3" class="ltx_text ltx_font_typewriter">FT</span> on the four datasets, respectively.
With a lower target accuracy such as 90%, the speedup brought by <span id="S6.SS1.p1.6.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> is even more significant, i.e., 27.4<math id="S6.SS1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.5.m5.1a"><mo id="S6.SS1.p1.5.m5.1.1" xref="S6.SS1.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.5.m5.1b"><times id="S6.SS1.p1.5.m5.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.5.m5.1c">\times</annotation></semantics></math>–260.0<math id="S6.SS1.p1.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p1.6.m6.1a"><mo id="S6.SS1.p1.6.m6.1.1" xref="S6.SS1.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.6.m6.1b"><times id="S6.SS1.p1.6.m6.1.1.cmml" xref="S6.SS1.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.6.m6.1c">\times</annotation></semantics></math>.
It takes at most one hour for <span id="S6.SS1.p1.6.5" class="ltx_text ltx_font_typewriter">FedAdapter</span> to reach a usable accuracy.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.8" class="ltx_p">More competitive baselines <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">LF<sub id="S6.SS1.p2.1.1.1" class="ltx_sub"><span id="S6.SS1.p2.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> and <span id="S6.SS1.p2.8.6" class="ltx_text ltx_font_typewriter">FTQ</span> only bring limited improvement over <span id="S6.SS1.p2.8.7" class="ltx_text ltx_font_typewriter">FT</span>, i.e., <math id="S6.SS1.p2.2.m1.1" class="ltx_math_unparsed" alttext="2.4\times" display="inline"><semantics id="S6.SS1.p2.2.m1.1a"><mrow id="S6.SS1.p2.2.m1.1b"><mn id="S6.SS1.p2.2.m1.1.1">2.4</mn><mo lspace="0.222em" id="S6.SS1.p2.2.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m1.1c">2.4\times</annotation></semantics></math>–<math id="S6.SS1.p2.3.m2.1" class="ltx_math_unparsed" alttext="3.9\times" display="inline"><semantics id="S6.SS1.p2.3.m2.1a"><mrow id="S6.SS1.p2.3.m2.1b"><mn id="S6.SS1.p2.3.m2.1.1">3.9</mn><mo lspace="0.222em" id="S6.SS1.p2.3.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m2.1c">3.9\times</annotation></semantics></math> speedup.
Dozens of hours are still needed for a single downstream task.
<span id="S6.SS1.p2.4.2" class="ltx_text ltx_font_typewriter">LFQ<sub id="S6.SS1.p2.4.2.1" class="ltx_sub"><span id="S6.SS1.p2.4.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> can benefit from both layer-freezing and quantization, therefore performing better than other baselines.
Though, <span id="S6.SS1.p2.8.8" class="ltx_text ltx_font_typewriter">FedAdapter</span> still beats <span id="S6.SS1.p2.5.3" class="ltx_text ltx_font_typewriter">LFQ<sub id="S6.SS1.p2.5.3.1" class="ltx_sub"><span id="S6.SS1.p2.5.3.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> nontrivially, especially for reaching a relatively lower target accuracy.
For example, <span id="S6.SS1.p2.8.9" class="ltx_text ltx_font_typewriter">FedAdapter</span> is <math id="S6.SS1.p2.6.m3.1" class="ltx_math_unparsed" alttext="12.8\times" display="inline"><semantics id="S6.SS1.p2.6.m3.1a"><mrow id="S6.SS1.p2.6.m3.1b"><mn id="S6.SS1.p2.6.m3.1.1">12.8</mn><mo lspace="0.222em" id="S6.SS1.p2.6.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.6.m3.1c">12.8\times</annotation></semantics></math> faster on <span id="S6.SS1.p2.8.10" class="ltx_text ltx_font_typewriter">SEMEVAL</span> to reach 90% relative target accuracy.
This is because <span id="S6.SS1.p2.8.11" class="ltx_text ltx_font_typewriter">FedAdapter</span> employs an upgrading mechanism on adapter configuration which enables fast boosting of the training accuracy.
Note that both <span id="S6.SS1.p2.7.4" class="ltx_text ltx_font_typewriter">LF<sub id="S6.SS1.p2.7.4.1" class="ltx_sub"><span id="S6.SS1.p2.7.4.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> and <span id="S6.SS1.p2.8.5" class="ltx_text ltx_font_typewriter">LFQ<sub id="S6.SS1.p2.8.5.1" class="ltx_sub"><span id="S6.SS1.p2.8.5.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> are not practical methods as they use the “optimal” tuning depth which is not likely to be known beforehand.
If an improper depth is chosen, their performance drastically deteriorates.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.3" class="ltx_p">We also extend our experiments to DistilBERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>)</cite>, a distilled version of BERT, and illustrate the results in Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
It shows that <span id="S6.SS1.p3.3.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> significantly outperforms the baselines on DistilBERT as well.
For instance, <span id="S6.SS1.p3.3.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> achieves <math id="S6.SS1.p3.1.m1.1" class="ltx_math_unparsed" alttext="14.89\times" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mrow id="S6.SS1.p3.1.m1.1b"><mn id="S6.SS1.p3.1.m1.1.1">14.89</mn><mo lspace="0.222em" id="S6.SS1.p3.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">14.89\times</annotation></semantics></math>–<math id="S6.SS1.p3.2.m2.1" class="ltx_math_unparsed" alttext="73.42\times" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mrow id="S6.SS1.p3.2.m2.1b"><mn id="S6.SS1.p3.2.m2.1.1">73.42</mn><mo lspace="0.222em" id="S6.SS1.p3.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">73.42\times</annotation></semantics></math> speedup over <span id="S6.SS1.p3.3.3" class="ltx_text ltx_font_typewriter">FT</span> to obtain the 99% relative target accuracy.
Interestingly, comparing DistilBert to BERT under <span id="S6.SS1.p3.3.4" class="ltx_text ltx_font_typewriter">FT</span>, we find the former achieves up to <math id="S6.SS1.p3.3.m3.1" class="ltx_math_unparsed" alttext="7.7\times" display="inline"><semantics id="S6.SS1.p3.3.m3.1a"><mrow id="S6.SS1.p3.3.m3.1b"><mn id="S6.SS1.p3.3.m3.1.1">7.7</mn><mo lspace="0.222em" id="S6.SS1.p3.3.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">7.7\times</annotation></semantics></math> speedup on <span id="S6.SS1.p3.3.5" class="ltx_text ltx_font_typewriter">AGNEWS</span> on Jetson TX2 since it is more lightweight in consideration of both computation and communication.
However, this advantage does not hold for all circumstances.
Comparing Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (d) with Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Implementation and Setups ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (d), we find that BERT has comparable performance with DistilBERT on <span id="S6.SS1.p3.3.6" class="ltx_text ltx_font_typewriter">ONTONOTES</span>.
It is likely attributed that, when the downstream task gets harder, the loss of model’s representation capacity during distilling incurs negative impacts.
Such a difference will compensate the large model size and computation complexity of the vanilla BERT.
Nevertheless, <span id="S6.SS1.p3.3.7" class="ltx_text ltx_font_typewriter">FedAdapter</span> performs consistently on both models because it judiciously tunes the pluggable adapters with different depths and widths.</p>
</div>
<figure id="S6.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:104.1pt;"><img src="/html/2205.10162/assets/x17.png" id="S6.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="876" height="306" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F6.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">20NEWS</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:104.1pt;"><img src="/html/2205.10162/assets/x18.png" id="S6.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="405" height="242" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F6.2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">SEMEVAL</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.5.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S6.F6.6.2" class="ltx_text" style="font-size:90%;">
<span id="S6.F6.6.2.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> outperforms baselines under all network bandwidths with 99% target accuracy.
</span></figcaption>
</figure>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.5" class="ltx_p"><span id="S6.SS1.p4.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">FedAdapter<span id="S6.SS1.p4.5.1.1" class="ltx_text ltx_font_serif"> outperforms baselines in various network environments.</span></span>
Figure <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">6</span></span> reports the performance of <span id="S6.SS1.p4.5.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> and baselines under various network environment from 0.1MB/s to 10MB/s, which cover the typical network capacity for nowaday WiFi and cellular bandwidth.
Our key observation is that <span id="S6.SS1.p4.5.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> consistently outperforms other baselines with different network conditions, and the improvement is more significant with lower network bandwidth.
For instance, with <math id="S6.SS1.p4.1.m1.1" class="ltx_Math" alttext="bandwidth=10MB/s" display="inline"><semantics id="S6.SS1.p4.1.m1.1a"><mrow id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml"><mrow id="S6.SS1.p4.1.m1.1.1.2" xref="S6.SS1.p4.1.m1.1.1.2.cmml"><mi id="S6.SS1.p4.1.m1.1.1.2.2" xref="S6.SS1.p4.1.m1.1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.3" xref="S6.SS1.p4.1.m1.1.1.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1a" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.4" xref="S6.SS1.p4.1.m1.1.1.2.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1b" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.5" xref="S6.SS1.p4.1.m1.1.1.2.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1c" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.6" xref="S6.SS1.p4.1.m1.1.1.2.6.cmml">w</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1d" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.7" xref="S6.SS1.p4.1.m1.1.1.2.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1e" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.8" xref="S6.SS1.p4.1.m1.1.1.2.8.cmml">d</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1f" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.9" xref="S6.SS1.p4.1.m1.1.1.2.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.2.1g" xref="S6.SS1.p4.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.2.10" xref="S6.SS1.p4.1.m1.1.1.2.10.cmml">h</mi></mrow><mo id="S6.SS1.p4.1.m1.1.1.1" xref="S6.SS1.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S6.SS1.p4.1.m1.1.1.3" xref="S6.SS1.p4.1.m1.1.1.3.cmml"><mrow id="S6.SS1.p4.1.m1.1.1.3.2" xref="S6.SS1.p4.1.m1.1.1.3.2.cmml"><mn id="S6.SS1.p4.1.m1.1.1.3.2.2" xref="S6.SS1.p4.1.m1.1.1.3.2.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.3.2.1" xref="S6.SS1.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.3.2.3" xref="S6.SS1.p4.1.m1.1.1.3.2.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p4.1.m1.1.1.3.2.1a" xref="S6.SS1.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S6.SS1.p4.1.m1.1.1.3.2.4" xref="S6.SS1.p4.1.m1.1.1.3.2.4.cmml">B</mi></mrow><mo id="S6.SS1.p4.1.m1.1.1.3.1" xref="S6.SS1.p4.1.m1.1.1.3.1.cmml">/</mo><mi id="S6.SS1.p4.1.m1.1.1.3.3" xref="S6.SS1.p4.1.m1.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><apply id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1"><eq id="S6.SS1.p4.1.m1.1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1.1"></eq><apply id="S6.SS1.p4.1.m1.1.1.2.cmml" xref="S6.SS1.p4.1.m1.1.1.2"><times id="S6.SS1.p4.1.m1.1.1.2.1.cmml" xref="S6.SS1.p4.1.m1.1.1.2.1"></times><ci id="S6.SS1.p4.1.m1.1.1.2.2.cmml" xref="S6.SS1.p4.1.m1.1.1.2.2">𝑏</ci><ci id="S6.SS1.p4.1.m1.1.1.2.3.cmml" xref="S6.SS1.p4.1.m1.1.1.2.3">𝑎</ci><ci id="S6.SS1.p4.1.m1.1.1.2.4.cmml" xref="S6.SS1.p4.1.m1.1.1.2.4">𝑛</ci><ci id="S6.SS1.p4.1.m1.1.1.2.5.cmml" xref="S6.SS1.p4.1.m1.1.1.2.5">𝑑</ci><ci id="S6.SS1.p4.1.m1.1.1.2.6.cmml" xref="S6.SS1.p4.1.m1.1.1.2.6">𝑤</ci><ci id="S6.SS1.p4.1.m1.1.1.2.7.cmml" xref="S6.SS1.p4.1.m1.1.1.2.7">𝑖</ci><ci id="S6.SS1.p4.1.m1.1.1.2.8.cmml" xref="S6.SS1.p4.1.m1.1.1.2.8">𝑑</ci><ci id="S6.SS1.p4.1.m1.1.1.2.9.cmml" xref="S6.SS1.p4.1.m1.1.1.2.9">𝑡</ci><ci id="S6.SS1.p4.1.m1.1.1.2.10.cmml" xref="S6.SS1.p4.1.m1.1.1.2.10">ℎ</ci></apply><apply id="S6.SS1.p4.1.m1.1.1.3.cmml" xref="S6.SS1.p4.1.m1.1.1.3"><divide id="S6.SS1.p4.1.m1.1.1.3.1.cmml" xref="S6.SS1.p4.1.m1.1.1.3.1"></divide><apply id="S6.SS1.p4.1.m1.1.1.3.2.cmml" xref="S6.SS1.p4.1.m1.1.1.3.2"><times id="S6.SS1.p4.1.m1.1.1.3.2.1.cmml" xref="S6.SS1.p4.1.m1.1.1.3.2.1"></times><cn type="integer" id="S6.SS1.p4.1.m1.1.1.3.2.2.cmml" xref="S6.SS1.p4.1.m1.1.1.3.2.2">10</cn><ci id="S6.SS1.p4.1.m1.1.1.3.2.3.cmml" xref="S6.SS1.p4.1.m1.1.1.3.2.3">𝑀</ci><ci id="S6.SS1.p4.1.m1.1.1.3.2.4.cmml" xref="S6.SS1.p4.1.m1.1.1.3.2.4">𝐵</ci></apply><ci id="S6.SS1.p4.1.m1.1.1.3.3.cmml" xref="S6.SS1.p4.1.m1.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">bandwidth=10MB/s</annotation></semantics></math>, <span id="S6.SS1.p4.5.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> reaches the 99% relative target accuracy <math id="S6.SS1.p4.2.m2.1" class="ltx_math_unparsed" alttext="5.6\times" display="inline"><semantics id="S6.SS1.p4.2.m2.1a"><mrow id="S6.SS1.p4.2.m2.1b"><mn id="S6.SS1.p4.2.m2.1.1">5.6</mn><mo lspace="0.222em" id="S6.SS1.p4.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p4.2.m2.1c">5.6\times</annotation></semantics></math> and <math id="S6.SS1.p4.3.m3.1" class="ltx_math_unparsed" alttext="9.2\times" display="inline"><semantics id="S6.SS1.p4.3.m3.1a"><mrow id="S6.SS1.p4.3.m3.1b"><mn id="S6.SS1.p4.3.m3.1.1">9.2</mn><mo lspace="0.222em" id="S6.SS1.p4.3.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p4.3.m3.1c">9.2\times</annotation></semantics></math> faster than <span id="S6.SS1.p4.5.5" class="ltx_text ltx_font_typewriter">FT</span> on <span id="S6.SS1.p4.5.6" class="ltx_text ltx_font_typewriter">20NEWS</span> and <span id="S6.SS1.p4.5.7" class="ltx_text ltx_font_typewriter">SEMEVAL</span>, seperately.
When the bandwidth goes down to 0.1MB/s, the improvement is as high as 137.7<math id="S6.SS1.p4.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.4.m4.1a"><mo id="S6.SS1.p4.4.m4.1.1" xref="S6.SS1.p4.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.4.m4.1b"><times id="S6.SS1.p4.4.m4.1.1.cmml" xref="S6.SS1.p4.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.4.m4.1c">\times</annotation></semantics></math> and 112.5<math id="S6.SS1.p4.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p4.5.m5.1a"><mo id="S6.SS1.p4.5.m5.1.1" xref="S6.SS1.p4.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.5.m5.1b"><times id="S6.SS1.p4.5.m5.1.1.cmml" xref="S6.SS1.p4.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.5.m5.1c">\times</annotation></semantics></math>, respectively.
The rationale behind this is that <span id="S6.SS1.p4.5.8" class="ltx_text ltx_font_typewriter">FedAdapter</span> brings the most network transmission reduction by inserting tiny adapter modules into the model.
Such a micro transmission package makes the communication process fast even with very low network bandwidth.
In reality, network fluctuation is common <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>)</cite>.
<span id="S6.SS1.p4.5.9" class="ltx_text ltx_font_typewriter">FedAdapter</span> enables a stable fine-tuning process and paves the way for the fundamental solution to stragglers or other possible communication problems <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib64" title="" class="ltx_ref">reisizadeh2020straggler, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>)</cite> that will drag the NLP fine-tuning slow.</p>
</div>
<figure id="S6.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:208.1pt;"><img src="/html/2205.10162/assets/x19.png" id="S6.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="142" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F7.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">20NEWS</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:208.1pt;"><img src="/html/2205.10162/assets/x20.png" id="S6.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="142" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F7.2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">SEMEVAL</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.4.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S6.F7.5.2" class="ltx_text" style="font-size:90%;">Convergence delays with a variety of client hardware.
Training targets 99% relative target accuracy.
“Heterogeneous” means the device capacity is uniformly distributed between three boards.</span></figcaption>
</figure>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.6" class="ltx_p"><span id="S6.SS1.p5.6.1" class="ltx_text ltx_font_typewriter ltx_font_bold">FedAdapter<span id="S6.SS1.p5.6.1.1" class="ltx_text ltx_font_serif"> outperforms baselines on various client hardware.</span></span>
<span id="S6.SS1.p5.6.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> also consistently outperforms other baselines with different device capacity as shown in Figure <a href="#S6.F7" title="Figure 7 ‣ 6.1. End-to-end Performance ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
On GPU-powered high-end embedded devices like Jetson TX2 and Jetson Nano, <span id="S6.SS1.p5.6.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> reaches the 99% relative target accuracy up tp 32.8<math id="S6.SS1.p5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p5.1.m1.1a"><mo id="S6.SS1.p5.1.m1.1.1" xref="S6.SS1.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.1.m1.1b"><times id="S6.SS1.p5.1.m1.1.1.cmml" xref="S6.SS1.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.1.m1.1c">\times</annotation></semantics></math> and 79.2<math id="S6.SS1.p5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p5.2.m2.1a"><mo id="S6.SS1.p5.2.m2.1.1" xref="S6.SS1.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.2.m2.1b"><times id="S6.SS1.p5.2.m2.1.1.cmml" xref="S6.SS1.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.2.m2.1c">\times</annotation></semantics></math> faster than the <span id="S6.SS1.p5.6.4" class="ltx_text ltx_font_typewriter">VanilaFT</span> on <span id="S6.SS1.p5.6.5" class="ltx_text ltx_font_typewriter">20NEWS</span> and <span id="S6.SS1.p5.6.6" class="ltx_text ltx_font_typewriter">SEMEVAL</span>, respectively.
On a much wimpy device RPI 4B, the speedup degrades to 3.8<math id="S6.SS1.p5.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p5.3.m3.1a"><mo id="S6.SS1.p5.3.m3.1.1" xref="S6.SS1.p5.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.3.m3.1b"><times id="S6.SS1.p5.3.m3.1.1.cmml" xref="S6.SS1.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.3.m3.1c">\times</annotation></semantics></math> and 7.0<math id="S6.SS1.p5.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p5.4.m4.1a"><mo id="S6.SS1.p5.4.m4.1.1" xref="S6.SS1.p5.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p5.4.m4.1b"><times id="S6.SS1.p5.4.m4.1.1.cmml" xref="S6.SS1.p5.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p5.4.m4.1c">\times</annotation></semantics></math>, respectively.
This is because <span id="S6.SS1.p5.6.7" class="ltx_text ltx_font_typewriter">FedAdapter</span> reduces two orders of magnitude in communication cost, but only one order of magnitude in computation cost.
Nevertheless, <span id="S6.SS1.p5.6.8" class="ltx_text ltx_font_typewriter">FedAdapter</span> can still bring remarkable improvement on the wimpy devices.
For example, on dataset <span id="S6.SS1.p5.6.9" class="ltx_text ltx_font_typewriter">20NEWS</span>, <span id="S6.SS1.p5.6.10" class="ltx_text ltx_font_typewriter">FedAdapter</span> reaches target accuracy <math id="S6.SS1.p5.5.m5.1" class="ltx_math_unparsed" alttext="5.03\times" display="inline"><semantics id="S6.SS1.p5.5.m5.1a"><mrow id="S6.SS1.p5.5.m5.1b"><mn id="S6.SS1.p5.5.m5.1.1">5.03</mn><mo lspace="0.222em" id="S6.SS1.p5.5.m5.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p5.5.m5.1c">5.03\times</annotation></semantics></math> and <math id="S6.SS1.p5.6.m6.1" class="ltx_math_unparsed" alttext="3.25\times" display="inline"><semantics id="S6.SS1.p5.6.m6.1a"><mrow id="S6.SS1.p5.6.m6.1b"><mn id="S6.SS1.p5.6.m6.1.1">3.25</mn><mo lspace="0.222em" id="S6.SS1.p5.6.m6.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p5.6.m6.1c">3.25\times</annotation></semantics></math> faster than the four baselines, respectively.
Moreover, <span id="S6.SS1.p5.6.11" class="ltx_text ltx_font_typewriter">FedAdapter</span> also shows improvement under heterogeneous settings, where the device capacity is assumed to be uniformly distributed between Jetson TX2, Jetson Nano and RPI 4B.
The profit is not as significant as on high-end devices because the per-round training is bottlenecked by the slower devices <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib64" title="" class="ltx_ref">reisizadeh2020straggler, </a>)</cite>.
Extensive research efforts have been invested to mitigate straggler issue <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>)</cite> and <span id="S6.SS1.p5.6.12" class="ltx_text ltx_font_typewriter">FedAdapter</span> is orthogonal to those techniques.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Significance of Key Designs</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.3" class="ltx_p">The benefits of <span id="S6.SS2.p1.3.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> come from: the adapters (<math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mi mathvariant="normal" id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS1" title="3.1. Plugable Adapters ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), the activation cache (<math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mi mathvariant="normal" id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><ci id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">\S</annotation></semantics></math><a href="#S4" title="4. FedNLP Activation Cache ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), the automatic adapter configuration and trial-and-error clients (<math id="S6.SS2.p1.3.m3.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S6.SS2.p1.3.m3.1a"><mi mathvariant="normal" id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><ci id="S6.SS2.p1.3.m3.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">\S</annotation></semantics></math><a href="#S3.SS3" title="3.3. The Online Configurator ‣ 3. Design ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
We now quantify their benefits.</p>
</div>
<figure id="S6.F8" class="ltx_figure"><img src="/html/2205.10162/assets/x21.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="226" height="67" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S6.F8.4.2" class="ltx_text" style="font-size:90%;">Model convergence delays with and without <span id="S6.F8.4.2.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s key designs, showing their significance.</span></figcaption>
</figure>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.7" class="ltx_p"><span id="S6.SS2.p2.7.3" class="ltx_text ltx_font_bold">Adapter and caching.</span>
Figure <a href="#S6.F8" title="Figure 8 ‣ 6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows benefit of adapters and caching.
Naive use of adapters, e.g. inserting at all layers, may brings notable benefit,
e.g., on Jetson TX2 and benchmark <span id="S6.SS2.p2.7.4" class="ltx_text ltx_font_typewriter">AGNEWS</span>, <span id="S6.SS2.p2.7.5" class="ltx_text ltx_font_typewriter">Vanilla-Adapter</span> is 10.1<math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mo id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><times id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\times</annotation></semantics></math> faster than <span id="S6.SS2.p2.7.6" class="ltx_text ltx_font_typewriter">FT</span>.
However, the delays are still too high.
On RPI 4B, naive use of adapters slows down the model convergence as compared to fine-tuning the whole model.
By using one static, oracle adapter configuration (<span id="S6.SS2.p2.2.1" class="ltx_text ltx_font_typewriter">Adapter<sub id="S6.SS2.p2.2.1.1" class="ltx_sub"><span id="S6.SS2.p2.2.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub></span>), the time-to-accuracy is reduced by up tp 23.0<math id="S6.SS2.p2.3.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS2.p2.3.m2.1a"><mo id="S6.SS2.p2.3.m2.1.1" xref="S6.SS2.p2.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m2.1b"><times id="S6.SS2.p2.3.m2.1.1.cmml" xref="S6.SS2.p2.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m2.1c">\times</annotation></semantics></math> compared to <span id="S6.SS2.p2.7.7" class="ltx_text ltx_font_typewriter">FT</span>.
Employing the activation cache technique (<span id="S6.SS2.p2.4.2" class="ltx_text ltx_font_typewriter">Adapter<sub id="S6.SS2.p2.4.2.1" class="ltx_sub"><span id="S6.SS2.p2.4.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">oracle</span></sub>+Cache</span>) further brings 2.1<math id="S6.SS2.p2.5.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS2.p2.5.m3.1a"><mo id="S6.SS2.p2.5.m3.1.1" xref="S6.SS2.p2.5.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.5.m3.1b"><times id="S6.SS2.p2.5.m3.1.1.cmml" xref="S6.SS2.p2.5.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.5.m3.1c">\times</annotation></semantics></math>–3.3<math id="S6.SS2.p2.6.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS2.p2.6.m4.1a"><mo id="S6.SS2.p2.6.m4.1.1" xref="S6.SS2.p2.6.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.6.m4.1b"><times id="S6.SS2.p2.6.m4.1.1.cmml" xref="S6.SS2.p2.6.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.6.m4.1c">\times</annotation></semantics></math> speedup.
In another micro experiment, Figure <a href="#S6.F11" title="Figure 11 ‣ 6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows that, with activation caching, the training time decreases almost linearly with fewer adapter layers to be updated (<math id="S6.SS2.p2.7.m5.1" class="ltx_Math" alttext="D-d_{split}" display="inline"><semantics id="S6.SS2.p2.7.m5.1a"><mrow id="S6.SS2.p2.7.m5.1.1" xref="S6.SS2.p2.7.m5.1.1.cmml"><mi id="S6.SS2.p2.7.m5.1.1.2" xref="S6.SS2.p2.7.m5.1.1.2.cmml">D</mi><mo id="S6.SS2.p2.7.m5.1.1.1" xref="S6.SS2.p2.7.m5.1.1.1.cmml">−</mo><msub id="S6.SS2.p2.7.m5.1.1.3" xref="S6.SS2.p2.7.m5.1.1.3.cmml"><mi id="S6.SS2.p2.7.m5.1.1.3.2" xref="S6.SS2.p2.7.m5.1.1.3.2.cmml">d</mi><mrow id="S6.SS2.p2.7.m5.1.1.3.3" xref="S6.SS2.p2.7.m5.1.1.3.3.cmml"><mi id="S6.SS2.p2.7.m5.1.1.3.3.2" xref="S6.SS2.p2.7.m5.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p2.7.m5.1.1.3.3.1" xref="S6.SS2.p2.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p2.7.m5.1.1.3.3.3" xref="S6.SS2.p2.7.m5.1.1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p2.7.m5.1.1.3.3.1a" xref="S6.SS2.p2.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p2.7.m5.1.1.3.3.4" xref="S6.SS2.p2.7.m5.1.1.3.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p2.7.m5.1.1.3.3.1b" xref="S6.SS2.p2.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p2.7.m5.1.1.3.3.5" xref="S6.SS2.p2.7.m5.1.1.3.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p2.7.m5.1.1.3.3.1c" xref="S6.SS2.p2.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p2.7.m5.1.1.3.3.6" xref="S6.SS2.p2.7.m5.1.1.3.3.6.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.7.m5.1b"><apply id="S6.SS2.p2.7.m5.1.1.cmml" xref="S6.SS2.p2.7.m5.1.1"><minus id="S6.SS2.p2.7.m5.1.1.1.cmml" xref="S6.SS2.p2.7.m5.1.1.1"></minus><ci id="S6.SS2.p2.7.m5.1.1.2.cmml" xref="S6.SS2.p2.7.m5.1.1.2">𝐷</ci><apply id="S6.SS2.p2.7.m5.1.1.3.cmml" xref="S6.SS2.p2.7.m5.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p2.7.m5.1.1.3.1.cmml" xref="S6.SS2.p2.7.m5.1.1.3">subscript</csymbol><ci id="S6.SS2.p2.7.m5.1.1.3.2.cmml" xref="S6.SS2.p2.7.m5.1.1.3.2">𝑑</ci><apply id="S6.SS2.p2.7.m5.1.1.3.3.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3"><times id="S6.SS2.p2.7.m5.1.1.3.3.1.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.1"></times><ci id="S6.SS2.p2.7.m5.1.1.3.3.2.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.2">𝑠</ci><ci id="S6.SS2.p2.7.m5.1.1.3.3.3.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.3">𝑝</ci><ci id="S6.SS2.p2.7.m5.1.1.3.3.4.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.4">𝑙</ci><ci id="S6.SS2.p2.7.m5.1.1.3.3.5.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.5">𝑖</ci><ci id="S6.SS2.p2.7.m5.1.1.3.3.6.cmml" xref="S6.SS2.p2.7.m5.1.1.3.3.6">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.7.m5.1c">D-d_{split}</annotation></semantics></math>), and the improvement from caching mechanism increases significantly as well.</p>
</div>
<figure id="S6.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F9.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x22.png" id="S6.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="284" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F9.1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F9.1.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">20NEWS</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F9.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:99.7pt;"><img src="/html/2205.10162/assets/x23.png" id="S6.F9.2.g1" class="ltx_graphics ltx_img_landscape" width="442" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F9.2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F9.2.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">SEMEVAL</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F9.5.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S6.F9.6.2" class="ltx_text" style="font-size:90%;">Time-to-accuracy throughout a training session. <span id="S6.F9.6.2.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s accuracy (red lines) always outperforms those of fixed adapter configuration (208 in total, aggregated as blue shades, for which blue dotted lines show averages).</span></figcaption>
</figure>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">Automatic configuration</span>
To demonstrate the importance of <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span>’s upgrading mechanism on the adapter’s tuning configuration, we exhaustively sweep through all adapter configurations (depth 0–12, width 8,16,..,128, 208 configurations in total) of BERT on <span id="S6.SS2.p3.1.3" class="ltx_text ltx_font_typewriter">20NEWS</span>, and aggregate their convergence curves as shaded areas shown in Figure <a href="#S6.F9" title="Figure 9 ‣ 6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
The blue line (dotted) is the average time-to-accuracy of all configurations while the red line (solid) is the curve of <span id="S6.SS2.p3.1.4" class="ltx_text ltx_font_typewriter">FedAdapter</span>.
Note that sweeping all configurations is very expensive: it takes thousands of GPU hours to run the benchmark in a subfigure.
The results show that <span id="S6.SS2.p3.1.5" class="ltx_text ltx_font_typewriter">FedAdapter</span> almost outperforms every configuration throughout a training session.
This is owing to <span id="S6.SS2.p3.1.6" class="ltx_text ltx_font_typewriter">FedAdapter</span> switching among different configurations that best suits the current training session.
Typically, we observe <span id="S6.SS2.p3.1.7" class="ltx_text ltx_font_typewriter">FedAdapter</span> uses 8–14 configurations per training session.</p>
</div>
<figure id="S6.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F11.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:95.4pt;"><img src="/html/2205.10162/assets/x24.png" id="S6.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="285" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S6.F11.1.2.2" class="ltx_text" style="font-size:90%;">Per-batch training time with/without activation caching. Model: DistilBERT. Device: Jetson TX2.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F11.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:95.4pt;"><img src="/html/2205.10162/assets/x25.png" id="S6.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="S6.F11.2.3.2" class="ltx_text" style="font-size:90%;">
Model converge time with different client numbers.
Dataset: <span id="S6.F11.2.3.2.1" class="ltx_text ltx_font_typewriter">20NEWS</span>. Model: BERT. Device: Jetson TX2.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_bold">Investment of extra clients</span>
<span id="S6.SS2.p4.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> uses more clients to identify whether it shall upgrade to a more complex adapter configuration through trial and error.
We evaluate the performance of <span id="S6.SS2.p4.1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> with different client numbers as compared to <span id="S6.SS2.p4.1.4" class="ltx_text ltx_font_typewriter">VanillaFT</span>.
As show in Figure <a href="#S6.F11" title="Figure 11 ‣ 6.2. Significance of Key Designs ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, <span id="S6.SS2.p4.1.5" class="ltx_text ltx_font_typewriter">VanillaFT</span> achieves the best performance with 6 participant clients per round.
On the other hand, using the extra clients for trial-and-error is much more beneficial, i.e., better scalability to the available clients.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Client Resource Cost</h3>

<figure id="S6.F12" class="ltx_figure"><img src="/html/2205.10162/assets/x26.png" id="S6.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="71" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>. </span><span id="S6.F12.3.2" class="ltx_text" style="font-size:90%;">Total network traffic of all client devices. Training targets 99% relative target accuracy.</span></figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.2" class="ltx_p"><span id="S6.SS3.p1.2.1" class="ltx_text ltx_font_bold">Network traffic.</span>
Figure <a href="#S6.F12" title="Figure 12 ‣ 6.3. Client Resource Cost ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> reports the total network traffic incurred during fine-tuning to reach 99% relative target accuracy.
It shows that <span id="S6.SS3.p1.2.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> saves 126.7<math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mo id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><times id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">\times</annotation></semantics></math> on average and up to <math id="S6.SS3.p1.2.m2.1" class="ltx_math_unparsed" alttext="220.7\times" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1b"><mn id="S6.SS3.p1.2.m2.1.1">220.7</mn><mo lspace="0.222em" id="S6.SS3.p1.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">220.7\times</annotation></semantics></math> (reducing from 2194.3 GB to 9.9 GB) network traffic compared to the <span id="S6.SS3.p1.2.3" class="ltx_text ltx_font_typewriter">FT</span> on dataset <span id="S6.SS3.p1.2.4" class="ltx_text ltx_font_typewriter">20NEWS</span>.
Note that reducing the network traffic not only speeds up the convergence,
but also mitigates the overhead on clients and the monetary cost to FL developers, which is billed by the amount of data transmitted on public cloud platforms, e.g., $0.01/GB on AWS <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib5" title="" class="ltx_ref">awsbill2022, </a>)</cite>.</p>
</div>
<figure id="S6.F13" class="ltx_figure"><img src="/html/2205.10162/assets/x27.png" id="S6.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="76" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>. </span><span id="S6.F13.4.2" class="ltx_text" style="font-size:90%;">Per-client average energy consumption, normalized to that of <span id="S6.F13.4.2.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>.
Training targets 99% relative target accuracy.
</span></figcaption>
</figure>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.5" class="ltx_p"><span id="S6.SS3.p2.5.2" class="ltx_text ltx_font_bold">Energy consumption.</span>
Figure <a href="#S6.F13" title="Figure 13 ‣ 6.3. Client Resource Cost ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> illustrates the average energy consumed during FedNLP tasks on each device.
It shows that <span id="S6.SS3.p2.5.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> saves the energy consumption remarkably, e.g., <math id="S6.SS3.p2.1.m1.1" class="ltx_math_unparsed" alttext="1.3\times" display="inline"><semantics id="S6.SS3.p2.1.m1.1a"><mrow id="S6.SS3.p2.1.m1.1b"><mn id="S6.SS3.p2.1.m1.1.1">1.3</mn><mo lspace="0.222em" id="S6.SS3.p2.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">1.3\times</annotation></semantics></math>–<math id="S6.SS3.p2.2.m2.1" class="ltx_math_unparsed" alttext="3.7\times" display="inline"><semantics id="S6.SS3.p2.2.m2.1a"><mrow id="S6.SS3.p2.2.m2.1b"><mn id="S6.SS3.p2.2.m2.1.1">3.7</mn><mo lspace="0.222em" id="S6.SS3.p2.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.2.m2.1c">3.7\times</annotation></semantics></math> reduction compared to <span id="S6.SS3.p2.3.1" class="ltx_text ltx_font_typewriter">LFQ<sub id="S6.SS3.p2.3.1.1" class="ltx_sub"><span id="S6.SS3.p2.3.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">Oracle</span></sub></span> and <math id="S6.SS3.p2.4.m3.1" class="ltx_math_unparsed" alttext="3.1\times" display="inline"><semantics id="S6.SS3.p2.4.m3.1a"><mrow id="S6.SS3.p2.4.m3.1b"><mn id="S6.SS3.p2.4.m3.1.1">3.1</mn><mo lspace="0.222em" id="S6.SS3.p2.4.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.4.m3.1c">3.1\times</annotation></semantics></math>–<math id="S6.SS3.p2.5.m4.1" class="ltx_math_unparsed" alttext="32.1\times" display="inline"><semantics id="S6.SS3.p2.5.m4.1a"><mrow id="S6.SS3.p2.5.m4.1b"><mn id="S6.SS3.p2.5.m4.1.1">32.1</mn><mo lspace="0.222em" id="S6.SS3.p2.5.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p2.5.m4.1c">32.1\times</annotation></semantics></math> reduction compared to <span id="S6.SS3.p2.5.4" class="ltx_text ltx_font_typewriter">FT</span>, respectively.
Such improvement comes from both the reduced network transmission time and the on-device training computations.</p>
</div>
<figure id="S6.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F14.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:104.1pt;"><img src="/html/2205.10162/assets/x28.png" id="S6.F14.1.g1" class="ltx_graphics ltx_img_landscape" width="761" height="298" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F14.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F14.1.2.2" class="ltx_text" style="font-size:90%;">DistilBERT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F14.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:104.1pt;"><img src="/html/2205.10162/assets/x29.png" id="S6.F14.2.g1" class="ltx_graphics ltx_img_landscape" width="382" height="246" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F14.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F14.2.2.2" class="ltx_text" style="font-size:90%;">BERT</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F14.4.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>. </span><span id="S6.F14.5.2" class="ltx_text" style="font-size:90%;">Peak memory usage of a client device. </span></figcaption>
</figure>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p"><span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_bold">Memory footprint</span>
Figure <a href="#S6.F14" title="Figure 14 ‣ 6.3. Client Resource Cost ‣ 6. Evaluation ‣ FedAdapter: Efficient Federated Learning for Modern NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> reports the peak memory footprint when fine-tuning on BERT and DistilBERT with different tuning depths.
It shows that <span id="S6.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> nontrivially reduces the memory usage either with shallow or deep tuning depth.
The reasons are twofold.
First, <span id="S6.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> only updates the parameters of a few adapters, so the gradients of other parameters are not calculated and the associated activations do not need to be stored.
Second, our activation caching technique avoids storing the unneeded parameters.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Related Work</h2>

<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning (transfer learning)</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">Inductive transfer learning has greatly advanced NLP research.
Howard et al. propose ULMFiT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib35" title="" class="ltx_ref">howard2018universal, </a>)</cite>, a universal transfer learning method matching the performance of training from scratch.
BERT <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>)</cite> was then introduced and becomes a standard pre-trained model in many NLP downstream tasks for its superior performance and generality.
Numerous variants <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib78" title="" class="ltx_ref">vaswani2017attention, </a>; <a href="#bib.bib20" title="" class="ltx_ref">devlin2018bert, </a>; <a href="#bib.bib67" title="" class="ltx_ref">sanh2019distilbert, </a>; <a href="#bib.bib33" title="" class="ltx_ref">hou2020dynabert, </a>; <a href="#bib.bib52" title="" class="ltx_ref">liu2020fastbert, </a>; <a href="#bib.bib74" title="" class="ltx_ref">sun2020mobilebert, </a>; <a href="#bib.bib91" title="" class="ltx_ref">zafrir2019q8bert, </a>; <a href="#bib.bib9" title="" class="ltx_ref">bai2020binarybert, </a>)</cite> of BERT have since been designed.
For instance, Sun et al. explore the space of strategies for fine-tuning BERT for text classification <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib73" title="" class="ltx_ref">sun2019fine, </a>)</cite>.
This work is motivated by those work and specifically targets FedNLP scenario.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedNLP</h4>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">is a key step towards the adoption of NLP models in practice.
However, there is very few literature investigating its implications at system aspect.
<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">lin2021fednlp, </a>)</cite> is the first research benchmark for FedNLP tasks and integrates representative language datasets.
<span id="S7.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> is built atop it and treats it as a baseline.
SEFL <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib80" title="" class="ltx_ref">deng2022secure, </a>)</cite> is a FedNLP framework that achieves data privacy without any trusted entities.
<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">basu2021benchmarking, </a>)</cite> studies how FedNLP can orchestrate with differential privacy.
None of above work addresses the high training cost of FedNLP.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adapters</h4>

<div id="S7.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px3.p1.1" class="ltx_p">Adapter is extensively studied to achieve parameter efficiency in continuous learning tasks.
It was first introduced for vision tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib62" title="" class="ltx_ref">rebuffi2017learning, </a>)</cite>.
The rationale is to encode task-specific representations in intermediate layers while preserving the knowledge learned from the pre-training dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib57" title="" class="ltx_ref">pfeiffer2021adapterfusion, </a>)</cite>.
Various adapter variants have been proposed to tradeoff trainable parameter numbers and training accuracy in NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib59" title="" class="ltx_ref">pfeiffer2020mad, </a>; <a href="#bib.bib50" title="" class="ltx_ref">li2021prefix, </a>; <a href="#bib.bib39" title="" class="ltx_ref">mahabadi2021compacter, </a>; <a href="#bib.bib29" title="" class="ltx_ref">he2021towards, </a>; <a href="#bib.bib75" title="" class="ltx_ref">sung2021training, </a>)</cite>.
Despite the popularity, the implications of adapter in FedNLP tasks have not been well examined.
For the first time, we treat adapter as a building block to address the training performance issue in FedNLP.
</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Optimizations for FL</h4>

<div id="S7.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px4.p1.1" class="ltx_p">Due to the decentralized nature, communication has been recognized as a major bottleneck in FL tasks <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">bonawitz2019towards, </a>; <a href="#bib.bib90" title="" class="ltx_ref">yang2021characterizing, </a>)</cite>.
Various optimizations <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib81" title="" class="ltx_ref">wang2020intermittent, </a>; <a href="#bib.bib48" title="" class="ltx_ref">li2021sample, </a>; <a href="#bib.bib86" title="" class="ltx_ref">wu2018error, </a>; <a href="#bib.bib12" title="" class="ltx_ref">bernstein2018signsgd, </a>; <a href="#bib.bib83" title="" class="ltx_ref">wangni2018gradient, </a>)</cite> have been proposed.
Among them, model compression/quantization <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib86" title="" class="ltx_ref">wu2018error, </a>; <a href="#bib.bib12" title="" class="ltx_ref">bernstein2018signsgd, </a>)</cite> is the mostly adopted and is directly compared in this work.
Apart from network transmission, data and device heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib63" title="" class="ltx_ref">reddi2020adaptive, </a>)</cite> are also unique challenges introduced in FL.
To mitigate the heterogeneity of client devices (therefore stragglers), Abdelmoniem et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib7" title="" class="ltx_ref">abdelmoniem2021towards, </a>)</cite> ask each client device to quantize their local model adaptively.
Hermes <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>)</cite> guides different mobile clients to find a small subnetwork through structured pruning for local training.
Most existing vision-based FL optimizations are CNN-specific <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib28" title="" class="ltx_ref">he2020group, </a>)</cite> and thus cannot apply to FedNLP.
Some of them are compatible with <span id="S7.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span>, e.g., intelligent client selection and data sampling <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib47" title="" class="ltx_ref">li2021hermes, </a>; <a href="#bib.bib49" title="" class="ltx_ref">lipyramidfl, </a>; <a href="#bib.bib55" title="" class="ltx_ref">nishio2019client, </a>; <a href="#bib.bib87" title="" class="ltx_ref">xu2020client, </a>; <a href="#bib.bib82" title="" class="ltx_ref">wang2021device, </a>; <a href="#bib.bib44" title="" class="ltx_ref">lai2020oort, </a>; <a href="#bib.bib94" title="" class="ltx_ref">zhao2021quality, </a>; <a href="#bib.bib48" title="" class="ltx_ref">li2021sample, </a>)</cite>.
<span id="S7.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> instead takes the first fundamental step towards practical FedNLP, and is compatible with above techniques.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p"><span id="S8.p1.1.1" class="ltx_text ltx_font_typewriter">FedAdapter</span> is a federated learning framework for fast NLP model fine-tuning.
<span id="S8.p1.1.2" class="ltx_text ltx_font_typewriter">FedAdapter</span> borrows the wisdom from prior work and uses adapter as the only trainable module in NLP model to reduce the training cost.
To identify the optimal adapter configuration on the fly, <span id="S8.p1.1.3" class="ltx_text ltx_font_typewriter">FedAdapter</span> integrates a progressive training paradigm and trail-and-error profiling technique.
<span id="S8.p1.1.4" class="ltx_text ltx_font_typewriter">FedAdapter</span> shows superior training speedup over existing approaches through our extensive experiments.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was supported by National Key Research and Development Program of China #2020YFB1805500, the Fundamental Research Funds for the Central Universities, and NSFC #62032003, #61922017, #61921003.
Mengwei Xu was partly supported by NSFC #62102045, Beijing Nova Program #Z211100002121118, and Young Elite Scientists Sponsorship Program by CAST #2021QNRC001.
The authors thank the anonymous reviewers and the shepherd for their insightful feedbacks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://developer.nvidia.com/embedded/jetson-tx2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/embedded/jetson-tx2</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/embedded/jetson-nano-developer-kit</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.raspberrypi.com/products/raspberry-pi-4-model-b/</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
The state of wifi vs mobile network experience as 5g arrives.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.opensignal.com/sites/opensignal-com/files/data/reports/global/data-2018-11/state_of_wifi_vs_mobile_opensignal_201811.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.opensignal.com/sites/opensignal-com/files/data/reports/global/data-2018-11/state_of_wifi_vs_mobile_opensignal_201811.pdf</a>,
2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Amazon ec2 on-demand pricing.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aws.amazon.com/ec2/pricing/on-demand/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/ec2/pricing/on-demand/</a>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît
Sagot.

</span>
<span class="ltx_bibblock">Ungoliant: An optimized pipeline for the generation of a very
large-scale multilingual web corpus.

</span>
<span class="ltx_bibblock">Proceedings of the Workshop on Challenges in the Management of Large
Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pages 1 – 9,
Mannheim, 2021. Leibniz-Institut für Deutsche Sprache.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ahmed M Abdelmoniem and Marco Canini.

</span>
<span class="ltx_bibblock">Towards mitigating device heterogeneity in federated learning via
adaptive model quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 1st Workshop on Machine Learning and
Systems</span>, pages 96–103, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Gizem Aras, Didem Makaroğlu, Seniz Demir, and Altan Cakir.

</span>
<span class="ltx_bibblock">An evaluation of recent neural sequence tagging models in turkish
named entity recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Expert Systems With Applications</span>, 182:115049, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu,
Michael Lyu, and Irwin King.

</span>
<span class="ltx_bibblock">Binarybert: Pushing the limit of bert quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pages 4334–4348, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Priyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib Singh,
and Fatemehsadat Mireshghallah.

</span>
<span class="ltx_bibblock">Benchmarking differential privacy and federated learning for bert
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.13973</span>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.

</span>
<span class="ltx_bibblock">Curriculum learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th annual international conference on
machine learning</span>, pages 41–48, 2009.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
Anandkumar.

</span>
<span class="ltx_bibblock">signsgd: Compressed optimisation for non-convex problems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
560–569. PMLR, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
Mazzocchi, Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of machine learning and systems</span>, 1:374–388, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Training deep nets with sublinear memory cost.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.06174</span>, 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Xiaoyuan Cheng, Yukun Hu, and Liz Varga.

</span>
<span class="ltx_bibblock">5g network deployment and the associated energy consumption in the
uk: A complex systems’ exploration.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Technological Forecasting and Social Change</span>, 180:121672, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Cody Coleman, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian
Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Analysis of dawnbench, a time-to-accuracy machine learning
performance benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ACM SIGOPS Operating Systems Review</span>, 53(1):14–25, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez,
and Ion Stoica.

</span>
<span class="ltx_bibblock">Clipper: A low-latency online prediction serving system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">14th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 17)</span>, pages 613–627, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales
Leonardis, Greg Slabaugh, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">A continual learning survey: Defying forgetting in classification
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.

</span>
<span class="ltx_bibblock">Neural architecture search: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 20(1):1997–2017,
2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent.

</span>
<span class="ltx_bibblock">Why does unsupervised pre-training help deep learning?

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the thirteenth international conference on
artificial intelligence and statistics</span>, pages 201–208. JMLR Workshop and
Conference Proceedings, 2010.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
Rogerio Feris.

</span>
<span class="ltx_bibblock">Spottune: transfer learning through adaptive fine-tuning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 4805–4814, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Bo Han, Feng Qian, Lusheng Ji, and Vijay Gopalakrishnan.

</span>
<span class="ltx_bibblock">Mp-dash: Adaptive video streaming over preference-aware multipath.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 12th International on Conference on
emerging Networking EXperiments and Technologies</span>, pages 129–143, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,
Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al.

</span>
<span class="ltx_bibblock">A survey on vision transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.

</span>
<span class="ltx_bibblock">Transformer in transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Seungyeop Han, Haichen Shen, Matthai Philipose, Sharad Agarwal, Alec Wolman,
and Arvind Krishnamurthy.

</span>
<span class="ltx_bibblock">Mcdnn: An approximation-based execution framework for deep stream
processing under resource constraints.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the 14th Annual International Conference on
Mobile Systems, Applications, and Services</span>, pages 123–136, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Federated learning of large cnns at the
edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
33:14068–14080, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
Neubig.

</span>
<span class="ltx_bibblock">Towards a unified view of parameter-efficient transfer learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O
Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz.

</span>
<span class="ltx_bibblock">Semeval-2010 task 8: Multi-way classification of semantic relations
between pairs of nominals.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.10422</span>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B
Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing.

</span>
<span class="ltx_bibblock">More effective distributed ml via a stale synchronous parallel
parameter server.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 26, 2013.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.

</span>
<span class="ltx_bibblock">Dynabert: Dynamic bert with adaptive width and depth.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
33:9782–9793, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
2790–2799. PMLR, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jeremy Howard and Sebastian Ruder.

</span>
<span class="ltx_bibblock">Universal language model fine-tuning for text classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 328–339, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700–4708, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Junxian Huang, Feng Qian, Yihua Guo, Yuanyuan Zhou, Qiang Xu, Zhuoqing Mao,
Subhabrata Sen, and Oliver Spatscheck.

</span>
<span class="ltx_bibblock">An in-depth study of lte: effect of network protocol and application
behavior on performance.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM SIGCOMM 2013 Conference</span>, volume 43,
pages 363–374, 08 2013.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder.

</span>
<span class="ltx_bibblock">Compacter: Efficient low-rank hypercomplex adapter layers.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
34:1022–1035, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere,
and Mikhail Smelyanskiy.

</span>
<span class="ltx_bibblock">On large-batch training for deep learning: Generalization gap and
sharp minima.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">5th International Conference on Learning Representations,
ICLR 2017</span>, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Mayara Khadhraoui, Hatem Bellaaj, Mehdi Ben Ammar, Habib Hamam, and Mohamed
Jmaiel.

</span>
<span class="ltx_bibblock">Survey of bert-base models for scientific text classification:
Covid-19 case study.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 12(6):2891, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.

</span>
<span class="ltx_bibblock">Code prediction by feeding trees to transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">2021 IEEE/ACM 43rd International Conference on Software
Engineering (ICSE)</span>, pages 150–162. IEEE, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Fan Lai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Oort: Efficient federated learning via guided participant selection.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ken Lang.

</span>
<span class="ltx_bibblock">Newsweeder: Learning to filter netnews.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Machine Learning Proceedings 1995</span>, pages 331–339. Elsevier,
1995.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Younghoo Lee, Joshua Saxe, and Richard Harang.

</span>
<span class="ltx_bibblock">Catbert: Context-aware tiny bert for detecting social engineering
emails.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.03484</span>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, and Yiran Chen.

</span>
<span class="ltx_bibblock">Hermes: an efficient federated learning framework for heterogeneous
mobile clients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th Annual International Conference on
Mobile Computing and Networking</span>, pages 420–437, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Anran Li, Lan Zhang, Juntao Tan, Yaxuan Qin, Junhao Wang, and Xiang-Yang Li.

</span>
<span class="ltx_bibblock">Sample-level data selection for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2021-IEEE Conference on Computer
Communications</span>, pages 1–10. IEEE, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Chenning Li, Xiao Zeng, Mi Zhang, and Zhichao Cao.

</span>
<span class="ltx_bibblock">Pyramidfl: A fine-grained client selection framework for efficient
federated learning.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pages 4582–4597, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin Wang, Yufen Hua, Christophe
Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fednlp: Benchmarking federated learning methods for natural language
processing tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics:
NAACL 2022</span>, pages 157–175, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju.

</span>
<span class="ltx_bibblock">Fastbert: a self-distilling bert with adaptive inference time.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 6035–6044, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.

</span>
<span class="ltx_bibblock">Deep learning face attributes in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 3730–3738, 2015.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Takayuki Nishio and Ryo Yonetani.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources
in mobile edge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">ICC 2019-2019 IEEE international conference on communications
(ICC)</span>, pages 1–7. IEEE, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang,
and Xuehai Qian.

</span>
<span class="ltx_bibblock">Capuchin: Tensor-based gpu memory management for deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Proceedings of the Twenty-Fifth International Conference on
Architectural Support for Programming Languages and Operating Systems</span>, pages
891–905, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and
Iryna Gurevych.

</span>
<span class="ltx_bibblock">Adapterfusion: Non-destructive task composition for transfer
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">EACL</span>, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan
Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Adapterhub: A framework for adapting transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</span>, pages 46–54, 2020.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder.

</span>
<span class="ltx_bibblock">Mad-x: An adapter-based framework for multi-task cross-lingual
transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pages 7654–7673, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders
Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong.

</span>
<span class="ltx_bibblock">Towards robust linguistic analysis using ontonotes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the Seventeenth Conference on Computational
Natural Language Learning</span>, pages 143–152, 2013.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Hang Qi, Evan R Sparks, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Paleo: A performance model for deep neural networks.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Learning multiple visual domains with residual adapters.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.00295</span>, 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh, Isidoros Tziotis, Hamed Hassani, Aryan Mokhtari, and
Ramtin Pedarsani.

</span>
<span class="ltx_bibblock">Straggler-resilient federated learning: Leveraging the interplay
between statistical accuracy and system heterogeneity.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">IEEE Journal on Selected Areas in Information Theory</span>,
3(2):197–205, 2022.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Youngmin Ro and Jin Young Choi.

</span>
<span class="ltx_bibblock">Autolr: Layer-wise pruning and auto-tuning of learning rates in
fine-tuning of deep networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume 35, pages 2486–2494, 2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
Pfeiffer, Nils Reimers, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Adapterdrop: On the efficiency of adapters in transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</span>, pages 7930–7946, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.01108</span>, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Taihua Shao, Yupu Guo, Honghui Chen, and Zepeng Hao.

</span>
<span class="ltx_bibblock">Transformer-based neural network for answer selection in question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 7:26146–26156, 2019.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai
Philipose, Arvind Krishnamurthy, and Ravi Sundaram.

</span>
<span class="ltx_bibblock">Nexus: A gpu cluster engine for accelerating dnn-based video
analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th ACM Symposium on Operating Systems
Principles</span>, pages 322–337, 2019.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Jaemin Shin, Yuanchun Li, Yunxin Liu, and Sung-Ju Lee.

</span>
<span class="ltx_bibblock">Fedbalancer: Data and pace control for efficient federated learning
on heterogeneous clients.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Ivonne Soldevilla and Nahum Flores.

</span>
<span class="ltx_bibblock">Natural language processing through bert for identifying gender-based
violence messages on social media.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Conference on Information
Communication and Software Engineering (ICICSE)</span>, pages 204–208. IEEE, 2021.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Joel Stremmel and Arjun Singh.

</span>
<span class="ltx_bibblock">Pretraining federated text models for next word prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">Future of Information and Communication Conference</span>, pages
477–488. Springer, 2021.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.

</span>
<span class="ltx_bibblock">How to fine-tune bert for text classification?

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">China national conference on Chinese computational
linguistics</span>, pages 194–206. Springer, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.

</span>
<span class="ltx_bibblock">Mobilebert: a compact task-agnostic bert for resource-limited
devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 2158–2170, 2020.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Yi-Lin Sung, Varun Nair, and Colin A Raffel.

</span>
<span class="ltx_bibblock">Training neural networks with fixed sparse masks.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan.

</span>
<span class="ltx_bibblock">Intellicode compose: Code generation using transformer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering</span>, pages 1433–1443, 2020.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Betty Van Aken, Benjamin Winter, Alexander Löser, and Felix A Gers.

</span>
<span class="ltx_bibblock">How does bert answer questions? a layer-wise analysis of transformer
representations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM International Conference on
Information and Knowledge Management</span>, pages 1823–1832, 2019.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Paul Voigt and Axel Von dem Bussche.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</span>, 10(3152676):10–5555, 2017.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Chenghong Wang, Jieren Deng, Xianrui Meng, Yijue Wang, Ji Li, Sheng Lin, Shuo
Han, Fei Miao, Sanguthevar Rajasekaran, and Caiwen Ding.

</span>
<span class="ltx_bibblock">A secure and efficient federated learning framework for nlp.

</span>
<span class="ltx_bibblock">In <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</span>, pages 7676–7682, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Haozhao Wang, Zhihao Qu, Song Guo, Xin Gao, Ruixuan Li, and Baoliu Ye.

</span>
<span class="ltx_bibblock">Intermittent pulling with local compensation for
communication-efficient distributed learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Emerging Topics in Computing</span>, 2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang,
and Christopher G Brinton.

</span>
<span class="ltx_bibblock">Device sampling for heterogeneous federated learning: Theory,
algorithms, and implementation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2021-IEEE Conference on Computer
Communications</span>, pages 1–10. IEEE, 2021.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang.

</span>
<span class="ltx_bibblock">Gradient sparsification for communication-efficient distributed
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 31, 2018.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, and Jimin Liang.

</span>
<span class="ltx_bibblock">Npenas: Neural predictor guided evolution for neural architecture
search.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2022.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 conference on empirical methods in
natural language processing: system demonstrations</span>, pages 38–45, 2020.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.

</span>
<span class="ltx_bibblock">Error compensated quantized sgd and its applications to large-scale
distributed optimization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5325–5333. PMLR, 2018.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Jie Xu and Heqiang Wang.

</span>
<span class="ltx_bibblock">Client selection and bandwidth allocation in wireless federated
learning networks: A long-term perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>, 20(2):1188–1200,
2020.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Mengwei Xu, Zhe Fu, Xiao Ma, Li Zhang, Yanan Li, Feng Qian, Shangguang Wang,
Ke Li, Jingyu Yang, and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">From cloud to edge: a first look at public edge platforms.

</span>
<span class="ltx_bibblock">In <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Proceedings of the 21st ACM Internet Measurement Conference</span>,
pages 37–53, 2021.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">Deeptype: On-device deep learning for input personalization service
with minimal privacy concern.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies</span>, 2(4):1–26, 2018.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu,
and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">Characterizing impacts of heterogeneity in federated learning upon
large-scale smartphone data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Proceedings of the Web Conference 2021</span>, pages 935–946,
2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.

</span>
<span class="ltx_bibblock">Q8bert: Quantized 8bit bert.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">2019 Fifth Workshop on Energy Efficient Machine Learning and
Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</span>, pages 36–39. IEEE, 2019.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Lei Zhang, Shuai Wang, and Bing Liu.

</span>
<span class="ltx_bibblock">Deep learning for sentiment analysis: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</span>, 8(4):e1253, 2018.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Zhao, and Yann LeCun.

</span>
<span class="ltx_bibblock">Character-level convolutional networks for text classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Yuxi Zhao and Xiaowen Gong.

</span>
<span class="ltx_bibblock">Quality-aware distributed computation and user selection for
cost-effective federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2021-IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS)</span>, pages 1–6. IEEE, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.10161" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.10162" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.10162">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.10162" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.10163" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 13:45:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
