<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion</title>
<!--Generated on Tue Sep 17 17:28:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.11406v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S1" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S2" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>RELATED WORKS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS1" title="In 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Reference-Augmented Multi-View Diffusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS2" title="In 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Meta-ControlNet.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS3" title="In 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Dynamic Reference Routing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS4" title="In 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Self-Reference Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS5" title="In 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Sparse-View 3D Reconstruction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.SS1" title="In 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Comparisons with State-of-the-Art Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.SS2" title="In 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Ablation Study and Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S6" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS1" title="In Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS2" title="In Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS3" title="In Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Meta-controlnet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS4" title="In Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Augmentation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A2" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Limitation and Failure Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3" title="In Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.SS1" title="In Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Additional Analysis on Enhanced Generalization Ability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.SS2" title="In Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>More Results</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhenwei Wang<sup class="ltx_sup" id="id13.13.id1">1</sup><span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Intern at Shanghai AI Lab. <sup class="ltx_sup" id="footnotex1.1">∗</sup>Equal Contribution.</span></span></span> <sup class="ltx_sup" id="id14.14.id2">∗</sup>
 Tengfei Wang<sup class="ltx_sup" id="id15.15.id3">2</sup><span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span> <sup class="ltx_sup" id="id16.16.id4">∗</sup>
 Zexin He<sup class="ltx_sup" id="id17.17.id5"><span class="ltx_text ltx_font_italic" id="id17.17.id5.1">3</span></sup><span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Intern at Shanghai AI Lab. <sup class="ltx_sup" id="footnotex3.1">∗</sup>Equal Contribution.</span></span></span>
  Gerhard Hancke<sup class="ltx_sup" id="id18.18.id6"><span class="ltx_text ltx_font_italic" id="id18.18.id6.1">1</span></sup>
 Ziwei Liu<sup class="ltx_sup" id="id19.19.id7"><span class="ltx_text ltx_font_italic" id="id19.19.id7.1">4</span></sup>
 Rynson W.H. Lau<sup class="ltx_sup" id="id20.20.id8"><span class="ltx_text ltx_font_italic" id="id20.20.id8.1">1</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">3</span></span></span></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id21.21.id9"><span class="ltx_text ltx_font_italic" id="id21.21.id9.1">1</span></sup>City University of Hong Kong
 <sup class="ltx_sup" id="id22.22.id10"><span class="ltx_text ltx_font_italic" id="id22.22.id10.1">2</span></sup>Shanghai AI Lab
 <sup class="ltx_sup" id="id23.23.id11"><span class="ltx_text ltx_font_italic" id="id23.23.id11.1">3</span></sup>CUHK
 <sup class="ltx_sup" id="id24.24.id12"><span class="ltx_text ltx_font_italic" id="id24.24.id12.1">4</span></sup>S-Lab, NTU
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id25.id1">In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of <em class="ltx_emph ltx_font_italic" id="id25.id1.1">Phidias</em>, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in significant generative improvements over existing methods. <em class="ltx_emph ltx_font_italic" id="id25.id1.2">Phidias</em> establishes a unified framework for 3D generation using text, image, and 3D conditions, offering versatile applications. Demo videos are at:
<a class="ltx_ref ltx_href ltx_font_typewriter" href="https://RAG-3D.github.io/" style="color:#FF0000;" title="">https://RAG-3D.github.io/</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="S0.F1.g1" src="extracted/5861237/Figures/teaser.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed model, <em class="ltx_emph ltx_font_italic" id="S0.F1.2.1">Phidias</em>, can produce high-quality 3D assets given 3D references, which can be obtained via retrieval (top two rows) or specified by users (bottom row). It supports 3D generation from a single image, a text prompt, or an existing 3D model.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The goal of 3D generative models is to empower artists and even beginners to effortlessly convert their design concepts into 3D models.
Consider the input image in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S0.F1" title="Figure 1 ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">1</span></a>. A skilled craftsman can, through a blend of skills and creativity, convert a 2D concept image into an exquisite 3D model. This creative process can originate from artists’ pure imagination or, more commonly, through examining one or more existing 3D models as a source of inspiration <cite class="ltx_cite ltx_citemacro_citep">(Bob, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib4" title="">2022</a>; Carvajal, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib5" title="">2023</a>)</cite>. Artists often refer to these pre-existing 3D models to improve the modeling quality. The question then arises: could we develop a reference-based 3D generative model that can replicate this capability?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Over the years, a plethora of works <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib39" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib23" title="">2023b</a>; Hong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib17" title="">2023</a>; Bensadoun et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib2" title="">2024</a>)</cite> steadily expanded the frontiers of 3D generative models.
These methods, while yielding stunning performance, still face several challenges. <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">1) Generation quality.</span>
A single image cannot furnish sufficient information for reconstructing a full 3D model, due to the ambiguity of this ill-posed task. This necessitates the generative model to “hallucinate” the unseen parts in a data-driven manner. However, this hallucination can lead to view inconsistency and imprecise geometries that appear abrupt and unrealistic. <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">2) Generalization ability.</span>
These models often struggle with out-of-domain cases, such as atypical input views or objects, constrained by the data coverage of existing 3D datasets <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib11" title="">2023</a>)</cite>.
Also, the growing variety and quantity of object categories exacerbate the difficulty for generative models to learn implicit
shape priors, with a limited model capacity v.s. an infinitely diverse array of objects. <span class="ltx_text ltx_font_bold" id="S1.p2.1.3">3) Controllability.</span> Due to the ambiguity, one input image can produce several plausible 3D models, each differing in shape, geometric style, and local patterns. Existing methods are constrained by limited diversity and controllability, which hinders the ability to predictably generate the desired 3D models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges, we propose to take 3D models as additional inputs to guide the generation, inspired by the success in retrieval augmented generation (RAG) for language <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib21" title="">2020</a>)</cite> and image <cite class="ltx_cite ltx_citemacro_citep">(Sheynin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib30" title="">2022</a>)</cite>.
Given an input image and a reference 3D model, we present <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">Phidias</em>, a novel reference-augmented diffusion model that unifies 3D generation from text, image, and 3D conditions. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S0.F1" title="Figure 1 ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">1</span></a>, the reference 3D model would help 1) <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">improve quality</span> by alleviating ambiguity with richer information for unseen views, 2) <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">enhance generalization capacity</span> by serving as a shape template or an external memory for generative models, and 3) <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">provide controllability</span> by indicating desired shape patterns and geometric styles.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our method proposed a reference-augmented multi-view diffusion model,
followed by sparse-view 3D reconstruction.
The goal is to produce 3D models faithful to the concept image with improved quality by incorporating relevant information from the 3D reference.
However, it is non-trivial to learn such a generative model due to the <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">Misalignment Dilemma</em>, where the discrepancy between the concept image and the 3D reference can lead to conflicts in the generation process. This requires our model to utilize the misaligned 3D reference adaptively.
To tackle this challenge, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">Phidias</em> leverages three key designs outlined below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The first is <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">meta-ControlNet</span>.
Consider 3D reference as conditions for diffusion models. Unlike previous image-to-image translation works <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib47" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib38" title="">2022</a>)</cite> that demand the generated images to closely follow the conditions, we treat reference model as auxiliary guidance to provide additional information. The generated multi-view images are expected to be consistent with the concept image, without requiring precise alignment with the reference model.
To this end, we build our method on ControlNet and propose a meta-control network that dynamically modulates conditioning strength when it conflicts with the concept image, based on their similarity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The second design is <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">dynamic reference routing</span> for further alleviating the misalignment.
Rather than using the same 3D reference for the full diffusion process, we adjust its resolution across denoise timesteps. This follows the dynamics of the reverse diffusion process <cite class="ltx_cite ltx_citemacro_citep">(Balaji et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib1" title="">2022</a>)</cite>, which generates coarse structure in high-noised timesteps and details in low-noised timesteps.
Thus, we can alleviate the generation conflicts by starting with a coarse 3D reference and progressively increasing its resolution as the reverse diffusion process goes on.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The final key design is <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">self-reference augmentations</span>. It is not feasible to gather large sets of 3D models and their matching references. A practical solution is to use the 3D model itself as its own reference (<span class="ltx_text ltx_font_italic" id="S1.p7.1.2">i.e., </span>self-reference) for self-supervised learning. The trained model, however, does not work well when the 3D reference does not align with the target image.
To avoid overfitting to a trivial solution, we apply a variety of augmentations to 3D models that simulate this misalignment. Furthermore, we introduce a progressive augmentation approach that leverages curriculum learning for diffusion models to effectively utilize references that vary in similarity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Taken together, the above ingredients work in concert to
enable <em class="ltx_emph ltx_font_italic" id="S1.p8.1.1">Phidias</em> to achieve stunning performance in 3D generation. Several application scenarios are thus supported: 1) Retrieval-augmented image-to-3D generation, 2) Retrieval-augmented text-to-3D generation, 3) Theme-aware 3D-to-3D generation, 4) Interactive 3D generation with coarse guidance, and 5) High-fidelity 3D completion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">We summarize our contributions as follows: 1) We propose the first reference-based 3D-aware diffusion model. 2) We design our model with three key component designs to enhance the performance. 3) Our model serves as a unified framework for 3D generation, which provides a variety of applications with text, image, and 3D inputs. 4) Extensive experiments show our method outperforms existing approaches qualitatively and quantitatively.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RELATED WORKS</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Image to 3D.</span>
Pioneering works <cite class="ltx_cite ltx_citemacro_citep">(Melas-Kyriazi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib25" title="">2023</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib35" title="">2023</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib8" title="">2024b</a>)</cite> perform 3D synthesis by distilling image diffusion priors <cite class="ltx_cite ltx_citemacro_citep">(Poole et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib27" title="">2023</a>)</cite>, but are time-consuming. Recent advancements have leveraged feed-forward models with 3D datasets. Some works use diffusion models to generate points <cite class="ltx_cite ltx_citemacro_citep">(Nichol et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib26" title="">2022</a>)</cite>, neural radiance fields <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib39" title="">2023</a>; Jun &amp; Nichol, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib19" title="">2023</a>; Gupta et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib13" title="">2023</a>; Hong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib16" title="">2024</a>)</cite>, SDF <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib10" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib46" title="">2024b</a>)</cite>, and gaussian splatting <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib45" title="">2024a</a>)</cite>. Another line of works uses transformers for auto-regressive generation <cite class="ltx_cite ltx_citemacro_citep">(Siddiqui et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib33" title="">2023</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib7" title="">2024a</a>)</cite> or sparse-view reconstruction <cite class="ltx_cite ltx_citemacro_citep">(Hong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib17" title="">2023</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib34" title="">2024</a>; Zou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib49" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib40" title="">2024a</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib43" title="">2024</a>)</cite>, which often rely on multi-view diffusion for better performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Multi-View Diffusion Models.</span>
Multi-view models reduce the complexities of 3D synthesis to consistent 2D synthesis. Seminal works <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib23" title="">2023b</a>)</cite> have shown novel view synthesis capabilities with pre-trained image diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib28" title="">2022</a>)</cite>. Later, a plethora of works explored multi-view diffusion models with better consistency <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib31" title="">2023a</a>; Wang &amp; Shi, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib37" title="">2023</a>; Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib32" title="">2023b</a>; Long et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib24" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib22" title="">2023a</a>)</cite> by introducing cross-view communication. More recent works <cite class="ltx_cite ltx_citemacro_citep">(Voleti et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib36" title="">2024</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib9" title="">2024c</a>; You et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib44" title="">2024</a>; Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib14" title="">2024</a>)</cite> leverage video priors for multi-view generation by injecting cameras into video diffusion models. However, they still struggle with generalized and controllable generation due to the ill-posed nature of this problem.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Reference-Augmented Generation.</span>
Retrieval-augmented generation (RAG) emerges to enhance the generation of both language <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib21" title="">2020</a>)</cite> and image <cite class="ltx_cite ltx_citemacro_citep">(Sheynin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib30" title="">2022</a>; Blattmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib3" title="">2022</a>)</cite> by incorporating relevant external information during the generation process. Under the context of 3D generation, the concept of reference-based generation is also widely applied. Some works <cite class="ltx_cite ltx_citemacro_citep">(Chaudhuri et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib6" title="">2011</a>; Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib20" title="">2013</a>; Schor et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib29" title="">2019</a>)</cite> probe into the database for compatible parts and assemble them into 3D shapes. Some works refer to a 3D exemplar model <cite class="ltx_cite ltx_citemacro_citep">(Wu &amp; Zheng, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib42" title="">2022</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib41" title="">2024b</a>)</cite> to produce customized 3D assets. Despite success in specific contexts, they are time-consuming with per-case optimization. In contrast, our method focuses on learning a generalized feed-forward model that applies to reference-augmented 3D generation.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S2.F2.g1" src="extracted/5861237/Figures/overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the <span class="ltx_text ltx_font_italic" id="S2.F2.2.1">Phidias</span> model. It generates a 3D model in two stages: (1) reference-augmented multi-view generation and (2) sparse-view 3D reconstruction. </figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach
</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Given one <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">concept image</em>, we aim at leveraging an additional <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">3D reference model</em> to alleviate 3D inconsistency issues and geometric ambiguity that exist in 3D generation.
The 3D reference model can be either provided by the user or retrieved from a large 3D database for different applications.
The overall pipeline of <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">Phidias</span> is shown in <span class="ltx_text" id="S3.p1.1.4" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S2.F2" title="Figure 2 ‣ 2 RELATED WORKS ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">2</span></a></span>, which involves two stages: reference-augmented multi-view generation and sparse-view 3D reconstruction.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Reference-Augmented Multi-View Diffusion</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Multi-view diffusion models incorporate camera conditions into well-trained image diffusion models for novel-view synthesis with supervised fine-tuning.
We aim to weave additional 3D references into these multi-view models for better generation quality, generalization ability, and controllability. Our approach can be built on arbitrary multi-view diffusion models, enabling reference-augmented 3D content creation from text, image, and 3D conditions. Specifically, we initialize our model with Zero123++ <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib31" title="">2023a</a>)</cite>, which simply tiles multi-view images for efficient generation conditioned on one input image <math alttext="\bm{c}_{\mathrm{image}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">image</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝒄</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">image</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\bm{c}_{\mathrm{image}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">bold_italic_c start_POSTSUBSCRIPT roman_image end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">To integrate 3D reference models <math alttext="\bm{c}_{\mathrm{ref}}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">𝒄</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">ref</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝒄</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ref</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\bm{c}_{\mathrm{ref}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">bold_italic_c start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT</annotation></semantics></math> into the diffusion process, we transform them into multi-view canonical coordinate maps (CCM) to condition the diffusion model. The choice of CCMs as the 3D representation is based on two reasons: 1) Multi-view images serve as more efficient and compatible inputs for diffusion models than meshes or voxels, as they have embedded camera viewing angles that correspond with the output images. 2) Reference models often share similar shapes with the concept image but vary significantly in texture details. By focusing on the geometry while omitting the texture, CCMs conditions can reduce generation conflicts arising from texture discrepancies. We add a conditioner branch to incorporate reference CCMs into the base multi-view diffusion model. The objective for training our diffusion model <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">ϵ</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">italic-ϵ</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> can be then formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathbb{E}_{t,\epsilon\sim\mathcal{N}(0,1)}\left[\|\epsilon-%
\epsilon_{\theta}\left(\bm{x}_{t},t,\bm{c}_{\mathrm{image}},\bm{c}_{\mathrm{%
ref}}\right)\|^{2}\right]" class="ltx_Math" display="block" id="S3.E1.m1.6"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.3" xref="S3.E1.m1.6.6.3.cmml">ℒ</mi><mo id="S3.E1.m1.6.6.2" xref="S3.E1.m1.6.6.2.cmml">=</mo><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.cmml"><msub id="S3.E1.m1.6.6.1.3" xref="S3.E1.m1.6.6.1.3.cmml"><mi id="S3.E1.m1.6.6.1.3.2" xref="S3.E1.m1.6.6.1.3.2.cmml">𝔼</mi><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><mrow id="S3.E1.m1.4.4.4.6.2" xref="S3.E1.m1.4.4.4.6.1.cmml"><mi id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">t</mi><mo id="S3.E1.m1.4.4.4.6.2.1" xref="S3.E1.m1.4.4.4.6.1.cmml">,</mo><mi id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">∼</mo><mrow id="S3.E1.m1.4.4.4.7" xref="S3.E1.m1.4.4.4.7.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.4.7.2" xref="S3.E1.m1.4.4.4.7.2.cmml">𝒩</mi><mo id="S3.E1.m1.4.4.4.7.1" xref="S3.E1.m1.4.4.4.7.1.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.4.7.3.2" xref="S3.E1.m1.4.4.4.7.3.1.cmml"><mo id="S3.E1.m1.4.4.4.7.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.4.7.3.1.cmml">(</mo><mn id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">0</mn><mo id="S3.E1.m1.4.4.4.7.3.2.2" xref="S3.E1.m1.4.4.4.7.3.1.cmml">,</mo><mn id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">1</mn><mo id="S3.E1.m1.4.4.4.7.3.2.3" stretchy="false" xref="S3.E1.m1.4.4.4.7.3.1.cmml">)</mo></mrow></mrow></mrow></msub><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.2.cmml"><mo id="S3.E1.m1.6.6.1.1.1.2" xref="S3.E1.m1.6.6.1.1.2.1.cmml">[</mo><msup id="S3.E1.m1.6.6.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.5" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.5.cmml">ϵ</mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.4.cmml">−</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.2.cmml">ϵ</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.3.cmml">θ</mi></msub><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.4.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml"><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.4" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.5" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">t</mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.6" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.2.cmml">𝒄</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.3.cmml">image</mi></msub><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.7" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.2.cmml">𝒄</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.3.cmml">ref</mi></msub><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.8" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.6.6.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><eq id="S3.E1.m1.6.6.2.cmml" xref="S3.E1.m1.6.6.2"></eq><ci id="S3.E1.m1.6.6.3.cmml" xref="S3.E1.m1.6.6.3">ℒ</ci><apply id="S3.E1.m1.6.6.1.cmml" xref="S3.E1.m1.6.6.1"><times id="S3.E1.m1.6.6.1.2.cmml" xref="S3.E1.m1.6.6.1.2"></times><apply id="S3.E1.m1.6.6.1.3.cmml" xref="S3.E1.m1.6.6.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.3.1.cmml" xref="S3.E1.m1.6.6.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.3.2.cmml" xref="S3.E1.m1.6.6.1.3.2">𝔼</ci><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><csymbol cd="latexml" id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5">similar-to</csymbol><list id="S3.E1.m1.4.4.4.6.1.cmml" xref="S3.E1.m1.4.4.4.6.2"><ci id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">𝑡</ci><ci id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E1.m1.4.4.4.7.cmml" xref="S3.E1.m1.4.4.4.7"><times id="S3.E1.m1.4.4.4.7.1.cmml" xref="S3.E1.m1.4.4.4.7.1"></times><ci id="S3.E1.m1.4.4.4.7.2.cmml" xref="S3.E1.m1.4.4.4.7.2">𝒩</ci><interval closure="open" id="S3.E1.m1.4.4.4.7.3.1.cmml" xref="S3.E1.m1.4.4.4.7.3.2"><cn id="S3.E1.m1.1.1.1.1.cmml" type="integer" xref="S3.E1.m1.1.1.1.1">0</cn><cn id="S3.E1.m1.2.2.2.2.cmml" type="integer" xref="S3.E1.m1.2.2.2.2">1</cn></interval></apply></apply></apply><apply id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1"><minus id="S3.E1.m1.6.6.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.4"></minus><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.5">italic-ϵ</ci><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.4"></times><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.2">italic-ϵ</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.5.3">𝜃</ci></apply><vector id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3"><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑡</ci><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.2">𝒄</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.2.2.2.3">image</ci></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.2">𝒄</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.3.3.3">ref</ci></apply></vector></apply></apply></apply><cn id="S3.E1.m1.6.6.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.6.6.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\mathcal{L}=\mathbb{E}_{t,\epsilon\sim\mathcal{N}(0,1)}\left[\|\epsilon-%
\epsilon_{\theta}\left(\bm{x}_{t},t,\bm{c}_{\mathrm{image}},\bm{c}_{\mathrm{%
ref}}\right)\|^{2}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.6d">caligraphic_L = blackboard_E start_POSTSUBSCRIPT italic_t , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_c start_POSTSUBSCRIPT roman_image end_POSTSUBSCRIPT , bold_italic_c start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.3">To leverage the powerful pertaining capability, only the additional conditioner for reference CCMs is trainable while the base multi-view diffusion is frozen.
However, a challenge in our task is that the 3D reference may not strictly align with the concept image or, more commonly, vary in most local parts. We found naive conditioner designs such as ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib47" title="">2023</a>)</cite> tend to produce undesirable artifacts, as they were originally designed for image-to-image translation where the generated images strictly align with the condition images. To mitigate this problem, we introduce three key designs for our reference-augmented diffusion model: (1) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.3.1">Meta-ControlNet</span> for adaptive control of the conditioning strength (<span class="ltx_text" id="S3.SS1.p2.3.2" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS2" title="3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.2</span></a></span>); (2) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.3.3">Dynamic Reference Routing</span> for dynamic adjustment of the 3D reference (<span class="ltx_text" id="S3.SS1.p2.3.4" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS3" title="3.3 Dynamic Reference Routing ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.3</span></a></span>); (3) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.3.5">Self-Reference Augmentation</span> for self-supervised training (<span class="ltx_text" id="S3.SS1.p2.3.6" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS4" title="3.4 Self-Reference Augmentation ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.4</span></a></span>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Meta-ControlNet.</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S3.F3.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Architectural designs for meta-ControlNet (a) and dynamic reference routing (b).</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">ControlNet is designed to add additional controls to pre-trained diffusion models for image-to-image translation. The conditions are derived from the ground-truth images for self-supervised learning, and thus the generated images are expected to follow the conditions. However, in our settings, the conditions are from the reference model, which often misaligns with the target 3D models we want to generate. The vanilla ControlNet fails to handle such cases.
This necessitates further architecture advancement to accordingly adjust conditioning strength when the reference conflicts with the concept image. To this end, we propose meta-ControlNet, as shown in <span class="ltx_text" id="S3.SS2.p1.1.1" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.F3" title="Figure 3 ‣ 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3</span></a></span> (a). Meta-ControlNet is comprised of two collaborative subnets, a base ControlNet and an additional meta-controller.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">Base ControlNet is comprised of an image encoder, a trainable copy of down-sampling blocks and middle blocks of the base multi-view diffusion, denoted as <math alttext="\mathcal{F}^{{\mathrm{base}}}_{\Theta}\left(\cdot\right)" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><msubsup id="S3.SS2.p2.1.m1.1.2.2" xref="S3.SS2.p2.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.2.2.2.2" xref="S3.SS2.p2.1.m1.1.2.2.2.2.cmml">ℱ</mi><mi id="S3.SS2.p2.1.m1.1.2.2.3" mathvariant="normal" xref="S3.SS2.p2.1.m1.1.2.2.3.cmml">Θ</mi><mi id="S3.SS2.p2.1.m1.1.2.2.2.3" xref="S3.SS2.p2.1.m1.1.2.2.2.3.cmml">base</mi></msubsup><mo id="S3.SS2.p2.1.m1.1.2.1" xref="S3.SS2.p2.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p2.1.m1.1.2.3.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><mo id="S3.SS2.p2.1.m1.1.2.3.2.1" xref="S3.SS2.p2.1.m1.1.2.cmml">(</mo><mo id="S3.SS2.p2.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p2.1.m1.1.1.cmml">⋅</mo><mo id="S3.SS2.p2.1.m1.1.2.3.2.2" xref="S3.SS2.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.2"><times id="S3.SS2.p2.1.m1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.2.1"></times><apply id="S3.SS2.p2.1.m1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.2.2.1.cmml" xref="S3.SS2.p2.1.m1.1.2.2">subscript</csymbol><apply id="S3.SS2.p2.1.m1.1.2.2.2.cmml" xref="S3.SS2.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.2.2.2.1.cmml" xref="S3.SS2.p2.1.m1.1.2.2">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.2.2.2.2.cmml" xref="S3.SS2.p2.1.m1.1.2.2.2.2">ℱ</ci><ci id="S3.SS2.p2.1.m1.1.2.2.2.3.cmml" xref="S3.SS2.p2.1.m1.1.2.2.2.3">base</ci></apply><ci id="S3.SS2.p2.1.m1.1.2.2.3.cmml" xref="S3.SS2.p2.1.m1.1.2.2.3">Θ</ci></apply><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{F}^{{\mathrm{base}}}_{\Theta}\left(\cdot\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">caligraphic_F start_POSTSUPERSCRIPT roman_base end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math>, and a series of <math alttext="1\times 1" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mn id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">1</mn><mo id="S3.SS2.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><cn id="S3.SS2.p2.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p2.2.m2.1.1.2">1</cn><cn id="S3.SS2.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">1\times 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">1 × 1</annotation></semantics></math> zero convolution layers (Zero Convs) <math alttext="\mathcal{Z}^{{\mathrm{base}}}_{\Theta}(\cdot)" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.2" xref="S3.SS2.p2.3.m3.1.2.cmml"><msubsup id="S3.SS2.p2.3.m3.1.2.2" xref="S3.SS2.p2.3.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.3.m3.1.2.2.2.2" xref="S3.SS2.p2.3.m3.1.2.2.2.2.cmml">𝒵</mi><mi id="S3.SS2.p2.3.m3.1.2.2.3" mathvariant="normal" xref="S3.SS2.p2.3.m3.1.2.2.3.cmml">Θ</mi><mi id="S3.SS2.p2.3.m3.1.2.2.2.3" xref="S3.SS2.p2.3.m3.1.2.2.2.3.cmml">base</mi></msubsup><mo id="S3.SS2.p2.3.m3.1.2.1" xref="S3.SS2.p2.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p2.3.m3.1.2.3.2" xref="S3.SS2.p2.3.m3.1.2.cmml"><mo id="S3.SS2.p2.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS2.p2.3.m3.1.2.cmml">(</mo><mo id="S3.SS2.p2.3.m3.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p2.3.m3.1.1.cmml">⋅</mo><mo id="S3.SS2.p2.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS2.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.2.cmml" xref="S3.SS2.p2.3.m3.1.2"><times id="S3.SS2.p2.3.m3.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.2.1"></times><apply id="S3.SS2.p2.3.m3.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.2.2.1.cmml" xref="S3.SS2.p2.3.m3.1.2.2">subscript</csymbol><apply id="S3.SS2.p2.3.m3.1.2.2.2.cmml" xref="S3.SS2.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.2.2.2.1.cmml" xref="S3.SS2.p2.3.m3.1.2.2">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.2.2.2.2.cmml" xref="S3.SS2.p2.3.m3.1.2.2.2.2">𝒵</ci><ci id="S3.SS2.p2.3.m3.1.2.2.2.3.cmml" xref="S3.SS2.p2.3.m3.1.2.2.2.3">base</ci></apply><ci id="S3.SS2.p2.3.m3.1.2.2.3.cmml" xref="S3.SS2.p2.3.m3.1.2.2.3">Θ</ci></apply><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathcal{Z}^{{\mathrm{base}}}_{\Theta}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">caligraphic_Z start_POSTSUPERSCRIPT roman_base end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math>. It takes reference CCM maps <math alttext="\bm{c}_{\mathrm{ref}}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">𝒄</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">ref</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝒄</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ref</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\bm{c}_{\mathrm{ref}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">bold_italic_c start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT</annotation></semantics></math> as input to produce the control signal. To deal with misaligned 3D reference, we introduce an additional meta-controller to modulate the conditioning strength according to different similarity levels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3">Meta-controller shares a similar architecture but has different parameters <math alttext="\Theta^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" mathvariant="normal" xref="S3.SS2.p3.1.m1.1.1.2.cmml">Θ</mi><mo id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">Θ</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\Theta^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>. It works as a knob that dynamically modulates base ControlNet to generate adaptive control signals. Meta-controller takes a pair <math alttext="\bm{c}_{\mathrm{pair}}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">𝒄</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">pair</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝒄</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">pair</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\bm{c}_{\mathrm{pair}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">bold_italic_c start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT</annotation></semantics></math> of the concept image and the front-view reference CCM as input to produce meta-control signals based on their similarities. The meta-control signals are injected into diffusion models in two ways. On the one hand, meta-controller produces multi-scale alignment features <math alttext="\bm{y}_{\mathrm{meta1}}=\mathcal{Z}^{{\mathrm{meta1}}}_{\Theta^{\prime}}\left(%
\mathcal{F}^{{\mathrm{meta}}}_{\Theta^{\prime}}\left(\bm{z}_{\mathrm{pair}}%
\right)\right)" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><msub id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3.2" xref="S3.SS2.p3.3.m3.1.1.3.2.cmml">𝒚</mi><mi id="S3.SS2.p3.3.m3.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.3.3.cmml">meta1</mi></msub><mo id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml"><msubsup id="S3.SS2.p3.3.m3.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.3.m3.1.1.1.3.2.2" xref="S3.SS2.p3.3.m3.1.1.1.3.2.2.cmml">𝒵</mi><msup id="S3.SS2.p3.3.m3.1.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.1.3.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.3.3.2" mathvariant="normal" xref="S3.SS2.p3.3.m3.1.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS2.p3.3.m3.1.1.1.3.3.3" xref="S3.SS2.p3.3.m3.1.1.1.3.3.3.cmml">′</mo></msup><mi id="S3.SS2.p3.3.m3.1.1.1.3.2.3" xref="S3.SS2.p3.3.m3.1.1.1.3.2.3.cmml">meta1</mi></msubsup><mo id="S3.SS2.p3.3.m3.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><msubsup id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.2.cmml">ℱ</mi><msup id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.2" mathvariant="normal" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.3.cmml">′</mo></msup><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.3.cmml">meta</mi></msubsup><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.2.cmml">𝒛</mi><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.3.cmml">pair</mi></msub><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><eq id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2"></eq><apply id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.3.2">𝒚</ci><ci id="S3.SS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3.3">meta1</ci></apply><apply id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.2"></times><apply id="S3.SS2.p3.3.m3.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3">subscript</csymbol><apply id="S3.SS2.p3.3.m3.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.3.2.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.3.2.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.2.2">𝒵</ci><ci id="S3.SS2.p3.3.m3.1.1.1.3.2.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.2.3">meta1</ci></apply><apply id="S3.SS2.p3.3.m3.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.3.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.3.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.3.2">Θ</ci><ci id="S3.SS2.p3.3.m3.1.1.1.3.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1"><times id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2"></times><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.2">ℱ</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.2.3">meta</ci></apply><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.2">Θ</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.2">𝒛</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.1.1.3">pair</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\bm{y}_{\mathrm{meta1}}=\mathcal{Z}^{{\mathrm{meta1}}}_{\Theta^{\prime}}\left(%
\mathcal{F}^{{\mathrm{meta}}}_{\Theta^{\prime}}\left(\bm{z}_{\mathrm{pair}}%
\right)\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">bold_italic_y start_POSTSUBSCRIPT meta1 end_POSTSUBSCRIPT = caligraphic_Z start_POSTSUPERSCRIPT meta1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_F start_POSTSUPERSCRIPT roman_meta end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT ) )</annotation></semantics></math> to be injected into base ControlNet. These features are applied to the down-sampling blocks of base ControlNet (<span class="ltx_text" id="S3.SS2.p3.3.1" style="color:#000000;">Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.E2" title="In 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">2</span></a></span>) at each scale to guide the encoding of reference and help produce base-signals as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{\mathrm{base}}=\mathcal{Z}^{{\mathrm{base}}}_{\Theta}\left(\mathcal{F}%
^{{\mathrm{base}}}_{\Theta}\left(\bm{y}_{\mathrm{meta1}},\bm{z}_{\mathrm{ref}}%
\right)\right)," class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">𝒚</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">base</mi></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.3.2.2.cmml">𝒵</mi><mi id="S3.E2.m1.1.1.1.1.1.3.3" mathvariant="normal" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml">Θ</mi><mi id="S3.E2.m1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.3.2.3.cmml">base</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.2.cmml">ℱ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.4.3" mathvariant="normal" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.3.cmml">Θ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">base</mi></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒚</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">meta1</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝒛</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">ref</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝒚</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">base</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.2">𝒵</ci><ci id="S3.E2.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.3">base</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">Θ</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.2">ℱ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.2.3">base</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.3">Θ</ci></apply><interval closure="open" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝒚</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">meta1</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝒛</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3">ref</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\bm{y}_{\mathrm{base}}=\mathcal{Z}^{{\mathrm{base}}}_{\Theta}\left(\mathcal{F}%
^{{\mathrm{base}}}_{\Theta}\left(\bm{y}_{\mathrm{meta1}},\bm{z}_{\mathrm{ref}}%
\right)\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">bold_italic_y start_POSTSUBSCRIPT roman_base end_POSTSUBSCRIPT = caligraphic_Z start_POSTSUPERSCRIPT roman_base end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( caligraphic_F start_POSTSUPERSCRIPT roman_base end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT meta1 end_POSTSUBSCRIPT , bold_italic_z start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.4">where <math alttext="\bm{z}_{\mathrm{ref}}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">𝒛</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">ref</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝒛</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ref</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\bm{z}_{\mathrm{ref}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">bold_italic_z start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{z}_{\mathrm{pair}}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">𝒛</mi><mi id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">pair</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">𝒛</ci><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">pair</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\bm{z}_{\mathrm{pair}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">bold_italic_z start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT</annotation></semantics></math> are the feature maps of <math alttext="\bm{c}_{\mathrm{ref}}" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">𝒄</mi><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">ref</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">𝒄</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">ref</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\bm{c}_{\mathrm{ref}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">bold_italic_c start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{c}_{\mathrm{pair}}" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">𝒄</mi><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">pair</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">𝒄</ci><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">pair</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">\bm{c}_{\mathrm{pair}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">bold_italic_c start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT</annotation></semantics></math> via the trainable encoders in <span class="ltx_text" id="S3.SS2.p4.4.1" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.F3" title="Figure 3 ‣ 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3</span></a></span> (a).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.3">On the other hand, meta-controller produces meta-signals <math alttext="\bm{y}_{\mathrm{meta2}}=\mathcal{Z}^{{\mathrm{meta2}}}_{\Theta^{\prime}}\left(%
\mathcal{F}^{{\mathrm{meta}}}_{\Theta^{\prime}}\left(\bm{z}_{\mathrm{pair}}%
\right)\right)" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><msub id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.3.2.cmml">𝒚</mi><mi id="S3.SS2.p5.1.m1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.3.3.cmml">meta2</mi></msub><mo id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml"><msubsup id="S3.SS2.p5.1.m1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.1.1.1.3.2.2" xref="S3.SS2.p5.1.m1.1.1.1.3.2.2.cmml">𝒵</mi><msup id="S3.SS2.p5.1.m1.1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.1.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.3.3.2" mathvariant="normal" xref="S3.SS2.p5.1.m1.1.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS2.p5.1.m1.1.1.1.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.3.3.3.cmml">′</mo></msup><mi id="S3.SS2.p5.1.m1.1.1.1.3.2.3" xref="S3.SS2.p5.1.m1.1.1.1.3.2.3.cmml">meta2</mi></msubsup><mo id="S3.SS2.p5.1.m1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml"><msubsup id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.2.cmml">ℱ</mi><msup id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.2" mathvariant="normal" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.2.cmml">Θ</mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.3.cmml">′</mo></msup><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.3.cmml">meta</mi></msubsup><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">𝒛</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">pair</mi></msub><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p5.1.m1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><eq id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2"></eq><apply id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2">𝒚</ci><ci id="S3.SS2.p5.1.m1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.3">meta2</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"><times id="S3.SS2.p5.1.m1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.2"></times><apply id="S3.SS2.p5.1.m1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3">subscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.3.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.3.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.2.2">𝒵</ci><ci id="S3.SS2.p5.1.m1.1.1.1.3.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.2.3">meta2</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.3.2">Θ</ci><ci id="S3.SS2.p5.1.m1.1.1.1.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1"><times id="S3.SS2.p5.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.2"></times><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.2">ℱ</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.2.3">meta</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.2">Θ</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.3.3.3">′</ci></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.2">𝒛</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.3">pair</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\bm{y}_{\mathrm{meta2}}=\mathcal{Z}^{{\mathrm{meta2}}}_{\Theta^{\prime}}\left(%
\mathcal{F}^{{\mathrm{meta}}}_{\Theta^{\prime}}\left(\bm{z}_{\mathrm{pair}}%
\right)\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">bold_italic_y start_POSTSUBSCRIPT meta2 end_POSTSUBSCRIPT = caligraphic_Z start_POSTSUPERSCRIPT meta2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_F start_POSTSUPERSCRIPT roman_meta end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_Θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT ) )</annotation></semantics></math> to be injected to the pretrained multi-view diffusion models. These features are added up to base-signal <math alttext="\bm{y}_{\mathrm{base}}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">𝒚</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">base</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝒚</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">base</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\bm{y}_{\mathrm{base}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">bold_italic_y start_POSTSUBSCRIPT roman_base end_POSTSUBSCRIPT</annotation></semantics></math> to directly apply for the pretrained diffusion models. Totally, the final outputs of meta-ControlNet are adaptive control signals <math alttext="\bm{y}_{\mathrm{adaptive}}" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><msub id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">𝒚</mi><mi id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">adaptive</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2">𝒚</ci><ci id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3">adaptive</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\bm{y}_{\mathrm{adaptive}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">bold_italic_y start_POSTSUBSCRIPT roman_adaptive end_POSTSUBSCRIPT</annotation></semantics></math> based on the similarity between the concept image and the 3D reference, as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{y}_{\mathrm{adaptive}}=\bm{y}_{\mathrm{base}}+\bm{y}_{\mathrm{meta2}}." class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">adaptive</mi></msub><mo id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.cmml">𝒚</mi><mi id="S3.E3.m1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.3.2.3.cmml">base</mi></msub><mo id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml">𝒚</mi><mi id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml">meta2</mi></msub></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" lspace="0em" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2">𝒚</ci><ci id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">adaptive</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2">𝒚</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3">base</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">𝒚</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3">meta2</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\bm{y}_{\mathrm{adaptive}}=\bm{y}_{\mathrm{base}}+\bm{y}_{\mathrm{meta2}}.</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">bold_italic_y start_POSTSUBSCRIPT roman_adaptive end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT roman_base end_POSTSUBSCRIPT + bold_italic_y start_POSTSUBSCRIPT meta2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dynamic Reference Routing</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3">Reference models typically align roughly with the concept image in terms of coarse shape, but diverge significantly in local details. This misalignment can cause confusion and conflicts, as the generation process relies on both the image and reference model. To address this issue, we propose a dynamic reference routing strategy that adjusts the reference resolution across denoise timesteps, as shown in <span class="ltx_text" id="S3.SS3.p1.3.1" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.F3" title="Figure 3 ‣ 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3</span></a></span> (b).
As widely observed during the reverse diffusion process, the coarse structure of a target image is determined in high-noised timesteps and fine details emerge later as the timestep goes on. This motivates us to start with low-resolution reference CCMs at high noise levels <math alttext="t_{h}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑡</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">t_{h}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math>. By lowering the resolution, reference models provide fewer details but exhibit smaller misalignment with the concept image. This enables reference models to assist in generating the global structure of 3D objects without significant conflicts. We then gradually increase the resolution of reference CCMs as the reverse diffusion process goes into middle noise levels <math alttext="t_{m}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">t</mi><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">t_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> and low noise levels <math alttext="t_{l}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">t</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝑡</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">t_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> to help refine local structures, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.3.2">e.g., </span>progressively generating a curly tail from a straight one (<span class="ltx_text" id="S3.SS3.p1.3.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.F3" title="Figure 3 ‣ 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3</span></a></span> (b)).
This design choice would ensure effective usage of both concept image and 3D reference during the multi-view image generation process while avoiding degraded generation caused by misalignment.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Self-Reference Augmentation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">A good reference model should resemble the target 3D model (with varied details) to provide additional geometric cues, but it is impractical to collect sufficient target-reference pairs for training. An intuitive solution is to retrieve a similar model from a large 3D database as the training reference. However, due to the limited variety in current databases, finding a perfect match is challenging. The retrieved reference can vary greatly in orientation, size and semantics. While this is a common situation in inference scenarios, where a very similar reference is often unavailable, we found training with these challenging pairs fails to effectively use the 3D reference. We conjecture that the learning process struggles due to the significant differences between the reference and target 3D, leading the diffusion model to disregard the references. To avoid the ‘idleness’ of reference, we developed a self-reference scheme that uses the target model as its own reference by applying various augmentations to mimic misalignment (refer to <span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">Appendix</span> <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS4" title="A.4 Augmentation Details ‣ Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">A.4</span></a>). This approach ensures that the reference models are somewhat aligned with the target and more compatible, alleviating the learning difficulty.
We further design a curriculum training strategy, which begins with minimal augmentations (very similar references) to force the diffusion model to rely on the reference for enhancement. Over time, we gradually increase augmentation strength and incorporate retrieved references, challenging the diffusion model to learn from references that do not closely match the target. Once trained, our model performs well with a variety of references, even those retrieved ones that are not very similar.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Sparse-View 3D Reconstruction</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.2">With multi-view images generated in the first stage, we can obtain final 3D models via sparse-view 3D reconstruction. This step can be built upon arbitrary sparse-view reconstruction models. Specifically, we finetune LGM <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib34" title="">2024</a>)</cite> by expanding the number of input views from 4 to 6 and the resolution of each view from <math alttext="256\times 256" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mrow id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mn id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">256</mn><mo id="S3.SS5.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><times id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1"></times><cn id="S3.SS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS5.p1.1.m1.1.1.2">256</cn><cn id="S3.SS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS5.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">256 × 256</annotation></semantics></math> to <math alttext="320\times 320" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mn id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">320</mn><mo id="S3.SS5.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><times id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></times><cn id="S3.SS5.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS5.p1.2.m2.1.1.2">320</cn><cn id="S3.SS5.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS5.p1.2.m2.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">320 × 320</annotation></semantics></math> so that the trained reconstruction model aligns with the multi-view images generated in our first stage.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate our method on image-to-3D generation, a significant area in 3D generation research. For each image, we retrieve a 3D reference model from a 3D database based on similarity <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib48" title="">2024</a>)</cite>. The database used is a subset of Objaverse, containing 40K models. We anticipate that performance could be further enhanced with a larger database in the future.
For the rest of this section, we compare <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Phidias</span> with state-of-the-art methods and conduct ablation analysis.
More results and implementation details can be found in <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">Appendix</span>.
Results on text-to-3D and 3D-to-3D generation can be found in <span class="ltx_text" id="S4.p1.1.3" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5" title="5 Applications ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">5</span></a></span>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="600" id="S4.F4.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Diverse retrieval-augmented image-to-3D results. <span class="ltx_text ltx_font_italic" id="S4.F4.2.1">Phidias</span> can generate diverse 3D models with different references for a single input image.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="552" id="S4.F5.g1" src="x3.png" width="805"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative comparisons on image-to-3D generation.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparisons with State-of-the-Art Methods</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We compare <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Phidias</span> with five image-to-3D baselines: CRM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib40" title="">2024a</a>)</cite>, LGM <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib34" title="">2024</a>)</cite>, InstantMesh <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib43" title="">2024</a>)</cite>, SV3D <cite class="ltx_cite ltx_citemacro_citep">(Voleti et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib36" title="">2024</a>)</cite>, and OpenLRM <cite class="ltx_cite ltx_citemacro_citep">(He &amp; Wang, <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib15" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Qualitative Results.</span>
<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">For visual diversity</span> (<span class="ltx_text" id="S4.SS1.p2.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">4</span></a></span>), given the same concept image, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.4">Phidias</span> can generate diverse 3D assets that are both faithful to the concept image and conforming to a specific retrieved 3D reference in geometry. <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.5">For visual comparisons</span> (<span class="ltx_text" id="S4.SS1.p2.1.6" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">5</span></a></span>), while the baseline methods can generate plausible results, they suffer from geometry distortion (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.7">e.g., </span>horse legs). Besides, none of the existing methods can benefit from the 3D reference for improved generalization ability (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.8">e.g., </span>excavator’s dipper) and controllability (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.9">e.g., </span>cat’s tail) as ours.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Quantitative Results.</span>
Following previous works, we conduct quantitative evaluation on google scanned objects (GSO) <cite class="ltx_cite ltx_citemacro_citep">(Downs et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib12" title="">2022</a>)</cite>. We remove duplicated objects with the same shape and randomly select 200 objects for evaluation.
<span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">For visual quality</span>, we report reconstruction metrics (PSNR, SSIM and LPIPS) on 20 novel views. We also report novel views’ CLIP similarity with paired GT (CLIP-P) and input image (CLIP-I). <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.3">For geometry quality</span>, we sample 50K points from mesh surface and compute Chamfer Distance (CD) and F-Score (with a threshold of 0.05). To align the generated mesh and GT, we unify their coordinate systems and re-scale them into a unit box.
We report our results with the retrieved reference, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.4">i.e., Ours (Retrieved Ref.)</span>, and GT mesh as reference, <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.5">i.e., Ours (GT Ref.)</span>, respectively. As shown in <span class="ltx_text" id="S4.SS1.p3.1.6" style="color:#000000;">Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.T1" title="Table 1 ‣ 4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">1</span></a></span>, ours, with either retrieved or GT reference, outperforms all baselines, benefiting from the proposed retrieval-augmented method. While the CD is slightly larger, we argue that our approach produces plausible 3D models given different references (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F7" title="Figure 7 ‣ 4.2 Ablation Study and Analysis ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">7</span></a>), though they can differ from GT mesh when computing chamfer distance.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative comparison with baselines on image-to-3D synthesis.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.7">
<tr class="ltx_tr" id="S4.T1.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T1.7.7.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.7.8.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1">PSNR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.1">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.1.m1.1"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><ci id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.1">CLIP-P <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.1.m1.1"><semantics id="S4.T1.4.4.4.1.m1.1a"><mo id="S4.T1.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.5.1">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.5.5.5.1.m1.1"><semantics id="S4.T1.5.5.5.1.m1.1a"><mo id="S4.T1.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T1.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><ci id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.6.6.6.1">CD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.6.6.6.1.m1.1"><semantics id="S4.T1.6.6.6.1.m1.1a"><mo id="S4.T1.6.6.6.1.m1.1.1" stretchy="false" xref="S4.T1.6.6.6.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.1.m1.1b"><ci id="S4.T1.6.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.6.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.7.7.1">F-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.7.7.7.1.m1.1"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.1.m1.1.1" stretchy="false" xref="S4.T1.7.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><ci id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.7.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.7.8.1">OpenLRM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.2">16.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.3">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.4">0.194</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.5">0.866</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.7.8.6">0.847</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.7">0.0446</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.8.8">0.805</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.7.9.1">LGM</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.2">14.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.3">0.807</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.4">0.219</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.5">0.869</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.9.6">0.871</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.9.7.1">0.0398</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.9.8">0.831</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.7.10.1">CRM</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.2">16.35</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.3">0.841</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.4">0.182</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.5">0.855</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.10.6">0.843</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.7">0.0443</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.10.8">0.796</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.7.11.1">SV3D</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.2">16.24</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.3">0.838</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.4">0.203</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.5">0.879</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.11.6">0.866</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.11.8">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.7.12.1">InstantMesh</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.2">14.63</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.3">0.796</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.4">0.235</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.5">0.882</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.7.12.6">0.880</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.7">0.0450</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.12.8">0.788</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.13">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.7.13.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.1.1">Ours (GT Ref.)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.2"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.2.1">20.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.3"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.3.1">0.870</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.4"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.4.1">0.117</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.5"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.5.1">0.911</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.7.13.6"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.6.1">0.885</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.7"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.7.1">0.0391</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.7.13.8"><span class="ltx_text ltx_font_bold" id="S4.T1.7.13.8.1">0.840</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.14">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T1.7.14.1"><span class="ltx_text ltx_font_bold" id="S4.T1.7.14.1.1">Ours (Retrieved Ref.)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.2.1">17.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.3.1">0.845</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.4.1">0.174</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.5.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.7.14.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.6.1">0.885</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.7">0.0402</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.7.14.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.14.8.1">0.833</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure ltx_minipage ltx_align_middle" id="S4.SS1.8" style="width:397.5pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.SS1.1.1.fig1" style="width:79.5pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>User study.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.SS1.1.1.fig1.1">
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.SS1.1.1.fig1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.1.1.fig1.1.1.1.1">Baseline</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.1.1.fig1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.1.1.fig1.1.1.2.1">Pref. Rate</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.SS1.1.1.fig1.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">OpenLRM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.1.1.fig1.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">94.7%</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.3">
<td class="ltx_td ltx_align_left" id="S4.SS1.1.1.fig1.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">LGM</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.1.1.fig1.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">95.8%</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.4">
<td class="ltx_td ltx_align_left" id="S4.SS1.1.1.fig1.1.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">CRM</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.1.1.fig1.1.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">93.7%</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.5">
<td class="ltx_td ltx_align_left" id="S4.SS1.1.1.fig1.1.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">SV3D</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.1.1.fig1.1.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">88.4%</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.1.1.fig1.1.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.SS1.1.1.fig1.1.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">InstantMesh</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.1.1.fig1.1.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">91.6%</td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.SS1.8.8.7" style="width:318.0pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Quantitative ablation study of the proposed components.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.SS1.8.8.7.7">
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.SS1.8.8.7.7.7.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.7.8.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.2.2.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.2.2.1.1.1.1.1">PSNR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.SS1.2.2.1.1.1.1.1.m1.1"><semantics id="S4.SS1.2.2.1.1.1.1.1.m1.1a"><mo id="S4.SS1.2.2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.SS1.2.2.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.2.2.1.1.1.1.1.m1.1b"><ci id="S4.SS1.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS1.2.2.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.2.2.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.2.2.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.3.3.2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.3.3.2.2.2.2.1">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.SS1.3.3.2.2.2.2.1.m1.1"><semantics id="S4.SS1.3.3.2.2.2.2.1.m1.1a"><mo id="S4.SS1.3.3.2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.SS1.3.3.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.3.3.2.2.2.2.1.m1.1b"><ci id="S4.SS1.3.3.2.2.2.2.1.m1.1.1.cmml" xref="S4.SS1.3.3.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.3.3.2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.3.3.2.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.4.4.3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.4.4.3.3.3.3.1">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS1.4.4.3.3.3.3.1.m1.1"><semantics id="S4.SS1.4.4.3.3.3.3.1.m1.1a"><mo id="S4.SS1.4.4.3.3.3.3.1.m1.1.1" stretchy="false" xref="S4.SS1.4.4.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.4.4.3.3.3.3.1.m1.1b"><ci id="S4.SS1.4.4.3.3.3.3.1.m1.1.1.cmml" xref="S4.SS1.4.4.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.4.4.3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.4.4.3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.5.5.4.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.5.5.4.4.4.4.1">CLIP-P <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.SS1.5.5.4.4.4.4.1.m1.1"><semantics id="S4.SS1.5.5.4.4.4.4.1.m1.1a"><mo id="S4.SS1.5.5.4.4.4.4.1.m1.1.1" stretchy="false" xref="S4.SS1.5.5.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.5.5.4.4.4.4.1.m1.1b"><ci id="S4.SS1.5.5.4.4.4.4.1.m1.1.1.cmml" xref="S4.SS1.5.5.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.5.5.4.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.5.5.4.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.SS1.6.6.5.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.6.6.5.5.5.5.1">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.SS1.6.6.5.5.5.5.1.m1.1"><semantics id="S4.SS1.6.6.5.5.5.5.1.m1.1a"><mo id="S4.SS1.6.6.5.5.5.5.1.m1.1.1" stretchy="false" xref="S4.SS1.6.6.5.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.6.6.5.5.5.5.1.m1.1b"><ci id="S4.SS1.6.6.5.5.5.5.1.m1.1.1.cmml" xref="S4.SS1.6.6.5.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.6.6.5.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.6.6.5.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.7.7.6.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.7.7.6.6.6.6.1">CD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.SS1.7.7.6.6.6.6.1.m1.1"><semantics id="S4.SS1.7.7.6.6.6.6.1.m1.1a"><mo id="S4.SS1.7.7.6.6.6.6.1.m1.1.1" stretchy="false" xref="S4.SS1.7.7.6.6.6.6.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.7.7.6.6.6.6.1.m1.1b"><ci id="S4.SS1.7.7.6.6.6.6.1.m1.1.1.cmml" xref="S4.SS1.7.7.6.6.6.6.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.7.7.6.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.7.7.6.6.6.6.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.SS1.8.8.7.7.7.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.7.7.1">F-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.SS1.8.8.7.7.7.7.1.m1.1"><semantics id="S4.SS1.8.8.7.7.7.7.1.m1.1a"><mo id="S4.SS1.8.8.7.7.7.7.1.m1.1.1" stretchy="false" xref="S4.SS1.8.8.7.7.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.8.8.7.7.7.7.1.m1.1b"><ci id="S4.SS1.8.8.7.7.7.7.1.m1.1.1.cmml" xref="S4.SS1.8.8.7.7.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.8.8.7.7.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.8.8.7.7.7.7.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.SS1.8.8.7.7.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">Base Model</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">14.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.227</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.855</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.SS1.8.8.7.7.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">0.859</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.7" style="padding-left:3.0pt;padding-right:3.0pt;">0.0424</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS1.8.8.7.7.8.8" style="padding-left:3.0pt;padding-right:3.0pt;">0.826</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.SS1.8.8.7.7.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">+ Meta-ControlNet</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">16.35</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.833</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.190</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.881</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.SS1.8.8.7.7.9.6" style="padding-left:3.0pt;padding-right:3.0pt;">0.878</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.7" style="padding-left:3.0pt;padding-right:3.0pt;">0.0407</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.9.8" style="padding-left:3.0pt;padding-right:3.0pt;">0.829</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.SS1.8.8.7.7.10.1" style="padding-left:3.0pt;padding-right:3.0pt;">+ Dynamic Ref. Routing</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.2" style="padding-left:3.0pt;padding-right:3.0pt;">14.76</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.816</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.221</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.868</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.SS1.8.8.7.7.10.6" style="padding-left:3.0pt;padding-right:3.0pt;">0.861</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.7" style="padding-left:3.0pt;padding-right:3.0pt;">0.0420</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.10.8" style="padding-left:3.0pt;padding-right:3.0pt;">0.826</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.SS1.8.8.7.7.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">+ Self-Ref. Augmentation</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.2" style="padding-left:3.0pt;padding-right:3.0pt;">16.57</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.840</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.182</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.880</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.SS1.8.8.7.7.11.6" style="padding-left:3.0pt;padding-right:3.0pt;">0.883</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.7" style="padding-left:3.0pt;padding-right:3.0pt;">0.0414</td>
<td class="ltx_td ltx_align_center" id="S4.SS1.8.8.7.7.11.8" style="padding-left:3.0pt;padding-right:3.0pt;">0.830</td>
</tr>
<tr class="ltx_tr" id="S4.SS1.8.8.7.7.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.SS1.8.8.7.7.12.1" style="padding-left:3.0pt;padding-right:3.0pt;">Full Model</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.2.1">17.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.3.1">0.845</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.4.1">0.174</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.5.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.SS1.8.8.7.7.12.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.6.1">0.885</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.7.1">0.0402</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS1.8.8.7.7.12.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS1.8.8.7.7.12.8.1">0.833</span></td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">User Study.</span>
We further conduct a user study to evaluate human preferences among different methods. We publicly invite 30 users to complete a questionnaire for pairwise comparisons. We show the preference rate (<span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.2">i.e., </span>the percentage of users prefer ours compared to a baseline method) in <span class="ltx_text" id="S4.SS1.p4.1.3" style="color:#000000;">Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.SS1" title="4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">4.1</span></a></span>, which suggests that our approach significantly outperforms existing methods in the image-to-3D task based on human preferences.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="382" id="S4.F6.g1" src="x4.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative ablation study of the proposed components.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study and Analysis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Ablation Studies.</span>
We conduct ablation studies across four settings: a base model employing a standard ControlNet trained with self-reference, and three variants (each integrating one proposed component into the base model). The quantitative results in <span class="ltx_text" id="S4.SS2.p1.1.2" style="color:#000000;">Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.SS1" title="4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">4.1</span></a></span> demonstrate clear improvements in both visual and geometric metrics with our proposed components.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">Effectiveness of Meta-ControlNet.</span>
To evaluate meta-ControlNet, we use both self-reference and retrieved reference for training, as the learning of Meta-Controller (<span class="ltx_text" id="S4.SS2.p2.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.F3" title="Figure 3 ‣ 3.2 Meta-ControlNet. ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3</span></a></span> (a) top) requires reference models with varying levels of similarity. As shown in <span class="ltx_text" id="S4.SS2.p2.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F6" title="Figure 6 ‣ 4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">6</span></a></span> (a), the base model trained with retrieved reference often ignores the reference, failing to follow the shape pattern (disconnected boat). This phenomenon stems from the considerable similarity variation among retrieved references, which confuses the diffusion model. The base model thereby struggles to determine when and how to use the reference as it lacks the ability to adjust to different levels of similarity. Consequently, they often end up with ignoring the reference models entirely. In contrast, meta-ControlNet equips the model with the capability to dynamically modulate the conditioning strength of the reference model, thereby effectively utilizing available references for improving or controlling the generation process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">Effectiveness of Dynamic Reference Routing.</span>
Dynamic reference routing aims to alleviate local conflicts between the reference and concept images. As illustrated in <span class="ltx_text" id="S4.SS2.p3.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F6" title="Figure 6 ‣ 4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">6</span></a></span> (b), when given a highly similar reference, the base model tends to rely heavily on it, leading to missing specific local details within the concept image, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">e.g., </span>the rope on the left.
By addressing these conflicts with dynamic routing, the model maintains the essential details of the concept image, while still benefiting from the guidance of the 3D reference.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1">Effectiveness of Self-Reference Augmentation.</span>
As shown in <span class="ltx_text" id="S4.SS2.p4.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F6" title="Figure 6 ‣ 4.1 Comparisons with State-of-the-Art Methods ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">6</span></a></span> (c), without self-reference augmentation, the base model predominantly depends on the provided reference for generation. When given a significantly misaligned reference, the model tends to follow the reference’s structure, resulting in an undesired outcome. Conversely, self-reference augmentation ensures that the generated models remain faithful to the concept image, while using the reference as geometry guidance.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Quantitative analysis on similarity levels of 3D reference.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.7">
<tr class="ltx_tr" id="S4.T4.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T4.7.7.8"><span class="ltx_text ltx_font_bold" id="S4.T4.7.7.8.1">Reference</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">PSNR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo id="S4.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.1">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T4.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><ci id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.4.4.4.1">CLIP-P <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.4.4.4.1.m1.1"><semantics id="S4.T4.4.4.4.1.m1.1a"><mo id="S4.T4.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.1.m1.1b"><ci id="S4.T4.4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.5.1">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.5.5.5.1.m1.1"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" stretchy="false" xref="S4.T4.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><ci id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.6.1">CD <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.6.6.6.1.m1.1"><semantics id="S4.T4.6.6.6.1.m1.1a"><mo id="S4.T4.6.6.6.1.m1.1.1" stretchy="false" xref="S4.T4.6.6.6.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.1.m1.1b"><ci id="S4.T4.6.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.6.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T4.7.7.7.1">F-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.7.7.7.1.m1.1"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" stretchy="false" xref="S4.T4.7.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><ci id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.7.8.1">Top-1 Retrieval</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.2"><span class="ltx_text ltx_font_bold" id="S4.T4.7.8.2.1">17.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.3"><span class="ltx_text ltx_font_bold" id="S4.T4.7.8.3.1">0.845</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.8.4.1">0.174</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.5"><span class="ltx_text ltx_font_bold" id="S4.T4.7.8.5.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.7.8.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.8.6.1">0.885</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.8.7.1">0.0402</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.7.8.8"><span class="ltx_text ltx_font_bold" id="S4.T4.7.8.8.1">0.833</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.7.9.1">Top-3 Retrieval</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.9.2.1">16.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.9.3.1">0.841</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.4"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.4.1">0.172</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.5"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.5.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.9.6"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.6.1">0.886</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.7"><span class="ltx_text ltx_font_bold" id="S4.T4.7.9.7.1">0.0395</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.9.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.9.8.1">0.830</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.7.10.1">Top-5 Retrieval</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.2">15.96</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.3">0.835</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.4">0.185</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.5">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.10.6">0.884</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.7">0.0408</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.10.8">0.819</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.7.11.1">Random Reference</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.2">14.74</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.3">0.820</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.4">0.226</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.5">0.884</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.7.11.6">0.882</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.7">0.0424</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.11.8">0.810</td>
</tr>
<tr class="ltx_tr" id="S4.T4.7.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T4.7.12.1">Without Reference</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.2">15.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.3">0.836</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.4">0.188</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.5">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.7.12.6">0.880</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.7">0.0416</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.7.12.8">0.814</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Analysis on Similarity Levels of 3D Reference.</span>
We analyze how similarity levels of 3D references would affect the performance. For each input, we retrieve three models ranked first (top-1), third (top-3), and fifth (top-5) in similarity scores, and randomly choose one model, to serve as 3D references. Quantitative results in <span class="ltx_text" id="S4.SS2.p5.1.2" style="color:#000000;">Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.T4" title="Table 4 ‣ 4.2 Ablation Study and Analysis ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">4</span></a></span> indicate that <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.3">Phidias</span> performs better with more similar references. <span class="ltx_text" id="S4.SS2.p5.1.4" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F7" title="Figure 7 ‣ 4.2 Ablation Study and Analysis ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">7</span></a></span> shows <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.5">Phidias</span> generates diverse plausible results with different references. All results remain faithful to the input image in the front view, but show variations in shapes influenced by the specific reference used. Also, we found <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.6">Phidias</span> can still generate plausible results even with a random 3D reference, indicating robustness to reference with different similarity levels.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="116" id="S4.F7.g1" src="extracted/5861237/Figures/abs_similarity_levels.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Qualitative analysis on similarity levels of 3D Reference.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications</h2>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="72" id="S5.F8.g1" src="extracted/5861237/Figures/app_text_to_3d.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_italic" id="S5.F8.2.1">Phidias</span> enables retrieval-augmented text-to-3D generation by first converting input text into a concept image, and then retrieving a 3D reference based on both the text and image. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Phidias</span> supports versatile applications beyond image-to-3D, such as text-to-3D, theme-aware 3D-to-3D, interactive 3D generation with coarse guidance, and high-fidelity 3D completion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Text to 3D.</span>
Text-to-3D generation can be converted to image-conditioned generation by transforming a text prompt into a concept image. However, the generated concept image can sometimes be atypical and may lose some information compared with original text input. To enhance generative quality, <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">Phidias</span> employs retrieval-augmented text-to-3D generation, as illustrated in <span class="ltx_text" id="S5.p2.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5.F8" title="Figure 8 ‣ 5 Applications ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">8</span></a></span>. This involves first retrieving a set of 3D references based on the concept image, and then selecting the one that most closely matches the text description as the final reference.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S5.F9.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_italic" id="S5.F9.2.1">Phidias</span> facilitates rapid, theme-aware 3D-to-3D generation by using an existing 3D model as a reference to transform its image variations into corresponding 3D variations.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Theme-Aware 3D-to-3D Generation.</span>
This task aims to create a gallery of theme-consistent 3D variations from existing 3D models. Previous work <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib41" title="">2024b</a>)</cite>
proposed an optimization-based approach, which is time-consuming. <span class="ltx_text ltx_font_italic" id="S5.p3.1.2">Phidias</span> supports fast generation by first generating image variations based on the input 3D model, and then transforming these variant images into 3D variations with the original 3D model itself as reference.
The results are shown in <span class="ltx_text" id="S5.p3.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5.F9" title="Figure 9 ‣ 5 Applications ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">9</span></a></span>, using 3D models from Sketchfab<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sketchfab.com/" title="">https://sketchfab.com/</a></span></span></span> and previous works as inputs.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="157" id="S5.F10.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_italic" id="S5.F10.2.1">Phidias</span> enables interactive 3D generation with coarse 3D shapes as guidance.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Interactive 3D Generation with Coarse Guidance.</span>
Interactive generation gives users more control over the outputs, empowering them to make quick edits and receive rapid feedback. <span class="ltx_text ltx_font_italic" id="S5.p4.1.2">Phidias</span> also provides this functionality, allowing users to continually adjust the geometry of generated 3D models using manually created coarse 3D shapes as reference models, as shown in <span class="ltx_text" id="S5.p4.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5.F10" title="Figure 10 ‣ 5 Applications ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">10</span></a></span>.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="133" id="S5.F11.g1" src="extracted/5861237/Figures/app_3d_completion.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_italic" id="S5.F11.2.1">Phidias</span> supports high-fidelity 3D completion by using the completed front views to guide the missing parts restoration and the original 3D model to help preserve the origin details.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">High-Fidelity 3D Completion.</span>
Given incomplete 3D models, as shown in <span class="ltx_text" id="S5.p5.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S5.F11" title="Figure 11 ‣ 5 Applications ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">11</span></a></span>, <span class="ltx_text ltx_font_italic" id="S5.p5.1.3">Phidias</span> can be used to restore the missing components. Specially, by generating a complete front view through image inpainting and referencing to the original 3D model, <span class="ltx_text ltx_font_italic" id="S5.p5.1.4">Phidias</span> can precisely predict and fill in the missing parts in novel views while maintaining the integrity and details of the origin, resulting in a seamlessly and coherently structured 3D model.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we introduced <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Phidias</span>, a 3D-aware diffusion model enhanced by 3D reference. By incorporating meta-ControlNet, dynamic reference routing, and self-reference augmentations, <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">Phidias</span> effectively leverages reference models with varying degrees of similarity for 3D generation. The proposed approach boosts the quality of 3D generation, expands its generalization capabilities, and improves user control. <span class="ltx_text ltx_font_italic" id="S6.p1.1.3">Phidias</span> offers a unified framework for creating high-quality 3D content from diverse modalities, such as text, images, and pre-existing 3D models, enabling versatile applications. We believe that <span class="ltx_text ltx_font_italic" id="S6.p1.1.4">Phidias</span> will inspire further research to advance the field of 3D generation.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSSx1.p1">
<p class="ltx_p" id="S6.SS0.SSSx1.p1.1">This work is partially supported by the National Key R&amp;D Program of China (2022ZD0160201) and Shanghai Artificial Intelligence Laboratory. This work is also in part supported by a GRF grant from the Research Grants Council of Hong Kong (Ref. No.: 11205620).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balaji et al. (2022)</span>
<span class="ltx_bibblock">
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al.

</span>
<span class="ltx_bibblock">ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2211.01324</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bensadoun et al. (2024)</span>
<span class="ltx_bibblock">
Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, et al.

</span>
<span class="ltx_bibblock">Meta 3d gen.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2407.02599</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et al. (2022)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer.

</span>
<span class="ltx_bibblock">Retrieval-augmented diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in Neural Information Processing Systems</em>, 35:15309–15324, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bob (2022)</span>
<span class="ltx_bibblock">
Bob.

</span>
<span class="ltx_bibblock">3D modeling 101: Comprehensive beginners guide, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wow-how.com/articles/3d-modeling-101-comprehensive-beginners-guide" title="">https://wow-how.com/articles/3d-modeling-101-comprehensive-beginners-guide</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carvajal (2023)</span>
<span class="ltx_bibblock">
Carlos Carvajal.

</span>
<span class="ltx_bibblock">The importance of references in 3d projects, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.linkedin.com/pulse/importance-references-3d-projects-carlos-carvajal/" title="">https://www.linkedin.com/pulse/importance-references-3d-projects-carlos-carvajal/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhuri et al. (2011)</span>
<span class="ltx_bibblock">
Siddhartha Chaudhuri, Evangelos Kalogerakis, Leonidas Guibas, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Probabilistic reasoning for assembly-based 3d modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Trans. Graph.</em>, 30(4), jul 2011.

</span>
<span class="ltx_bibblock">ISSN 0730-0301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang.

</span>
<span class="ltx_bibblock">Meshanything: Artist-created mesh generation with autoregressive transformers, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ECCV</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024c)</span>
<span class="ltx_bibblock">
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu.

</span>
<span class="ltx_bibblock">V3d: Video diffusion models are effective 3d generators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2403.06738</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui.

</span>
<span class="ltx_bibblock">Sdfusion: Multimodal 3d shape completion, reconstruction, and generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  4456–4465, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deitke et al. (2023)</span>
<span class="ltx_bibblock">
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Objaverse: A universe of annotated 3d objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  13142–13153, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Downs et al. (2022)</span>
<span class="ltx_bibblock">
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke.

</span>
<span class="ltx_bibblock">Google scanned objects: A high-quality dataset of 3d scanned household items.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">2022 International Conference on Robotics and Automation (ICRA)</em>, pp.  2553–2560. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2023)</span>
<span class="ltx_bibblock">
Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oğuz.

</span>
<span class="ltx_bibblock">3dgen: Triplane latent diffusion for textured mesh generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2303.05371</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2024)</span>
<span class="ltx_bibblock">
Junlin Han, Filippos Kokkinos, and Philip Torr.

</span>
<span class="ltx_bibblock">Vfusion3d: Learning scalable 3d generative models from video diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">European Conference on Computer Vision (ECCV)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He &amp; Wang (2023)</span>
<span class="ltx_bibblock">
Zexin He and Tengfei Wang.

</span>
<span class="ltx_bibblock">Openlrm: Open-source large reconstruction models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/3DTopia/OpenLRM" title="">https://github.com/3DTopia/OpenLRM</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. (2024)</span>
<span class="ltx_bibblock">
Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu.

</span>
<span class="ltx_bibblock">3dtopia: Large text-to-3d generation model with hybrid diffusion priors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2403.02234</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. (2023)</span>
<span class="ltx_bibblock">
Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan.

</span>
<span class="ltx_bibblock">Lrm: Large reconstruction model for single image to 3d.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2311.04400</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et al. (2021)</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jun &amp; Nichol (2023)</span>
<span class="ltx_bibblock">
Heewoo Jun and Alex Nichol.

</span>
<span class="ltx_bibblock">Shap-e: Generating conditional 3d implicit functions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2305.02463</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2013)</span>
<span class="ltx_bibblock">
Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha Chaudhuri, Stephen DiVerdi, and Thomas Funkhouser.

</span>
<span class="ltx_bibblock">Learning part-based templates from large collections of 3d shapes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">ACM Trans. Graph.</em>, jul 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su.

</span>
<span class="ltx_bibblock">One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2311.07885</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Zero-1-to-3: Zero-shot one image to 3d object.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  9298–9309, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. (2023)</span>
<span class="ltx_bibblock">
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al.

</span>
<span class="ltx_bibblock">Wonder3d: Single image to 3d using cross-domain diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.15008</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melas-Kyriazi et al. (2023)</span>
<span class="ltx_bibblock">
Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">RealFusion: 360 reconstruction of any object from a single image.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nichol et al. (2022)</span>
<span class="ltx_bibblock">
Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.

</span>
<span class="ltx_bibblock">Point-e: A system for generating 3d point clouds from complex prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2212.08751</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poole et al. (2023)</span>
<span class="ltx_bibblock">
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.

</span>
<span class="ltx_bibblock">DreamFusion: Text-to-3D using 2D diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schor et al. (2019)</span>
<span class="ltx_bibblock">
Nadav Schor, Oren Katzir, Hao Zhang, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Componet: Learning to generate the unseen by part synthesis and composition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  8758–8767, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV.2019.00885</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheynin et al. (2022)</span>
<span class="ltx_bibblock">
Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman.

</span>
<span class="ltx_bibblock">Knn-diffusion: Image generation via large-scale retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2204.02849</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023a)</span>
<span class="ltx_bibblock">
Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su.

</span>
<span class="ltx_bibblock">Zero123++: a single image to consistent multi-view diffusion base model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2310.15110</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023b)</span>
<span class="ltx_bibblock">
Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang.

</span>
<span class="ltx_bibblock">Mvdream: Multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2308.16512</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddiqui et al. (2023)</span>
<span class="ltx_bibblock">
Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner.

</span>
<span class="ltx_bibblock">Meshgpt: Generating triangle meshes with decoder-only transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2311.15475</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Lgm: Large multi-view gaussian model for high-resolution 3d content creation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2402.05054</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen.

</span>
<span class="ltx_bibblock">Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  22819–22829, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voleti et al. (2024)</span>
<span class="ltx_bibblock">
Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2403.12008</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang &amp; Shi (2023)</span>
<span class="ltx_bibblock">
Peng Wang and Yichun Shi.

</span>
<span class="ltx_bibblock">Imagedream: Image-prompt multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2312.02201</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen.

</span>
<span class="ltx_bibblock">Pretraining is all you need for image-to-image translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv:2205.12952</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al.

</span>
<span class="ltx_bibblock">Rodin: A generative model for sculpting 3d digital avatars using diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  4563–4573, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu.

</span>
<span class="ltx_bibblock">Crm: Single image to 3d textured mesh with convolutional reconstruction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2403.05034</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau.

</span>
<span class="ltx_bibblock">Themestation: Generating theme-aware 3d assets from few exemplars.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">SIGGRAPH</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu &amp; Zheng (2022)</span>
<span class="ltx_bibblock">
Rundi Wu and Changxi Zheng.

</span>
<span class="ltx_bibblock">Learning to generate 3d shapes from a single example.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">ACM Transactions on Graphics (TOG)</em>, 41(6), 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan.

</span>
<span class="ltx_bibblock">Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2404.07191</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al. (2024)</span>
<span class="ltx_bibblock">
Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou.

</span>
<span class="ltx_bibblock">Nvs-solver: Video diffusion model as zero-shot novel view synthesizer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2405.15364</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo.

</span>
<span class="ltx_bibblock">Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2403.19655</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu.

</span>
<span class="ltx_bibblock">Clay: A controllable large-scale generative model for creating high-quality 3d assets, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  3836–3847, October 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Uni3d: Exploring unified 3d representation at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. (2023)</span>
<span class="ltx_bibblock">
Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang.

</span>
<span class="ltx_bibblock">Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2312.09147</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>dataset</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.16"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.16.1">Training set.</span> To train our <span class="ltx_text ltx_font_italic" id="A1.SS1.p1.16.2">reference-augmented multi-view diffusion model</span>, we use a filtered subset of the Objaverse <cite class="ltx_cite ltx_citemacro_citep">(Deitke et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib11" title="">2023</a>)</cite> dataset, excluding low-quality 3D models as described in <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib34" title="">2024</a>)</cite>. Additionally, we apply further filtering to remove objects that are too thin and eliminate data originating from scans, both of which are intended to ensure the quality of subsequent retrieval. We also exclude objects with an excessively high number of vertices or faces to optimize the costly point cloud extraction process and reduce computational time. These refinements result in a final training set comprising approximately 64K 3D objects. For each object, we normalize it within a unit sphere, and render 1 concept image, 6 canonical coordinate maps (CCMs), and 6 target RGBA images, following the camera distribution protocol of Zero123++ <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib31" title="">2023a</a>)</cite>. In particular, the concept image is rendered using randomly sampled azimuth and elevation angles from a predefined range. The poses of the six corresponding CCMs and target images consist of interleaving absolute elevations of {<math alttext="20" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1.1"><semantics id="A1.SS1.p1.1.m1.1a"><mn id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><cn id="A1.SS1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.m1.1d">20</annotation></semantics></math>°, <math alttext="-10" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m2.1"><semantics id="A1.SS1.p1.2.m2.1a"><mrow id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml"><mo id="A1.SS1.p1.2.m2.1.1a" xref="A1.SS1.p1.2.m2.1.1.cmml">−</mo><mn id="A1.SS1.p1.2.m2.1.1.2" xref="A1.SS1.p1.2.m2.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><apply id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"><minus id="A1.SS1.p1.2.m2.1.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"></minus><cn id="A1.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS1.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">-10</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.2.m2.1d">- 10</annotation></semantics></math>°, <math alttext="20" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m3.1"><semantics id="A1.SS1.p1.3.m3.1a"><mn id="A1.SS1.p1.3.m3.1.1" xref="A1.SS1.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m3.1b"><cn id="A1.SS1.p1.3.m3.1.1.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.3.m3.1c">20</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.3.m3.1d">20</annotation></semantics></math>°, <math alttext="-10" class="ltx_Math" display="inline" id="A1.SS1.p1.4.m4.1"><semantics id="A1.SS1.p1.4.m4.1a"><mrow id="A1.SS1.p1.4.m4.1.1" xref="A1.SS1.p1.4.m4.1.1.cmml"><mo id="A1.SS1.p1.4.m4.1.1a" xref="A1.SS1.p1.4.m4.1.1.cmml">−</mo><mn id="A1.SS1.p1.4.m4.1.1.2" xref="A1.SS1.p1.4.m4.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.4.m4.1b"><apply id="A1.SS1.p1.4.m4.1.1.cmml" xref="A1.SS1.p1.4.m4.1.1"><minus id="A1.SS1.p1.4.m4.1.1.1.cmml" xref="A1.SS1.p1.4.m4.1.1"></minus><cn id="A1.SS1.p1.4.m4.1.1.2.cmml" type="integer" xref="A1.SS1.p1.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.4.m4.1c">-10</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.4.m4.1d">- 10</annotation></semantics></math>°, <math alttext="20" class="ltx_Math" display="inline" id="A1.SS1.p1.5.m5.1"><semantics id="A1.SS1.p1.5.m5.1a"><mn id="A1.SS1.p1.5.m5.1.1" xref="A1.SS1.p1.5.m5.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.5.m5.1b"><cn id="A1.SS1.p1.5.m5.1.1.cmml" type="integer" xref="A1.SS1.p1.5.m5.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.5.m5.1c">20</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.5.m5.1d">20</annotation></semantics></math>°, <math alttext="-10" class="ltx_Math" display="inline" id="A1.SS1.p1.6.m6.1"><semantics id="A1.SS1.p1.6.m6.1a"><mrow id="A1.SS1.p1.6.m6.1.1" xref="A1.SS1.p1.6.m6.1.1.cmml"><mo id="A1.SS1.p1.6.m6.1.1a" xref="A1.SS1.p1.6.m6.1.1.cmml">−</mo><mn id="A1.SS1.p1.6.m6.1.1.2" xref="A1.SS1.p1.6.m6.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.6.m6.1b"><apply id="A1.SS1.p1.6.m6.1.1.cmml" xref="A1.SS1.p1.6.m6.1.1"><minus id="A1.SS1.p1.6.m6.1.1.1.cmml" xref="A1.SS1.p1.6.m6.1.1"></minus><cn id="A1.SS1.p1.6.m6.1.1.2.cmml" type="integer" xref="A1.SS1.p1.6.m6.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.6.m6.1c">-10</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.6.m6.1d">- 10</annotation></semantics></math>°}, and relative azimuths of {<math alttext="\phi+30" class="ltx_Math" display="inline" id="A1.SS1.p1.7.m7.1"><semantics id="A1.SS1.p1.7.m7.1a"><mrow id="A1.SS1.p1.7.m7.1.1" xref="A1.SS1.p1.7.m7.1.1.cmml"><mi id="A1.SS1.p1.7.m7.1.1.2" xref="A1.SS1.p1.7.m7.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.7.m7.1.1.1" xref="A1.SS1.p1.7.m7.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.7.m7.1.1.3" xref="A1.SS1.p1.7.m7.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.7.m7.1b"><apply id="A1.SS1.p1.7.m7.1.1.cmml" xref="A1.SS1.p1.7.m7.1.1"><plus id="A1.SS1.p1.7.m7.1.1.1.cmml" xref="A1.SS1.p1.7.m7.1.1.1"></plus><ci id="A1.SS1.p1.7.m7.1.1.2.cmml" xref="A1.SS1.p1.7.m7.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.7.m7.1.1.3.cmml" type="integer" xref="A1.SS1.p1.7.m7.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.7.m7.1c">\phi+30</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.7.m7.1d">italic_ϕ + 30</annotation></semantics></math>°, <math alttext="\phi+90" class="ltx_Math" display="inline" id="A1.SS1.p1.8.m8.1"><semantics id="A1.SS1.p1.8.m8.1a"><mrow id="A1.SS1.p1.8.m8.1.1" xref="A1.SS1.p1.8.m8.1.1.cmml"><mi id="A1.SS1.p1.8.m8.1.1.2" xref="A1.SS1.p1.8.m8.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.8.m8.1.1.1" xref="A1.SS1.p1.8.m8.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.8.m8.1.1.3" xref="A1.SS1.p1.8.m8.1.1.3.cmml">90</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.8.m8.1b"><apply id="A1.SS1.p1.8.m8.1.1.cmml" xref="A1.SS1.p1.8.m8.1.1"><plus id="A1.SS1.p1.8.m8.1.1.1.cmml" xref="A1.SS1.p1.8.m8.1.1.1"></plus><ci id="A1.SS1.p1.8.m8.1.1.2.cmml" xref="A1.SS1.p1.8.m8.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.8.m8.1.1.3.cmml" type="integer" xref="A1.SS1.p1.8.m8.1.1.3">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.8.m8.1c">\phi+90</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.8.m8.1d">italic_ϕ + 90</annotation></semantics></math>°, <math alttext="\phi+150" class="ltx_Math" display="inline" id="A1.SS1.p1.9.m9.1"><semantics id="A1.SS1.p1.9.m9.1a"><mrow id="A1.SS1.p1.9.m9.1.1" xref="A1.SS1.p1.9.m9.1.1.cmml"><mi id="A1.SS1.p1.9.m9.1.1.2" xref="A1.SS1.p1.9.m9.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.9.m9.1.1.1" xref="A1.SS1.p1.9.m9.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.9.m9.1.1.3" xref="A1.SS1.p1.9.m9.1.1.3.cmml">150</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.9.m9.1b"><apply id="A1.SS1.p1.9.m9.1.1.cmml" xref="A1.SS1.p1.9.m9.1.1"><plus id="A1.SS1.p1.9.m9.1.1.1.cmml" xref="A1.SS1.p1.9.m9.1.1.1"></plus><ci id="A1.SS1.p1.9.m9.1.1.2.cmml" xref="A1.SS1.p1.9.m9.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.9.m9.1.1.3.cmml" type="integer" xref="A1.SS1.p1.9.m9.1.1.3">150</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.9.m9.1c">\phi+150</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.9.m9.1d">italic_ϕ + 150</annotation></semantics></math>°, <math alttext="\phi+210" class="ltx_Math" display="inline" id="A1.SS1.p1.10.m10.1"><semantics id="A1.SS1.p1.10.m10.1a"><mrow id="A1.SS1.p1.10.m10.1.1" xref="A1.SS1.p1.10.m10.1.1.cmml"><mi id="A1.SS1.p1.10.m10.1.1.2" xref="A1.SS1.p1.10.m10.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.10.m10.1.1.1" xref="A1.SS1.p1.10.m10.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.10.m10.1.1.3" xref="A1.SS1.p1.10.m10.1.1.3.cmml">210</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.10.m10.1b"><apply id="A1.SS1.p1.10.m10.1.1.cmml" xref="A1.SS1.p1.10.m10.1.1"><plus id="A1.SS1.p1.10.m10.1.1.1.cmml" xref="A1.SS1.p1.10.m10.1.1.1"></plus><ci id="A1.SS1.p1.10.m10.1.1.2.cmml" xref="A1.SS1.p1.10.m10.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.10.m10.1.1.3.cmml" type="integer" xref="A1.SS1.p1.10.m10.1.1.3">210</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.10.m10.1c">\phi+210</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.10.m10.1d">italic_ϕ + 210</annotation></semantics></math>°, <math alttext="\phi+270" class="ltx_Math" display="inline" id="A1.SS1.p1.11.m11.1"><semantics id="A1.SS1.p1.11.m11.1a"><mrow id="A1.SS1.p1.11.m11.1.1" xref="A1.SS1.p1.11.m11.1.1.cmml"><mi id="A1.SS1.p1.11.m11.1.1.2" xref="A1.SS1.p1.11.m11.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.11.m11.1.1.1" xref="A1.SS1.p1.11.m11.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.11.m11.1.1.3" xref="A1.SS1.p1.11.m11.1.1.3.cmml">270</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.11.m11.1b"><apply id="A1.SS1.p1.11.m11.1.1.cmml" xref="A1.SS1.p1.11.m11.1.1"><plus id="A1.SS1.p1.11.m11.1.1.1.cmml" xref="A1.SS1.p1.11.m11.1.1.1"></plus><ci id="A1.SS1.p1.11.m11.1.1.2.cmml" xref="A1.SS1.p1.11.m11.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.11.m11.1.1.3.cmml" type="integer" xref="A1.SS1.p1.11.m11.1.1.3">270</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.11.m11.1c">\phi+270</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.11.m11.1d">italic_ϕ + 270</annotation></semantics></math>°, <math alttext="\phi+330" class="ltx_Math" display="inline" id="A1.SS1.p1.12.m12.1"><semantics id="A1.SS1.p1.12.m12.1a"><mrow id="A1.SS1.p1.12.m12.1.1" xref="A1.SS1.p1.12.m12.1.1.cmml"><mi id="A1.SS1.p1.12.m12.1.1.2" xref="A1.SS1.p1.12.m12.1.1.2.cmml">ϕ</mi><mo id="A1.SS1.p1.12.m12.1.1.1" xref="A1.SS1.p1.12.m12.1.1.1.cmml">+</mo><mn id="A1.SS1.p1.12.m12.1.1.3" xref="A1.SS1.p1.12.m12.1.1.3.cmml">330</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.12.m12.1b"><apply id="A1.SS1.p1.12.m12.1.1.cmml" xref="A1.SS1.p1.12.m12.1.1"><plus id="A1.SS1.p1.12.m12.1.1.1.cmml" xref="A1.SS1.p1.12.m12.1.1.1"></plus><ci id="A1.SS1.p1.12.m12.1.1.2.cmml" xref="A1.SS1.p1.12.m12.1.1.2">italic-ϕ</ci><cn id="A1.SS1.p1.12.m12.1.1.3.cmml" type="integer" xref="A1.SS1.p1.12.m12.1.1.3">330</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.12.m12.1c">\phi+330</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.12.m12.1d">italic_ϕ + 330</annotation></semantics></math>°}, where <math alttext="\phi" class="ltx_Math" display="inline" id="A1.SS1.p1.13.m13.1"><semantics id="A1.SS1.p1.13.m13.1a"><mi id="A1.SS1.p1.13.m13.1.1" xref="A1.SS1.p1.13.m13.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.13.m13.1b"><ci id="A1.SS1.p1.13.m13.1.1.cmml" xref="A1.SS1.p1.13.m13.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.13.m13.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.13.m13.1d">italic_ϕ</annotation></semantics></math> represents the azimuth of the concept image.
To train our <span class="ltx_text ltx_font_italic" id="A1.SS1.p1.16.3">sparse-view 3D reconstruction model</span>, we adopt the same training set and render images from 32 randomly sampled camera views. All images are rendered at a resolution of <math alttext="512\times 512" class="ltx_Math" display="inline" id="A1.SS1.p1.14.m14.1"><semantics id="A1.SS1.p1.14.m14.1a"><mrow id="A1.SS1.p1.14.m14.1.1" xref="A1.SS1.p1.14.m14.1.1.cmml"><mn id="A1.SS1.p1.14.m14.1.1.2" xref="A1.SS1.p1.14.m14.1.1.2.cmml">512</mn><mo id="A1.SS1.p1.14.m14.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.14.m14.1.1.1.cmml">×</mo><mn id="A1.SS1.p1.14.m14.1.1.3" xref="A1.SS1.p1.14.m14.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.14.m14.1b"><apply id="A1.SS1.p1.14.m14.1.1.cmml" xref="A1.SS1.p1.14.m14.1.1"><times id="A1.SS1.p1.14.m14.1.1.1.cmml" xref="A1.SS1.p1.14.m14.1.1.1"></times><cn id="A1.SS1.p1.14.m14.1.1.2.cmml" type="integer" xref="A1.SS1.p1.14.m14.1.1.2">512</cn><cn id="A1.SS1.p1.14.m14.1.1.3.cmml" type="integer" xref="A1.SS1.p1.14.m14.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.14.m14.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.14.m14.1d">512 × 512</annotation></semantics></math>, a fixed absolute field of view (FOV) of <math alttext="30" class="ltx_Math" display="inline" id="A1.SS1.p1.15.m15.1"><semantics id="A1.SS1.p1.15.m15.1a"><mn id="A1.SS1.p1.15.m15.1.1" xref="A1.SS1.p1.15.m15.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.15.m15.1b"><cn id="A1.SS1.p1.15.m15.1.1.cmml" type="integer" xref="A1.SS1.p1.15.m15.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.15.m15.1c">30</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.15.m15.1d">30</annotation></semantics></math>°, and a fixed camera distance of <math alttext="1.866" class="ltx_Math" display="inline" id="A1.SS1.p1.16.m16.1"><semantics id="A1.SS1.p1.16.m16.1a"><mn id="A1.SS1.p1.16.m16.1.1" xref="A1.SS1.p1.16.m16.1.1.cmml">1.866</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.16.m16.1b"><cn id="A1.SS1.p1.16.m16.1.1.cmml" type="float" xref="A1.SS1.p1.16.m16.1.1">1.866</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.16.m16.1c">1.866</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.16.m16.1d">1.866</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.1">Retrieval data and method.</span> We leverage Uni3D <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib48" title="">2024</a>)</cite> to retrieve a 3D reference from an input image. In Uni3D, the latent space of the point cloud encoder is aligned to the OpenCLIP <cite class="ltx_cite ltx_citemacro_citep">(Ilharco et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib18" title="">2021</a>)</cite> image embedding space, facilitating seamless image-to-PointCloud retrieval.
Before retrieval, point clouds are sampled from meshes according to the probability distribution of face areas, ensuring denser sampling in regions with larger surface areas. Each point cloud contains 10K points. As point cloud preprocessing is time-consuming, we limit our retrieval to a subset of 40K objects from Objaverse. Our retrieval database contains precomputed embeddings generated by the Uni3D point cloud encoder, which are compared with the query vector of an input image using cosine similarity. To obtain the query vector, we first apply normalization transforms to align the input image with the pre-trained EVA02-E-14-plus model from OpenCLIP, which acts as the query encoder. The normalized image is then encoded into a feature vector. The top candidates are selected based on the highest similarity scores, and a softmax function is applied to the top-k scores to enable probabilistic sampling, ensuring efficient and accurate matching between the input image and the corresponding point clouds.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>training</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.3"><span class="ltx_text ltx_font_bold" id="A1.SS2.p1.3.1">Reference-augmented multi-view diffusion model.</span>
<span class="ltx_text ltx_font_italic" id="A1.SS2.p1.3.2">White-Background Zero123++.</span> As discussed in <span class="ltx_text" id="A1.SS2.p1.3.3" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS1" title="3.1 Reference-Augmented Multi-View Diffusion ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.1</span></a></span>, we select Zero123++ as our initial multi-view diffusion model. Upon receiving an input image, Zero123++ generates a tailored multi-view image at a resolution of <math alttext="960\times 640" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.1"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mn id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">960</mn><mo id="A1.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><times id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1"></times><cn id="A1.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1.2">960</cn><cn id="A1.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">960\times 640</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.1.m1.1d">960 × 640</annotation></semantics></math>, comprising six <math alttext="320\times 320" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2.1"><semantics id="A1.SS2.p1.2.m2.1a"><mrow id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">320</mn><mo id="A1.SS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><times id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1.1"></times><cn id="A1.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.p1.2.m2.1.1.2">320</cn><cn id="A1.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.p1.2.m2.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.2.m2.1d">320 × 320</annotation></semantics></math> views arranged in a <math alttext="3\times 2" class="ltx_Math" display="inline" id="A1.SS2.p1.3.m3.1"><semantics id="A1.SS2.p1.3.m3.1a"><mrow id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml"><mn id="A1.SS2.p1.3.m3.1.1.2" xref="A1.SS2.p1.3.m3.1.1.2.cmml">3</mn><mo id="A1.SS2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="A1.SS2.p1.3.m3.1.1.3" xref="A1.SS2.p1.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.1b"><apply id="A1.SS2.p1.3.m3.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1"><times id="A1.SS2.p1.3.m3.1.1.1.cmml" xref="A1.SS2.p1.3.m3.1.1.1"></times><cn id="A1.SS2.p1.3.m3.1.1.2.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1.2">3</cn><cn id="A1.SS2.p1.3.m3.1.1.3.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.1c">3\times 2</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.3.m3.1d">3 × 2</annotation></semantics></math> grid. The original Zero123++ produces images with a gray background, which can result in floaters and cloud-like artifacts during the subsequent sparse-view 3D reconstruction phase. To mitigate this issue, we initialize our model with a variant of Zero123++ <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#bib.bib43" title="">2024</a>)</cite>, which is finetuned to generate multi-view images with a white background.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.6"><span class="ltx_text ltx_font_italic" id="A1.SS2.p2.6.1">Training Details.</span> During the training of our reference-augmented multi-view diffusion model, we use the rendered concept image and six CCMs of a 3D object as conditions, and six corresponding target images tailored to a <math alttext="960\times 640" class="ltx_Math" display="inline" id="A1.SS2.p2.1.m1.1"><semantics id="A1.SS2.p2.1.m1.1a"><mrow id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1.cmml"><mn id="A1.SS2.p2.1.m1.1.1.2" xref="A1.SS2.p2.1.m1.1.1.2.cmml">960</mn><mo id="A1.SS2.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS2.p2.1.m1.1.1.3" xref="A1.SS2.p2.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><apply id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1"><times id="A1.SS2.p2.1.m1.1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1.1"></times><cn id="A1.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="A1.SS2.p2.1.m1.1.1.2">960</cn><cn id="A1.SS2.p2.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p2.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">960\times 640</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.1.m1.1d">960 × 640</annotation></semantics></math> image as ground truth image for denoising. All images and CCMs have a white background. We concatenate the concept image and the front-view CCM along the RGB channel as the input for meta-ControlNet. For the proposed dynamic reference routing, we dynamically downsample the original CCMs to lower resolutions and then upsample them to <math alttext="320\times 320" class="ltx_Math" display="inline" id="A1.SS2.p2.2.m2.1"><semantics id="A1.SS2.p2.2.m2.1a"><mrow id="A1.SS2.p2.2.m2.1.1" xref="A1.SS2.p2.2.m2.1.1.cmml"><mn id="A1.SS2.p2.2.m2.1.1.2" xref="A1.SS2.p2.2.m2.1.1.2.cmml">320</mn><mo id="A1.SS2.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS2.p2.2.m2.1.1.3" xref="A1.SS2.p2.2.m2.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.2.m2.1b"><apply id="A1.SS2.p2.2.m2.1.1.cmml" xref="A1.SS2.p2.2.m2.1.1"><times id="A1.SS2.p2.2.m2.1.1.1.cmml" xref="A1.SS2.p2.2.m2.1.1.1"></times><cn id="A1.SS2.p2.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.p2.2.m2.1.1.2">320</cn><cn id="A1.SS2.p2.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.p2.2.m2.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.2.m2.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.2.m2.1d">320 × 320</annotation></semantics></math>, using the nearest neighbor. Specifically, we start with a resolution of 16 at noise levels of <math alttext="[0,0.05)" class="ltx_Math" display="inline" id="A1.SS2.p2.3.m3.2"><semantics id="A1.SS2.p2.3.m3.2a"><mrow id="A1.SS2.p2.3.m3.2.3.2" xref="A1.SS2.p2.3.m3.2.3.1.cmml"><mo id="A1.SS2.p2.3.m3.2.3.2.1" stretchy="false" xref="A1.SS2.p2.3.m3.2.3.1.cmml">[</mo><mn id="A1.SS2.p2.3.m3.1.1" xref="A1.SS2.p2.3.m3.1.1.cmml">0</mn><mo id="A1.SS2.p2.3.m3.2.3.2.2" xref="A1.SS2.p2.3.m3.2.3.1.cmml">,</mo><mn id="A1.SS2.p2.3.m3.2.2" xref="A1.SS2.p2.3.m3.2.2.cmml">0.05</mn><mo id="A1.SS2.p2.3.m3.2.3.2.3" stretchy="false" xref="A1.SS2.p2.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.3.m3.2b"><interval closure="closed-open" id="A1.SS2.p2.3.m3.2.3.1.cmml" xref="A1.SS2.p2.3.m3.2.3.2"><cn id="A1.SS2.p2.3.m3.1.1.cmml" type="integer" xref="A1.SS2.p2.3.m3.1.1">0</cn><cn id="A1.SS2.p2.3.m3.2.2.cmml" type="float" xref="A1.SS2.p2.3.m3.2.2">0.05</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.3.m3.2c">[0,0.05)</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.3.m3.2d">[ 0 , 0.05 )</annotation></semantics></math> and gradually increase the resolution to 32 and 64 at noise levels of <math alttext="[0.05,0.4)" class="ltx_Math" display="inline" id="A1.SS2.p2.4.m4.2"><semantics id="A1.SS2.p2.4.m4.2a"><mrow id="A1.SS2.p2.4.m4.2.3.2" xref="A1.SS2.p2.4.m4.2.3.1.cmml"><mo id="A1.SS2.p2.4.m4.2.3.2.1" stretchy="false" xref="A1.SS2.p2.4.m4.2.3.1.cmml">[</mo><mn id="A1.SS2.p2.4.m4.1.1" xref="A1.SS2.p2.4.m4.1.1.cmml">0.05</mn><mo id="A1.SS2.p2.4.m4.2.3.2.2" xref="A1.SS2.p2.4.m4.2.3.1.cmml">,</mo><mn id="A1.SS2.p2.4.m4.2.2" xref="A1.SS2.p2.4.m4.2.2.cmml">0.4</mn><mo id="A1.SS2.p2.4.m4.2.3.2.3" stretchy="false" xref="A1.SS2.p2.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.4.m4.2b"><interval closure="closed-open" id="A1.SS2.p2.4.m4.2.3.1.cmml" xref="A1.SS2.p2.4.m4.2.3.2"><cn id="A1.SS2.p2.4.m4.1.1.cmml" type="float" xref="A1.SS2.p2.4.m4.1.1">0.05</cn><cn id="A1.SS2.p2.4.m4.2.2.cmml" type="float" xref="A1.SS2.p2.4.m4.2.2">0.4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.4.m4.2c">[0.05,0.4)</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.4.m4.2d">[ 0.05 , 0.4 )</annotation></semantics></math> and <math alttext="[0.4,1.0]" class="ltx_Math" display="inline" id="A1.SS2.p2.5.m5.2"><semantics id="A1.SS2.p2.5.m5.2a"><mrow id="A1.SS2.p2.5.m5.2.3.2" xref="A1.SS2.p2.5.m5.2.3.1.cmml"><mo id="A1.SS2.p2.5.m5.2.3.2.1" stretchy="false" xref="A1.SS2.p2.5.m5.2.3.1.cmml">[</mo><mn id="A1.SS2.p2.5.m5.1.1" xref="A1.SS2.p2.5.m5.1.1.cmml">0.4</mn><mo id="A1.SS2.p2.5.m5.2.3.2.2" xref="A1.SS2.p2.5.m5.2.3.1.cmml">,</mo><mn id="A1.SS2.p2.5.m5.2.2" xref="A1.SS2.p2.5.m5.2.2.cmml">1.0</mn><mo id="A1.SS2.p2.5.m5.2.3.2.3" stretchy="false" xref="A1.SS2.p2.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.5.m5.2b"><interval closure="closed" id="A1.SS2.p2.5.m5.2.3.1.cmml" xref="A1.SS2.p2.5.m5.2.3.2"><cn id="A1.SS2.p2.5.m5.1.1.cmml" type="float" xref="A1.SS2.p2.5.m5.1.1">0.4</cn><cn id="A1.SS2.p2.5.m5.2.2.cmml" type="float" xref="A1.SS2.p2.5.m5.2.2">1.0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.5.m5.2c">[0.4,1.0]</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.5.m5.2d">[ 0.4 , 1.0 ]</annotation></semantics></math>, respectively. For self-reference augmentations (<span class="ltx_text" id="A1.SS2.p2.6.2" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.SS4" title="A.4 Augmentation Details ‣ Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">A.4</span></a></span>), the probabilities of applying random resize, flip horizontal, grid distortion, shift, and retrieved reference are set to 0.4, 0.5, 0.1, 0.5, and 0.2, respectively. We train the model for 10,000 steps, beginning with 1000 warm-up steps with minimal augmentations. We use the AdamW optimizer with a learning rate of <math alttext="1.0\times 10^{-5}" class="ltx_Math" display="inline" id="A1.SS2.p2.6.m6.1"><semantics id="A1.SS2.p2.6.m6.1a"><mrow id="A1.SS2.p2.6.m6.1.1" xref="A1.SS2.p2.6.m6.1.1.cmml"><mn id="A1.SS2.p2.6.m6.1.1.2" xref="A1.SS2.p2.6.m6.1.1.2.cmml">1.0</mn><mo id="A1.SS2.p2.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p2.6.m6.1.1.1.cmml">×</mo><msup id="A1.SS2.p2.6.m6.1.1.3" xref="A1.SS2.p2.6.m6.1.1.3.cmml"><mn id="A1.SS2.p2.6.m6.1.1.3.2" xref="A1.SS2.p2.6.m6.1.1.3.2.cmml">10</mn><mrow id="A1.SS2.p2.6.m6.1.1.3.3" xref="A1.SS2.p2.6.m6.1.1.3.3.cmml"><mo id="A1.SS2.p2.6.m6.1.1.3.3a" xref="A1.SS2.p2.6.m6.1.1.3.3.cmml">−</mo><mn id="A1.SS2.p2.6.m6.1.1.3.3.2" xref="A1.SS2.p2.6.m6.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.6.m6.1b"><apply id="A1.SS2.p2.6.m6.1.1.cmml" xref="A1.SS2.p2.6.m6.1.1"><times id="A1.SS2.p2.6.m6.1.1.1.cmml" xref="A1.SS2.p2.6.m6.1.1.1"></times><cn id="A1.SS2.p2.6.m6.1.1.2.cmml" type="float" xref="A1.SS2.p2.6.m6.1.1.2">1.0</cn><apply id="A1.SS2.p2.6.m6.1.1.3.cmml" xref="A1.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p2.6.m6.1.1.3.1.cmml" xref="A1.SS2.p2.6.m6.1.1.3">superscript</csymbol><cn id="A1.SS2.p2.6.m6.1.1.3.2.cmml" type="integer" xref="A1.SS2.p2.6.m6.1.1.3.2">10</cn><apply id="A1.SS2.p2.6.m6.1.1.3.3.cmml" xref="A1.SS2.p2.6.m6.1.1.3.3"><minus id="A1.SS2.p2.6.m6.1.1.3.3.1.cmml" xref="A1.SS2.p2.6.m6.1.1.3.3"></minus><cn id="A1.SS2.p2.6.m6.1.1.3.3.2.cmml" type="integer" xref="A1.SS2.p2.6.m6.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.6.m6.1c">1.0\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.6.m6.1d">1.0 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and a total batch size of 48. The whole training process takes around 10 hours on 8 NVIDIA A100 (80G) GPUs.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.5"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.5.1">Sparse-view 3D reconstruction model.</span>
As discussed in <span class="ltx_text" id="A1.SS2.p3.5.2" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS5" title="3.5 Sparse-View 3D Reconstruction ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.5</span></a></span>, we employ LGM to convert the synthesized multi-view images into a 3D model. The original LGM is designed to reconstruct a 3D model from four input views at a resolution of <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1"><semantics id="A1.SS2.p3.1.m1.1a"><mrow id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1.cmml"><mn id="A1.SS2.p3.1.m1.1.1.2" xref="A1.SS2.p3.1.m1.1.1.2.cmml">256</mn><mo id="A1.SS2.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS2.p3.1.m1.1.1.3" xref="A1.SS2.p3.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b"><apply id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1"><times id="A1.SS2.p3.1.m1.1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1.1"></times><cn id="A1.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="A1.SS2.p3.1.m1.1.1.2">256</cn><cn id="A1.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p3.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.1.m1.1d">256 × 256</annotation></semantics></math>. However, this does not align with the multi-view images generated in our first stage, which consist of six views at a resolution of <math alttext="320\times 320" class="ltx_Math" display="inline" id="A1.SS2.p3.2.m2.1"><semantics id="A1.SS2.p3.2.m2.1a"><mrow id="A1.SS2.p3.2.m2.1.1" xref="A1.SS2.p3.2.m2.1.1.cmml"><mn id="A1.SS2.p3.2.m2.1.1.2" xref="A1.SS2.p3.2.m2.1.1.2.cmml">320</mn><mo id="A1.SS2.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS2.p3.2.m2.1.1.3" xref="A1.SS2.p3.2.m2.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.2.m2.1b"><apply id="A1.SS2.p3.2.m2.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1"><times id="A1.SS2.p3.2.m2.1.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1.1"></times><cn id="A1.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.p3.2.m2.1.1.2">320</cn><cn id="A1.SS2.p3.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.p3.2.m2.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.2.m2.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.2.m2.1d">320 × 320</annotation></semantics></math>. To adapt LGM to our specific inputs, we take its pretrained weights as initialization and finetune it to support six input images at <math alttext="320\times 320" class="ltx_Math" display="inline" id="A1.SS2.p3.3.m3.1"><semantics id="A1.SS2.p3.3.m3.1a"><mrow id="A1.SS2.p3.3.m3.1.1" xref="A1.SS2.p3.3.m3.1.1.cmml"><mn id="A1.SS2.p3.3.m3.1.1.2" xref="A1.SS2.p3.3.m3.1.1.2.cmml">320</mn><mo id="A1.SS2.p3.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p3.3.m3.1.1.1.cmml">×</mo><mn id="A1.SS2.p3.3.m3.1.1.3" xref="A1.SS2.p3.3.m3.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.3.m3.1b"><apply id="A1.SS2.p3.3.m3.1.1.cmml" xref="A1.SS2.p3.3.m3.1.1"><times id="A1.SS2.p3.3.m3.1.1.1.cmml" xref="A1.SS2.p3.3.m3.1.1.1"></times><cn id="A1.SS2.p3.3.m3.1.1.2.cmml" type="integer" xref="A1.SS2.p3.3.m3.1.1.2">320</cn><cn id="A1.SS2.p3.3.m3.1.1.3.cmml" type="integer" xref="A1.SS2.p3.3.m3.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.3.m3.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.3.m3.1d">320 × 320</annotation></semantics></math>.
Simultaneously changing the number of input views and image resolutions can destabilize the training process. We therefore separate the finetuning of number of input views and input resolution. Specifically, we first finetune the model with six input views at the original resolution for 60 epochs and then further finetune the model at a higher resolution of <math alttext="320\times 320" class="ltx_Math" display="inline" id="A1.SS2.p3.4.m4.1"><semantics id="A1.SS2.p3.4.m4.1a"><mrow id="A1.SS2.p3.4.m4.1.1" xref="A1.SS2.p3.4.m4.1.1.cmml"><mn id="A1.SS2.p3.4.m4.1.1.2" xref="A1.SS2.p3.4.m4.1.1.2.cmml">320</mn><mo id="A1.SS2.p3.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p3.4.m4.1.1.1.cmml">×</mo><mn id="A1.SS2.p3.4.m4.1.1.3" xref="A1.SS2.p3.4.m4.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.4.m4.1b"><apply id="A1.SS2.p3.4.m4.1.1.cmml" xref="A1.SS2.p3.4.m4.1.1"><times id="A1.SS2.p3.4.m4.1.1.1.cmml" xref="A1.SS2.p3.4.m4.1.1.1"></times><cn id="A1.SS2.p3.4.m4.1.1.2.cmml" type="integer" xref="A1.SS2.p3.4.m4.1.1.2">320</cn><cn id="A1.SS2.p3.4.m4.1.1.3.cmml" type="integer" xref="A1.SS2.p3.4.m4.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.4.m4.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.4.m4.1d">320 × 320</annotation></semantics></math> for another 60 epochs. The finetuning process is conducted on 32 NVIDIA A100 (80G) GPUs using the AdamW optimizer with a learning rate of <math alttext="2.0\times 10^{-4}" class="ltx_Math" display="inline" id="A1.SS2.p3.5.m5.1"><semantics id="A1.SS2.p3.5.m5.1a"><mrow id="A1.SS2.p3.5.m5.1.1" xref="A1.SS2.p3.5.m5.1.1.cmml"><mn id="A1.SS2.p3.5.m5.1.1.2" xref="A1.SS2.p3.5.m5.1.1.2.cmml">2.0</mn><mo id="A1.SS2.p3.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p3.5.m5.1.1.1.cmml">×</mo><msup id="A1.SS2.p3.5.m5.1.1.3" xref="A1.SS2.p3.5.m5.1.1.3.cmml"><mn id="A1.SS2.p3.5.m5.1.1.3.2" xref="A1.SS2.p3.5.m5.1.1.3.2.cmml">10</mn><mrow id="A1.SS2.p3.5.m5.1.1.3.3" xref="A1.SS2.p3.5.m5.1.1.3.3.cmml"><mo id="A1.SS2.p3.5.m5.1.1.3.3a" xref="A1.SS2.p3.5.m5.1.1.3.3.cmml">−</mo><mn id="A1.SS2.p3.5.m5.1.1.3.3.2" xref="A1.SS2.p3.5.m5.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.5.m5.1b"><apply id="A1.SS2.p3.5.m5.1.1.cmml" xref="A1.SS2.p3.5.m5.1.1"><times id="A1.SS2.p3.5.m5.1.1.1.cmml" xref="A1.SS2.p3.5.m5.1.1.1"></times><cn id="A1.SS2.p3.5.m5.1.1.2.cmml" type="float" xref="A1.SS2.p3.5.m5.1.1.2">2.0</cn><apply id="A1.SS2.p3.5.m5.1.1.3.cmml" xref="A1.SS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p3.5.m5.1.1.3.1.cmml" xref="A1.SS2.p3.5.m5.1.1.3">superscript</csymbol><cn id="A1.SS2.p3.5.m5.1.1.3.2.cmml" type="integer" xref="A1.SS2.p3.5.m5.1.1.3.2">10</cn><apply id="A1.SS2.p3.5.m5.1.1.3.3.cmml" xref="A1.SS2.p3.5.m5.1.1.3.3"><minus id="A1.SS2.p3.5.m5.1.1.3.3.1.cmml" xref="A1.SS2.p3.5.m5.1.1.3.3"></minus><cn id="A1.SS2.p3.5.m5.1.1.3.3.2.cmml" type="integer" xref="A1.SS2.p3.5.m5.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.5.m5.1c">2.0\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.5.m5.1d">2.0 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and a total batch size of 192. The whole finetuning process takes around four days.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Meta-controlnet</h3>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="574" id="A1.F12.g1" src="extracted/5861237/Figures/detailed_meta-controlnet.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Detailed architecture design of meta-ControlNet.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">A detailed figure of the proposed meta-ControlNet in the style of vanilla ControlNet is shown in <span class="ltx_text" id="A1.SS3.p1.1.1" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.F12" title="Figure 12 ‣ A.3 Meta-controlnet ‣ Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">12</span></a></span>, where <math alttext="\bm{c}_{pair}" class="ltx_Math" display="inline" id="A1.SS3.p1.1.m1.1"><semantics id="A1.SS3.p1.1.m1.1a"><msub id="A1.SS3.p1.1.m1.1.1" xref="A1.SS3.p1.1.m1.1.1.cmml"><mi id="A1.SS3.p1.1.m1.1.1.2" xref="A1.SS3.p1.1.m1.1.1.2.cmml">𝒄</mi><mrow id="A1.SS3.p1.1.m1.1.1.3" xref="A1.SS3.p1.1.m1.1.1.3.cmml"><mi id="A1.SS3.p1.1.m1.1.1.3.2" xref="A1.SS3.p1.1.m1.1.1.3.2.cmml">p</mi><mo id="A1.SS3.p1.1.m1.1.1.3.1" xref="A1.SS3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A1.SS3.p1.1.m1.1.1.3.3" xref="A1.SS3.p1.1.m1.1.1.3.3.cmml">a</mi><mo id="A1.SS3.p1.1.m1.1.1.3.1a" xref="A1.SS3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A1.SS3.p1.1.m1.1.1.3.4" xref="A1.SS3.p1.1.m1.1.1.3.4.cmml">i</mi><mo id="A1.SS3.p1.1.m1.1.1.3.1b" xref="A1.SS3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="A1.SS3.p1.1.m1.1.1.3.5" xref="A1.SS3.p1.1.m1.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.1.m1.1b"><apply id="A1.SS3.p1.1.m1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS3.p1.1.m1.1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS3.p1.1.m1.1.1.2.cmml" xref="A1.SS3.p1.1.m1.1.1.2">𝒄</ci><apply id="A1.SS3.p1.1.m1.1.1.3.cmml" xref="A1.SS3.p1.1.m1.1.1.3"><times id="A1.SS3.p1.1.m1.1.1.3.1.cmml" xref="A1.SS3.p1.1.m1.1.1.3.1"></times><ci id="A1.SS3.p1.1.m1.1.1.3.2.cmml" xref="A1.SS3.p1.1.m1.1.1.3.2">𝑝</ci><ci id="A1.SS3.p1.1.m1.1.1.3.3.cmml" xref="A1.SS3.p1.1.m1.1.1.3.3">𝑎</ci><ci id="A1.SS3.p1.1.m1.1.1.3.4.cmml" xref="A1.SS3.p1.1.m1.1.1.3.4">𝑖</ci><ci id="A1.SS3.p1.1.m1.1.1.3.5.cmml" xref="A1.SS3.p1.1.m1.1.1.3.5">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.1.m1.1c">\bm{c}_{pair}</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.p1.1.m1.1d">bold_italic_c start_POSTSUBSCRIPT italic_p italic_a italic_i italic_r end_POSTSUBSCRIPT</annotation></semantics></math> is a pair of the concept image and the front-view reference CCM.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Augmentation Details</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We implement a series of augmentations to facilitate the training of our diffusion model in a self-reference manner, where the ground truth 3D model serves as its own reference. These augmentations are designed to simulate the misalignment between the 3D reference and the concept image.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p2.1.1">Resize and horizontal flip.</span> Due to the self-reference strategy, reference CCMs are always pixel-wise aligned with the concept image. However, during inference, references often differ in scale or exhibit mirror symmetry. For example, a reference 3D character might hold a weapon in the opposite hand compared to the concept image. To address this, we apply random resizing and horizontal flipping to the reference model, simulating scale variations and mirror-symmetric structures.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p3.1.1">Grid distortion and shift.</span> During inference, the reference may exhibit asymmetric similarity with the target 3D model across different views. For instance, a reference building might closely resemble the concept image from the front but differ significantly from the side. To address this, we apply multi-view jitter through grid distortion and shifting. Specifically, we independently distort and shift each view of the reference CCMs using a random grid and a random shift offset during training, simulating such asymmetric similarity across views.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS4.p4">
<p class="ltx_p" id="A1.SS4.p4.1"><span class="ltx_text ltx_font_italic" id="A1.SS4.p4.1.1">Retrieved Reference.</span> Although the retrieved 3D reference alone is insufficient for model training, as discussed in <span class="ltx_text" id="A1.SS4.p4.1.2" style="color:#000000;">Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S3.SS4" title="3.4 Self-Reference Augmentation ‣ 3 Approach ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">3.4</span></a></span>, it can still serve as a strong augmentation to simulate significant misalignment. Therefore, we assign a small probability of using the retrieved model as the reference during training.</p>
</div>
<figure class="ltx_figure" id="A1.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="694" id="A1.F13.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Analysis on different input viewpoints. We compare the performance of <span class="ltx_text ltx_font_italic" id="A1.F13.2.1">Phidias</span> with five baseline methods by reconstructing 3D objects from video frames with various viewpoints. For each case, we show two rendered images at novel views.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Limitation and Failure Cases</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Despite promising results, <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Phidias</span> still has several limitations for further improvement. As a retrieval-augmented generation model, the performance can be affected by the retrieval method and the scale and quality of 3D reference database. Currently, the 3D database we used for retrieval only consists of 40K objects, making it difficult to find a very similar match. Also, mainstream 3D retrieval methods rely on semantic similarity, which may not always yield the best match. For example, retrieved reference models with misaligned poses or structures can lead to undesired outcomes, as shown in <span class="ltx_text" id="A2.p1.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.F14" title="Figure 14 ‣ C.2 More Results ‣ Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">14</span></a></span>. Future works that improve the retrieval accuracy and expand the 3D reference database could mitigate these issues. Additionally, the limited resolution of the backbone multi-view diffusion model (<math alttext="320\times 320" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mn id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">320</mn><mo id="A2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.p1.1.m1.1.1.1.cmml">×</mo><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><times id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></times><cn id="A2.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.p1.1.m1.1.1.2">320</cn><cn id="A2.p1.1.m1.1.1.3.cmml" type="integer" xref="A2.p1.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">320\times 320</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">320 × 320</annotation></semantics></math>) restricts the handling of high-resolution images. Enhancing the resolution of the diffusion model could further improve the quality of the generated 3D models.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Results</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Additional Analysis on Enhanced Generalization Ability</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.1">Phidias</span> takes an additional 3D reference as input to improve generative quality (<span class="ltx_text" id="A3.SS1.p1.1.2" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">5</span></a></span>) and provide greater controllability (<span class="ltx_text" id="A3.SS1.p1.1.3" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">4</span></a></span>) for 3D generation. We argue that <span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.4">Phidias</span> can also enhance generalization ability when given input images from atypical viewpoints. When reconstructing 3D objects from video frames with varying views (<span class="ltx_text" id="A3.SS1.p1.1.5" style="color:#000000;">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A1.F13" title="Figure 13 ‣ A.4 Augmentation Details ‣ Appendix A Implementation Details ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">13</span></a></span>), we observe that the baseline methods perform well with typical view angles (<span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.6">i.e., </span>frame 1) but struggle with atypical input view angles (<span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.7">e.g., </span>frame 3 and 4). Conversely, <span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.8">Phidias</span> produces plausible results given all four input views, demonstrating robust generalization ability across both typical and atypical viewpoints.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>More Results</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">More results on theme-aware 3D-to-3D generation are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.F15" title="Figure 15 ‣ C.2 More Results ‣ Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">15</span></a>. More results on text-to-3D and image-to-3D generation are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.F16" title="Figure 16 ‣ C.2 More Results ‣ Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">16</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11406v1#A3.F17" title="Figure 17 ‣ C.2 More Results ‣ Appendix C Additional Results ‣ Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<figure class="ltx_figure" id="A3.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="138" id="A3.F14.g1" src="x8.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Failure cases. There are two typical failure cases due to bad retrieval: (a) misaligned pose and (b) misaligned structure.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1079" id="A3.F15.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Additional results on theme-aware 3D-to-3D generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="A3.F16.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Additional results on retrieval-augmented text-to-3D generation.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="889" id="A3.F17.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Additional results on retrieval-augmented image-to-3D generation.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 17:28:52 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
