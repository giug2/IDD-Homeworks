<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.14362] From Distributed Machine Learning to Federated Learning: A Survey</title><meta property="og:description" content="In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of laws or regulations, the distributed data and computing resources cannot…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Distributed Machine Learning to Federated Learning: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="From Distributed Machine Learning to Federated Learning: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.14362">

<!--Generated on Sun Mar 17 06:19:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning Distributed system Parallel computing Security,  Privacy">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">∎


</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
<span id="footnotex10" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> Corresponding author. 
<br class="ltx_break"><span id="footnotex11" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span> Baidu Inc., Beijing, China.
<br class="ltx_break"><span id="footnotex12" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">6</span></span></span></span> Computer Science and Software Engineering Department, Auburn University, Alabama, United States.
<br class="ltx_break"><span id="footnotex13" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">7</span></span></span></span> Computer and Information Science Department, University of Oregon, United States.
</span></span></span>
<h1 class="ltx_title ltx_title_document">From Distributed Machine Learning to Federated Learning: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ji Liu<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jizhou Huang<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yang Zhou<span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">6</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xuhong Li<span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shilei Ji<span id="footnotex5" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Haoyi Xiong<span id="footnotex6" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Dejing Dou<span id="footnotex7" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span><span id="footnotex8" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">5</span></span></span></span><span id="footnotex9" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">7</span></span></span></span>
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of laws or regulations, the distributed data and computing resources cannot be <span id="id1.id1.1" class="ltx_text" style="color:#000000;">aggregated or</span> directly shared among different regions or organizations for machine learning tasks. Federated learning emerges as an efficient approach to exploit distributed data and computing resources, so as to collaboratively train machine learning models<span id="id1.id1.2" class="ltx_text" style="color:#000000;">. At the same time, federated learning obeys</span> the laws and regulations and ensures data security and data privacy. In this paper, we provide a comprehensive survey of existing works for federated learning. <span id="id1.id1.3" class="ltx_text" style="color:#000000;">First, we propose a functional architecture of federated learning systems and a taxonomy of related techniques. Second, we explain the federated learning systems from four aspects: diverse types of parallelism, aggregation algorithms, data communication, and the security of federated learning systems. Third, we present four widely used federated systems based on the functional architecture. Finally, we summarize the limitations and propose future research directions.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated learning Distributed system Parallel computing Security, Privacy
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">{textblock*}</span>
<p id="p2.2" class="ltx_p">8cm(3cm,15cm) <span id="p2.2.1" class="ltx_text" style="font-size:207%;">To appear in KAIS</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With billions of connected Internet of Things (IoT) devices <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">abou2019co </a></cite>, smartphones <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib132" title="" class="ltx_ref">ochiai2019real </a></cite> and large websites around the world, in recent years, we have witnessed huge amounts of data generated and dispersed over various mobile devices of end users, or the data centers of different organizations. As the data contain sensitive information of end users or organizations, such as facial images, location-based services, health information <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib114" title="" class="ltx_ref">lyu2020threats </a></cite>, or personal economic status, moving the raw data from personal devices or data centers of multiple organizations to a centralized server or data center may pose immediate or potential information leakage. Due to the concerns of data security and data privacy, legal restrictions, such as the Cybersecurity Law of the People’s Republic (CLPR) of China <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib160" title="" class="ltx_ref">CCL </a></cite>, the General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib133" title="" class="ltx_ref">GDPR </a></cite> in European Union, the Personal Data Protection Act (PDP) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">chik2013singapore </a></cite> in Singapore, the California Consumer Privacy Act (CCPA) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">CCPA </a></cite>, and the Consumer Privacy Bill of Rights (CPBR) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">Gaff2014 </a></cite> in the United States, have been introduced and put in practice, which makes data aggregation from distributed devices, multiple regions, or organizations, almost impossible <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>. In addition, computing and storage resources are also typically distributed in multiple regions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib106" title="" class="ltx_ref">liu2018efficient </a></cite> and organizations <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib98" title="" class="ltx_ref">liaqat2017federated </a></cite>, which cannot be aggregated in a single data center.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) emerges as an efficient approach to exploit the distributed resources to collaboratively train a machine learning model. FL is a distributed machine learning approach where multiple users collaboratively train a model, while keeping the raw data decentralized without being moved to a single server or data center <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a>; <a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>.
FL not only exploits the distributed resources to efficiently carry out the training process of machine learning, but also promises to provide security and privacy for the decentralized raw data. Within FL, the raw data, or the data generated based on the raw data with security processing, serves as the training data. FL only allows the intermediate data to be transferred among the distributed computing resources while avoiding the transfer of training data.
The distributed computing resources refer to mobile devices of end users or servers of multiple organizations.
FL brings the code to the data, instead of bringing the data to the code, and it addresses the fundamental problems of privacy, ownership, and locality of data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite>.
In this way, FL can enable multiple users to train a model while satisfying the legal data restrictions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Traditional centralized machine learning approaches typically gather the distributed raw data generated on different devices or organizations to a single server or a cluster with shared data storage, which may bring serious data privacy and security concerns <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>.
The centralized approaches, in general, are associated with diverse challenges, including computational power and training time, and most importantly, security and privacy with respect to distributed data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib129" title="" class="ltx_ref">mothukuri2021survey </a></cite>.
FL differs from the centralized approach in three aspects. First, FL does not allow direct raw data communication, while the centralized approach has no restriction. Second, FL exploits the distributed computing resources in multiple regions or organizations, while the centralized approach generally only utilizes a single server or a cluster in a single region, which belongs to a single organization. Third, FL generally takes advantage of encryption or other defense techniques to ensure the data privacy or security, while the centralized approach pays little attention to these security issues <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The term “federated learning” was first introduced in 2016 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite>, which focuses on the unbalanced and non-Independent and Identically Distributed (non-IID) data in mobile devices.
The concept of FL was extended to three data scenarios, i.e., horizontal, vertical, and hybrid <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a>; <a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>.
The horizontal FL addresses the decentralized data of the same features, while the identifications are different. The vertical FL handles the decentralized data of the same identifications with different features. The hybrid FL deals with the data of different identifications and different features. Then, FL is formally defined as a machine learning approach where multiple clients collaborate in solving a machine learning problem while the raw data is stored locally and is neither exchanged nor transferred <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">An FL system is an efficient tool to carry out FL with decentralized data and resources. Several open-source FL systems, e.g., FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref">FATE </a></cite>, PaddleFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">PaddleFL </a></cite>, TensorflowFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">TFF </a></cite>, and Pysyft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref">Pysyft </a></cite>, are now intensively used by both research communities, e.g., healthcare <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref">brisimi2018federated </a></cite>, and computer visions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib109" title="" class="ltx_ref">liu2020fedvision </a>; <a href="#bib.bib66" title="" class="ltx_ref">hefedcv </a></cite>, and by industrial groups, e.g., WeBank <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib183" title="" class="ltx_ref">FLWhitePaper </a></cite>. Although various FL systems exist, the architecture of FL systems has common features: In particular, they share the capability to collaboratively train a machine learning model. Most FL systems are composed of four layers, i.e., presentation, user services, FL training, and infrastructure. These four layers enable FL system users to design, execute, and analyze machine learning models with distributed data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Although FL differs from the centralized machine learning approaches, it not only utilizes novel techniques designed for FL, but also takes advantage of the techniques designed for distributed machine learning.
FL exploits parallelization techniques designed for distributed machine learning.
For instance, horizontal FL exploits the data parallelism, which trains multiple instances of the same model on different subsets of the training dataset <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>. Vertical FL utilizes model parallelism to distribute parallel paths of a single model to multiple devices in order to handle the data of different features <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>.
Multiple aggregation algorithms <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">chen2019communication </a></cite> are proposed to aggregate the models in distributed computing resources.
Data transfer techniques are also utilized in FL, e.g., model compression <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">caldas2018expanding </a></cite>.
As FL promises to provide data security and data privacy, diverse defense techniques, e.g., differential privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib121" title="" class="ltx_ref">mcmahan2017learning </a></cite>, homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">hardy2017private </a></cite>, and Robustness Aggregation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib142" title="" class="ltx_ref">pillutla2019robust </a></cite>, are designed to address the possible attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib181" title="" class="ltx_ref">wang2019beyond </a>; <a href="#bib.bib71" title="" class="ltx_ref">hitaj2017deep </a>; <a href="#bib.bib49" title="" class="ltx_ref">geiping2020inverting </a></cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">There have been a few surveys of FL. Some works <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a>; <a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a>; <a href="#bib.bib89" title="" class="ltx_ref">li2019survey </a></cite> provide a comprehensive study of FL, from the taxonomy of FL to the techniques, e.g., the efficiency, data privacy, security, and applications of FL. Some surveys <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib129" title="" class="ltx_ref">mothukuri2021survey </a>; <a href="#bib.bib89" title="" class="ltx_ref">li2019survey </a>; <a href="#bib.bib114" title="" class="ltx_ref">lyu2020threats </a></cite> focus on the data privacy and security of FL. Other surveys present the application of FL in a specific area, e.g., healthcare informatics <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib188" title="" class="ltx_ref">xu2020federated </a></cite>, mobile edge networks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib99" title="" class="ltx_ref">lim2020federated </a></cite>, and neural architecture searches <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>, and they personalize global models to work better for individual clients <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib87" title="" class="ltx_ref">kulkarni2020survey </a></cite>. However, few of them present the architecture of FL or analyze parallelization techniques in FL.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In this paper, we provide a survey of federated learning and the related parallelization techniques. The main contributions of this paper are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A four-layer FL system architecture, which is useful for discussing the techniques for FL. This architecture can also be a baseline for other work and can help with the assessment and comparison of FL systems.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A taxonomy of FL-related techniques, including the parallelization techniques, the aggregation algorithms, and the techniques for data communication and security, with a comparative analysis of the existing solutions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A discussion of research issues to improve the efficiency and security of FL systems.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">This paper is organized as follows. Section <a href="#S2" title="2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of the execution of FL, including the FL system architectures and basic functional architecture of FL systems. Section <a href="#S3" title="3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> focuses on the techniques used for distributed training of FL and aggregation methods. Section <a href="#S4" title="4 Data Manipulation ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the techniques for distributed execution, data communication and data security of FL. Section <a href="#S5" title="5 Federated Learning Frameworks ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates the existing FL frameworks.
Section <a href="#S6" title="6 Research Directions ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> discusses the open issues raised for the execution of FL with distributed resources. Section <a href="#S7" title="7 Conclusion ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes the main findings of this study.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>An Overview of Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we introduce the basic concepts of federated learning. Then, we present the life cycle of FL models. Afterwards, we detail the functional architecture and the corresponding functionality of FL systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Basic Concepts</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Machine learning is the process to automatically extract the models or patterns from data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib53" title="" class="ltx_ref">goodfellow2016deep </a></cite>.
The models or patterns are expressed as machine learning models.
A machine learning model is an ensemble of a model structure, which is typically expressed as a Directed Acyclic Graph (DAG), data processing units, e.g., activation functions in Deep Neural Networks (DNNs), and the associated parameters or hyper-parameters.
The input data can be processed through a machine learning model to generate the output, e.g., the prediction results or the classification results, which is the inference process.
The machine learning model is generated based on the training data, which is the training process.
During the training process, the parameters or the model structure of the machine learning model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib68" title="" class="ltx_ref">He2020CVPR </a>; <a href="#bib.bib61" title="" class="ltx_ref">he2020towards </a></cite> is adjusted based on a training algorithm in order to improve the performance, e.g., the accuracy or the generalization capacity. The training algorithm is also denoted by machine learning algorithms. The duration of the training process is training time.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.5" class="ltx_p">According to whether the training data have labels, the training process of machine learning can be classified into four types <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>, i.e., supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. Supervised learning represents that a machine learning task exploits the training data composed of input features and the corresponding labels <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>. In this paper, we focus on this type of training data. For instance, each data point in the training dataset contains <math id="S2.SS1.p2.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S2.SS1.p2.1.m1.2a"><mrow id="S2.SS1.p2.1.m1.2.3.2" xref="S2.SS1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.2.3.2.1" xref="S2.SS1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">x</mi><mo id="S2.SS1.p2.1.m1.2.3.2.2" xref="S2.SS1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.p2.1.m1.2.3.2.3" xref="S2.SS1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.2b"><interval closure="open" id="S2.SS1.p2.1.m1.2.3.1.cmml" xref="S2.SS1.p2.1.m1.2.3.2"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑥</ci><ci id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.2c">(x,y)</annotation></semantics></math>, where <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">x</annotation></semantics></math> represents the input features and <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">y</annotation></semantics></math> represents the desired output value. Unsupervised learning represents that a machine learning task exploits the training data, which only consists of input features without output values; i.e., each data point only contains <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">x</annotation></semantics></math> and does not have <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">y</annotation></semantics></math>. Semi-supervised learning represents that one (generally small) part of the training data contains output values, while the other (generally small) part of the training does not. Reinforcement learning represents that each iteration in the training process considers its observation of the environment from the last iteration.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">While the training data become huge, e.g., on the order of terabyte <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">canini2012sibyl </a></cite>, or when the training data is inherently distributed or too big to store on single machines <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>, the training process is carried out using distributed resources, which is distributed machine learning. One of the important features of the distributed machine learning is that it can significantly accelerate the training speed so as to reduce the training time. Diverse parallelization techniques are used in distributed machine learning. For instance, Graphics Processing Units (GPUs) using Single Instruction Multiple Data (SIMD) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">flynn1972some </a></cite> and Tensor Processing Units (TPUs) using Multiple Instructions Multiple Data (MIMD) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">flynn1972some </a></cite> are exploited <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>. In addition, distributed machine learning takes advantage of three types of parallelism to parallelize the training process, i.e., data parallelism <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>, model parallelism <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>, and pipeline parallelism <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib74" title="" class="ltx_ref">huang2018gpipe </a>; <a href="#bib.bib64" title="" class="ltx_ref">He2021PipeTransformer </a>; <a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite>. With the data parallelism approach, the training data is partitioned as many times as the number of computing resources, and all computing resources subsequently apply the same machine learning algorithm to process different chunks of the data sets <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>. With the model parallelism approach, exact copies of the entirety of the data (the training data or the intermediate data)
are processed by each computing resource, each of which exploits different parts of the machine learning model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a></cite>. The pipeline parallelism approach combines the data parallelism and the model parallelism. With this approach, each computing resource processes a part of the training data with a part of the machine learning model, while the processing, e.g., computation or communication, at each node can be parallelized <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib131" title="" class="ltx_ref">narayanan2019pipedream </a></cite>.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">FL is a distributed machine learning approach where multiple users collaboratively train a model, while keeping the raw data distributed without being moved to a single server or data center <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a>; <a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>.
The model used for FL is denoted by FL model.
FL is first proposed to handle the unbalanced and non-Independent and Identically Distributed (non-IID) data of the same features in mobile devices <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite>.
Then, the concept of FL is extended to the distributed data of diverse features in multiple organizations <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite> or various regions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib80" title="" class="ltx_ref">mcmahan2021advances </a></cite>.
FL systems are used within one or multiple phases of the life cycle of FL models. An FL system is a distributed system to manage the distributed training process with distributed resources.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">FL is a special type of distributed machine learning, which differs from other distributed machine learning approaches in the following three points.
First, FL does not allow direct raw data communication, while other approaches have no restriction.
As the raw data are of multiple ownerships, FL approaches with this restriction can meet the requirements defined by the related laws, e.g., CLPR <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib160" title="" class="ltx_ref">CCL </a></cite>, GDPR <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib133" title="" class="ltx_ref">GDPR </a></cite>, PDPA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">chik2013singapore </a></cite>, CCPA <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">CCPA </a></cite>, and CPBR <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">Gaff2014 </a></cite>.
In particular, the consent (GDPR Article 6) and the data minimalization principle (GDPR Article 5) limit data collection and storage to only what is consumer-consented and what is absolutely necessary for processing <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib99" title="" class="ltx_ref">lim2020federated </a></cite>.
Second, FL exploits the distributed computing resources in multiple regions or organizations, while the other approaches generally only utilize a single server or a cluster in a single region, which belongs to a single organization. FL enables the collaboration among multiple organizations.
Third, FL generally takes advantage of encryption or other defense techniques to ensure the data privacy or security, while the other approaches pay little attention to this security issue <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>. FL promises to ensure the privacy and security of the raw data, as the leakage of information may incur significant financial <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">UberLoss </a>; <a href="#bib.bib150" title="" class="ltx_ref">GoogleLoss </a></cite> and reputational <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib158" title="" class="ltx_ref">FacebookLoss </a></cite> losses.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.6" class="ltx_p">During the training process of FL, an optimization problem is solved as shown in Formula <a href="#S2.E1" title="In 2.1 Basic Concepts ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Given <math id="S2.SS1.p6.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p6.1.m1.1a"><mi id="S2.SS1.p6.1.m1.1.1" xref="S2.SS1.p6.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.1.m1.1b"><ci id="S2.SS1.p6.1.m1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.1.m1.1c">n</annotation></semantics></math> training dataset <math id="S2.SS1.p6.2.m2.4" class="ltx_Math" alttext="\mathcal{D}={D_{1},D_{2},...,D_{n}}" display="inline"><semantics id="S2.SS1.p6.2.m2.4a"><mrow id="S2.SS1.p6.2.m2.4.4" xref="S2.SS1.p6.2.m2.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p6.2.m2.4.4.5" xref="S2.SS1.p6.2.m2.4.4.5.cmml">𝒟</mi><mo id="S2.SS1.p6.2.m2.4.4.4" xref="S2.SS1.p6.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS1.p6.2.m2.4.4.3.3" xref="S2.SS1.p6.2.m2.4.4.3.4.cmml"><msub id="S2.SS1.p6.2.m2.2.2.1.1.1" xref="S2.SS1.p6.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS1.p6.2.m2.2.2.1.1.1.2" xref="S2.SS1.p6.2.m2.2.2.1.1.1.2.cmml">D</mi><mn id="S2.SS1.p6.2.m2.2.2.1.1.1.3" xref="S2.SS1.p6.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p6.2.m2.4.4.3.3.4" xref="S2.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p6.2.m2.3.3.2.2.2" xref="S2.SS1.p6.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS1.p6.2.m2.3.3.2.2.2.2" xref="S2.SS1.p6.2.m2.3.3.2.2.2.2.cmml">D</mi><mn id="S2.SS1.p6.2.m2.3.3.2.2.2.3" xref="S2.SS1.p6.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p6.2.m2.4.4.3.3.5" xref="S2.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p6.2.m2.1.1" xref="S2.SS1.p6.2.m2.1.1.cmml">…</mi><mo id="S2.SS1.p6.2.m2.4.4.3.3.6" xref="S2.SS1.p6.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p6.2.m2.4.4.3.3.3" xref="S2.SS1.p6.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS1.p6.2.m2.4.4.3.3.3.2" xref="S2.SS1.p6.2.m2.4.4.3.3.3.2.cmml">D</mi><mi id="S2.SS1.p6.2.m2.4.4.3.3.3.3" xref="S2.SS1.p6.2.m2.4.4.3.3.3.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.2.m2.4b"><apply id="S2.SS1.p6.2.m2.4.4.cmml" xref="S2.SS1.p6.2.m2.4.4"><eq id="S2.SS1.p6.2.m2.4.4.4.cmml" xref="S2.SS1.p6.2.m2.4.4.4"></eq><ci id="S2.SS1.p6.2.m2.4.4.5.cmml" xref="S2.SS1.p6.2.m2.4.4.5">𝒟</ci><list id="S2.SS1.p6.2.m2.4.4.3.4.cmml" xref="S2.SS1.p6.2.m2.4.4.3.3"><apply id="S2.SS1.p6.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p6.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p6.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p6.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS1.p6.2.m2.2.2.1.1.1.2">𝐷</ci><cn type="integer" id="S2.SS1.p6.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS1.p6.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p6.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.p6.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS1.p6.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p6.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS1.p6.2.m2.3.3.2.2.2.2">𝐷</ci><cn type="integer" id="S2.SS1.p6.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS1.p6.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p6.2.m2.1.1.cmml" xref="S2.SS1.p6.2.m2.1.1">…</ci><apply id="S2.SS1.p6.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.p6.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS1.p6.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p6.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS1.p6.2.m2.4.4.3.3.3.2">𝐷</ci><ci id="S2.SS1.p6.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS1.p6.2.m2.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.2.m2.4c">\mathcal{D}={D_{1},D_{2},...,D_{n}}</annotation></semantics></math>, where each data point <math id="S2.SS1.p6.3.m3.2" class="ltx_Math" alttext="(x,y)\sim\mathcal{D}" display="inline"><semantics id="S2.SS1.p6.3.m3.2a"><mrow id="S2.SS1.p6.3.m3.2.3" xref="S2.SS1.p6.3.m3.2.3.cmml"><mrow id="S2.SS1.p6.3.m3.2.3.2.2" xref="S2.SS1.p6.3.m3.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS1.p6.3.m3.2.3.2.2.1" xref="S2.SS1.p6.3.m3.2.3.2.1.cmml">(</mo><mi id="S2.SS1.p6.3.m3.1.1" xref="S2.SS1.p6.3.m3.1.1.cmml">x</mi><mo id="S2.SS1.p6.3.m3.2.3.2.2.2" xref="S2.SS1.p6.3.m3.2.3.2.1.cmml">,</mo><mi id="S2.SS1.p6.3.m3.2.2" xref="S2.SS1.p6.3.m3.2.2.cmml">y</mi><mo stretchy="false" id="S2.SS1.p6.3.m3.2.3.2.2.3" xref="S2.SS1.p6.3.m3.2.3.2.1.cmml">)</mo></mrow><mo id="S2.SS1.p6.3.m3.2.3.1" xref="S2.SS1.p6.3.m3.2.3.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p6.3.m3.2.3.3" xref="S2.SS1.p6.3.m3.2.3.3.cmml">𝒟</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.3.m3.2b"><apply id="S2.SS1.p6.3.m3.2.3.cmml" xref="S2.SS1.p6.3.m3.2.3"><csymbol cd="latexml" id="S2.SS1.p6.3.m3.2.3.1.cmml" xref="S2.SS1.p6.3.m3.2.3.1">similar-to</csymbol><interval closure="open" id="S2.SS1.p6.3.m3.2.3.2.1.cmml" xref="S2.SS1.p6.3.m3.2.3.2.2"><ci id="S2.SS1.p6.3.m3.1.1.cmml" xref="S2.SS1.p6.3.m3.1.1">𝑥</ci><ci id="S2.SS1.p6.3.m3.2.2.cmml" xref="S2.SS1.p6.3.m3.2.2">𝑦</ci></interval><ci id="S2.SS1.p6.3.m3.2.3.3.cmml" xref="S2.SS1.p6.3.m3.2.3.3">𝒟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.3.m3.2c">(x,y)\sim\mathcal{D}</annotation></semantics></math>, the problem of FL is to learn a function <math id="S2.SS1.p6.4.m4.1" class="ltx_Math" alttext="\widehat{F}" display="inline"><semantics id="S2.SS1.p6.4.m4.1a"><mover accent="true" id="S2.SS1.p6.4.m4.1.1" xref="S2.SS1.p6.4.m4.1.1.cmml"><mi id="S2.SS1.p6.4.m4.1.1.2" xref="S2.SS1.p6.4.m4.1.1.2.cmml">F</mi><mo id="S2.SS1.p6.4.m4.1.1.1" xref="S2.SS1.p6.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.4.m4.1b"><apply id="S2.SS1.p6.4.m4.1.1.cmml" xref="S2.SS1.p6.4.m4.1.1"><ci id="S2.SS1.p6.4.m4.1.1.1.cmml" xref="S2.SS1.p6.4.m4.1.1.1">^</ci><ci id="S2.SS1.p6.4.m4.1.1.2.cmml" xref="S2.SS1.p6.4.m4.1.1.2">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.4.m4.1c">\widehat{F}</annotation></semantics></math> from all possible hypotheses <math id="S2.SS1.p6.5.m5.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S2.SS1.p6.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p6.5.m5.1.1" xref="S2.SS1.p6.5.m5.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.5.m5.1b"><ci id="S2.SS1.p6.5.m5.1.1.cmml" xref="S2.SS1.p6.5.m5.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.5.m5.1c">\mathcal{H}</annotation></semantics></math>, while minimizing the expectation of loss over the distribution of all the dataset <math id="S2.SS1.p6.6.m6.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S2.SS1.p6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p6.6.m6.1.1" xref="S2.SS1.p6.6.m6.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.6.m6.1b"><ci id="S2.SS1.p6.6.m6.1.1.cmml" xref="S2.SS1.p6.6.m6.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.6.m6.1c">\mathcal{D}</annotation></semantics></math>.</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.5" class="ltx_Math" alttext="\widehat{F}=\underset{F\in\mathcal{H}}{\mathrm{argmin}}\underset{(x,y)\in\mathcal{D}}{\mathbb{E}}L(y,F(x))," display="block"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5.1" xref="S2.E1.m1.5.5.1.1.cmml"><mrow id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.1.cmml"><mover accent="true" id="S2.E1.m1.5.5.1.1.3" xref="S2.E1.m1.5.5.1.1.3.cmml"><mi id="S2.E1.m1.5.5.1.1.3.2" xref="S2.E1.m1.5.5.1.1.3.2.cmml">F</mi><mo id="S2.E1.m1.5.5.1.1.3.1" xref="S2.E1.m1.5.5.1.1.3.1.cmml">^</mo></mover><mo id="S2.E1.m1.5.5.1.1.2" xref="S2.E1.m1.5.5.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.5.5.1.1.1" xref="S2.E1.m1.5.5.1.1.1.cmml"><munder accentunder="true" id="S2.E1.m1.5.5.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.3.cmml"><mi id="S2.E1.m1.5.5.1.1.1.3.2" xref="S2.E1.m1.5.5.1.1.1.3.2.cmml">argmin</mi><mrow id="S2.E1.m1.5.5.1.1.1.3.1" xref="S2.E1.m1.5.5.1.1.1.3.1.cmml"><mi id="S2.E1.m1.5.5.1.1.1.3.1.2" xref="S2.E1.m1.5.5.1.1.1.3.1.2.cmml">F</mi><mo id="S2.E1.m1.5.5.1.1.1.3.1.1" xref="S2.E1.m1.5.5.1.1.1.3.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.5.5.1.1.1.3.1.3" xref="S2.E1.m1.5.5.1.1.1.3.1.3.cmml">ℋ</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.2.cmml">​</mo><munder accentunder="true" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">𝔼</mi><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mrow id="S2.E1.m1.2.2.2.4.2" xref="S2.E1.m1.2.2.2.4.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.4.2.1" xref="S2.E1.m1.2.2.2.4.1.cmml">(</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">x</mi><mo id="S2.E1.m1.2.2.2.4.2.2" xref="S2.E1.m1.2.2.2.4.1.cmml">,</mo><mi id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">y</mi><mo stretchy="false" id="S2.E1.m1.2.2.2.4.2.3" xref="S2.E1.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.2.5" xref="S2.E1.m1.2.2.2.5.cmml">𝒟</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.2a" xref="S2.E1.m1.5.5.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.5.5.1.1.1.4" xref="S2.E1.m1.5.5.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.2b" xref="S2.E1.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">(</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">y</mi><mo id="S2.E1.m1.5.5.1.1.1.1.1.3" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">,</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.5.5.1.1.1.1.1.1.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.1.1.1.1.1.1.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.E1.m1.5.5.1.1.1.1.1.1.3.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.2.1" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">x</mi><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.1.3.2.2" xref="S2.E1.m1.5.5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.5.5.1.1.1.1.1.4" xref="S2.E1.m1.5.5.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.5.5.1.2" xref="S2.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.1.1.cmml" xref="S2.E1.m1.5.5.1"><eq id="S2.E1.m1.5.5.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.2"></eq><apply id="S2.E1.m1.5.5.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.3"><ci id="S2.E1.m1.5.5.1.1.3.1.cmml" xref="S2.E1.m1.5.5.1.1.3.1">^</ci><ci id="S2.E1.m1.5.5.1.1.3.2.cmml" xref="S2.E1.m1.5.5.1.1.3.2">𝐹</ci></apply><apply id="S2.E1.m1.5.5.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1"><times id="S2.E1.m1.5.5.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.2"></times><apply id="S2.E1.m1.5.5.1.1.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3"><apply id="S2.E1.m1.5.5.1.1.1.3.1.cmml" xref="S2.E1.m1.5.5.1.1.1.3.1"><in id="S2.E1.m1.5.5.1.1.1.3.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.3.1.1"></in><ci id="S2.E1.m1.5.5.1.1.1.3.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.3.1.2">𝐹</ci><ci id="S2.E1.m1.5.5.1.1.1.3.1.3.cmml" xref="S2.E1.m1.5.5.1.1.1.3.1.3">ℋ</ci></apply><ci id="S2.E1.m1.5.5.1.1.1.3.2.cmml" xref="S2.E1.m1.5.5.1.1.1.3.2">argmin</ci></apply><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><in id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3"></in><interval closure="open" id="S2.E1.m1.2.2.2.4.1.cmml" xref="S2.E1.m1.2.2.2.4.2"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝑥</ci><ci id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2">𝑦</ci></interval><ci id="S2.E1.m1.2.2.2.5.cmml" xref="S2.E1.m1.2.2.2.5">𝒟</ci></apply><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">𝔼</ci></apply><ci id="S2.E1.m1.5.5.1.1.1.4.cmml" xref="S2.E1.m1.5.5.1.1.1.4">𝐿</ci><interval closure="open" id="S2.E1.m1.5.5.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1"><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">𝑦</ci><apply id="S2.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1"><times id="S2.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.5.5.1.1.1.1.1.1.2">𝐹</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑥</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">\widehat{F}=\underset{F\in\mathcal{H}}{\mathrm{argmin}}\underset{(x,y)\in\mathcal{D}}{\mathbb{E}}L(y,F(x)),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p6.9" class="ltx_p">where <math id="S2.SS1.p6.7.m1.3" class="ltx_Math" alttext="L(y,F(x))" display="inline"><semantics id="S2.SS1.p6.7.m1.3a"><mrow id="S2.SS1.p6.7.m1.3.3" xref="S2.SS1.p6.7.m1.3.3.cmml"><mi id="S2.SS1.p6.7.m1.3.3.3" xref="S2.SS1.p6.7.m1.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.7.m1.3.3.2" xref="S2.SS1.p6.7.m1.3.3.2.cmml">​</mo><mrow id="S2.SS1.p6.7.m1.3.3.1.1" xref="S2.SS1.p6.7.m1.3.3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.7.m1.3.3.1.1.2" xref="S2.SS1.p6.7.m1.3.3.1.2.cmml">(</mo><mi id="S2.SS1.p6.7.m1.2.2" xref="S2.SS1.p6.7.m1.2.2.cmml">y</mi><mo id="S2.SS1.p6.7.m1.3.3.1.1.3" xref="S2.SS1.p6.7.m1.3.3.1.2.cmml">,</mo><mrow id="S2.SS1.p6.7.m1.3.3.1.1.1" xref="S2.SS1.p6.7.m1.3.3.1.1.1.cmml"><mi id="S2.SS1.p6.7.m1.3.3.1.1.1.2" xref="S2.SS1.p6.7.m1.3.3.1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.7.m1.3.3.1.1.1.1" xref="S2.SS1.p6.7.m1.3.3.1.1.1.1.cmml">​</mo><mrow id="S2.SS1.p6.7.m1.3.3.1.1.1.3.2" xref="S2.SS1.p6.7.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p6.7.m1.3.3.1.1.1.3.2.1" xref="S2.SS1.p6.7.m1.3.3.1.1.1.cmml">(</mo><mi id="S2.SS1.p6.7.m1.1.1" xref="S2.SS1.p6.7.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.7.m1.3.3.1.1.1.3.2.2" xref="S2.SS1.p6.7.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS1.p6.7.m1.3.3.1.1.4" xref="S2.SS1.p6.7.m1.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.7.m1.3b"><apply id="S2.SS1.p6.7.m1.3.3.cmml" xref="S2.SS1.p6.7.m1.3.3"><times id="S2.SS1.p6.7.m1.3.3.2.cmml" xref="S2.SS1.p6.7.m1.3.3.2"></times><ci id="S2.SS1.p6.7.m1.3.3.3.cmml" xref="S2.SS1.p6.7.m1.3.3.3">𝐿</ci><interval closure="open" id="S2.SS1.p6.7.m1.3.3.1.2.cmml" xref="S2.SS1.p6.7.m1.3.3.1.1"><ci id="S2.SS1.p6.7.m1.2.2.cmml" xref="S2.SS1.p6.7.m1.2.2">𝑦</ci><apply id="S2.SS1.p6.7.m1.3.3.1.1.1.cmml" xref="S2.SS1.p6.7.m1.3.3.1.1.1"><times id="S2.SS1.p6.7.m1.3.3.1.1.1.1.cmml" xref="S2.SS1.p6.7.m1.3.3.1.1.1.1"></times><ci id="S2.SS1.p6.7.m1.3.3.1.1.1.2.cmml" xref="S2.SS1.p6.7.m1.3.3.1.1.1.2">𝐹</ci><ci id="S2.SS1.p6.7.m1.1.1.cmml" xref="S2.SS1.p6.7.m1.1.1">𝑥</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.7.m1.3c">L(y,F(x))</annotation></semantics></math> refers to the loss of <math id="S2.SS1.p6.8.m2.1" class="ltx_Math" alttext="F(x)" display="inline"><semantics id="S2.SS1.p6.8.m2.1a"><mrow id="S2.SS1.p6.8.m2.1.2" xref="S2.SS1.p6.8.m2.1.2.cmml"><mi id="S2.SS1.p6.8.m2.1.2.2" xref="S2.SS1.p6.8.m2.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.8.m2.1.2.1" xref="S2.SS1.p6.8.m2.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.8.m2.1.2.3.2" xref="S2.SS1.p6.8.m2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.8.m2.1.2.3.2.1" xref="S2.SS1.p6.8.m2.1.2.cmml">(</mo><mi id="S2.SS1.p6.8.m2.1.1" xref="S2.SS1.p6.8.m2.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.8.m2.1.2.3.2.2" xref="S2.SS1.p6.8.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.8.m2.1b"><apply id="S2.SS1.p6.8.m2.1.2.cmml" xref="S2.SS1.p6.8.m2.1.2"><times id="S2.SS1.p6.8.m2.1.2.1.cmml" xref="S2.SS1.p6.8.m2.1.2.1"></times><ci id="S2.SS1.p6.8.m2.1.2.2.cmml" xref="S2.SS1.p6.8.m2.1.2.2">𝐹</ci><ci id="S2.SS1.p6.8.m2.1.1.cmml" xref="S2.SS1.p6.8.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.8.m2.1c">F(x)</annotation></semantics></math> to the label <math id="S2.SS1.p6.9.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p6.9.m3.1a"><mi id="S2.SS1.p6.9.m3.1.1" xref="S2.SS1.p6.9.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.9.m3.1b"><ci id="S2.SS1.p6.9.m3.1.1.cmml" xref="S2.SS1.p6.9.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.9.m3.1c">y</annotation></semantics></math>.
During the training process, the Stochastic Gradient Descent (SGD) approach <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib145" title="" class="ltx_ref">robbins1951stochastic </a>; <a href="#bib.bib206" title="" class="ltx_ref">zinkevich2010parallelized </a></cite> is generally used to minimize the loss function using Formula <a href="#S2.E2" title="In 2.1 Basic Concepts ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="F_{k+1}(x)\leftarrow F_{k}(x)-\eta_{k}\nabla F_{k}(x)," display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml"><msub id="S2.E2.m1.4.4.1.1.2.2" xref="S2.E2.m1.4.4.1.1.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml">F</mi><mrow id="S2.E2.m1.4.4.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.2.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.2.2.3.2" xref="S2.E2.m1.4.4.1.1.2.2.3.2.cmml">k</mi><mo id="S2.E2.m1.4.4.1.1.2.2.3.1" xref="S2.E2.m1.4.4.1.1.2.2.3.1.cmml">+</mo><mn id="S2.E2.m1.4.4.1.1.2.2.3.3" xref="S2.E2.m1.4.4.1.1.2.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.2.1" xref="S2.E2.m1.4.4.1.1.2.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.2.3.2" xref="S2.E2.m1.4.4.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.2.3.2.1" xref="S2.E2.m1.4.4.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.2.3.2.2" xref="S2.E2.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml">←</mo><mrow id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml"><mrow id="S2.E2.m1.4.4.1.1.3.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml"><msub id="S2.E2.m1.4.4.1.1.3.2.2" xref="S2.E2.m1.4.4.1.1.3.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.3.2.2.2" xref="S2.E2.m1.4.4.1.1.3.2.2.2.cmml">F</mi><mi id="S2.E2.m1.4.4.1.1.3.2.2.3" xref="S2.E2.m1.4.4.1.1.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.2.1" xref="S2.E2.m1.4.4.1.1.3.2.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.3.2.3.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.2.3.2.1" xref="S2.E2.m1.4.4.1.1.3.2.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">x</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.2.3.2.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.3.1" xref="S2.E2.m1.4.4.1.1.3.1.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1.3.3" xref="S2.E2.m1.4.4.1.1.3.3.cmml"><msub id="S2.E2.m1.4.4.1.1.3.3.2" xref="S2.E2.m1.4.4.1.1.3.3.2.cmml"><mi id="S2.E2.m1.4.4.1.1.3.3.2.2" xref="S2.E2.m1.4.4.1.1.3.3.2.2.cmml">η</mi><mi id="S2.E2.m1.4.4.1.1.3.3.2.3" xref="S2.E2.m1.4.4.1.1.3.3.2.3.cmml">k</mi></msub><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.3.1" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.3.3.3" xref="S2.E2.m1.4.4.1.1.3.3.3.cmml"><mo rspace="0.167em" id="S2.E2.m1.4.4.1.1.3.3.3.1" xref="S2.E2.m1.4.4.1.1.3.3.3.1.cmml">∇</mo><msub id="S2.E2.m1.4.4.1.1.3.3.3.2" xref="S2.E2.m1.4.4.1.1.3.3.3.2.cmml"><mi id="S2.E2.m1.4.4.1.1.3.3.3.2.2" xref="S2.E2.m1.4.4.1.1.3.3.3.2.2.cmml">F</mi><mi id="S2.E2.m1.4.4.1.1.3.3.3.2.3" xref="S2.E2.m1.4.4.1.1.3.3.3.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.3.3.1a" xref="S2.E2.m1.4.4.1.1.3.3.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.3.3.4.2" xref="S2.E2.m1.4.4.1.1.3.3.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.4.2.1" xref="S2.E2.m1.4.4.1.1.3.3.cmml">(</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">x</mi><mo stretchy="false" id="S2.E2.m1.4.4.1.1.3.3.4.2.2" xref="S2.E2.m1.4.4.1.1.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><ci id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1">←</ci><apply id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"><times id="S2.E2.m1.4.4.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.2.1"></times><apply id="S2.E2.m1.4.4.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2">𝐹</ci><apply id="S2.E2.m1.4.4.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3"><plus id="S2.E2.m1.4.4.1.1.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.1"></plus><ci id="S2.E2.m1.4.4.1.1.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.2.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci></apply><apply id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"><minus id="S2.E2.m1.4.4.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.1"></minus><apply id="S2.E2.m1.4.4.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2"><times id="S2.E2.m1.4.4.1.1.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.2.1"></times><apply id="S2.E2.m1.4.4.1.1.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.3.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.3.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2.2">𝐹</ci><ci id="S2.E2.m1.4.4.1.1.3.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2.3">𝑘</ci></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑥</ci></apply><apply id="S2.E2.m1.4.4.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.3.3"><times id="S2.E2.m1.4.4.1.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.3.1"></times><apply id="S2.E2.m1.4.4.1.1.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.3.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.3.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.3.2.2">𝜂</ci><ci id="S2.E2.m1.4.4.1.1.3.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.3.3.2.3">𝑘</ci></apply><apply id="S2.E2.m1.4.4.1.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3"><ci id="S2.E2.m1.4.4.1.1.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3.1">∇</ci><apply id="S2.E2.m1.4.4.1.1.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.3.3.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.3.3.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3.2.2">𝐹</ci><ci id="S2.E2.m1.4.4.1.1.3.3.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.3.3.3.2.3">𝑘</ci></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">F_{k+1}(x)\leftarrow F_{k}(x)-\eta_{k}\nabla F_{k}(x),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p6.19" class="ltx_p">where <math id="S2.SS1.p6.10.m1.1" class="ltx_Math" alttext="F_{k}(x)" display="inline"><semantics id="S2.SS1.p6.10.m1.1a"><mrow id="S2.SS1.p6.10.m1.1.2" xref="S2.SS1.p6.10.m1.1.2.cmml"><msub id="S2.SS1.p6.10.m1.1.2.2" xref="S2.SS1.p6.10.m1.1.2.2.cmml"><mi id="S2.SS1.p6.10.m1.1.2.2.2" xref="S2.SS1.p6.10.m1.1.2.2.2.cmml">F</mi><mi id="S2.SS1.p6.10.m1.1.2.2.3" xref="S2.SS1.p6.10.m1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p6.10.m1.1.2.1" xref="S2.SS1.p6.10.m1.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.10.m1.1.2.3.2" xref="S2.SS1.p6.10.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.10.m1.1.2.3.2.1" xref="S2.SS1.p6.10.m1.1.2.cmml">(</mo><mi id="S2.SS1.p6.10.m1.1.1" xref="S2.SS1.p6.10.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.10.m1.1.2.3.2.2" xref="S2.SS1.p6.10.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.10.m1.1b"><apply id="S2.SS1.p6.10.m1.1.2.cmml" xref="S2.SS1.p6.10.m1.1.2"><times id="S2.SS1.p6.10.m1.1.2.1.cmml" xref="S2.SS1.p6.10.m1.1.2.1"></times><apply id="S2.SS1.p6.10.m1.1.2.2.cmml" xref="S2.SS1.p6.10.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.10.m1.1.2.2.1.cmml" xref="S2.SS1.p6.10.m1.1.2.2">subscript</csymbol><ci id="S2.SS1.p6.10.m1.1.2.2.2.cmml" xref="S2.SS1.p6.10.m1.1.2.2.2">𝐹</ci><ci id="S2.SS1.p6.10.m1.1.2.2.3.cmml" xref="S2.SS1.p6.10.m1.1.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p6.10.m1.1.1.cmml" xref="S2.SS1.p6.10.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.10.m1.1c">F_{k}(x)</annotation></semantics></math> refers to the learned model in the <math id="S2.SS1.p6.11.m2.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.SS1.p6.11.m2.1a"><msup id="S2.SS1.p6.11.m2.1.1" xref="S2.SS1.p6.11.m2.1.1.cmml"><mi id="S2.SS1.p6.11.m2.1.1.2" xref="S2.SS1.p6.11.m2.1.1.2.cmml">k</mi><mrow id="S2.SS1.p6.11.m2.1.1.3" xref="S2.SS1.p6.11.m2.1.1.3.cmml"><mi id="S2.SS1.p6.11.m2.1.1.3.2" xref="S2.SS1.p6.11.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.11.m2.1.1.3.1" xref="S2.SS1.p6.11.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p6.11.m2.1.1.3.3" xref="S2.SS1.p6.11.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.11.m2.1b"><apply id="S2.SS1.p6.11.m2.1.1.cmml" xref="S2.SS1.p6.11.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.11.m2.1.1.1.cmml" xref="S2.SS1.p6.11.m2.1.1">superscript</csymbol><ci id="S2.SS1.p6.11.m2.1.1.2.cmml" xref="S2.SS1.p6.11.m2.1.1.2">𝑘</ci><apply id="S2.SS1.p6.11.m2.1.1.3.cmml" xref="S2.SS1.p6.11.m2.1.1.3"><times id="S2.SS1.p6.11.m2.1.1.3.1.cmml" xref="S2.SS1.p6.11.m2.1.1.3.1"></times><ci id="S2.SS1.p6.11.m2.1.1.3.2.cmml" xref="S2.SS1.p6.11.m2.1.1.3.2">𝑡</ci><ci id="S2.SS1.p6.11.m2.1.1.3.3.cmml" xref="S2.SS1.p6.11.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.11.m2.1c">k^{th}</annotation></semantics></math> iteration, <math id="S2.SS1.p6.12.m3.1" class="ltx_Math" alttext="\nabla F_{k}(x)" display="inline"><semantics id="S2.SS1.p6.12.m3.1a"><mrow id="S2.SS1.p6.12.m3.1.2" xref="S2.SS1.p6.12.m3.1.2.cmml"><mrow id="S2.SS1.p6.12.m3.1.2.2" xref="S2.SS1.p6.12.m3.1.2.2.cmml"><mo rspace="0.167em" id="S2.SS1.p6.12.m3.1.2.2.1" xref="S2.SS1.p6.12.m3.1.2.2.1.cmml">∇</mo><msub id="S2.SS1.p6.12.m3.1.2.2.2" xref="S2.SS1.p6.12.m3.1.2.2.2.cmml"><mi id="S2.SS1.p6.12.m3.1.2.2.2.2" xref="S2.SS1.p6.12.m3.1.2.2.2.2.cmml">F</mi><mi id="S2.SS1.p6.12.m3.1.2.2.2.3" xref="S2.SS1.p6.12.m3.1.2.2.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p6.12.m3.1.2.1" xref="S2.SS1.p6.12.m3.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.12.m3.1.2.3.2" xref="S2.SS1.p6.12.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.12.m3.1.2.3.2.1" xref="S2.SS1.p6.12.m3.1.2.cmml">(</mo><mi id="S2.SS1.p6.12.m3.1.1" xref="S2.SS1.p6.12.m3.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.12.m3.1.2.3.2.2" xref="S2.SS1.p6.12.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.12.m3.1b"><apply id="S2.SS1.p6.12.m3.1.2.cmml" xref="S2.SS1.p6.12.m3.1.2"><times id="S2.SS1.p6.12.m3.1.2.1.cmml" xref="S2.SS1.p6.12.m3.1.2.1"></times><apply id="S2.SS1.p6.12.m3.1.2.2.cmml" xref="S2.SS1.p6.12.m3.1.2.2"><ci id="S2.SS1.p6.12.m3.1.2.2.1.cmml" xref="S2.SS1.p6.12.m3.1.2.2.1">∇</ci><apply id="S2.SS1.p6.12.m3.1.2.2.2.cmml" xref="S2.SS1.p6.12.m3.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.12.m3.1.2.2.2.1.cmml" xref="S2.SS1.p6.12.m3.1.2.2.2">subscript</csymbol><ci id="S2.SS1.p6.12.m3.1.2.2.2.2.cmml" xref="S2.SS1.p6.12.m3.1.2.2.2.2">𝐹</ci><ci id="S2.SS1.p6.12.m3.1.2.2.2.3.cmml" xref="S2.SS1.p6.12.m3.1.2.2.2.3">𝑘</ci></apply></apply><ci id="S2.SS1.p6.12.m3.1.1.cmml" xref="S2.SS1.p6.12.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.12.m3.1c">\nabla F_{k}(x)</annotation></semantics></math> refers to the gradient of the model at the <math id="S2.SS1.p6.13.m4.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.SS1.p6.13.m4.1a"><msup id="S2.SS1.p6.13.m4.1.1" xref="S2.SS1.p6.13.m4.1.1.cmml"><mi id="S2.SS1.p6.13.m4.1.1.2" xref="S2.SS1.p6.13.m4.1.1.2.cmml">k</mi><mrow id="S2.SS1.p6.13.m4.1.1.3" xref="S2.SS1.p6.13.m4.1.1.3.cmml"><mi id="S2.SS1.p6.13.m4.1.1.3.2" xref="S2.SS1.p6.13.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.13.m4.1.1.3.1" xref="S2.SS1.p6.13.m4.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p6.13.m4.1.1.3.3" xref="S2.SS1.p6.13.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.13.m4.1b"><apply id="S2.SS1.p6.13.m4.1.1.cmml" xref="S2.SS1.p6.13.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.13.m4.1.1.1.cmml" xref="S2.SS1.p6.13.m4.1.1">superscript</csymbol><ci id="S2.SS1.p6.13.m4.1.1.2.cmml" xref="S2.SS1.p6.13.m4.1.1.2">𝑘</ci><apply id="S2.SS1.p6.13.m4.1.1.3.cmml" xref="S2.SS1.p6.13.m4.1.1.3"><times id="S2.SS1.p6.13.m4.1.1.3.1.cmml" xref="S2.SS1.p6.13.m4.1.1.3.1"></times><ci id="S2.SS1.p6.13.m4.1.1.3.2.cmml" xref="S2.SS1.p6.13.m4.1.1.3.2">𝑡</ci><ci id="S2.SS1.p6.13.m4.1.1.3.3.cmml" xref="S2.SS1.p6.13.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.13.m4.1c">k^{th}</annotation></semantics></math> iteration based on the model already obtained <math id="S2.SS1.p6.14.m5.1" class="ltx_Math" alttext="F_{k}(x)" display="inline"><semantics id="S2.SS1.p6.14.m5.1a"><mrow id="S2.SS1.p6.14.m5.1.2" xref="S2.SS1.p6.14.m5.1.2.cmml"><msub id="S2.SS1.p6.14.m5.1.2.2" xref="S2.SS1.p6.14.m5.1.2.2.cmml"><mi id="S2.SS1.p6.14.m5.1.2.2.2" xref="S2.SS1.p6.14.m5.1.2.2.2.cmml">F</mi><mi id="S2.SS1.p6.14.m5.1.2.2.3" xref="S2.SS1.p6.14.m5.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p6.14.m5.1.2.1" xref="S2.SS1.p6.14.m5.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.14.m5.1.2.3.2" xref="S2.SS1.p6.14.m5.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.14.m5.1.2.3.2.1" xref="S2.SS1.p6.14.m5.1.2.cmml">(</mo><mi id="S2.SS1.p6.14.m5.1.1" xref="S2.SS1.p6.14.m5.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.14.m5.1.2.3.2.2" xref="S2.SS1.p6.14.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.14.m5.1b"><apply id="S2.SS1.p6.14.m5.1.2.cmml" xref="S2.SS1.p6.14.m5.1.2"><times id="S2.SS1.p6.14.m5.1.2.1.cmml" xref="S2.SS1.p6.14.m5.1.2.1"></times><apply id="S2.SS1.p6.14.m5.1.2.2.cmml" xref="S2.SS1.p6.14.m5.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.14.m5.1.2.2.1.cmml" xref="S2.SS1.p6.14.m5.1.2.2">subscript</csymbol><ci id="S2.SS1.p6.14.m5.1.2.2.2.cmml" xref="S2.SS1.p6.14.m5.1.2.2.2">𝐹</ci><ci id="S2.SS1.p6.14.m5.1.2.2.3.cmml" xref="S2.SS1.p6.14.m5.1.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p6.14.m5.1.1.cmml" xref="S2.SS1.p6.14.m5.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.14.m5.1c">F_{k}(x)</annotation></semantics></math> and the training dataset, <math id="S2.SS1.p6.15.m6.1" class="ltx_Math" alttext="\eta_{k}" display="inline"><semantics id="S2.SS1.p6.15.m6.1a"><msub id="S2.SS1.p6.15.m6.1.1" xref="S2.SS1.p6.15.m6.1.1.cmml"><mi id="S2.SS1.p6.15.m6.1.1.2" xref="S2.SS1.p6.15.m6.1.1.2.cmml">η</mi><mi id="S2.SS1.p6.15.m6.1.1.3" xref="S2.SS1.p6.15.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.15.m6.1b"><apply id="S2.SS1.p6.15.m6.1.1.cmml" xref="S2.SS1.p6.15.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.15.m6.1.1.1.cmml" xref="S2.SS1.p6.15.m6.1.1">subscript</csymbol><ci id="S2.SS1.p6.15.m6.1.1.2.cmml" xref="S2.SS1.p6.15.m6.1.1.2">𝜂</ci><ci id="S2.SS1.p6.15.m6.1.1.3.cmml" xref="S2.SS1.p6.15.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.15.m6.1c">\eta_{k}</annotation></semantics></math> refers to the learning rate, and <math id="S2.SS1.p6.16.m7.1" class="ltx_Math" alttext="F_{k+1}(x)" display="inline"><semantics id="S2.SS1.p6.16.m7.1a"><mrow id="S2.SS1.p6.16.m7.1.2" xref="S2.SS1.p6.16.m7.1.2.cmml"><msub id="S2.SS1.p6.16.m7.1.2.2" xref="S2.SS1.p6.16.m7.1.2.2.cmml"><mi id="S2.SS1.p6.16.m7.1.2.2.2" xref="S2.SS1.p6.16.m7.1.2.2.2.cmml">F</mi><mrow id="S2.SS1.p6.16.m7.1.2.2.3" xref="S2.SS1.p6.16.m7.1.2.2.3.cmml"><mi id="S2.SS1.p6.16.m7.1.2.2.3.2" xref="S2.SS1.p6.16.m7.1.2.2.3.2.cmml">k</mi><mo id="S2.SS1.p6.16.m7.1.2.2.3.1" xref="S2.SS1.p6.16.m7.1.2.2.3.1.cmml">+</mo><mn id="S2.SS1.p6.16.m7.1.2.2.3.3" xref="S2.SS1.p6.16.m7.1.2.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p6.16.m7.1.2.1" xref="S2.SS1.p6.16.m7.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.16.m7.1.2.3.2" xref="S2.SS1.p6.16.m7.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.16.m7.1.2.3.2.1" xref="S2.SS1.p6.16.m7.1.2.cmml">(</mo><mi id="S2.SS1.p6.16.m7.1.1" xref="S2.SS1.p6.16.m7.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.16.m7.1.2.3.2.2" xref="S2.SS1.p6.16.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.16.m7.1b"><apply id="S2.SS1.p6.16.m7.1.2.cmml" xref="S2.SS1.p6.16.m7.1.2"><times id="S2.SS1.p6.16.m7.1.2.1.cmml" xref="S2.SS1.p6.16.m7.1.2.1"></times><apply id="S2.SS1.p6.16.m7.1.2.2.cmml" xref="S2.SS1.p6.16.m7.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.16.m7.1.2.2.1.cmml" xref="S2.SS1.p6.16.m7.1.2.2">subscript</csymbol><ci id="S2.SS1.p6.16.m7.1.2.2.2.cmml" xref="S2.SS1.p6.16.m7.1.2.2.2">𝐹</ci><apply id="S2.SS1.p6.16.m7.1.2.2.3.cmml" xref="S2.SS1.p6.16.m7.1.2.2.3"><plus id="S2.SS1.p6.16.m7.1.2.2.3.1.cmml" xref="S2.SS1.p6.16.m7.1.2.2.3.1"></plus><ci id="S2.SS1.p6.16.m7.1.2.2.3.2.cmml" xref="S2.SS1.p6.16.m7.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.SS1.p6.16.m7.1.2.2.3.3.cmml" xref="S2.SS1.p6.16.m7.1.2.2.3.3">1</cn></apply></apply><ci id="S2.SS1.p6.16.m7.1.1.cmml" xref="S2.SS1.p6.16.m7.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.16.m7.1c">F_{k+1}(x)</annotation></semantics></math> refers to the update model of the <math id="S2.SS1.p6.17.m8.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S2.SS1.p6.17.m8.1a"><msup id="S2.SS1.p6.17.m8.1.1" xref="S2.SS1.p6.17.m8.1.1.cmml"><mi id="S2.SS1.p6.17.m8.1.1.2" xref="S2.SS1.p6.17.m8.1.1.2.cmml">k</mi><mrow id="S2.SS1.p6.17.m8.1.1.3" xref="S2.SS1.p6.17.m8.1.1.3.cmml"><mi id="S2.SS1.p6.17.m8.1.1.3.2" xref="S2.SS1.p6.17.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p6.17.m8.1.1.3.1" xref="S2.SS1.p6.17.m8.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p6.17.m8.1.1.3.3" xref="S2.SS1.p6.17.m8.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.17.m8.1b"><apply id="S2.SS1.p6.17.m8.1.1.cmml" xref="S2.SS1.p6.17.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.17.m8.1.1.1.cmml" xref="S2.SS1.p6.17.m8.1.1">superscript</csymbol><ci id="S2.SS1.p6.17.m8.1.1.2.cmml" xref="S2.SS1.p6.17.m8.1.1.2">𝑘</ci><apply id="S2.SS1.p6.17.m8.1.1.3.cmml" xref="S2.SS1.p6.17.m8.1.1.3"><times id="S2.SS1.p6.17.m8.1.1.3.1.cmml" xref="S2.SS1.p6.17.m8.1.1.3.1"></times><ci id="S2.SS1.p6.17.m8.1.1.3.2.cmml" xref="S2.SS1.p6.17.m8.1.1.3.2">𝑡</ci><ci id="S2.SS1.p6.17.m8.1.1.3.3.cmml" xref="S2.SS1.p6.17.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.17.m8.1c">k^{th}</annotation></semantics></math> iteration. Within each iteration, there are two phases, i.e., forward propagation and backward propagation. The forward propagation calculates the output based on the input data <math id="S2.SS1.p6.18.m9.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p6.18.m9.1a"><mi id="S2.SS1.p6.18.m9.1.1" xref="S2.SS1.p6.18.m9.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.18.m9.1b"><ci id="S2.SS1.p6.18.m9.1.1.cmml" xref="S2.SS1.p6.18.m9.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.18.m9.1c">x</annotation></semantics></math> using the model, while the backward propagation calculates the gradients <math id="S2.SS1.p6.19.m10.1" class="ltx_Math" alttext="\nabla F_{k}(x)" display="inline"><semantics id="S2.SS1.p6.19.m10.1a"><mrow id="S2.SS1.p6.19.m10.1.2" xref="S2.SS1.p6.19.m10.1.2.cmml"><mrow id="S2.SS1.p6.19.m10.1.2.2" xref="S2.SS1.p6.19.m10.1.2.2.cmml"><mo rspace="0.167em" id="S2.SS1.p6.19.m10.1.2.2.1" xref="S2.SS1.p6.19.m10.1.2.2.1.cmml">∇</mo><msub id="S2.SS1.p6.19.m10.1.2.2.2" xref="S2.SS1.p6.19.m10.1.2.2.2.cmml"><mi id="S2.SS1.p6.19.m10.1.2.2.2.2" xref="S2.SS1.p6.19.m10.1.2.2.2.2.cmml">F</mi><mi id="S2.SS1.p6.19.m10.1.2.2.2.3" xref="S2.SS1.p6.19.m10.1.2.2.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.SS1.p6.19.m10.1.2.1" xref="S2.SS1.p6.19.m10.1.2.1.cmml">​</mo><mrow id="S2.SS1.p6.19.m10.1.2.3.2" xref="S2.SS1.p6.19.m10.1.2.cmml"><mo stretchy="false" id="S2.SS1.p6.19.m10.1.2.3.2.1" xref="S2.SS1.p6.19.m10.1.2.cmml">(</mo><mi id="S2.SS1.p6.19.m10.1.1" xref="S2.SS1.p6.19.m10.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS1.p6.19.m10.1.2.3.2.2" xref="S2.SS1.p6.19.m10.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.19.m10.1b"><apply id="S2.SS1.p6.19.m10.1.2.cmml" xref="S2.SS1.p6.19.m10.1.2"><times id="S2.SS1.p6.19.m10.1.2.1.cmml" xref="S2.SS1.p6.19.m10.1.2.1"></times><apply id="S2.SS1.p6.19.m10.1.2.2.cmml" xref="S2.SS1.p6.19.m10.1.2.2"><ci id="S2.SS1.p6.19.m10.1.2.2.1.cmml" xref="S2.SS1.p6.19.m10.1.2.2.1">∇</ci><apply id="S2.SS1.p6.19.m10.1.2.2.2.cmml" xref="S2.SS1.p6.19.m10.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p6.19.m10.1.2.2.2.1.cmml" xref="S2.SS1.p6.19.m10.1.2.2.2">subscript</csymbol><ci id="S2.SS1.p6.19.m10.1.2.2.2.2.cmml" xref="S2.SS1.p6.19.m10.1.2.2.2.2">𝐹</ci><ci id="S2.SS1.p6.19.m10.1.2.2.2.3.cmml" xref="S2.SS1.p6.19.m10.1.2.2.2.3">𝑘</ci></apply></apply><ci id="S2.SS1.p6.19.m10.1.1.cmml" xref="S2.SS1.p6.19.m10.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.19.m10.1c">\nabla F_{k}(x)</annotation></semantics></math> and updates the model.
When the calculation is distributed among multiple computing resources, the gradients or models of each computing resource are aggregated using an aggregation algorithm (see details in Section <a href="#S3.SS2" title="3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), in order to achieve consensus of multiple models and to generate a global model. <span id="S2.SS1.p6.19.1" class="ltx_text" style="color:#000000;">The learning rate can be dynamically adapted using a local adaptive optimizer, e.g., Adam, and/or cross-round learning rate schedulers <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib199" title="" class="ltx_ref">zhang2021federated </a></cite>.</span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>FL Model Life Cycle</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The life cycle of an FL model is a description of the state transitions of an FL model from creation to completion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a>; <a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>. Lo <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref">lo2021architectural </a></cite> propose that the life cycle of an FL model consists of 8 phases: initiated, broadcast, trained, transmitted, aggregated, evaluated, deployed, and monitored. Kairouz <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite> propose that the life cycle of an FL model includes 6 phases: problem identification, client instrumentation, simulation prototyping, federated model training, model evaluation, and deployment. However, they focus on the FL with distributed data in mobile devices. In this paper, we adopt a combination of workflow life cycle views <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib111" title="" class="ltx_ref">lo2021architectural </a>; <a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite> with a few variations <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a>; <a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite>, condensed into four phases:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">The composition phase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite> is for the creation of an FL model, which is used to address a specific machine learning problem, e.g., classification. First, a machine learning model is created to address the problem with certain requirements, e.g., the requirement of accuracy. Then, the machine learning model is adapted to FL scenarios. For instance, if the distributed data is of different features, the machine learning model is partitioned (see details in Section <a href="#S3.SS1.SSS2" title="3.1.2 Model Parallelism ‣ 3.1 Parallelism &amp; FL Types ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>) to process the distributed data.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The FL training phase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a>; <a href="#bib.bib111" title="" class="ltx_ref">lo2021architectural </a>; <a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite> is for the training phase of the FL model. During this phase, a training strategy, which includes parallelism and aggregation algorithms (see details in Section <a href="#S3" title="3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), is used to update the parameters, hyper parameters, and even the structure of the network, in order to improve the accuracy and the generalization capacity of the FL model.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">The FL model evaluation phase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a>; <a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite> is to apply the trained FL models, in order to analyze the performance of the trained FL models on a simulation platform or a real distributed system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib65" title="" class="ltx_ref">he2020fedml </a></cite>. As a result, the FL models with the best performance are selected. If the FL models do not meet the requirements, the FL model should be modified, or the training phase should be carried out again.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">The FL model deployment phase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite> is to deploy the FL model in a real-life scenario to process the data. If the final model can be shared without restriction, there is no difference between the FL model deployment and the model generated from a traditional centralized approach. Otherwise, the deployment of the final model should consider the ownership of the corresponding parts.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Functional Architecture of FL Systems</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The functional architecture of an FL system can be layered as follows <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite>: presentation, user services, FL training, and infrastructure. Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Functional Architecture of FL Systems ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows this architecture. The higher layers exploit the lower layers to provide their own functionality. A user interacts with an FL system through the presentation layer and realizes independent functionalities at the user services layer. During the training phase of FL models, a Federated Learning Execution Plan (FLEP) is generated, and the corresponding distributed training is carried out at the FL training layer. The FLEP is composed of a type of parallelism, a scheduling strategy, and a fault-tolerance mechanism. The FL system manages the physical resources through the infrastructure layer for the distributed training.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2104.14362/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.2.1" class="ltx_text" style="color:#000000;">Functional architecture of an FL system.</span></figcaption>
</figure>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Presentation Layer</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The presentation layer is a User Interface (UI) for the interaction between Users and FL systems at one or multiple stages of the FL model life cycle. The UI can be textual or graphical. This interface is responsible for designing a new FL model or choosing an existing machine learning model as an FL model. In addition, this layer also supports the modules at the user services layer, e.g., shows the status of the distributed training process. The textual UI is largely used for designing FL models <span id="S2.SS3.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">based on the command line or scripts <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib100" title="" class="ltx_ref">Lin2021FedNLP </a></cite></span>. The models can be directly expressed using Python, with the textual interface in PaddleFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">PaddleFL </a></cite>, TensorFlowFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">TFF </a></cite>, PySyft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib148" title="" class="ltx_ref">ryffel2018generic </a></cite>, and FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref">FATE </a></cite>. A graphic UI can make the interaction more practical, while the users can drag or drop the data processing element to design an FL model. For instance, FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref">FATE </a></cite> provides a Graphic UI (GUI) through a web portal. However, the graphic portal also exploits textual programming languages as inner representations of an FL model.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>User Services Layer</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The user service layer supports the expected functionalities, i.e., <span id="S2.SS3.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">monitoring and steering and log; interpretability and explainability; and graph data.</span> The monitoring enables the users to get the real-time status of the distributed training process. As the training process of FL models can be very long, e.g., from several hours to days <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>, it is of much importance to track the execution status, which allows the user to verify whether the training proceeds normally. The log service is generally supported by major FL systems, which can be used to analyze the training process. In addition, the log generated during the training process can be used to debug the system or adjust the FL model. FATE provides a visual monitoring board to users through its GUI. When there are unexpected results or errors during the training process, steering enables users to adjust the training process in order to reduce the time necessary to carry out the distributed training from scratch. Most FL systems can enable the users to stop the training, while the adjustment of parameters is not fully supported by major FL systems. The interpretability of FL is to describe the internals of an FL system in a way that is understandable to humans <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">gilpin2018explaining </a></cite>. The explainability focuses on explaining the representation of data inside an FL model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib52" title="" class="ltx_ref">gilpin2018explaining </a></cite>. With interpretability and explainability, the FL system can <span id="S2.SS3.SSS2.p1.1.2" class="ltx_text" style="color:#000000;">provide</span> a description of the results of the trained FL model based on the training data and the distributed training process. Shapley values have been used to provide the interpretability <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib175" title="" class="ltx_ref">wang2019interpret </a></cite>, while both the interpretability and explainability remain open challenges as each is hard to fully support.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p"><span id="S2.SS3.SSS2.p2.1.1" class="ltx_text" style="color:#000000;">In the real world, graph data widely exist in multiple domains, and a bunch of FL approaches have been proposed to handle the decentralized graph data for community detection <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib84" title="" class="ltx_ref">Ke2021Federated </a></cite>, financial crime <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib164" title="" class="ltx_ref">Suzumura2019Towards </a></cite>, and especially knowledge graph completion <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">Chen2020FedE </a></cite>.
FL is particularly useful in the field of knowledge graph completion, as a knowledge graph could not only contain text but also images or other type of data, i.e., multimodal Knowledge Graphs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib202" title="" class="ltx_ref">zhao2021multimodal </a></cite>; and the completion is realized in a collaborative fashion within an FL system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib102" title="" class="ltx_ref">lin2020improving </a></cite>.
The decentralized graphs can be inter-graph, i.e., the decentralized data belongs to multiple graphs, or intra-graph, i.e., the decentralized data is within one big graph <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib198" title="" class="ltx_ref">Zhang2021FederatedGraph </a></cite>, while most of the existing works focus on the intra-graph situation.
Horizontal FL techniques can be exploited on the Graph Neural Networks (GNN) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib185" title="" class="ltx_ref">wu2021fedgnn </a>; <a href="#bib.bib125" title="" class="ltx_ref">Meng2021Cross </a></cite> with encryption techniques <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib78" title="" class="ltx_ref">jiang2020federated </a></cite> (see details in Section <a href="#S2.SS3.SSS4" title="2.3.4 Infrastructure Layer ‣ 2.3 Functional Architecture of FL Systems ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.4</span></a>) while the performance of FL may be much worse than that of centralized GNNs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib62" title="" class="ltx_ref">he2021fedgraphnn </a></cite>.
The aggregation algorithms (see details in Section <a href="#S3.SS2.SSS1" title="3.2.1 Centralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) are also proposed based on the FedAvg <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib202" title="" class="ltx_ref">zhao2021multimodal </a></cite> or optimal transportation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib102" title="" class="ltx_ref">lin2020improving </a></cite>.
In addition, decentralized aggregation algorithms (see details in Section <a href="#S3.SS2.SSS3" title="3.2.3 Decentralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>) are also proposed to deal with the decentralized graph data for social network <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib67" title="" class="ltx_ref">He2019Central </a></cite> and drug discovery <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib63" title="" class="ltx_ref">He2021SpreadGNN </a></cite>.
While the fine-tuning of the FL system is time-consuming, Bayesian optimization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib203" title="" class="ltx_ref">Zheng2021ASFGNN </a></cite> and evolutionary optimization strategies <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib174" title="" class="ltx_ref">Wang2021AGCNS </a></cite> are utilized to automatically tune the hyper-parameters and the network structure, respectively.
Graph data can be vertically distributed where the features of the nodes are distributed across multiple data owners, and a data owner may or may not have the edges. Vertical FL exploits embeddings <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib204" title="" class="ltx_ref">zhou2020vertically </a>; <a href="#bib.bib29" title="" class="ltx_ref">Chen2020FedE </a></cite> or autoencoders <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib202" title="" class="ltx_ref">zhao2021multimodal </a></cite> to represent the nodes, which can be transferred to train a GNN. In addition, the differential privacy (see details in Section <a href="#S2.SS3.SSS4" title="2.3.4 Infrastructure Layer ‣ 2.3 Functional Architecture of FL Systems ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.4</span></a>) is combined with the embeddings to protect the data privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib140" title="" class="ltx_ref">peng2021federated </a></cite> of knowledge graphs. </span></p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>FL Training Layer</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">The FL training layer carries out the distributed training process with distributed data and computing resources. This layer consists of three modules: parallelization, scheduling, and fault-tolerance. FL parallelization exploits diverse types of parallelism, e.g., data parallelism, model parallelism, and pipeline parallelism, to generate executable tasks. Through the FL scheduling module, an FL system produces a Scheduling Plan (SP) of executable tasks, which aims at fully exploiting distributed computing resources and preventing training stalling. During the training process, the SP is generally defined by a training algorithm, which aggregates the updates, i.e., gradients or models, from each computing resource in order to generate a final machine learning model. The FL fault-tolerance mechanism handles the failures or errors of task execution and the connection of distributed resources. Reactive approaches are generally exploited, e.g., using check-points, restart, and task replication <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">Bonawitz19 </a></cite>. A reactive approach reduces the effect of failures after perceiving failures <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">ganga2013fault </a></cite>. An FLEP, which captures the execution directives, typically the result of compiling and optimizing the training process of FL models, is generated at this layer.</p>
</div>
</section>
<section id="S2.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Infrastructure Layer</h4>

<div id="S2.SS3.SSS4.p1" class="ltx_para">
<p id="S2.SS3.SSS4.p1.1" class="ltx_p">The infrastructure layer provides the interaction between an FL system and the distributed resources, including the computing resources, storage resources, network resources, and data resources. This layer contains three modules: a data security module, a data transfer module, and a distributed execution module. The data security module generally exploits Differential Privacy (DP) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">abadi2016deep </a></cite> and encryption techniques, e.g., homomorphic <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">aono2017privacy </a></cite>, to protect the raw data used during the training process. Although the raw data cannot be directly transferred, intermediate data, e.g., the gradients or models, can be communicated among distributed computing resources. The data transfer module exploits data compression techniques <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib161" title="" class="ltx_ref">Stich2018 </a></cite> to improve the data transfer efficiency.
At this layer, the FLEP generated at the FL training layer is carried out within the distributed execution module; i.e., concrete tasks are executed in distributed computing resources.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Distributed Training</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the distributed training process for FL. First, we present three types of parallelism in distributed training and the application within FL. The parallelism approaches are generally implemented in the parallelization module. Then, we discuss existing aggregation algorithms for the distributed training, which is implemented in the scheduling module.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Parallelism &amp; FL Types</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Three types of parallelism exist for distributed machine learning: data parallelism, model parallelism, and pipeline parallelism <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib170" title="" class="ltx_ref">verbraeken2020survey </a>; <a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite>. FL can be classified to three types, i.e., horizontal, vertical, and hybrid <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a>; <a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>. The horizontal FL generally exploits data parallelism, and the vertical FL typically takes advantage of model parallelism. However, the hybrid FL relies on transfer learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib139" title="" class="ltx_ref">pan2009survey </a></cite>, which is not a parallelism approach and is out of the scope of this paper.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2104.14362/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of a neural network.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p">In this section, we take an example of a neural network as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Parallelism &amp; FL Types ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to explain the parallelism. In the example, we assume that the model contains three layers and seven data processing nodes (neurons), i.e., <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">A</mi><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">A_{1}</annotation></semantics></math>, <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">A</mi><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">A_{2}</annotation></semantics></math>, <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="B_{1}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">B</mi><mn id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">B_{1}</annotation></semantics></math>, <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="B_{2}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">B</mi><mn id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">B_{2}</annotation></semantics></math>, <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="B_{3}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">B</mi><mn id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">B_{3}</annotation></semantics></math>, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="C_{1}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝐶</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">C_{1}</annotation></semantics></math>, <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="C_{2}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">C</mi><mn id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝐶</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">C_{2}</annotation></semantics></math>, <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">D</annotation></semantics></math>. The arrows represent the data flow among different data processing nodes. The execution of the data processing nodes at each layer can be carried out in parallel, while the execution of different layers should be performed sequentially. The input data contains 4 data points. We assume two/three computing resources owned by two/three users. Each has a part of the input data.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2104.14362/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="276" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Data parallelism. The forward and backward process of <math id="S3.F3.5.m1.1" class="ltx_Math" alttext="I^{1}" display="inline"><semantics id="S3.F3.5.m1.1b"><msup id="S3.F3.5.m1.1.1" xref="S3.F3.5.m1.1.1.cmml"><mi id="S3.F3.5.m1.1.1.2" xref="S3.F3.5.m1.1.1.2.cmml">I</mi><mn id="S3.F3.5.m1.1.1.3" xref="S3.F3.5.m1.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F3.5.m1.1c"><apply id="S3.F3.5.m1.1.1.cmml" xref="S3.F3.5.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.5.m1.1.1.1.cmml" xref="S3.F3.5.m1.1.1">superscript</csymbol><ci id="S3.F3.5.m1.1.1.2.cmml" xref="S3.F3.5.m1.1.1.2">𝐼</ci><cn type="integer" id="S3.F3.5.m1.1.1.3.cmml" xref="S3.F3.5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.m1.1d">I^{1}</annotation></semantics></math> and <math id="S3.F3.6.m2.1" class="ltx_Math" alttext="I^{2}" display="inline"><semantics id="S3.F3.6.m2.1b"><msup id="S3.F3.6.m2.1.1" xref="S3.F3.6.m2.1.1.cmml"><mi id="S3.F3.6.m2.1.1.2" xref="S3.F3.6.m2.1.1.2.cmml">I</mi><mn id="S3.F3.6.m2.1.1.3" xref="S3.F3.6.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F3.6.m2.1c"><apply id="S3.F3.6.m2.1.1.cmml" xref="S3.F3.6.m2.1.1"><csymbol cd="ambiguous" id="S3.F3.6.m2.1.1.1.cmml" xref="S3.F3.6.m2.1.1">superscript</csymbol><ci id="S3.F3.6.m2.1.1.2.cmml" xref="S3.F3.6.m2.1.1.2">𝐼</ci><cn type="integer" id="S3.F3.6.m2.1.1.3.cmml" xref="S3.F3.6.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.m2.1d">I^{2}</annotation></semantics></math> is performed in computing resource 1, while that of <math id="S3.F3.7.m3.1" class="ltx_Math" alttext="I^{3}" display="inline"><semantics id="S3.F3.7.m3.1b"><msup id="S3.F3.7.m3.1.1" xref="S3.F3.7.m3.1.1.cmml"><mi id="S3.F3.7.m3.1.1.2" xref="S3.F3.7.m3.1.1.2.cmml">I</mi><mn id="S3.F3.7.m3.1.1.3" xref="S3.F3.7.m3.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F3.7.m3.1c"><apply id="S3.F3.7.m3.1.1.cmml" xref="S3.F3.7.m3.1.1"><csymbol cd="ambiguous" id="S3.F3.7.m3.1.1.1.cmml" xref="S3.F3.7.m3.1.1">superscript</csymbol><ci id="S3.F3.7.m3.1.1.2.cmml" xref="S3.F3.7.m3.1.1.2">𝐼</ci><cn type="integer" id="S3.F3.7.m3.1.1.3.cmml" xref="S3.F3.7.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.m3.1d">I^{3}</annotation></semantics></math> and <math id="S3.F3.8.m4.1" class="ltx_Math" alttext="I^{4}" display="inline"><semantics id="S3.F3.8.m4.1b"><msup id="S3.F3.8.m4.1.1" xref="S3.F3.8.m4.1.1.cmml"><mi id="S3.F3.8.m4.1.1.2" xref="S3.F3.8.m4.1.1.2.cmml">I</mi><mn id="S3.F3.8.m4.1.1.3" xref="S3.F3.8.m4.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F3.8.m4.1c"><apply id="S3.F3.8.m4.1.1.cmml" xref="S3.F3.8.m4.1.1"><csymbol cd="ambiguous" id="S3.F3.8.m4.1.1.1.cmml" xref="S3.F3.8.m4.1.1">superscript</csymbol><ci id="S3.F3.8.m4.1.1.2.cmml" xref="S3.F3.8.m4.1.1.2">𝐼</ci><cn type="integer" id="S3.F3.8.m4.1.1.3.cmml" xref="S3.F3.8.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.m4.1d">I^{4}</annotation></semantics></math> is performed in computing resource 2 at the same time. Then the model or gradient is transferred to calculate an average model or gradient to be sent to each computing resource for the following training.</figcaption>
</figure>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Data Parallelism</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Data parallelism is realized by having the data processing performed in parallel at different computing resources, with the same model, on different data points. As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Parallelism &amp; FL Types ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, data parallelism is exploited when the ensemble of data points is distributed among different computing resources. During the training process of FL, the training data is not transferred among different computing resources, while the intermediate data, e.g., the models or the gradients <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\nabla F_{k}(x)" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml"><mrow id="S3.SS1.SSS1.p1.1.m1.1.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.cmml"><mo rspace="0.167em" id="S3.SS1.SSS1.p1.1.m1.1.2.2.1" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.1.cmml">∇</mo><msub id="S3.SS1.SSS1.p1.1.m1.1.2.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2.cmml"><mi id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2.2.cmml">F</mi><mi id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.3" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p1.1.m1.1.2.1" xref="S3.SS1.SSS1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS1.p1.1.m1.1.2.3.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.2.3.2.1" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.1.2.3.2.2" xref="S3.SS1.SSS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2"><times id="S3.SS1.SSS1.p1.1.m1.1.2.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.1"></times><apply id="S3.SS1.SSS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2"><ci id="S3.SS1.SSS1.p1.1.m1.1.2.2.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.1">∇</ci><apply id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2.2">𝐹</ci><ci id="S3.SS1.SSS1.p1.1.m1.1.2.2.2.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.2.2.2.3">𝑘</ci></apply></apply><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">\nabla F_{k}(x)</annotation></semantics></math> in Formula <a href="#S2.E2" title="In 2.1 Basic Concepts ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, are transferred.
The data in each computing resource can be Independent and Identically Distributed Data (IID) or non-IID. FL focuses on the non-IID <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite>, while other distributed machine learning approaches mainly focus on IID data.
With the data parallelism, the FL is horizontal <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>, i.e., the data and the calculation are horizontally distributed among multiple computing resources.
In addition, this parallelism generally corresponds to the cross-device FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>, where a large number of devices (mobiles or edge devices) collaboratively participate in training a single global model to have good accuracy.
When the number of devices is small, e.g., 2-100, and the computing resources are from diverse organizations, this parallelism also corresponds to cross-silo FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>. In addition to the general data-parallel schemes for federated learning, some specific privacy-preserved distributed statistical tricks have been invented for federated sparse models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">bian2017multi </a>; <a href="#bib.bib17" title="" class="ltx_ref">bian2020mp2sda </a></cite>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2104.14362/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model parallelism. The dashed arrows represent inter-computing resource communication. For each input data point <math id="S3.F4.4.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.F4.4.m1.1b"><mi id="S3.F4.4.m1.1.1" xref="S3.F4.4.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.F4.4.m1.1c"><ci id="S3.F4.4.m1.1.1.cmml" xref="S3.F4.4.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m1.1d">I</annotation></semantics></math>, different parts, i.e., <math id="S3.F4.5.m2.1" class="ltx_Math" alttext="I_{A_{1}}" display="inline"><semantics id="S3.F4.5.m2.1b"><msub id="S3.F4.5.m2.1.1" xref="S3.F4.5.m2.1.1.cmml"><mi id="S3.F4.5.m2.1.1.2" xref="S3.F4.5.m2.1.1.2.cmml">I</mi><msub id="S3.F4.5.m2.1.1.3" xref="S3.F4.5.m2.1.1.3.cmml"><mi id="S3.F4.5.m2.1.1.3.2" xref="S3.F4.5.m2.1.1.3.2.cmml">A</mi><mn id="S3.F4.5.m2.1.1.3.3" xref="S3.F4.5.m2.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.F4.5.m2.1c"><apply id="S3.F4.5.m2.1.1.cmml" xref="S3.F4.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F4.5.m2.1.1.1.cmml" xref="S3.F4.5.m2.1.1">subscript</csymbol><ci id="S3.F4.5.m2.1.1.2.cmml" xref="S3.F4.5.m2.1.1.2">𝐼</ci><apply id="S3.F4.5.m2.1.1.3.cmml" xref="S3.F4.5.m2.1.1.3"><csymbol cd="ambiguous" id="S3.F4.5.m2.1.1.3.1.cmml" xref="S3.F4.5.m2.1.1.3">subscript</csymbol><ci id="S3.F4.5.m2.1.1.3.2.cmml" xref="S3.F4.5.m2.1.1.3.2">𝐴</ci><cn type="integer" id="S3.F4.5.m2.1.1.3.3.cmml" xref="S3.F4.5.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.m2.1d">I_{A_{1}}</annotation></semantics></math> and <math id="S3.F4.6.m3.1" class="ltx_Math" alttext="I_{A_{2}}" display="inline"><semantics id="S3.F4.6.m3.1b"><msub id="S3.F4.6.m3.1.1" xref="S3.F4.6.m3.1.1.cmml"><mi id="S3.F4.6.m3.1.1.2" xref="S3.F4.6.m3.1.1.2.cmml">I</mi><msub id="S3.F4.6.m3.1.1.3" xref="S3.F4.6.m3.1.1.3.cmml"><mi id="S3.F4.6.m3.1.1.3.2" xref="S3.F4.6.m3.1.1.3.2.cmml">A</mi><mn id="S3.F4.6.m3.1.1.3.3" xref="S3.F4.6.m3.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S3.F4.6.m3.1c"><apply id="S3.F4.6.m3.1.1.cmml" xref="S3.F4.6.m3.1.1"><csymbol cd="ambiguous" id="S3.F4.6.m3.1.1.1.cmml" xref="S3.F4.6.m3.1.1">subscript</csymbol><ci id="S3.F4.6.m3.1.1.2.cmml" xref="S3.F4.6.m3.1.1.2">𝐼</ci><apply id="S3.F4.6.m3.1.1.3.cmml" xref="S3.F4.6.m3.1.1.3"><csymbol cd="ambiguous" id="S3.F4.6.m3.1.1.3.1.cmml" xref="S3.F4.6.m3.1.1.3">subscript</csymbol><ci id="S3.F4.6.m3.1.1.3.2.cmml" xref="S3.F4.6.m3.1.1.3.2">𝐴</ci><cn type="integer" id="S3.F4.6.m3.1.1.3.3.cmml" xref="S3.F4.6.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m3.1d">I_{A_{2}}</annotation></semantics></math>, are distributed at different computing resources.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Model Parallelism</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">Model parallelism is realized by having independent data processing nodes distributed at different computing resources, so as to process the data points of specific features.
Two data processing nodes can be either independent, i.e., the execution of any node does not depend on the output of the other one; or dependent, i.e., there is a data dependency between them <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib105" title="" class="ltx_ref">liu2015survey </a></cite>.
As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.1 Data Parallelism ‣ 3.1 Parallelism &amp; FL Types ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, model parallelism is achieved when different parts of each data point are distributed at different computing resources.
For instance, the data process on Node <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">A_{1}</annotation></semantics></math> and that of <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><msub id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">A_{2}</annotation></semantics></math> can be carried out in parallel.
With the model parallelism, vertical FL, where the data points and calculation are vertically distributed among multiple computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib59" title="" class="ltx_ref">hardy2017private </a>; <a href="#bib.bib190" title="" class="ltx_ref">yang2019federated </a></cite>, is realized.
In this case, the original model needs to be partitioned to be distributed at different computing resources.
Two organizations generally apply this type of FL when each organization owns parts of the features of users and they would like to collaboratively train a model using the data of all the features, which corresponds to cross-silo FL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib79" title="" class="ltx_ref">kairouz2019advances </a></cite>.
Most studies of vertical federated learning only support two parties (with or without a central coordinator) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib205" title="" class="ltx_ref">zhu2021 </a></cite>.
For instance, SecureGBM <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref">feng2019securegbm </a></cite> is proposed to train a tree-based Gradient Boosting Machine (GBM).
In order to support multiple parties, the idea of multi-view learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib187" title="" class="ltx_ref">xu2013survey </a></cite> is exploited in a multi-participant, multi-class vertical federated learning framework <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref">feng2020multi </a></cite>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2104.14362/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Pipeline parallelism. The dashed arrows represent inter computing resource communication.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Pipeline Parallelism</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.6" class="ltx_p">Pipeline parallelism is realized by having dependent data processing nodes distributed at different computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib131" title="" class="ltx_ref">narayanan2019pipedream </a>; <a href="#bib.bib74" title="" class="ltx_ref">huang2018gpipe </a></cite>. As shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.1.2 Model Parallelism ‣ 3.1 Parallelism &amp; FL Types ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the data processing nodes are distributed at multiple computing resources. While data point <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="I^{3}_{A}" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><msubsup id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.2.cmml">I</mi><mi id="S3.SS1.SSS3.p1.1.m1.1.1.3" xref="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml">A</mi><mn id="S3.SS1.SSS3.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.3.cmml">3</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><apply id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.2">𝐼</ci><cn type="integer" id="S3.SS1.SSS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.2.3">3</cn></apply><ci id="S3.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">I^{3}_{A}</annotation></semantics></math> is processed in computing resource 1, the outputs of <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="A_{1}" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><msub id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p1.2.m2.1.1.2" xref="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS3.p1.2.m2.1.1.3" xref="S3.SS1.SSS3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><apply id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">A_{1}</annotation></semantics></math> and <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="A_{2}" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><msub id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p1.3.m3.1.1.2" xref="S3.SS1.SSS3.p1.3.m3.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS3.p1.3.m3.1.1.3" xref="S3.SS1.SSS3.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><apply id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1.2">𝐴</ci><cn type="integer" id="S3.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">A_{2}</annotation></semantics></math> are processed in computing resource 2, and the outputs of <math id="S3.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="B_{1}" display="inline"><semantics id="S3.SS1.SSS3.p1.4.m4.1a"><msub id="S3.SS1.SSS3.p1.4.m4.1.1" xref="S3.SS1.SSS3.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p1.4.m4.1.1.2" xref="S3.SS1.SSS3.p1.4.m4.1.1.2.cmml">B</mi><mn id="S3.SS1.SSS3.p1.4.m4.1.1.3" xref="S3.SS1.SSS3.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.4.m4.1b"><apply id="S3.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.4.m4.1c">B_{1}</annotation></semantics></math>, <math id="S3.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="B_{2}" display="inline"><semantics id="S3.SS1.SSS3.p1.5.m5.1a"><msub id="S3.SS1.SSS3.p1.5.m5.1.1" xref="S3.SS1.SSS3.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS3.p1.5.m5.1.1.2" xref="S3.SS1.SSS3.p1.5.m5.1.1.2.cmml">B</mi><mn id="S3.SS1.SSS3.p1.5.m5.1.1.3" xref="S3.SS1.SSS3.p1.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.5.m5.1b"><apply id="S3.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.SSS3.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.5.m5.1c">B_{2}</annotation></semantics></math>, and <math id="S3.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="B_{3}" display="inline"><semantics id="S3.SS1.SSS3.p1.6.m6.1a"><msub id="S3.SS1.SSS3.p1.6.m6.1.1" xref="S3.SS1.SSS3.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS3.p1.6.m6.1.1.2" xref="S3.SS1.SSS3.p1.6.m6.1.1.2.cmml">B</mi><mn id="S3.SS1.SSS3.p1.6.m6.1.1.3" xref="S3.SS1.SSS3.p1.6.m6.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.6.m6.1b"><apply id="S3.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1.2">𝐵</ci><cn type="integer" id="S3.SS1.SSS3.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.6.m6.1c">B_{3}</annotation></semantics></math> are processed in computing resource 3. With this type of parallelism, the dependent data processing nodes can process the data in parallel. As this parallelism may incur many inter-computing resource data transfers, it is not widely used for FL.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Aggregation Algorithms</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">With the horizontal FL and data parallelism, aggregation algorithms are used to aggregate the models or gradients generated from the forward and backward propagation in each computing resource. The aggregation algorithms can be either centralized, or hierarchical, and decentralized. The centralized aggregation algorithms generally rely on a centralized server, i.e., a parameter server, to synchronize or schedule the execution of distributed computing resources, while hierarchical aggregation algorithms rely on multiple parameter servers for the model aggregation.
The decentralized aggregation algorithms make each computing resource equally perform the calculation based on a predefined protocol, without relying on a centralized server. <span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Please refer to <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib177" title="" class="ltx_ref">Wang2021Guide </a></cite> for the details of federated optimization. The characteristics are summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which can be used to choose appropriate algorithms in a specific situation. </span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.2.1" class="ltx_text" style="color:#000000;">Comparison among diverse types of aggregation algorithms. “Complexity” represents the complexity to implement the algorithms (“H” represents high complexity, “M” represents medium complexity, and “L” represents low complexity). “Trust” represents whether the aggregation algorithms require that the data owners trust the centralized server. “Imbalance” represents whether the algorithms can address the unbalanced data. “High-latency” represents whether the algorithms can support the high-latency model or gradient data transfer. “Y” represents that the algorithms support the functionality while “N” represents that the algorithms do not support the functionality.</span></figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.3.1.1" class="ltx_tr">
<td id="S3.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Type</td>
<td id="S3.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Complexity</td>
<td id="S3.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trust</td>
<td id="S3.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Imbalance</td>
<td id="S3.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">High-latency</td>
</tr>
<tr id="S3.T1.3.2.2" class="ltx_tr">
<td id="S3.T1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Centralized</td>
<td id="S3.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">L</td>
<td id="S3.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Y</td>
<td id="S3.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Hierarchical</td>
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">M</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Y</td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Y</td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Y</td>
</tr>
<tr id="S3.T1.3.4.4" class="ltx_tr">
<td id="S3.T1.3.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Decentralized</td>
<td id="S3.T1.3.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">H</td>
<td id="S3.T1.3.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T1.3.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T1.3.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Y</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Centralized Aggregation</h4>

<figure id="S3.F6" class="ltx_figure"><img src="/html/2104.14362/assets/figures/cenagg.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The architecture of centralized aggregation. “CR” represents computer resource. <math id="S3.F6.5.m1.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S3.F6.5.m1.1b"><mi id="S3.F6.5.m1.1.1" xref="S3.F6.5.m1.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S3.F6.5.m1.1c"><ci id="S3.F6.5.m1.1.1.cmml" xref="S3.F6.5.m1.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.5.m1.1d">\omega</annotation></semantics></math> represents the local parameters or the weights of the model calculated in each computing resource. <math id="S3.F6.6.m2.1" class="ltx_Math" alttext="\mathrm{g}" display="inline"><semantics id="S3.F6.6.m2.1b"><mi mathvariant="normal" id="S3.F6.6.m2.1.1" xref="S3.F6.6.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.F6.6.m2.1c"><ci id="S3.F6.6.m2.1.1.cmml" xref="S3.F6.6.m2.1.1">g</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.6.m2.1d">\mathrm{g}</annotation></semantics></math> represents the local gradients in backward propagation in each computing resource. <math id="S3.F6.7.m3.1" class="ltx_Math" alttext="\overline{\omega}" display="inline"><semantics id="S3.F6.7.m3.1b"><mover accent="true" id="S3.F6.7.m3.1.1" xref="S3.F6.7.m3.1.1.cmml"><mi id="S3.F6.7.m3.1.1.2" xref="S3.F6.7.m3.1.1.2.cmml">ω</mi><mo id="S3.F6.7.m3.1.1.1" xref="S3.F6.7.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F6.7.m3.1c"><apply id="S3.F6.7.m3.1.1.cmml" xref="S3.F6.7.m3.1.1"><ci id="S3.F6.7.m3.1.1.1.cmml" xref="S3.F6.7.m3.1.1.1">¯</ci><ci id="S3.F6.7.m3.1.1.2.cmml" xref="S3.F6.7.m3.1.1.2">𝜔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.7.m3.1d">\overline{\omega}</annotation></semantics></math> represents the global model calculated in the parameter server. <math id="S3.F6.8.m4.1" class="ltx_Math" alttext="\overline{\mathrm{g}}" display="inline"><semantics id="S3.F6.8.m4.1b"><mover accent="true" id="S3.F6.8.m4.1.1" xref="S3.F6.8.m4.1.1.cmml"><mi mathvariant="normal" id="S3.F6.8.m4.1.1.2" xref="S3.F6.8.m4.1.1.2.cmml">g</mi><mo id="S3.F6.8.m4.1.1.1" xref="S3.F6.8.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F6.8.m4.1c"><apply id="S3.F6.8.m4.1.1.cmml" xref="S3.F6.8.m4.1.1"><ci id="S3.F6.8.m4.1.1.1.cmml" xref="S3.F6.8.m4.1.1.1">¯</ci><ci id="S3.F6.8.m4.1.1.2.cmml" xref="S3.F6.8.m4.1.1.2">g</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.8.m4.1d">\overline{\mathrm{g}}</annotation></semantics></math> represents the global gradients calculated in the parameter server.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2.1 Centralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, a single parameter server is used to calculate the average models or gradients sent from multiple computing resources (mobiles).
The weights of the model (model) or the gradients are calculated in each computing resource, which are transferred to a parameter server.
The parameter server calculates global gradients or global models according to a centralized aggregation algorithm.
The global gradients or global models are transferred to each computing resource for the following computation.
The update of the model is based on the SGD defined in Formula <a href="#S2.E2" title="In 2.1 Basic Concepts ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> in both computing resources, or on the parameter server.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.6" class="ltx_p">A bunch of centralized aggregation algorithms have been proposed.
Federated Averaging (FedAvg) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite> algorithm is introduced as the aggregation method in Google’s implementation of an FL system. A centralized server aggregates the machine learning models from selected users. Then, a global model is generated using a weighted sum of each aggregated machine learning model. Afterward, the global model is shared with selected users, and the training process is continued in the computing resource of selected users.
However, the trained model of FedAvg may be biased towards computing resources with favorable network conditions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib91" title="" class="ltx_ref">li2020federated </a></cite>.
While FedAvg is a straightforward approach, some other methods are proposed to address additional problems.
A Federated Stochastic Block Coordinate Descent (FedBCD) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib110" title="" class="ltx_ref">liu2019communication </a></cite> algorithm is proposed to reduce the number of communication rounds by enabling multiple local updates before the model communication between a user and the server. In addition, FedBCD also considers the regularization during the training process. The training problem with regularization can be formulated as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.6" class="ltx_Math" alttext="\widehat{F}=\underset{F_{\theta}\in\mathcal{H}}{\mathrm{argmin}}\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}L(y,F_{\theta}(x))+\lambda\cdot\gamma(\theta)," display="block"><semantics id="S3.E3.m1.6a"><mrow id="S3.E3.m1.6.6.1" xref="S3.E3.m1.6.6.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1" xref="S3.E3.m1.6.6.1.1.cmml"><mover accent="true" id="S3.E3.m1.6.6.1.1.3" xref="S3.E3.m1.6.6.1.1.3.cmml"><mi id="S3.E3.m1.6.6.1.1.3.2" xref="S3.E3.m1.6.6.1.1.3.2.cmml">F</mi><mo id="S3.E3.m1.6.6.1.1.3.1" xref="S3.E3.m1.6.6.1.1.3.1.cmml">^</mo></mover><mo id="S3.E3.m1.6.6.1.1.2" xref="S3.E3.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.6.6.1.1.1" xref="S3.E3.m1.6.6.1.1.1.cmml"><mrow id="S3.E3.m1.6.6.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.cmml"><munder accentunder="true" id="S3.E3.m1.6.6.1.1.1.1.3" xref="S3.E3.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E3.m1.6.6.1.1.1.1.3.2" xref="S3.E3.m1.6.6.1.1.1.1.3.2.cmml">argmin</mi><mrow id="S3.E3.m1.6.6.1.1.1.1.3.1" xref="S3.E3.m1.6.6.1.1.1.1.3.1.cmml"><msub id="S3.E3.m1.6.6.1.1.1.1.3.1.2" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2.cmml"><mi id="S3.E3.m1.6.6.1.1.1.1.3.1.2.2" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2.2.cmml">F</mi><mi id="S3.E3.m1.6.6.1.1.1.1.3.1.2.3" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2.3.cmml">θ</mi></msub><mo id="S3.E3.m1.6.6.1.1.1.1.3.1.1" xref="S3.E3.m1.6.6.1.1.1.1.3.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.6.6.1.1.1.1.3.1.3" xref="S3.E3.m1.6.6.1.1.1.1.3.1.3.cmml">ℋ</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.1.2.cmml">​</mo><munder accentunder="true" id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mi id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml">𝔼</mi><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.4.2" xref="S3.E3.m1.2.2.2.4.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.4.2.1" xref="S3.E3.m1.2.2.2.4.1.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">x</mi><mo id="S3.E3.m1.2.2.2.4.2.2" xref="S3.E3.m1.2.2.2.4.1.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">y</mi><mo stretchy="false" id="S3.E3.m1.2.2.2.4.2.3" xref="S3.E3.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.2.5" xref="S3.E3.m1.2.2.2.5.cmml">𝒟</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.1.1.2a" xref="S3.E3.m1.6.6.1.1.1.1.2.cmml">​</mo><mi id="S3.E3.m1.6.6.1.1.1.1.4" xref="S3.E3.m1.6.6.1.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.1.1.2b" xref="S3.E3.m1.6.6.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.6.6.1.1.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml">y</mi><mo id="S3.E3.m1.6.6.1.1.1.1.1.1.3" xref="S3.E3.m1.6.6.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.E3.m1.6.6.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.2.cmml">F</mi><mi id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E3.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">x</mi><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.1.1.1.4" xref="S3.E3.m1.6.6.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.6.6.1.1.1.2" xref="S3.E3.m1.6.6.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.6.6.1.1.1.3" xref="S3.E3.m1.6.6.1.1.1.3.cmml"><mrow id="S3.E3.m1.6.6.1.1.1.3.2" xref="S3.E3.m1.6.6.1.1.1.3.2.cmml"><mi id="S3.E3.m1.6.6.1.1.1.3.2.2" xref="S3.E3.m1.6.6.1.1.1.3.2.2.cmml">λ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.6.6.1.1.1.3.2.1" xref="S3.E3.m1.6.6.1.1.1.3.2.1.cmml">⋅</mo><mi id="S3.E3.m1.6.6.1.1.1.3.2.3" xref="S3.E3.m1.6.6.1.1.1.3.2.3.cmml">γ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.1.1.1.3.1" xref="S3.E3.m1.6.6.1.1.1.3.1.cmml">​</mo><mrow id="S3.E3.m1.6.6.1.1.1.3.3.2" xref="S3.E3.m1.6.6.1.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.3.3.2.1" xref="S3.E3.m1.6.6.1.1.1.3.cmml">(</mo><mi id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml">θ</mi><mo stretchy="false" id="S3.E3.m1.6.6.1.1.1.3.3.2.2" xref="S3.E3.m1.6.6.1.1.1.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.6.6.1.2" xref="S3.E3.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.6b"><apply id="S3.E3.m1.6.6.1.1.cmml" xref="S3.E3.m1.6.6.1"><eq id="S3.E3.m1.6.6.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.2"></eq><apply id="S3.E3.m1.6.6.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.3"><ci id="S3.E3.m1.6.6.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.3.1">^</ci><ci id="S3.E3.m1.6.6.1.1.3.2.cmml" xref="S3.E3.m1.6.6.1.1.3.2">𝐹</ci></apply><apply id="S3.E3.m1.6.6.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1"><plus id="S3.E3.m1.6.6.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.2"></plus><apply id="S3.E3.m1.6.6.1.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1"><times id="S3.E3.m1.6.6.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.2"></times><apply id="S3.E3.m1.6.6.1.1.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3"><apply id="S3.E3.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1"><in id="S3.E3.m1.6.6.1.1.1.1.3.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.1"></in><apply id="S3.E3.m1.6.6.1.1.1.1.3.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.1.1.3.1.2.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.1.1.3.1.2.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2.2">𝐹</ci><ci id="S3.E3.m1.6.6.1.1.1.1.3.1.2.3.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.2.3">𝜃</ci></apply><ci id="S3.E3.m1.6.6.1.1.1.1.3.1.3.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.1.3">ℋ</ci></apply><ci id="S3.E3.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.3.2">argmin</ci></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3">similar-to</csymbol><interval closure="open" id="S3.E3.m1.2.2.2.4.1.cmml" xref="S3.E3.m1.2.2.2.4.2"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑥</ci><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">𝑦</ci></interval><ci id="S3.E3.m1.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.5">𝒟</ci></apply><ci id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3">𝔼</ci></apply><ci id="S3.E3.m1.6.6.1.1.1.1.4.cmml" xref="S3.E3.m1.6.6.1.1.1.1.4">𝐿</ci><interval closure="open" id="S3.E3.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1"><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">𝑦</ci><apply id="S3.E3.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1"><times id="S3.E3.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.1"></times><apply id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.2">𝐹</ci><ci id="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.6.6.1.1.1.1.1.1.1.2.3">𝜃</ci></apply><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">𝑥</ci></apply></interval></apply><apply id="S3.E3.m1.6.6.1.1.1.3.cmml" xref="S3.E3.m1.6.6.1.1.1.3"><times id="S3.E3.m1.6.6.1.1.1.3.1.cmml" xref="S3.E3.m1.6.6.1.1.1.3.1"></times><apply id="S3.E3.m1.6.6.1.1.1.3.2.cmml" xref="S3.E3.m1.6.6.1.1.1.3.2"><ci id="S3.E3.m1.6.6.1.1.1.3.2.1.cmml" xref="S3.E3.m1.6.6.1.1.1.3.2.1">⋅</ci><ci id="S3.E3.m1.6.6.1.1.1.3.2.2.cmml" xref="S3.E3.m1.6.6.1.1.1.3.2.2">𝜆</ci><ci id="S3.E3.m1.6.6.1.1.1.3.2.3.cmml" xref="S3.E3.m1.6.6.1.1.1.3.2.3">𝛾</ci></apply><ci id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.6c">\widehat{F}=\underset{F_{\theta}\in\mathcal{H}}{\mathrm{argmin}}\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}L(y,F_{\theta}(x))+\lambda\cdot\gamma(\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.5" class="ltx_p">where <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">F</annotation></semantics></math>, <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mi id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><ci id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">D</annotation></semantics></math>, <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">\mathcal{H}</annotation></semantics></math> are the same as those in Formula <a href="#S2.E1" title="In 2.1 Basic Concepts ‣ 2 An Overview of Federated Learning ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, while <math id="S3.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\gamma(\cdot)" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.1a"><mrow id="S3.SS2.SSS1.p2.4.m4.1.2" xref="S3.SS2.SSS1.p2.4.m4.1.2.cmml"><mi id="S3.SS2.SSS1.p2.4.m4.1.2.2" xref="S3.SS2.SSS1.p2.4.m4.1.2.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.4.m4.1.2.1" xref="S3.SS2.SSS1.p2.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS2.SSS1.p2.4.m4.1.2.3.2" xref="S3.SS2.SSS1.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p2.4.m4.1.2.3.2.1" xref="S3.SS2.SSS1.p2.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.SSS1.p2.4.m4.1.2.3.2.2" xref="S3.SS2.SSS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.1b"><apply id="S3.SS2.SSS1.p2.4.m4.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.2"><times id="S3.SS2.SSS1.p2.4.m4.1.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.2.1"></times><ci id="S3.SS2.SSS1.p2.4.m4.1.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.2.2">𝛾</ci><ci id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.1c">\gamma(\cdot)</annotation></semantics></math> denotes the regularizer and <math id="S3.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.SSS1.p2.5.m5.1a"><mi id="S3.SS2.SSS1.p2.5.m5.1.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.5.m5.1b"><ci id="S3.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.5.m5.1c">\lambda</annotation></semantics></math> is the hyper-parameter. The regularization is exploited to improve the generalization capacity of the trained machine learning model. As the fairness among multiple users is important for an FL system, the Stochastic Agnostic Federated Learning (SAFL) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib128" title="" class="ltx_ref">mohri2019agnostic </a></cite> algorithm and the FedMGDA+ <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib73" title="" class="ltx_ref">hu2020fedmgda </a></cite> algorithm are proposed to achieve fairness during the training process of FL. The fairness represents that the data distribution among multiple users can be equally considered without the influence of unrelated factors.
Fairness may also refer to two other concepts: (1) A user gets a final model according to the contribution <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib115" title="" class="ltx_ref">lyu2020towards </a></cite>; and/or (2) Uniform accuracy distribution among all the distributed computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib93" title="" class="ltx_ref">li2019fair </a></cite>, which are out of the scope of this paper.
In addition, while the computing resources may be heterogeneous, FedProx <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib92" title="" class="ltx_ref">Li2020 </a></cite> is proposed to tackle the heterogeneity in an FL system. FedProx enables multiple iterations in each computing resource, while minimizing a cost function based on the local loss function and the global model. Furthermore, in order to address permutation of data processing nodes during the training process, Federated Matched Averaging (FedMA) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib176" title="" class="ltx_ref">Wang2020Federated </a></cite> is proposed. FedMA exploits an existing approach, i.e., BBP-MAP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib193" title="" class="ltx_ref">yurochkin2019bayesian </a></cite>, to generate a matrix, in order to align the data processing nodes of the models from computing resources and the server. SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib81" title="" class="ltx_ref">karimireddy2020scaffold </a></cite> is proposed to reduce the communication rounds, using stateful variables in the distributed computing resources. Attention-augmented mechanism is exploited in Attentive Federated Aggregation (FedAttOpt) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib77" title="" class="ltx_ref">jiang2020decentralized </a></cite> to aggregate the knowledge generated from each computing resource (client), based on the contribution of the model from each client.
When the data distribution is heterogeneous among users, personalization remains an open problem. In order to address this problem, the model can be split into local layers and global layers, which has been proposed in adaptive personalized federated learning (APFL) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">deng2020adaptive </a></cite>, FedPer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">arivazhagan2019federated </a></cite>, and pFedMe <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref">dinh2020personalized </a></cite>. The local layers are trained with the decentralized data in each computing resource of users, while the global layers are trained in the computing resources of users and the server. However, it is difficult to choose a dataset and its partition among clients to measure the personalization brought by APFL or FedPer, so as to prove the improvement compared with FedAvg.
The attention-augmented mechanism helps reduce the communication rounds.
In addition, knowledge distillation can also be exploited to aggregate the models, while requiring that there is data in the centralized server <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib60" title="" class="ltx_ref">He2020Group </a></cite>.
All these algorithms can handle non-IID data. A comparison among the aforementioned algorithms is proposed in Table <a href="#S3.T2" title="Table 2 ‣ 3.2.1 Centralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison among aggregation algorithms. “Reg” represents regularization. Heterogeneity represents that the computing resources are heterogeneous. “Firness” represents that the data distribution among multiple users can be equally considered without the influence of unrelated factors. “Permutation” refers to the permutation of data processing nodes during the training process. “C-E” represents communication efficient. “S” represents that the algorithm supports the functionality, while “N” represents that the algorithm does not have support. </figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Algorithm</td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Reg</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Fairness</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Heterogeneity</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Permutation</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">C-E</td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedAvg</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedBCD</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SAFL</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedMGDA+</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedProx</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<td id="S3.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FedMA</td>
<td id="S3.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
<td id="S3.T2.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
</tr>
<tr id="S3.T2.1.8.8" class="ltx_tr">
<td id="S3.T2.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SCAFFOLD</td>
<td id="S3.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">S</td>
</tr>
<tr id="S3.T2.1.9.9" class="ltx_tr">
<td id="S3.T2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">FedAttOpt</td>
<td id="S3.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">N</td>
<td id="S3.T2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">S</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Hierarchical Aggregation</h4>

<figure id="S3.F7" class="ltx_figure"><img src="/html/2104.14362/assets/figures/hier.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The architecture of hierarchical aggregation. “CR” represents computer resource. <math id="S3.F7.5.m1.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S3.F7.5.m1.1b"><mi id="S3.F7.5.m1.1.1" xref="S3.F7.5.m1.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S3.F7.5.m1.1c"><ci id="S3.F7.5.m1.1.1.cmml" xref="S3.F7.5.m1.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.5.m1.1d">\omega</annotation></semantics></math> represents the local parameters or the weights of the model calculated in each computing resource. <math id="S3.F7.6.m2.1" class="ltx_Math" alttext="\mathrm{g}" display="inline"><semantics id="S3.F7.6.m2.1b"><mi mathvariant="normal" id="S3.F7.6.m2.1.1" xref="S3.F7.6.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.F7.6.m2.1c"><ci id="S3.F7.6.m2.1.1.cmml" xref="S3.F7.6.m2.1.1">g</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.6.m2.1d">\mathrm{g}</annotation></semantics></math> represents the local gradients in backward propagation in each computing resource. <math id="S3.F7.7.m3.1" class="ltx_Math" alttext="\overline{\omega}" display="inline"><semantics id="S3.F7.7.m3.1b"><mover accent="true" id="S3.F7.7.m3.1.1" xref="S3.F7.7.m3.1.1.cmml"><mi id="S3.F7.7.m3.1.1.2" xref="S3.F7.7.m3.1.1.2.cmml">ω</mi><mo id="S3.F7.7.m3.1.1.1" xref="S3.F7.7.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F7.7.m3.1c"><apply id="S3.F7.7.m3.1.1.cmml" xref="S3.F7.7.m3.1.1"><ci id="S3.F7.7.m3.1.1.1.cmml" xref="S3.F7.7.m3.1.1.1">¯</ci><ci id="S3.F7.7.m3.1.1.2.cmml" xref="S3.F7.7.m3.1.1.2">𝜔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.7.m3.1d">\overline{\omega}</annotation></semantics></math> represents the region or global model calculated in each parameter server. <math id="S3.F7.8.m4.1" class="ltx_Math" alttext="\overline{\mathrm{g}}" display="inline"><semantics id="S3.F7.8.m4.1b"><mover accent="true" id="S3.F7.8.m4.1.1" xref="S3.F7.8.m4.1.1.cmml"><mi mathvariant="normal" id="S3.F7.8.m4.1.1.2" xref="S3.F7.8.m4.1.1.2.cmml">g</mi><mo id="S3.F7.8.m4.1.1.1" xref="S3.F7.8.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.F7.8.m4.1c"><apply id="S3.F7.8.m4.1.1.cmml" xref="S3.F7.8.m4.1.1"><ci id="S3.F7.8.m4.1.1.1.cmml" xref="S3.F7.8.m4.1.1.1">¯</ci><ci id="S3.F7.8.m4.1.1.2.cmml" xref="S3.F7.8.m4.1.1.2">g</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.8.m4.1d">\overline{\mathrm{g}}</annotation></semantics></math> represents the region or global gradients calculated in each parameter server. The region model or gradients are calculated by a region parameter server, while the global model or gradients are calculated by a global parameter server.</figcaption>
</figure>
<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.2.2 Hierarchical Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, a hierarchical architecture is also exploited using multiple parameter servers.
A two-layer hierarchical architecture is proposed to reduce the time to transfer models between a parameter server and computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">abad2020hierarchical </a></cite>. The hierarchical architecture uses a global parameter server (GPS) and multiple region parameter servers. Each region parameter server (RPS) is implemented in a cell base station where the computing resources (mobiles) can be connected, with low latency. A hierarchical algorithm, i.e., Hierarchical Federated Learning (HFL) is deployed to realize the model aggregation. Within each iteration of HFL, each RPS calculates an average model using the models of the computing resources within its cluster. It sends the averaged model to the GPS, and it receives a global averaged model at every certain iteration. Afterward, it broadcasts the averaged model to all its computing resources. Some other algorithms, e.g., HierFAVG <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib107" title="" class="ltx_ref">liu2020client </a></cite>, HFEL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib112" title="" class="ltx_ref">luo2020hfel </a></cite>, and LanFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib192" title="" class="ltx_ref">yuan2020hierarchical </a></cite>, are similar to HFL, while the SPS is an edge or Local-Area Network (LAN) parameter server and the MPS is a parameter server implemented on the cloud or a Wide-Area Network (WAN). These algorithms take advantage of hierarchical architecture to reduce high-latency model or gradient data transfer, so as to accelerate the training process. In addition, by well-clustering the computing resources to groups, the hierarchical architecture is also exploited to address unbalanced data distributed among multiple computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">briggs2020federated </a>; <a href="#bib.bib126" title="" class="ltx_ref">mhaisen2021optimal </a></cite>, or to address data privacy <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib172" title="" class="ltx_ref">wainakh2020enhancing </a></cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Decentralized Aggregation</h4>

<figure id="S3.F8" class="ltx_figure"><img src="/html/2104.14362/assets/figures/decentral.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The architecture of decentralized aggregation. “CR” represents computer resource. <math id="S3.F8.3.m1.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S3.F8.3.m1.1b"><mi id="S3.F8.3.m1.1.1" xref="S3.F8.3.m1.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S3.F8.3.m1.1c"><ci id="S3.F8.3.m1.1.1.cmml" xref="S3.F8.3.m1.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.3.m1.1d">\omega</annotation></semantics></math> represents the local parameters, or the weights of the model calculated in each computing resource. <math id="S3.F8.4.m2.1" class="ltx_Math" alttext="\mathrm{g}" display="inline"><semantics id="S3.F8.4.m2.1b"><mi mathvariant="normal" id="S3.F8.4.m2.1.1" xref="S3.F8.4.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.F8.4.m2.1c"><ci id="S3.F8.4.m2.1.1.cmml" xref="S3.F8.4.m2.1.1">g</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.4.m2.1d">\mathrm{g}</annotation></semantics></math> represents the local gradients in backward propagation in each computing resource. When two computing resources are neighbors, they can communicate with each other.</figcaption>
</figure>
<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.3" class="ltx_p">While collaboratively training a machine learning model with a decentralized aggregation algorithm, the computing resources can be organized with a <span id="S3.SS2.SSS3.p1.3.1" class="ltx_text ltx_font_italic">connected</span> topology and can communicate with a peer-to-peer manner, as shown in Figure <a href="#S3.F8" title="Figure 8 ‣ 3.2.3 Decentralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The degree and connectivity of the topology affects the communication efficiency and the convergence rate of the aggregation algorithm.
For a given topology, we define <math id="S3.SS2.SSS3.p1.1.m1.2" class="ltx_Math" alttext="w_{i,j}" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.2a"><msub id="S3.SS2.SSS3.p1.1.m1.2.3" xref="S3.SS2.SSS3.p1.1.m1.2.3.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.2.3.2" xref="S3.SS2.SSS3.p1.1.m1.2.3.2.cmml">w</mi><mrow id="S3.SS2.SSS3.p1.1.m1.2.2.2.4" xref="S3.SS2.SSS3.p1.1.m1.2.2.2.3.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.1.1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.SSS3.p1.1.m1.2.2.2.4.1" xref="S3.SS2.SSS3.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS2.SSS3.p1.1.m1.2.2.2.2" xref="S3.SS2.SSS3.p1.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.2b"><apply id="S3.SS2.SSS3.p1.1.m1.2.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.2.3.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.3">subscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.2.3.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.3.2">𝑤</ci><list id="S3.SS2.SSS3.p1.1.m1.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.2.4"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS2.SSS3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.2c">w_{i,j}</annotation></semantics></math>, the weight to scale information flowing from node <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">j</annotation></semantics></math> to node <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mi id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><ci id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">i</annotation></semantics></math>, as follows</p>
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.6" class="ltx_Math" alttext="\displaystyle w_{ij}\begin{cases}&gt;0&amp;\mbox{if node $j$ is connected to node $i$, or $i=j$;}\\
=0&amp;\mbox{otherwise.}\vspace{-2mm}\end{cases}" display="inline"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.7" xref="S3.E4.m1.6.7.cmml"><msub id="S3.E4.m1.6.7.2" xref="S3.E4.m1.6.7.2.cmml"><mi id="S3.E4.m1.6.7.2.2" xref="S3.E4.m1.6.7.2.2.cmml">w</mi><mrow id="S3.E4.m1.6.7.2.3" xref="S3.E4.m1.6.7.2.3.cmml"><mi id="S3.E4.m1.6.7.2.3.2" xref="S3.E4.m1.6.7.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.7.2.3.1" xref="S3.E4.m1.6.7.2.3.1.cmml">​</mo><mi id="S3.E4.m1.6.7.2.3.3" xref="S3.E4.m1.6.7.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.7.1" xref="S3.E4.m1.6.7.1.cmml">​</mo><mrow id="S3.E4.m1.6.6a" xref="S3.E4.m1.6.7.3.1.cmml"><mo id="S3.E4.m1.6.6a.7" xref="S3.E4.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E4.m1.6.6.6a" xref="S3.E4.m1.6.7.3.1.cmml"><mtr id="S3.E4.m1.6.6.6aa" xref="S3.E4.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ab" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.4.4.4.4.4.1" xref="S3.E4.m1.4.4.4.4.4.1.cmml"><mi id="S3.E4.m1.4.4.4.4.4.1.2" xref="S3.E4.m1.4.4.4.4.4.1.2.cmml"></mi><mo id="S3.E4.m1.4.4.4.4.4.1.1" xref="S3.E4.m1.4.4.4.4.4.1.1.cmml">&gt;</mo><mn id="S3.E4.m1.4.4.4.4.4.1.3" xref="S3.E4.m1.4.4.4.4.4.1.3.cmml">0</mn></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ac" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.3.3.3.3.3.3" xref="S3.E4.m1.3.3.3.3.3.3e.cmml"><mtext id="S3.E4.m1.3.3.3.3.3.3a" xref="S3.E4.m1.3.3.3.3.3.3e.cmml">if node </mtext><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml">j</mi><mtext id="S3.E4.m1.3.3.3.3.3.3b" xref="S3.E4.m1.3.3.3.3.3.3e.cmml"> is connected to node </mtext><mi id="S3.E4.m1.2.2.2.2.2.2.2.2.m2.1.1" xref="S3.E4.m1.2.2.2.2.2.2.2.2.m2.1.1.cmml">i</mi><mtext id="S3.E4.m1.3.3.3.3.3.3c" xref="S3.E4.m1.3.3.3.3.3.3e.cmml">, or </mtext><mrow id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.cmml"><mi id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.2" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.2.cmml">i</mi><mo id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.1" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.1.cmml">=</mo><mi id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.3" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.3.cmml">j</mi></mrow><mtext id="S3.E4.m1.3.3.3.3.3.3d" xref="S3.E4.m1.3.3.3.3.3.3e.cmml">;</mtext></mrow></mtd></mtr><mtr id="S3.E4.m1.6.6.6ad" xref="S3.E4.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6ae" xref="S3.E4.m1.6.7.3.1.cmml"><mrow id="S3.E4.m1.5.5.5.5.1.1" xref="S3.E4.m1.5.5.5.5.1.1.cmml"><mi id="S3.E4.m1.5.5.5.5.1.1.2" xref="S3.E4.m1.5.5.5.5.1.1.2.cmml"></mi><mo id="S3.E4.m1.5.5.5.5.1.1.1" xref="S3.E4.m1.5.5.5.5.1.1.1.cmml">=</mo><mn id="S3.E4.m1.5.5.5.5.1.1.3" xref="S3.E4.m1.5.5.5.5.1.1.3.cmml">0</mn></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E4.m1.6.6.6af" xref="S3.E4.m1.6.7.3.1.cmml"><mtext id="S3.E4.m1.6.6.6.6.2.1" xref="S3.E4.m1.6.6.6.6.2.1a.cmml">otherwise.</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.7.cmml" xref="S3.E4.m1.6.7"><times id="S3.E4.m1.6.7.1.cmml" xref="S3.E4.m1.6.7.1"></times><apply id="S3.E4.m1.6.7.2.cmml" xref="S3.E4.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.7.2.1.cmml" xref="S3.E4.m1.6.7.2">subscript</csymbol><ci id="S3.E4.m1.6.7.2.2.cmml" xref="S3.E4.m1.6.7.2.2">𝑤</ci><apply id="S3.E4.m1.6.7.2.3.cmml" xref="S3.E4.m1.6.7.2.3"><times id="S3.E4.m1.6.7.2.3.1.cmml" xref="S3.E4.m1.6.7.2.3.1"></times><ci id="S3.E4.m1.6.7.2.3.2.cmml" xref="S3.E4.m1.6.7.2.3.2">𝑖</ci><ci id="S3.E4.m1.6.7.2.3.3.cmml" xref="S3.E4.m1.6.7.2.3.3">𝑗</ci></apply></apply><apply id="S3.E4.m1.6.7.3.1.cmml" xref="S3.E4.m1.6.6a"><csymbol cd="latexml" id="S3.E4.m1.6.7.3.1.1.cmml" xref="S3.E4.m1.6.6a.7">cases</csymbol><apply id="S3.E4.m1.4.4.4.4.4.1.cmml" xref="S3.E4.m1.4.4.4.4.4.1"><gt id="S3.E4.m1.4.4.4.4.4.1.1.cmml" xref="S3.E4.m1.4.4.4.4.4.1.1"></gt><csymbol cd="latexml" id="S3.E4.m1.4.4.4.4.4.1.2.cmml" xref="S3.E4.m1.4.4.4.4.4.1.2">absent</csymbol><cn type="integer" id="S3.E4.m1.4.4.4.4.4.1.3.cmml" xref="S3.E4.m1.4.4.4.4.4.1.3">0</cn></apply><ci id="S3.E4.m1.3.3.3.3.3.3e.cmml" xref="S3.E4.m1.3.3.3.3.3.3"><mrow id="S3.E4.m1.3.3.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3"><mtext id="S3.E4.m1.3.3.3.3.3.3a.cmml" xref="S3.E4.m1.3.3.3.3.3.3">if node </mtext><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.m1.1.1">j</mi><mtext id="S3.E4.m1.3.3.3.3.3.3b.cmml" xref="S3.E4.m1.3.3.3.3.3.3"> is connected to node </mtext><mi id="S3.E4.m1.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.2.2.m2.1.1">i</mi><mtext id="S3.E4.m1.3.3.3.3.3.3c.cmml" xref="S3.E4.m1.3.3.3.3.3.3">, or </mtext><mrow id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1"><mi id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.2.cmml" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.2">i</mi><mo id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.1.cmml" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.1">=</mo><mi id="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.3.cmml" xref="S3.E4.m1.3.3.3.3.3.3.3.3.m3.1.1.3">j</mi></mrow><mtext id="S3.E4.m1.3.3.3.3.3.3d.cmml" xref="S3.E4.m1.3.3.3.3.3.3">;</mtext></mrow></ci><apply id="S3.E4.m1.5.5.5.5.1.1.cmml" xref="S3.E4.m1.5.5.5.5.1.1"><eq id="S3.E4.m1.5.5.5.5.1.1.1.cmml" xref="S3.E4.m1.5.5.5.5.1.1.1"></eq><csymbol cd="latexml" id="S3.E4.m1.5.5.5.5.1.1.2.cmml" xref="S3.E4.m1.5.5.5.5.1.1.2">absent</csymbol><cn type="integer" id="S3.E4.m1.5.5.5.5.1.1.3.cmml" xref="S3.E4.m1.5.5.5.5.1.1.3">0</cn></apply><ci id="S3.E4.m1.6.6.6.6.2.1a.cmml" xref="S3.E4.m1.6.6.6.6.2.1"><mtext id="S3.E4.m1.6.6.6.6.2.1.cmml" xref="S3.E4.m1.6.6.6.6.2.1">otherwise.</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\displaystyle w_{ij}\begin{cases}&gt;0&amp;\mbox{if node $j$ is connected to node $i$, or $i=j$;}\\
=0&amp;\mbox{otherwise.}\vspace{-2mm}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS3.p1.17" class="ltx_p">We further define the <span id="S3.SS2.SSS3.p1.17.4" class="ltx_text ltx_font_bold">topology matrix</span> <math id="S3.SS2.SSS3.p1.4.m1.3" class="ltx_Math" alttext="W=[w_{ij}]_{i,j=0}^{n-1}\in\mathbb{R}^{n\times n}" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m1.3a"><mrow id="S3.SS2.SSS3.p1.4.m1.3.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.3.cmml">W</mi><mo id="S3.SS2.SSS3.p1.4.m1.3.3.4" xref="S3.SS2.SSS3.p1.4.m1.3.3.4.cmml">=</mo><msubsup id="S3.SS2.SSS3.p1.4.m1.3.3.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.cmml"><mrow id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.2.1.cmml">[</mo><msub id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.2.cmml">w</mi><mrow id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS2.SSS3.p1.4.m1.2.2.2" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.cmml"><mrow id="S3.SS2.SSS3.p1.4.m1.2.2.2.4.2" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.4.1.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.1.1.1.1" xref="S3.SS2.SSS3.p1.4.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.SSS3.p1.4.m1.2.2.2.4.2.1" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.4.1.cmml">,</mo><mi id="S3.SS2.SSS3.p1.4.m1.2.2.2.2" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.2.cmml">j</mi></mrow><mo id="S3.SS2.SSS3.p1.4.m1.2.2.2.3" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.3.cmml">=</mo><mn id="S3.SS2.SSS3.p1.4.m1.2.2.2.5" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.5.cmml">0</mn></mrow><mrow id="S3.SS2.SSS3.p1.4.m1.3.3.1.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.2.cmml">n</mi><mo id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.1.cmml">−</mo><mn id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.3.cmml">1</mn></mrow></msubsup><mo id="S3.SS2.SSS3.p1.4.m1.3.3.5" xref="S3.SS2.SSS3.p1.4.m1.3.3.5.cmml">∈</mo><msup id="S3.SS2.SSS3.p1.4.m1.3.3.6" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.6.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS3.p1.4.m1.3.3.6.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.2" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.1" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.1.cmml">×</mo><mi id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.3" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m1.3b"><apply id="S3.SS2.SSS3.p1.4.m1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3"><and id="S3.SS2.SSS3.p1.4.m1.3.3a.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3"></and><apply id="S3.SS2.SSS3.p1.4.m1.3.3b.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3"><eq id="S3.SS2.SSS3.p1.4.m1.3.3.4.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.4"></eq><ci id="S3.SS2.SSS3.p1.4.m1.3.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.3">𝑊</ci><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m1.3.3.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1">superscript</csymbol><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1">subscript</csymbol><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.2">𝑤</ci><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3"><times id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S3.SS2.SSS3.p1.4.m1.2.2.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.2.2.2"><eq id="S3.SS2.SSS3.p1.4.m1.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.3"></eq><list id="S3.SS2.SSS3.p1.4.m1.2.2.2.4.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.4.2"><ci id="S3.SS2.SSS3.p1.4.m1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.1.1.1.1">𝑖</ci><ci id="S3.SS2.SSS3.p1.4.m1.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.2">𝑗</ci></list><cn type="integer" id="S3.SS2.SSS3.p1.4.m1.2.2.2.5.cmml" xref="S3.SS2.SSS3.p1.4.m1.2.2.2.5">0</cn></apply></apply><apply id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3"><minus id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.1"></minus><ci id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.SSS3.p1.4.m1.3.3.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.1.3.3">1</cn></apply></apply></apply><apply id="S3.SS2.SSS3.p1.4.m1.3.3c.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3"><in id="S3.SS2.SSS3.p1.4.m1.3.3.5.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.5"></in><share href="#S3.SS2.SSS3.p1.4.m1.3.3.1.cmml" id="S3.SS2.SSS3.p1.4.m1.3.3d.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3"></share><apply id="S3.SS2.SSS3.p1.4.m1.3.3.6.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m1.3.3.6.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6">superscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m1.3.3.6.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.2">ℝ</ci><apply id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3"><times id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.1"></times><ci id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.2">𝑛</ci><ci id="S3.SS2.SSS3.p1.4.m1.3.3.6.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m1.3.3.6.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m1.3c">W=[w_{ij}]_{i,j=0}^{n-1}\in\mathbb{R}^{n\times n}</annotation></semantics></math> as the matrix to represent the topology.
In the remainder of this paper,
we assume that <math id="S3.SS2.SSS3.p1.5.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.SSS3.p1.5.m2.1a"><mi id="S3.SS2.SSS3.p1.5.m2.1.1" xref="S3.SS2.SSS3.p1.5.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m2.1b"><ci id="S3.SS2.SSS3.p1.5.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m2.1c">W</annotation></semantics></math> satisfies
<math id="S3.SS2.SSS3.p1.6.m3.1" class="ltx_Math" alttext="W\mathds{1}=\mathds{1}" display="inline"><semantics id="S3.SS2.SSS3.p1.6.m3.1a"><mrow id="S3.SS2.SSS3.p1.6.m3.1.1" xref="S3.SS2.SSS3.p1.6.m3.1.1.cmml"><mrow id="S3.SS2.SSS3.p1.6.m3.1.1.2" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.cmml"><mi id="S3.SS2.SSS3.p1.6.m3.1.1.2.2" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.6.m3.1.1.2.1" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.1.cmml">​</mo><mn id="S3.SS2.SSS3.p1.6.m3.1.1.2.3" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.3.cmml">𝟙</mn></mrow><mo id="S3.SS2.SSS3.p1.6.m3.1.1.1" xref="S3.SS2.SSS3.p1.6.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS3.p1.6.m3.1.1.3" xref="S3.SS2.SSS3.p1.6.m3.1.1.3.cmml">𝟙</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m3.1b"><apply id="S3.SS2.SSS3.p1.6.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1"><eq id="S3.SS2.SSS3.p1.6.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.1"></eq><apply id="S3.SS2.SSS3.p1.6.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.2"><times id="S3.SS2.SSS3.p1.6.m3.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.1"></times><ci id="S3.SS2.SSS3.p1.6.m3.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.2">𝑊</ci><cn type="integer" id="S3.SS2.SSS3.p1.6.m3.1.1.2.3.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.2.3">1</cn></apply><cn type="integer" id="S3.SS2.SSS3.p1.6.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p1.6.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m3.1c">W\mathds{1}=\mathds{1}</annotation></semantics></math>
and <math id="S3.SS2.SSS3.p1.7.m4.1" class="ltx_Math" alttext="\mathds{1}^{T}W=\mathds{1}^{T}" display="inline"><semantics id="S3.SS2.SSS3.p1.7.m4.1a"><mrow id="S3.SS2.SSS3.p1.7.m4.1.1" xref="S3.SS2.SSS3.p1.7.m4.1.1.cmml"><mrow id="S3.SS2.SSS3.p1.7.m4.1.1.2" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.cmml"><msup id="S3.SS2.SSS3.p1.7.m4.1.1.2.2" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2.cmml"><mn id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.2" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2.2.cmml">𝟙</mn><mi id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.3" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.7.m4.1.1.2.1" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS3.p1.7.m4.1.1.2.3" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.3.cmml">W</mi></mrow><mo id="S3.SS2.SSS3.p1.7.m4.1.1.1" xref="S3.SS2.SSS3.p1.7.m4.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS3.p1.7.m4.1.1.3" xref="S3.SS2.SSS3.p1.7.m4.1.1.3.cmml"><mn id="S3.SS2.SSS3.p1.7.m4.1.1.3.2" xref="S3.SS2.SSS3.p1.7.m4.1.1.3.2.cmml">𝟙</mn><mi id="S3.SS2.SSS3.p1.7.m4.1.1.3.3" xref="S3.SS2.SSS3.p1.7.m4.1.1.3.3.cmml">T</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.7.m4.1b"><apply id="S3.SS2.SSS3.p1.7.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1"><eq id="S3.SS2.SSS3.p1.7.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.1"></eq><apply id="S3.SS2.SSS3.p1.7.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2"><times id="S3.SS2.SSS3.p1.7.m4.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.1"></times><apply id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.1.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2">superscript</csymbol><cn type="integer" id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.2.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2.2">1</cn><ci id="S3.SS2.SSS3.p1.7.m4.1.1.2.2.3.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.2.3">𝑇</ci></apply><ci id="S3.SS2.SSS3.p1.7.m4.1.1.2.3.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.2.3">𝑊</ci></apply><apply id="S3.SS2.SSS3.p1.7.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.7.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.SSS3.p1.7.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.3.2">1</cn><ci id="S3.SS2.SSS3.p1.7.m4.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.7.m4.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.7.m4.1c">\mathds{1}^{T}W=\mathds{1}^{T}</annotation></semantics></math>,
i.e., both the row sum and the column sum of <math id="S3.SS2.SSS3.p1.8.m5.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.SSS3.p1.8.m5.1a"><mi id="S3.SS2.SSS3.p1.8.m5.1.1" xref="S3.SS2.SSS3.p1.8.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.8.m5.1b"><ci id="S3.SS2.SSS3.p1.8.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.8.m5.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.8.m5.1c">W</annotation></semantics></math> are equal to <math id="S3.SS2.SSS3.p1.9.m6.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSS3.p1.9.m6.1a"><mn id="S3.SS2.SSS3.p1.9.m6.1.1" xref="S3.SS2.SSS3.p1.9.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.9.m6.1b"><cn type="integer" id="S3.SS2.SSS3.p1.9.m6.1.1.cmml" xref="S3.SS2.SSS3.p1.9.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.9.m6.1c">1</annotation></semantics></math>,
so as to guarantee that the neighborhood averaging will asymptotically approach the global averaging <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">chatterjee1977towards </a>; <a href="#bib.bib154" title="" class="ltx_ref">seneta2006non </a>; <a href="#bib.bib151" title="" class="ltx_ref">sayed2014adaptation </a></cite>.
When a computing resource <math id="S3.SS2.SSS3.p1.10.m7.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS3.p1.10.m7.1a"><mi id="S3.SS2.SSS3.p1.10.m7.1.1" xref="S3.SS2.SSS3.p1.10.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.10.m7.1b"><ci id="S3.SS2.SSS3.p1.10.m7.1.1.cmml" xref="S3.SS2.SSS3.p1.10.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.10.m7.1c">j</annotation></semantics></math> is directly connected to computing resource <math id="S3.SS2.SSS3.p1.11.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS3.p1.11.m8.1a"><mi id="S3.SS2.SSS3.p1.11.m8.1.1" xref="S3.SS2.SSS3.p1.11.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.11.m8.1b"><ci id="S3.SS2.SSS3.p1.11.m8.1.1.cmml" xref="S3.SS2.SSS3.p1.11.m8.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.11.m8.1c">i</annotation></semantics></math>, i.e., <math id="S3.SS2.SSS3.p1.12.m9.2" class="ltx_Math" alttext="w_{i,j}\neq 0" display="inline"><semantics id="S3.SS2.SSS3.p1.12.m9.2a"><mrow id="S3.SS2.SSS3.p1.12.m9.2.3" xref="S3.SS2.SSS3.p1.12.m9.2.3.cmml"><msub id="S3.SS2.SSS3.p1.12.m9.2.3.2" xref="S3.SS2.SSS3.p1.12.m9.2.3.2.cmml"><mi id="S3.SS2.SSS3.p1.12.m9.2.3.2.2" xref="S3.SS2.SSS3.p1.12.m9.2.3.2.2.cmml">w</mi><mrow id="S3.SS2.SSS3.p1.12.m9.2.2.2.4" xref="S3.SS2.SSS3.p1.12.m9.2.2.2.3.cmml"><mi id="S3.SS2.SSS3.p1.12.m9.1.1.1.1" xref="S3.SS2.SSS3.p1.12.m9.1.1.1.1.cmml">i</mi><mo id="S3.SS2.SSS3.p1.12.m9.2.2.2.4.1" xref="S3.SS2.SSS3.p1.12.m9.2.2.2.3.cmml">,</mo><mi id="S3.SS2.SSS3.p1.12.m9.2.2.2.2" xref="S3.SS2.SSS3.p1.12.m9.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS2.SSS3.p1.12.m9.2.3.1" xref="S3.SS2.SSS3.p1.12.m9.2.3.1.cmml">≠</mo><mn id="S3.SS2.SSS3.p1.12.m9.2.3.3" xref="S3.SS2.SSS3.p1.12.m9.2.3.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.12.m9.2b"><apply id="S3.SS2.SSS3.p1.12.m9.2.3.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3"><neq id="S3.SS2.SSS3.p1.12.m9.2.3.1.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3.1"></neq><apply id="S3.SS2.SSS3.p1.12.m9.2.3.2.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.12.m9.2.3.2.1.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3.2">subscript</csymbol><ci id="S3.SS2.SSS3.p1.12.m9.2.3.2.2.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3.2.2">𝑤</ci><list id="S3.SS2.SSS3.p1.12.m9.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.2.2.4"><ci id="S3.SS2.SSS3.p1.12.m9.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.12.m9.1.1.1.1">𝑖</ci><ci id="S3.SS2.SSS3.p1.12.m9.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.2.2.2">𝑗</ci></list></apply><cn type="integer" id="S3.SS2.SSS3.p1.12.m9.2.3.3.cmml" xref="S3.SS2.SSS3.p1.12.m9.2.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.12.m9.2c">w_{i,j}\neq 0</annotation></semantics></math>, computing resource <math id="S3.SS2.SSS3.p1.13.m10.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS3.p1.13.m10.1a"><mi id="S3.SS2.SSS3.p1.13.m10.1.1" xref="S3.SS2.SSS3.p1.13.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.13.m10.1b"><ci id="S3.SS2.SSS3.p1.13.m10.1.1.cmml" xref="S3.SS2.SSS3.p1.13.m10.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.13.m10.1c">j</annotation></semantics></math> is the neighbor of computing resource <math id="S3.SS2.SSS3.p1.14.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS3.p1.14.m11.1a"><mi id="S3.SS2.SSS3.p1.14.m11.1.1" xref="S3.SS2.SSS3.p1.14.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.14.m11.1b"><ci id="S3.SS2.SSS3.p1.14.m11.1.1.cmml" xref="S3.SS2.SSS3.p1.14.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.14.m11.1c">i</annotation></semantics></math>.
<span id="S3.SS2.SSS3.p1.17.3" class="ltx_text" style="color:#000000;">Please note that the weight <math id="S3.SS2.SSS3.p1.15.1.m1.2" class="ltx_Math" alttext="w_{i,j}" display="inline"><semantics id="S3.SS2.SSS3.p1.15.1.m1.2a"><msub id="S3.SS2.SSS3.p1.15.1.m1.2.3" xref="S3.SS2.SSS3.p1.15.1.m1.2.3.cmml"><mi mathcolor="#000000" id="S3.SS2.SSS3.p1.15.1.m1.2.3.2" xref="S3.SS2.SSS3.p1.15.1.m1.2.3.2.cmml">w</mi><mrow id="S3.SS2.SSS3.p1.15.1.m1.2.2.2.4" xref="S3.SS2.SSS3.p1.15.1.m1.2.2.2.3.cmml"><mi mathcolor="#000000" id="S3.SS2.SSS3.p1.15.1.m1.1.1.1.1" xref="S3.SS2.SSS3.p1.15.1.m1.1.1.1.1.cmml">i</mi><mo mathcolor="#000000" id="S3.SS2.SSS3.p1.15.1.m1.2.2.2.4.1" xref="S3.SS2.SSS3.p1.15.1.m1.2.2.2.3.cmml">,</mo><mi mathcolor="#000000" id="S3.SS2.SSS3.p1.15.1.m1.2.2.2.2" xref="S3.SS2.SSS3.p1.15.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.15.1.m1.2b"><apply id="S3.SS2.SSS3.p1.15.1.m1.2.3.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.15.1.m1.2.3.1.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.2.3">subscript</csymbol><ci id="S3.SS2.SSS3.p1.15.1.m1.2.3.2.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.2.3.2">𝑤</ci><list id="S3.SS2.SSS3.p1.15.1.m1.2.2.2.3.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.2.2.2.4"><ci id="S3.SS2.SSS3.p1.15.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS2.SSS3.p1.15.1.m1.2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.15.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.15.1.m1.2c">w_{i,j}</annotation></semantics></math> denotes the confidence node <math id="S3.SS2.SSS3.p1.16.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS3.p1.16.2.m2.1a"><mi mathcolor="#000000" id="S3.SS2.SSS3.p1.16.2.m2.1.1" xref="S3.SS2.SSS3.p1.16.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.16.2.m2.1b"><ci id="S3.SS2.SSS3.p1.16.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.16.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.16.2.m2.1c">i</annotation></semantics></math> has in the information it receives from node <math id="S3.SS2.SSS3.p1.17.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.SSS3.p1.17.3.m3.1a"><mi mathcolor="#000000" id="S3.SS2.SSS3.p1.17.3.m3.1.1" xref="S3.SS2.SSS3.p1.17.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.17.3.m3.1b"><ci id="S3.SS2.SSS3.p1.17.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.17.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.17.3.m3.1c">j</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib88" title="" class="ltx_ref">lalitha2019peer </a></cite>, which is different from the bandwidth or data transfer capacity in a network.</span>
The centralized aggregation algorithm is a special type of decentralized aggregation with a star topology while only the centralized server communicates with its neighbors.
<span id="S3.SS2.SSS3.p1.17.5" class="ltx_text" style="color:#000000;">A well designed topology, e.g., an exponential graph <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Assran2019Stochastic </a></cite>, can improve the convergence rate, which accelerates the training speed.</span></p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">With the decentralized SGD (D-SGD), each computing resource maintains a local copy of
the global model parameters, and it updates the local copy using the models of its neighbors.
According to the order to conduct neighborhood averaging and gradient descent, D-SGD has two common types of realizations: Average-With-Communication (AWC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib96" title="" class="ltx_ref">lian2017can </a>; <a href="#bib.bib88" title="" class="ltx_ref">lalitha2019peer </a></cite> and Average-Before-Communication (ABC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">chen2012diffusion </a>; <a href="#bib.bib178" title="" class="ltx_ref">wang2019matcha </a></cite>.
AWC can overlap communication and gradient computation, while ABC needs to sequentially calculate and communicate the gradient or model.
However, ABC is robust <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib152" title="" class="ltx_ref">sayed2013diffusion </a></cite>, and it converges fast in terms of iterations by exploiting its large learning rate.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">In addition, the decentralized aggregation algorithms can be classified to Full Communication (FC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib96" title="" class="ltx_ref">lian2017can </a></cite> and Partial Communication (PC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib178" title="" class="ltx_ref">wang2019matcha </a>; <a href="#bib.bib168" title="" class="ltx_ref">vanhaesebrouck2017decentralized </a></cite> according to the number of neighbors. Within the iterations of FC, each computing resource calculates an averaged model or gradient, based on all the models or gradients of the last version from all its neighbors. However, within the iterations of PC, each computing resource calculates an averaged model or gradient based on one or multiple chosen neighbors. With PC, the selection of the neighbors can be based on a gossip algorithm <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib72" title="" class="ltx_ref">Hu2019Decentralized </a></cite>. For instance, a random neighbor can be selected <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib168" title="" class="ltx_ref">vanhaesebrouck2017decentralized </a></cite>; the neighbors that provide benign models are selected to avoid attack <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib130" title="" class="ltx_ref">munoz2019byzantine </a></cite>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Manipulation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">At the infrastructure layer of an FL system, there are three types of data manipulation: data security mechanisms, data transfer, and distributed data processing within the distributed execution module.
We first present the techniques for the distributed execution in an FL system.
Then, we present the techniques for data transfer during the training process of an FL system.
Finally, as data security is of much importance to an FL system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib129" title="" class="ltx_ref">mothukuri2021survey </a></cite>, we present the techniques to protect the data security.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Distributed Data Processing</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">While the bandwidth within a single data center is high, e.g., InfiniBand, the High Performance Computing (HPC) libraries, e.g., Message Passing Interface (MPI) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib56" title="" class="ltx_ref">gropp1999using </a></cite> or NVIDIA Collective Communications Library (NCCL) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">awan2018optimized </a></cite>, are widely exploited for distributed data processing <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib171" title="" class="ltx_ref">vishnu2016distributed </a></cite>. With MPI or NCCL, the gradients or models in each computing resource can be easily calculated using ring-AllReduce algorithm <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib51" title="" class="ltx_ref">ring-allreduce </a></cite>. However, one of the drawbacks of the HPC libraries is that they lack support for fault-tolerance, as the HPC libraries are designed for high performance servers with high quality networks. When any computing resource within the network becomes unavailable, the distributed training process may be broken.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">However, as an FL system is generally implemented for the collaboration of large amounts of mobile device users or different organizations, the network connection among computing resources is of moderate quality, i.e., the bandwidth is not as good as that within a single data center, and the latency is high.
For instance, the Internet upload speed is typically much slower than the download speed <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">konevcny2016federated </a></cite>. Also, some users with unstable wireless communication channels may consequently drop out due to disconnection from the Internet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib99" title="" class="ltx_ref">lim2020federated </a></cite>.
In this environment, the connection between computing resources and parameter servers has a high possibility of becoming disabled. Thus, Remote Procedure Call (RPC) frameworks are widely exploited, as this kind of framework can ignore the disconnected computing resources and continue the distributed training of an FL system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">beutel2020flower </a></cite>, e.g., PaddleFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">PaddleFL </a></cite>, PySyft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref">Pysyft </a></cite>, or TensorflowFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">TFF </a></cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Transfer</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As the network connection is of moderate quality, the data transfer module mainly focuses on data compression to transfer intermediate data, e.g., gradients or models.
Sketched updates are proposed for gradient compression to accelerate the data transfer during the distributed training within a single data center <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib76" title="" class="ltx_ref">jiang2018sketchml </a>; <a href="#bib.bib82" title="" class="ltx_ref">karimireddy2019error </a>; <a href="#bib.bib75" title="" class="ltx_ref">ivkin2019communication </a>; <a href="#bib.bib159" title="" class="ltx_ref">spring2019compressing </a></cite>.
With the data parallelism and centralized aggregation algorithm, before sending the intermediate data, the intermediate data can be sketched with subsampling <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">konevcny2016federated </a></cite>, quantization <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">konevcny2016federated </a>; <a href="#bib.bib57" title="" class="ltx_ref">haddadpour2020federated </a>; <a href="#bib.bib157" title="" class="ltx_ref">shlezinger2020uveqfed </a>; <a href="#bib.bib156" title="" class="ltx_ref">shlezinger2020federated </a>; <a href="#bib.bib162" title="" class="ltx_ref">sun2020adaptive </a>; <a href="#bib.bib189" title="" class="ltx_ref">xu2020ternary </a></cite>, sparsification <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib162" title="" class="ltx_ref">sun2020adaptive </a>; <a href="#bib.bib117" title="" class="ltx_ref">Malekijoo2021FEDZIP </a></cite>, or projection to lower dimensional spaces <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib147" title="" class="ltx_ref">rothchild2020fetchsgd </a></cite>, in each computing resource, in order to reduce the cost to transfer data.
Subsampling refers to transferring only a random subset of the intermediate data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">konevcny2016federated </a></cite>.
Quantization methods encode each value using a fixed number of bits, so as to reduce the length of gradients or models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib86" title="" class="ltx_ref">konevcny2016federated </a></cite>.
With the sparsification approach, only selected parts of the intermediate data are transferred, while the selection is based on a threshold, e.g., the gradients larger than a threshold are selected <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib162" title="" class="ltx_ref">sun2020adaptive </a></cite>. Then, when the intermediate data is received in the server, they are decompressed to be aggregated according to the aggregation algorithms presented in Section <a href="#S3.SS2.SSS1" title="3.2.1 Centralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>. The convergence of the quantization approach is analyzed in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref">haddadpour2020federated </a></cite>, which shows that this approach can also provide good convergence rates <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib57" title="" class="ltx_ref">haddadpour2020federated </a></cite>. In addition, irrelevant intermediate data can be precluded to be transferred to the server, in order to substantially reduce the communication overhead <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib180" title="" class="ltx_ref">luping2019cmfl </a></cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data Security</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Data security is of much importance for data processing. The problem of data security is related to significant financial <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">UberLoss </a>; <a href="#bib.bib150" title="" class="ltx_ref">GoogleLoss </a></cite> and reputational <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib158" title="" class="ltx_ref">FacebookLoss </a></cite> losses. For instance, Uber had to pay $148,000,000 to settle the investigation incurred by a breach of 600,000 drivers’ personal information in 2016 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">Uber </a></cite>.
Data security mainly includes two aspects, i.e., data privacy and model security.
Data privacy refers to the protection of raw data to avoid raw data information leakage during or after the distributed training of FL systems.
Model security refers to the protection of the security of trained models, in order to avoid wrong output based on the trained models incurred by malicious attacks.
In this section, we first present the techniques to protect data privacy. Then we present the defense methods for model security.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Data Privacy</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">The techniques to protect data privacy consist of three types: Trusted Execution Environment (TEE), encryption, Differential Privacy (DP), and anti-Generative Adversarial Network (GAN) methods. These techniques can be combined in FL systems, e.g., the combination of DP and TEE in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref">hao2019towards </a></cite>, the combination of encryption and DP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib194" title="" class="ltx_ref">zhang2020batchcrypt </a></cite>, and the combination of DP and anti-GAN <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib166" title="" class="ltx_ref">triastcyn2020federated </a></cite>.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">A TEE is an environment where the execution is secured and no information can be leaked to unauthorized users.
Intel SGX technique <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib119" title="" class="ltx_ref">mckeen2013innovative </a></cite> has been first proposed as a secure environment while providing a set of security-related instruction codes built within Intel Central Processing Units (CPUs).
Then, the implementation of machine learning models has been carried out in the TEE, i.e., Intel SGX, in order to enable collaborative data analysis based on machine learning algorithms while providing a security guarantee <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib134" title="" class="ltx_ref">ohrimenko2016oblivious </a></cite>. Afterwards, the TEE has been exploited in FL systems, in order to protect the privacy of data in two ways. The first way is to put the entire training process in the TEE of each distributed computing resource to protect the data privacy during the distributed training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib127" title="" class="ltx_ref">mo2019efficient </a>; <a href="#bib.bib31" title="" class="ltx_ref">chen2020training </a></cite>. The second way is to use TEE to check a small part of the distributed training, while exploiting insecure computing resources, e.g., GPUs, to reduce the training time <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib200" title="" class="ltx_ref">zhang2020enabling </a></cite>.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">As an encryption technique, homomorphic encryption has been used to ensure the data privacy for FL systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib58" title="" class="ltx_ref">hao2019towards </a>; <a href="#bib.bib43" title="" class="ltx_ref">feng2019securegbm </a></cite>.
Homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib191" title="" class="ltx_ref">yi2014homomorphic </a></cite> allows specific types of computations to be carried out on encrypted input data, and to generate an encrypted result, which matches the result of the same computations on the decrypted input data.
<span id="S4.SS3.SSS1.p3.1.1" class="ltx_text" style="color:#000000;">Two main branches of homomorphic encryption exist, i.e., fully homomorphic encryption and partially homomorphic encryption.
The fully homomorphic encryption supports both addition and multiplication on ciphertext, while partially homomorphic encryption only supports either an addition or a multiplication operation on ciphertext, which corresponds to less computational flexibility and better runtime efficiency.
Both the fully and partially homomorphic encryptions can be exploited with the horizontal and vertical federated learning.</span>
As sharing gradients also leaks the information of training data <span id="S4.SS3.SSS1.p3.1.2" class="ltx_text" style="color:#000000;">in horizontal federated learning</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib201" title="" class="ltx_ref">zhao2020idlg </a>; <a href="#bib.bib95" title="" class="ltx_ref">li2019quantification </a>; <a href="#bib.bib49" title="" class="ltx_ref">geiping2020inverting </a></cite>, it is of much importance to protect the privacy of the intermediate data.
Thus, the intermediate data can be encrypted using a homomorphic encryption algorithm before being sent to a parameter server <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib115" title="" class="ltx_ref">lyu2020towards </a>; <a href="#bib.bib118" title="" class="ltx_ref">mandal2019privfl </a></cite>. In this way, the intermediate data remain encrypted during the aggregation process while only the computing resource can decrypt the encrypted intermediate data.
Even if the transferred encrypted intermediate data is leaked, the information of gradients or models remains safe, and the privacy of the training data is ensured.
<span id="S4.SS3.SSS1.p3.1.3" class="ltx_text" style="color:#000000;">In addition, partial homomorphic encryption, e.g., Paillier <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib138" title="" class="ltx_ref">paillier1999public </a></cite>, is exploited in vertical federated learning <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">ccatak2018cpp </a>; <a href="#bib.bib24" title="" class="ltx_ref">ccatak2015secure </a></cite>.</span>
However, the homomorphic encryption incurs significant costs in computation and communication during distributed training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib194" title="" class="ltx_ref">zhang2020batchcrypt </a></cite>. In order to reduce the overhead of homomorphic encryption, a set of quantized gradients are encrypted <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib194" title="" class="ltx_ref">zhang2020batchcrypt </a></cite>.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p">Differential Privacy (DP) protects the data privacy by adding artificial noise to a small part of raw data, while ensuring that the modification does not substantially affect the performance of the machine learning models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib184" title="" class="ltx_ref">wei2020federated </a>; <a href="#bib.bib39" title="" class="ltx_ref">dwork2008differential </a>; <a href="#bib.bib3" title="" class="ltx_ref">abadi2016deep </a>; <a href="#bib.bib50" title="" class="ltx_ref">geyer2017differentially </a></cite>.
DP is widely used in FL systems as the first step to process the raw data, and the output is the training data to be used for the distributed training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib184" title="" class="ltx_ref">wei2020federated </a>; <a href="#bib.bib141" title="" class="ltx_ref">phan2020scalable </a>; <a href="#bib.bib149" title="" class="ltx_ref">sabater2020distributed </a>; <a href="#bib.bib97" title="" class="ltx_ref">liang2020exploring </a>; <a href="#bib.bib108" title="" class="ltx_ref">liu2020fedsel </a>; <a href="#bib.bib83" title="" class="ltx_ref">katevas2020policy </a></cite>. With more added noise, the privacy is better protected, i.e., there is less possibility to leak raw data information, while it takes more time to converge for the machine learning models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib184" title="" class="ltx_ref">wei2020federated </a></cite>. A trade-off between the privacy protection and the convergence performance can be made by selecting a certain number of distributed resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib184" title="" class="ltx_ref">wei2020federated </a>; <a href="#bib.bib153" title="" class="ltx_ref">seif2020wireless </a>; <a href="#bib.bib167" title="" class="ltx_ref">truex2019hybrid </a></cite>. However, DP may not be able to ensure the data privacy under certain attacks, e.g., Generative Adversarial Network (GAN) attacks <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">hitaj2017deep </a></cite>.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p">A well-trained machine learning model can leak information about the training data based on the intermediate data, e.g., gradients <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">hitaj2017deep </a>; <a href="#bib.bib8" title="" class="ltx_ref">ateniese2015hacking </a>; <a href="#bib.bib124" title="" class="ltx_ref">melis2019exploiting </a></cite>. GANs can be used to generate data similar to the training data based on a well-trained machine learning model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib54" title="" class="ltx_ref">Goodfellow2015Explaining </a></cite> in either a parameter server <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib181" title="" class="ltx_ref">wang2019beyond </a></cite> or a distributed computing resource <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">hitaj2017deep </a></cite>.
The adversary can reconstruct other participating clients’ private data, even if it has no knowledge of the label information using the GANs.
Thus, during the distributed training process of FL systems, a malicious user can exploit GANs to infer the training data of other users. DP can be used to prevent the GAN-based attack <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib71" title="" class="ltx_ref">hitaj2017deep </a>; <a href="#bib.bib166" title="" class="ltx_ref">triastcyn2020federated </a></cite>. In addition, fake training data can be generated based on a GAN and original raw data, which is then used during the distributed training process to prevent the GAN-based attack <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib113" title="" class="ltx_ref">luo2020exploiting </a></cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Model Security</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">We mainly focus on poisoning attacks in this section.
The objective of poisoning attacks is to reduce the accuracy of machine learning models using artificially designed data, i.e., data poisoning, or models, i.e., model poisoning, in one or several distributed computing resources during the model aggregation process (see details in in Section <a href="#S3.SS2.SSS1" title="3.2.1 Centralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). There are two ways to carry out poisoning attacks, i.e., data poisoning and model poisoning.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.2" class="ltx_p">Data poisoning can be realized by modifying the features <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">fung2018mitigating </a></cite> or the labels <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib165" title="" class="ltx_ref">tolpegin2020data </a></cite> of the input data. For instance, malicious users can modify the data points of a certain class <math id="S4.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mi id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><ci id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">C</annotation></semantics></math> to other classes, and they can then use the modified data points to participate in the distributed training.
The modification of the labels is denoted by the label flipping attack.
As a result, the accuracy of the trained model has low accuracy in terms of Class <math id="S4.SS3.SSS2.p2.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><mi id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><ci id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">C</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib165" title="" class="ltx_ref">tolpegin2020data </a></cite>.
Model poisoning refers to the attacks in which the updated intermediate data, e.g., gradients or models, are poisoned before being sent to a parameter server in order to reduce the accuracy of the trained model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">chen2020backdoor </a>; <a href="#bib.bib163" title="" class="ltx_ref">sun2019can </a></cite>. The goal of the model poisoning is to reduce the performance of the trained model on targeted tasks or classes, while the performance of the model remains unchanged in terms of other tasks or classes <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib163" title="" class="ltx_ref">sun2019can </a></cite>.
Data poisoning eventually realizes the model poisoning, as it enables some computing resources to update poisoned intermediate data based on the calculation of poisoned training data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">fung2018mitigating </a></cite>.
However, model poisoning can be more powerful than data poisoning, as model poisoning directly influences the weights of the models and trains in a way that benefits the attack <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">bagdasaryan2020backdoor </a></cite>.
Both the data poisoning and the model poisoning rely on the backdoor attacks to modify the training data or the intermediate data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib163" title="" class="ltx_ref">sun2019can </a>; <a href="#bib.bib27" title="" class="ltx_ref">chen2020backdoor </a>; <a href="#bib.bib46" title="" class="ltx_ref">fung2018mitigating </a></cite>. Backdoor attacks are performed by embedding the hidden instructions into machine learning models, so that the infected model performs well on benign testing samples when the backdoor is not activated, while its prediction will be changed to the attacker-specified target label when the backdoor is activated by the attacker <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib94" title="" class="ltx_ref">li2020backdoor </a></cite>.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">In order to defend against these data attacks or model attacks, the malicious users should be identified by analyzing the updated intermediate data using dimensionality reduction methods, e.g., Principal Component Analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib165" title="" class="ltx_ref">tolpegin2020data </a></cite>, anomaly detection <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib90" title="" class="ltx_ref">li2019abnormal </a>; <a href="#bib.bib101" title="" class="ltx_ref">lin2019free </a></cite>, or interpretability techniques <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">bhagoji2019analyzing </a></cite>.
In addition, the model poisoning can be incurred by Byzantine failures of certain distributed computing resources <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib41" title="" class="ltx_ref">fang2020local </a></cite>. With Byzantine failures, some computing resources (bad users) are manipulated by attackers during the distributed training process, which significantly degrades the performance of the global model in terms of test error <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib41" title="" class="ltx_ref">fang2020local </a></cite>.
In order to make the training process robust against the Byzantine failures, the bad users can be identified by analyzing the updated intermediate data using a hidden Markov model <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref">eddy2004hidden </a>; <a href="#bib.bib130" title="" class="ltx_ref">munoz2019byzantine </a></cite> or via secure aggregation protocols <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib70" title="" class="ltx_ref">he2020secure </a></cite>.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Federated Learning Frameworks</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">FL systems are widely applied in diverse domains, e.g.,
mobile service, healthcare <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib188" title="" class="ltx_ref">xu2020federated </a></cite>, and finance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib89" title="" class="ltx_ref">li2019survey </a></cite>.
An FL system generally exploits an FL framework, which is deployed on distributed resources.
In this section, we present four widely used FL frameworks: PaddleFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">PaddleFL </a></cite>, TensorFlowFederated <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">TFF </a></cite>, FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref">FATE </a></cite>, and PySyft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib136" title="" class="ltx_ref">Pysyft </a></cite>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>PaddleFL</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">PaddleFL is an open source federated learning framework based on PaddlePaddle <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib116" title="" class="ltx_ref">Ma2019 </a></cite>, which is supported by Baidu. At the presentation layer, PaddleFL provides a textual UI for the interaction between users and the FL system. At the User Services layer, PaddleFL provides the log and monitoring supports, and it can leverage the interpretability module <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">PaddleInterpretability </a></cite> of PaddlePaddle in the future. At the FL training layer, PaddleFL can realize data parallelism (horizontal FL) and model parallelism (vertical FL). It supports multiple aggregation algorithms, e.g., FedAvg, and fault-tolerance. At the infrastructure layer, PaddleFL exploits RPC for the distributed execution. PaddleFL exploits DP to protect the data security. PaddleFL is widely used in multiple domains, e.g., Natural Language Processing (NLP), Computing Vision (CV) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib109" title="" class="ltx_ref">liu2020fedvision </a></cite>, and recommendation.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>TensorFlowFederated</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">TensorFlow Federated (TFF) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib55" title="" class="ltx_ref">TFF </a></cite> is an open-source framework for federated learning on decentralized data, which is supported by Google. TFF also provides a textual UI through Python. TFF supports the monitoring and log functionality at the user service layer. TFF supports data parallelism (horizontal FL), multiple aggregation algorithms, and fault-tolerance of mobile devices. TFF exploits RPC for the distributed execution and DP for the protection of data privacy.
TFF enables Android mobile users to predict the next word while using the keyboard on their mobile phones <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a>; <a href="#bib.bib122" title="" class="ltx_ref">McMahan2018 </a></cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>FATE</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">FATE <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib182" title="" class="ltx_ref">FATE </a></cite> is an open-source FL framework supported by WeBank.
FATE provides both a graphical and textual UI.
FATE can support the monitoring of distributed training through a web portal.
FATE takes advantage of database management systems (DBMS) to track the execution status.
FATE can enable horizontal (data parallelism), vertical (model parallelism), and hybrid federated learning.
FATE exploits both the DP and HE to protect the data privacy.
In addition, FATE exploits RPC to perform the distributed execution.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>PySyft</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">PySyft <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib148" title="" class="ltx_ref">ryffel2018generic </a></cite> is an open-source FL framework based on the PyTorch framework <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib144" title="" class="ltx_ref">Pytorch </a></cite>.
PySyft is written in Python and provides a textual UI based on Python.
PySyft mainly supports the data parallelism and model parallelism based on an aggregator or orchestrating server. The aggregator or orchestrating server sends a part of the model to participating clients to process local data and gets results for federated averaging. PySyft exploits DP and encryption techniques to protect the data security. PySyft exploits multiple communication protocols for distributed execution, e.g., RPC, websocket <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref">fette2011websocket </a></cite> etc.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span><span id="S5.SS5.1.1" class="ltx_text" style="color:#000000;">Concluding Remarks</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text" style="color:#000000;">Diverse FL frameworks exist while each has its advantage. We summarize the characteristics of each framework in Table <a href="#S5.T3" title="Table 3 ‣ 5.5 Concluding Remarks ‣ 5 Federated Learning Frameworks ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, so as to help select a proper framework for use. From the table, we can see that all the frameworks implement the centralized aggregation algorithms, while employing DP and HE for the data security. PaddleFL can exploit Paddle to realize data, model, and pipeline parallelism. FATE and TFF are based on Tensorflow as the engine, while FATE can provide Web portal UI, which is convenient for novices. PySyft is compatible with PyTorch, which can easily handle the PyTorch-based tasks, while PaddleFL is compatible with Paddle, which can easily deal with rich pre-trained models published in PaddleHub <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib137" title="" class="ltx_ref">PaddleHub </a></cite>.</span></p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.2.1" class="ltx_text" style="color:#000000;">Comparison among diverse frameworks. “Aggregation” represents the type of aggregation algorithms. “Textual” represents the textual UI, while “Web” represents Web portal.</span></figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.3.1.1" class="ltx_tr">
<td id="S5.T3.3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Framework</td>
<td id="S5.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Engine</td>
<td id="S5.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Aggregation</td>
<td id="S5.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UI</td>
<td id="S5.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Parallelism</td>
<td id="S5.T3.3.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Security</td>
</tr>
<tr id="S5.T3.3.2.2" class="ltx_tr">
<td id="S5.T3.3.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">PaddleFL</td>
<td id="S5.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Paddle</td>
<td id="S5.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Centralized</td>
<td id="S5.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">textual</td>
<td id="S5.T3.3.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Data/Model/Pipeline</td>
<td id="S5.T3.3.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DP/HE</td>
</tr>
<tr id="S5.T3.3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">TFF</td>
<td id="S5.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TensorFlow</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Centralized</td>
<td id="S5.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">textual</td>
<td id="S5.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Data/Model</td>
<td id="S5.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DP/HE</td>
</tr>
<tr id="S5.T3.3.4.4" class="ltx_tr">
<td id="S5.T3.3.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FATE</td>
<td id="S5.T3.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TensorFlow</td>
<td id="S5.T3.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Centralized</td>
<td id="S5.T3.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Web</td>
<td id="S5.T3.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Data/Model</td>
<td id="S5.T3.3.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DP/HE</td>
</tr>
<tr id="S5.T3.3.5.5" class="ltx_tr">
<td id="S5.T3.3.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">PySyft</td>
<td id="S5.T3.3.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">PyTorch</td>
<td id="S5.T3.3.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Centralized</td>
<td id="S5.T3.3.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">textual</td>
<td id="S5.T3.3.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Data</td>
<td id="S5.T3.3.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">DP/HE</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text" style="color:#000000;">Table <a href="#S5.T4" title="Table 4 ‣ 5.5 Concluding Remarks ‣ 5 Federated Learning Frameworks ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> represents the support of diverse types of FL in terms of data distribution. All the frameworks support horizontal FL, while vertical FL is supported by three frameworks except TFF. PySyft cannot directly support the vertical FL, while PyVertical <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib146" title="" class="ltx_ref">Romanini2021 </a></cite>, which is built upon PySyft, can be used to support vertical FL with the compatibility of PyTorch models. The hybrid FL is only supported by Paddle and FATE. In addition, all the frameworks support the execution with GPU. In practice, although PaddleFL may correspond to slightly longer time, the accuracy of the trained model can be higher that of TFF and FATE, while PySyft may generate “out of memory” errors <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib85" title="" class="ltx_ref">Kholod2021 </a></cite>.</span></p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S5.T4.4.2" class="ltx_text" style="color:#000000;">Comparison among diverse frameworks for the support of diverse FL types, e.g., horizontal FL, vertical FL, and hybrid FL, and GPU. <math id="S5.T4.3.1.m1.1" class="ltx_Math" alttext="\|" display="inline"><semantics id="S5.T4.3.1.m1.1b"><mo mathcolor="#000000" id="S5.T4.3.1.m1.1.1" xref="S5.T4.3.1.m1.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.1.m1.1c"><ci id="S5.T4.3.1.m1.1.1.cmml" xref="S5.T4.3.1.m1.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.1.m1.1d">\|</annotation></semantics></math>✓<math id="S5.T4.4.2.m2.1" class="ltx_Math" alttext="\|" display="inline"><semantics id="S5.T4.4.2.m2.1b"><mo mathcolor="#000000" id="S5.T4.4.2.m2.1.1" xref="S5.T4.4.2.m2.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.2.m2.1c"><ci id="S5.T4.4.2.m2.1.1.cmml" xref="S5.T4.4.2.m2.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.2.m2.1d">\|</annotation></semantics></math> represents that the support is not realized by itself but a close one.</span></figcaption>
<table id="S5.T4.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.6.3.1" class="ltx_tr">
<td id="S5.T4.6.3.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" colspan="2"></td>
<td id="S5.T4.6.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PaddleFL</td>
<td id="S5.T4.6.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">TFF</td>
<td id="S5.T4.6.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FATE</td>
<td id="S5.T4.6.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PySyft</td>
</tr>
<tr id="S5.T4.6.4.2" class="ltx_tr">
<td id="S5.T4.6.4.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T4.6.4.2.1.1" class="ltx_text">Types</span></td>
<td id="S5.T4.6.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Horizontal</td>
<td id="S5.T4.6.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S5.T4.6.2" class="ltx_tr">
<td id="S5.T4.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Vertical</td>
<td id="S5.T4.6.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T4.6.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T4.5.1.1.m1.1" class="ltx_Math" alttext="\|" display="inline"><semantics id="S5.T4.5.1.1.m1.1a"><mo id="S5.T4.5.1.1.m1.1.1" xref="S5.T4.5.1.1.m1.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.1.1.m1.1b"><ci id="S5.T4.5.1.1.m1.1.1.cmml" xref="S5.T4.5.1.1.m1.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.1.1.m1.1c">\|</annotation></semantics></math>✓<math id="S5.T4.6.2.2.m2.1" class="ltx_Math" alttext="\|" display="inline"><semantics id="S5.T4.6.2.2.m2.1a"><mo id="S5.T4.6.2.2.m2.1.1" xref="S5.T4.6.2.2.m2.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.2.2.m2.1b"><ci id="S5.T4.6.2.2.m2.1.1.cmml" xref="S5.T4.6.2.2.m2.1.1">∥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.2.2.m2.1c">\|</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.6.5.3" class="ltx_tr">
<td id="S5.T4.6.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hybrid</td>
<td id="S5.T4.6.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T4.6.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
</tr>
<tr id="S5.T4.6.6.4" class="ltx_tr">
<td id="S5.T4.6.6.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="2">GPU</td>
<td id="S5.T4.6.6.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.6.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
<td id="S5.T4.6.6.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Research Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although much work has been done on the FL systems, there remain
some limitations, e.g., interpretability of FL, decentralized aggregation, FL on graphs, benchmarks of FL systems, and applications to distributed intelligent systems. This section
discusses the limitations of the existing frameworks and proposes new research directions.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Benchmarks</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Several datasets exist for experiments on FL systems. For instance, Federated Extended MNIST (FEMNIST) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">caldas2018leaf </a></cite> is built by partitioning the data in Extended MNIST <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref">cohen2017emnist </a></cite> based on each writer.
Shakespeare <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib120" title="" class="ltx_ref">mcmahan2017communication </a></cite> is built from The Complete Works of William Shakespeare <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib155" title="" class="ltx_ref">shakespeare2007complete </a></cite> based on each speaking role.
Both of these datasets can be used for horizontal FL. However, no public datasets exist for vertical FL or transfer FL. In addition, no open decentralized IID or non-IID distribution of popular datasets, e.g., ImageNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">deng2009imagenet </a></cite>, exist for FL systems.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Interpretability</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Deep neural networks have excellent performance in various areas, while it is often difficult to understand the results of deep neural network models, especially within FL systems.
Shapley values have been used to provide the interpretability <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib175" title="" class="ltx_ref">wang2019interpret </a></cite>, while it focuses on vertical FL. When multiple users collaboratively train an FL model, it remains an open problem to evaluate the contributions of each user, which helps provide evidence for the incentive of each user.
The primary incentive for clients to participate in federated learning is obtaining better models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib87" title="" class="ltx_ref">kulkarni2020survey </a></cite>, while the benefit of participating in federated learning for clients who have sufficient private data to train accurate local models is disputable.
Interpretability can help understand the contributions of each user and provide an objective opinion on the incentive strategy within an FL system.
In addition, the interpretability helps domain experts to understand the relationship between data and the final trained model in critical domains, e.g., healthcare and finance.
However, the interpretability within FL systems remains an open problem.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Decentralized Aggregation</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Current aggregation algorithms of FL systems focus on the full connection or star connection topology, while other topologies, e.g., dynamic exponential-2 graph, may help accelerate the distributed training with FL systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib96" title="" class="ltx_ref">lian2017can </a></cite>. <span id="S6.SS3.p1.1.1" class="ltx_text" style="color:#000000;">In addition, well-known graph algorithms, e.g., graph partitioning algorithms, and ad-hoc policies can be exploited to help better distribute computing resources with the topology defined in Section <a href="#S3.SS2.SSS3" title="3.2.3 Decentralized Aggregation ‣ 3.2 Aggregation Algorithms ‣ 3 Distributed Training ‣ From Distributed Machine Learning to Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a> in order to improve the efficiency of FL systems. </span> While the peer-to-peer communication enables the FL with an arbitrary topology matrix, the data security under diverse attacks, e.g., data or model poisoning, GAN-based attacks, remain open problems and deserve further investigation.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Federated Learning on Graphs</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Graphs or graph neural networks (GNN) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib169" title="" class="ltx_ref">velivckovic2017graph </a></cite> have gained increasing popularity in multiple domains, e.g., social network, knowledge graph, and recommender system. FL frameworks for graphs, i.e., GraphFL <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib173" title="" class="ltx_ref">wang2020graphfl </a></cite>, and GNN, i.e., SGNN <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib123" title="" class="ltx_ref">mei2019sgnn </a></cite>, have been proposed to train a model with decentralized graphs. However, the data security of FL on graphs remains an open problem. <span id="S6.SS4.p1.1.1" class="ltx_text" style="color:#000000;">In addition, while a multimodal knowledge graph could not only contain text but also images or other type of data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib202" title="" class="ltx_ref">zhao2021multimodal </a></cite>, it is worth further exploration to efficiently support the multimodel knowledge graph construction within an FL system <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib102" title="" class="ltx_ref">lin2020improving </a></cite>.</span></p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span><span id="S6.SS5.1.1" class="ltx_text" style="color:#000000;">Imbalanced Data</span>
</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p"><span id="S6.SS5.p1.1.1" class="ltx_text" style="color:#000000;">Although FL focuses on the non-IID data, the real-world decentralized data usually exhibit an imbalanced distribution <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib69" title="" class="ltx_ref">He2009Imbalanced </a>; <a href="#bib.bib186" title="" class="ltx_ref">wu2021adversarial </a></cite>. While the imbalanced data exist in multiple areas, such as computer vision <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib135" title="" class="ltx_ref">oksuz2020imbalance </a></cite>, bioinformatics, and biomedicine <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib195" title="" class="ltx_ref">zhang2017feature </a></cite>, learning from such data requires special attention upon data sampling <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib195" title="" class="ltx_ref">zhang2017feature </a>; <a href="#bib.bib197" title="" class="ltx_ref">zhang2021empirical </a></cite>, data augmentation <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib135" title="" class="ltx_ref">oksuz2020imbalance </a></cite>, and loss function designs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib179" title="" class="ltx_ref">wang2021addressing </a></cite>. The imbalanced data is related to diverse tasks, e.g., two-class or multi-class classification <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">bi2018empirical </a>; <a href="#bib.bib196" title="" class="ltx_ref">zhang2019multi </a></cite>. However, an optimized approach can be proposed to address the imbalanced data within FL systems.</span></p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Applications to Distributed Intelligent Systems</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">Machine learning algorithms have been widely used to boost the performance of intelligent systems, while FL systems could further enhance intelligent systems <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib103" title="" class="ltx_ref">liu2020two </a></cite> in distributed computing environments <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib104" title="" class="ltx_ref">liu2016multi </a>; <a href="#bib.bib143" title="" class="ltx_ref">pineda2016managing </a></cite> with privacy and security ensured. An intelligent system is a group of machines that has the capacity to gather data, analyze the data, and respond to other systems or the world around.
With FL systems, the distributed data can be exploited to generate models of high performance so as to produce smart responses.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we discussed the current state of the art of FL systems, including the functional architecture of FL systems, distributed training, and data manipulation.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">First, we presented an overview of FL systems. In particular, we introduced the life cycle of FL models, including four phases. Then, we presented the four-layer functional architecture of FL systems, including presentation, user services, FL training, and infrastructure, and we presented each layer in detail.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Second, we detailed the distributed training with two parts, i.e., parallelism and aggregation algorithms. We presented three types of parallelism, including data parallelism, model parallelism, and pipeline parallelism. We associate each parallelism to a corresponding type of FL. For instance, data parallelism is associated with the horizontal FL, which corresponds to cross-device or cross-silo FL. Model parallelism is related to vertical FL and cross-silo FL. We presented the features of different aggregation algorithms in three types: centralized aggregation, hierarchical aggregation, and decentralized aggregation.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Third, we presented the techniques for data manipulation within FL systems. We showed that FL systems prefer RPC for the distributed execution, to handle the fault-tolerance because of moderate network connection. Intermediate data are sketched in order to compress the data, so as to reduce the data communication time. In addition, we presented the data privacy and model security attacks and corresponding defense techniques, e.g., DP, HE, TEE, and the analysis of updated intermediate data for malicious user identification.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">We mainly introduced four FL systems: PaddleFL, TensorFlowFederated, FATE, and PySyft. The current solutions primarily focus on the horizontal FL. And we identified five research directions that deserve further investigation: benchmarks, interpretability, decentralized aggregation, FL on graphs, imbalanced data, and the applications of FL systems to distributed intelligent systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
California consumer privacy act home page.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.caprivacy.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.caprivacy.org/</a>.

</span>
<span class="ltx_bibblock">Online; accessed 14/02/2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M Salehi Heydar Abad, Emre Ozfatura, Deniz Gunduz, and Ozgur Ercetin.

</span>
<span class="ltx_bibblock">Hierarchical federated learning across heterogeneous cellular
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Acoustics, Speech and Signal Processing
(ICASSP)</span>, pages 8866–8870, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ACM SIGSAC conf. on computer and communications security</span>,
pages 308–318, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Zakaria Abou El Houda, Abdelhakim Hafid, and Lyes Khoukhi.

</span>
<span class="ltx_bibblock">Co-iot: a collaborative ddos mitigation scheme in iot environment
based on blockchain using sdn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Global Communications Conference (GLOBECOM)</span>, pages
1–6, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning via additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
13(5):1333–1345, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav
Choudhary.

</span>
<span class="ltx_bibblock">Federated learning with personalization layers.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.00818</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.

</span>
<span class="ltx_bibblock">Stochastic gradient push for distributed deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, volume 97, pages
344–353, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico
Vitali, and Giovanni Felici.

</span>
<span class="ltx_bibblock">Hacking smart machines with smarter ones: How to extract meaningful
data from machine learning classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Int. Journal of Security and Networks</span>, 10(3):137–150, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ammar Ahmad Awan, Ching-Hsiang Chu, Hari Subramoni, and Dhabaleswar K Panda.

</span>
<span class="ltx_bibblock">Optimized broadcast for deep learning workloads on dense-gpu
infiniband clusters: Mpi or nccl?

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">European MPI Users’ Group Meeting</span>, pages 1–9, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Artificial Intelligence and Statistics
(AISTATS)</span>, pages 2938–2948, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Baidu.

</span>
<span class="ltx_bibblock">Federated deep learning in paddlepaddle.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/PaddleFL</a>.

</span>
<span class="ltx_bibblock">Online; accessed 16/02/2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Baidu.

</span>
<span class="ltx_bibblock">Paddlepaddle interpretability.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/PaddlePaddle/InterpretDL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/InterpretDL</a>.

</span>
<span class="ltx_bibblock">Online; accessed 13/03/2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and
Nicholas D Lane.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.14390</span>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.

</span>
<span class="ltx_bibblock">Analyzing federated learning through an adversarial lens.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 634–643,
2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jingjun Bi and Chongsheng Zhang.

</span>
<span class="ltx_bibblock">An empirical comparison on state-of-the-art multi-class imbalance
learning algorithms and a new diversified ensemble learning scheme.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Knowledge-Based Systems</span>, 158:81–93, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jiang Bian, Haoyi Xiong, Wei Cheng, Wenqing Hu, Zhishan Guo, and Yanjie Fu.

</span>
<span class="ltx_bibblock">Multi-party sparse discriminant learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Data Mining (ICDM)</span>,
pages 745–750. IEEE, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jiang Bian, Haoyi Xiong, Yanjie Fu, Jun Huan, and Zhishan Guo.

</span>
<span class="ltx_bibblock">Mp2sda: Multi-party parallelized sparse discriminant learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Knowledge Discovery from Data (TKDD)</span>,
14(3):1–22, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Konecný, Stefano
Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage,
and Jason Roselander.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Machine Learning and Systems (MLSys)</span>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Christopher Briggs, Zhong Fan, and Peter Andras.

</span>
<span class="ltx_bibblock">Federated learning with hierarchical clustering of local updates to
improve training on non-iid data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Int. Joint Conf. on Neural Networks (IJCNN)</span>, pages 1–9.
IEEE, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch
Paschalidis, and Wei Shi.

</span>
<span class="ltx_bibblock">Federated learning of predictive models from federated electronic
health records.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Int. journal of Medical Informatics (IJMI)</span>, 112:59–67, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub
Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.01097</span>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by reducing client resource
requirements.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.07210</span>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFadden, Ken Goldman, Mike
Gunter, Jeremiah Harmsen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, et al.

</span>
<span class="ltx_bibblock">Sibyl: A system for large scale supervised machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Technical Talk</span>, 1:113, 2012.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ferhat Özgür Çatak.

</span>
<span class="ltx_bibblock">Secure multi-party computation based privacy preserving extreme
learning machine algorithm over vertically distributed data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Neural Information Processing (ICONIP)</span>,
pages 337–345, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ferhat Özgür Çatak and Ahmet Fatih Mustacoglu.

</span>
<span class="ltx_bibblock">Cpp-elm: Cryptographically privacy-preserving extreme learning
machine for cloud systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Int. Journal of Computational Intelligence Systems</span>,
11(1):33–44, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Samprit Chatterjee and Eugene Seneta.

</span>
<span class="ltx_bibblock">Towards consensus: Some convergence theorems on repeated averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Journal of Applied Probability</span>, pages 89–97, 1977.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Chien-Lun Chen, Leana Golubchik, and Marco Paolieri.

</span>
<span class="ltx_bibblock">Backdoor attacks on federated meta-learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.07026</span>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jianshu Chen and Ali H Sayed.

</span>
<span class="ltx_bibblock">Diffusion adaptation strategies for distributed optimization and
learning over networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, 60(8):4289–4305, 2012.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, and Huajun Chen.

</span>
<span class="ltx_bibblock">Fede: Embedding knowledge graphs in federated setting.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.12882</span>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Yang Chen, Xiaoyan Sun, and Yaochu Jin.

</span>
<span class="ltx_bibblock">Communication-efficient federated deep learning with layerwise
asynchronous model update and temporally weighted aggregation.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
31(10):4229–4238, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yu Chen, Fang Luo, Tong Li, Tao Xiang, Zheli Liu, and Jin Li.

</span>
<span class="ltx_bibblock">A training-integrity privacy-preserving federated learning scheme
with trusted execution environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Information Sciences</span>, 522:69–79, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Warren B Chik.

</span>
<span class="ltx_bibblock">The singapore personal data protection act and an assessment of
future trends in data privacy reform.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Computer Law &amp; Security Review</span>, 29(5):554–575, 2013.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik.

</span>
<span class="ltx_bibblock">Emnist: Extending mnist to handwritten letters.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Int. Joint Conf. on Neural Networks (IJCNN)</span>, pages
2921–2926, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Kate Conger.

</span>
<span class="ltx_bibblock">Uber settles data breach investigation for $148 million.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nytimes.com/2018/09/26/technology/uber-data-breach.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nytimes.com/2018/09/26/technology/uber-data-breach.html</a>.

</span>
<span class="ltx_bibblock">Online; accessed 17/02/2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kate Conger.

</span>
<span class="ltx_bibblock">Uber settles data breach investigation for $148 million, 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nytimes.com/2018/09/26/technology/uber-data-breach.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nytimes.com/2018/09/26/technology/uber-data-breach.html</a>.

</span>
<span class="ltx_bibblock">Online; accessed 28/02/2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, pages 248–255, 2009.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Adaptive personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.13461</span>, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen.

</span>
<span class="ltx_bibblock">Personalized federated learning with moreau envelopes.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.08848</span>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Cynthia Dwork.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Int. conf. on theory and applications of models of
computation</span>, pages 1–19, 2008.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sean R Eddy.

</span>
<span class="ltx_bibblock">What is a hidden markov model?

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Nature biotechnology</span>, 22(10):1315–1316, 2004.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong.

</span>
<span class="ltx_bibblock">Local model poisoning attacks to byzantine-robust federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">USENIX Security Symposium (USENIX Security)</span>, pages
1605–1622, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Siwei Feng and Han Yu.

</span>
<span class="ltx_bibblock">Multi-participant multi-class vertical federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.11154</span>, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Zhi Feng, Haoyi Xiong, Chuanyuan Song, Sijia Yang, Baoxin Zhao, Licheng Wang,
Zeyu Chen, Shengwen Yang, Liping Liu, and Jun Huan.

</span>
<span class="ltx_bibblock">Securegbm: Secure multi-party gradient boosting.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Big Data (Big Data)</span>, pages 1312–1321,
2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ian Fette and Alexey Melnikov.

</span>
<span class="ltx_bibblock">The websocket protocol, 2011.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Michael J Flynn.

</span>
<span class="ltx_bibblock">Some computer organizations and their effectiveness.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Computers</span>, 100(9):948–960, 1972.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.

</span>
<span class="ltx_bibblock">Mitigating sybils in federated learning poisoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.04866</span>, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
B. M. Gaff, H. E. Sussman, and J. Geetter.

</span>
<span class="ltx_bibblock">Privacy and big data.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Computer</span>, 47(6):7–9, 2014.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
K Ganga and S Karthik.

</span>
<span class="ltx_bibblock">A fault tolerent approach in scientific workflow systems based on
cloud computing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Pattern Recognition, Informatics and Mobile
Engineering</span>, pages 387–390, 2013.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.

</span>
<span class="ltx_bibblock">Inverting gradients–how easy is it to break privacy in federated
learning?

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.14053</span>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo Klein, and Moin Nabi.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.07557</span>, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Andrew Gibiansky.

</span>
<span class="ltx_bibblock">Bringing hpc techniques to deep learning.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/</a>,
2017.

</span>
<span class="ltx_bibblock">Online; accessed 2020-08-12.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and
Lalana Kagal.

</span>
<span class="ltx_bibblock">Explaining explanations: An overview of interpretability of machine
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Data Science and Advanced Analytics
(DSAA)</span>, pages 80–89. IEEE, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Deep learning</span>, volume 1.

</span>
<span class="ltx_bibblock">MIT press Cambridge, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.

</span>
<span class="ltx_bibblock">Explaining and harnessing adversarial examples.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Learning Representations (ICLR)</span>, 2015.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Tensorflow federated: Machine learning on decentralized data.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>.

</span>
<span class="ltx_bibblock">Online; accessed 16/02/2021.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
William Gropp, William D Gropp, Ewing Lusk, Anthony Skjellum, and Argonne
Distinguished Fellow Emeritus Ewing Lusk.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Using MPI: portable parallel programming with the
message-passing interface</span>, volume 1.

</span>
<span class="ltx_bibblock">MIT press, 1999.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Federated learning with compression: Unified analysis and sharp
guarantees.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.01154</span>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Meng Hao, Hongwei Li, Guowen Xu, Sen Liu, and Haomiao Yang.

</span>
<span class="ltx_bibblock">Towards efficient and privacy-preserving federated deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Communications (ICC)</span>, pages 1–6, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.10677</span>, 2017.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Collaborative training of large cnns on the
edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.14513</span>, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Towards non-iid and invisible data with fednas: Federated deep
learning via neural architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.08546</span>, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Yu Rong, Peilin Zhao, Junzhou
Huang, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fedgraphnn: A federated learning system and benchmark for graph
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.07145</span>, 2021.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman
Avestimehr.

</span>
<span class="ltx_bibblock">Spreadgnn: Serverless multi-task federated learning for graph neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.02743</span>, 2021.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Pipetransformer: Automated elastic pipelining for distributed
training of large-scale models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning</span>, volume 139 of <span id="bib.bib64.2.2" class="ltx_text ltx_font_italic">Machine
Learning Research</span>, pages 4150–4159, 2021.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang
Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.13518</span>, 2020.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Chaoyang He, Alay Dilipbhai Shah, Zhenheng Tang, Di Fan, Adarshan Naiynar
Sivashunmugam, Keerti Bhogaraju, Mita Shimpi, Li Shen, Xiaowen Chu, Mahdi
Soltanolkotabi, et al.

</span>
<span class="ltx_bibblock">Fedcv: A federated learning framework for diverse computer vision
tasks.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, and Ji Liu.

</span>
<span class="ltx_bibblock">Central server free federated learning over single-sided trust social
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.04956</span>, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang.

</span>
<span class="ltx_bibblock">Milenas: Efficient neural architecture search via mixed-level
reformulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conf. on Computer Vision and Pattern Recognition
(CVPR)</span>, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Haibo He and Edwardo A. Garcia.

</span>
<span class="ltx_bibblock">Learning from imbalanced data.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering (TKDE)</span>,
21(9):1263–1284, 2009.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Lie He, Sai Praneeth Karimireddy, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Secure byzantine-robust machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.04747</span>, 2020.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">ACM SIGSAC Conference on Computer and Communications
Security</span>, pages 603–618, 2017.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Chenghao Hu, Jingyan Jiang, and Zhi Wang.

</span>
<span class="ltx_bibblock">Decentralized federated learning: A segmented gossip approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.07782</span>, 2019.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu.

</span>
<span class="ltx_bibblock">Fedmgda+: Federated learning meets multi-objective optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.11489</span>, 2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao
Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline
parallelism.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.06965</span>, 2018.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
and Raman Arora.

</span>
<span class="ltx_bibblock">Communication-efficient distributed sgd with sketching.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.04488</span>, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui.

</span>
<span class="ltx_bibblock">Sketchml: Accelerating distributed machine learning with data
sketches.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Management of Data</span>, pages 1269–1284, 2018.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Jing Jiang, Shaoxiong Ji, and Guodong Long.

</span>
<span class="ltx_bibblock">Decentralized knowledge acquisition for mobile internet applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">World Wide Web</span>, pages 1–17, 2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Meng Jiang, Taeho Jung, Ryan Karl, and Tong Zhao.

</span>
<span class="ltx_bibblock">Federated dynamic gnn with secure aggregation.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.07351</span>, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Aurélien Bellet Brendan Avent, Arjun
Nitin Bhagoji Mehdi Bennis, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, Rafael G.L. D’Oliveira, Salim El Rouayheb, David Evans, Josh
Gardner, Zachary Garrett, Adrià Gascón, Phillip B. Gibbons Badih Ghazi,
Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben
Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi
Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage,
Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun,
Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang,
Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
14(1), 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 5132–5143,
2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Error feedback fixes signsgd and other gradient compression schemes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 3252–3261,
2019.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Kleomenis Katevas, Eugene Bagdasaryan, Jason Waterman, Mohamad Mounir Safadieh,
Eleanor Birrell, Hamed Haddadi, and Deborah Estrin.

</span>
<span class="ltx_bibblock">Policy-based federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">arXiv e-prints</span>, pages arXiv–2003, 2020.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Chuyang Ke and Jean Honorio.

</span>
<span class="ltx_bibblock">Federated myopic community detection with one-shot communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.07255</span>, 2021.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Ivan Kholod, Evgeny Yanaki, Dmitry Fomichev, Evgeniy Shalugin, Evgenia
Novikova, Evgeny Filippov, and Mats Nordlund.

</span>
<span class="ltx_bibblock">Open-source federated learning frameworks for iot: A comparative
review and analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 21(1):167, 2021.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant.

</span>
<span class="ltx_bibblock">Survey of personalization techniques for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">World Conf. on Smart Trends in Systems, Security and
Sustainability (WorldS4)</span>, pages 794–797, 2020.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, and Farinaz Koushanfar.

</span>
<span class="ltx_bibblock">Peer-to-peer federated learning on graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901.11173</span>, 2019.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, and Bingsheng He.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.09693</span>, 2019.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Suyi Li, Yong Cheng, Yang Liu, Wei Wang, and Tianjian Chen.

</span>
<span class="ltx_bibblock">Abnormal client behavior detection in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.09933</span>, 2019.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50–60, 2020.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Machine Learning and Systems</span>, volume 2, pages 429–450,
2020.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.10497</span>, 2019.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia.

</span>
<span class="ltx_bibblock">Backdoor learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.08745</span>, 2020.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Zhaorui Li, Zhicong Huang, Chaochao Chen, and Cheng Hong.

</span>
<span class="ltx_bibblock">Quantification of the leakage in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.05467</span>, 2019.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.

</span>
<span class="ltx_bibblock">Can decentralized algorithms outperform centralized algorithms? a
case study for decentralized parallel stochastic gradient descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</span>, pages 5330–5340, 2017.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Zhicong Liang, Bao Wang, Quanquan Gu, Stanley Osher, and Yuan Yao.

</span>
<span class="ltx_bibblock">Exploring private federated learning with laplacian smoothing.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.00218</span>, 2020.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Misbah Liaqat, Victor Chang, Abdullah Gani, Siti Hafizah Ab Hamid, Muhammad
Toseef, Umar Shoaib, and Rana Liaqat Ali.

</span>
<span class="ltx_bibblock">Federated cloud resource management: Review and discussion.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">Journal of Network and Computer Applications</span>, 77:87–105, 2017.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 22(3):2031–2063,
2020.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi
Soltanolkotabi, Xiang Ren, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fednlp: A research platform for federated learning in natural
language processing.

</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.08815</span>, 2021.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Jierui Lin, Min Du, and Jian Liu.

</span>
<span class="ltx_bibblock">Free-riders in federated learning: Attacks and defenses.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.12560</span>, 2019.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Yilun Lin, Chaochao Chen, Cen Chen, and Li Wang.

</span>
<span class="ltx_bibblock">Improving federated relational data modeling via basis alignment and
weight penalty.

</span>
<span class="ltx_bibblock"><span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.11369</span>, 2020.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Ji Liu, Carlyna Bondiombouy, Lei Mo, and Patrick Valduriez.

</span>
<span class="ltx_bibblock">Two-phase scheduling for efficient vehicle sharing.

</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems
(TITS)</span>, 2020.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Ji Liu, Esther Pacitti, Patrick Valduriez, Daniel De Oliveira, and Marta
Mattoso.

</span>
<span class="ltx_bibblock">Multi-objective scheduling of scientific workflows in multisite
clouds.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 63:76–95, 2016.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Ji Liu, Esther Pacitti, Patrick Valduriez, and Marta Mattoso.

</span>
<span class="ltx_bibblock">A survey of data-intensive scientific workflow management.

</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">Journal of Grid Computing</span>, 13(4):457–493, 2015.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Ji Liu, Luis Pineda, Esther Pacitti, Alexandru Costan, Patrick Valduriez,
Gabriel Antoniu, and Marta Mattoso.

</span>
<span class="ltx_bibblock">Efficient scheduling of scientific workflows using hot metadata in a
multisite cloud.

</span>
<span class="ltx_bibblock"><span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering (TKDE)</span>,
31(10):1940–1953, 2018.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief.

</span>
<span class="ltx_bibblock">Client-edge-cloud hierarchical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Communications (ICC)</span>, pages 1–6, 2020.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Ruixuan Liu, Yang Cao, Masatoshi Yoshikawa, and Hong Chen.

</span>
<span class="ltx_bibblock">Fedsel: Federated sgd under local differential privacy with top-k
dimension selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Database Systems for Advanced Applications</span>,
pages 485–501, 2020.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng,
Tianjian Chen, Han Yu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Fedvision: An online visual object detection platform powered by
federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">AAAI Conf. on Artificial Intelligence</span>, volume 34, pages
13172–13179, 2020.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi
Hong, and Qiang Yang.

</span>
<span class="ltx_bibblock">A communication efficient collaborative learning framework for
distributed features.

</span>
<span class="ltx_bibblock"><span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.11187</span>, 2019.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-young Paik, Xiwei Xu, and Chen Wang.

</span>
<span class="ltx_bibblock">Architectural patterns for the design of federated learning systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.02373</span>, 2021.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Siqi Luo, Xu Chen, Qiong Wu, Zhi Zhou, and Shuai Yu.

</span>
<span class="ltx_bibblock">Hfel: Joint edge association and resource allocation for
cost-efficient hierarchical federated edge learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Wireless Communications</span>,
19(10):6535–6548, 2020.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Xinjian Luo and Xiangqi Zhu.

</span>
<span class="ltx_bibblock">Exploiting defenses against gan-based feature inference attacks in
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.12571</span>, 2020.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Threats to federated learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.02133</span>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong
Jin, Han Yu, and Kee Siong Ng.

</span>
<span class="ltx_bibblock">Towards fair and privacy-preserving federated deep models.

</span>
<span class="ltx_bibblock"><span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems (TPDS)</span>,
31(11):2524–2541, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Yanjun Ma, Dianhai Yu adn Tian Wu, and Haifeng Wang.

</span>
<span class="ltx_bibblock">Paddlepaddle: An open-source deep learning platform from industrial
practice.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">Frontiers of Data and Computing</span>, 1(1):105, 2019.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Amirhossein Malekijoo, Mohammad Javad Fadaeieslam, Hanieh Malekijou, Morteza
Homayounfar, Farshid Alizadeh-Shabdiz, and Reza Rawassizadeh.

</span>
<span class="ltx_bibblock">FEDZIP: A compression framework for communication-efficient
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.01593</span>, 2021.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Kalikinkar Mandal and Guang Gong.

</span>
<span class="ltx_bibblock">PrivFL: Practical privacy-preserving federated regressions on
high-dimensional data over mobile networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">ACM SIGSAC Conf. on Cloud Computing Security Workshop</span>, pages
57–68, 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Frank McKeen, Ilya Alexandrovich, Alex Berenzon, Carlos V. Rozas, Hisham Shafi,
Vedvyas Shanbhogue, and Uday R. Savagaonkar.

</span>
<span class="ltx_bibblock">Innovative instructions and software model for isolated execution.

</span>
<span class="ltx_bibblock">In <span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">Int. Workshop on Hardware and Architectural Support for
Security and Privacy</span>, 2013.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Artificial Intelligence and Statistics
(AISTATS)</span>, pages 1273–1282, 2017.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.06963</span>, 2017.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Learning Representations (ICLR)</span>, 2018.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Guangxu Mei, Ziyu Guo, Shijun Liu, and Li Pan.

</span>
<span class="ltx_bibblock">Sgnn: A graph neural network based federated learning approach by
hiding structure.

</span>
<span class="ltx_bibblock">In <span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Big Data (Big Data)</span>, pages 2560–2568,
2019.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">IEEE Symposium on Security and Privacy (SP)</span>, pages
691–706, 2019.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Chuizheng Meng, Sirisha Rambhatla, and Yan Liu.

</span>
<span class="ltx_bibblock">Cross-node federated graph neural network for spatio-temporal data
modeling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD)</span>, 2021.

</span>
<span class="ltx_bibblock">To appear.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Naram Mhaisen, Alaa Awad, Amr Mohamed, Aiman Erbad, and Mohsen Guizani.

</span>
<span class="ltx_bibblock">Optimal user-edge assignment in hierarchical federated learning based
on statistical properties and network topology constraints.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Network Science and Engineering</span>, 2021.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Fan Mo and Hamed Haddadi.

</span>
<span class="ltx_bibblock">Efficient and private federated learning using tee.

</span>
<span class="ltx_bibblock">In <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">EuroSys</span>, 2019.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 4615–4625,
2019.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
Dehghantanha, and Gautam Srivastava.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 115:619–640, 2021.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Luis Muñoz-González, Kenneth T Co, and Emil C Lupu.

</span>
<span class="ltx_bibblock">Byzantine-robust federated machine learning through adaptive model
averaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.05125</span>, 2019.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R
Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Pipedream: generalized pipeline parallelism for dnn training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">ACM Symposium on Operating Systems Principles</span>, pages 1–15,
2019.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Keiichi Ochiai, Kohei Senkawa, Naoki Yamamoto, Yuya Tanaka, and Yusuke
Fukazawa.

</span>
<span class="ltx_bibblock">Real-time on-device troubleshooting recommendation for smartphones.

</span>
<span class="ltx_bibblock">In <span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">ACM SIGKDD Int. Conf. on Knowledge Discovery &amp; Data Mining</span>,
pages 2783–2791, 2019.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Official Journal of the European Union.

</span>
<span class="ltx_bibblock">General data protection regulation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679</a>.

</span>
<span class="ltx_bibblock">Online; accessed 12/02/2021.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Sebastian
Nowozin, Kapil Vaswani, and Manuel Costa.

</span>
<span class="ltx_bibblock">Oblivious multi-party machine learning on trusted processors.

</span>
<span class="ltx_bibblock">In <math id="bib.bib134.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib134.1.m1.1a"><mo stretchy="false" id="bib.bib134.1.m1.1.1" xref="bib.bib134.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib134.1.m1.1b"><ci id="bib.bib134.1.m1.1.1.cmml" xref="bib.bib134.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib134.1.m1.1c">\{</annotation></semantics></math><span id="bib.bib134.4.3" class="ltx_text ltx_font_italic">USENIX<math id="bib.bib134.2.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib134.2.1.m1.1a"><mo stretchy="false" id="bib.bib134.2.1.m1.1.1" xref="bib.bib134.2.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib134.2.1.m1.1b"><ci id="bib.bib134.2.1.m1.1.1.cmml" xref="bib.bib134.2.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib134.2.1.m1.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib134.3.2.m2.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib134.3.2.m2.1a"><mo stretchy="false" id="bib.bib134.3.2.m2.1.1" xref="bib.bib134.3.2.m2.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib134.3.2.m2.1b"><ci id="bib.bib134.3.2.m2.1.1.cmml" xref="bib.bib134.3.2.m2.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib134.3.2.m2.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib134.4.3.m3.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib134.4.3.m3.1a"><mo stretchy="false" id="bib.bib134.4.3.m3.1.1" xref="bib.bib134.4.3.m3.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib134.4.3.m3.1b"><ci id="bib.bib134.4.3.m3.1.1.cmml" xref="bib.bib134.4.3.m3.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib134.4.3.m3.1c">\}</annotation></semantics></math> Security)</span>,
pages 619–636, 2016.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Kemal Oksuz, Baris Can Cam, Sinan Kalkan, and Emre Akbas.

</span>
<span class="ltx_bibblock">Imbalance problems in object detection: A review.

</span>
<span class="ltx_bibblock"><span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
2020.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
OpenMined.

</span>
<span class="ltx_bibblock">Pysyft.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/OpenMined/PySyft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenMined/PySyft</a>.

</span>
<span class="ltx_bibblock">Online; accessed 22/02/2021.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
PaddlePaddle, Baidu.

</span>
<span class="ltx_bibblock">Paddlehub.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/PaddlePaddle/PaddleHub" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/PaddleHub</a>.

</span>
<span class="ltx_bibblock">Online; accessed 01/10/2021.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Pascal Paillier.

</span>
<span class="ltx_bibblock">Public-key cryptosystems based on composite degree residuosity
classes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Int. Conf. on the theory and applications of cryptographic
techniques</span>, pages 223–238, 1999.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Sinno Jialin Pan and Qiang Yang.

</span>
<span class="ltx_bibblock">A survey on transfer learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering (TKDE)</span>,
22(10):1345–1359, 2009.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Hao Peng, Haoran Li, Yangqiu Song, Vincent Zheng, and Jianxin Li.

</span>
<span class="ltx_bibblock">Federated knowledge graphs embedding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">ACM Int. Conf. on Information and Knowledge Management
(CIKM)</span>, pages 1–10, 2021.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Hai Phan, My T Thai, Han Hu, Ruoming Jin, Tong Sun, and Dejing Dou.

</span>
<span class="ltx_bibblock">Scalable differential privacy with certified robustness in
adversarial learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 7683–7694,
2020.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.

</span>
<span class="ltx_bibblock">Robust aggregation for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.13445</span>, 2019.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Luis Pineda-Morales, Ji Liu, Alexandru Costan, Esther Pacitti, Gabriel Antoniu,
Patrick Valduriez, and Marta Mattoso.

</span>
<span class="ltx_bibblock">Managing hot metadata for scientific workflows on multisite clouds.

</span>
<span class="ltx_bibblock">In <span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Big Data (Big Data)</span>, pages 390–397,
2016.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Pytorch.

</span>
<span class="ltx_bibblock">Pytorch.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://pytorch.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/</a>.

</span>
<span class="ltx_bibblock">Online; accessed 13/03/2021.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Herbert Robbins and Sutton Monro.

</span>
<span class="ltx_bibblock">A stochastic approximation method.

</span>
<span class="ltx_bibblock"><span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">The annals of mathematical statistics</span>, pages 400–407, 1951.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas
Ismail, Tudor Cebere, Robert Sandmann, Robin Roehm, and Michael A. Hoeh.

</span>
<span class="ltx_bibblock">Pyvertical: A vertical federated learning framework for
multi-headed splitnn.

</span>
<span class="ltx_bibblock"><span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.00489</span>, 2021.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
Vladimir Braverman, Joseph Gonzalez, and Raman Arora.

</span>
<span class="ltx_bibblock">Fetchsgd: Communication-efficient federated learning with sketching.

</span>
<span class="ltx_bibblock">In <span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 8253–8265,
2020.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.04017</span>, 2018.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
César Sabater, Aurélien Bellet, and Jan Ramon.

</span>
<span class="ltx_bibblock">Distributed differentially private averaging with improved utility
and robustness to malicious parties.

</span>
<span class="ltx_bibblock"><span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.07218</span>, 2020.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Adam Satariano.

</span>
<span class="ltx_bibblock">Google is fined $57 million under europe’s data privacy law.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nytimes.com/2019/01/21/technology/google-europe-gdpr-fine.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nytimes.com/2019/01/21/technology/google-europe-gdpr-fine.html</a>.

</span>
<span class="ltx_bibblock">Online; accessed 28/02/2021.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Ali H Sayed.

</span>
<span class="ltx_bibblock">Adaptation, learning, and optimization over networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">Foundations and Trends in Machine Learning</span>,
7(ARTICLE):311–801, 2014.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Ali H Sayed, Sheng-Yuan Tu, Jianshu Chen, Xiaochuan Zhao, and Zaid J Towfic.

</span>
<span class="ltx_bibblock">Diffusion strategies for adaptation and learning over networks: an
examination of distributed strategies and network behavior.

</span>
<span class="ltx_bibblock"><span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 30(3):155–171, 2013.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Mohamed Seif, Ravi Tandon, and Ming Li.

</span>
<span class="ltx_bibblock">Wireless federated learning with local differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">IEEE Int. Symposium on Information Theory (ISIT)</span>, pages
2604–2609, 2020.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Eugene Seneta.

</span>
<span class="ltx_bibblock"><span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">Non-negative matrices and Markov chains</span>.

</span>
<span class="ltx_bibblock">Springer Science &amp; Business Media, 2006.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
William Shakespeare.

</span>
<span class="ltx_bibblock"><span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">The complete works of William Shakespeare</span>.

</span>
<span class="ltx_bibblock">Wordsworth Editions, 2007.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Nir Shlezinger, Mingzhe Chen, Yonina C Eldar, H Vincent Poor, and Shuguang Cui.

</span>
<span class="ltx_bibblock">Federated learning with quantization constraints.

</span>
<span class="ltx_bibblock">In <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Acoustics, Speech and Signal Processing
(ICASSP)</span>, pages 8851–8855, 2020.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Nir Shlezinger, Mingzhe Chen, Yonina C Eldar, H Vincent Poor, and Shuguang Cui.

</span>
<span class="ltx_bibblock">Uveqfed: Universal vector quantization for federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, 2020.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
JASON SILVERSTEIN.

</span>
<span class="ltx_bibblock">Hundreds of millions of facebook user records were exposed on amazon
cloud server.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.cbsnews.com/news/millions-facebook-user-records-exposed-amazon-cloud-server/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.cbsnews.com/news/millions-facebook-user-records-exposed-amazon-cloud-server/</a>.

</span>
<span class="ltx_bibblock">Online; accessed 28/02/2021.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Ryan Spring, Anastasios Kyrillidis, Vijai Mohan, and Anshumali Shrivastava.

</span>
<span class="ltx_bibblock">Compressing gradient optimizers via count-sketches.

</span>
<span class="ltx_bibblock">In <span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 5946–5955,
2019.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Standing Committee of the National People’s Congress.

</span>
<span class="ltx_bibblock">Cybersecurity law of the people’s republic of china.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.newamerica.org/cybersecurity-initiative/digichina/blog/translation-cybersecurity-law-peoples-republic-china/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.newamerica.org/cybersecurity-initiative/digichina/blog/translation-cybersecurity-law-peoples-republic-china/</a>.

</span>
<span class="ltx_bibblock">Online; accessed 22/02/2021.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Sparsified SGD with memory.

</span>
<span class="ltx_bibblock">In <span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</span>, volume 31, 2018.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Haijian Sun, Xiang Ma, and Rose Qingyang Hu.

</span>
<span class="ltx_bibblock">Adaptive federated learning with gradient compression in uplink noma.

</span>
<span class="ltx_bibblock"><span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Vehicular Technology</span>, 2020.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Can you really backdoor federated learning?

</span>
<span class="ltx_bibblock"><span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.07963</span>, 2019.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Toyotaro Suzumura, Yi Zhou, Nathalie Barcardo, Guangnan Ye, Keith Houck, Ryo
Kawahara, Ali Anwar, Lucia Larise Stavarache, Daniel Klyashtorny, Heiko
Ludwig, and Kumar Bhaskaran.

</span>
<span class="ltx_bibblock">Towards federated graph learning for collaborative financial crimes
detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.12946</span>, 2019.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu.

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">European Symposium on Research in Computer Security</span>, pages
480–501. Springer, 2020.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Aleksei Triastcyn and Boi Faltings.

</span>
<span class="ltx_bibblock">Federated generative privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, 35(4):50–57, 2020.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui
Zhang, and Yi Zhou.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">ACM Workshop on Artificial Intelligence and Security</span>, pages
1–11, 2019.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Paul Vanhaesebrouck, Aurélien Bellet, and Marc Tommasi.

</span>
<span class="ltx_bibblock">Decentralized collaborative learning of personalized models over
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Artificial Intelligence and Statistics
(AISTATS)</span>, pages 509–517, 2017.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Graph attention networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Learning Representations (ICLR)</span>, 2018.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim
Verbelen, and Jan S Rellermeyer.

</span>
<span class="ltx_bibblock">A survey on distributed machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 53(2):1–33, 2020.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Abhinav Vishnu, Charles Siegel, and Jeffrey Daily.

</span>
<span class="ltx_bibblock">Distributed tensorflow with mpi.

</span>
<span class="ltx_bibblock"><span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.02339</span>, 2016.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Aidmar Wainakh, Alejandro Sanchez Guinea, Tim Grube, and Max
Mühlhäuser.

</span>
<span class="ltx_bibblock">Enhancing privacy via hierarchical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">IEEE European Symposium on Security and Privacy Workshops
(EuroS&amp;PW)</span>, pages 344–347, 2020.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Binghui Wang, Ang Li, Hai Li, and Yiran Chen.

</span>
<span class="ltx_bibblock">Graphfl: A federated learning framework for semi-supervised node
classification on graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.04187</span>, 2020.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Chunnan Wang, Bozhou Chen, Geng Li, and Hongzhi Wang.

</span>
<span class="ltx_bibblock">FL-AGCNS: federated learning framework for automatic graph
convolutional network search.

</span>
<span class="ltx_bibblock"><span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.04141</span>, 2021.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Guan Wang.

</span>
<span class="ltx_bibblock">Interpret federated learning with shapley values.

</span>
<span class="ltx_bibblock"><span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.04519</span>, 2019.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock">In <span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Learning Representations (ICLR)</span>, 2020.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan,
Blaise Agüera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman
Avestimehr, Katharine Daly, Deepesh Data, Suhas N. Diggavi, Hubert Eichner,
Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip Hanzely, Andrew
Hard, Chaoyang He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Martin Jaggi,
Tara Javidi, Peter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub
Konečný, Sanmi Koyejo, Tian Li, Luyang Liu, Mehryar Mohri, Hang Qi,
Sashank J. Reddi, Peter Richtárik, Karan Singhal, Virginia Smith, Mahdi
Soltanolkotabi, Weikang Song, Ananda Theertha Suresh, Sebastian U. Stich,
Ameet Talwalkar, Hongyi Wang, Blake E. Woodworth, Shanshan Wu, Felix X. Yu,
Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong Zhang, Chunxiang Zheng, Chen Zhu,
and Wennan Zhu.

</span>
<span class="ltx_bibblock">A field guide to federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.06917</span>, 2021.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar.

</span>
<span class="ltx_bibblock">Matcha: Speeding up decentralized SGD via matching decomposition
sampling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">Indian Control Conference (ICC)</span>, pages 299–300, 2019.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Lixu Wang, Shichao Xu, Xiao Wang, and Qi Zhu.

</span>
<span class="ltx_bibblock">Addressing class imbalance in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">AAAI Conf. on Artificial Intelligence</span>, volume 35, pages
10165–10173, 2021.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Luping WANG, Wei WANG, and LI Bo.

</span>
<span class="ltx_bibblock">CMFL: Mitigating communication overhead for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">IEEE Int. Conf. on Distributed Computing Systems (ICDCS)</span>,
pages 954–964, 2019.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">IEEE Conf. on Computer Communications (INFOCOM)</span>, pages
2512–2520, 2019.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
WeBank.

</span>
<span class="ltx_bibblock">Federated ai technology enabler (FATE).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/FederatedAI/FATE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FederatedAI/FATE</a>.

</span>
<span class="ltx_bibblock">Online; accessed 16/02/2021.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
WeBank.

</span>
<span class="ltx_bibblock">Federated learning white paper v2.0.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aisp-1251170195.cos.ap-hongkong.myqcloud.com/wp-content/uploads/pdf/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%99%BD%E7%9A%AE%E4%B9%A6_v2.0.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aisp-1251170195.cos.ap-hongkong.myqcloud.com/wp-content/uploads/pdf/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%99%BD%E7%9A%AE%E4%B9%A6_v2.0.pdf</a>.

</span>
<span class="ltx_bibblock">Online; accessed 14/02/2021.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,
Tony QS Quek, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454–3469, 2020.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie.

</span>
<span class="ltx_bibblock">Fedgnn: Federated graph neural network for privacy-preserving
recommendation.

</span>
<span class="ltx_bibblock"><span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.04925</span>, 2021.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang, and Dahua Lin.

</span>
<span class="ltx_bibblock">Adversarial robustness under long-tailed distribution.

</span>
<span class="ltx_bibblock">In <span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, pages 8659–8668, 2021.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Chang Xu, Dacheng Tao, and Chao Xu.

</span>
<span class="ltx_bibblock">A survey on multi-view learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1304.5634</span>, 2013.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
Jie Xu, Benjamin S Glicksberg, Chang Su, Peter Walker, Jiang Bian, and Fei
Wang.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">Journal of Healthcare Informatics Research</span>, pages 1–19, 2020.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Jinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng.

</span>
<span class="ltx_bibblock">Ternary compression for communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2020.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Xun Yi, Russell Paulet, and Elisa Bertino.

</span>
<span class="ltx_bibblock">Homomorphic encryption.

</span>
<span class="ltx_bibblock">In <span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">Homomorphic Encryption and Applications</span>, pages 27–46.
Springer, 2014.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Jinliang Yuan, Mengwei Xu, Xiao Ma, Ao Zhou, Xuanzhe Liu, and Shangguang Wang.

</span>
<span class="ltx_bibblock">Hierarchical federated learning through lan-wan orchestration.

</span>
<span class="ltx_bibblock"><span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11612</span>, 2020.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia
Hoang, and Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Bayesian nonparametric federated learning of neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib193.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Machine Learning (ICML)</span>, pages 7252–7261,
2019.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu.

</span>
<span class="ltx_bibblock">Batchcrypt: Efficient homomorphic encryption for cross-silo federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">USENIX Annual Technical Conference (USENIX ATC)</span>, pages
493–506, 2020.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Chongsheng Zhang, Jingjun Bi, and Paolo Soda.

</span>
<span class="ltx_bibblock">Feature selection and resampling in class imbalance learning: Which
comes first? an empirical study in the biological domain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib195.1.1" class="ltx_text ltx_font_italic">Int. Conf. on Bioinformatics and Biomedicine (BIBM)</span>, pages
933–938, 2017.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Chongsheng Zhang, Jingjun Bi, Shixin Xu, Enislay Ramentol, Gaojuan Fan, Baojun
Qiao, and Hamido Fujita.

</span>
<span class="ltx_bibblock">Multi-imbalance: An open-source software for multi-class imbalance
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">Knowledge-Based Systems</span>, 174:137–143, 2019.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Chongsheng Zhang, Paolo Soda, Jingjun Bi, Gaojuan Fan, George Almpanidis, and
Salvador Garcia.

</span>
<span class="ltx_bibblock">An empirical study on the joint impact of feature selection and data
resampling on imbalance classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.00201</span>, 2021.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Huanding Zhang, Tao Shen, Fei Wu, Mingyang Yin, Hongxia Yang, and Chao Wu.

</span>
<span class="ltx_bibblock">Federated graph learning - A position paper.

</span>
<span class="ltx_bibblock"><span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.11099</span>, 2021.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Tuo Zhang, Chaoyang He, Tianhao Ma, Mark Ma, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: A federated learning
framework for on-device anomaly data detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.07976</span>, 2021.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Xiaoli Zhang, Fengting Li, Zeyu Zhang, Qi Li, Cong Wang, and Jianping Wu.

</span>
<span class="ltx_bibblock">Enabling execution assurance of federated learning at untrusted
participants.

</span>
<span class="ltx_bibblock">In <span id="bib.bib200.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM Conf. on Computer Communications</span>, pages
1877–1886, 2020.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen.

</span>
<span class="ltx_bibblock">idlg: Improved deep leakage from gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.02610</span>, 2020.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Yuchen Zhao, Payam Barnaghi, and Hamed Haddadi.

</span>
<span class="ltx_bibblock">Multimodal federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.04833</span>, 2021.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Longfei Zheng, Jun Zhou, Chaochao Chen, Bingzhe Wu, Li Wang, and Benyu Zhang.

</span>
<span class="ltx_bibblock">Asfgnn: Automated separated-federated graph neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">Peer-to-Peer Networking and Applications</span>, 14(3):1692–1704,
2021.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Jun Zhou, Chaochao Chen, Longfei Zheng, Huiwen Wu, Jia Wu, Xiaolin Zheng,
Bingzhe Wu, Ziqi Liu, and Li Wang.

</span>
<span class="ltx_bibblock">Vertically federated graph neural network for privacy-preserving node
classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib204.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.11903</span>, 2020.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Hangyu Zhu, Haoyu Zhang, and Yaochu Jin.

</span>
<span class="ltx_bibblock">From federated learning to federated neural architecture search: a
survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">Complex &amp; Intelligent Systems</span>, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Martin Zinkevich, Markus Weimer, Alexander J Smola, and Lihong Li.

</span>
<span class="ltx_bibblock">Parallelized stochastic gradient descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</span>, volume 4, page 4. Citeseer, 2010.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.14361" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.14362" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.14362">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.14362" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.14363" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 06:19:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
