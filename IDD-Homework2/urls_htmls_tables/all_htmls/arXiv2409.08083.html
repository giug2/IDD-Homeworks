<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality</title>
<!--Generated on Thu Sep 12 14:37:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.08083v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S1" title="In SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Additional Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S2" title="In SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Additional Controlled Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S3" title="In SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Additional Comparisons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S4" title="In SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Additional Details for Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S5" title="In SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Additional Qualitative Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenyang Lei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"> Center for Artificial Intelligence and Robotics, HKISI, CAS
</span>
<span class="ltx_contact ltx_role_affiliation">Princeton University, Department of Computer Science
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liyi Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"> Center for Artificial Intelligence and Robotics, HKISI, CAS
</span>
<span class="ltx_contact ltx_role_affiliation">The Hong Kong Polytechnic University
</span>
<span class="ltx_contact ltx_role_affiliation">these authors contributed equally to this work
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jun Cen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">The Hong Kong University of Science and Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"> Center for Artificial Intelligence and Robotics, HKISI, CAS
</span>
<span class="ltx_contact ltx_role_affiliation">The Hong Kong Polytechnic University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhen Lei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"> Center for Artificial Intelligence and Robotics, HKISI, CAS
</span>
<span class="ltx_contact ltx_role_affiliation">State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Heide
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Princeton University, Department of Computer Science
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziwei Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Nanyang Technological University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qifeng Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">The Hong Kong University of Science and Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhaoxiang Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"> Center for Artificial Intelligence and Robotics, HKISI, CAS
</span>
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Foundation models like ChatGPT and Sora that are trained on a huge scale of data have made a revolutionary social impact. However, it is extremely challenging for sensors in many different fields to collect similar scales of natural images to train strong foundation models. To this end, this work presents a simple and effective
framework <span class="ltx_text ltx_font_typewriter" id="id1.id1.1">SimMAT</span> to study an open problem: the transferability from vision foundation models trained on natural RGB images to other image modalities of different physical properties (e.g., polarization). SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained foundation model. We apply SimMAT to a representative vision foundation model Segment Anything Model (SAM) to support any evaluated new image modality. Given the absence of relevant benchmarks, we construct a new benchmark to evaluate the transfer learning performance. Our experiments confirm the intriguing potential of transferring vision foundation models in enhancing other sensorsâ€™ performance. Specifically, SimMAT can improve the segmentation performance (mIoU) from 22.15% to 53.88% on average for evaluated modalities and consistently outperforms other baselines. We hope that SimMAT can raise awareness of cross-modal transfer learning and benefit various fields for better results with vision foundation models.</p>
</div>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Foundation models have revolutionized computer visionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib3" title="">3</a>]</cite> and natural language processingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib5" title="">5</a>]</cite> across the fields, from personal assistance to self-driving vehicles and medical diagnosisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib11" title="">11</a>]</cite>. Diverse downstream tasks rely directly or indirectly on foundation models by finetuning foundation models that are pretrained on large-scale data with pretext tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib12" title="">12</a>]</cite>. However, while diverse types of sensorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib20" title="">20</a>]</cite> are applied in various domains in the world, e.g., medical imaging, robotics, and fundamental science, not all of them benefit from the development of foundation models.
This is because it is challenging for other sensorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib22" title="">22</a>]</cite> to collect large-scale training data like natural images, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F1" title="Figure 1 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">This work explores the following problem: transferring the vision foundation models to modalities other than natural images. While training foundation models and finetuning them on downstream tasks has been extensively studiedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib25" title="">25</a>]</cite>, the potential of generalizing foundation models to novel image modalities is not fully explored. Arguably, transferring the foundation models to various input modalities like task transfer learning
has the potential to unleash the power of the foundation model on specific sensors: we can utilize the advantages of sensors
in capturing specific physical properties of objects in the world with a strong foundation model.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">The challenges for transferring vision foundation models to other image modalities come from two sides: the modality misalignment and the finetuning cost. A key challenge of cross-modality transfer learning comes from the modality gap: the captured physical signals and the data representation can be highly different, such as the dimensions, the dynamic ranges, and semantic information. Among many differences, the dimension misalignment is one of the major challenges, preventing people from finetuning on new modalities directly. A simple example is that RGB images capture the visible color of objects with three channels. In contrast, a polarization sensor can capture the polarization state of light with more than three channels, preventing it from utilizing the pretrained weights directly. The second challenge comes from the finetuning cost, which is increasing rapidly along with the quick growth of the model size of foundation models. To this end, a systematical analysis for applying different parameter-efficient finetuning strategies to cross-sensor transfer learning can be beneficial.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Researchers have attempted to explore cross-modal transfer learning in different modalities but most works focus on transferring a pretrained modality to another specific modality, including from language to visionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib27" title="">27</a>]</cite> or protein sequencesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib28" title="">28</a>]</cite>, from natural images to medical imagingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib29" title="">29</a>]</cite>. Few literatures have studied how to design a general cross-modal transfer framework for different image modalities. For example, Lu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite> proposes a framework FPT for transferring pretrained transformers to different inputs. Most recently, Shen et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib30" title="">30</a>]</cite> propose a general cross-modal transfer learning framework for diverse modalities, including language, vision, tabular, <math alttext="etc" class="ltx_Math" display="inline" id="Sx1.p4.1.m1.1"><semantics id="Sx1.p4.1.m1.1a"><mrow id="Sx1.p4.1.m1.1.1" xref="Sx1.p4.1.m1.1.1.cmml"><mi id="Sx1.p4.1.m1.1.1.2" xref="Sx1.p4.1.m1.1.1.2.cmml">e</mi><mo id="Sx1.p4.1.m1.1.1.1" xref="Sx1.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx1.p4.1.m1.1.1.3" xref="Sx1.p4.1.m1.1.1.3.cmml">t</mi><mo id="Sx1.p4.1.m1.1.1.1a" xref="Sx1.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx1.p4.1.m1.1.1.4" xref="Sx1.p4.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p4.1.m1.1b"><apply id="Sx1.p4.1.m1.1.1.cmml" xref="Sx1.p4.1.m1.1.1"><times id="Sx1.p4.1.m1.1.1.1.cmml" xref="Sx1.p4.1.m1.1.1.1"></times><ci id="Sx1.p4.1.m1.1.1.2.cmml" xref="Sx1.p4.1.m1.1.1.2">ğ‘’</ci><ci id="Sx1.p4.1.m1.1.1.3.cmml" xref="Sx1.p4.1.m1.1.1.3">ğ‘¡</ci><ci id="Sx1.p4.1.m1.1.1.4.cmml" xref="Sx1.p4.1.m1.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p4.1.m1.1c">etc</annotation><annotation encoding="application/x-llamapun" id="Sx1.p4.1.m1.1d">italic_e italic_t italic_c</annotation></semantics></math>. However, they do not carefully handle the modality misalignmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite> or require large computational costÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>]</cite> or extra dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib30" title="">30</a>]</cite>. Besides, they do not take into account finetuning strategies, which is quite important in practice. In contrast, we study how to transfer the vision foundation model comprehensively, including handling modality misalignment and analyzing fine-tuning strategies.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">To investigate this problem, we introduce SimMAT: a simple framework for modality-agnostic transfer learning from a vision foundation model to any imaging modality. SimMAT consists of a modality-agnostic layer and a pretrained vision foundation model. First, SimMAT is designed to accept any imaging modality as input for transfer learning. It does not require domain-specific knowledge, such as the relationship between the modality and natural images. With extensive exploratory experiments and analysis, we propose a simple and effective strategy to align the target modality and the pretrained vision modality in SimMAT. Secondly, we provide a comprehensive empirical study of parameter-efficient fine-tuning (PEFT) strategies on cross-modal transfer learning. Specifically, we compare the best performance of different strategies, including LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite>, MLP AdapterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib32" title="">32</a>]</cite>, prompt tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib33" title="">33</a>]</cite>, and full-finetuning. Results confirm that the performance of parameter-efficient finetuning could be better than full-finetuning when the training data is limited, which is consistent with the observations in in-modality finetuning.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">In this paper, we focus on applying SimMAT to a recent vision foundation model Segment Anything Model (SAM)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>]</cite> so that it can be used for segmentation in different image modalities.
SAM is trained on 11 million images for a fundamental image segmentation task. To enable a fair comparison to study the transferring of SAM on novel modality, we build a dedicatedly designed segmentation benchmark that consists of datasets captured by various types of sensors, including polarization sensors, depth sensors, thermal sensors, and other types of sensors.</p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">Extensive results demonstrate that SimMAT can achieve significant performance improvement across image modalities compared with models that are trained on specific modalities only. We find that SimMAT does improve the performance of other modalities despite these sensors capturing different physical properties in different representations. We hope our SimMAT can serve as a flexible and solid tool for transferring vision foundation models to other image modalities in different areas. Besides, we believe our findings can provide insights to explore the possibility of building a foundation model that processes any sensor modality for any task.</p>
</div>
</section>
<section class="ltx_section" id="Sx2" lang="en">
<h2 class="ltx_title ltx_title_section">Results</h2>
<section class="ltx_subsection" id="Sx2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Framework overview</h3>
<div class="ltx_para" id="Sx2.SSx1.p1">
<p class="ltx_p" id="Sx2.SSx1.p1.6">To explore the transferability of vision foundation models to other image modalities, we introduce <span class="ltx_text ltx_font_typewriter" id="Sx2.SSx1.p1.6.1">SimMAT</span>, a modality-agnostic transfer learning method, illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F3" title="Figure 3 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">3</span></a>. SimMAT consists of a MAT layer <math alttext="m" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.1.m1.1"><semantics id="Sx2.SSx1.p1.1.m1.1a"><mi id="Sx2.SSx1.p1.1.m1.1.1" xref="Sx2.SSx1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.1.m1.1b"><ci id="Sx2.SSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.1.m1.1d">italic_m</annotation></semantics></math> and a foundation model <math alttext="f" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.2.m2.1"><semantics id="Sx2.SSx1.p1.2.m2.1a"><mi id="Sx2.SSx1.p1.2.m2.1.1" xref="Sx2.SSx1.p1.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.2.m2.1b"><ci id="Sx2.SSx1.p1.2.m2.1.1.cmml" xref="Sx2.SSx1.p1.2.m2.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.2.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.2.m2.1d">italic_f</annotation></semantics></math>. SimMAT is designed to accept any type of image modality as input for transfer learning. It does not require domain-specific knowledge, such as the relationship between the modality and natural images. We select SAM as the representative vision foundation model <math alttext="f" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.3.m3.1"><semantics id="Sx2.SSx1.p1.3.m3.1a"><mi id="Sx2.SSx1.p1.3.m3.1.1" xref="Sx2.SSx1.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.3.m3.1b"><ci id="Sx2.SSx1.p1.3.m3.1.1.cmml" xref="Sx2.SSx1.p1.3.m3.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.3.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.3.m3.1d">italic_f</annotation></semantics></math>. The original module in SAM receives a three-dimensional RGB image to an embedding with <math alttext="d" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.4.m4.1"><semantics id="Sx2.SSx1.p1.4.m4.1a"><mi id="Sx2.SSx1.p1.4.m4.1.1" xref="Sx2.SSx1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.4.m4.1b"><ci id="Sx2.SSx1.p1.4.m4.1.1.cmml" xref="Sx2.SSx1.p1.4.m4.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.4.m4.1d">italic_d</annotation></semantics></math> dimensions. Given an input <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.5.m5.1"><semantics id="Sx2.SSx1.p1.5.m5.1a"><mi id="Sx2.SSx1.p1.5.m5.1.1" xref="Sx2.SSx1.p1.5.m5.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.5.m5.1b"><ci id="Sx2.SSx1.p1.5.m5.1.1.cmml" xref="Sx2.SSx1.p1.5.m5.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.5.m5.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.5.m5.1d">bold_x</annotation></semantics></math>, the output <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.6.m6.1"><semantics id="Sx2.SSx1.p1.6.m6.1a"><mi id="Sx2.SSx1.p1.6.m6.1.1" xref="Sx2.SSx1.p1.6.m6.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.6.m6.1b"><ci id="Sx2.SSx1.p1.6.m6.1.1.cmml" xref="Sx2.SSx1.p1.6.m6.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.6.m6.1c">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.6.m6.1d">bold_y</annotation></semantics></math> is obtained by:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
<tbody id="Sx2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{y}=f(\mathbf{e})=f(m(\mathbf{x}))," class="ltx_Math" display="inline" id="Sx2.E1.m1.3"><semantics id="Sx2.E1.m1.3a"><mrow id="Sx2.E1.m1.3.3.1" xref="Sx2.E1.m1.3.3.1.1.cmml"><mrow id="Sx2.E1.m1.3.3.1.1" xref="Sx2.E1.m1.3.3.1.1.cmml"><mi id="Sx2.E1.m1.3.3.1.1.3" xref="Sx2.E1.m1.3.3.1.1.3.cmml">ğ²</mi><mo id="Sx2.E1.m1.3.3.1.1.4" xref="Sx2.E1.m1.3.3.1.1.4.cmml">=</mo><mrow id="Sx2.E1.m1.3.3.1.1.5" xref="Sx2.E1.m1.3.3.1.1.5.cmml"><mi id="Sx2.E1.m1.3.3.1.1.5.2" xref="Sx2.E1.m1.3.3.1.1.5.2.cmml">f</mi><mo id="Sx2.E1.m1.3.3.1.1.5.1" xref="Sx2.E1.m1.3.3.1.1.5.1.cmml">â¢</mo><mrow id="Sx2.E1.m1.3.3.1.1.5.3.2" xref="Sx2.E1.m1.3.3.1.1.5.cmml"><mo id="Sx2.E1.m1.3.3.1.1.5.3.2.1" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.5.cmml">(</mo><mi id="Sx2.E1.m1.1.1" xref="Sx2.E1.m1.1.1.cmml">ğ</mi><mo id="Sx2.E1.m1.3.3.1.1.5.3.2.2" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.5.cmml">)</mo></mrow></mrow><mo id="Sx2.E1.m1.3.3.1.1.6" xref="Sx2.E1.m1.3.3.1.1.6.cmml">=</mo><mrow id="Sx2.E1.m1.3.3.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.cmml"><mi id="Sx2.E1.m1.3.3.1.1.1.3" xref="Sx2.E1.m1.3.3.1.1.1.3.cmml">f</mi><mo id="Sx2.E1.m1.3.3.1.1.1.2" xref="Sx2.E1.m1.3.3.1.1.1.2.cmml">â¢</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="Sx2.E1.m1.3.3.1.1.1.1.1.1.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">m</mi><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.1.3.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="Sx2.E1.m1.2.2" xref="Sx2.E1.m1.2.2.cmml">ğ±</mi><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.1.3.2.2" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="Sx2.E1.m1.3.3.1.2" xref="Sx2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E1.m1.3b"><apply id="Sx2.E1.m1.3.3.1.1.cmml" xref="Sx2.E1.m1.3.3.1"><and id="Sx2.E1.m1.3.3.1.1a.cmml" xref="Sx2.E1.m1.3.3.1"></and><apply id="Sx2.E1.m1.3.3.1.1b.cmml" xref="Sx2.E1.m1.3.3.1"><eq id="Sx2.E1.m1.3.3.1.1.4.cmml" xref="Sx2.E1.m1.3.3.1.1.4"></eq><ci id="Sx2.E1.m1.3.3.1.1.3.cmml" xref="Sx2.E1.m1.3.3.1.1.3">ğ²</ci><apply id="Sx2.E1.m1.3.3.1.1.5.cmml" xref="Sx2.E1.m1.3.3.1.1.5"><times id="Sx2.E1.m1.3.3.1.1.5.1.cmml" xref="Sx2.E1.m1.3.3.1.1.5.1"></times><ci id="Sx2.E1.m1.3.3.1.1.5.2.cmml" xref="Sx2.E1.m1.3.3.1.1.5.2">ğ‘“</ci><ci id="Sx2.E1.m1.1.1.cmml" xref="Sx2.E1.m1.1.1">ğ</ci></apply></apply><apply id="Sx2.E1.m1.3.3.1.1c.cmml" xref="Sx2.E1.m1.3.3.1"><eq id="Sx2.E1.m1.3.3.1.1.6.cmml" xref="Sx2.E1.m1.3.3.1.1.6"></eq><share href="https://arxiv.org/html/2409.08083v1#Sx2.E1.m1.3.3.1.1.5.cmml" id="Sx2.E1.m1.3.3.1.1d.cmml" xref="Sx2.E1.m1.3.3.1"></share><apply id="Sx2.E1.m1.3.3.1.1.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1"><times id="Sx2.E1.m1.3.3.1.1.1.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.2"></times><ci id="Sx2.E1.m1.3.3.1.1.1.3.cmml" xref="Sx2.E1.m1.3.3.1.1.1.3">ğ‘“</ci><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1"><times id="Sx2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.1"></times><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.2">ğ‘š</ci><ci id="Sx2.E1.m1.2.2.cmml" xref="Sx2.E1.m1.2.2">ğ±</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E1.m1.3c">\displaystyle\mathbf{y}=f(\mathbf{e})=f(m(\mathbf{x})),</annotation><annotation encoding="application/x-llamapun" id="Sx2.E1.m1.3d">bold_y = italic_f ( bold_e ) = italic_f ( italic_m ( bold_x ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SSx1.p1.12">where <math alttext="\mathbf{e}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.7.m1.1"><semantics id="Sx2.SSx1.p1.7.m1.1a"><mi id="Sx2.SSx1.p1.7.m1.1.1" xref="Sx2.SSx1.p1.7.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.7.m1.1b"><ci id="Sx2.SSx1.p1.7.m1.1.1.cmml" xref="Sx2.SSx1.p1.7.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.7.m1.1c">\mathbf{e}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.7.m1.1d">bold_e</annotation></semantics></math> is the output embedding of our MAT layer. The MAT layer <math alttext="m" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.8.m2.1"><semantics id="Sx2.SSx1.p1.8.m2.1a"><mi id="Sx2.SSx1.p1.8.m2.1.1" xref="Sx2.SSx1.p1.8.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.8.m2.1b"><ci id="Sx2.SSx1.p1.8.m2.1.1.cmml" xref="Sx2.SSx1.p1.8.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.8.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.8.m2.1d">italic_m</annotation></semantics></math> transfers a new input <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.9.m3.1"><semantics id="Sx2.SSx1.p1.9.m3.1a"><mi id="Sx2.SSx1.p1.9.m3.1.1" xref="Sx2.SSx1.p1.9.m3.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.9.m3.1b"><ci id="Sx2.SSx1.p1.9.m3.1.1.cmml" xref="Sx2.SSx1.p1.9.m3.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.9.m3.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.9.m3.1d">bold_x</annotation></semantics></math> with modality dimension <math alttext="C" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.10.m4.1"><semantics id="Sx2.SSx1.p1.10.m4.1a"><mi id="Sx2.SSx1.p1.10.m4.1.1" xref="Sx2.SSx1.p1.10.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.10.m4.1b"><ci id="Sx2.SSx1.p1.10.m4.1.1.cmml" xref="Sx2.SSx1.p1.10.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.10.m4.1c">C</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.10.m4.1d">italic_C</annotation></semantics></math> to a modality embedding with the original vision embedding dimension <math alttext="d" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.11.m5.1"><semantics id="Sx2.SSx1.p1.11.m5.1a"><mi id="Sx2.SSx1.p1.11.m5.1.1" xref="Sx2.SSx1.p1.11.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.11.m5.1b"><ci id="Sx2.SSx1.p1.11.m5.1.1.cmml" xref="Sx2.SSx1.p1.11.m5.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.11.m5.1c">d</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.11.m5.1d">italic_d</annotation></semantics></math>. In our experiments, we observe that different designs of MAT layers lead to significantly different model accuracy. For the foundation model <math alttext="f" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.12.m6.1"><semantics id="Sx2.SSx1.p1.12.m6.1a"><mi id="Sx2.SSx1.p1.12.m6.1.1" xref="Sx2.SSx1.p1.12.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.12.m6.1b"><ci id="Sx2.SSx1.p1.12.m6.1.1.cmml" xref="Sx2.SSx1.p1.12.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.12.m6.1c">f</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.12.m6.1d">italic_f</annotation></semantics></math>, we apply different finetuning strategies to study this problem, including parameter-efficient finetuning and full-finetuning.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Dataset construction</h3>
<div class="ltx_para" id="Sx2.SSx2.p1">
<p class="ltx_p" id="Sx2.SSx2.p1.1">This section presents the details of our dataset. Since there is no existing benchmark that covers different types of modalities for the promotable segmentation task of SAM, we construct a new benchmark named Any Image Modality Segmentation (AIMS) benchmark. Specifically, we choose five representative sensors in different fields and their corresponding images as follows:</p>
<ul class="ltx_itemize" id="Sx2.I1">
<li class="ltx_item" id="Sx2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx2.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i1.p1.1.1">Polarization Images</span> capture the polarization state of the light. The polarization image is a nine-channel image. The polarization state is closely related to the shape and materials of objects and can be used for challenging tasks for conventional intensity cameras, such as camouflaged object detection, transparent object segment, reflection removal, <math alttext="etc" class="ltx_Math" display="inline" id="Sx2.I1.i1.p1.1.m1.1"><semantics id="Sx2.I1.i1.p1.1.m1.1a"><mrow id="Sx2.I1.i1.p1.1.m1.1.1" xref="Sx2.I1.i1.p1.1.m1.1.1.cmml"><mi id="Sx2.I1.i1.p1.1.m1.1.1.2" xref="Sx2.I1.i1.p1.1.m1.1.1.2.cmml">e</mi><mo id="Sx2.I1.i1.p1.1.m1.1.1.1" xref="Sx2.I1.i1.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx2.I1.i1.p1.1.m1.1.1.3" xref="Sx2.I1.i1.p1.1.m1.1.1.3.cmml">t</mi><mo id="Sx2.I1.i1.p1.1.m1.1.1.1a" xref="Sx2.I1.i1.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx2.I1.i1.p1.1.m1.1.1.4" xref="Sx2.I1.i1.p1.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx2.I1.i1.p1.1.m1.1b"><apply id="Sx2.I1.i1.p1.1.m1.1.1.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1"><times id="Sx2.I1.i1.p1.1.m1.1.1.1.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1.1"></times><ci id="Sx2.I1.i1.p1.1.m1.1.1.2.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1.2">ğ‘’</ci><ci id="Sx2.I1.i1.p1.1.m1.1.1.3.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1.3">ğ‘¡</ci><ci id="Sx2.I1.i1.p1.1.m1.1.1.4.cmml" xref="Sx2.I1.i1.p1.1.m1.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.I1.i1.p1.1.m1.1c">etc</annotation><annotation encoding="application/x-llamapun" id="Sx2.I1.i1.p1.1.m1.1d">italic_e italic_t italic_c</annotation></semantics></math>. We adopt RGBP-GlassÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib34" title="">34</a>]</cite> and ZJU-RGBPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib35" title="">35</a>]</cite> in our benchmark. RGBP-Glass contains 3207 and 1304 images for training and evaluation, respectively. ZJU-RGBP includes 344 training images and 50 validation images.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx2.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i2.p1.1.1">Depth Images</span> capture scene geometry, which is commonly used in diverse applications, including robotics, autonomous driving, and computational photography. The depth image captured from the camera is a one-channel image. In our benchmark, we adopt the public NYUv2 datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib36" title="">36</a>]</cite>, which contains 1449 RGBD samples covering 40 categories.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx2.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i3.p1.1.1">HHA Images</span> are processed features obtained from depth images, which we analyze as a new modalityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib37" title="">37</a>]</cite>. The HHA encoding is a method for representing depth images in a way that captures additional geometric information beyond just depth. HHA uses three channels at each pixel to encode the horizontal disparity, the height above ground, and the angle with gravity.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx2.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i4.p1.1.1">Thermal Images</span> capture thermal radiation coming from scenes or environments despite the weather and illumination conditions,
which are commonly in various areas. The thermal images are usually one-channel. In our benchmark, we adopt the public Thermal-based glass segmentation datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib38" title="">38</a>]</cite>, which contains 5551 images with segmentation labels.</p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="Sx2.I1.i5.p1">
<p class="ltx_p" id="Sx2.I1.i5.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i5.p1.1.1">NIR Images</span> can capture the light in near-infrared frequency, which are commonly used in low-light vision. The NIR (Near-Infrared) images are usually one-channel. We adopt the IVRG-NIR datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib39" title="">39</a>]</cite> in our benchmark, which consists of 477 NIR images and their ground truth.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx2.SSx2.p2">
<p class="ltx_p" id="Sx2.SSx2.p2.1">We select these modalities as they capture significantly different properties of scenes compared with conventional intensity cameras, and they are quite different from each other. Besides, there are publicly available segmentation datasets for these modalities, and the effectiveness of the novel modality has been proven in previous works. Compared with the training data of RGB-based SAM, which contains 11 million images and more than 1 billion masks, most datasets have a limited number of training images and masks. The segmentation labels of SAM are instance-level segmentation. However, for some segmentation datasets, only semantic labels are provided, which is different from the requirement of the SAM training setting. Hence, post-processing is required to convert the ground truth format to the SAM training setting. Details are presented in the Supplementary Information.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx3">
<h3 class="ltx_title ltx_title_subsection">Performance evaluation</h3>
<div class="ltx_para" id="Sx2.SSx3.p1">
<p class="ltx_p" id="Sx2.SSx3.p1.2">We evaluate SimMAT for segmentation transfer across modalities on our constructed dataset. Following the protocol of the interactive setting adopted in SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>]</cite>, the center point of an instance is used as the default click prompt fed into the network. We adopt ViT-Base as the image encoder backbone of the pretrained SAM for all experiments. As the best learning rate can be different for each model, we sweep the learning rates and report the best performance for each model for a fair comparison. For all evaluated modalities, we only require the number of channels <math alttext="C" class="ltx_Math" display="inline" id="Sx2.SSx3.p1.1.m1.1"><semantics id="Sx2.SSx3.p1.1.m1.1a"><mi id="Sx2.SSx3.p1.1.m1.1.1" xref="Sx2.SSx3.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.p1.1.m1.1b"><ci id="Sx2.SSx3.p1.1.m1.1.1.cmml" xref="Sx2.SSx3.p1.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.p1.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx3.p1.1.m1.1d">italic_C</annotation></semantics></math> and then build our SimMAT for end-to-end training. Before exploring how to perform transfer learning, we first implement baseline approaches as references, <math alttext="i.e." class="ltx_Math" display="inline" id="Sx2.SSx3.p1.2.m2.3"><semantics id="Sx2.SSx3.p1.2.m2.3a"><mrow id="Sx2.SSx3.p1.2.m2.3.3.1"><mrow id="Sx2.SSx3.p1.2.m2.3.3.1.1.2" xref="Sx2.SSx3.p1.2.m2.3.3.1.1.1.cmml"><mi id="Sx2.SSx3.p1.2.m2.1.1" xref="Sx2.SSx3.p1.2.m2.1.1.cmml">i</mi><mo id="Sx2.SSx3.p1.2.m2.3.3.1.1.2.1" lspace="0em" rspace="0.167em" xref="Sx2.SSx3.p1.2.m2.3.3.1.1.1a.cmml">.</mo><mi id="Sx2.SSx3.p1.2.m2.2.2" xref="Sx2.SSx3.p1.2.m2.2.2.cmml">e</mi></mrow><mo id="Sx2.SSx3.p1.2.m2.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx3.p1.2.m2.3b"><apply id="Sx2.SSx3.p1.2.m2.3.3.1.1.1.cmml" xref="Sx2.SSx3.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx2.SSx3.p1.2.m2.3.3.1.1.1a.cmml" xref="Sx2.SSx3.p1.2.m2.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="Sx2.SSx3.p1.2.m2.1.1.cmml" xref="Sx2.SSx3.p1.2.m2.1.1">ğ‘–</ci><ci id="Sx2.SSx3.p1.2.m2.2.2.cmml" xref="Sx2.SSx3.p1.2.m2.2.2">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx3.p1.2.m2.3c">i.e.</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx3.p1.2.m2.3d">italic_i . italic_e .</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="Sx2.SSx3.p1.2.1">training from scratch</span>. The most naive baseline is to inherit the SAM architecture without pretrained weights and train the network only with the new modality data. While we understand it is quite challenging to train a Transformer model effectively with a small amount of data, we adopt this method as a baseline to keep experimental factors the same for reference. This baseline approach only achieves a low 22.15% average mIoU on our benchmark.</p>
</div>
<div class="ltx_para" id="Sx2.SSx3.p2">
<p class="ltx_p" id="Sx2.SSx3.p2.1">Training with SimMAT can achieve significantly better performance compared with training the models on specific data from scratch on our evaluated dataset. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F4" title="Figure 4 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">4</span></a>(a) presents the results of our approach on different modalities. The results of training from scratch for all modalities are poor, which only gets 22.15% mIoU on different modalities. As a comparison, training the model with SimMAT achieves 53.88% mIoU, which is significantly better than training from scratch. This phenomenon is according to our expectations as the transformer is data-hungry and requires a large number of data for training. Since the dataset size for these sensors is usually small, they cannot train a good foundation model. This significant improvement demonstrates the potential and importance of cross-modal transfer learning from vision foundation models to other modalities in different fields.
We further analyze the visual results to better understand the phenomenon in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F2" title="Figure 2 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">2</span></a>. The perceptual performance of training from scratch is poor, where the mask is inaccurate. As a comparison, training the model with SimMAT can obtain accurate and sharp segmentation results. We observe similar phenomena in all evaluated modalities, which demonstrates the effectiveness of our proposed SimMAT.</p>
</div>
<div class="ltx_para" id="Sx2.SSx3.p3">
<p class="ltx_p" id="Sx2.SSx3.p3.1">We further study a data representation that combines natural images and a paired new modality. This representation has been studied in previous methods as a multi-modality representationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib33" title="">33</a>]</cite>.
However, in our setting, while we use both natural images and new modality, we <span class="ltx_text ltx_font_italic" id="Sx2.SSx3.p3.1.1">assume</span> that we do not know the modality sequence: we do not adopt any RGB prior knowledge to process the RGB images individually so that it could be a special case of new modality named pseudo-new modality. Note that while this prior knowledge is easy to obtain, we just use these pseudo-new modalities to validate the effectiveness of our approach. Specifically, we shuffle the channels to avoid using domain knowledge. Since we have access to RGB images in this experiment, we provide an additional reference method of directly inputting the RGB image to SAM, which we denote as SAM zero-shot. As we can see in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F4" title="Figure 4 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">4</span></a>(b), SAM zero-shot achieves reasonable performance but is far from our approach. As a comparison, our SimMAT framework with different finetuning strategies achieves much better performance compared with baselines. More qualitative results are presented in the supplementary materials due to limited space.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx4">
<h3 class="ltx_title ltx_title_subsection">Controlled experiments of MAT</h3>
<div class="ltx_para" id="Sx2.SSx4.p1">
<p class="ltx_p" id="Sx2.SSx4.p1.1">Dimension misalignment is an inevitable challenge in cross-modal transfer learning: the dimension of target modality can differ from the dimension of pretrained models, e.g., from vision to languageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib40" title="">40</a>]</cite>, from natural images to medical imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>]</cite>. While there are some commonly adopted naive strategies, such as using linear layers to change the dimensions, handling dimension misalignment with satisfying performance is still an open problem to solve up to date. In this section, we provide extensive ablation experiments here to analyze the design of our MAT layer. We just changed the design of the MAT layer for all experiments. Due to limited computing resources, all experiments are conducted using polarization imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib34" title="">34</a>]</cite> that consist of unpolarized intensity images, angle of linear polarization images, and degree of linear polarization images as the target new modality <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="Sx2.SSx4.p1.1.m1.1"><semantics id="Sx2.SSx4.p1.1.m1.1a"><mi id="Sx2.SSx4.p1.1.m1.1.1" xref="Sx2.SSx4.p1.1.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.1.m1.1b"><ci id="Sx2.SSx4.p1.1.m1.1.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx4.p1.1.m1.1d">bold_x</annotation></semantics></math>. Polarization images have nine channels. We load the pretrained weights for the foundation model and train the whole model jointly. We adopt LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite> to train these models.</p>
</div>
<div class="ltx_para" id="Sx2.SSx4.p2">
<p class="ltx_p" id="Sx2.SSx4.p2.2">TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.T1" title="Table 1 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">1</span></a> shows different existing methods for solving the dimension misalignments in different tasks. While some representative works like ORCAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib30" title="">30</a>]</cite> and BLIP-2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib41" title="">41</a>]</cite> propose methods for aligning dimensions, they require source-target paired data (e.g., image-text pair), which are not available under our setting. Hence, their modules are not applicable. There are some direct dimension alignment strategies adopted in priors works. We apply these methods in our SimMAT framework but they cannot achieve satisfying performance. <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p2.2.1">(1) Randomly initialized MAT layer.</span> We start by replacing the original patch embedding with a randomly initialized projection layer. This strategy is quite direct and naive, which has been commonly used in prior worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite>. Compared to training from scratch, this implementation fully utilizes the pretrained vision model weights. As a result, mIoU is improved from 25.43% to 58.89% compared to training from scratch, validating the potential of modality-agnostic transfer learning. However, the performance is significantly worse than our proposed strategy achieving 72.69% mIoU. <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p2.2.2">(2) Inherited vision embedding with linear layer.</span> Another common strategy to align different dimensions is using a linear layer (<math alttext="i.e." class="ltx_Math" display="inline" id="Sx2.SSx4.p2.1.m1.3"><semantics id="Sx2.SSx4.p2.1.m1.3a"><mrow id="Sx2.SSx4.p2.1.m1.3.3.1"><mrow id="Sx2.SSx4.p2.1.m1.3.3.1.1.2" xref="Sx2.SSx4.p2.1.m1.3.3.1.1.1.cmml"><mi id="Sx2.SSx4.p2.1.m1.1.1" xref="Sx2.SSx4.p2.1.m1.1.1.cmml">i</mi><mo id="Sx2.SSx4.p2.1.m1.3.3.1.1.2.1" lspace="0em" rspace="0.167em" xref="Sx2.SSx4.p2.1.m1.3.3.1.1.1a.cmml">.</mo><mi id="Sx2.SSx4.p2.1.m1.2.2" xref="Sx2.SSx4.p2.1.m1.2.2.cmml">e</mi></mrow><mo id="Sx2.SSx4.p2.1.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p2.1.m1.3b"><apply id="Sx2.SSx4.p2.1.m1.3.3.1.1.1.cmml" xref="Sx2.SSx4.p2.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx2.SSx4.p2.1.m1.3.3.1.1.1a.cmml" xref="Sx2.SSx4.p2.1.m1.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="Sx2.SSx4.p2.1.m1.1.1.cmml" xref="Sx2.SSx4.p2.1.m1.1.1">ğ‘–</ci><ci id="Sx2.SSx4.p2.1.m1.2.2.cmml" xref="Sx2.SSx4.p2.1.m1.2.2">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p2.1.m1.3c">i.e.</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx4.p2.1.m1.3d">italic_i . italic_e .</annotation></semantics></math>, a fully connected layer). Many prior works adopt this strategy due to its simplicity and efficiency, including LLaVaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib40" title="">40</a>]</cite>. With this simple strategy, we improve the mIoU score from 58.89% to 63.96% compared with the randomly initialized MAT layer. However, the performance is still worse than our 72.69%. We believe it is because the transformation of the linear layer is too simple to align two different image modalities, preventing it from achieving satisfying performance. <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p2.2.3">(3) Transpose to batch dimension.</span> Another strategy is to transpose the feature dimension to the batch dimension and process them separately, such as the method used in MedicalSAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>]</cite>. Interestingly, we observe this implementation can achieve better performance than the prior two versions. Specifically, it gets 70.90% on our evaluated dataset, which is close to our results. However, it suffers from a practical resource problem: it is inefficient for some image modalities. Specifically, for single-channel images, the FLOPs of this method are similar to ours. However, it uses around 9<math alttext="\times" class="ltx_Math" display="inline" id="Sx2.SSx4.p2.2.m2.1"><semantics id="Sx2.SSx4.p2.2.m2.1a"><mo id="Sx2.SSx4.p2.2.m2.1.1" xref="Sx2.SSx4.p2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p2.2.m2.1b"><times id="Sx2.SSx4.p2.2.m2.1.1.cmml" xref="Sx2.SSx4.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx4.p2.2.m2.1d">Ã—</annotation></semantics></math> FLOPs compared with our MAT layer when using polarization images, which makes it quite challenging to train the models for many areas.</p>
</div>
<div class="ltx_para" id="Sx2.SSx4.p3">
<p class="ltx_p" id="Sx2.SSx4.p3.1">We present a simple yet effective MAT layer for aligning new modalities. As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.T1" title="Table 1 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">1</span></a>, our design achieves 72.69% mIoU, which is the best among different strategies. In addition, our MAT is also quite efficient. Different from these methods of building a complex module to bridge the modality gap, we find the two key factors to utilize the vision embedding layer for a new modality. First, the mapping from the novel modality to the pretrained RGB feature space is non-linear. Introducing a linear projection layer fails to achieve satisfactory performance due to limited mapping ability. Instead, SimMAT stacks convolutional layers with ReLU as an intermediate non-linear activation function. A similar conclusion is observed in contrastive learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib45" title="">45</a>]</cite>: replacing the linear layer with MLP as the projection head can achieve better performance. Second, the receptive field is important for novel imaging modalities. It helps capture the cues from neighbor regions and benefit the pixel to learn more rich context; such observation has been well studied and verified in the RGB modality, which inspired us to enlarge the receptive field of SimMAT by setting the convolutional kernel size. Specifically, we stack <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.1">n</span> convolutional layers with <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.2">k</span> kernel size and dimension <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.3">d</span>, we set {<span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.4">n</span>, <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.5">k</span>, <span class="ltx_text ltx_font_italic" id="Sx2.SSx4.p3.1.6">d</span>} to {2, 3, 64} in default since it achieves the best performance, more detailed experiment results are present in supplemental materials.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx5">
<h3 class="ltx_title ltx_title_subsection">Empirical analysis of finetuning strategies</h3>
<div class="ltx_para" id="Sx2.SSx5.p1">
<p class="ltx_p" id="Sx2.SSx5.p1.1">In this part, we conduct empirical experiments to explore which strategy works better for modality-agnostic transfer learning. After handling the dimension misalignment with our designed MAT layer, SimMAT can adopt existing finetuning strategies easily like in-modality transfer learning. However, it is not validated systematically on many sensors in different areas, such as polarizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib16" title="">16</a>]</cite>. We mainly explore two commonly used finetuning styles here: (1) Full Finetuning (FFT): full finetuning is commonly used as it usually achieves satisfying performance easily. Following this setting, we make both the MAT layer and pretrained backbone learnable. (2) Parameter-efficient Finetuning (PEFT): as the parameters of foundation models are usually very large, full finetuning a model might be extremely resource-hungry. PEFT strategies usually fix the original parameters and introduce a small amount of learnable new parameters. In our experiments, we select representative parameter-efficient finetuning methods, including LoRA, MLP Adapter, prompt tuning, <math alttext="etc" class="ltx_Math" display="inline" id="Sx2.SSx5.p1.1.m1.1"><semantics id="Sx2.SSx5.p1.1.m1.1a"><mrow id="Sx2.SSx5.p1.1.m1.1.1" xref="Sx2.SSx5.p1.1.m1.1.1.cmml"><mi id="Sx2.SSx5.p1.1.m1.1.1.2" xref="Sx2.SSx5.p1.1.m1.1.1.2.cmml">e</mi><mo id="Sx2.SSx5.p1.1.m1.1.1.1" xref="Sx2.SSx5.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx2.SSx5.p1.1.m1.1.1.3" xref="Sx2.SSx5.p1.1.m1.1.1.3.cmml">t</mi><mo id="Sx2.SSx5.p1.1.m1.1.1.1a" xref="Sx2.SSx5.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx2.SSx5.p1.1.m1.1.1.4" xref="Sx2.SSx5.p1.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx5.p1.1.m1.1b"><apply id="Sx2.SSx5.p1.1.m1.1.1.cmml" xref="Sx2.SSx5.p1.1.m1.1.1"><times id="Sx2.SSx5.p1.1.m1.1.1.1.cmml" xref="Sx2.SSx5.p1.1.m1.1.1.1"></times><ci id="Sx2.SSx5.p1.1.m1.1.1.2.cmml" xref="Sx2.SSx5.p1.1.m1.1.1.2">ğ‘’</ci><ci id="Sx2.SSx5.p1.1.m1.1.1.3.cmml" xref="Sx2.SSx5.p1.1.m1.1.1.3">ğ‘¡</ci><ci id="Sx2.SSx5.p1.1.m1.1.1.4.cmml" xref="Sx2.SSx5.p1.1.m1.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx5.p1.1.m1.1c">etc</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx5.p1.1.m1.1d">italic_e italic_t italic_c</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="Sx2.SSx5.p2">
<p class="ltx_p" id="Sx2.SSx5.p2.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F4" title="Figure 4 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">4</span></a>(b) presents the results of different finetuning strategies. We sweep the learning rate for each finetuning method and choose the best result for comparison. All finetuning strategies can improve the segmentation performance compared with training from scratch. LoRA and Adapter can achieve similar performance with full finetuning while they only use much fewer trainable parameters. Besides, while prompt tuning can improve the performance compared with training the models from scratch, the results fall below that of the other two PEFT methods despite having a close number of trainable parameters, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F4" title="Figure 4 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">4</span></a>(b). We believe this is attributed to the initial noise brought by the prompts embedding. It fails to find an initialization that ensures prompt embeddings do not disturb the model output at the first forward pass. In comparison, the effects of both LoRA and MLP adapter on the model can be initialized as zero.</p>
</div>
<div class="ltx_para" id="Sx2.SSx5.p3">
<p class="ltx_p" id="Sx2.SSx5.p3.1">We further study the effect of learning rate for different finetuning strategies. Prior worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib46" title="">46</a>]</cite> noticed that different tuning strategy holds different best learning rates, we observe consistent results as presented in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F5" title="Figure 5 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">5</span></a>(a).
Full finetuning achieves a peak performance 69.19% mIoU at lr=1e-5, while parameter-efficient finetuning achieves the best 72.69% mIoU at lr=3e-4.
We suspect the reason is the number of trainable parameters. Full finetuning makes all parameters learnable; a small learning rate prevents the model from deviating far away from the pretrained weights.
While LoRA or MLP Adapter with only 4% trainable parameters demands a larger learning rate for efficient learning.</p>
</div>
<div class="ltx_para" id="Sx2.SSx5.p4">
<p class="ltx_p" id="Sx2.SSx5.p4.1">To study the relationship between the number of finetuned images and the pretrained model, we provide a controlled experiment. We split the training set randomly according to different ratios. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F5" title="Figure 5 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">5</span></a>(b) shows the results. We notice that using RGB-based pretrained SAM can significantly improve the performance on different image modalities, especially when the training images of specific modalities are limited.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3" lang="en">
<h2 class="ltx_title ltx_title_section">Discussion</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Foundation models (Large Models) have revolutionized artificial intelligence areas, such as ChatGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib47" title="">47</a>]</cite> in Natural Language Processing and SAM (Segment Anything Model) in Computer Vision. Driven by the availability of large-scale image data, several foundation models have recently been proposedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib50" title="">50</a>]</cite> for vision tasks including image recognition, segmentation, conditional generation, etc. As a result, numerous downstream tasks can achieve impressive performance.
Nevertheless, except for conventional cameras, the available data of many image sensors is not large enough, preventing applications in different areas from benefitting from the significant progress of foundation models. Transferring the ability of vision foundation models to new data-limited image modalities is promising, but this line of work has not been fully explored or studied.</p>
</div>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1">In this work, we confirm the potential of modality-agnostic cross-modal transfer learning from vision foundation models to other image modalities beyond natural images. To this end, we introduce a training paradigm SimMAT to study this problem. We conduct extensive exploratory experiments to propose a good MAT layer for receiving different types of new modalities. We explore different finetuning strategies and report our observations. Based on these experiments, we validate the transfer performance of our proposed SimMAT through a vision foundation model SAM on a variety of sensors. The significant margins achieved by SimMAT suggest that the generic cross-modal transfer learning is still underexplored. We envision SimMAT to be useful for other vision foundation models and other unevaluated modalities that are not studied in this work.</p>
</div>
<div class="ltx_para" id="Sx3.p3">
<p class="ltx_p" id="Sx3.p3.1">While achieving substantial improvements, we believe the upper bound of modality-agnostic transfer learning is not achieved with our method, implying rich future research in this direction. Possible research directions are described as follows: <span class="ltx_text ltx_font_bold" id="Sx3.p3.1.1">(1) Domain-specific knowledge.</span> We argue using domain-specific knowledge is always a good choice for improving performance, but it does not conflict with our modality-agnostic SimMAT. Designing domain-specific strategies can cost more time and effort at the beginning. In contrast, SimMAT can be applied to validate the effectiveness of a novel sensor for specific sensors much more efficiently. With the positive validation from SimMAT, researchers can then focus on combining domain-specific knowledge. <span class="ltx_text ltx_font_bold" id="Sx3.p3.1.2">(2) Why not collect more data?</span> Collecting more data is one of the best ways to train a stronger foundation modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>]</cite>. We believe this argument works for most image modalities as well. Alternative methods include creating more synthetic data for training, but it is non-trivial to synthesize other modalities. However, even if data would be free to collect, we argue our proposed SimMAT can be used to validate the effectiveness of a sensor efficiently with a low requirement for the training data and training cost.
<span class="ltx_text ltx_font_bold" id="Sx3.p3.1.3">(3) Zero-shot modality transfer?</span> While many existing foundation models demonstrate impressive zero-shot performance on new tasks, it is still extremely challenging for them to achieve satisfying zero-shot performance on many new modalities. We believe training a MAT layer is useful and necessary at this stage since the features are quite different between natural images and other sensors.</p>
</div>
</section>
<section class="ltx_section" id="Sx4" lang="en">
<h2 class="ltx_title ltx_title_section">Method</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">In this section, we present the setting of our methods, including the details of the data, the foundation models, and the training protocols.</p>
</div>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Related work</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">Unimodal transfer learning is commonly used in computer vision. Transfer learning first pretrains the model to learn prior knowledge and then fine-tunes it on another downstream task. It has shown to be effective in various areasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib52" title="">52</a>]</cite>. Most transfer learning is conducted on the same input modality. For example, the models are first trained on the ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib53" title="">53</a>]</cite> in a contrastive learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib56" title="">56</a>]</cite> or a masking inpainting wayÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib59" title="">59</a>]</cite> and then used for the downstream task with RGB image input. Besides, the pertaining model works well in other scenarios like predictions of RNA secondary structureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib60" title="">60</a>]</cite>, metal-organic frameworkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib62" title="">62</a>]</cite> and fault slipÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1">However, the modalities suffering from limited training data fail to perform pretrain-tuning paradigm. For example, modalities like polarization, structured lightÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib64" title="">64</a>]</cite>, and event cameraÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib65" title="">65</a>]</cite>, which have proven instrumental in 3D imaging and auto-driving, but there is no large scale data for pertaining. Cross-modal transfer learning is a potential way to solve this problem, which has been studied, but most research explores this problem in a modality-specific style for a specific pair of modalities. For example, Radhakrishnan et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib66" title="">66</a>]</cite> study transfer learning on image classification and virtual drug screening applications. Many worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib67" title="">67</a>]</cite> study the transferability from language models to vision. Zhang et al. employ the vision-language foundation model for biomedical tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib68" title="">68</a>]</cite>.
Vinod et al. Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib28" title="">28</a>]</cite> attempts to apply language models to protein sequencesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib28" title="">28</a>]</cite>. Wu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>]</cite> and Ma et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib29" title="">29</a>]</cite> attempt to transfer the vision segmentation model to medical imaging.
Domain adaptation aims to transfer source domain knowledge to the target domain. Numerous methods are proposed to reduce the cross-domain discrepancy through adversarial learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib71" title="">71</a>]</cite>, or introduce pseudo labels with self-supervised learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib74" title="">74</a>]</cite>.
Different from these methods considering the domain gap in RGB images, we try to alleviate the modality gap between RGB images and other image modalities, it is more challenging since the physical forward model changes between the image channels.
While heterogeneous domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib76" title="">76</a>]</cite> extensively discusses the feature space change, they usually assume the source training data is available.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p3">
<p class="ltx_p" id="Sx4.SSx1.p3.1">Few worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite> study a general and modality-agnostic cross-modal transferring workflow of transferring the knowledge from pretrained data to downstream tasks. Lu et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite> proposes a framework FPT for transferring pretrained transformers to different inputs. However, they only study pretrained language models in a frozen way. Most recently, Shen et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib30" title="">30</a>]</cite> propose a general cross-modal transfer learning framework for diverse modalities, including language, vision, tabular, <math alttext="etc" class="ltx_Math" display="inline" id="Sx4.SSx1.p3.1.m1.1"><semantics id="Sx4.SSx1.p3.1.m1.1a"><mrow id="Sx4.SSx1.p3.1.m1.1.1" xref="Sx4.SSx1.p3.1.m1.1.1.cmml"><mi id="Sx4.SSx1.p3.1.m1.1.1.2" xref="Sx4.SSx1.p3.1.m1.1.1.2.cmml">e</mi><mo id="Sx4.SSx1.p3.1.m1.1.1.1" xref="Sx4.SSx1.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx4.SSx1.p3.1.m1.1.1.3" xref="Sx4.SSx1.p3.1.m1.1.1.3.cmml">t</mi><mo id="Sx4.SSx1.p3.1.m1.1.1.1a" xref="Sx4.SSx1.p3.1.m1.1.1.1.cmml">â¢</mo><mi id="Sx4.SSx1.p3.1.m1.1.1.4" xref="Sx4.SSx1.p3.1.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p3.1.m1.1b"><apply id="Sx4.SSx1.p3.1.m1.1.1.cmml" xref="Sx4.SSx1.p3.1.m1.1.1"><times id="Sx4.SSx1.p3.1.m1.1.1.1.cmml" xref="Sx4.SSx1.p3.1.m1.1.1.1"></times><ci id="Sx4.SSx1.p3.1.m1.1.1.2.cmml" xref="Sx4.SSx1.p3.1.m1.1.1.2">ğ‘’</ci><ci id="Sx4.SSx1.p3.1.m1.1.1.3.cmml" xref="Sx4.SSx1.p3.1.m1.1.1.3">ğ‘¡</ci><ci id="Sx4.SSx1.p3.1.m1.1.1.4.cmml" xref="Sx4.SSx1.p3.1.m1.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p3.1.m1.1c">etc</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx1.p3.1.m1.1d">italic_e italic_t italic_c</annotation></semantics></math>. Nevertheless, they do not take into account finetuning strategies, which is quite important in practice.
In this paper, we are interested in exploring the transferability of the vision foundation model by investigating modality misalignment and finetuning strategies comprehensively.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p4">
<p class="ltx_p" id="Sx4.SSx1.p4.1">Parameter-efficient finetuning is also closely related to our research. Fully finetuning a large transformer model costs large GPU memory and training time. Parameter-efficient finetuning solves this problem by freezing the pretrained foundation model and only finetuning a small number of parameters, which has been shown to achieve comparable or even better performance than fully finetuning. It was first proposed in the natural language processing taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite> and then explored in the computer vision taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib77" title="">77</a>]</cite>. Visual prompt tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib46" title="">46</a>]</cite> adds some learnable tokens before each transformer block. Visual adapterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib77" title="">77</a>]</cite> inserts small multilayer perceptrons (MLPs) to the feed-forward network in a residual wayÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib78" title="">78</a>]</cite>. Prefix-tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib79" title="">79</a>]</cite> adds a few parameters before each multi-head attention layer. LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite> optimize the rank-decomposition matrices of layersâ€™ change and achieve zero inference latency.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p5">
<p class="ltx_p" id="Sx4.SSx1.p5.1">Unified architectures and learning algorithms for various data modalities have emerged recently. Designing a foundation modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib81" title="">81</a>]</cite> for various modalities becomes a goal for the community. The transformer architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib82" title="">82</a>]</cite> has been proven to be very effective in different domains, including natural languageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib83" title="">83</a>]</cite>, visionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib85" title="">85</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib86" title="">86</a>]</cite>, point cloudsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib89" title="">89</a>]</cite>, audioÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib90" title="">90</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib92" title="">92</a>]</cite>, and so on. PerceiverÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib93" title="">93</a>]</cite> is proposed for the general perception of various types of data modalities. PerceiverIOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib94" title="">94</a>]</cite> unify the input and output structures, which demonstrate the effectiveness of images and videos. Tamkin et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib95" title="">95</a>]</cite> construct a benchmark for domain-agnostic self-supervised learning algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib97" title="">97</a>]</cite>, including natural images, language, and sensors. Meta-transformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib98" title="">98</a>]</cite> uses a unified encoder for 12 modalities with each modality using a specific encoding way. Our SimMAT is also designed to handle different image modalities in a modality-agnostic formulation.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Framework architecture of SimMAT</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.7">We propose a simple yet effective modality-agnostic transfer learning framework to transfer the ability of vision foundation models to sensors in different applications, which we name SimMAT. As shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F1" title="Figure 1 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">1</span></a>, our framework is inspired by the attractive finetuning performance of in-modality transfer learning, from a pretrained task to different downstream tasks. Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#Sx6.F3" title="Figure 3 â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">3</span></a> shows the framework of SimMAT, which consists of a MAT layer and a foundation model. Different from in-modality transfer learning, novel image modalities captured from alternative sensors may have different dimensions, which prevents us from using existing finetuning methods directly. To address this problem, <span class="ltx_text ltx_font_typewriter" id="Sx4.SSx2.p1.7.1">SimMAT</span> introduces a modality-agnostic transfer layer (MAT). To keep our framework simple, the only difference is the MAT layer compared with in-modality finetuning. Considering a new input data <math alttext="\mathbf{x}\in\mathbb{R}^{H\times W\times C}" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.1.m1.1"><semantics id="Sx4.SSx2.p1.1.m1.1a"><mrow id="Sx4.SSx2.p1.1.m1.1.1" xref="Sx4.SSx2.p1.1.m1.1.1.cmml"><mi id="Sx4.SSx2.p1.1.m1.1.1.2" xref="Sx4.SSx2.p1.1.m1.1.1.2.cmml">ğ±</mi><mo id="Sx4.SSx2.p1.1.m1.1.1.1" xref="Sx4.SSx2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="Sx4.SSx2.p1.1.m1.1.1.3" xref="Sx4.SSx2.p1.1.m1.1.1.3.cmml"><mi id="Sx4.SSx2.p1.1.m1.1.1.3.2" xref="Sx4.SSx2.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="Sx4.SSx2.p1.1.m1.1.1.3.3" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.cmml"><mi id="Sx4.SSx2.p1.1.m1.1.1.3.3.2" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo id="Sx4.SSx2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="Sx4.SSx2.p1.1.m1.1.1.3.3.3" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo id="Sx4.SSx2.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="Sx4.SSx2.p1.1.m1.1.1.3.3.4" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.1.m1.1b"><apply id="Sx4.SSx2.p1.1.m1.1.1.cmml" xref="Sx4.SSx2.p1.1.m1.1.1"><in id="Sx4.SSx2.p1.1.m1.1.1.1.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.1"></in><ci id="Sx4.SSx2.p1.1.m1.1.1.2.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.2">ğ±</ci><apply id="Sx4.SSx2.p1.1.m1.1.1.3.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx4.SSx2.p1.1.m1.1.1.3.1.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3">superscript</csymbol><ci id="Sx4.SSx2.p1.1.m1.1.1.3.2.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.2">â„</ci><apply id="Sx4.SSx2.p1.1.m1.1.1.3.3.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.3"><times id="Sx4.SSx2.p1.1.m1.1.1.3.3.1.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.1"></times><ci id="Sx4.SSx2.p1.1.m1.1.1.3.3.2.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.2">ğ»</ci><ci id="Sx4.SSx2.p1.1.m1.1.1.3.3.3.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.3">ğ‘Š</ci><ci id="Sx4.SSx2.p1.1.m1.1.1.3.3.4.cmml" xref="Sx4.SSx2.p1.1.m1.1.1.3.3.4">ğ¶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.1.m1.1c">\mathbf{x}\in\mathbb{R}^{H\times W\times C}</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.1.m1.1d">bold_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_H Ã— italic_W Ã— italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> that is captured from a novel sensor, the dimension <math alttext="C" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.2.m2.1"><semantics id="Sx4.SSx2.p1.2.m2.1a"><mi id="Sx4.SSx2.p1.2.m2.1.1" xref="Sx4.SSx2.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.2.m2.1b"><ci id="Sx4.SSx2.p1.2.m2.1.1.cmml" xref="Sx4.SSx2.p1.2.m2.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.2.m2.1d">italic_C</annotation></semantics></math> is a modality-specific value (e.g., <math alttext="C=3" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.3.m3.1"><semantics id="Sx4.SSx2.p1.3.m3.1a"><mrow id="Sx4.SSx2.p1.3.m3.1.1" xref="Sx4.SSx2.p1.3.m3.1.1.cmml"><mi id="Sx4.SSx2.p1.3.m3.1.1.2" xref="Sx4.SSx2.p1.3.m3.1.1.2.cmml">C</mi><mo id="Sx4.SSx2.p1.3.m3.1.1.1" xref="Sx4.SSx2.p1.3.m3.1.1.1.cmml">=</mo><mn id="Sx4.SSx2.p1.3.m3.1.1.3" xref="Sx4.SSx2.p1.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.3.m3.1b"><apply id="Sx4.SSx2.p1.3.m3.1.1.cmml" xref="Sx4.SSx2.p1.3.m3.1.1"><eq id="Sx4.SSx2.p1.3.m3.1.1.1.cmml" xref="Sx4.SSx2.p1.3.m3.1.1.1"></eq><ci id="Sx4.SSx2.p1.3.m3.1.1.2.cmml" xref="Sx4.SSx2.p1.3.m3.1.1.2">ğ¶</ci><cn id="Sx4.SSx2.p1.3.m3.1.1.3.cmml" type="integer" xref="Sx4.SSx2.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.3.m3.1c">C=3</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.3.m3.1d">italic_C = 3</annotation></semantics></math> for a RGB image and <math alttext="C=1" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.4.m4.1"><semantics id="Sx4.SSx2.p1.4.m4.1a"><mrow id="Sx4.SSx2.p1.4.m4.1.1" xref="Sx4.SSx2.p1.4.m4.1.1.cmml"><mi id="Sx4.SSx2.p1.4.m4.1.1.2" xref="Sx4.SSx2.p1.4.m4.1.1.2.cmml">C</mi><mo id="Sx4.SSx2.p1.4.m4.1.1.1" xref="Sx4.SSx2.p1.4.m4.1.1.1.cmml">=</mo><mn id="Sx4.SSx2.p1.4.m4.1.1.3" xref="Sx4.SSx2.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.4.m4.1b"><apply id="Sx4.SSx2.p1.4.m4.1.1.cmml" xref="Sx4.SSx2.p1.4.m4.1.1"><eq id="Sx4.SSx2.p1.4.m4.1.1.1.cmml" xref="Sx4.SSx2.p1.4.m4.1.1.1"></eq><ci id="Sx4.SSx2.p1.4.m4.1.1.2.cmml" xref="Sx4.SSx2.p1.4.m4.1.1.2">ğ¶</ci><cn id="Sx4.SSx2.p1.4.m4.1.1.3.cmml" type="integer" xref="Sx4.SSx2.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.4.m4.1c">C=1</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.4.m4.1d">italic_C = 1</annotation></semantics></math> for a depth image). The design of MAT is open. In this paper, we propose a compact design of MAT layer. This module <math alttext="m" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.5.m5.1"><semantics id="Sx4.SSx2.p1.5.m5.1a"><mi id="Sx4.SSx2.p1.5.m5.1.1" xref="Sx4.SSx2.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.5.m5.1b"><ci id="Sx4.SSx2.p1.5.m5.1.1.cmml" xref="Sx4.SSx2.p1.5.m5.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.5.m5.1d">italic_m</annotation></semantics></math> requires a pretrained RGB embedding layer <math alttext="m_{v}" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.6.m6.1"><semantics id="Sx4.SSx2.p1.6.m6.1a"><msub id="Sx4.SSx2.p1.6.m6.1.1" xref="Sx4.SSx2.p1.6.m6.1.1.cmml"><mi id="Sx4.SSx2.p1.6.m6.1.1.2" xref="Sx4.SSx2.p1.6.m6.1.1.2.cmml">m</mi><mi id="Sx4.SSx2.p1.6.m6.1.1.3" xref="Sx4.SSx2.p1.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.6.m6.1b"><apply id="Sx4.SSx2.p1.6.m6.1.1.cmml" xref="Sx4.SSx2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="Sx4.SSx2.p1.6.m6.1.1.1.cmml" xref="Sx4.SSx2.p1.6.m6.1.1">subscript</csymbol><ci id="Sx4.SSx2.p1.6.m6.1.1.2.cmml" xref="Sx4.SSx2.p1.6.m6.1.1.2">ğ‘š</ci><ci id="Sx4.SSx2.p1.6.m6.1.1.3.cmml" xref="Sx4.SSx2.p1.6.m6.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.6.m6.1c">m_{v}</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.6.m6.1d">italic_m start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> and the novel modality dimension <math alttext="C" class="ltx_Math" display="inline" id="Sx4.SSx2.p1.7.m7.1"><semantics id="Sx4.SSx2.p1.7.m7.1a"><mi id="Sx4.SSx2.p1.7.m7.1.1" xref="Sx4.SSx2.p1.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p1.7.m7.1b"><ci id="Sx4.SSx2.p1.7.m7.1.1.cmml" xref="Sx4.SSx2.p1.7.m7.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p1.7.m7.1c">C</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx2.p1.7.m7.1d">italic_C</annotation></semantics></math> as input. As for the backbone, we load the pretrained weights from the vision foundation model directly. We will add a few trainable parameters if we use the parameter-efficient finetuning strategies, similar to in-modality finetuning.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">Experimental setup</h3>
<div class="ltx_para" id="Sx4.SSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.p1.2">Visual foundation models have developed very fastÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib51" title="">51</a>]</cite>. This paper selects Segment Anything Model (SAM)Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib1" title="">1</a>]</cite> as a backbone for exploring experiments as it is one of the most representative foundation models in computer vision. SAM has three components: an image encoder, a prompt encoder, and a mask decoder. The image encoder receives image patches as input and computes image features. The prompt encoder embeds prompts, <math alttext="i.e." class="ltx_Math" display="inline" id="Sx4.SSx3.p1.1.m1.3"><semantics id="Sx4.SSx3.p1.1.m1.3a"><mrow id="Sx4.SSx3.p1.1.m1.3.3.1"><mrow id="Sx4.SSx3.p1.1.m1.3.3.1.1.2" xref="Sx4.SSx3.p1.1.m1.3.3.1.1.1.cmml"><mi id="Sx4.SSx3.p1.1.m1.1.1" xref="Sx4.SSx3.p1.1.m1.1.1.cmml">i</mi><mo id="Sx4.SSx3.p1.1.m1.3.3.1.1.2.1" lspace="0em" rspace="0.167em" xref="Sx4.SSx3.p1.1.m1.3.3.1.1.1a.cmml">.</mo><mi id="Sx4.SSx3.p1.1.m1.2.2" xref="Sx4.SSx3.p1.1.m1.2.2.cmml">e</mi></mrow><mo id="Sx4.SSx3.p1.1.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.1.m1.3b"><apply id="Sx4.SSx3.p1.1.m1.3.3.1.1.1.cmml" xref="Sx4.SSx3.p1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx4.SSx3.p1.1.m1.3.3.1.1.1a.cmml" xref="Sx4.SSx3.p1.1.m1.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="Sx4.SSx3.p1.1.m1.1.1.cmml" xref="Sx4.SSx3.p1.1.m1.1.1">ğ‘–</ci><ci id="Sx4.SSx3.p1.1.m1.2.2.cmml" xref="Sx4.SSx3.p1.1.m1.2.2">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.1.m1.3c">i.e.</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx3.p1.1.m1.3d">italic_i . italic_e .</annotation></semantics></math>, points, boxes, text, or masks. Both image features and prompt embedding are fed into a lightweight mask decoder to obtain mask predictions.
The released SAM model is trained on the large-scale SA-1B dataset, which contains over 1 billion
automatically generated masks (400Ã— more masks than any existing segmentation datasets)
and 11 million images. Several worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib29" title="">29</a>]</cite> focus on adapting the SAM to different domains of RGB images, while we use SAM as the vision foundation model to explore the modality transfer task. Although some worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib105" title="">105</a>]</cite> have discussed the SAM adaption with specific modality (<math alttext="e.g." class="ltx_Math" display="inline" id="Sx4.SSx3.p1.2.m2.3"><semantics id="Sx4.SSx3.p1.2.m2.3a"><mrow id="Sx4.SSx3.p1.2.m2.3.3.1"><mrow id="Sx4.SSx3.p1.2.m2.3.3.1.1.2" xref="Sx4.SSx3.p1.2.m2.3.3.1.1.1.cmml"><mi id="Sx4.SSx3.p1.2.m2.1.1" xref="Sx4.SSx3.p1.2.m2.1.1.cmml">e</mi><mo id="Sx4.SSx3.p1.2.m2.3.3.1.1.2.1" lspace="0em" rspace="0.167em" xref="Sx4.SSx3.p1.2.m2.3.3.1.1.1a.cmml">.</mo><mi id="Sx4.SSx3.p1.2.m2.2.2" xref="Sx4.SSx3.p1.2.m2.2.2.cmml">g</mi></mrow><mo id="Sx4.SSx3.p1.2.m2.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.2.m2.3b"><apply id="Sx4.SSx3.p1.2.m2.3.3.1.1.1.cmml" xref="Sx4.SSx3.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx4.SSx3.p1.2.m2.3.3.1.1.1a.cmml" xref="Sx4.SSx3.p1.2.m2.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="Sx4.SSx3.p1.2.m2.1.1.cmml" xref="Sx4.SSx3.p1.2.m2.1.1">ğ‘’</ci><ci id="Sx4.SSx3.p1.2.m2.2.2.cmml" xref="Sx4.SSx3.p1.2.m2.2.2">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.2.m2.3c">e.g.</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx3.p1.2.m2.3d">italic_e . italic_g .</annotation></semantics></math>, MRI, depth), we are toward a more general setting handling an arbitrary modality.</p>
</div>
<div class="ltx_para" id="Sx4.SSx3.p2">
<p class="ltx_p" id="Sx4.SSx3.p2.2">We train the model using the same loss function of the vision foundation model (<math alttext="i.e." class="ltx_Math" display="inline" id="Sx4.SSx3.p2.1.m1.3"><semantics id="Sx4.SSx3.p2.1.m1.3a"><mrow id="Sx4.SSx3.p2.1.m1.3.3.1"><mrow id="Sx4.SSx3.p2.1.m1.3.3.1.1.2" xref="Sx4.SSx3.p2.1.m1.3.3.1.1.1.cmml"><mi id="Sx4.SSx3.p2.1.m1.1.1" xref="Sx4.SSx3.p2.1.m1.1.1.cmml">i</mi><mo id="Sx4.SSx3.p2.1.m1.3.3.1.1.2.1" lspace="0em" rspace="0.167em" xref="Sx4.SSx3.p2.1.m1.3.3.1.1.1a.cmml">.</mo><mi id="Sx4.SSx3.p2.1.m1.2.2" xref="Sx4.SSx3.p2.1.m1.2.2.cmml">e</mi></mrow><mo id="Sx4.SSx3.p2.1.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p2.1.m1.3b"><apply id="Sx4.SSx3.p2.1.m1.3.3.1.1.1.cmml" xref="Sx4.SSx3.p2.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="Sx4.SSx3.p2.1.m1.3.3.1.1.1a.cmml" xref="Sx4.SSx3.p2.1.m1.3.3.1.1.2.1">formulae-sequence</csymbol><ci id="Sx4.SSx3.p2.1.m1.1.1.cmml" xref="Sx4.SSx3.p2.1.m1.1.1">ğ‘–</ci><ci id="Sx4.SSx3.p2.1.m1.2.2.cmml" xref="Sx4.SSx3.p2.1.m1.2.2">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p2.1.m1.3c">i.e.</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx3.p2.1.m1.3d">italic_i . italic_e .</annotation></semantics></math>, SAM in this paper). In our experiments, we investigate different finetuning strategies, including LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite>, MLP AdapterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib77" title="">77</a>]</cite>, full finetuning, <math alttext="etc" class="ltx_Math" display="inline" id="Sx4.SSx3.p2.2.m2.1"><semantics id="Sx4.SSx3.p2.2.m2.1a"><mrow id="Sx4.SSx3.p2.2.m2.1.1" xref="Sx4.SSx3.p2.2.m2.1.1.cmml"><mi id="Sx4.SSx3.p2.2.m2.1.1.2" xref="Sx4.SSx3.p2.2.m2.1.1.2.cmml">e</mi><mo id="Sx4.SSx3.p2.2.m2.1.1.1" xref="Sx4.SSx3.p2.2.m2.1.1.1.cmml">â¢</mo><mi id="Sx4.SSx3.p2.2.m2.1.1.3" xref="Sx4.SSx3.p2.2.m2.1.1.3.cmml">t</mi><mo id="Sx4.SSx3.p2.2.m2.1.1.1a" xref="Sx4.SSx3.p2.2.m2.1.1.1.cmml">â¢</mo><mi id="Sx4.SSx3.p2.2.m2.1.1.4" xref="Sx4.SSx3.p2.2.m2.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p2.2.m2.1b"><apply id="Sx4.SSx3.p2.2.m2.1.1.cmml" xref="Sx4.SSx3.p2.2.m2.1.1"><times id="Sx4.SSx3.p2.2.m2.1.1.1.cmml" xref="Sx4.SSx3.p2.2.m2.1.1.1"></times><ci id="Sx4.SSx3.p2.2.m2.1.1.2.cmml" xref="Sx4.SSx3.p2.2.m2.1.1.2">ğ‘’</ci><ci id="Sx4.SSx3.p2.2.m2.1.1.3.cmml" xref="Sx4.SSx3.p2.2.m2.1.1.3">ğ‘¡</ci><ci id="Sx4.SSx3.p2.2.m2.1.1.4.cmml" xref="Sx4.SSx3.p2.2.m2.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p2.2.m2.1c">etc</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx3.p2.2.m2.1d">italic_e italic_t italic_c</annotation></semantics></math>. We train the model on each modality respectively. We implement the SimMAT using PyTorch. More details for the training can be found in the supplementary materials.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx5" lang="en">
<h2 class="ltx_title ltx_title_section">Data Availability</h2>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">The source images used throughout this work are publicly available. All captured data used to generate the findings in this work will be made public.</p>
</div>
</section>
<section class="ltx_section" id="Sx6" lang="en">
<h2 class="ltx_title ltx_title_section">Code Availability</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">Our code will be publicly available at <a class="ltx_ref ltx_href" href="https://github.com/mt-cly/SimMAT/" title="">https://github.com/mt-cly/SimMAT/</a>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Kirillov, A. <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 4015â€“4026 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bai, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Sequential modeling enables scalable learning for large vision models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 22861â€“22872 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bommasani, R. <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.1"><span class="ltx_ERROR undefined" id="bib.bib3.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2108.07258</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Kenton, J. D. M.-W.Â C. &amp; Toutanova, L.Â K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of naacL-HLT</em>, vol.Â 1, 2 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Brown, T. <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.1"><span class="ltx_ERROR undefined" id="bib.bib5.2.1.1">\JournalTitle</span>Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib5.3.2">33</span>, 1877â€“1901 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ma, J. <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Segment anything in medical images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.1"><span class="ltx_ERROR undefined" id="bib.bib6.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib6.3.2">15</span>, 654 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Zhou, J. <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Pre-trained multimodal large language model enhances dermatological diagnosis using skingpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.1"><span class="ltx_ERROR undefined" id="bib.bib7.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib7.3.2">15</span>, 5649 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yang, Z. <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">A vision chip with complementary pathways for open-world sensing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.1"><span class="ltx_ERROR undefined" id="bib.bib8.2.1.1">\JournalTitle</span>Nature</em> <span class="ltx_text ltx_font_bold" id="bib.bib8.3.2">629</span>, 1027â€“1033 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gehrig, D. &amp; Scaramuzza, D.

</span>
<span class="ltx_bibblock">Low-latency automotive vision with event cameras.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1"><span class="ltx_ERROR undefined" id="bib.bib9.1.1.1">\JournalTitle</span>Nature</em> <span class="ltx_text ltx_font_bold" id="bib.bib9.2.2">629</span>, 1034â€“1040 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yako, M. <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Video-rate hyperspectral camera based on a cmos-compatible random array of fabryâ€“pÃ©rot filters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.1"><span class="ltx_ERROR undefined" id="bib.bib10.2.1.1">\JournalTitle</span>Nature Photonics</em> <span class="ltx_text ltx_font_bold" id="bib.bib10.3.2">17</span>, 218â€“223 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Merchant, A. <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Scaling deep learning for materials discovery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.1"><span class="ltx_ERROR undefined" id="bib.bib11.2.1.1">\JournalTitle</span>Nature</em> <span class="ltx_text ltx_font_bold" id="bib.bib11.3.2">624</span>, 80â€“85 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Pan, S.Â J. &amp; Yang, Q.

</span>
<span class="ltx_bibblock">A survey on transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1"><span class="ltx_ERROR undefined" id="bib.bib12.1.1.1">\JournalTitle</span>IEEE Transactions on knowledge and data engineering</em> <span class="ltx_text ltx_font_bold" id="bib.bib12.2.2">22</span>, 1345â€“1359 (2009).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Wu, J. <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Medical sam adapter: Adapting segment anything model for medical image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.1"><span class="ltx_ERROR undefined" id="bib.bib13.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.12620</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Huang, X. <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Polarization structured light 3d depth image sensor for scenes with reflective surfaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.1"><span class="ltx_ERROR undefined" id="bib.bib14.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib14.3.2">14</span>, 6855 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lei, C. <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Shape from polarization for complex scenes in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.1">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</em>, 12632â€“12641 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Lei, C. <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Polarized reflection removal with perfect alignment in the wild.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 1750â€“1758 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Sun, Z., Wang, J., Wu, Y. &amp; Nayar, S.

</span>
<span class="ltx_bibblock">Seeing far in the dark with patterned flash.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">European Conference on Computer Vision</em>, 709â€“727 (Springer, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Gallego, G. <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Event-based vision: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.1"><span class="ltx_ERROR undefined" id="bib.bib18.2.1.1">\JournalTitle</span>IEEE transactions on pattern analysis and machine intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bib18.3.2">44</span>, 154â€“180 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Dong, W. <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Hyperspectral image super-resolution via non-negative structured sparse representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.2.1"><span class="ltx_ERROR undefined" id="bib.bib19.2.1.1">\JournalTitle</span>IEEE Transactions on Image Processing</em> <span class="ltx_text ltx_font_bold" id="bib.bib19.3.2">25</span>, 2337â€“2352 (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tseng, E. <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Neural nano-optics for high-quality thin lens imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.1"><span class="ltx_ERROR undefined" id="bib.bib20.2.1.1">\JournalTitle</span>Nature communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib20.3.2">12</span>, 6493 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wang, R. <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Sub-surface thermal measurement in additive manufacturing via machine learning-enabled high-resolution fiber optic sensing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.1"><span class="ltx_ERROR undefined" id="bib.bib21.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib21.3.2">15</span>, 7568 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mao, Q., Liao, Z., Yuan, J. &amp; Zhu, R.

</span>
<span class="ltx_bibblock">Multimodal tactile sensing fused with vision for dexterous robotic housekeeping.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1"><span class="ltx_ERROR undefined" id="bib.bib22.1.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib22.2.2">15</span>, 6871 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ye, S. <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Superanimal pretrained pose estimation models for behavioral analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.1"><span class="ltx_ERROR undefined" id="bib.bib23.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib23.3.2">15</span>, 5165 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Pai, S. <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Foundation model for cancer imaging biomarkers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.1"><span class="ltx_ERROR undefined" id="bib.bib24.2.1.1">\JournalTitle</span>Nature machine intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bib24.3.2">6</span>, 354â€“367 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Cai, H. <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Pretrainable geometric graph neural network for antibody affinity maturation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.2.1"><span class="ltx_ERROR undefined" id="bib.bib25.2.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib25.3.2">15</span>, 7785 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lu, K., Grover, A., Abbeel, P. &amp; Mordatch, I.

</span>
<span class="ltx_bibblock">Frozen pretrained transformers as universal computation engines.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.Â 36, 7628â€“7636 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Dinh, T. <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Lift: Language-interfaced fine-tuning for non-language machine learning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.2.1"><span class="ltx_ERROR undefined" id="bib.bib27.2.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib27.3.2">35</span>, 11763â€“11784 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Vinod, R., Chen, P.-Y. &amp; Das, P.

</span>
<span class="ltx_bibblock">Reprogramming pretrained language models for protein sequence representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1"><span class="ltx_ERROR undefined" id="bib.bib28.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2301.02120</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ma, J. &amp; Wang, B.

</span>
<span class="ltx_bibblock">Segment anything in medical images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1"><span class="ltx_ERROR undefined" id="bib.bib29.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.12306</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Shen, J. <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Cross-modal fine-tuning: Align then refine.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.1">International Conference on Machine Learning</em>, 31030â€“31056 (PMLR, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Hu, E.Â J. <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.1">International Conference on Learning Representations</em> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Houlsby, N. <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.1">International conference on machine learning</em>, 2790â€“2799 (PMLR, 2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Zhu, J., Lai, S., Chen, X., Wang, D. &amp; Lu, H.

</span>
<span class="ltx_bibblock">Visual prompt multi-modal tracking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9516â€“9526 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Mei, H. <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Glass segmentation using intensity and spectral polarization cues.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 12622â€“12631 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Xiang, K., Yang, K. &amp; Wang, K.

</span>
<span class="ltx_bibblock">Polarization-driven semantic segmentation via efficient attention-bridged fusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1"><span class="ltx_ERROR undefined" id="bib.bib35.1.1.1">\JournalTitle</span>Optics Express</em> <span class="ltx_text ltx_font_bold" id="bib.bib35.2.2">29</span>, 4802â€“4820 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
NathanÂ Silberman, P.Â K., DerekÂ Hoiem &amp; Fergus, R.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from rgbd images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">ECCV</em> (2012).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Gupta, S., Girshick, R., ArbelÃ¡ez, P. &amp; Malik, J.

</span>
<span class="ltx_bibblock">Learning rich features from rgb-d images for object detection and segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13</em>, 345â€“360 (Springer, 2014).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Huo, D., Wang, J., Qian, Y. &amp; Yang, Y.-H.

</span>
<span class="ltx_bibblock">Glass segmentation with rgb-thermal image pairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1"><span class="ltx_ERROR undefined" id="bib.bib38.1.1.1">\JournalTitle</span>IEEE Transactions on Image Processing</em> <span class="ltx_text ltx_font_bold" id="bib.bib38.2.2">32</span>, 1911â€“1926 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Brown, M. &amp; SÃ¼sstrunk, S.

</span>
<span class="ltx_bibblock">Multi-spectral sift for scene category recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">CVPR 2011</em>, 177â€“184 (IEEE, 2011).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q. &amp; Lee, Y.Â J.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1"><span class="ltx_ERROR undefined" id="bib.bib40.1.1.1">\JournalTitle</span>Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib40.2.2">36</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S. &amp; Hoi, S.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International conference on machine learning</em>, 19730â€“19742 (PMLR, 2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Sun, Y., Zuo, W. &amp; Liu, M.

</span>
<span class="ltx_bibblock">Rtfnet: Rgb-thermal fusion network for semantic segmentation of urban scenes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1"><span class="ltx_ERROR undefined" id="bib.bib42.1.1.1">\JournalTitle</span>IEEE Robotics and Automation Letters</em> <span class="ltx_text ltx_font_bold" id="bib.bib42.2.2">4</span>, 2576â€“2583 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Singh, A.Â D. <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Depth estimation from camera image and mmwave radar point cloud.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9275â€“9285 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">International conference on machine learning</em>, 1597â€“1607 (PMLR, 2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
He, K., Fan, H., Wu, Y., Xie, S. &amp; Girshick, R.

</span>
<span class="ltx_bibblock">Momentum contrast for unsupervised visual representation learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 9729â€“9738 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Jia, M. <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Visual prompt tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.2.1">European Conference on Computer Vision</em>, 709â€“727 (Springer, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Ouyang, L. <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.2.1"><span class="ltx_ERROR undefined" id="bib.bib47.2.1.1">\JournalTitle</span>Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib47.3.2">35</span>, 27730â€“27744 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Radford, A. <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.2.1">International conference on machine learning</em>, 8748â€“8763 (PMLR, 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Zhang, L., Rao, A. &amp; Agrawala, M.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 3836â€“3847 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Lai, X. <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Lisa: Reasoning segmentation via large language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9579â€“9589 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models (2021).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url" href="2112.10752" title="">2112.10752</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zamir, A.Â R. <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Taskonomy: Disentangling task transfer learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.2.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Deng, J. <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.2.1">2009 IEEE conference on computer vision and pattern recognition</em>, 248â€“255 (Ieee, 2009).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Chen, X., Fan, H., Girshick, R. &amp; He, K.

</span>
<span class="ltx_bibblock">Improved baselines with momentum contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1"><span class="ltx_ERROR undefined" id="bib.bib54.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2003.04297</em> (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Fan, H. <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Multiscale vision transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.2.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 6824â€“6835 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Grill, J.-B. <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Bootstrap your own latent-a new approach to self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.2.1"><span class="ltx_ERROR undefined" id="bib.bib56.2.1.1">\JournalTitle</span>Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib56.3.2">33</span>, 21271â€“21284 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
He, K. <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.2.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 16000â€“16009 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Xie, Z. <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Simmim: A simple framework for masked image modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9653â€“9663 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Bao, H., Dong, L., Piao, S. &amp; Wei, F.

</span>
<span class="ltx_bibblock">Beit: Bert pre-training of image transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">International Conference on Learning Representations</em> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Singh, J., Hanson, J., Paliwal, K. &amp; Zhou, Y.

</span>
<span class="ltx_bibblock">Rna secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1"><span class="ltx_ERROR undefined" id="bib.bib60.1.1.1">\JournalTitle</span>Nature communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib60.2.2">10</span>, 5407 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Kang, Y., Park, H., Smit, B. &amp; Kim, J.

</span>
<span class="ltx_bibblock">A multi-modal pre-training transformer for universal transfer learning in metalâ€“organic frameworks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1"><span class="ltx_ERROR undefined" id="bib.bib61.1.1.1">\JournalTitle</span>Nature Machine Intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bib61.2.2">5</span>, 309â€“318 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Pan, J.

</span>
<span class="ltx_bibblock">Transfer learning for metalâ€“organic frameworks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1"><span class="ltx_ERROR undefined" id="bib.bib62.1.1.1">\JournalTitle</span>Nature Computational Science</em> <span class="ltx_text ltx_font_bold" id="bib.bib62.2.2">3</span>, 280â€“280 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wang, K., Johnson, C.Â W., Bennett, K.Â C. &amp; Johnson, P.Â A.

</span>
<span class="ltx_bibblock">Predicting fault slip via transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1"><span class="ltx_ERROR undefined" id="bib.bib63.1.1.1">\JournalTitle</span>Nature communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib63.2.2">12</span>, 7319 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Choi, E. <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Neural 360 structured light with learned metasurfaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.2.1"><span class="ltx_ERROR undefined" id="bib.bib64.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2306.13361</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Guo, R. <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Eventlfm: Event camera integrated fourier light field microscopy for ultrafast 3d imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.2.1"><span class="ltx_ERROR undefined" id="bib.bib65.2.1.1">\JournalTitle</span>Light: Science &amp; Applications</em> <span class="ltx_text ltx_font_bold" id="bib.bib65.3.2">13</span>, 144 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Radhakrishnan, A., RuizÂ Luyten, M., Prasad, N. &amp; Uhler, C.

</span>
<span class="ltx_bibblock">Transfer learning with kernel methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1"><span class="ltx_ERROR undefined" id="bib.bib66.1.1.1">\JournalTitle</span>Nature Communications</em> <span class="ltx_text ltx_font_bold" id="bib.bib66.2.2">14</span>, 5570 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Li, B. <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Llava-onevision: Easy visual task transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.2.1"><span class="ltx_ERROR undefined" id="bib.bib67.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2408.03326</em> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Zhang, K. <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">A generalist visionâ€“language foundation model for diverse biomedical tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.2.1"><span class="ltx_ERROR undefined" id="bib.bib68.2.1.1">\JournalTitle</span>Nature Medicine</em> 1â€“13 (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Vu, T.-H., Jain, H., Bucher, M., Cord, M. &amp; PÃ©rez, P.

</span>
<span class="ltx_bibblock">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2517â€“2526 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Pan, F., Shin, I., Rameau, F., Lee, S. &amp; Kweon, I.Â S.

</span>
<span class="ltx_bibblock">Unsupervised intra-domain adaptation for semantic segmentation through self-supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 3764â€“3773 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Lai, X. <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Decouplenet: Decoupled network for domain adaptive semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.2.1">European Conference on Computer Vision</em>, 369â€“387 (Springer, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Chen, M., Xue, H. &amp; Cai, D.

</span>
<span class="ltx_bibblock">Domain adaptation for semantic segmentation with maximum squares loss.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2090â€“2099 (2019).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Zou, Y., Yu, Z., Kumar, B. &amp; Wang, J.

</span>
<span class="ltx_bibblock">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 289â€“305 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Wang, P. <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Uncertainty-aware clustering for unsupervised domain adaptive object re-identification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.2.1"><span class="ltx_ERROR undefined" id="bib.bib74.2.1.1">\JournalTitle</span>IEEE Transactions on Multimedia</em> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Liu, F., Zhang, G. &amp; Lu, J.

</span>
<span class="ltx_bibblock">Heterogeneous domain adaptation: An unsupervised approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1"><span class="ltx_ERROR undefined" id="bib.bib75.1.1.1">\JournalTitle</span>IEEE transactions on neural networks and learning systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib75.2.2">31</span>, 5588â€“5602 (2020).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Luo, Y., Wen, Y., Liu, T. &amp; Tao, D.

</span>
<span class="ltx_bibblock">Transferring knowledge fragments for learning distance metric from a heterogeneous domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1"><span class="ltx_ERROR undefined" id="bib.bib76.1.1.1">\JournalTitle</span>IEEE transactions on pattern analysis and machine intelligence</em> <span class="ltx_text ltx_font_bold" id="bib.bib76.2.2">41</span>, 1013â€“1026 (2018).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Chen, S. <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Adaptformer: Adapting vision transformers for scalable visual recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.2.1"><span class="ltx_ERROR undefined" id="bib.bib77.2.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib77.3.2">35</span>, 16664â€“16678 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S. &amp; Sun, J.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 770â€“778 (2016).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Li, X.Â L. &amp; Liang, P.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1"><span class="ltx_ERROR undefined" id="bib.bib79.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2101.00190</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Wang, W. <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Internimage: Exploring large-scale vision foundation models with deformable convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 14408â€“14419 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Girdhar, R. <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Imagebind: One embedding space to bind them all.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 15180â€“15190 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Vaswani, A. <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.2.1"><span class="ltx_ERROR undefined" id="bib.bib82.2.1.1">\JournalTitle</span>Advances in neural information processing systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib82.3.2">30</span> (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Touvron, H. <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.2.1"><span class="ltx_ERROR undefined" id="bib.bib83.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2307.09288</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Dosovitskiy, A. <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.2.1">International Conference on Learning Representations</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Liu, Z. <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted windows.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.2.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 10012â€“10022 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Wang, Y., Huang, R., Song, S., Huang, Z. &amp; Huang, G.

</span>
<span class="ltx_bibblock">Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1"><span class="ltx_ERROR undefined" id="bib.bib86.1.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib86.2.2">34</span>, 11960â€“11973 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Guo, M.-H. <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Pct: Point cloud transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.2.1"><span class="ltx_ERROR undefined" id="bib.bib87.2.1.1">\JournalTitle</span>Computational Visual Media</em> <span class="ltx_text ltx_font_bold" id="bib.bib87.3.2">7</span>, 187â€“199 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Zhao, H., Jiang, L., Jia, J., Torr, P.Â H. &amp; Koltun, V.

</span>
<span class="ltx_bibblock">Point transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 16259â€“16268 (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Wu, X., Lao, Y., Jiang, L., Liu, X. &amp; Zhao, H.

</span>
<span class="ltx_bibblock">Point transformer v2: Grouped vector attention and partition-based pooling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1"><span class="ltx_ERROR undefined" id="bib.bib89.1.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib89.2.2">35</span>, 33330â€“33342 (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Gong, Y., Chung, Y.-A. &amp; Glass, J.

</span>
<span class="ltx_bibblock">Ast: Audio spectrogram transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1"><span class="ltx_ERROR undefined" id="bib.bib90.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2104.01778</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Chen, K. <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib91.2.1">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 646â€“650 (IEEE, 2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Verma, P. &amp; Berger, J.

</span>
<span class="ltx_bibblock">Audio transformers: Transformer architectures for large scale audio understanding. adieu convolutions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1"><span class="ltx_ERROR undefined" id="bib.bib92.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2105.00335</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Jaegle, A. <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Perceiver: General perception with iterative attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.2.1">International conference on machine learning</em>, 4651â€“4664 (PMLR, 2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Jaegle, A. <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Perceiver io: A general architecture for structured inputs &amp; outputs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.2.1">International Conference on Learning Representations</em> (2022).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Tamkin, A. <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Dabs: A domain-agnostic benchmark for self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.2.1"><span class="ltx_ERROR undefined" id="bib.bib95.2.1.1">\JournalTitle</span>Advances in neural information processing systems</em> (2021).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Wu, H. <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Randomized quantization: A generic augmentation for data agnostic self-supervised learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib96.2.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 16305â€“16316 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Wang, W. <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Internimage: Exploring large-scale vision foundation models with deformable convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib97.2.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 14408â€“14419 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Zhang, Y. <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Meta-transformer: A unified framework for multimodal learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.2.1"><span class="ltx_ERROR undefined" id="bib.bib98.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2307.10802</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Zou, X. <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Generalized decoding for pixel, image, and language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib99.2.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 15116â€“15127 (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Mizrahi, D. <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">4m: Massively multimodal masked modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.2.1"><span class="ltx_ERROR undefined" id="bib.bib100.2.1.1">\JournalTitle</span>Advances in Neural Information Processing Systems</em> <span class="ltx_text ltx_font_bold" id="bib.bib100.3.2">36</span> (2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Zou, X. <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Segment everything everywhere all at once.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.2.1">NeurIPS</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Ji, G.-P. <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Sam struggles in concealed scenesâ€“empirical study onâ€ segment anythingâ€.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.2.1"><span class="ltx_ERROR undefined" id="bib.bib102.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.06022</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Tang, L., Xiao, H. &amp; Li, B.

</span>
<span class="ltx_bibblock">Can sam segment anything? when sam meets camouflaged object detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1"><span class="ltx_ERROR undefined" id="bib.bib103.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.04709</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Chen, T. <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Sam fails to segment anything?â€“sam-adapter: Adapting sam in underperformed scenes: Camouflage, shadow, and more.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.2.1"><span class="ltx_ERROR undefined" id="bib.bib104.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2304.09148</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Cen, J. <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Sad: Segment any rgbd.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.2.1"><span class="ltx_ERROR undefined" id="bib.bib105.2.1.1">\JournalTitle</span>arXiv preprint arXiv:2305.14207</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Zhang, J. <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">etÂ al.</em>
</span>
<span class="ltx_bibblock">Cmx: Cross-modal fusion for rgb-x semantic segmentation with transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.2.1"><span class="ltx_ERROR undefined" id="bib.bib106.2.1.1">\JournalTitle</span>IEEE Transactions on Intelligent Transportation Systems</em> (2023).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. &amp; Neubig, G.

</span>
<span class="ltx_bibblock">Towards a unified view of parameter-efficient transfer learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1"><span class="ltx_ERROR undefined" id="bib.bib107.1.1.1">\JournalTitle</span>arXiv preprint arXiv:2110.04366</em> (2021).

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Sx6.F1" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="564" id="Sx6.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="Sx6.F1.6.1">Transferability Across Modalities.</span> <span class="ltx_text ltx_font_bold" id="Sx6.F1.7.2">a</span>, the number of natural images is significantly larger than images in other modalities in different areas, including medical imaging, thermal images, depth images, and polarization images. <span class="ltx_text ltx_font_bold" id="Sx6.F1.8.3">b</span>, natural images can train vision foundation models, which can be applied to achieve strong performance on different downstream tasks. <span class="ltx_text ltx_font_bold" id="Sx6.F1.9.4">c</span>, it is very challenging for other modalities to benefit from training foundation models due to limited data. <span class="ltx_text ltx_font_bold" id="Sx6.F1.10.5">d</span>, our proposed SimMAT explores the transferability from the pretrained vision foundation model to different modalities. </figcaption>
</figure>
<figure class="ltx_figure" id="Sx6.F2" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="627" id="Sx6.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="Sx6.F2.2.1">Qualitative Results.</span> We transfer the segment anything ability of SAM to different modalities, including segmentation from depth, thermal, polarization , HHA, and NIR images. The proposed method significantly improves segmentation quality compared to SAM zero-shot and training from scratch. </figcaption>
</figure>
<figure class="ltx_figure" id="Sx6.F3" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="139" id="Sx6.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="Sx6.F3.14.1">Details of SimMAT.</span> <span class="ltx_text ltx_font_bold" id="Sx6.F3.15.2">a</span>. SimMAT receives new modality <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="Sx6.F3.6.m1.1"><semantics id="Sx6.F3.6.m1.1b"><mi id="Sx6.F3.6.m1.1.1" xref="Sx6.F3.6.m1.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="Sx6.F3.6.m1.1c"><ci id="Sx6.F3.6.m1.1.1.cmml" xref="Sx6.F3.6.m1.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx6.F3.6.m1.1d">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="Sx6.F3.6.m1.1e">bold_x</annotation></semantics></math> as input and pass it through a modality-agnostic transfer layer <math alttext="m" class="ltx_Math" display="inline" id="Sx6.F3.7.m2.1"><semantics id="Sx6.F3.7.m2.1b"><mi id="Sx6.F3.7.m2.1.1" xref="Sx6.F3.7.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="Sx6.F3.7.m2.1c"><ci id="Sx6.F3.7.m2.1.1.cmml" xref="Sx6.F3.7.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx6.F3.7.m2.1d">m</annotation><annotation encoding="application/x-llamapun" id="Sx6.F3.7.m2.1e">italic_m</annotation></semantics></math> to obtain an embedding <math alttext="\mathbf{e}" class="ltx_Math" display="inline" id="Sx6.F3.8.m3.1"><semantics id="Sx6.F3.8.m3.1b"><mi id="Sx6.F3.8.m3.1.1" xref="Sx6.F3.8.m3.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="Sx6.F3.8.m3.1c"><ci id="Sx6.F3.8.m3.1.1.cmml" xref="Sx6.F3.8.m3.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx6.F3.8.m3.1d">\mathbf{e}</annotation><annotation encoding="application/x-llamapun" id="Sx6.F3.8.m3.1e">bold_e</annotation></semantics></math>. The embedding matches the dimension of a pretrained foundation model <math alttext="f" class="ltx_Math" display="inline" id="Sx6.F3.9.m4.1"><semantics id="Sx6.F3.9.m4.1b"><mi id="Sx6.F3.9.m4.1.1" xref="Sx6.F3.9.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="Sx6.F3.9.m4.1c"><ci id="Sx6.F3.9.m4.1.1.cmml" xref="Sx6.F3.9.m4.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx6.F3.9.m4.1d">f</annotation><annotation encoding="application/x-llamapun" id="Sx6.F3.9.m4.1e">italic_f</annotation></semantics></math>, and then we obtain the output <math alttext="\mathbf{y}" class="ltx_Math" display="inline" id="Sx6.F3.10.m5.1"><semantics id="Sx6.F3.10.m5.1b"><mi id="Sx6.F3.10.m5.1.1" xref="Sx6.F3.10.m5.1.1.cmml">ğ²</mi><annotation-xml encoding="MathML-Content" id="Sx6.F3.10.m5.1c"><ci id="Sx6.F3.10.m5.1.1.cmml" xref="Sx6.F3.10.m5.1.1">ğ²</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx6.F3.10.m5.1d">\mathbf{y}</annotation><annotation encoding="application/x-llamapun" id="Sx6.F3.10.m5.1e">bold_y</annotation></semantics></math>. The input and foundation are designed in a generic formulation for different modalities and foundation models. <span class="ltx_text ltx_font_bold" id="Sx6.F3.16.3">b</span>. in this work, we select SAM as a representative foundation model for a detailed study. </figcaption>
</figure>
<figure class="ltx_figure" id="Sx6.F4" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="845" id="Sx6.F4.g1" src="x4.png" width="456"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="Sx6.F4.5.1">Performance Evaluation on Different Modalities.</span> <span class="ltx_text ltx_font_bold" id="Sx6.F4.6.2">a</span>. The proposed method SimMAT improves the segmentation performance significantly on all evaluated modalities compared with training the models from scratch. Specifically, SimMAT improves the mIoU from 22.15% to 53.88% for all evaluated modalities on average. Besides, the peak performance between finetuning and parameter-efficient finetuning is similar.
<span class="ltx_text ltx_font_bold" id="Sx6.F4.7.3">b</span>. Results on Pseudo New Modalities. We combine natural images with a novel image modality as a pseudo new modality: note that we do not use the information that which three channels are for natural images and which channels are for new modalities. For example, our MAT is effective in improving the finetuning performance on all evaluated pseudo new modalities. Besides, the peak performance between finetuning and parameter-efficient finetuning is similar.
<span class="ltx_text ltx_font_bold" id="Sx6.F4.8.4">c</span>. We provide controlled experiments for different finetuning strategies on new modalities. Parameter-efficient finetuning strategies can achieve comparable performance compared with full finetuning by using much less trainable parameters.
</figcaption>
</figure>
<figure class="ltx_figure" id="Sx6.F5" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="Sx6.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="Sx6.F5.4.1">The Effect of Learning Rate and Training Data Size.</span> The models are evaluated on the polarization modality. <span class="ltx_text ltx_font_bold" id="Sx6.F5.5.2">a</span>. the full fine-tuning and parameter efficient tuning achieve peak performance in different learning rates. <span class="ltx_text ltx_font_bold" id="Sx6.F5.6.3">b</span>. increasing the scale of training data brings consistent performance improvement across different training strategies.</figcaption>
</figure>
<figure class="ltx_table" id="Sx6.T1" lang="en">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx6.T1.3" style="width:433.6pt;height:110.8pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-102.2pt,25.9pt) scale(0.679683498474356,0.679683498474356) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx6.T1.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx6.T1.3.1.1.1" style="background-color:#D9D9D9;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Sx6.T1.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx6.T1.3.1.1.1.1.1" style="background-color:#D9D9D9;">Choice of MAT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Sx6.T1.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx6.T1.3.1.1.1.2.1" style="background-color:#D9D9D9;">Required data</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Sx6.T1.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx6.T1.3.1.1.1.3.1" style="background-color:#D9D9D9;">Efficiency</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="Sx6.T1.3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx6.T1.3.1.1.1.4.1" style="background-color:#D9D9D9;">Performance (% mIoU)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx6.T1.3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.2.1.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.2.1.1.1" style="background-color:#D9D9D9;">Training from scratch</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.2.1.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.2.1.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.2.1.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.2.1.3.1" style="background-color:#CCFFCC;">Efficient</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.2.1.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.2.1.4.1" style="background-color:#CCFFCC;">25.43 (Bad performance)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.3.2.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.3.2.1.1" style="background-color:#D9D9D9;">Querying transformer in BLIP-2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib41" title="">41</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.3.2.2" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.3.2.2.1" style="background-color:#FFCCCC;">Source-target pair</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.3.2.3" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.3.2.3.1" style="background-color:#FFCCCC;">NA (Not applicable)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.3.2.4" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.3.2.4.1" style="background-color:#FFCCCC;">NA (Not applicable)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.4.3.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.4.3.1.1" style="background-color:#D9D9D9;">Dimension alignment in ORCAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib30" title="">30</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.4.3.2" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.4.3.2.1" style="background-color:#FFCCCC;">Source-target pair</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.4.3.3" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.4.3.3.1" style="background-color:#FFCCCC;">NA (Not applicable)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.4.3.4" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.4.3.4.1" style="background-color:#FFCCCC;">NA (Not applicable)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.5.4.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.5.4.1.1" style="background-color:#D9D9D9;">Randomly Initialized Patch EmbeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib26" title="">26</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.5.4.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.5.4.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.5.4.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.5.4.3.1" style="background-color:#CCFFCC;">Efficient</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.5.4.4" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.5.4.4.1" style="background-color:#FFCCCC;">58.89 (Bad performance)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.6.5.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.6.5.1.1" style="background-color:#D9D9D9;">Linear Layer + Trainable Patch EmbeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib40" title="">40</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.6.5.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.6.5.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.6.5.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.6.5.3.1" style="background-color:#CCFFCC;">Efficient</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.6.5.4" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.6.5.4.1" style="background-color:#FFCCCC;">63.96 (Moderate Performance)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.7.6.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.7.6.1.1" style="background-color:#D9D9D9;">Transformers + Trainable Patch EmbeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib106" title="">106</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.7.6.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.7.6.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.7.6.3" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.7.6.3.1" style="background-color:#FFCCCC;">More parameters</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.7.6.4" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.7.6.4.1" style="background-color:#FFCCCC;">26.67 (Bad performance)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.8.7.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.8.7.1.1" style="background-color:#D9D9D9;">Transpose to Batch DimensionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib13" title="">13</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.8.7.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.8.7.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.8.7.3" style="background-color:#FFCCCC;"><span class="ltx_text" id="Sx6.T1.3.1.8.7.3.1" style="background-color:#FFCCCC;">Much more FLOPS</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx6.T1.3.1.8.7.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.8.7.4.1" style="background-color:#CCFFCC;">70.90 (Good performance)</span></td>
</tr>
<tr class="ltx_tr" id="Sx6.T1.3.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Sx6.T1.3.1.9.8.1" style="background-color:#D9D9D9;"><span class="ltx_text" id="Sx6.T1.3.1.9.8.1.1" style="background-color:#D9D9D9;">Ours</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Sx6.T1.3.1.9.8.2" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.9.8.2.1" style="background-color:#CCFFCC;">Target data only</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Sx6.T1.3.1.9.8.3" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.9.8.3.1" style="background-color:#CCFFCC;">Efficient</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="Sx6.T1.3.1.9.8.4" style="background-color:#CCFFCC;"><span class="ltx_text" id="Sx6.T1.3.1.9.8.4.1" style="background-color:#CCFFCC;">72.69 (Best performance)</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="Sx6.T1.5.1">Exploring Modality-agnostic Transfer Layers. </span> Randomly initializing a patch embedding for each modality leads to worse results than any method that inherits the vision embedding layer. A simple <math alttext="1\times 1" class="ltx_Math" display="inline" id="Sx6.T1.2.m1.1"><semantics id="Sx6.T1.2.m1.1b"><mrow id="Sx6.T1.2.m1.1.1" xref="Sx6.T1.2.m1.1.1.cmml"><mn id="Sx6.T1.2.m1.1.1.2" xref="Sx6.T1.2.m1.1.1.2.cmml">1</mn><mo id="Sx6.T1.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Sx6.T1.2.m1.1.1.1.cmml">Ã—</mo><mn id="Sx6.T1.2.m1.1.1.3" xref="Sx6.T1.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx6.T1.2.m1.1c"><apply id="Sx6.T1.2.m1.1.1.cmml" xref="Sx6.T1.2.m1.1.1"><times id="Sx6.T1.2.m1.1.1.1.cmml" xref="Sx6.T1.2.m1.1.1.1"></times><cn id="Sx6.T1.2.m1.1.1.2.cmml" type="integer" xref="Sx6.T1.2.m1.1.1.2">1</cn><cn id="Sx6.T1.2.m1.1.1.3.cmml" type="integer" xref="Sx6.T1.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx6.T1.2.m1.1d">1\times 1</annotation><annotation encoding="application/x-llamapun" id="Sx6.T1.2.m1.1e">1 Ã— 1</annotation></semantics></math> convolution can improve the performance already. Interestingly, when the pretrained vision embedding layer is used, the performance would be better if we fixed the weights. All models are trained with Adapter finetuning strategy.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1" lang="en">
<p class="ltx_p" id="p1.1">This supplementary document provides additional descriptions and results to support the findings from the main manuscript.</p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Additional Training Details</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We report the effect of different finetuning strategies on trainable parameters in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S1.T2" title="Table 2 â€£ 1 Additional Training Details â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">2</span></a>. The foundation model SAM with ViT-BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib84" title="">84</a>]</cite> as backbone contains 93.7M parameters from the image encoder, prompt encoder, and mask decoder.
Full finetuning makes all parameters trainable.
For parameter-efficient tuning, we implement four typical methods including LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib31" title="">31</a>]</cite>, MLP adapterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib32" title="">32</a>]</cite>, and prompt tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib46" title="">46</a>]</cite>. Following He et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib107" title="">107</a>]</cite>, we balance their trainable parameters to achieve approximately 4% of full parameters for fair comparison.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The detailed training configuration is presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S1.T3" title="Table 3 â€£ 1 Additional Training Details â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">3</span></a>.
We fix the training epoch to 50 and set the batch size as 4 regardless of the number of training samples in different modality datasets.
We sweep the learning rates from 3e-6 to 3e-3 and report the peak performance as the final result.
The input modality images are resized to (1024, 1024) to meet the requirements of SAM.</p>
</div>
<figure class="ltx_table" id="S1.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T2.1" style="width:260.2pt;height:117.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(10.6pt,-4.8pt) scale(1.08884810992236,1.08884810992236) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T2.1.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Finetuning Strategies</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.1.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Trainable Parameters (M)</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S1.T2.1.1.2.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"></th>
<td class="ltx_td ltx_align_center" id="S1.T2.1.1.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">of Foundation Model</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T2.1.1.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">LoRA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.1.3.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.3</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T2.1.1.4.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">MLP adapter</th>
<td class="ltx_td ltx_align_center" id="S1.T2.1.1.4.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.9</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T2.1.1.5.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Prompt tuning</th>
<td class="ltx_td ltx_align_center" id="S1.T2.1.1.5.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.4</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S1.T2.1.1.6.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Full finetuning</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S1.T2.1.1.6.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">93.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span class="ltx_text ltx_font_bold" id="S1.T2.3.1">The Number of Trainable Parameters in Foundation Model (SAM) with Different Finetuing Strategies.</span> Three parameter-efficient finetuning methods hold similar trainable parameters, which are much less than the trainable parameters of full finetuning strategies.</figcaption>
</figure>
<figure class="ltx_table" id="S1.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T3.2.2" style="width:303.5pt;height:183.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(3.2pt,-1.9pt) scale(1.02142110623298,1.02142110623298) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T3.2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T3.2.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T3.2.2.2.3.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Config</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T3.2.2.2.3.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Value</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T3.2.2.2.4.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">optimizer</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T3.2.2.2.4.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Adam</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">optimizer momentum</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S1.T3.1.1.1.1.1.m1.1"><semantics id="S1.T3.1.1.1.1.1.m1.1a"><msub id="S1.T3.1.1.1.1.1.m1.1.1" xref="S1.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S1.T3.1.1.1.1.1.m1.1.1.2" xref="S1.T3.1.1.1.1.1.m1.1.1.2.cmml">Î²</mi><mn id="S1.T3.1.1.1.1.1.m1.1.1.3" xref="S1.T3.1.1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.T3.1.1.1.1.1.m1.1b"><apply id="S1.T3.1.1.1.1.1.m1.1.1.cmml" xref="S1.T3.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S1.T3.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S1.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S1.T3.1.1.1.1.1.m1.1.1.2">ğ›½</ci><cn id="S1.T3.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S1.T3.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T3.1.1.1.1.1.m1.1c">\beta_{1}</annotation><annotation encoding="application/x-llamapun" id="S1.T3.1.1.1.1.1.m1.1d">italic_Î² start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>,<math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S1.T3.2.2.2.2.2.m2.1"><semantics id="S1.T3.2.2.2.2.2.m2.1a"><msub id="S1.T3.2.2.2.2.2.m2.1.1" xref="S1.T3.2.2.2.2.2.m2.1.1.cmml"><mi id="S1.T3.2.2.2.2.2.m2.1.1.2" xref="S1.T3.2.2.2.2.2.m2.1.1.2.cmml">Î²</mi><mn id="S1.T3.2.2.2.2.2.m2.1.1.3" xref="S1.T3.2.2.2.2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S1.T3.2.2.2.2.2.m2.1b"><apply id="S1.T3.2.2.2.2.2.m2.1.1.cmml" xref="S1.T3.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S1.T3.2.2.2.2.2.m2.1.1.1.cmml" xref="S1.T3.2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S1.T3.2.2.2.2.2.m2.1.1.2.cmml" xref="S1.T3.2.2.2.2.2.m2.1.1.2">ğ›½</ci><cn id="S1.T3.2.2.2.2.2.m2.1.1.3.cmml" type="integer" xref="S1.T3.2.2.2.2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T3.2.2.2.2.2.m2.1c">\beta_{2}</annotation><annotation encoding="application/x-llamapun" id="S1.T3.2.2.2.2.2.m2.1d">italic_Î² start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>=0.9,0.999</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.5.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">batch size</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.5.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">4</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.6.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">epoch</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.6.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">50</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.7.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">learning rate</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.7.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">{3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3}</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.8.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">learning rate schedule</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.8.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">step decay</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.9.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">schedule step size</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.9.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">10 epoch</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S1.T3.2.2.2.10.8.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">schedule gamma</th>
<td class="ltx_td ltx_align_center" id="S1.T3.2.2.2.10.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.5</td>
</tr>
<tr class="ltx_tr" id="S1.T3.2.2.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S1.T3.2.2.2.11.9.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">augmentation</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S1.T3.2.2.2.11.9.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_text" id="S1.T3.2.2.2.11.9.2.1">Resize</span>(1024, 1024)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> <span class="ltx_text ltx_font_bold" id="S1.T3.4.1">The Training Setting for Our Experiments.</span> </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Additional Controlled Experiments</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We provide the study of the hyper-parameter setting of SimMAT by applying it to the Polarization modality. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S2.F6" title="Figure 6 â€£ 2 Additional Controlled Experiments â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">6</span></a>, SimMAT stack the <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">n</span> convolutional layers with <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">k</span> kernel size and dimension <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">d</span>. SimMAT achieves best 72.7% mIoU by setting <span class="ltx_text ltx_font_italic" id="S2.p1.1.4">n</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.5">k</span>, <span class="ltx_text ltx_font_italic" id="S2.p1.1.6">d</span>} as {2, 3, 64}. Further increasing the number of stacked layers and dimensional does not bring additional improvements, we suspect it is caused by the factor that introducing more trainable parameters makes training of SimMAT more challenging.
Note that when the kernel size is set to 1 and layers are set to 2, the implementation is the same as an MLP layer adopted in contrastive learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib45" title="">45</a>]</cite>. When the kernel size is set to 1 and layers are set to 1, the implementation is the same as a simple linear layer. One can observe that setting kernel size to 3 achieves peak performance with the best tradeoff between the receptive field and trainable parameters.</p>
</div>
<figure class="ltx_figure" id="S2.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle ltx_transformed_outer" id="S2.F6.3" style="width:130.1pt;height:105.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(142.5pt,-34.5pt) scale(2.91890283998548,2.91890283998548) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.F6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.F6.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.F6.3.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.F6.3.1.1.1.1.1">k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.3.1.1.1.2">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.3.1.1.1.3">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.3.1.1.1.4">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.F6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.F6.3.1.2.1.1">mIoU(%)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.3.1.2.1.2">71.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.3.1.2.1.3">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.3.1.2.1.4">71.3</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S2.F6.4" style="width:130.1pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.F6.4.1" style="width:433.6pt;height:105.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(142.5pt,-34.5pt) scale(2.91890283998548,2.91890283998548) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.F6.4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.F6.4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.F6.4.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.F6.4.1.1.1.1.1.1">d</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.4.1.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.4.1.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.4.1.1.1.1.4">96</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.F6.4.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.F6.4.1.1.2.1.1">mIoU(%)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.4.1.1.2.1.2">71.5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.4.1.1.2.1.3">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.4.1.1.2.1.4">72.7</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle ltx_transformed_outer" id="S2.F6.5" style="width:151.8pt;height:106pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(106.4pt,-26.0pt) scale(1.963558759719,1.963558759719) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.F6.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.F6.5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.F6.5.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S2.F6.5.1.1.1.1.1">n</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.5.1.1.1.2">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.5.1.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.5.1.1.1.4">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.5.1.1.1.5">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.F6.5.1.1.1.6">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.F6.5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.F6.5.1.2.1.1">mIoU(%)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.F6.5.1.2.1.2">69.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.F6.5.1.2.1.3">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.F6.5.1.2.1.4">71.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.F6.5.1.2.1.5">71.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.F6.5.1.2.1.6">71.8</td>
</tr>
<tr class="ltx_tr" id="S2.F6.5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.F6.5.1.3.2.1">Params(K)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.5.1.3.2.2">0.03</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.5.1.3.2.3">5.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.5.1.3.2.4">42.3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.5.1.3.2.5">79.3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.F6.5.1.3.2.6">116.2</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S2.F6.6" style="width:130.1pt;">The effect of kernel size.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S2.F6.7" style="width:130.1pt;">The effect of dimension.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S2.F6.8" style="width:151.8pt;">The effect of layers.</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S2.F6.10.1">The Effect of the Configuration of our MAT layers, evaluated on Polarization modality.</span> Based on the above results, we set the <math alttext="k,d,n" class="ltx_Math" display="inline" id="S2.F6.2.m1.3"><semantics id="S2.F6.2.m1.3b"><mrow id="S2.F6.2.m1.3.4.2" xref="S2.F6.2.m1.3.4.1.cmml"><mi id="S2.F6.2.m1.1.1" xref="S2.F6.2.m1.1.1.cmml">k</mi><mo id="S2.F6.2.m1.3.4.2.1" xref="S2.F6.2.m1.3.4.1.cmml">,</mo><mi id="S2.F6.2.m1.2.2" xref="S2.F6.2.m1.2.2.cmml">d</mi><mo id="S2.F6.2.m1.3.4.2.2" xref="S2.F6.2.m1.3.4.1.cmml">,</mo><mi id="S2.F6.2.m1.3.3" xref="S2.F6.2.m1.3.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F6.2.m1.3c"><list id="S2.F6.2.m1.3.4.1.cmml" xref="S2.F6.2.m1.3.4.2"><ci id="S2.F6.2.m1.1.1.cmml" xref="S2.F6.2.m1.1.1">ğ‘˜</ci><ci id="S2.F6.2.m1.2.2.cmml" xref="S2.F6.2.m1.2.2">ğ‘‘</ci><ci id="S2.F6.2.m1.3.3.cmml" xref="S2.F6.2.m1.3.3">ğ‘›</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.F6.2.m1.3d">k,d,n</annotation><annotation encoding="application/x-llamapun" id="S2.F6.2.m1.3e">italic_k , italic_d , italic_n</annotation></semantics></math> to 3, 64, and 2, respectively, considering the trade-off of performance of efficiency.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Additional Comparisons</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We report the training curve of SimMAT and baselines on the Polarization dataset in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S3.F7" title="Figure 7 â€£ 3 Additional Comparisons â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">7</span></a>.
One can observe the training from scratch only achieves 25.43% mIoU, significantly worse than other methods using prestrained weight as initialization.
To tackle the channel misalignment between RGB modality and new modality input, two straightforward ideas are to build a new randomly initialized patch embedding or prepend a 1<math alttext="\times" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><times id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">Ã—</annotation></semantics></math>1 convolution layer for dimension projection. While these two methods achieve significant improvement over training from scratch, their performance is suboptimal. Our SimMAT achieves a better performance over these two commonly adopted naive baselines.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Besides, we compare our SimMAT to two SOTA methods with pseudo new modality (RGBX) input.
<span class="ltx_text ltx_font_bold" id="S3.p2.1.1">ViPT</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib33" title="">33</a>]</cite> introduce a modality-complementary prompter (MCP) block to fuse features from RGB and other modalities like thermal and depth.
<span class="ltx_text ltx_font_bold" id="S3.p2.1.2">CMX</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib106" title="">106</a>]</cite> replicate the pretrained RGB encoder to tackle X modality, and place the proposed Feature Rectification Module (FRM) after each block to perform interaction of RGB features and X features. <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">Note that these two baselines utilize the prior information about which channels are for RGB embedding while our framework does not utilize this information.</span>
We reimplement the above two methods on SAM following their original finetuning methods and evaluate their performance on our benchmark.
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S3.T4" title="Table 4 â€£ 3 Additional Comparisons â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">4</span></a>, CMXÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib106" title="">106</a>]</cite> does not achieve satisfying performance on finetuning the foundation model SAM. We suspect the unsatisfying performance is caused by the noise introduced from FRM, which appended after each block deviates the features from its original distribution, making the learning difficult.
While ViPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib33" title="">33</a>]</cite> can achieve reasonable performance, its performance lags behind SimMAT.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S3.F7.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S3.F7.2.1">The Training Curves for SimMAT and Baselines.</span> SimMAT achieves the best performance.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T4.1" style="width:411.9pt;height:103.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.3pt,2.1pt) scale(0.961064713187116,0.961064713187116) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T4.1.1.1.1.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.1.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T4.1.1.1.1.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.2.1" style="font-size:90%;">Params</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T4.1.1.1.1.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.3.1" style="font-size:90%;">Finetuning methods</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.1.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.4.1" style="font-size:90%;">RGB-T</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.1.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.5.1" style="font-size:90%;">RGB-D</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.1.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.6.1" style="font-size:90%;">RGB-HHA</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.1.7" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.1.1.7.1" style="font-size:90%;">RGB-NIR</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.2.2.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_text" id="S3.T4.1.1.2.2.1.1" style="font-size:90%;">CMX*Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.1.1.2.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib106" title="">106</a><span class="ltx_text" id="S3.T4.1.1.2.2.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.2.2.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.2.1" style="font-size:90%;">403.8M</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.2.2.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.3.1" style="font-size:90%;">Full finetuning</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.2.2.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.4.1" style="font-size:90%;">44.91</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.2.2.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.5.1" style="font-size:90%;">36.41</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.2.2.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.6.1" style="font-size:90%;">37.33</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.2.2.7" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.2.2.7.1" style="font-size:90%;">34.75</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.1.1.3.3.1" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span class="ltx_text" id="S3.T4.1.1.3.3.1.1" style="font-size:90%;">ViPT*Â </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T4.1.1.3.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib33" title="">33</a><span class="ltx_text" id="S3.T4.1.1.3.3.1.3.2" style="font-size:90%;">]</span></cite>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S3.T4.1.1.3.3.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.2.1" style="font-size:90%;">94.5M</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S3.T4.1.1.3.3.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.3.1" style="font-size:90%;">Prompt tuning</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.3.3.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.4.1" style="font-size:90%;">75.93</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.3.3.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.5.1" style="font-size:90%;">48.89</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.3.3.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.6.1" style="font-size:90%;">49.50</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.3.3.7" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.3.3.7.1" style="font-size:90%;">51.90</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.4.4.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.1.1" style="font-size:90%;">SimMAT</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T4.1.1.4.4.2" rowspan="3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.2.1" style="font-size:90%;">94.4M</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.1.1.4.4.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.3.1" style="font-size:90%;">LoRA</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.4.4.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.4.1" style="font-size:90%;">84.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.4.4.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.5.1" style="font-size:90%;">57.56</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.4.4.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.4.4.6.1" style="font-size:90%;">56.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" id="S3.T4.1.1.4.4.7" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.4.4.7.1" style="font-size:90%;">57.14</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T4.1.1.5.5.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.5.5.1.1" style="font-size:90%;">SimMAT</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S3.T4.1.1.5.5.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.5.5.2.1" style="font-size:90%;">MLP Adapter</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.5.5.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.5.5.3.1" style="font-size:90%;">85.29</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.5.5.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.5.5.4.1" style="font-size:90%;">57.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.5.5.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.5.5.5.1" style="font-size:90%;">57.25</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center" id="S3.T4.1.1.5.5.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.5.5.6.1" style="font-size:90%;">55.81</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T4.1.1.6.6.1" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.1.1" style="font-size:90%;">SimMAT</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T4.1.1.6.6.2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.2.1" style="font-size:90%;">Full finetuning</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T4.1.1.6.6.3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.3.1" style="font-size:90%;">82.68</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T4.1.1.6.6.4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.4.1" style="font-size:90%;">56.96</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T4.1.1.6.6.5" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.5.1" style="font-size:90%;">57.17</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" id="S3.T4.1.1.6.6.6" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span class="ltx_text" id="S3.T4.1.1.6.6.6.1" style="font-size:90%;">56.37</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="S3.T4.5.1">Comparison of SimMAT with Other Methods Tackling Pseudo New Modality (RGBX).</span> While with fewer parameters, SimMAT achieves better performance across four pseudo new modalities. Note that ViPT and CMX can tackle RGBX only. * means reproduced implementation in SAM.
</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Additional Details for Benchmark</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To study the problem of cross-modality transfer learning of SAM, we construct a new benchmark by collecting image segmentation datasets from different modalities, as described in the main paper. However, the segmentation labels of SAM are instance-level segmentation, but some segmentation datasets (<span class="ltx_text ltx_font_italic" id="S4.p1.1.1">e.g.,</span> ZJU-RGBPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib35" title="">35</a>]</cite>, NYUv2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#bib.bib36" title="">36</a>]</cite>) only provide semantic labels. Hence, to align with the output of SAM, we perform post-processing to convert the semantic labels to instance labels by decomposing non-connected components.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S4.F8" title="Figure 8 â€£ 4 Additional Details for Benchmark â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">8</span></a> shows the post-processing effect. Given a semantic map label, we partition it into separate masks if they are not pixel-connected to each other. Each separate mask serves as an instance label and is responsible only for the clicks that lie within it.
The evaluation metric IoU is calculated for each instance. Instead of average IoU over semantic categories, we take the average IoU of all instances as the mIoU results.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="341" id="S4.F8.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold" id="S4.F8.2.1">The Illustration of Segmentation Generation Pipeline in Our Benchmark.</span> The semantic-level segmentation ground truth is split into instance-level segmentation ground truth.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Additional Qualitative Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We provide more qualitative visualization results from FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S5.F9" title="Figure 9 â€£ 5 Additional Qualitative Results â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">9</span></a> to FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S5.F13" title="Figure 13 â€£ 5 Additional Qualitative Results â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">13</span></a>. For the SAM zero-shot performance, we use the provided RGB reference as the input. We present the results on diverse image modalities for better understanding. As shown in the figure, the performance of training from scratch and zero-shot is generally unsatisfying. With our proposed SimMAT framework, the segmentation performance can be improved significantly.
For example, in the first column of thermal modality in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.08083v1#S5.F10" title="Figure 10 â€£ 5 Additional Qualitative Results â€£ SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality"><span class="ltx_text ltx_ref_tag">10</span></a>, we can see that both training from scratch and zero-shot fail to segment the â€œwindowâ€ completely. As a comparison, our method achieves accurate segmentation, which is quite close to the ground truth mask.</p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="934" id="S5.F9.g1" src="x8.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold" id="S5.F9.2.1">Additional Qualitative Results in Depth Modality. </span> Our approach can perform better than zero-shot and training from scratch. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="934" id="S5.F10.g1" src="x9.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold" id="S5.F10.2.1">Additional Qualitative Results in Thermal Modality. </span> Our approach can perform better than zero-shot and training from scratch. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="934" id="S5.F11.g1" src="x10.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold" id="S5.F11.2.1">Additional Qualitative Results in Polarization Modality. </span> Our approach can perform better than zero-shot and training from scratch. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="934" id="S5.F12.g1" src="x11.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold" id="S5.F12.2.1">Additional Qualitative Results in NIR Modality. </span> Our approach can perform better than zero-shot and training from scratch. </figcaption>
</figure>
<figure class="ltx_figure" id="S5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="934" id="S5.F13.g1" src="x12.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span class="ltx_text ltx_font_bold" id="S5.F13.2.1">Additional Qualitative Results in HHA Modality. </span> Our approach can perform better than zero-shot and training from scratch. </figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 14:37:07 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
