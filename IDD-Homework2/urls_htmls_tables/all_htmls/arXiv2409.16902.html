<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2</title>
<!--Generated on Wed Sep 25 13:06:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.16902v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S1" title="In Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S2" title="In Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>UW-COT Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S3" title="In Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S4" title="In Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chunhui Zhang<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">1,2,3</span></sup>, Li Liu<sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">2</span></sup>, Guanjie Huang<sup class="ltx_sup" id="id13.13.id3"><span class="ltx_text ltx_font_italic" id="id13.13.id3.1">2</span></sup>, Hao Wen<sup class="ltx_sup" id="id14.14.id4"><span class="ltx_text ltx_font_italic" id="id14.14.id4.1">3</span></sup>, Xi Zhou<sup class="ltx_sup" id="id15.15.id5"><span class="ltx_text ltx_font_italic" id="id15.15.id5.1">3</span></sup>, Yanfeng Wang<sup class="ltx_sup" id="id16.16.id6"><span class="ltx_text ltx_font_italic" id="id16.16.id6.1">1,4</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id7"><span class="ltx_text ltx_font_italic" id="id17.17.id7.1">1</span></sup> Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai 200240, China
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id8"><span class="ltx_text ltx_font_italic" id="id18.18.id8.1">2</span></sup> The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id9"><span class="ltx_text ltx_font_italic" id="id19.19.id9.1">3</span></sup> CloudWalk Technology Co., Ltd, 201203, China
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id10"><span class="ltx_text ltx_font_italic" id="id20.20.id10.1">4</span></sup> Shanghai AI Laboratory, Shanghai 200232, China
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Corresponding author. E-mail: avrillliu@hkust-gz.edu.cn.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id21.id1">Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale training datasets. However, existing tracking datasets are primarily focused on open-air scenarios, which greatly limits the development of object tracking in underwater environments. To address this issue, we take a step forward by proposing the first large-scale underwater camouflaged object tracking dataset, namely UW-COT. Based on the proposed dataset, this paper presents an experimental evaluation of several advanced visual object tracking methods and the latest advancements in image and video segmentation. Specifically, we compare the performance of the Segment Anything Model (SAM) and its updated version, SAM 2, in challenging underwater environments. Our findings highlight the improvements in SAM 2 over SAM, demonstrating its enhanced capability to handle the complexities of underwater camouflaged objects. Compared to current advanced visual object tracking methods, the latest video segmentation foundation model SAM 2 also exhibits significant advantages, providing valuable insights into the development of more effective tracking technologies for underwater scenarios. The dataset will be accessible at <span class="ltx_text" id="id21.id1.1" style="color:#FF00FF;">https://github.com/983632847/Awesome-Multimodal-Object-Tracking.</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Visual object tracking (VOT) involves continuously locating a target object within a video sequence and has applications in autonomous vehicles <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">xie2024diffusiontrack</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">wang2024elysium</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">wang2024event</span> </a></cite>, surveillance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">zhang2024awesome</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">cai2024hiptrack</span> </a></cite>, and robotics <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">zhang2022webuav</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">zhang2023all</span> </a></cite>. Its significance lies in enabling machines to perceive and interpret dynamic environments, supporting tasks like motion analysis and decision-making <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">li2024visual</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">hu2024multi</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">peng2024vasttrack</span> </a></cite>. Although significant progress has been made in terrestrial and open-air scenarios <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">huang2019got</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">fan2021lasot</span> </a></cite>, tracking in underwater environments remains challenging due to factors like visual camouflage, light scattering, and low contrast, which limit the effectiveness of conventional algorithms <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">zhang2024webuot</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">alawode2023improving</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">alawode2022utb180</span> </a></cite>. Consequently, there is a pressing need for specialized datasets and methods to tackle the complexities of underwater tracking, especially when objects blend with their surroundings, known as camouflaged objects. <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">However, despite its importance, underwater camouflaged object tracking remains an unexplored field.</em></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Current VOT methods can be broadly categorized into several types. Traditional correlation filter-based methods <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">zhang2019robust</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">ge2020cascaded</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">ge2019distilling</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">zhang2020accurate</span> </a></cite> utilize efficient convolution operations to track objects, whereas Siamese-based methods <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">bertinetto2016fully</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">li2019siamrpn++</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">li2018high</span> </a></cite> employ dual-stream networks to learn a similarity function between the target object and the search region. Transformer-based methods, such as OSTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">ye2022joint</span> </a></cite>, SeqTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">chen2023seqtrack</span> </a></cite>, and ARTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">wei2023autoregressive</span> </a></cite>, leverage the attention mechanism to model complex dependencies in sequences, resulting in enhanced tracking precision. In recent years, Mamba-based approaches like Mamba-FETrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">huang2024mamba</span> </a></cite>, and MambaVT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">lai2024mambavt</span> </a></cite> have emerged, offering robust performance through feature enhancement and advanced attention mechanisms. In addition to these methods, SAM-based models have also gained traction. These models, such as SAM-DA <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">fu2023sam</span> </a></cite>, Tracking Anything <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">yang2023track</span> </a></cite>, SAM-Track <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">cheng2023segment</span> </a></cite>, and the latest SAM 2 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ravi2024sam</span> </a></cite>, focus on the precise segmentation of objects across frames, making them particularly suitable for challenging environments like underwater scenarios where object boundaries are difficult to discern.</p>
</div>
<figure class="ltx_figure ltx_minipage ltx_align_middle" id="S1.F1.1" style="width:433.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="321" id="S1.F1.1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Some examples come from the constructed UW-COT dataset. A camouflaged underwater target object is annotated in each video sequence.</figcaption>
</figure>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of UW-COT with existing camouflaged object tracking dataset (COTD <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">guo2024cotd</span> </a></cite>) and video camouflaged object detection datasets (CAD <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">bideau2016s</span> </a></cite> and MoCA-Mask <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">cheng2022implicit</span> </a></cite>). UW-COT is currently the largest underwater camouflaged object tracking dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.12" style="width:552.6pt;height:132.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1">
<tr class="ltx_tr" id="S1.T1.12.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S1.T1.12.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span><span class="ltx_text" id="S1.T1.12.1.1.1.1" style="font-size:80%;"> Dataset</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.2.1" style="font-size:80%;">Year</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.3.1" style="font-size:80%;">Videos</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.4.1" style="font-size:80%;">Classes</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.5" style="padding-left:1.5pt;padding-right:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1.1.5.1">
<tr class="ltx_tr" id="S1.T1.12.1.1.5.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.5.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.5.1.1.1.1" style="font-size:80%;">Min</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.1.5.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.5.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.5.1.2.1.1" style="font-size:80%;">frame</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.6" style="padding-left:1.5pt;padding-right:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1.1.6.1">
<tr class="ltx_tr" id="S1.T1.12.1.1.6.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.6.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.6.1.1.1.1" style="font-size:80%;">Mean</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.1.6.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.6.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.6.1.2.1.1" style="font-size:80%;">frame</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.7" style="padding-left:1.5pt;padding-right:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1.1.7.1">
<tr class="ltx_tr" id="S1.T1.12.1.1.7.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.7.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.7.1.1.1.1" style="font-size:80%;">Max</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.1.7.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.7.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.7.1.2.1.1" style="font-size:80%;">frame</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.8" style="padding-left:1.5pt;padding-right:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1.1.8.1">
<tr class="ltx_tr" id="S1.T1.12.1.1.8.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.8.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.8.1.1.1.1" style="font-size:80%;">Total</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.1.8.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.8.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.8.1.2.1.1" style="font-size:80%;">frames</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.9" style="padding-left:1.5pt;padding-right:1.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.12.1.1.9.1">
<tr class="ltx_tr" id="S1.T1.12.1.1.9.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.9.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.9.1.1.1.1" style="font-size:80%;">Labeled</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.1.9.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.9.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.9.1.2.1.1" style="font-size:80%;">frames</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.10" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.10.1" style="font-size:80%;">Annotation</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.11" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.11.1" style="font-size:80%;">Link</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.1.12" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.1.12.1" style="font-size:80%;">Underwater</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S1.T1.12.1.2.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text ltx_font_bold" id="S1.T1.12.1.2.1.1" style="font-size:80%;">CAD</span><span class="ltx_text" id="S1.T1.12.1.2.1.2" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">bideau2016s</span> </a></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.2.1" style="font-size:80%;">2016</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.3.1" style="font-size:80%;">9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.4.1" style="font-size:80%;">6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.5.1" style="font-size:80%;">30</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.6.1" style="font-size:80%;">93</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.7.1" style="font-size:80%;">218</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.8.1" style="font-size:80%;">836</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.9" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.9.1" style="font-size:80%;">191</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.10" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.10.1" style="font-size:80%;">Mask</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.11" style="padding-left:1.5pt;padding-right:1.5pt;"><a class="ltx_ref ltx_href" href="http://vis-www.cs.umass.edu/motionSegmentation/" style="font-size:80%;color:#FF00FF;" title="">URL</a></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.2.12" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.2.12.1" style="font-size:80%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S1.T1.12.1.3.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text ltx_font_bold" id="S1.T1.12.1.3.1.1" style="font-size:80%;">MoCA-Mask</span><span class="ltx_text" id="S1.T1.12.1.3.1.2" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">cheng2022implicit</span> </a></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.2.1" style="font-size:80%;">2022</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.3.1" style="font-size:80%;">87</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.4.1" style="font-size:80%;">45</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.5.1" style="font-size:80%;">23</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.6.1" style="font-size:80%;">264</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.7.1" style="font-size:80%;">1,296</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.8.1" style="font-size:80%;">23 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.9" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.9.1" style="font-size:80%;">23 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.10" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.10.1" style="font-size:80%;">Mask</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.11" style="padding-left:1.5pt;padding-right:1.5pt;"><a class="ltx_ref ltx_href" href="https://xueliancheng.github.io/SLT-Net-project/" style="font-size:80%;color:#FF00FF;" title="">URL</a></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.3.12" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.3.12.1" style="font-size:80%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S1.T1.12.1.4.1" style="padding-left:1.5pt;padding-right:1.5pt;">
<span class="ltx_text ltx_font_bold" id="S1.T1.12.1.4.1.1" style="font-size:80%;">COTD</span><span class="ltx_text" id="S1.T1.12.1.4.1.2" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">guo2024cotd</span> </a></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.2.1" style="font-size:80%;">2024</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.3.1" style="font-size:80%;">200</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.4.1" style="font-size:80%;">20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.6.1" style="font-size:80%;">400</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.8.1" style="font-size:80%;">80 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.9" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.9.1" style="font-size:80%;">80 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.10" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.10.1" style="font-size:80%;">BBox</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.11" style="padding-left:1.5pt;padding-right:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/openat25/HIPTrack-MLS" style="font-size:80%;color:#FF00FF;" title="">URL</a></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S1.T1.12.1.4.12" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.4.12.1" style="font-size:80%;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S1.T1.12.1.5.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.12.1.5.1.1" style="font-size:80%;">UW-COT (Ours)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.2" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.2.1" style="font-size:80%;">2024</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.3" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.3.1" style="font-size:80%;">220</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.4" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.4.1" style="font-size:80%;">96</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.5" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.5.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.6" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.6.1" style="font-size:80%;">722</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.7" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.7.1" style="font-size:80%;">7,448</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.8" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.8.1" style="font-size:80%;">159 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.9" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.9.1" style="font-size:80%;">159 K</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.10" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.10.1" style="font-size:80%;">BBox+Mask</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.11" style="padding-left:1.5pt;padding-right:1.5pt;"><a class="ltx_ref ltx_href" href="https://github.com/983632847/Awesome-Multimodal-Object-Tracking" style="font-size:80%;color:#FF00FF;" title="">URL</a></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S1.T1.12.1.5.12" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_text" id="S1.T1.12.1.5.12.1" style="font-size:80%;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.12.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S1.T1.12.1.6.1" style="padding-left:1.5pt;padding-right:1.5pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S1.T1.12.1.6.2" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.3" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.4" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.5" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.6" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.7" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.8" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.9" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.10" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.11" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
<td class="ltx_td" id="S1.T1.12.1.6.12" style="padding-left:1.5pt;padding-right:1.5pt;"></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Following the latest large-scale underwater object tracking dataset (<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">i.e.</span>, WebUOT-1M <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">zhang2024webuot</span> </a></cite>), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S1.F1.1" title="Figure 1 ‣ 1 Introduction ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">1</span></a> and Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">1</span></a>), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (<span class="ltx_text ltx_font_italic" id="S1.p3.1.2">e.g.</span>, HQ-SAM <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">ke2024segment</span> </a></cite> and SAM-2 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ravi2024sam</span> </a></cite>). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>UW-COT Dataset</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.5">Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (<span class="ltx_text ltx_font_italic" id="S2.p1.5.1">e.g.</span>, YouTube<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.youtube.com/</span></span></span>) and existing tracking datasets (<span class="ltx_text ltx_font_italic" id="S2.p1.5.2">e.g.</span>, WebUOT-1M <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">zhang2024webuot</span> </a></cite> and VastTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">peng2024vasttrack</span> </a></cite>), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasets <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">huang2019got</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">fan2019lasot</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">zhang2022webuav</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">peng2024vasttrack</span> </a>; <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">zhang2024webuot</span> </a></cite>, we provide a bounding box annotation for the camouflaged object in each frame, represented as <math alttext="[x,y,w,h]" class="ltx_Math" display="inline" id="S2.p1.1.m1.4"><semantics id="S2.p1.1.m1.4a"><mrow id="S2.p1.1.m1.4.5.2" xref="S2.p1.1.m1.4.5.1.cmml"><mo id="S2.p1.1.m1.4.5.2.1" stretchy="false" xref="S2.p1.1.m1.4.5.1.cmml">[</mo><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">x</mi><mo id="S2.p1.1.m1.4.5.2.2" xref="S2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">y</mi><mo id="S2.p1.1.m1.4.5.2.3" xref="S2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">w</mi><mo id="S2.p1.1.m1.4.5.2.4" xref="S2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S2.p1.1.m1.4.4" xref="S2.p1.1.m1.4.4.cmml">h</mi><mo id="S2.p1.1.m1.4.5.2.5" stretchy="false" xref="S2.p1.1.m1.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.4b"><list id="S2.p1.1.m1.4.5.1.cmml" xref="S2.p1.1.m1.4.5.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑥</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">𝑦</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">𝑤</ci><ci id="S2.p1.1.m1.4.4.cmml" xref="S2.p1.1.m1.4.4">ℎ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.4c">[x,y,w,h]</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.4d">[ italic_x , italic_y , italic_w , italic_h ]</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_x</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">italic_y</annotation></semantics></math> denote the coordinates of the top-left corner of the object, and <math alttext="w" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">w</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.1d">italic_w</annotation></semantics></math> and <math alttext="h" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.1d">italic_h</annotation></semantics></math> indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (<span class="ltx_text ltx_font_italic" id="S2.p1.5.3">e.g.</span>, HQ-SAM <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">ke2024segment</span> </a></cite> and SAM-2 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ravi2024sam</span> </a></cite>) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S1.F1.1" title="Figure 1 ‣ 1 Introduction ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">1</span></a> and Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">1</span></a>. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="212" id="S2.F2.sf1.g1" src="x2.png" width="207"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="212" id="S2.F2.sf2.g1" src="x3.png" width="207"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="212" id="S2.F2.sf3.g1" src="x4.png" width="207"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="212" id="S2.F2.sf4.g1" src="x5.png" width="207"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall performance on the UW-COT dataset using AUC, Pre, cAUC, and nPre scores.</figcaption>
</figure>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Impact of using different points as the point prompt for SAM 2 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ravi2024sam</span> </a></cite>. We report AUC, nPre, Pre, cAUC, and mACC scores on the UW-COT dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.1" style="width:418.1pt;height:180pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span> Method</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">Point type</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">AUC (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">nPre (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">Pre (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">cAUC (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">mACC (%)</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.2.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S2.T2.1.1.2.1.1">SAM 2-tiny</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">Center point</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">52.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">50.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">51.7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.3">
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">Random point</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">36.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">40.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">36.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">35.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">36.5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.4.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S2.T2.1.1.4.1.1">SAM 2-small</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">Center point</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">51.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">58.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">52.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">50.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4.7" style="padding-left:4.3pt;padding-right:4.3pt;">51.5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.5">
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">Random point</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">35.4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">39.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">34.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">34.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">35.0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.6.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S2.T2.1.1.6.1.1">SAM 2-base plus</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">Center point</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.4" style="padding-left:4.3pt;padding-right:4.3pt;">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.5" style="padding-left:4.3pt;padding-right:4.3pt;">54.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.6" style="padding-left:4.3pt;padding-right:4.3pt;">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6.7" style="padding-left:4.3pt;padding-right:4.3pt;">53.7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.7">
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">Random point</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.2" style="padding-left:4.3pt;padding-right:4.3pt;">39.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.3" style="padding-left:4.3pt;padding-right:4.3pt;">44.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.4" style="padding-left:4.3pt;padding-right:4.3pt;">39.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.5" style="padding-left:4.3pt;padding-right:4.3pt;">38.5</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.7.6" style="padding-left:4.3pt;padding-right:4.3pt;">39.2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.8.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S2.T2.1.1.8.1.1">SAM 2-large</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">Center point</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">58.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.4" style="padding-left:4.3pt;padding-right:4.3pt;">65.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.5" style="padding-left:4.3pt;padding-right:4.3pt;">61.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.6" style="padding-left:4.3pt;padding-right:4.3pt;">58.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.8.7" style="padding-left:4.3pt;padding-right:4.3pt;">59.3</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.9">
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">Random point</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.2" style="padding-left:4.3pt;padding-right:4.3pt;">43.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.3" style="padding-left:4.3pt;padding-right:4.3pt;">48.3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.4" style="padding-left:4.3pt;padding-right:4.3pt;">44.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.5" style="padding-left:4.3pt;padding-right:4.3pt;">42.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.9.6" style="padding-left:4.3pt;padding-right:4.3pt;">43.6</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.10">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.10.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S2.T2.1.1.10.2" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td" id="S2.T2.1.1.10.3" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td" id="S2.T2.1.1.10.4" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td" id="S2.T2.1.1.10.5" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td" id="S2.T2.1.1.10.6" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td" id="S2.T2.1.1.10.7" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S2.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of different models for SAM 2 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">ravi2024sam</span> </a></cite>. We report AUC, nPre, Pre, cAUC, and mACC scores on the UW-COT dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T3.1" style="width:421.9pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1">
<tr class="ltx_tr" id="S2.T3.1.1.1">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.1.1" style="padding-left:2.3pt;padding-right:2.3pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span> Method</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.2" style="padding-left:2.3pt;padding-right:2.3pt;">Size (M)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3" style="padding-left:2.3pt;padding-right:2.3pt;">Speed (FPS)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.4" style="padding-left:2.3pt;padding-right:2.3pt;">AUC (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5" style="padding-left:2.3pt;padding-right:2.3pt;">nPre (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.6" style="padding-left:2.3pt;padding-right:2.3pt;">Pre (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.7" style="padding-left:2.3pt;padding-right:2.3pt;">cAUC (%)</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.8" style="padding-left:2.3pt;padding-right:2.3pt;">mACC (%)</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T3.1.1.2.1" style="padding-left:2.3pt;padding-right:2.3pt;">SAM 2-tiny</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.2" style="padding-left:2.3pt;padding-right:2.3pt;">38.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.3" style="padding-left:2.3pt;padding-right:2.3pt;">47.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.4" style="padding-left:2.3pt;padding-right:2.3pt;">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.5" style="padding-left:2.3pt;padding-right:2.3pt;">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.6" style="padding-left:2.3pt;padding-right:2.3pt;">52.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.7" style="padding-left:2.3pt;padding-right:2.3pt;">50.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.1.2.8" style="padding-left:2.3pt;padding-right:2.3pt;">51.7</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.3">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.3.1" style="padding-left:2.3pt;padding-right:2.3pt;">SAM 2-small</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.2" style="padding-left:2.3pt;padding-right:2.3pt;">46</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.3" style="padding-left:2.3pt;padding-right:2.3pt;">43.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.4" style="padding-left:2.3pt;padding-right:2.3pt;">51.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.5" style="padding-left:2.3pt;padding-right:2.3pt;">58.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.6" style="padding-left:2.3pt;padding-right:2.3pt;">52.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.7" style="padding-left:2.3pt;padding-right:2.3pt;">50.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.3.8" style="padding-left:2.3pt;padding-right:2.3pt;">51.5</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.4">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.4.1" style="padding-left:2.3pt;padding-right:2.3pt;">SAM 2-base plus</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.2" style="padding-left:2.3pt;padding-right:2.3pt;">80.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.3" style="padding-left:2.3pt;padding-right:2.3pt;">34.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.4" style="padding-left:2.3pt;padding-right:2.3pt;">53.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.5" style="padding-left:2.3pt;padding-right:2.3pt;">60.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.6" style="padding-left:2.3pt;padding-right:2.3pt;">54.5</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.7" style="padding-left:2.3pt;padding-right:2.3pt;">52.6</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.4.8" style="padding-left:2.3pt;padding-right:2.3pt;">53.7</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.5">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.5.1" style="padding-left:2.3pt;padding-right:2.3pt;">SAM 2-large</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.2" style="padding-left:2.3pt;padding-right:2.3pt;">224.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.3" style="padding-left:2.3pt;padding-right:2.3pt;">24.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.4" style="padding-left:2.3pt;padding-right:2.3pt;">58.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.5" style="padding-left:2.3pt;padding-right:2.3pt;">65.3</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.6" style="padding-left:2.3pt;padding-right:2.3pt;">61.7</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.7" style="padding-left:2.3pt;padding-right:2.3pt;">58.0</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.5.8" style="padding-left:2.3pt;padding-right:2.3pt;">59.3</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.6">
<td class="ltx_td ltx_align_left" id="S2.T3.1.1.6.1" style="padding-left:2.3pt;padding-right:2.3pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S2.T3.1.1.6.2" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.3" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.4" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.5" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.6" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.7" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
<td class="ltx_td" id="S2.T3.1.1.6.8" style="padding-left:2.3pt;padding-right:2.3pt;"></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Main Results.</span> We evaluate SAM-based tracking methods (<span class="ltx_text ltx_font_italic" id="S3.p1.1.2">i.e.</span>, SAM-DA <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">fu2023sam</span> </a></cite>, and Tracking Anything <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">yang2023track</span> </a></cite>), SAM 2, and three current SOTA visual object tracking methods (<span class="ltx_text ltx_font_italic" id="S3.p1.1.3">i.e.</span>, OSTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">ye2022joint</span> </a></cite>, SeqTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">chen2023seqtrack</span> </a></cite>, and ARTrack <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">wei2023autoregressive</span> </a></cite>) on the proposed UW-COT dataset. The results are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S2.F2" title="Figure 2 ‣ 2 UW-COT Dataset ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">2</span></a>. Our observations are as follows: 1) SAM 2 outperforms SAM-based trackers (SAM-DA and Tracking-Anything) on UW-COT, which can be attributed to a series of improvements SAM 2 introduces over SAM for video and image tasks, such as improving temporal consistency, robustness to occlusions, feature embeddings, computational efficiency, motion estimation accuracy, generalization to new domains, and integration of contextual information. 2) SAM 2 achieves the best performance, surpassing current state-of-the-art VOT methods. As a foundation model for video segmentation, SAM 2’s success reflects a promising effort to address the dynamic challenges present in video data, such as fast motion, deformation, similar distractors, and occlusion, and to provide a more generalized solution for video object tracking and beyond.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Ablation Studies.</span> We take SAM 2 as an example to explore the impact of different ways of point prompts (<span class="ltx_text ltx_font_italic" id="S3.p2.1.2">i.e.</span>, center point and random point within the initial target box) and model sizes. From Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S2.T2" title="Table 2 ‣ 2 UW-COT Dataset ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">2</span></a>, we find that using the center point as a prompt yields significantly better results than using random points. This suggests that for the interactive segmentation model SAM 2, the quality of the prompt is very important. The results in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.16902v1#S2.T3" title="Table 3 ‣ 2 UW-COT Dataset ‣ Towards Underwater Camouflaged Object Tracking: An Experimental Evaluation of SAM and SAM 2"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate that larger model sizes generally lead to better performance, but the speed of the model decreases significantly. We also discovered an interesting phenomenon: when the number of model parameters is relatively small, a smaller model (<span class="ltx_text ltx_font_italic" id="S3.p2.1.3">e.g.</span>, SAM 2-tiny) can even outperform a larger model (<span class="ltx_text ltx_font_italic" id="S3.p2.1.4">e.g.</span>, SAM 2-small). We suspect this may be due to overfitting, or that small models are more sensitive to the quality of the training data.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this paper, we introduce UW-COT, the first large-scale benchmark for underwater camouflaged object tracking, and demonstrate the superior performance of SAM 2 over other tracking methods, showcasing its potential for enhancing underwater tracking technologies. In the future, we plan to expand the scale and modalities of this dataset, as well as explore various underwater vision tasks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.2.2.1" style="font-size:90%;">(1)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.4.1" style="font-size:90%;">
Basit Alawode, Fayaz Ali Dharejo, Mehnaz Ummar, Yuhang Guo, Arif Mahmood, Naoufel Werghi, Fahad Shahbaz Khan, and Sajid Javed.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.5.1" style="font-size:90%;">Improving underwater visual tracking with a large scale dataset and image enhancement.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.6.1" style="font-size:90%;">arXiv preprint arXiv:2308.15816</span><span class="ltx_text" id="bib.bib1.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.2.2.1" style="font-size:90%;">(2)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.4.1" style="font-size:90%;">
Basit Alawode, Yuhang Guo, Mehnaz Ummar, Naoufel Werghi, Jorge Dias, Ajmal Mian, and Sajid Javed.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.5.1" style="font-size:90%;">Utb180: A high-quality benchmark for underwater tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.7.2" style="font-size:90%;">Proceedings of the Asian Conference on Computer Vision</span><span class="ltx_text" id="bib.bib2.8.3" style="font-size:90%;">, pages 3326–3342, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.2.2.1" style="font-size:90%;">(3)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.4.1" style="font-size:90%;">
Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.5.1" style="font-size:90%;">Fully-convolutional siamese networks for object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.7.2" style="font-size:90%;">European Conference on Computer Vision Workshop</span><span class="ltx_text" id="bib.bib3.8.3" style="font-size:90%;">, pages 850–865, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.2.2.1" style="font-size:90%;">(4)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.4.1" style="font-size:90%;">
Pia Bideau and Erik Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.5.1" style="font-size:90%;">It’s moving! a probabilistic model for causal motion segmentation in moving camera videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib4.7.2" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14</span><span class="ltx_text" id="bib.bib4.8.3" style="font-size:90%;">, pages 433–449. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.2.2.1" style="font-size:90%;">(5)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.4.1" style="font-size:90%;">
Wenrui Cai, Qingjie Liu, and Yunhong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.5.1" style="font-size:90%;">Hiptrack: Visual tracking with historical prompts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib5.8.3" style="font-size:90%;">, pages 19258–19267, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.2.2.1" style="font-size:90%;">(6)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.4.1" style="font-size:90%;">
Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.5.1" style="font-size:90%;">Seqtrack: Sequence to sequence learning for visual object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib6.8.3" style="font-size:90%;">, pages 14572–14581, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.2.2.1" style="font-size:90%;">(7)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.4.1" style="font-size:90%;">
Xuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.5.1" style="font-size:90%;">Implicit motion handling for video camouflaged object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib7.8.3" style="font-size:90%;">, pages 13864–13873, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.2.2.1" style="font-size:90%;">(8)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.4.1" style="font-size:90%;">
Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.5.1" style="font-size:90%;">Segment and track anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.6.1" style="font-size:90%;">arXiv preprint arXiv:2305.06558</span><span class="ltx_text" id="bib.bib8.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.2.2.1" style="font-size:90%;">(9)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.4.1" style="font-size:90%;">
Heng Fan, Hexin Bai, Liting Lin, and </span><span class="ltx_text ltx_font_italic" id="bib.bib9.5.2" style="font-size:90%;">et al.</span><span class="ltx_text" id="bib.bib9.6.3" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">Lasot: A high-quality large-scale single object tracking benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.8.1" style="font-size:90%;">International journal of computer vision</span><span class="ltx_text" id="bib.bib9.9.2" style="font-size:90%;">, 129(2):439–461, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.2.2.1" style="font-size:90%;">(10)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.4.1" style="font-size:90%;">
Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.5.1" style="font-size:90%;">Lasot: A high-quality benchmark for large-scale single object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib10.8.3" style="font-size:90%;">, pages 5374–5383, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.2.2.1" style="font-size:90%;">(11)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.4.1" style="font-size:90%;">
Changhong Fu, Liangliang Yao, Haobo Zuo, Guangze Zheng, and Jia Pan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.5.1" style="font-size:90%;">Sam-da: Uav tracks anything at night with sam-powered domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.6.1" style="font-size:90%;">arXiv preprint arXiv:2307.01024</span><span class="ltx_text" id="bib.bib11.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.2.2.1" style="font-size:90%;">(12)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.4.1" style="font-size:90%;">
Shiming Ge, Zhao Luo, Chunhui Zhang, Yingying Hua, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.5.1" style="font-size:90%;">Distilling channels for efficient deep tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.6.1" style="font-size:90%;">IEEE Transactions on Image Processing</span><span class="ltx_text" id="bib.bib12.7.2" style="font-size:90%;">, 29:2610–2621, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.2.2.1" style="font-size:90%;">(13)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.4.1" style="font-size:90%;">
Shiming Ge, Chunhui Zhang, Shikun Li, Dan Zeng, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.5.1" style="font-size:90%;">Cascaded correlation refinement for robust deep tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.6.1" style="font-size:90%;">IEEE transactions on neural networks and learning systems</span><span class="ltx_text" id="bib.bib13.7.2" style="font-size:90%;">, 32(3):1276–1288, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.2.2.1" style="font-size:90%;">(14)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.4.1" style="font-size:90%;">
Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.5.1" style="font-size:90%;">Camouflaged object tracking: A benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.6.1" style="font-size:90%;">arXiv preprint arXiv:2408.13877</span><span class="ltx_text" id="bib.bib14.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.2.2.1" style="font-size:90%;">(15)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.4.1" style="font-size:90%;">
Shiyu Hu, Dailing Zhang, Xiaokun Feng, Xuchen Li, Xin Zhao, Kaiqi Huang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.5.1" style="font-size:90%;">A multi-modal global instance tracking benchmark (mgit): Better locating target in complex spatio-temporal and causal relationship.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.6.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib15.7.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.2.2.1" style="font-size:90%;">(16)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.4.1" style="font-size:90%;">
Ju Huang, Shiao Wang, Shuai Wang, Zhe Wu, Xiao Wang, and Bo Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.5.1" style="font-size:90%;">Mamba-fetrack: Frame-event tracking via state space model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.6.1" style="font-size:90%;">arXiv preprint arXiv:2404.18174</span><span class="ltx_text" id="bib.bib16.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.2.2.1" style="font-size:90%;">(17)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.4.1" style="font-size:90%;">
Lianghua Huang, Xin Zhao, and Kaiqi Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.5.1" style="font-size:90%;">Got-10k: A large high-diversity benchmark for generic object tracking in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.6.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib17.7.2" style="font-size:90%;">, 43(5):1562–1577, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.2.2.1" style="font-size:90%;">(18)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.4.1" style="font-size:90%;">
Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.5.1" style="font-size:90%;">Segment anything in high quality.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.6.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib18.7.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.2.2.1" style="font-size:90%;">(19)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.4.1" style="font-size:90%;">
Simiao Lai, Chang Liu, Jiawen Zhu, Ben Kang, Yang Liu, Dong Wang, and Huchuan Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.5.1" style="font-size:90%;">Mambavt: Spatio-temporal contextual modeling for robust rgb-t tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.6.1" style="font-size:90%;">arXiv preprint arXiv:2408.07889</span><span class="ltx_text" id="bib.bib19.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.2.2.1" style="font-size:90%;">(20)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.4.1" style="font-size:90%;">
Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.5.1" style="font-size:90%;">Siamrpn++: Evolution of siamese visual tracking with very deep networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib20.8.3" style="font-size:90%;">, pages 4282–4291, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.2.2.1" style="font-size:90%;">(21)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.4.1" style="font-size:90%;">
Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.5.1" style="font-size:90%;">High performance visual tracking with siamese region proposal network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib21.7.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib21.8.3" style="font-size:90%;">, pages 8971–8980, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.2.2.1" style="font-size:90%;">(22)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.4.1" style="font-size:90%;">
Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, and Kaiqi Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.5.1" style="font-size:90%;">Visual language tracking with multi-modal interaction: A robust benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.6.1" style="font-size:90%;">arXiv preprint arXiv:2409.08887</span><span class="ltx_text" id="bib.bib22.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.2.2.1" style="font-size:90%;">(23)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.4.1" style="font-size:90%;">
Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, and Libo Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.5.1" style="font-size:90%;">Vasttrack: Vast category visual object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.6.1" style="font-size:90%;">arXiv preprint arXiv:2403.03493</span><span class="ltx_text" id="bib.bib23.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.2.2.1" style="font-size:90%;">(24)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.4.1" style="font-size:90%;">
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.5.1" style="font-size:90%;">Sam 2: Segment anything in images and videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.6.1" style="font-size:90%;">arXiv preprint arXiv:2408.00714</span><span class="ltx_text" id="bib.bib24.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.2.2.1" style="font-size:90%;">(25)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.4.1" style="font-size:90%;">
Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, and Can Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.5.1" style="font-size:90%;">Elysium: Exploring object-level perception in videos via mllm.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.6.1" style="font-size:90%;">arXiv preprint arXiv:2403.16558</span><span class="ltx_text" id="bib.bib25.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.2.2.1" style="font-size:90%;">(26)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.4.1" style="font-size:90%;">
Xiao Wang, Shiao Wang, Chuanming Tang, Lin Zhu, Bo Jiang, Yonghong Tian, and Jin Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.5.1" style="font-size:90%;">Event stream-based visual object tracking: A high-resolution benchmark dataset and a novel baseline.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib26.8.3" style="font-size:90%;">, pages 19248–19257, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.2.2.1" style="font-size:90%;">(27)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.4.1" style="font-size:90%;">
Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yihong Gong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.5.1" style="font-size:90%;">Autoregressive visual tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib27.8.3" style="font-size:90%;">, pages 9697–9706, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.2.2.1" style="font-size:90%;">(28)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.4.1" style="font-size:90%;">
Fei Xie, Zhongdao Wang, and Chao Ma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.5.1" style="font-size:90%;">Diffusiontrack: Point set diffusion model for visual object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.7.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib28.8.3" style="font-size:90%;">, pages 19113–19124, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.2.2.1" style="font-size:90%;">(29)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.4.1" style="font-size:90%;">
Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.5.1" style="font-size:90%;">Track anything: Segment anything meets videos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.6.1" style="font-size:90%;">arXiv preprint arXiv:2304.11968</span><span class="ltx_text" id="bib.bib29.7.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.2.2.1" style="font-size:90%;">(30)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.4.1" style="font-size:90%;">
Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.5.1" style="font-size:90%;">Joint feature learning and relation modeling for tracking: A one-stream framework.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.7.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib30.8.3" style="font-size:90%;">, pages 341–357, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.2.2.1" style="font-size:90%;">(31)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.4.1" style="font-size:90%;">
Chunhui Zhang, Shiming Ge, Yingying Hua, and Dan Zeng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.5.1" style="font-size:90%;">Robust deep tracking with two-step augmentation discriminative correlation filters.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.7.2" style="font-size:90%;">2019 IEEE International Conference on Multimedia and Expo (ICME)</span><span class="ltx_text" id="bib.bib31.8.3" style="font-size:90%;">, pages 1774–1779. IEEE, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.2.2.1" style="font-size:90%;">(32)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.4.1" style="font-size:90%;">
Chunhui Zhang, Shiming Ge, Kangkai Zhang, and Dan Zeng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.5.1" style="font-size:90%;">Accurate uav tracking with distance-injected overlap maximization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.7.2" style="font-size:90%;">Proceedings of the 28th ACM International Conference on Multimedia</span><span class="ltx_text" id="bib.bib32.8.3" style="font-size:90%;">, pages 565–573, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.2.2.1" style="font-size:90%;">(33)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.4.1" style="font-size:90%;">
Chunhui Zhang, Guanjie Huang, Li Liu, Shan Huang, Yinan Yang, Xiang Wan, Shiming Ge, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.5.1" style="font-size:90%;">Webuav-3m: A benchmark for unveiling the power of million-scale deep uav tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.6.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib33.7.2" style="font-size:90%;">, 45(7):9186–9205, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.2.2.1" style="font-size:90%;">(34)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.4.1" style="font-size:90%;">
Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, and Yanfeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.5.1" style="font-size:90%;">Webuot-1m: Advancing deep underwater object tracking with a million-scale benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.6.1" style="font-size:90%;">arXiv preprint arXiv:2405.19818</span><span class="ltx_text" id="bib.bib34.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.2.2.1" style="font-size:90%;">(35)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.4.1" style="font-size:90%;">
Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, and Yanfeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.5.1" style="font-size:90%;">Awesome multi-modal object tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.6.1" style="font-size:90%;">arXiv preprint arXiv:2405.14200</span><span class="ltx_text" id="bib.bib35.7.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.2.2.1" style="font-size:90%;">(36)</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.4.1" style="font-size:90%;">
Chunhui Zhang, Xin Sun, Yiqian Yang, Li Liu, Qiong Liu, Xi Zhou, and Yanfeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.5.1" style="font-size:90%;">All in one: Exploring unified vision-language tracking with multi-modal alignment.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.6.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.7.2" style="font-size:90%;">Proceedings of the 31st ACM International Conference on Multimedia</span><span class="ltx_text" id="bib.bib36.8.3" style="font-size:90%;">, pages 5552–5561, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 13:06:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
