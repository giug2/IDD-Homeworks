<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph</title>
<!--Generated on Sat Aug  3 03:06:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Multi-modal knowledge graph,  Knowledge representation,  Data refinement" lang="en" name="keywords"/>
<base href="/html/2408.01679v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S1" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S2" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS1" title="In 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS2" title="In 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Framework Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS3" title="In 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Acquisition</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS3.SSS1" title="In 3.3. Data Acquisition ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Data Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS3.SSS2" title="In 3.3. Data Acquisition ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Entity Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS3.SSS3" title="In 3.3. Data Acquisition ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Image Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS4" title="In 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Image Filtering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS4.SSS1" title="In 3.4. Image Filtering ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>PCL Feature Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS4.SSS2" title="In 3.4. Image Filtering ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Image Selection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.SS5" title="In 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Triple Completion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S4" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S5" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Demonstration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S6" title="In MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuan Yi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Wangxuan Institute of Computer Technology, Peking University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yixuan2020@stu.pku.edu.cn">yixuan2020@stu.pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanzeng Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Wangxuan Institute of Computer Technology, Peking University</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:liyanzeng@stu.pku.edu.cn">liyanzeng@stu.pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Zou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Wangxuan Institute of Computer Technology, Peking University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zoulei@pku.edu.cn">zoulei@pku.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Multi-modal knowledge graphs have emerged as a powerful approach for information representation, combining data from different modalities such as text, images, and videos. While several such graphs have been constructed and have played important roles in applications like visual question answering and recommendation systems, challenges persist in their development. These include the scarcity of high-quality Chinese knowledge graphs and limited domain coverage in existing multi-modal knowledge graphs. This paper introduces MMPKUBase, a robust and extensive Chinese multi-modal knowledge graph that covers diverse domains, including birds, mammals, ferns, and more, comprising over 50,000 entities and over 1 million filtered images. To ensure data quality, we employ Prototypical Contrastive Learning and the Isolation Forest algorithm to refine the image data. Additionally, we have developed a user-friendly platform to facilitate image attribute exploration.</p>
</div>
<div class="ltx_keywords">Multi-modal knowledge graph, Knowledge representation, Data refinement
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Knowledge graphs are a crucial method for organizing and representing connected information. A traditional knowledge graph is usually a text-based network, including entities with textual content and relationships between them. Major examples include Wikidata <cite class="ltx_cite ltx_citemacro_citep">(Vrandečić and Krötzsch, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib16" title="">2014</a>)</cite>, DBpedia <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib3" title="">2007</a>)</cite>, Freebase <cite class="ltx_cite ltx_citemacro_citep">(Bollacker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib4" title="">2008</a>)</cite>, and YAGO <cite class="ltx_cite ltx_citemacro_citep">(Suchanek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib13" title="">2007</a>)</cite>. These knowledge graphs are excel at describing semantic relationships and supporting various downstream applications. However, they do not integrate and utilize multi-modal information. As multi-modal data continues to grow, this limitation is becoming increasingly apparent.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In recent years, multi-modal knowledge graphs has gained significant attention. These graphs integrate various modalities like text, images, and videos to form comprehensive knowledge representations. They play a vital role in various tasks, including visual question answering systems <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib19" title="">2020</a>)</cite>, recommendation engines <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib14" title="">2020</a>; Tao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib15" title="">2021</a>)</cite>, etc. While the research community has made substantial progress in constructing high-quality multi-modal knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib7" title="">2020a</a>; Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib18" title="">2021</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib10" title="">2022</a>; Oñoro-Rubio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib12" title="">2017</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib9" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib17" title="">2020</a>; Alberts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib2" title="">2020</a>)</cite>, several challenges and limitations persist:</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the abundance of knowledge graphs in English and other languages, there is a lack of high-quality Chinese knowledge graphs. This shortage limits the use of multi-modal knowledge graphs in the Chinese-speaking world. Furthermore, many existing multi-modal knowledge graphs fall short in terms of domain coverage. Quality is yet another critical issue: the images incorporated into multi-modal knowledge graphs often exhibit disparities in terms of quality and accuracy.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we present MMPKUBase, a comprehensive Chinese multi-modal knowledge graph, with an effective and robust construction methodology.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of this paper are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce MMPKUBase, a extensive multi-modal knowledge graph presented in the Chinese language, featuring over 52,180 entities and 1,542,894 images across varied domains including birds, mammals, ferns, monocotyledons, Rosales plants, architecture, archaeological sites, automobiles and military.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We use Prototypical Contrastive Learning <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib6" title="">2020b</a>)</cite> to extract image features and refine the data by implementing the Isolation Forest algorithm <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib8" title="">2008</a>)</cite>, retaining 1,227,013 high-quality images. These images are not only representative but also suitable for downstream tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We develop an user-friendly demonstration platform for easy interaction with the knowledge graph and exploration of image attributes.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">With the development of artificial intelligence technology and the rapid growth of multi-modal data, the construction of high-quality multi-modal knowledge graphs has made significant progress.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The process of constructing multi-modal knowledge graphs involves two main approaches: developing new multi-modal knowledge graphs based on multi-modal documents <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib7" title="">2020a</a>; Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib18" title="">2021</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib10" title="">2022</a>)</cite>, and collecting and selecting relevant images for entities within existing traditional knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Oñoro-Rubio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib12" title="">2017</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib9" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib17" title="">2020</a>; Alberts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib2" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In the first approach, a sophisticated system for extracting knowledge is typically established, consisting of branches for text and visual information extraction, followed by the fusion of textual and visual data. GAIA <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib7" title="">2020a</a>)</cite> encompasses branches for extracting text-based knowledge, visual knowledge, and modules for fusing cross-media knowledge. Both branches make use of the same types in the DARPA AIDA ontology, using the same multi-modal document set as input for integration within the cross-media knowledge fusion module. Resin <cite class="ltx_cite ltx_citemacro_citep">(Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib18" title="">2021</a>)</cite>, much like GAIA, utilizes both text and visual branches, merging them through the coreference of visual and textual elements. Furthermore, a module for schema matching is leveraged to align the information extracted with suitable schema from a schema repository. MMEKG <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib10" title="">2022</a>)</cite>, drawing inspiration from GAIA and RESIN knowledge extracting systems, extends and enhances the framework. It replaces all Cross-encoder with Bi-encoder and employs multi-task joint model training for event relation extraction to improve efficiency.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In the second approach, the primary focus is on searching for pertinent images of entities within existing knowledge graphs from sources like search engines, followed by quality control. ImageGraph <cite class="ltx_cite ltx_citemacro_citep">(Oñoro-Rubio et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib12" title="">2017</a>)</cite> uses entities from the knowledge graph as query strings to retrieve images from search engines, treating these images as attributes values of the entities. MMKG <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib9" title="">2019</a>)</cite> extends this method to multiple knowledge graphs, applying rules to eliminate damaged, low-quality, and duplicated images to maintain the quality of the data. To obtain both highly pertinent and varied images for corresponding textual entities, Richpedia <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib17" title="">2020</a>)</cite> extracts visual feature vectors using VGG16 and reduces dimensionality, applying k-means and cosine similarity for selection. Additionally, Richpedia utilizes related hyperlinks and text from Wikipedia to discover relationships between image entities. VisualSem <cite class="ltx_cite ltx_citemacro_citep">(Alberts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib2" title="">2020</a>)</cite>, acknowledging that many entities do not require visual understanding, initiates the process with high-quality visual entities from the multilingual and multi-modal resource BabelNet and explores other related visual entities.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">In the context of this paper, the focus aligns with the second approach. The study primarily involves the augmentation of the existing Chinese knowledge graph PKUBase with images obtained from search engines, spanning a diverse array of domains. A novel method for image quality control is introduced to ensure the reliability and relevance of the incorporated images.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section first establishes the definition of MMPKUBase. This is followed by an overview of the creation pipeline for MMPKUBase. Then, the intricate details of each component will be explored in depth.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Definition</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The key distinction between a traditional knowledge graph and MMPKUBase in this work lies in the nature of attribute values, with traditional KGs using single-modal attribute values, while the multi-modal knowledge graph incorporate multi-modal attribute values to represent information in various forms, such as text and images.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.8">In this work, a multi-modal knowledge graph is denoted as <math alttext="\mathcal{G}_{mm}=\{\mathcal{E},\mathcal{R},\mathcal{A},\mathcal{V}_{mm},%
\mathcal{T}\}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.5"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml"><msub id="S3.SS1.p2.1.m1.5.5.3" xref="S3.SS1.p2.1.m1.5.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5.3.2" xref="S3.SS1.p2.1.m1.5.5.3.2.cmml">𝒢</mi><mrow id="S3.SS1.p2.1.m1.5.5.3.3" xref="S3.SS1.p2.1.m1.5.5.3.3.cmml"><mi id="S3.SS1.p2.1.m1.5.5.3.3.2" xref="S3.SS1.p2.1.m1.5.5.3.3.2.cmml">m</mi><mo id="S3.SS1.p2.1.m1.5.5.3.3.1" xref="S3.SS1.p2.1.m1.5.5.3.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.1.m1.5.5.3.3.3" xref="S3.SS1.p2.1.m1.5.5.3.3.3.cmml">m</mi></mrow></msub><mo id="S3.SS1.p2.1.m1.5.5.2" xref="S3.SS1.p2.1.m1.5.5.2.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.5.5.1.1" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml"><mo id="S3.SS1.p2.1.m1.5.5.1.1.2" stretchy="false" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">{</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ℰ</mi><mo id="S3.SS1.p2.1.m1.5.5.1.1.3" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">ℛ</mi><mo id="S3.SS1.p2.1.m1.5.5.1.1.4" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">𝒜</mi><mo id="S3.SS1.p2.1.m1.5.5.1.1.5" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">,</mo><msub id="S3.SS1.p2.1.m1.5.5.1.1.1" xref="S3.SS1.p2.1.m1.5.5.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5.1.1.1.2" xref="S3.SS1.p2.1.m1.5.5.1.1.1.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.1.m1.5.5.1.1.1.3" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.5.5.1.1.1.3.2" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.2.cmml">m</mi><mo id="S3.SS1.p2.1.m1.5.5.1.1.1.3.1" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.1.m1.5.5.1.1.1.3.3" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.3.cmml">m</mi></mrow></msub><mo id="S3.SS1.p2.1.m1.5.5.1.1.6" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.4.4" xref="S3.SS1.p2.1.m1.4.4.cmml">𝒯</mi><mo id="S3.SS1.p2.1.m1.5.5.1.1.7" stretchy="false" xref="S3.SS1.p2.1.m1.5.5.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><apply id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5"><eq id="S3.SS1.p2.1.m1.5.5.2.cmml" xref="S3.SS1.p2.1.m1.5.5.2"></eq><apply id="S3.SS1.p2.1.m1.5.5.3.cmml" xref="S3.SS1.p2.1.m1.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.5.5.3.1.cmml" xref="S3.SS1.p2.1.m1.5.5.3">subscript</csymbol><ci id="S3.SS1.p2.1.m1.5.5.3.2.cmml" xref="S3.SS1.p2.1.m1.5.5.3.2">𝒢</ci><apply id="S3.SS1.p2.1.m1.5.5.3.3.cmml" xref="S3.SS1.p2.1.m1.5.5.3.3"><times id="S3.SS1.p2.1.m1.5.5.3.3.1.cmml" xref="S3.SS1.p2.1.m1.5.5.3.3.1"></times><ci id="S3.SS1.p2.1.m1.5.5.3.3.2.cmml" xref="S3.SS1.p2.1.m1.5.5.3.3.2">𝑚</ci><ci id="S3.SS1.p2.1.m1.5.5.3.3.3.cmml" xref="S3.SS1.p2.1.m1.5.5.3.3.3">𝑚</ci></apply></apply><set id="S3.SS1.p2.1.m1.5.5.1.2.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ℰ</ci><ci id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">ℛ</ci><ci id="S3.SS1.p2.1.m1.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3">𝒜</ci><apply id="S3.SS1.p2.1.m1.5.5.1.1.1.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.5.5.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.5.5.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1.2">𝒱</ci><apply id="S3.SS1.p2.1.m1.5.5.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3"><times id="S3.SS1.p2.1.m1.5.5.1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.1"></times><ci id="S3.SS1.p2.1.m1.5.5.1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.2">𝑚</ci><ci id="S3.SS1.p2.1.m1.5.5.1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.5.5.1.1.1.3.3">𝑚</ci></apply></apply><ci id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4">𝒯</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">\mathcal{G}_{mm}=\{\mathcal{E},\mathcal{R},\mathcal{A},\mathcal{V}_{mm},%
\mathcal{T}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.5d">caligraphic_G start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT = { caligraphic_E , caligraphic_R , caligraphic_A , caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT , caligraphic_T }</annotation></semantics></math>, where <math alttext="\mathcal{E},\mathcal{R},\mathcal{A},\mathcal{V}_{mm}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.4"><semantics id="S3.SS1.p2.2.m2.4a"><mrow id="S3.SS1.p2.2.m2.4.4.1" xref="S3.SS1.p2.2.m2.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">ℰ</mi><mo id="S3.SS1.p2.2.m2.4.4.1.2" xref="S3.SS1.p2.2.m2.4.4.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.2.2" xref="S3.SS1.p2.2.m2.2.2.cmml">ℛ</mi><mo id="S3.SS1.p2.2.m2.4.4.1.3" xref="S3.SS1.p2.2.m2.4.4.2.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.3.3" xref="S3.SS1.p2.2.m2.3.3.cmml">𝒜</mi><mo id="S3.SS1.p2.2.m2.4.4.1.4" xref="S3.SS1.p2.2.m2.4.4.2.cmml">,</mo><msub id="S3.SS1.p2.2.m2.4.4.1.1" xref="S3.SS1.p2.2.m2.4.4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.4.4.1.1.2" xref="S3.SS1.p2.2.m2.4.4.1.1.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.2.m2.4.4.1.1.3" xref="S3.SS1.p2.2.m2.4.4.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.4.4.1.1.3.2" xref="S3.SS1.p2.2.m2.4.4.1.1.3.2.cmml">m</mi><mo id="S3.SS1.p2.2.m2.4.4.1.1.3.1" xref="S3.SS1.p2.2.m2.4.4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.2.m2.4.4.1.1.3.3" xref="S3.SS1.p2.2.m2.4.4.1.1.3.3.cmml">m</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.4b"><list id="S3.SS1.p2.2.m2.4.4.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ℰ</ci><ci id="S3.SS1.p2.2.m2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2">ℛ</ci><ci id="S3.SS1.p2.2.m2.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3">𝒜</ci><apply id="S3.SS1.p2.2.m2.4.4.1.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.4.4.1.1.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.4.4.1.1.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.2">𝒱</ci><apply id="S3.SS1.p2.2.m2.4.4.1.1.3.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.3"><times id="S3.SS1.p2.2.m2.4.4.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.3.1"></times><ci id="S3.SS1.p2.2.m2.4.4.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.3.2">𝑚</ci><ci id="S3.SS1.p2.2.m2.4.4.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.4.4.1.1.3.3">𝑚</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.4c">\mathcal{E},\mathcal{R},\mathcal{A},\mathcal{V}_{mm}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.4d">caligraphic_E , caligraphic_R , caligraphic_A , caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math> represent the set of entities, relationships, attributes and attribute values, respectively. A triple in the knowledge graph can be an element in <math alttext="\mathcal{E}\times\mathcal{R}\times\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">ℰ</mi><mo id="S3.SS1.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.3.m3.1.1.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">ℛ</mi><mo id="S3.SS1.p2.3.m3.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.3.m3.1.1.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1.4" xref="S3.SS1.p2.3.m3.1.1.4.cmml">ℰ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></times><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ℰ</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">ℛ</ci><ci id="S3.SS1.p2.3.m3.1.1.4.cmml" xref="S3.SS1.p2.3.m3.1.1.4">ℰ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{E}\times\mathcal{R}\times\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">caligraphic_E × caligraphic_R × caligraphic_E</annotation></semantics></math> or <math alttext="\mathcal{E}\times\mathcal{A}\times\mathcal{V}_{mm}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">ℰ</mi><mo id="S3.SS1.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.4.m4.1.1.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">𝒜</mi><mo id="S3.SS1.p2.4.m4.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.4.m4.1.1.1.cmml">×</mo><msub id="S3.SS1.p2.4.m4.1.1.4" xref="S3.SS1.p2.4.m4.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.4.2" xref="S3.SS1.p2.4.m4.1.1.4.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.4.m4.1.1.4.3" xref="S3.SS1.p2.4.m4.1.1.4.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.4.3.2" xref="S3.SS1.p2.4.m4.1.1.4.3.2.cmml">m</mi><mo id="S3.SS1.p2.4.m4.1.1.4.3.1" xref="S3.SS1.p2.4.m4.1.1.4.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.4.m4.1.1.4.3.3" xref="S3.SS1.p2.4.m4.1.1.4.3.3.cmml">m</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ℰ</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝒜</ci><apply id="S3.SS1.p2.4.m4.1.1.4.cmml" xref="S3.SS1.p2.4.m4.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.4.2">𝒱</ci><apply id="S3.SS1.p2.4.m4.1.1.4.3.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3"><times id="S3.SS1.p2.4.m4.1.1.4.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3.1"></times><ci id="S3.SS1.p2.4.m4.1.1.4.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3.2">𝑚</ci><ci id="S3.SS1.p2.4.m4.1.1.4.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.4.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{E}\times\mathcal{A}\times\mathcal{V}_{mm}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">caligraphic_E × caligraphic_A × caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">caligraphic_T</annotation></semantics></math> represents the set of these triples. The set of attribute values <math alttext="\mathcal{V}_{mm}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2" xref="S3.SS1.p2.6.m6.1.1.3.2.cmml">m</mi><mo id="S3.SS1.p2.6.m6.1.1.3.1" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝒱</ci><apply id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><times id="S3.SS1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.1"></times><ci id="S3.SS1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.2">𝑚</ci><ci id="S3.SS1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{V}_{mm}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math> is multi-modal, including a subset of text attributes <math alttext="\mathcal{V}_{text}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p2.7.m7.1.1.3.1a" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.7.m7.1.1.3.4" xref="S3.SS1.p2.7.m7.1.1.3.4.cmml">x</mi><mo id="S3.SS1.p2.7.m7.1.1.3.1b" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.7.m7.1.1.3.5" xref="S3.SS1.p2.7.m7.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝒱</ci><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><times id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1"></times><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">𝑡</ci><ci id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">𝑒</ci><ci id="S3.SS1.p2.7.m7.1.1.3.4.cmml" xref="S3.SS1.p2.7.m7.1.1.3.4">𝑥</ci><ci id="S3.SS1.p2.7.m7.1.1.3.5.cmml" xref="S3.SS1.p2.7.m7.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{V}_{text}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">caligraphic_V start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and a subset of image attributes <math alttext="\mathcal{V}_{image}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">𝒱</mi><mrow id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p2.8.m8.1.1.3.1" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml">m</mi><mo id="S3.SS1.p2.8.m8.1.1.3.1a" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m8.1.1.3.4" xref="S3.SS1.p2.8.m8.1.1.3.4.cmml">a</mi><mo id="S3.SS1.p2.8.m8.1.1.3.1b" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m8.1.1.3.5" xref="S3.SS1.p2.8.m8.1.1.3.5.cmml">g</mi><mo id="S3.SS1.p2.8.m8.1.1.3.1c" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p2.8.m8.1.1.3.6" xref="S3.SS1.p2.8.m8.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝒱</ci><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><times id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.1"></times><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">𝑖</ci><ci id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3">𝑚</ci><ci id="S3.SS1.p2.8.m8.1.1.3.4.cmml" xref="S3.SS1.p2.8.m8.1.1.3.4">𝑎</ci><ci id="S3.SS1.p2.8.m8.1.1.3.5.cmml" xref="S3.SS1.p2.8.m8.1.1.3.5">𝑔</ci><ci id="S3.SS1.p2.8.m8.1.1.3.6.cmml" xref="S3.SS1.p2.8.m8.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathcal{V}_{image}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">caligraphic_V start_POSTSUBSCRIPT italic_i italic_m italic_a italic_g italic_e end_POSTSUBSCRIPT</annotation></semantics></math>. For example, the knowledge graph might include triples like ¡Eiffel Tower, Height, 300m¿, where the attribute value is textual, or ¡Eiffel Tower, Has Image, [Image of Eiffel Tower]¿, where the attribute value is an image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Framework Overview</h3>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="322" id="S3.F1.g1" src="extracted/5771922/img/framework.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overview construction pipeline of MMPKUBase</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.F1" title="Figure 1 ‣ 3.2. Framework Overview ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overview construction pipeline of MMPKUBase, including data acquisition, image filtering and triple completion. The data acquisition phase involves the selection of entities from PKUBase and the collection of pertinent visual resources from search engine. Feature extraction and image quality control processes ensure the purity of the data repository. In the triple completion phase, images are treated as attribute values and stored in the knowledge graph.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Data Acquisition</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The process of data acquisition is subdivided into two phases: entity selection and image retrieval. First, the sources of data will be clarified:</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Data Sources</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">The data to build this knowledge graph comes from two data sources: PKUBase<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://pkubase.gstore.cn" title="">http://pkubase.gstore.cn</a></span></span></span> and Baidu Image<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://image.baidu.com/" title="">https://image.baidu.com/</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">PKUBase</span> is a massive Chinese structured knowledge resource constructed from semi-structured and unstructured trusted texts through a series of natural language processing tools and machine learning methods, containing a standard Chinese category system framework, nearly 10 million Chinese entities, and more than 60 million Chinese knowledge entries. Its comprehensive and extensive data making it an ideal source of textual information for our project.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Baidu Image</span> provides a reliable image search service based on all kinds of pictures extracted from billions of Chinese web pages, and is widely used in Chinese communities. So far, the Baidu image search engine can retrieve nearly one hundred million images. It will be used to acquire images associated with the entities in our knowledge graph.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Entity Selection</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">In this phase, entities with clear visual attributes are prioritized in our project to enhance performance in visually-oriented queries and applications, while entities without direct visual representations are excluded to optimize resource use. Consequently, nine domains have been selected based on their distinct visual characteristics: birds, mammals, ferns, monocotyledons, Rosales, architecture, archaeological sites, automobiles and military.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">After selecting the domain, the entity selection process was executed through a combination of automated and manual methods.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">In PKUBase, entities are associated with a predicate “type”, which we utilized to filter relevant entities for different domains. For domains of architecture, archaeological sites, automobiles, and military, entities are filtered using the triplet ¡entity, type, domain name¿. For the animal and plant domains, the filtering process began with ¡entity name, type, animal/plant¿, followed by a refinement based on biological taxonomy. For instance, entities in the Birds domain are selected from those that are categorized as animals and meet the ¡Entity, Category, Aves¿ criteria. After the automatic filtering, entities unrelated to the intended domains, such as humans or books, were excluded using a manually curated lexicon.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>Image Retrieval</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Once the entities are selected, the next step is to gather relevant visual data. This is achieved through a process of image crawling, whereby the chosen entities’ names serve as queries for image searches on the Baidu Image. For each selected entity, up to 30 of the most relevant images are crawled from the search results. If the search results yield fewer than 30 images, all available images are crawled. The number was chosen to ensure that the multi-modal knowledge graph is enriched with a diverse and comprehensive set of visual data directly related to the selected entities.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.1">In summation, our data acquisition process has yielded a total of 52,180 selected entities and 1,539,894 related images.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Image Filtering</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">This stage, the image filtering process, demands rigorous screening to exclude images that could potentially compromise the quality of MMPKUBase. Firstly, corrupted images or those in animated formats are removed, resulting in a remaining set of 1,535,005 images. Secondly, images that exhibit little relevance to the entities are filtered out as outliers. To achieve this, we utilize Prototypical Contrastive Learning (PCL) <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib6" title="">2020b</a>)</cite> for image feature extraction and apply Isolation Forest <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib8" title="">2008</a>)</cite> for filtering.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>PCL Feature Generation</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">This phase focus on the acquisition of image features, employing Prototypical Contrastive Learning as a key technique.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">Prototypical Contrastive Learning (PCL) represents a unsupervised representation learning approach that amalgamates the principles of contrastive learning and clustering. In PCL, prototypes are defined as representative embeddings of a group of semantically similar instances. An instance can belong to multiple prototypes of varying granularity. PCL learn an embedding space that make samples more similar to the prototype they are associated with, compared to other prototypes. In the context of our work, the images collected from various thematic domains exhibit features that align with this concept. For instance, images of the entity ‘mandarin duck’ and ‘cotton pygmy goose’ can be associated with medium-grained prototypes unique to their respective categories, while simultaneously belonging to a higher-level ‘Anatidae’ prototype. Furthermore, within the image collection of the entity ‘mandarin duck’, distinct prototypes can represent the male and female birds, reflecting finer granularity.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">For each thematic domain, we individually train PCL models (with ResNet50<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib5" title="">2016</a>)</cite> as the backbone, running for 100 epochs), which subsequently yield 128-dimensional image representations. These representations preserve the distinctive visual traits and semantic nuances of each image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Image Selection</h4>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="432" id="S3.F2.1.g1" src="extracted/5771922/img/filtering_example.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The results of filtering a collection of images for a specific architectural entity using the Isolation Forest method.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Our method entails a selection process predicated on the similarity of images within the search results. It is assumed that the majority of the search results are sufficiently relevant to the entity. Consequently, it is expected that, for each entity’s search results, relevant images will aggregate into several clusters, while irrelevant search outcomes will be identified as anomalies, distinct and distant from the clusters.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">To effectively identify and isolate these irrelevant or noisy images, the Isolation Forest technique, a robust outlier detection method in high-dimensional datasets, is employed. This approach is applied to the PCL features extracted in the previous step for each entity’s search results. The ‘contamination’ parameter, representing the proportion of outliers within the dataset, is configured at 0.2. To exemplify our approach and visualize the clusters and outliers, Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.F2" title="Figure 2 ‣ 3.4.2. Image Selection ‣ 3.4. Image Filtering ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_tag">2</span></a> provides an example. In this filtering process, images of frequently occurring architectural structures cluster together, reflecting their high relevance, while images with lower relevance and infrequent occurrences are identified as outliers. UMAP (Uniform Manifold Approximation and Projection) <cite class="ltx_cite ltx_citemacro_citep">(McInnes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#bib.bib11" title="">2018</a>)</cite> is employed to reduce dimension for visualization purposes.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Triple Completion</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.2">In the final phase of our methodology, the curated and filtered images are connected to entities as multi-modal attributes in the set <math alttext="\mathcal{V}_{mm}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">𝒱</mi><mrow id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.cmml">m</mi><mo id="S3.SS5.p1.1.m1.1.1.3.1" xref="S3.SS5.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.1.m1.1.1.3.3" xref="S3.SS5.p1.1.m1.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝒱</ci><apply id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><times id="S3.SS5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2">𝑚</ci><ci id="S3.SS5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\mathcal{V}_{mm}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math>. Triples in <math alttext="\mathcal{E}\times\mathcal{A}\times\mathcal{V}_{mm}" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">ℰ</mi><mo id="S3.SS5.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p1.2.m2.1.1.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">𝒜</mi><mo id="S3.SS5.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS5.p1.2.m2.1.1.1.cmml">×</mo><msub id="S3.SS5.p1.2.m2.1.1.4" xref="S3.SS5.p1.2.m2.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.2.m2.1.1.4.2" xref="S3.SS5.p1.2.m2.1.1.4.2.cmml">𝒱</mi><mrow id="S3.SS5.p1.2.m2.1.1.4.3" xref="S3.SS5.p1.2.m2.1.1.4.3.cmml"><mi id="S3.SS5.p1.2.m2.1.1.4.3.2" xref="S3.SS5.p1.2.m2.1.1.4.3.2.cmml">m</mi><mo id="S3.SS5.p1.2.m2.1.1.4.3.1" xref="S3.SS5.p1.2.m2.1.1.4.3.1.cmml">⁢</mo><mi id="S3.SS5.p1.2.m2.1.1.4.3.3" xref="S3.SS5.p1.2.m2.1.1.4.3.3.cmml">m</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><times id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></times><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">ℰ</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝒜</ci><apply id="S3.SS5.p1.2.m2.1.1.4.cmml" xref="S3.SS5.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.4.1.cmml" xref="S3.SS5.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.4.2.cmml" xref="S3.SS5.p1.2.m2.1.1.4.2">𝒱</ci><apply id="S3.SS5.p1.2.m2.1.1.4.3.cmml" xref="S3.SS5.p1.2.m2.1.1.4.3"><times id="S3.SS5.p1.2.m2.1.1.4.3.1.cmml" xref="S3.SS5.p1.2.m2.1.1.4.3.1"></times><ci id="S3.SS5.p1.2.m2.1.1.4.3.2.cmml" xref="S3.SS5.p1.2.m2.1.1.4.3.2">𝑚</ci><ci id="S3.SS5.p1.2.m2.1.1.4.3.3.cmml" xref="S3.SS5.p1.2.m2.1.1.4.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\mathcal{E}\times\mathcal{A}\times\mathcal{V}_{mm}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">caligraphic_E × caligraphic_A × caligraphic_V start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math> , of the form ¡entity, with image, [image of entity]¿, are subsequently incorporated into the multi-modal knowledge graph, following the RDF format<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.w3.org/RDF/" title="">https://www.w3.org/RDF/</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">With the completion of this phase, the construction of MMPKUBase, our multi-modal knowledge graph, is finalized.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="500" id="S3.F3.1.g1" src="extracted/5771922/img/demo1min.png" width="884"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Query examples from the demonstration platform. The SPARQL query is designed to locate entities whose names contain the substring ‘BMW’ and retrieve their associated image attributes.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Statistics</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S4.T1" title="Table 1 ‣ 4. Statistics ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_tag">1</span></a> provides a comprehensive overview of the statistical data pertinent to the development of MMPKUBase, encompassing the number of entities within each thematic domain and the quantity of images both before and after the filtering process. This data plays a pivotal role in assessing the richness of visual content and the level of information available for each thematic domain.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1">Topic</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1">Entities</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1">Images</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.1">Filtered Images</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1">Birds</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.2">10,554</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.3">314,977</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.4">251,326</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.1">Mammals</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2">4,031</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.3">118,550</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.4">94,341</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.1">Ferns</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.2">2,995</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3">90,092</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.4">69,506</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.1">Monocotyledons</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.2">7,822</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.3">231,570</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4">184,479</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.1">Rosales Plants</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.2">4,545</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.3">135,183</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.4">107,934</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.1">Architecture</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.2">3,622</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.3">106,360</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.7.4">84,835</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.1">Archaeological Sites</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.2">5,064</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.3">148,956</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.4">118,706</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.1">Automobiles</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.2">1,911</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.3">57,139</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.4">45,666</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10">
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.1">Military</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.2">11,636</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.3">340,067</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.4">270,220</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.1.11.1">Total</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S4.T1.1.11.2">52,180</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S4.T1.1.11.3">1,542,894</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S4.T1.1.11.4">1,227,013</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Entity and Image Count Information.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Demonstration</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Leveraging the RDF-formatted dataset constructed in the previous sections, we have develop a platform for retrieving and visualizing multi-modal data. This platform enables users to perform SPARQL queries within MMPKUBase, facilitating the retrieval of pertinent entities along with their corresponding visual attributes.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2408.01679v1#S3.F3" title="Figure 3 ‣ 3.5. Triple Completion ‣ 3. Method ‣ MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal Knowledge Graph"><span class="ltx_text ltx_ref_tag">3</span></a> represents a query example. Users can input SPARQL query statements in the search box located at the top of the interface to obtain and explore the multi-modal knowledge graph. The result part is divided into two sections. On the left side, structured query responses are displayed and on the right side, images are presented alongside their corresponding entity names, allowing users to brows the multi-modal attributes associated with the entities.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper introduce MMPKUBase, a comprehensive and extensive multi-modal knowledge graph in the Chinese language. MMPKUBase encompasses a diverse range of domains, including birds, mammals, ferns, monocotyledons, Rosales plants, architecture, archaeological sites, automobiles, and military. With over fifty thousand entities and images in the million range, it serves as a valuable resource for various applications. Prototypical Contrastive Learning and the Isolation Forest algorithm is employed to ensure the quality and reliability of our data, resulting in a collection of high-quality and representative images that can be applied to a wide array of downstream tasks. Furthermore, to make MMPKUBase accessible and user-friendly, we developed an intuitive demonstration platform that enables users to query and explore image attributes.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">As future work, our focus will be to seamlessly integrate MMPKUBase into real-world applications, unlocking its full potential. Additionally, we are committed to continually enhancing the size and diversity of the knowledge graph, ensuring it encompasses an even broader array of entities and domains.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberts et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Houda Alberts, Teresa Huang, Yash Deshpande, Yibo Liu, Kyunghyun Cho, Clara Vania, and Iacer Calixto. 2020.

</span>
<span class="ltx_bibblock">Visualsem: a high-quality knowledge graph for vision and language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">arXiv preprint arXiv:2008.09150</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007.

</span>
<span class="ltx_bibblock">Dbpedia: A nucleus for a web of open data. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">international semantic web conference</em>. Springer, 722–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bollacker et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring human knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</em>. 1247–1250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 770–778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. 2020b.

</span>
<span class="ltx_bibblock">Prototypical contrastive learning of unsupervised representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:2005.04966</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, et al<span class="ltx_text" id="bib.bib7.3.1">.</span> 2020a.

</span>
<span class="ltx_bibblock">Gaia: A fine-grained multimedia knowledge extraction system. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>. 77–86.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008.

</span>
<span class="ltx_bibblock">Isolation forest. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">2008 eighth ieee international conference on data mining</em>. IEEE, 413–422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S Rosenblum. 2019.

</span>
<span class="ltx_bibblock">MMKG: multi-modal knowledge graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">The Semantic Web: 16th International Conference, ESWC 2019, Portorož, Slovenia, June 2–6, 2019, Proceedings 16</em>. Springer, 459–474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yubo Ma, Zehao Wang, Mukai Li, Yixin Cao, Meiqi Chen, Xinze Li, Wenqi Sun, Kunquan Deng, Kun Wang, Aixin Sun, and Jing Shao. 2022.

</span>
<span class="ltx_bibblock">MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>. Association for Computational Linguistics, Dublin, Ireland, 231–239.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.acl-demo.23" title="">https://doi.org/10.18653/v1/2022.acl-demo.23</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McInnes et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Leland McInnes, John Healy, and James Melville. 2018.

</span>
<span class="ltx_bibblock">Umap: Uniform manifold approximation and projection for dimension reduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">arXiv preprint arXiv:1802.03426</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oñoro-Rubio et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Daniel Oñoro-Rubio, Mathias Niepert, Alberto García-Durán, Roberto González, and Roberto J López-Sastre. 2017.

</span>
<span class="ltx_bibblock">Answering visual-relational queries in web-extracted knowledge graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:1709.02314</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suchanek et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007.

</span>
<span class="ltx_bibblock">Yago: a core of semantic knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 16th international conference on World Wide Web</em>. 697–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. 2020.

</span>
<span class="ltx_bibblock">Multi-modal Knowledge Graphs for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em> (Virtual Event, Ireland) <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.2">(CIKM ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1405–1414.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3340531.3411947" title="">https://doi.org/10.1145/3340531.3411947</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shaohua Tao, Runhe Qiu, Yuan Ping, and Hui Ma. 2021.

</span>
<span class="ltx_bibblock">Multi-modal knowledge-aware reinforcement learning network for explainable recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Knowledge-Based Systems</em> 227 (2021), 107217.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vrandečić and Krötzsch (2014)</span>
<span class="ltx_bibblock">
Denny Vrandečić and Markus Krötzsch. 2014.

</span>
<span class="ltx_bibblock">Wikidata: a free collaborative knowledgebase.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Commun. ACM</em> 57, 10 (2014), 78–85.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Meng Wang, Haofen Wang, Guilin Qi, and Qiushuo Zheng. 2020.

</span>
<span class="ltx_bibblock">Richpedia: a large-scale, comprehensive multi-modal knowledge graph.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Big Data Research</em> 22 (2020), 100159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Haoyang Wen, Ying Lin, Tuan Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, et al<span class="ltx_text" id="bib.bib18.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Resin: A dockerized schema-guided cross-document cross-lingual cross-media information extraction and event tracking system. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations</em>. 133–143.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jing Yu, Zihao Zhu, Yujing Wang, Weifeng Zhang, Yue Hu, and Jianlong Tan. 2020.

</span>
<span class="ltx_bibblock">Cross-modal knowledge reasoning for knowledge-based visual question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Pattern Recognition</em> 108 (2020), 107563.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug  3 03:06:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
