<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.11906] Fusion in Context: A Multimodal Approach to Affective State Recognition</title><meta property="og:description" content="Accurate recognition of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. Howev‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fusion in Context: A Multimodal Approach to Affective State Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fusion in Context: A Multimodal Approach to Affective State Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.11906">

<!--Generated on Sat Oct  5 22:19:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fusion in Context: A Multimodal Approach to Affective State Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Youssef Mohamed<sup id="id9.2.id1" class="ltx_sup"><span id="id9.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> S√©verin Lemaignan<sup id="id10.2.id1" class="ltx_sup"><span id="id10.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Arzu G√ºneysu<sup id="id11.2.id1" class="ltx_sup"><span id="id11.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Patric Jensfelt<sup id="id12.2.id1" class="ltx_sup"><span id="id12.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> and Christian Smith<sup id="id13.5.id1" class="ltx_sup"><span id="id13.5.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes">*This work was in part financially supported by Digital Futures<sup id="id14.6.id1" class="ltx_sup"><span id="id14.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Youssef Mohamed, Patric Jensfelt, and Christian Smith are with KTH: The Royal Institute of Technology, Stockholm, Sweden
<span id="id15.7.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{ymo, patric, ccs}@kth.se</span><sup id="id16.8.id1" class="ltx_sup"><span id="id16.8.id1.1" class="ltx_text ltx_font_italic">2</span></sup>S√©verin Lemaignan is with PAL Robotics, Barcelona, Spain
<span id="id17.9.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">severin.lemaignan@pal-robotics.com</span><sup id="id18.10.id1" class="ltx_sup"><span id="id18.10.id1.1" class="ltx_text ltx_font_italic">3</span></sup>Arzu G√ºneysu is with Ume√• University, Ume√•, Sweden
<span id="id19.11.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">arzu.guneysu@umu.se</span>
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Accurate recognition of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusion, combining modalities like facial expressions, speech, and physiological signals, has shown promise in improving affect recognition. This paper proposes a transformer-based multimodal fusion approach that leverages facial thermal data, facial action units, and textual context information for context-aware emotion recognition. We explore modality-specific encoders to learn tailored representations, which are then fused using additive fusion and processed by a shared transformer encoder to capture temporal dependencies and interactions. The proposed method is evaluated on a dataset collected from participants engaged in a tangible tabletop Pacman game designed to induce various affective states. Our results demonstrate the effectiveness of incorporating contextual information and multimodal fusion for affective state recognition.</p>
</div>
<section id="S0.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Keywords</h5>

<div id="S0.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S0.SS0.SSS0.Px1.p1.1" class="ltx_p">Human detection, computer vision, social human-robot interaction.</p>
</div>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Accurately perceiving and interpreting human emotions is a fundamental challenge in affective computing and human-robot interaction (HRI) fields. Since emotions significantly influence our behavior, decisions, and social interactions, creating systems that can reliably recognize and understand these emotional states is essential. Such advancements would enable robots to engage in more natural, effective interactions by better comprehending and responding to human needs and preferences.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One of the key challenges in affective state recognition lies in accounting for the contextual information surrounding affect expressions. In other words, the same facial expression or physiological signal may convey different affective meanings depending on the context in which it occurs. For instance, a smile in a social setting may indicate enjoyment, while a similar smile in a different context could signify sarcasm or discomfort. Failing to consider these contextual factors can lead to misinterpretations and inaccurate affect recognition, hindering the effectiveness of affective computing systems.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Multimodal fusion, which integrates diverse data streams such as facial expressions, speech, and physiological signals, has shown promise in enhancing affect recognition performance compared to unimodal methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, effectively fusing these modalities and incorporating contextual information remains a significant challenge in the field.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address this challenge, we propose a transformer-based multimodal fusion approach for context-aware emotion recognition. Our method leverages recent advances in deep learning, particularly transformer architectures, which have demonstrated remarkable capabilities in capturing temporal dependencies and modeling complex interactions between modalities¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our approach utilizes modality-specific encoders to extract tailored representations from facial thermal data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, action units<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and textual context information<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. These representations are then combined and processed by a shared transformer encoder. This architecture enables effective integration of contextual cues and leverages the complementary information from multiple modalities, providing a more comprehensive understanding of affective states.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate our method on a dataset collected from participants in a tabletop Pacman game <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> re-designed to elicit various affective states, including enjoyment, boredom, and frustration. By doing so, we aim to demonstrate the potential of our approach in real-world emotion recognition scenarios.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The primary contributions of this paper are as follows.</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Contextual Information Integration:</span> Demonstrates the importance of incorporating contextual information as a separate modality to enhance affect recognition accuracy when added to other physiological and visual modalities.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Transformer-based Multimodal Architecture:</span> Proposes a transformer architecture with additive fusion of modality-specific representations, effectively capturing temporal dependencies and interactions for improved affective state detection.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our experimental results on the Pacman game dataset demonstrate the relative effectiveness of different modalities and their combinations in our transformer-based multimodal fusion approach. By examining various configurations of thermal data, action units, and contextual information, we provide insights into the contributions of each modality to affective state recognition.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Context-Aware Emotion Recognition</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Context-aware emotion recognition is crucial for improving affective computing systems‚Äô accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Various approaches have been proposed to incorporate context, each with its own limitations.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Mittal et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> developed a multimodal and context-aware model using multiplicative fusion to combine facial expressions, speech, and physiological signals. It employs a graph-based attention mechanism to weight modalities based on context. However, this approach may struggle with complex, non-linear relationships between modalities and context.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Wang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed a context-aware network with a hierarchical attention mechanism for video data. It learns to focus on salient emotional cues, considering facial expressions, body language, voice tone, and environmental context simultaneously. The reliance on predefined hierarchical structures, however, may limit its adaptability to diverse scenarios.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Kim et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> introduced a deep semantic feature fusion approach for video emotion recognition, combining facial expressions, audio features, and textual context using hierarchical fusion. While innovative, their method may not fully capture the nuanced interplay between different contextual elements and emotional expressions.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">These studies demonstrate the importance of incorporating context in affect recognition systems, but often treat context as a fixed set of features, potentially overlooking its dynamic nature.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">We propose a transformer-based architecture for more effective multimodal integration, overcoming limitations of fixed fusion strategies. By leveraging natural language processing and transformers, our method achieves a more comprehensive and adaptable integration of context in emotion recognition, potentially leading to more accurate and robust affective computing systems.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Multimodal Machine Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multimodal machine learning integrates multiple modalities like text, audio, images, and videos¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Key principles driving innovations include modality heterogeneity, connections, and interactions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These principles are fundamental to our work, integrating facial thermal data, action units, and textual context.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Core technical challenges include representation and alignment¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Representation involves encoding diverse modalities with distinct statistical properties, while alignment concerns mapping corresponding elements across modalities. These challenges are particularly relevant in emotion recognition, where facial expressions, thermal, and contextual information must be coherently integrated.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Jiang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> highlighted the importance of constructing meaningful latent modality structures, suggesting that exact modality alignment may not be optimal for tasks like emotion recognition.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Our approach addresses these principles through modality-specific encoders and a shared transformer encoder capturing temporal dependencies and interactions.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Transformer Multimodal Fusion</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Transformer-based architectures have gained popularity for multimodal fusion due to their ability to capture inter-modality interactions and model temporal dependencies.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">A recent advancement in this field is the work of Faye et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, who proposed the Context-Based Multimodal Fusion (CBMF) model. This approach combines modality fusion and data distribution alignment using context vectors fused with modality embeddings. The CBMF model is particularly relevant to our work as it shares our focus on integrating contextual information directly into the fusion process.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Our method builds upon recent advancements in multi-modal analysis for manipulation detection. While some approaches use complex interaction mechanisms¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we employ separate encoding processes for each modality followed by additive fusion, allowing effective integration without intricate cross-modal attention mechanisms.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This study utilizes a dataset collected by¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which captures participants‚Äô affective states during a tangible Pacman game designed with multiple configurations and can induce four affects: frustration, enjoyment, boredom and neutral <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Data Modalities</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The dataset comprises three main modalities: thermal data from facial regions of interest (ROIs), visual data in the form of Action Units (AUs) extracted from RGB images, and text data from game-play logs and settings (see Table¬†<a href="#S3.T1" title="TABLE I ‚Ä£ III-A1 Thermal and Visual Features ‚Ä£ III-A Data Modalities ‚Ä£ III Dataset ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>).</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Thermal and Visual Features</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Figure¬†<a href="#S3.F1" title="Figure 1 ‚Ä£ III-A1 Thermal and Visual Features ‚Ä£ III-A Data Modalities ‚Ä£ III Dataset ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the facial landmarks and AUs extracted using OpenFace (left), and the thermal ROIs (right).</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.11906/assets/figs/facefacegame.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Facial landmarks and Action Units extracted using OpenFace (left) and thermal regions of interest (right)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Table¬†<a href="#S3.T1" title="TABLE I ‚Ä£ III-A1 Thermal and Visual Features ‚Ä£ III-A Data Modalities ‚Ä£ III Dataset ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the extracted features for each modality.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Features for thermal, visual, and text modalities</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="S3.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Modality</span></span>
</span>
</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Features</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="S3.T1.4.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Thermal</span></span>
</span>
</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ROIs:</span><span id="S3.T1.4.2.1.2.1.1.2" class="ltx_text" style="font-size:90%;"> Nose, Forehead, Cheek, Lower lip</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.3.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Metrics:</span><span id="S3.T1.4.3.2.2.1.1.2" class="ltx_text" style="font-size:90%;"> Avg, Change, Max, Min temperature</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<td id="S3.T1.4.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="S3.T1.4.4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Visual</span></span>
</span>
</td>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.4.3.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">AUs:</span><span id="S3.T1.4.4.3.2.1.1.2" class="ltx_text" style="font-size:90%;"> 1, 2, 4-7, 9, 10, 12, 14, 15, 17, 20, 23, 25, 26, 28, 45</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<td id="S3.T1.4.5.4.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.4.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.5.4.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Metrics:</span><span id="S3.T1.4.5.4.2.1.1.2" class="ltx_text" style="font-size:90%;"> Avg, Change, Max, Min intensity</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.6.5" class="ltx_tr">
<td id="S3.T1.4.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.5.1.1.1" class="ltx_p" style="width:51.2pt;"><span id="S3.T1.4.6.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Text</span></span>
</span>
</td>
<td id="S3.T1.4.6.5.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.5.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.6.5.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Game Outcomes:</span><span id="S3.T1.4.6.5.2.1.1.2" class="ltx_text" style="font-size:90%;"> Win/Loss</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.7.6" class="ltx_tr">
<td id="S3.T1.4.7.6.1" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T1.4.7.6.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.4.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.6.2.1.1" class="ltx_p" style="width:162.2pt;"><span id="S3.T1.4.7.6.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Round Settings:</span><span id="S3.T1.4.7.6.2.1.1.2" class="ltx_text" style="font-size:90%;"> Fruit, Ghosts, Speed, Rotation</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Dataset Composition</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our dataset includes four distinct affective states: baseline (neutral), enjoyment, boredom, and frustration. We use a 7-second window for analysis, aligning with the methodology of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This approach is grounded in the well-established understanding that physiological signals typically manifest on the face within a 5-15 second timeframe, while facial expressions can take up to 4 seconds to appear and often persist for longer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Context Extraction and Classification</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We have gained access to the raw video data, the videos of each participant were then processed to capture their interactions within the game environment. The contextual data was categorized into two types: <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Game-Only Context (GOC)</span> and <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">Full Context (FC)</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For both types of context, we first generated descriptive sentences, which were then transformed into high-dimensional vector representations using the OpenAI embedding model <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">embedding-large</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This model converted each descriptive sentence into a 3072-dimensional vector, capturing the semantic nuances of the context.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Game-Only Context (GOC) Embedding</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">For the GOC, we included only game-related information. An example sentence might be:</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<blockquote id="S4.SS1.SSS1.p2.1" class="ltx_quote">
<p id="S4.SS1.SSS1.p2.1.1" class="ltx_p"><span id="S4.SS1.SSS1.p2.1.1.1" class="ltx_text ltx_font_italic">‚ÄùThe person is playing a Pacman game with difficulty level: easy‚Äù</span></p>
</blockquote>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">The description of the game difficulty was based on the speed of the robots, their number, and the amount of rotation needed to collect the points, which was classified into three settings: easy, medium, and hard¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Full Context (FC) Embedding</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">For the FC, we combined the game settings and difficulty level with facial expression descriptions. To capture the temporal dynamics of facial expressions, we implemented a sliding window on the raw video data. We sub-sampled the video stream to 1 frame per second, as we do not expect the signals to move at a faster rate ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.2" class="ltx_p">For each instance, we extracted a pair of consecutive frames: the current frame at time <math id="S4.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mi id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><ci id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">t</annotation></semantics></math> and its predecessor at time <math id="S4.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="t-1" display="inline"><semantics id="S4.SS1.SSS2.p2.2.m2.1a"><mrow id="S4.SS1.SSS2.p2.2.m2.1.1" xref="S4.SS1.SSS2.p2.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.1.1.2" xref="S4.SS1.SSS2.p2.2.m2.1.1.2.cmml">t</mi><mo id="S4.SS1.SSS2.p2.2.m2.1.1.1" xref="S4.SS1.SSS2.p2.2.m2.1.1.1.cmml">‚àí</mo><mn id="S4.SS1.SSS2.p2.2.m2.1.1.3" xref="S4.SS1.SSS2.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.2.m2.1b"><apply id="S4.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1"><minus id="S4.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1.1"></minus><ci id="S4.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1.2">ùë°</ci><cn type="integer" id="S4.SS1.SSS2.p2.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.2.m2.1c">t-1</annotation></semantics></math>. This two-frame window slides throughout the duration of the video, allowing us to capture the evolution of facial expressions over time¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">We used GPT-4(V) model, accessed through its API, for facial expression analysis. Each frame pair was submitted to the model using the following prompt:</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<blockquote id="S4.SS1.SSS2.p4.2" class="ltx_quote">
<p id="S4.SS1.SSS2.p4.2.2" class="ltx_p"><span id="S4.SS1.SSS2.p4.2.2.2" class="ltx_text ltx_font_italic">Given two images, the first of the face at time <math id="S4.SS1.SSS2.p4.1.1.1.m1.1" class="ltx_Math" alttext="t-1" display="inline"><semantics id="S4.SS1.SSS2.p4.1.1.1.m1.1a"><mrow id="S4.SS1.SSS2.p4.1.1.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.2" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.2.cmml">t</mi><mo id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.1" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.1.cmml">‚àí</mo><mn id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.3" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.1.1.m1.1b"><apply id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1"><minus id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.1"></minus><ci id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.2">ùë°</ci><cn type="integer" id="S4.SS1.SSS2.p4.1.1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p4.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.1.1.m1.1c">t-1</annotation></semantics></math> and the second at time <math id="S4.SS1.SSS2.p4.2.2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS1.SSS2.p4.2.2.2.m2.1a"><mi id="S4.SS1.SSS2.p4.2.2.2.m2.1.1" xref="S4.SS1.SSS2.p4.2.2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.2.2.2.m2.1b"><ci id="S4.SS1.SSS2.p4.2.2.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p4.2.2.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.2.2.2.m2.1c">t</annotation></semantics></math>, describe the current emotional state of the person in one brief sentence, considering the presence and intensities of facial expressions.</span></p>
</blockquote>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p">An example of a full context sentence combining game information and facial expression data would be:</p>
</div>
<div id="S4.SS1.SSS2.p6" class="ltx_para">
<blockquote id="S4.SS1.SSS2.p6.2" class="ltx_quote">
<p id="S4.SS1.SSS2.p6.1.1" class="ltx_p"><span id="S4.SS1.SSS2.p6.1.1.1" class="ltx_text ltx_font_italic ltx_inline-block" style="width:0.0pt;position:relative; bottom:12.9pt;"><span id="S4.SS1.SSS2.p6.1.1.1.1" class="ltx_text" style="font-size:70%;">Game Context </span></span><math id="S4.SS1.SSS2.p6.1.1.m1.1" class="ltx_Math" alttext="\overbrace{\hbox{\leavevmode\hbox{\footnotesize The person is playing a Pacman game with difficulty level: easy}}}" display="inline"><semantics id="S4.SS1.SSS2.p6.1.1.m1.1a"><mover accent="true" id="S4.SS1.SSS2.p6.1.1.m1.1.1" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="80%" id="S4.SS1.SSS2.p6.1.1.m1.1.1.2" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.2a.cmml">The person is playing a Pacman game with difficulty level: easy</mtext><mo id="S4.SS1.SSS2.p6.1.1.m1.1.1.1" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.1.cmml">‚èû</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p6.1.1.m1.1b"><apply id="S4.SS1.SSS2.p6.1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p6.1.1.m1.1.1"><ci id="S4.SS1.SSS2.p6.1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.1">‚èû</ci><ci id="S4.SS1.SSS2.p6.1.1.m1.1.1.2a.cmml" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="80%" id="S4.SS1.SSS2.p6.1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p6.1.1.m1.1.1.2">The person is playing a Pacman game with difficulty level: easy</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p6.1.1.m1.1c">\overbrace{\hbox{\leavevmode\hbox{\footnotesize The person is playing a Pacman game with difficulty level: easy}}}</annotation></semantics></math><span id="S4.SS1.SSS2.p6.1.1.2" class="ltx_text ltx_font_italic"></span></p>
<p id="S4.SS1.SSS2.p6.2.2" class="ltx_p"><span id="S4.SS1.SSS2.p6.2.2.1" class="ltx_text ltx_font_italic ltx_inline-block" style="width:0.0pt;position:relative; bottom:12.9pt;"><span id="S4.SS1.SSS2.p6.2.2.1.1" class="ltx_text" style="font-size:70%;">GPT-4(V) Output (Facial Description) </span></span><math id="S4.SS1.SSS2.p6.2.2.m1.1" class="ltx_Math" alttext="\overbrace{\hbox{\leavevmode\hbox{\footnotesize with a look of wonder or amazement with raised eyebrows}}}" display="inline"><semantics id="S4.SS1.SSS2.p6.2.2.m1.1a"><mover accent="true" id="S4.SS1.SSS2.p6.2.2.m1.1.1" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="80%" id="S4.SS1.SSS2.p6.2.2.m1.1.1.2" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.2a.cmml">with a look of wonder or amazement with raised eyebrows</mtext><mo id="S4.SS1.SSS2.p6.2.2.m1.1.1.1" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.1.cmml">‚èû</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p6.2.2.m1.1b"><apply id="S4.SS1.SSS2.p6.2.2.m1.1.1.cmml" xref="S4.SS1.SSS2.p6.2.2.m1.1.1"><ci id="S4.SS1.SSS2.p6.2.2.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.1">‚èû</ci><ci id="S4.SS1.SSS2.p6.2.2.m1.1.1.2a.cmml" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="80%" id="S4.SS1.SSS2.p6.2.2.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p6.2.2.m1.1.1.2">with a look of wonder or amazement with raised eyebrows</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p6.2.2.m1.1c">\overbrace{\hbox{\leavevmode\hbox{\footnotesize with a look of wonder or amazement with raised eyebrows}}}</annotation></semantics></math><span id="S4.SS1.SSS2.p6.2.2.2" class="ltx_text ltx_font_italic"></span></p>
</blockquote>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Transformer</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we present a transformer-based model for multimodal affect recognition. The three inputs comprises thermal features, visual features (action units), and contextual data. The raw input is discretized using a 7-second sliding window¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> stepping 1 second at a time to match the classification into one of the four affective states provided by¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The latter is the target output for the transformer network.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In the following subsections, we present our transformer model, discussing its architecture, key components, and the hyperparameters used for optimization.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.5.1.1" class="ltx_text">IV-B</span>1 </span>Architecture</h4>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.11906/assets/figs/image12.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Multimodal Transformer Architecture: Integrates action units (16), facial thermal data (144), and context text embeddings (3072) through modality-specific encoders. Additive fusion processed by transformer encoder with positional encoding, followed by classification head for 4 affective states prediction.</span></figcaption>
</figure>
<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The proposed multimodal fusion method uses a hierarchical transformer-based architecture with three components: modality-specific encoding, additive fusion, and shared transformer encoder. Each input modality is processed by a dedicated transformer encoder branch, learning representations tailored to their unique characteristics. An additive fusion mechanism combines these representations, followed by a shared transformer encoder for further processing.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.9" class="ltx_p">Let <math id="S4.SS2.SSS1.p2.1.m1.7" class="ltx_Math" alttext="X^{(1)},X^{(2)},\ldots,X^{(M)}" display="inline"><semantics id="S4.SS2.SSS1.p2.1.m1.7a"><mrow id="S4.SS2.SSS1.p2.1.m1.7.7.3" xref="S4.SS2.SSS1.p2.1.m1.7.7.4.cmml"><msup id="S4.SS2.SSS1.p2.1.m1.5.5.1.1" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.5.5.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.2.cmml">X</mi><mrow id="S4.SS2.SSS1.p2.1.m1.1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.1.1.1.3.1" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.cmml">(</mo><mn id="S4.SS2.SSS1.p2.1.m1.1.1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.1.1.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.cmml">)</mo></mrow></msup><mo id="S4.SS2.SSS1.p2.1.m1.7.7.3.4" xref="S4.SS2.SSS1.p2.1.m1.7.7.4.cmml">,</mo><msup id="S4.SS2.SSS1.p2.1.m1.6.6.2.2" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.6.6.2.2.2" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.2.cmml">X</mi><mrow id="S4.SS2.SSS1.p2.1.m1.2.2.1.3" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.2.2.1.3.1" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.cmml">(</mo><mn id="S4.SS2.SSS1.p2.1.m1.2.2.1.1" xref="S4.SS2.SSS1.p2.1.m1.2.2.1.1.cmml">2</mn><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.2.2.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.cmml">)</mo></mrow></msup><mo id="S4.SS2.SSS1.p2.1.m1.7.7.3.5" xref="S4.SS2.SSS1.p2.1.m1.7.7.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.SSS1.p2.1.m1.4.4" xref="S4.SS2.SSS1.p2.1.m1.4.4.cmml">‚Ä¶</mi><mo id="S4.SS2.SSS1.p2.1.m1.7.7.3.6" xref="S4.SS2.SSS1.p2.1.m1.7.7.4.cmml">,</mo><msup id="S4.SS2.SSS1.p2.1.m1.7.7.3.3" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.cmml"><mi id="S4.SS2.SSS1.p2.1.m1.7.7.3.3.2" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.2.cmml">X</mi><mrow id="S4.SS2.SSS1.p2.1.m1.3.3.1.3" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.3.3.1.3.1" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.cmml">(</mo><mi id="S4.SS2.SSS1.p2.1.m1.3.3.1.1" xref="S4.SS2.SSS1.p2.1.m1.3.3.1.1.cmml">M</mi><mo stretchy="false" id="S4.SS2.SSS1.p2.1.m1.3.3.1.3.2" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.7b"><list id="S4.SS2.SSS1.p2.1.m1.7.7.4.cmml" xref="S4.SS2.SSS1.p2.1.m1.7.7.3"><apply id="S4.SS2.SSS1.p2.1.m1.5.5.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.5.5.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1">superscript</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.5.5.1.1.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.5.5.1.1.2">ùëã</ci><cn type="integer" id="S4.SS2.SSS1.p2.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.1">1</cn></apply><apply id="S4.SS2.SSS1.p2.1.m1.6.6.2.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.6.6.2.2.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2">superscript</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.6.6.2.2.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.6.6.2.2.2">ùëã</ci><cn type="integer" id="S4.SS2.SSS1.p2.1.m1.2.2.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.2.2.1.1">2</cn></apply><ci id="S4.SS2.SSS1.p2.1.m1.4.4.cmml" xref="S4.SS2.SSS1.p2.1.m1.4.4">‚Ä¶</ci><apply id="S4.SS2.SSS1.p2.1.m1.7.7.3.3.cmml" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.1.m1.7.7.3.3.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3">superscript</csymbol><ci id="S4.SS2.SSS1.p2.1.m1.7.7.3.3.2.cmml" xref="S4.SS2.SSS1.p2.1.m1.7.7.3.3.2">ùëã</ci><ci id="S4.SS2.SSS1.p2.1.m1.3.3.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.3.3.1.1">ùëÄ</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.7c">X^{(1)},X^{(2)},\ldots,X^{(M)}</annotation></semantics></math> denote input tensors for <math id="S4.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.SSS1.p2.2.m2.1a"><mi id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.1b"><ci id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.1c">M</annotation></semantics></math> modalities, where <math id="S4.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="X^{(i)}\in\mathbb{R}^{N\times D_{i}}" display="inline"><semantics id="S4.SS2.SSS1.p2.3.m3.1a"><mrow id="S4.SS2.SSS1.p2.3.m3.1.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.cmml"><msup id="S4.SS2.SSS1.p2.3.m3.1.2.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.cmml"><mi id="S4.SS2.SSS1.p2.3.m3.1.2.2.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.2.cmml">X</mi><mrow id="S4.SS2.SSS1.p2.3.m3.1.1.1.3" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.3.m3.1.1.1.3.1" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.cmml">(</mo><mi id="S4.SS2.SSS1.p2.3.m3.1.1.1.1" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.SS2.SSS1.p2.3.m3.1.1.1.3.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.cmml">)</mo></mrow></msup><mo id="S4.SS2.SSS1.p2.3.m3.1.2.1" xref="S4.SS2.SSS1.p2.3.m3.1.2.1.cmml">‚àà</mo><msup id="S4.SS2.SSS1.p2.3.m3.1.2.3" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.cmml"><mi id="S4.SS2.SSS1.p2.3.m3.1.2.3.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.2.cmml">‚Ñù</mi><mrow id="S4.SS2.SSS1.p2.3.m3.1.2.3.3" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.cmml"><mi id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.1" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.1.cmml">√ó</mo><msub id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.cmml"><mi id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.2" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.2.cmml">D</mi><mi id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.3" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.3.cmml">i</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.3.m3.1b"><apply id="S4.SS2.SSS1.p2.3.m3.1.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2"><in id="S4.SS2.SSS1.p2.3.m3.1.2.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.1"></in><apply id="S4.SS2.SSS1.p2.3.m3.1.2.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.3.m3.1.2.2.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.2">superscript</csymbol><ci id="S4.SS2.SSS1.p2.3.m3.1.2.2.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.2.2">ùëã</ci><ci id="S4.SS2.SSS1.p2.3.m3.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.1">ùëñ</ci></apply><apply id="S4.SS2.SSS1.p2.3.m3.1.2.3.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.3.m3.1.2.3.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3">superscript</csymbol><ci id="S4.SS2.SSS1.p2.3.m3.1.2.3.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.2">‚Ñù</ci><apply id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3"><times id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.1"></times><ci id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.2">ùëÅ</ci><apply id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3">subscript</csymbol><ci id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.2.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.2">ùê∑</ci><ci id="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.3.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.2.3.3.3.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.3.m3.1c">X^{(i)}\in\mathbb{R}^{N\times D_{i}}</annotation></semantics></math> represents the <math id="S4.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS1.p2.4.m4.1a"><mi id="S4.SS2.SSS1.p2.4.m4.1.1" xref="S4.SS2.SSS1.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.4.m4.1b"><ci id="S4.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p2.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.4.m4.1c">i</annotation></semantics></math>-th modality with <math id="S4.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.SSS1.p2.5.m5.1a"><mi id="S4.SS2.SSS1.p2.5.m5.1.1" xref="S4.SS2.SSS1.p2.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.5.m5.1b"><ci id="S4.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS1.p2.5.m5.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.5.m5.1c">N</annotation></semantics></math> samples and <math id="S4.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S4.SS2.SSS1.p2.6.m6.1a"><msub id="S4.SS2.SSS1.p2.6.m6.1.1" xref="S4.SS2.SSS1.p2.6.m6.1.1.cmml"><mi id="S4.SS2.SSS1.p2.6.m6.1.1.2" xref="S4.SS2.SSS1.p2.6.m6.1.1.2.cmml">D</mi><mi id="S4.SS2.SSS1.p2.6.m6.1.1.3" xref="S4.SS2.SSS1.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.6.m6.1b"><apply id="S4.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS2.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.6.m6.1.1.1.cmml" xref="S4.SS2.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p2.6.m6.1.1.2.cmml" xref="S4.SS2.SSS1.p2.6.m6.1.1.2">ùê∑</ci><ci id="S4.SS2.SSS1.p2.6.m6.1.1.3.cmml" xref="S4.SS2.SSS1.p2.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.6.m6.1c">D_{i}</annotation></semantics></math> features. Each modality-specific encoder <math id="S4.SS2.SSS1.p2.7.m7.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S4.SS2.SSS1.p2.7.m7.1a"><msub id="S4.SS2.SSS1.p2.7.m7.1.1" xref="S4.SS2.SSS1.p2.7.m7.1.1.cmml"><mi id="S4.SS2.SSS1.p2.7.m7.1.1.2" xref="S4.SS2.SSS1.p2.7.m7.1.1.2.cmml">f</mi><mi id="S4.SS2.SSS1.p2.7.m7.1.1.3" xref="S4.SS2.SSS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.7.m7.1b"><apply id="S4.SS2.SSS1.p2.7.m7.1.1.cmml" xref="S4.SS2.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.7.m7.1.1.1.cmml" xref="S4.SS2.SSS1.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p2.7.m7.1.1.2.cmml" xref="S4.SS2.SSS1.p2.7.m7.1.1.2">ùëì</ci><ci id="S4.SS2.SSS1.p2.7.m7.1.1.3.cmml" xref="S4.SS2.SSS1.p2.7.m7.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.7.m7.1c">f_{i}</annotation></semantics></math> processes input <math id="S4.SS2.SSS1.p2.8.m8.1" class="ltx_Math" alttext="X^{(i)}" display="inline"><semantics id="S4.SS2.SSS1.p2.8.m8.1a"><msup id="S4.SS2.SSS1.p2.8.m8.1.2" xref="S4.SS2.SSS1.p2.8.m8.1.2.cmml"><mi id="S4.SS2.SSS1.p2.8.m8.1.2.2" xref="S4.SS2.SSS1.p2.8.m8.1.2.2.cmml">X</mi><mrow id="S4.SS2.SSS1.p2.8.m8.1.1.1.3" xref="S4.SS2.SSS1.p2.8.m8.1.2.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.8.m8.1.1.1.3.1" xref="S4.SS2.SSS1.p2.8.m8.1.2.cmml">(</mo><mi id="S4.SS2.SSS1.p2.8.m8.1.1.1.1" xref="S4.SS2.SSS1.p2.8.m8.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.SS2.SSS1.p2.8.m8.1.1.1.3.2" xref="S4.SS2.SSS1.p2.8.m8.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.8.m8.1b"><apply id="S4.SS2.SSS1.p2.8.m8.1.2.cmml" xref="S4.SS2.SSS1.p2.8.m8.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.8.m8.1.2.1.cmml" xref="S4.SS2.SSS1.p2.8.m8.1.2">superscript</csymbol><ci id="S4.SS2.SSS1.p2.8.m8.1.2.2.cmml" xref="S4.SS2.SSS1.p2.8.m8.1.2.2">ùëã</ci><ci id="S4.SS2.SSS1.p2.8.m8.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.8.m8.1.1.1.1">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.8.m8.1c">X^{(i)}</annotation></semantics></math> to produce representation <math id="S4.SS2.SSS1.p2.9.m9.1" class="ltx_Math" alttext="Z^{(i)}" display="inline"><semantics id="S4.SS2.SSS1.p2.9.m9.1a"><msup id="S4.SS2.SSS1.p2.9.m9.1.2" xref="S4.SS2.SSS1.p2.9.m9.1.2.cmml"><mi id="S4.SS2.SSS1.p2.9.m9.1.2.2" xref="S4.SS2.SSS1.p2.9.m9.1.2.2.cmml">Z</mi><mrow id="S4.SS2.SSS1.p2.9.m9.1.1.1.3" xref="S4.SS2.SSS1.p2.9.m9.1.2.cmml"><mo stretchy="false" id="S4.SS2.SSS1.p2.9.m9.1.1.1.3.1" xref="S4.SS2.SSS1.p2.9.m9.1.2.cmml">(</mo><mi id="S4.SS2.SSS1.p2.9.m9.1.1.1.1" xref="S4.SS2.SSS1.p2.9.m9.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.SS2.SSS1.p2.9.m9.1.1.1.3.2" xref="S4.SS2.SSS1.p2.9.m9.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.9.m9.1b"><apply id="S4.SS2.SSS1.p2.9.m9.1.2.cmml" xref="S4.SS2.SSS1.p2.9.m9.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p2.9.m9.1.2.1.cmml" xref="S4.SS2.SSS1.p2.9.m9.1.2">superscript</csymbol><ci id="S4.SS2.SSS1.p2.9.m9.1.2.2.cmml" xref="S4.SS2.SSS1.p2.9.m9.1.2.2">ùëç</ci><ci id="S4.SS2.SSS1.p2.9.m9.1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.9.m9.1.1.1.1">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.9.m9.1c">Z^{(i)}</annotation></semantics></math>:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.3" class="ltx_Math" alttext="Z^{(i)}=f_{i}(X^{(i)})" display="block"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml"><msup id="S4.E1.m1.3.3.3" xref="S4.E1.m1.3.3.3.cmml"><mi id="S4.E1.m1.3.3.3.2" xref="S4.E1.m1.3.3.3.2.cmml">Z</mi><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.3.1" xref="S4.E1.m1.3.3.3.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.3.3.3.cmml">)</mo></mrow></msup><mo id="S4.E1.m1.3.3.2" xref="S4.E1.m1.3.3.2.cmml">=</mo><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.cmml"><msub id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.3.3.1.3.cmml"><mi id="S4.E1.m1.3.3.1.3.2" xref="S4.E1.m1.3.3.1.3.2.cmml">f</mi><mi id="S4.E1.m1.3.3.1.3.3" xref="S4.E1.m1.3.3.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.2.cmml">‚Äã</mo><mrow id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.3.3.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.cmml">(</mo><msup id="S4.E1.m1.3.3.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.2.cmml">X</mi><mrow id="S4.E1.m1.2.2.1.3" xref="S4.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.3.1" xref="S4.E1.m1.3.3.1.1.1.1.cmml">(</mo><mi id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml">i</mi><mo stretchy="false" id="S4.E1.m1.2.2.1.3.2" xref="S4.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></msup><mo stretchy="false" id="S4.E1.m1.3.3.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3"><eq id="S4.E1.m1.3.3.2.cmml" xref="S4.E1.m1.3.3.2"></eq><apply id="S4.E1.m1.3.3.3.cmml" xref="S4.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3">superscript</csymbol><ci id="S4.E1.m1.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.2">ùëç</ci><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">ùëñ</ci></apply><apply id="S4.E1.m1.3.3.1.cmml" xref="S4.E1.m1.3.3.1"><times id="S4.E1.m1.3.3.1.2.cmml" xref="S4.E1.m1.3.3.1.2"></times><apply id="S4.E1.m1.3.3.1.3.cmml" xref="S4.E1.m1.3.3.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.3.1.cmml" xref="S4.E1.m1.3.3.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.1.3.2.cmml" xref="S4.E1.m1.3.3.1.3.2">ùëì</ci><ci id="S4.E1.m1.3.3.1.3.3.cmml" xref="S4.E1.m1.3.3.1.3.3">ùëñ</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1">superscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.2">ùëã</ci><ci id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1.1">ùëñ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">Z^{(i)}=f_{i}(X^{(i)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Additive fusion combines modality-specific representations:</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="Z_{\text{fused}}=\sum_{i=1}^{M}Z^{(i)}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.2" xref="S4.E2.m1.1.2.cmml"><msub id="S4.E2.m1.1.2.2" xref="S4.E2.m1.1.2.2.cmml"><mi id="S4.E2.m1.1.2.2.2" xref="S4.E2.m1.1.2.2.2.cmml">Z</mi><mtext id="S4.E2.m1.1.2.2.3" xref="S4.E2.m1.1.2.2.3a.cmml">fused</mtext></msub><mo rspace="0.111em" id="S4.E2.m1.1.2.1" xref="S4.E2.m1.1.2.1.cmml">=</mo><mrow id="S4.E2.m1.1.2.3" xref="S4.E2.m1.1.2.3.cmml"><munderover id="S4.E2.m1.1.2.3.1" xref="S4.E2.m1.1.2.3.1.cmml"><mo movablelimits="false" id="S4.E2.m1.1.2.3.1.2.2" xref="S4.E2.m1.1.2.3.1.2.2.cmml">‚àë</mo><mrow id="S4.E2.m1.1.2.3.1.2.3" xref="S4.E2.m1.1.2.3.1.2.3.cmml"><mi id="S4.E2.m1.1.2.3.1.2.3.2" xref="S4.E2.m1.1.2.3.1.2.3.2.cmml">i</mi><mo id="S4.E2.m1.1.2.3.1.2.3.1" xref="S4.E2.m1.1.2.3.1.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.2.3.1.2.3.3" xref="S4.E2.m1.1.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.2.3.1.3" xref="S4.E2.m1.1.2.3.1.3.cmml">M</mi></munderover><msup id="S4.E2.m1.1.2.3.2" xref="S4.E2.m1.1.2.3.2.cmml"><mi id="S4.E2.m1.1.2.3.2.2" xref="S4.E2.m1.1.2.3.2.2.cmml">Z</mi><mrow id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.2.3.2.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.3.1" xref="S4.E2.m1.1.2.3.2.cmml">(</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.E2.m1.1.1.1.3.2" xref="S4.E2.m1.1.2.3.2.cmml">)</mo></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.2.cmml" xref="S4.E2.m1.1.2"><eq id="S4.E2.m1.1.2.1.cmml" xref="S4.E2.m1.1.2.1"></eq><apply id="S4.E2.m1.1.2.2.cmml" xref="S4.E2.m1.1.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.2.2.1.cmml" xref="S4.E2.m1.1.2.2">subscript</csymbol><ci id="S4.E2.m1.1.2.2.2.cmml" xref="S4.E2.m1.1.2.2.2">ùëç</ci><ci id="S4.E2.m1.1.2.2.3a.cmml" xref="S4.E2.m1.1.2.2.3"><mtext mathsize="70%" id="S4.E2.m1.1.2.2.3.cmml" xref="S4.E2.m1.1.2.2.3">fused</mtext></ci></apply><apply id="S4.E2.m1.1.2.3.cmml" xref="S4.E2.m1.1.2.3"><apply id="S4.E2.m1.1.2.3.1.cmml" xref="S4.E2.m1.1.2.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.2.3.1.1.cmml" xref="S4.E2.m1.1.2.3.1">superscript</csymbol><apply id="S4.E2.m1.1.2.3.1.2.cmml" xref="S4.E2.m1.1.2.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.2.3.1.2.1.cmml" xref="S4.E2.m1.1.2.3.1">subscript</csymbol><sum id="S4.E2.m1.1.2.3.1.2.2.cmml" xref="S4.E2.m1.1.2.3.1.2.2"></sum><apply id="S4.E2.m1.1.2.3.1.2.3.cmml" xref="S4.E2.m1.1.2.3.1.2.3"><eq id="S4.E2.m1.1.2.3.1.2.3.1.cmml" xref="S4.E2.m1.1.2.3.1.2.3.1"></eq><ci id="S4.E2.m1.1.2.3.1.2.3.2.cmml" xref="S4.E2.m1.1.2.3.1.2.3.2">ùëñ</ci><cn type="integer" id="S4.E2.m1.1.2.3.1.2.3.3.cmml" xref="S4.E2.m1.1.2.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.2.3.1.3.cmml" xref="S4.E2.m1.1.2.3.1.3">ùëÄ</ci></apply><apply id="S4.E2.m1.1.2.3.2.cmml" xref="S4.E2.m1.1.2.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.2.3.2.1.cmml" xref="S4.E2.m1.1.2.3.2">superscript</csymbol><ci id="S4.E2.m1.1.2.3.2.2.cmml" xref="S4.E2.m1.1.2.3.2.2">ùëç</ci><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">ùëñ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Z_{\text{fused}}=\sum_{i=1}^{M}Z^{(i)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.3" class="ltx_p">A shared transformer encoder <math id="S4.SS2.SSS1.p5.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S4.SS2.SSS1.p5.1.m1.1a"><mi id="S4.SS2.SSS1.p5.1.m1.1.1" xref="S4.SS2.SSS1.p5.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p5.1.m1.1b"><ci id="S4.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p5.1.m1.1.1">ùëî</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p5.1.m1.1c">g</annotation></semantics></math> processes <math id="S4.SS2.SSS1.p5.2.m2.1" class="ltx_Math" alttext="Z_{\text{fused}}" display="inline"><semantics id="S4.SS2.SSS1.p5.2.m2.1a"><msub id="S4.SS2.SSS1.p5.2.m2.1.1" xref="S4.SS2.SSS1.p5.2.m2.1.1.cmml"><mi id="S4.SS2.SSS1.p5.2.m2.1.1.2" xref="S4.SS2.SSS1.p5.2.m2.1.1.2.cmml">Z</mi><mtext id="S4.SS2.SSS1.p5.2.m2.1.1.3" xref="S4.SS2.SSS1.p5.2.m2.1.1.3a.cmml">fused</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p5.2.m2.1b"><apply id="S4.SS2.SSS1.p5.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p5.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p5.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p5.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p5.2.m2.1.1.2">ùëç</ci><ci id="S4.SS2.SSS1.p5.2.m2.1.1.3a.cmml" xref="S4.SS2.SSS1.p5.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS2.SSS1.p5.2.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p5.2.m2.1.1.3">fused</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p5.2.m2.1c">Z_{\text{fused}}</annotation></semantics></math> to produce final encoded representation <math id="S4.SS2.SSS1.p5.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.SS2.SSS1.p5.3.m3.1a"><mi id="S4.SS2.SSS1.p5.3.m3.1.1" xref="S4.SS2.SSS1.p5.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p5.3.m3.1b"><ci id="S4.SS2.SSS1.p5.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p5.3.m3.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p5.3.m3.1c">H</annotation></semantics></math>:</p>
</div>
<div id="S4.SS2.SSS1.p6" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="H=g(Z_{\text{fused}})" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml">H</mi><mo id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.3" xref="S4.E3.m1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.2" xref="S4.E3.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.1.1.2.cmml">Z</mi><mtext id="S4.E3.m1.1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.3a.cmml">fused</mtext></msub><mo stretchy="false" id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"></eq><ci id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3">ùêª</ci><apply id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><times id="S4.E3.m1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.2"></times><ci id="S4.E3.m1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.3">ùëî</ci><apply id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1.1.2">ùëç</ci><ci id="S4.E3.m1.1.1.1.1.1.1.3a.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S4.E3.m1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.1.3">fused</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">H=g(Z_{\text{fused}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS1.p7" class="ltx_para">
<p id="S4.SS2.SSS1.p7.1" class="ltx_p"><math id="S4.SS2.SSS1.p7.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S4.SS2.SSS1.p7.1.m1.1a"><mi id="S4.SS2.SSS1.p7.1.m1.1.1" xref="S4.SS2.SSS1.p7.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p7.1.m1.1b"><ci id="S4.SS2.SSS1.p7.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p7.1.m1.1.1">ùêª</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p7.1.m1.1c">H</annotation></semantics></math> undergoes positional encoding and is fed into a classification head (linear layer) for affective state recognition.</p>
</div>
<div id="S4.SS2.SSS1.p8" class="ltx_para">
<p id="S4.SS2.SSS1.p8.1" class="ltx_p">This method captures interactions between diverse modalities while leveraging the transformer architecture‚Äôs ability to model temporal dependencies and complex relationships within the fused representation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.5.1.1" class="ltx_text">IV-B</span>2 </span>Hyper-parameters</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The model‚Äôs hyperparameters were selected using a gridsearch algorithm, testing various combinations on the full dataset. For each hyperparameter, we explored four different options, with numerical parameters varied by factors of 10. This search revealed that the model‚Äôs performance was most sensitive to the learning rate and the choice of optimizer, while other parameters exhibited robust performance across a range of values.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The final hyperparameter configuration for the transformer model was as follows:</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Transformer networks: <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">1</span></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Batch size: <span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">1024</span></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Learning rate: <span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">0.0001</span></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Number of epochs: <span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">50</span></p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Attention heads: <span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">2</span></p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">Optimizer: <span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">RMSprop</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">We used k-fold (k = 29), training on 28 groups, and testing on the remaining one, resulting in 29 total evaluations. This approach was applied across all models, with multiple runs using different random seeds for robustness.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p">To mitigate overfitting, we implemented early stopping during the training process with a patience value of 5. This approach ensured that the model‚Äôs training was halted when no improvement was observed in the validation loss for five consecutive epochs, thereby optimizing the model‚Äôs generalization.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluated our proposed transformer model using various input modality configurations. Table¬†<a href="#S5.T2" title="TABLE II ‚Ä£ V Results ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents an ablation study, showing F1 scores for different combinations of input modalities.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.10.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S5.T2.11.2" class="ltx_text" style="font-size:90%;">Performance Comparison: Modality Combinations for Affective State Prediction</span></figcaption>
<table id="S5.T2.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.8.9.1" class="ltx_tr">
<th id="S5.T2.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.8.9.1.1.1" class="ltx_text" style="font-size:90%;">Configuration</span></th>
<th id="S5.T2.8.9.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.8.9.1.2.1" class="ltx_text" style="font-size:90%;">F1 Score (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.1.1.2.1" class="ltx_text" style="font-size:90%;">GPT-4(V) Only</span></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mn mathsize="90%" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><cn type="integer" id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">23</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.2.2.2.1" class="ltx_text" style="font-size:90%;">Full Context (FC)</span></td>
<td id="S5.T2.2.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.2.2.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S5.T2.2.2.1.m1.1a"><mn mathsize="90%" id="S5.T2.2.2.1.m1.1.1" xref="S5.T2.2.2.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.1.m1.1b"><cn type="integer" id="S5.T2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.1.m1.1c">25</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.3.3.2.1" class="ltx_text" style="font-size:90%;">Thermal Data</span></td>
<td id="S5.T2.3.3.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.3.3.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.T2.3.3.1.m1.1a"><mn mathsize="90%" id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><cn type="integer" id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">30</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.4.4" class="ltx_tr">
<td id="S5.T2.4.4.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.4.4.2.1" class="ltx_text" style="font-size:90%;">Thermal + FC</span></td>
<td id="S5.T2.4.4.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.4.4.1.m1.1" class="ltx_Math" alttext="58" display="inline"><semantics id="S5.T2.4.4.1.m1.1a"><mn mathsize="90%" id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml">58</mn><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><cn type="integer" id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1">58</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">58</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.5.5" class="ltx_tr">
<td id="S5.T2.5.5.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.5.5.2.1" class="ltx_text" style="font-size:90%;">Action Units (AU)</span></td>
<td id="S5.T2.5.5.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.5.5.1.m1.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S5.T2.5.5.1.m1.1a"><mn mathsize="90%" id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><cn type="integer" id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">65</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.6.6" class="ltx_tr">
<td id="S5.T2.6.6.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.6.6.2.1" class="ltx_text" style="font-size:90%;">AU + FC</span></td>
<td id="S5.T2.6.6.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.6.6.1.m1.1" class="ltx_Math" alttext="75" display="inline"><semantics id="S5.T2.6.6.1.m1.1a"><mn mathsize="90%" id="S5.T2.6.6.1.m1.1.1" xref="S5.T2.6.6.1.m1.1.1.cmml">75</mn><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.1.m1.1b"><cn type="integer" id="S5.T2.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.1.m1.1.1">75</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.1.m1.1c">75</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.7.7" class="ltx_tr">
<td id="S5.T2.7.7.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.7.7.2.1" class="ltx_text" style="font-size:90%;">Thermal + AU + GOC</span></td>
<td id="S5.T2.7.7.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.7.7.1.m1.1" class="ltx_Math" alttext="76" display="inline"><semantics id="S5.T2.7.7.1.m1.1a"><mn mathsize="90%" id="S5.T2.7.7.1.m1.1.1" xref="S5.T2.7.7.1.m1.1.1.cmml">76</mn><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.1.m1.1b"><cn type="integer" id="S5.T2.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1">76</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.1.m1.1c">76</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.8.8" class="ltx_tr">
<td id="S5.T2.8.8.2" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.8.8.2.1" class="ltx_text" style="font-size:90%;">Thermal + AU</span></td>
<td id="S5.T2.8.8.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S5.T2.8.8.1.m1.1" class="ltx_Math" alttext="84" display="inline"><semantics id="S5.T2.8.8.1.m1.1a"><mn mathsize="90%" id="S5.T2.8.8.1.m1.1.1" xref="S5.T2.8.8.1.m1.1.1.cmml">84</mn><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.1.m1.1b"><cn type="integer" id="S5.T2.8.8.1.m1.1.1.cmml" xref="S5.T2.8.8.1.m1.1.1">84</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.1.m1.1c">84</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.8.10.1" class="ltx_tr">
<td id="S5.T2.8.10.1.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.8.10.1.1.1" class="ltx_text" style="font-size:90%;">Thermal + AU + FC</span></td>
<td id="S5.T2.8.10.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.8.10.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">89</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Thermal + Action Units + Full Context</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The combination of Thermal, Action Units (AU), and Full Context (FC) modalities yielded an F1 score of 89%. Fig.¬†<a href="#S5.F3" title="Figure 3 ‚Ä£ V-A Thermal + Action Units + Full Context ‚Ä£ V Results ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the normalized confusion matrix for this configuration.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2409.11906/assets/figs/thermal+FC+AU.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Confusion Matrix for Thermal + AU + FC configuration.</span></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The model detected the neutral (baseline) state with 91.1% accuracy. Enjoyment recognition achieved the highest accuracy at 96.9%. Boredom detection showed an accuracy of 78.3%, which is lower compared to other emotional states. Frustration detection reached an accuracy of 85.8%.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Misclassifications were relatively low across categories. Notably, 10.83% of boredom instances were misclassified as neutral, and 10.45% of frustration instances were misclassified as enjoyment. Other misclassification rates remained below 10%.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Thermal + Action Units + Game-Only Context</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Excluding facial descriptions while retaining thermal data, action units, and game-only context resulted in an F1 score of 76% and an average accuracy of 79.53%. Baseline and enjoyment states were detected with high accuracy (84% and 89%, respectively), while boredom exhibited some confusion with enjoyment and baseline states.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Thermal + Full Context</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The Thermal + FC configuration achieved an F1 score of 57.51% and an average accuracy of 63.78% (see Figure¬†<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:AUThermalConf</span>. The confusion matrix for this setup is as follows:</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">This configuration showed moderate performance, with the baseline state being the most accurately detected. However, there was notable confusion between other affective states, particularly between boredom and enjoyment, and between frustration and baseline.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Individual Modalities</span>
</h3>

<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS1.5.1.1" class="ltx_text">V-D</span>1 </span>GPT-4V</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">We have also used GPT-4V on each frame of the video with the prompt ‚ÄùGiven the facial expressions and the context of playing a pacman game. Detect one output of four emotional states of the person in the image: Baseline, Boredom, Frustration and Enjoyment. Only write the output‚Äù
This resulted in an f1 score of 23%, with major misclassification between all classes.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS2.5.1.1" class="ltx_text">V-D</span>2 </span>Full Context Only</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para">
<p id="S5.SS4.SSS2.p1.1" class="ltx_p">Using only FC resulted in a low F1 score of 25.76% and an average accuracy of 32.14%, indicating significant misclassification across all affective states.</p>
</div>
</section>
<section id="S5.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS3.5.1.1" class="ltx_text">V-D</span>3 </span>Action Units Only</h4>

<div id="S5.SS4.SSS3.p1" class="ltx_para">
<p id="S5.SS4.SSS3.p1.1" class="ltx_p">The AU-only configuration achieved an F1 score of 64.85% and an average accuracy of 67.38%. It showed high accuracy for the baseline state (93.34%) but exhibited confusion between frustration and enjoyment states.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The results of our study underscore the importance of multimodal input configurations for affective state detection. The proposed transformer model demonstrated an improved performance when combining thermal data, action units (AU), and contextual information, achieving an F1 score of 89%. This finding aligns with previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that emphasize the effectiveness of multimodal approaches in affect recognition.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The configuration using thermal data and AU alone yielded an F1 score of 84%, which is a substantial improvement over the use of either modality independently (30% for thermal data alone and 65% for AU alone). This result corroborates earlier research by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> on the value of thermal imagery and facial action units in emotion recognition. However, our results indicate that the combination of these modalities is more effective than using them in isolation, supporting the hypothesis that thermal and facial action data capture distinct but complementary aspects of affective expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Incorporating contextual information further enhanced the model‚Äôs performance, as evidenced by the increase in the F1 score to 89%. This improvement is particularly noteworthy in the recognition of enjoyment and frustration states, which showed significant gains in detection accuracy. This finding is supported by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, who demonstrated that context-aware models could significantly enhance emotion recognition by providing additional situational cues that help disambiguate similar affective states.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Interestingly, the addition of Game Only-Context (GOC) led to a decreased F1 score compared to using only thermal and AU modalities, suggesting that GOC may introduce noise rather than providing sufficient context for the transformer. In contrast, the addition of Full-Context (FC) improved the accuracy of thermal data from 30% to 58% and AU data from 65% to 75%. These findings align with previous research by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, emphasizing that the quality of added modalities is crucial, not just their quantity.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">The reduction in F1 score when incorporating certain additional modalities is not unprecedented. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> observed similar effects in multimodal affect recognition, attributing such decreases to potential inter-modality conflicts or insufficient integration strategies.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Unlike the results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, our transformer-based model with FC data (Figure <a href="#S5.F3" title="Figure 3 ‚Ä£ V-A Thermal + Action Units + Full Context ‚Ä£ V Results ‚Ä£ Fusion in Context: A Multimodal Approach to Affective State Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) nearly eliminates confusion between enjoyment and frustration, dramatically improving their classification (95.15% and 85.82% respectively).</p>
</div>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p">The FC-only configuration performed poorly, with an F1 score of 25.76%, indicating that contextual information alone is insufficient for accurate affective state detection. This is consistent with the findings of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, who reported that while context can enhance emotion recognition, it cannot replace direct physiological or facial cues. Similarly, the single-modality configurations (thermal data or AU alone) showed limitations, particularly in differentiating between enjoyment and frustration or boredom and other states. These results underscore the necessity of multimodal approaches for affective state detection, as single modalities lack the comprehensive coverage needed to capture the full spectrum of affective state expressions.</p>
</div>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">Despite using a 7-second analysis window, our system is designed for real-time implementation on robots. This approach is effective because the system‚Äôs primary goal is to detect the user‚Äôs affective state during specific tasks, which typically takes 5-15 seconds.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our study demonstrates the efficacy of multimodal integration for affective state detection. By fusing thermal data, action units, and contextual information, our transformer-based model achieved an impressive F1 score of 89%, outperforming GPT-4(V)‚Äôs 23%. This performance difference can be attributed to two key factors: First, our multimodal approach provides a more comprehensive view of affective states, capturing nuances that may be missed in purely visual analysis. Second, unlike GPT-4(V)‚Äôs general-purpose design, our model is specifically tailored for affective state detection in our experimental context.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">These findings underscore the importance of diverse data sources and advanced fusion techniques in developing accurate affective state detection systems. Moreover, they show the potential for more advancements in affective computing through targeted multimodal approaches and specialized model architectures.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span id="bib.bib1.1.1" class="ltx_text ltx_font_smallcaps">Baltru≈°aitis, T., Ahuja, C., and Morency, L.-P.</span>

</span>
<span class="ltx_bibblock">Challenges and applications in multimodal machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text ltx_font_italic">The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition-Volume 2</span> (2018), 17‚Äì48.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<span id="bib.bib2.1.1" class="ltx_text ltx_font_smallcaps">Baltru≈°aitis, T., Ahuja, C., and Morency, L.-P.</span>

</span>
<span class="ltx_bibblock">Multimodal machine learning: A survey and taxonomy.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence 41</span>, 2 (2018), 423‚Äì443.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<span id="bib.bib3.1.1" class="ltx_text ltx_font_smallcaps">Bano, S., Suveges, T., Zhang, J., and Mckenna, S.¬†J.</span>

</span>
<span class="ltx_bibblock">Multimodal egocentric analysis of focused interactions.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text ltx_font_italic">IEEE Access 6</span> (2018), 37493‚Äì37505.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span id="bib.bib4.1.1" class="ltx_text ltx_font_smallcaps">Barrett, L.¬†F., Adolphs, R., Marsella, S., Martinez, A.¬†M., and Pollak, S.¬†D.</span>

</span>
<span class="ltx_bibblock">Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text ltx_font_italic">Psychological science in the public interest 20</span>, 1 (2019), 1‚Äì68.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
<span id="bib.bib5.1.1" class="ltx_text ltx_font_smallcaps">Bosch, N.</span>

</span>
<span class="ltx_bibblock">Multimodal affect detection in the wild: Accuracy, availability, and generalizability.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.2.1" class="ltx_text ltx_font_italic">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</span> (2015), pp.¬†645‚Äì649.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
<span id="bib.bib6.1.1" class="ltx_text ltx_font_smallcaps">Busso, C., Deng, Z., Yildirim, S., Bulut, M., Lee, C.¬†M., Kazemzadeh, A., Lee, S., Neumann, U., and Narayanan, S.</span>

</span>
<span class="ltx_bibblock">Analysis of emotion recognition using facial expressions, speech and multimodal information.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.2.1" class="ltx_text ltx_font_italic">Proceedings of the 6th International Conference on Multimodal Interfaces (ICMI)</span> (2004), pp.¬†205‚Äì211.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<span id="bib.bib7.1.1" class="ltx_text ltx_font_smallcaps">Ekman, P., and Friesen, W.¬†V.</span>

</span>
<span class="ltx_bibblock">Facial action coding system.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text ltx_font_italic">Environmental Psychology &amp; Nonverbal Behavior</span> (1978).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<span id="bib.bib8.1.1" class="ltx_text ltx_font_smallcaps">Faye, B., Azzag, H., Lebbah, M., and Bouchaffra, D.</span>

</span>
<span class="ltx_bibblock">Context-based multimodal fusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.04650</span> (2024).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
<span id="bib.bib9.1.1" class="ltx_text ltx_font_smallcaps">Gunes, H., and Schuller, B.</span>

</span>
<span class="ltx_bibblock">Automatic, dimensional and continuous emotion recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text ltx_font_italic">International Journal of Synthetic Emotions (IJSE) 1</span>, 1 (2010), 68‚Äì99.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
<span id="bib.bib10.1.1" class="ltx_text ltx_font_smallcaps">Guneysu¬†Ozgur, A., Wessel, M.¬†J., Johal, W., Sharma, K., √ñzg√ºr, A., Vuadens, P., Mondada, F., Hummel, F.¬†C., and Dillenbourg, P.</span>

</span>
<span class="ltx_bibblock">Iterative design of an upper limb rehabilitation game with tangible robots.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.2.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction</span> (New York, NY, USA, 2018), HRI ‚Äô18, Association for Computing Machinery, p.¬†241‚Äì250.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
<span id="bib.bib11.1.1" class="ltx_text ltx_font_smallcaps">Hoang, M.-H., Kim, S.-H., Yang, H.-J., and Lee, G.-S.</span>

</span>
<span class="ltx_bibblock">Context-aware emotion recognition based on visual relationship detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text ltx_font_italic">IEEE Access 9</span> (2021), 90465‚Äì90474.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
<span id="bib.bib12.1.1" class="ltx_text ltx_font_smallcaps">Jiang, Q., Chen, C., Zhao, H., Chen, L., Ping, Q., Tran, S.¬†D., Xu, Y., Zeng, B., and Chilimbi, T.</span>

</span>
<span class="ltx_bibblock">Understanding and constructing latent modality structures in multi-modal representation learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.05952</span> (2023).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
<span id="bib.bib13.1.1" class="ltx_text ltx_font_smallcaps">Kim, Y., Lee, H., and Kim, B.</span>

</span>
<span class="ltx_bibblock">Emotion in context: Deep semantic feature fusion for video emotion recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.2.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM on Multimedia Conference</span> (2016), ACM, pp.¬†1275‚Äì1284.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
<span id="bib.bib14.1.1" class="ltx_text ltx_font_smallcaps">Lemaignan, S., Andriella, A., Ferrini, L., Juricic, L., Mohamed, Y., and Ros, R.</span>

</span>
<span class="ltx_bibblock">Social embeddings: Concept and initial investigation.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text ltx_font_italic">Open Research Europe 4</span>, 63 (2024), 63.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
<span id="bib.bib15.1.1" class="ltx_text ltx_font_smallcaps">Liang, P.¬†P., Zadeh, A., and Morency, L.-P.</span>

</span>
<span class="ltx_bibblock">Foundations and trends in multimodal machine learning: Principles, challenges, and open questions.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.03430</span> (2022).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
<span id="bib.bib16.1.1" class="ltx_text ltx_font_smallcaps">Lu, H., Niu, X., Wang, J., Wang, Y., Hu, Q., Tang, J., Zhang, Y., Yuan, K., Huang, B., Yu, Z., et¬†al.</span>

</span>
<span class="ltx_bibblock">Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.05916</span> (2024).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
<span id="bib.bib17.1.1" class="ltx_text ltx_font_smallcaps">Ma, F., Sun, B., and Li, S.</span>

</span>
<span class="ltx_bibblock">Facial expression recognition with visual transformers and attentional selective fusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing 14</span>, 2 (2021), 1236‚Äì1248.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
<span id="bib.bib18.1.1" class="ltx_text ltx_font_smallcaps">Mittal, T., Bera, A., and Manocha, D.</span>

</span>
<span class="ltx_bibblock">Multimodal and context-aware emotion perception model with multiplicative fusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text ltx_font_italic">IEEE MultiMedia 28</span>, 2 (2021), 67‚Äì75.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
<span id="bib.bib19.1.1" class="ltx_text ltx_font_smallcaps">Mohamed, Y., Ballardini, G., Parreira, M.¬†T., Lemaignan, S., and Leite, I.</span>

</span>
<span class="ltx_bibblock">Automatic frustration detection using thermal imaging.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.2.1" class="ltx_text ltx_font_italic">2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</span> (2022), IEEE, pp.¬†451‚Äì459.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
<span id="bib.bib20.1.1" class="ltx_text ltx_font_smallcaps">Mohamed, Y., G√ºneysu, A., Lemaignan, S., and Leite, I.</span>

</span>
<span class="ltx_bibblock">Multi-modal affect detection using thermal and optical imaging in a gamified robotic exercise.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text ltx_font_italic">International Journal of Social Robotics</span> (2023), 1‚Äì17.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
<span id="bib.bib21.1.1" class="ltx_text ltx_font_smallcaps">OpenAI</span>.

</span>
<span class="ltx_bibblock">Openai embeddings: Language models as apis.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text ltx_font_italic">OpenAI Blog</span> (March 2021).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
<span id="bib.bib22.1.1" class="ltx_text ltx_font_smallcaps">Paul, E., and Rosenberg, E.¬†E.</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text ltx_font_italic">What the face reveals : basic and applied studies of spontaneous expression using the facial action coding system (FACS)</span>.

</span>
<span class="ltx_bibblock">Oxford University Press, 2005.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
<span id="bib.bib23.1.1" class="ltx_text ltx_font_smallcaps">Pavlidis, I., Eberhardt, N.¬†L., and Levine, J.¬†A.</span>

</span>
<span class="ltx_bibblock">Seeing through the face of deception.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text ltx_font_italic">Nature 415</span>, 6867 (2002), 35‚Äì35.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
<span id="bib.bib24.1.1" class="ltx_text ltx_font_smallcaps">Poria, S., Cambria, E., Bajpai, R., and Hussain, A.</span>

</span>
<span class="ltx_bibblock">A review of affective computing: From unimodal analysis to multimodal fusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text ltx_font_italic">Information Fusion 37</span> (2017), 98‚Äì125.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
<span id="bib.bib25.1.1" class="ltx_text ltx_font_smallcaps">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.¬†N., Kaiser, ≈Å., and Polosukhin, I.</span>

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems 30</span> (2017).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
<span id="bib.bib26.1.1" class="ltx_text ltx_font_smallcaps">Wagner, J., Andre, E., Lingenfelser, F., and Kim, J.</span>

</span>
<span class="ltx_bibblock">Exploring fusion methods for multimodal emotion recognition with missing data.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing 2</span>, 4 (2011), 206‚Äì218.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
<span id="bib.bib27.1.1" class="ltx_text ltx_font_smallcaps">Wang, J., Liu, B., Miao, C., Zhao, Z., Zhuang, W., Chu, Q., and Yu, N.</span>

</span>
<span class="ltx_bibblock">Exploiting modality-specific features for multi-modal manipulation detection and grounding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.2.1" class="ltx_text ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span> (2024), IEEE, pp.¬†4935‚Äì4939.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
<span id="bib.bib28.1.1" class="ltx_text ltx_font_smallcaps">Wang, Y., Li, M., Liu, S., and Li, M.</span>

</span>
<span class="ltx_bibblock">Context-aware emotion recognition networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.05913</span> (2019).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.11905" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.11906" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.11906">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.11906" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.11907" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:19:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
