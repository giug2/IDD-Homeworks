<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>HLB: Benchmarking LLMs’ Humanlikeness in Language Use</title>
<!--Generated on Tue Sep 24 09:10:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15890v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S1" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S2" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S2.SS1" title="In 2 Related Work ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Psychological Experimentation on LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S2.SS2" title="In 2 Related Work ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Psycholinguistic Experimentation on LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3.SS1" title="In 3 Methodology ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Human Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3.SS2" title="In 3 Methodology ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>LLM Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3.SS3" title="In 3 Methodology ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Response Coding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3.SS4" title="In 3 Methodology ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Humanlikeness Scoring</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S4" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Result</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S4.SS1" title="In 4 Result ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Comparative Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S4.SS2" title="In 4 Result ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Case analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S5" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S6" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S7" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#A1" title="In HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">HLB: Benchmarking LLMs’ Humanlikeness in Language Use</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Xufeng Duan<sup class="ltx_sup" id="id1.1.id1.1">1</sup></span> <span class="ltx_text ltx_font_bold" id="id2.2.id2">Bei Xiao<sup class="ltx_sup" id="id2.2.id2.1">1</sup></span> <span class="ltx_text ltx_font_bold" id="id3.3.id3">Xuemei Tang<sup class="ltx_sup" id="id3.3.id3.1">1</sup></span> <span class="ltx_text ltx_font_bold" id="id4.4.id4">Zhenguang G. Cai<sup class="ltx_sup" id="id4.4.id4.1">1,2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id4.4.id4.2"><span class="ltx_text ltx_font_medium" id="id4.4.id4.2.1">1</span></sup></span>Department of Linguistics and Modern Languages, The Chinese University of Hong Kong
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">2</sup>Brain and Mind Institute, The Chinese University of Hong Kong
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.6.id6">xufeng.duan@link.cuhk.edu.hk</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see <a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/XX" title="">this link</a>). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.</p>
<p class="ltx_p" id="id8.id2">For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">HLB: Benchmarking LLMs’ Humanlikeness in Language Use</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Xufeng Duan<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1</sup> Bei Xiao<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">1</sup> Xuemei Tang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">1</sup> Zhenguang G. Cai<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">1,2</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1">1</sup>Department of Linguistics and Modern Languages, The Chinese University of Hong Kong</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">2</sup>Brain and Mind Institute, The Chinese University of Hong Kong</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.1">xufeng.duan@link.cuhk.edu.hk</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, large language models (LLMs) have made significant advancements. Models like OpenAI’s GPT series and Meta’s Llama family can generate human-like text, engage in coherent dialogues, and answer complex questions, often producing responses that are indistinguishable from those of humans in certain evaluations <cite class="ltx_cite ltx_citemacro_cite">Tsubota and Kano (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib37" title="">2024</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib4" title="">2024</a>)</cite> conducted a systematic evaluation of human-like language use in models such as ChatGPT and Vicuna, demonstrating that LLMs closely replicate human language patterns in many aspects. However, despite these successes, questions remain about how accurately these models capture the deeper, nuanced patterns of human language use. In other words, the full extent of their similarity to human behavior remains unclear.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The importance of evaluating humanlikeness in language use is further underscored by the increasing reliance on synthetic data for model training, particularly in dialogue models. While synthetic data generation facilitates efficient scaling of model training, it raises concerns about models diverging from real-world human language patterns<cite class="ltx_cite ltx_citemacro_cite">del Rio-Chanona et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib7" title="">2024</a>)</cite>. Studies have shown that synthetic data can degrade model performance after retraining<cite class="ltx_cite ltx_citemacro_cite">Shumailov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib33" title="">2024</a>)</cite>. This makes it imperative to assess the humanlikeness of LLMs rigorously across various aspects of language use, to ensure that models do not lose the diversity and richness of human language data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address this challenge, we introduce a psycholinguistic benchmark designed to provide a systematic and comprehensive evaluation of how closely LLMs align with human linguistic behavior.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although numerous benchmarks and leaderboards have been developed to assess the performance of LLMs on downstream NLP tasks, they often fail to capture the finer, human-like qualities of language use. Current NLP benchmarks typically focus on task-based accuracy or performance <cite class="ltx_cite ltx_citemacro_cite">Lewkowycz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib20" title="">2022</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib41" title="">2023</a>); Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib26" title="">2024</a>); Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib15" title="">2021</a>); Zellers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib40" title="">2019</a>)</cite>, overlooking the broader psycholinguistic dimensions that characterize how humans process and produce language. Furthermore, few studies have systematically compared the language use of LLMs and human participants across multiple linguistic levels. This gap highlights the need for a new benchmark that can robustly measure the extent to which LLMs replicate human language behavior in real-world, diverse linguistic contexts.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we address this gap by presenting a psycholinguistic benchmark study that evaluates the humanlikeness of 20 LLMs. Our benchmark consists of 10 representative psycholinguistic experiments, adapted from <cite class="ltx_cite ltx_citemacro_citet">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib4" title="">2024</a>)</cite>, which cover five core linguistic aspects: sound, word, syntax, semantics, and discourse, with two experiments dedicated to each aspect (see <a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_tag">1</span></a>). We collected approximately 50 to 100 responses per item from over 2,000 human participants. Additionally, we gathered 100 responses per item from each of the 20 LLMs, including well-known models such as GPT-4o, GPT-3.5, Llama 2, Llama 3, Llama 3.1, and other state-of-the-art models (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_tag">1</span></a>). To quantify humanlikeness, we developed an auto-coding algorithm that efficiently and reliably extracts language use patterns from responses. The humanlikeness metric was then calculated based on the similarity between the response distributions of humans and LLMs, using a comparison of their probability distributions.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our findings reveal significant, nuanced differences in how LLMs perform across various linguistic aspects, offering a new benchmark for evaluating the humanlikeness of LLMs in natural language use. This benchmark introduces psycholinguistic methods to model evaluation and provides the first framework for systematically assessing the humanlikeness of LLMs in language use.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="492" id="S1.F1.g1" src="extracted/5875496/benchmark_figure1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The benchmark framework. The example prompt is taken from the sound-gender association task, where humans can infer the gender of a novel name (e.g., <span class="ltx_text ltx_font_italic" id="S1.F1.3.1">Pelcra</span> for Female; <span class="ltx_text ltx_font_italic" id="S1.F1.4.2">Pelcrad</span> for Male) based on phonology.</figcaption>
</figure>
<figure class="ltx_table" id="S1.T1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S1.T1.1">{tblr}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S1.T1.2">cell22 = r=2,
cell42 = r=2,
cell62 = r=2,
cell82 = r=2,
cell102 = r=2,
hline1-2,12 = -,
hline4,6,8,10 = 2-3,
colspec = p0.35cm p1.2cm p4.4cm,

Exp &amp; Level  Task 
<br class="ltx_break"/>E1  Sound  sound-shape association <cite class="ltx_cite ltx_citemacro_cite">Köhler (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib19" title="">1967</a>)</cite>
<br class="ltx_break"/>E2   sound-gender association <cite class="ltx_cite ltx_citemacro_cite">Cassidy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib5" title="">1999</a>)</cite>
<br class="ltx_break"/>E3  Word  word length and predictivity <cite class="ltx_cite ltx_citemacro_cite">Mahowald et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib21" title="">2013</a>)</cite>
<br class="ltx_break"/>E4   word meaning priming <cite class="ltx_cite ltx_citemacro_cite">Rodd et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib31" title="">2013</a>)</cite>
<br class="ltx_break"/>E5  Syntax  structural priming <cite class="ltx_cite ltx_citemacro_cite">Pickering and Branigan (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib27" title="">1998</a>)</cite>
<br class="ltx_break"/>E6   syntactic ambiguity resolution <cite class="ltx_cite ltx_citemacro_cite">Altmann and Steedman (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib2" title="">1988</a>)</cite>
<br class="ltx_break"/>E7  Meaning  implausible sentence interpretation <cite class="ltx_cite ltx_citemacro_cite">Gibson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib13" title="">2013</a>)</cite>
<br class="ltx_break"/>E8   semantic illusion <cite class="ltx_cite ltx_citemacro_cite">Erickson and Mattson (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib9" title="">1981</a>)</cite>
<br class="ltx_break"/>E9  Discourse  implicit causality <cite class="ltx_cite ltx_citemacro_cite">Garvey and Caramazza (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib12" title="">1974</a>)</cite>
<br class="ltx_break"/>E10   drawing inferences <cite class="ltx_cite ltx_citemacro_cite">Singer and Spear (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib34" title="">2015</a>)</cite></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The experiments in this benchmark.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Recent advances in LLMs have led to the development of various benchmarks designed to evaluate their linguistic capabilities. Standard benchmarks like GLUE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib39" title="">2018</a>)</cite> and SuperGLUE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib38" title="">2019</a>)</cite> assess models across a range of natural language processing (NLP) tasks, including sentence classification, textual entailment, and question answering. However, these benchmarks primarily focus on task-based accuracy and often overlook the more intricate aspects of humanlike language processing. While these evaluations provide valuable insights into model performance, they do not fully capture the extent to which LLMs comprehend and generate language in a humanlike manner. As <cite class="ltx_cite ltx_citemacro_citet">Manning et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib22" title="">2020</a>)</cite> note, LLMs are powerful statistical models that can identify patterns in vast datasets, but these benchmarks do not adequately test how well models replicate human patterns of language use due to the interplay of complex cognitive biases.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Psychological Experimentation on LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A growing body of research has begun applying classical psychological experiments to evaluate LLMs in more domain-specific and cognitively demanding tasks. For example, <cite class="ltx_cite ltx_citemacro_citet">Binz and Schulz (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib3" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Dasgupta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib6" title="">2023</a>)</cite> used well-known psychological paradigms, such as the Linda problem and the Wason selection task, to probe LLMs’ abilities in judgment and decision-making. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Sap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib32" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Trott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib36" title="">2023</a>)</cite> explored whether LLMs exhibit theory of mind, a key component of human social cognition, while <cite class="ltx_cite ltx_citemacro_citet">Miotto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib24" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Karra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib18" title="">2023</a>)</cite> examined LLMs’ personality traits. In the domain of behavioral economics, Horton (2023) conducted experiments with GPT-3 to explore its decision-making processes.
These studies suggest that LLMs can be treated as cognitive agents in psychological experiments, providing insights into how LLMs align with humans in reasoning, behavior, and decision-making. Moreover, they help shed light on the underlying mechanisms of LLMs, as seen in the work of <cite class="ltx_cite ltx_citemacro_citet">Huang and Chang (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib16" title="">2023</a>)</cite>and <cite class="ltx_cite ltx_citemacro_citet">Qiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib28" title="">2023</a>)</cite>, who analyzed reasoning patterns in LLMs. <cite class="ltx_cite ltx_citemacro_citet">Hagendorff (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib14" title="">2023</a>)</cite> further provided a comprehensive review of LLM performance in psychological tests, showing that while LLMs demonstrate sophisticated behaviors, they often diverge from human cognition. These divergences highlight the need for more robust frameworks to understand the limitations of LLMs in mimicking human thought processes.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Psycholinguistic Experimentation on LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Psycholinguistic approaches offer a deeper analysis by testing LLMs on how well they replicate the cognitive processes underlying human language processing. <cite class="ltx_cite ltx_citemacro_citet">Ettinger (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib10" title="">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Futrell (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib11" title="">2019</a>)</cite> have subjected models like BERT to psycholinguistic tasks such as syntactic ambiguity resolution and structural priming, revealing both the strengths and limitations of LLMs in replicating human language processing. <cite class="ltx_cite ltx_citemacro_citet">Michaelov and Bergen (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib23" title="">2023</a>)</cite> used structural priming tasks to investigate how LLMs internalize syntactic structures, while <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib17" title="">2024</a>)</cite> H examined LLMs’ ability to resolve syntactic ambiguity. <cite class="ltx_cite ltx_citemacro_citet">Qiu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib29" title="">2023</a>)</cite> explored how well LLMs handle pragmatic reasoning.
These studies demonstrate that LLMs can, to some extent, mimic humanlike behavior in controlled experiments. However, divergences in processing reveal the distinctions between machine learning models and humans. A recent review by <cite class="ltx_cite ltx_citemacro_citet">Demszky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib8" title="">2023</a>)</cite> emphasized the need for benchmarks that incorporate psychological paradigms to evaluate LLMs. The authors argue that by applying psycholinguistic methods, researchers can better understand how closely LLMs approximate human cognition and where they fall short.
Despite extensive research on LLMs’ performance across various tasks, there is still no benchmark that includes human language processing data to reveal the extent to which LLMs resemble humans, particularly in language use. This paper addresses that gap by adapting 10 psycholinguistic experiments from <cite class="ltx_cite ltx_citemacro_citet">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib4" title="">2024</a>)</cite> to evaluate how closely LLMs align with human language behavior, covering phenomena ranging from sound symbolism to discourse comprehension.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Human Experiments</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Experimental Design</span> The human experiments were conducted using Qualtrics, an online survey platform <cite class="ltx_cite ltx_citemacro_cite">Qualtrics (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib30" title="">2024</a>)</cite>. The study included ten psycholinguistic tasks that spanned various linguistic levels, from sound, word, syntax, and meaning to discourse comprehension, with two experiments for each level (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#A1" title="Appendix A Appendix ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_tag">A</span></a> for details). We exposed a participant to only one trial on each experiment, with a total of 10 trials across all the experiments. This setup minimized trial-level effects and facilitated direct comparisons with LLMs, which were tested under similar conditions (presenting instructions and stimuli in a single prompt) to avoid context effects within individual conversations.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Procedure</span> After providing consent, participants completed the ten psycholinguistic tasks (presented in a random order); four attention checks were randomly interspersed among the trials to later identify participants for random responding. Each experimental task began with an instructional screen, some of which included examples to clarify task requirements. The examples were carefully designed to differ from the experimental stimuli to prevent potential priming effects. For instance, in a sentence-completion task, an illustrative example that did not resemble the experimental stimuli and did not induce target words for any stimuli was used. The priming tasks (which included pairs of priming and target stimuli) were spread across multiple pages to avoid strategic responses in case participants realise the relation between the prime and the target. The overall experimental procedure was streamlined for clarity and efficiency, with each session lasting approximately 8 to 10 minutes (mean = 8.336, <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">SD</span> = 4.171).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Participants</span> Participants were recruited from the crowd-sourcing platform Quatrics and restricted to native English speakers residing in the UK and US, according to their registration on Prolific. They were required to use a desktop computer to complete the tasks. Among the 2,205 participants taking part in the experiments, 290 were excluded for not well adhering to the experimental instructions, including completing the study too quickly, showing low effort, or not finishing the experiment, according to the Qualtrics system. The remaining 1,915 participants were further checked for language nativeness and their accuracy with attention checks. After a thorough screening process—excluding those who were not native speakers, failed attention checks, or exhibited irregularities such as excessively short completion times or multiple participation attempts—the final valid sample consisted of 1,905 participants. The sample was composed of participants as follows: female (n = 1,051), male (n = 838), preferred not to disclose (n = 16), with an average age of 44.8 years (range: 18 to 89 years). Educational levels included: no formal education (n = 2), elementary school (n = 12), high school (n = 672), bachelor’s degree (n = 862), and master’s degree (n = 357). This sample of participants resulted in each item being tested in a minimum average of 24 trials (e.g., Word Length and Predictability) and up to an average of 96 trials (e.g., Sound-Shape Association Task).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LLM Experiments</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Experimental Design</span> To compare human responses with those generated by LLMs, we employed the same 10 psycholinguistic tasks designed for human participants. 20 LLMs (See Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#S3.T2" title="Table 2 ‣ 3.4 Humanlikeness Scoring ‣ 3 Methodology ‣ HLB: Benchmarking LLMs’ Humanlikeness in Language Use"><span class="ltx_text ltx_ref_tag">2</span></a>) were selected for evaluation, including models from prominent families like OpenAI’s GPT series (GPT-4o, GPT-3.5), Meta’s Llama series (Llama 2, Llama 3, Llama 3.1) and Mistral series<cite class="ltx_cite ltx_citemacro_cite">OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib25" title="">2024</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib35" title="">2023</a>); AI (<a class="ltx_ref" href="https://arxiv.org/html/2409.15890v1#bib.bib1" title="">2024</a>)</cite>. Each model provided 100 responses per item in each experiment, ensuring that the response data was comparable to the human data. Similar to the human experimental design, LLMs followed a one-trial-per-run paradigm, ensuring that responses were generated independently for each item to prevent context effects. The input format for the LLMs closely mirrored the instructions provided to human participants. Careful modification of human prompts was performed to ensure that task instructions were clear and interpretable by LLMs. This allowed for a direct comparison between human and LLM performance on the same tasks under identical conditions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Response Collection Procedure</span> This closely mirror that in the human experiments. Each LLM was presented with the task instructions and the stimulus combined into a single prompt. We collected 100 responses (across different conditions) for each item in an experiment in order to ensure a sufficiently large dataset for robust analysis of the response distributions. For OpenAI models, responses were obtained through the OpenAI API, while models hosted on Hugging Face were accessed using the Hugging Face Inference API. All requests to the models were made using their default parameters to encourage variability in responses. The collected responses were stored and processed for subsequent coding and analysis.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Response Coding</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Development and Validation</span> We employed an auto-coding algorithm across 10 experiments to assess agreement between human annotations and machine-generated labels. This algorithm utilized spaCy’s <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">en_core_web_trf-3.7.3</span> model for syntactic parsing (e.g., structural priming and syntactic ambiguity resolution tasks) and regular expressions to detect answer patterns in others. Across 20,953 trials of human response data, we computed Cohen’s Kappa (<span class="ltx_ERROR undefined" id="S3.SS3.p1.1.3">\textkappa</span>), a measure that corrects for chance agreement between the results from manually coding and auto-coding algorithm, defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K=\frac{P_{0}-P_{e}}{1-P_{e}}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">K</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml">P</mi><mn id="S3.E1.m1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.3.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">−</mo><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">P</mi><mi id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml">e</mi></msub></mrow><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">1</mn><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">−</mo><msub id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">P</mi><mi id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml">e</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝐾</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><divide id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3"></divide><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><minus id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1"></minus><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">𝑃</ci><cn id="S3.E1.m1.1.1.3.2.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.3.2.2.3">0</cn></apply><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝑃</ci><ci id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3">𝑒</ci></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><minus id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></minus><cn id="S3.E1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.3.3.2">1</cn><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">𝑃</ci><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">K=\frac{P_{0}-P_{e}}{1-P_{e}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_K = divide start_ARG italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_P start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">where Po is the observed agreement, and Pe is the expected agreement by chance.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The Kappa score was κ = 0.993, indicating near-perfect agreement (<span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.1">z</span> = 451, <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">p</span> &lt; 0.001). This demonstrates the high accuracy of the auto-coding algorithm in replicating human annotations.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Humanlikeness Scoring</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To quantify the humanlikeness of LLM responses, we used Jensen-Shannon (JS) divergence to compare the response distributions between human participants and LLMs. JS divergence, a symmetric measure of similarity between two probability distributions, is ideal for assessing how closely LLM responses mirror human behavior across linguistic levels. For each task, the auto-coding algorithm generated response distributions for both humans and LLMs. We computed <span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">humanlikeness score (HS)</span> for each item as:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle HS_{\text{item}}" class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1.1"><semantics id="S3.E2X.2.1.1.m1.1a"><mrow id="S3.E2X.2.1.1.m1.1.1" xref="S3.E2X.2.1.1.m1.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.1.1.2" xref="S3.E2X.2.1.1.m1.1.1.2.cmml">H</mi><mo id="S3.E2X.2.1.1.m1.1.1.1" xref="S3.E2X.2.1.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.E2X.2.1.1.m1.1.1.3" xref="S3.E2X.2.1.1.m1.1.1.3.cmml"><mi id="S3.E2X.2.1.1.m1.1.1.3.2" xref="S3.E2X.2.1.1.m1.1.1.3.2.cmml">S</mi><mtext id="S3.E2X.2.1.1.m1.1.1.3.3" xref="S3.E2X.2.1.1.m1.1.1.3.3a.cmml">item</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.1b"><apply id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1"><times id="S3.E2X.2.1.1.m1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.1"></times><ci id="S3.E2X.2.1.1.m1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.1.1.2">𝐻</ci><apply id="S3.E2X.2.1.1.m1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.1.1.3.2">𝑆</ci><ci id="S3.E2X.2.1.1.m1.1.1.3.3a.cmml" xref="S3.E2X.2.1.1.m1.1.1.3.3"><mtext id="S3.E2X.2.1.1.m1.1.1.3.3.cmml" mathsize="70%" xref="S3.E2X.2.1.1.m1.1.1.3.3">item</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.1c">\displaystyle HS_{\text{item}}</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.2.1.1.m1.1d">italic_H italic_S start_POSTSUBSCRIPT item end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=1-JS(P,Q)" class="ltx_Math" display="inline" id="S3.E2X.3.2.2.m1.2"><semantics id="S3.E2X.3.2.2.m1.2a"><mrow id="S3.E2X.3.2.2.m1.2.3" xref="S3.E2X.3.2.2.m1.2.3.cmml"><mi id="S3.E2X.3.2.2.m1.2.3.2" xref="S3.E2X.3.2.2.m1.2.3.2.cmml"></mi><mo id="S3.E2X.3.2.2.m1.2.3.1" xref="S3.E2X.3.2.2.m1.2.3.1.cmml">=</mo><mrow id="S3.E2X.3.2.2.m1.2.3.3" xref="S3.E2X.3.2.2.m1.2.3.3.cmml"><mn id="S3.E2X.3.2.2.m1.2.3.3.2" xref="S3.E2X.3.2.2.m1.2.3.3.2.cmml">1</mn><mo id="S3.E2X.3.2.2.m1.2.3.3.1" xref="S3.E2X.3.2.2.m1.2.3.3.1.cmml">−</mo><mrow id="S3.E2X.3.2.2.m1.2.3.3.3" xref="S3.E2X.3.2.2.m1.2.3.3.3.cmml"><mi id="S3.E2X.3.2.2.m1.2.3.3.3.2" xref="S3.E2X.3.2.2.m1.2.3.3.3.2.cmml">J</mi><mo id="S3.E2X.3.2.2.m1.2.3.3.3.1" xref="S3.E2X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S3.E2X.3.2.2.m1.2.3.3.3.3" xref="S3.E2X.3.2.2.m1.2.3.3.3.3.cmml">S</mi><mo id="S3.E2X.3.2.2.m1.2.3.3.3.1a" xref="S3.E2X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mrow id="S3.E2X.3.2.2.m1.2.3.3.3.4.2" xref="S3.E2X.3.2.2.m1.2.3.3.3.4.1.cmml"><mo id="S3.E2X.3.2.2.m1.2.3.3.3.4.2.1" stretchy="false" xref="S3.E2X.3.2.2.m1.2.3.3.3.4.1.cmml">(</mo><mi id="S3.E2X.3.2.2.m1.1.1" xref="S3.E2X.3.2.2.m1.1.1.cmml">P</mi><mo id="S3.E2X.3.2.2.m1.2.3.3.3.4.2.2" xref="S3.E2X.3.2.2.m1.2.3.3.3.4.1.cmml">,</mo><mi id="S3.E2X.3.2.2.m1.2.2" xref="S3.E2X.3.2.2.m1.2.2.cmml">Q</mi><mo id="S3.E2X.3.2.2.m1.2.3.3.3.4.2.3" stretchy="false" xref="S3.E2X.3.2.2.m1.2.3.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.3.2.2.m1.2b"><apply id="S3.E2X.3.2.2.m1.2.3.cmml" xref="S3.E2X.3.2.2.m1.2.3"><eq id="S3.E2X.3.2.2.m1.2.3.1.cmml" xref="S3.E2X.3.2.2.m1.2.3.1"></eq><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.2.3.2.cmml" xref="S3.E2X.3.2.2.m1.2.3.2">absent</csymbol><apply id="S3.E2X.3.2.2.m1.2.3.3.cmml" xref="S3.E2X.3.2.2.m1.2.3.3"><minus id="S3.E2X.3.2.2.m1.2.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.1"></minus><cn id="S3.E2X.3.2.2.m1.2.3.3.2.cmml" type="integer" xref="S3.E2X.3.2.2.m1.2.3.3.2">1</cn><apply id="S3.E2X.3.2.2.m1.2.3.3.3.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.3"><times id="S3.E2X.3.2.2.m1.2.3.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.3.1"></times><ci id="S3.E2X.3.2.2.m1.2.3.3.3.2.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.3.2">𝐽</ci><ci id="S3.E2X.3.2.2.m1.2.3.3.3.3.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.3.3">𝑆</ci><interval closure="open" id="S3.E2X.3.2.2.m1.2.3.3.3.4.1.cmml" xref="S3.E2X.3.2.2.m1.2.3.3.3.4.2"><ci id="S3.E2X.3.2.2.m1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1">𝑃</ci><ci id="S3.E2X.3.2.2.m1.2.2.cmml" xref="S3.E2X.3.2.2.m1.2.2">𝑄</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.3.2.2.m1.2c">\displaystyle=1-JS(P,Q)</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.3.2.2.m1.2d">= 1 - italic_J italic_S ( italic_P , italic_Q )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=1-\frac{1}{2}\left[KL(P\parallel M)+KL(Q\parallel M)\right]" class="ltx_Math" display="inline" id="S3.E2Xa.2.1.1.m1.1"><semantics id="S3.E2Xa.2.1.1.m1.1a"><mrow id="S3.E2Xa.2.1.1.m1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.cmml"></mi><mo id="S3.E2Xa.2.1.1.m1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.cmml"><mn id="S3.E2Xa.2.1.1.m1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.3.cmml">1</mn><mo id="S3.E2Xa.2.1.1.m1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.2.cmml">−</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2Xa.2.1.1.m1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.cmml"><mfrac id="S3.E2Xa.2.1.1.m1.1.1.1.1.3a" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.cmml"><mn id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.2.cmml">1</mn><mn id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.3.cmml">2</mn></mfrac></mstyle><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.2.cmml"><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml">K</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.4" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.4.cmml">L</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2a" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">P</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">∥</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">M</mi></mrow><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.3.cmml">K</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.4" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.4.cmml">L</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2a" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.2" stretchy="false" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">Q</mi><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">∥</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">M</mi></mrow><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.3" stretchy="false" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xa.2.1.1.m1.1b"><apply id="S3.E2Xa.2.1.1.m1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1"><eq id="S3.E2Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2"></eq><csymbol cd="latexml" id="S3.E2Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3">absent</csymbol><apply id="S3.E2Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1"><minus id="S3.E2Xa.2.1.1.m1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.2"></minus><cn id="S3.E2Xa.2.1.1.m1.1.1.1.3.cmml" type="integer" xref="S3.E2Xa.2.1.1.m1.1.1.1.3">1</cn><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1"><times id="S3.E2Xa.2.1.1.m1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.2"></times><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3"><divide id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3"></divide><cn id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.2">1</cn><cn id="S3.E2Xa.2.1.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.3.3">2</cn></apply><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1"><plus id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.3"></plus><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1"><times id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.3">𝐾</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.4">𝐿</ci><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑃</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑀</ci></apply></apply><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2"><times id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.2"></times><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.3">𝐾</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.4">𝐿</ci><apply id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.1">conditional</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.2">𝑄</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.2.1.1.1.3">𝑀</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.2.1.1.m1.1c">\displaystyle=1-\frac{1}{2}\left[KL(P\parallel M)+KL(Q\parallel M)\right]</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xa.2.1.1.m1.1d">= 1 - divide start_ARG 1 end_ARG start_ARG 2 end_ARG [ italic_K italic_L ( italic_P ∥ italic_M ) + italic_K italic_L ( italic_Q ∥ italic_M ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">where P and Q are the human and LLM response distributions, and M is their average. For each experiment, we average the scores across all items. The overall humanlikeness score across all experiments is then computed as:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle HS_{\text{Overall}}" class="ltx_Math" display="inline" id="S3.E3X.2.1.1.m1.1"><semantics id="S3.E3X.2.1.1.m1.1a"><mrow id="S3.E3X.2.1.1.m1.1.1" xref="S3.E3X.2.1.1.m1.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.2.cmml">H</mi><mo id="S3.E3X.2.1.1.m1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.E3X.2.1.1.m1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.3.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.3.2" xref="S3.E3X.2.1.1.m1.1.1.3.2.cmml">S</mi><mtext id="S3.E3X.2.1.1.m1.1.1.3.3" xref="S3.E3X.2.1.1.m1.1.1.3.3a.cmml">Overall</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.1b"><apply id="S3.E3X.2.1.1.m1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1"><times id="S3.E3X.2.1.1.m1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1"></times><ci id="S3.E3X.2.1.1.m1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.2">𝐻</ci><apply id="S3.E3X.2.1.1.m1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.2">𝑆</ci><ci id="S3.E3X.2.1.1.m1.1.1.3.3a.cmml" xref="S3.E3X.2.1.1.m1.1.1.3.3"><mtext id="S3.E3X.2.1.1.m1.1.1.3.3.cmml" mathsize="70%" xref="S3.E3X.2.1.1.m1.1.1.3.3">Overall</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.1c">\displaystyle HS_{\text{Overall}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3X.2.1.1.m1.1d">italic_H italic_S start_POSTSUBSCRIPT Overall end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{1}{m}\sum_{j=1}^{m}\left(\frac{1}{n_{j}}\sum_{i=1}^{n_{j}}%
\left(1-\frac{1}{2}\left[\text{KL}(P_{i}\parallel M_{i})\right.\right.\right." class="ltx_math_unparsed" display="inline" id="S3.E3X.3.2.2.m1.1"><semantics id="S3.E3X.3.2.2.m1.1a"><mrow id="S3.E3X.3.2.2.m1.1b"><mo id="S3.E3X.3.2.2.m1.1.1">=</mo><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.2"><mfrac id="S3.E3X.3.2.2.m1.1.2a"><mn id="S3.E3X.3.2.2.m1.1.2.2">1</mn><mi id="S3.E3X.3.2.2.m1.1.2.3">m</mi></mfrac></mstyle><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.3"><munderover id="S3.E3X.3.2.2.m1.1.3a"><mo id="S3.E3X.3.2.2.m1.1.3.2.2" movablelimits="false">∑</mo><mrow id="S3.E3X.3.2.2.m1.1.3.2.3"><mi id="S3.E3X.3.2.2.m1.1.3.2.3.2">j</mi><mo id="S3.E3X.3.2.2.m1.1.3.2.3.1">=</mo><mn id="S3.E3X.3.2.2.m1.1.3.2.3.3">1</mn></mrow><mi id="S3.E3X.3.2.2.m1.1.3.3">m</mi></munderover></mstyle><mrow id="S3.E3X.3.2.2.m1.1.4"><mo id="S3.E3X.3.2.2.m1.1.4.1">(</mo><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.4.2"><mfrac id="S3.E3X.3.2.2.m1.1.4.2a"><mn id="S3.E3X.3.2.2.m1.1.4.2.2">1</mn><msub id="S3.E3X.3.2.2.m1.1.4.2.3"><mi id="S3.E3X.3.2.2.m1.1.4.2.3.2">n</mi><mi id="S3.E3X.3.2.2.m1.1.4.2.3.3">j</mi></msub></mfrac></mstyle><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.4.3"><munderover id="S3.E3X.3.2.2.m1.1.4.3a"><mo id="S3.E3X.3.2.2.m1.1.4.3.2.2" movablelimits="false">∑</mo><mrow id="S3.E3X.3.2.2.m1.1.4.3.2.3"><mi id="S3.E3X.3.2.2.m1.1.4.3.2.3.2">i</mi><mo id="S3.E3X.3.2.2.m1.1.4.3.2.3.1">=</mo><mn id="S3.E3X.3.2.2.m1.1.4.3.2.3.3">1</mn></mrow><msub id="S3.E3X.3.2.2.m1.1.4.3.3"><mi id="S3.E3X.3.2.2.m1.1.4.3.3.2">n</mi><mi id="S3.E3X.3.2.2.m1.1.4.3.3.3">j</mi></msub></munderover></mstyle><mrow id="S3.E3X.3.2.2.m1.1.4.4"><mo id="S3.E3X.3.2.2.m1.1.4.4.1">(</mo><mn id="S3.E3X.3.2.2.m1.1.4.4.2">1</mn><mo id="S3.E3X.3.2.2.m1.1.4.4.3">−</mo><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.4.4.4"><mfrac id="S3.E3X.3.2.2.m1.1.4.4.4a"><mn id="S3.E3X.3.2.2.m1.1.4.4.4.2">1</mn><mn id="S3.E3X.3.2.2.m1.1.4.4.4.3">2</mn></mfrac></mstyle><mrow id="S3.E3X.3.2.2.m1.1.4.4.5"><mo id="S3.E3X.3.2.2.m1.1.4.4.5.1">[</mo><mtext id="S3.E3X.3.2.2.m1.1.4.4.5.2">KL</mtext><mrow id="S3.E3X.3.2.2.m1.1.4.4.5.3"><mo id="S3.E3X.3.2.2.m1.1.4.4.5.3.1" stretchy="false">(</mo><msub id="S3.E3X.3.2.2.m1.1.4.4.5.3.2"><mi id="S3.E3X.3.2.2.m1.1.4.4.5.3.2.2">P</mi><mi id="S3.E3X.3.2.2.m1.1.4.4.5.3.2.3">i</mi></msub><mo id="S3.E3X.3.2.2.m1.1.4.4.5.3.3" lspace="0em" rspace="0.167em">∥</mo><msub id="S3.E3X.3.2.2.m1.1.4.4.5.3.4"><mi id="S3.E3X.3.2.2.m1.1.4.4.5.3.4.2">M</mi><mi id="S3.E3X.3.2.2.m1.1.4.4.5.3.4.3">i</mi></msub><mo id="S3.E3X.3.2.2.m1.1.4.4.5.3.5" stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.E3X.3.2.2.m1.1c">\displaystyle=\frac{1}{m}\sum_{j=1}^{m}\left(\frac{1}{n_{j}}\sum_{i=1}^{n_{j}}%
\left(1-\frac{1}{2}\left[\text{KL}(P_{i}\parallel M_{i})\right.\right.\right.</annotation><annotation encoding="application/x-llamapun" id="S3.E3X.3.2.2.m1.1d">= divide start_ARG 1 end_ARG start_ARG italic_m end_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( 1 - divide start_ARG 1 end_ARG start_ARG 2 end_ARG [ KL ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad\qquad\qquad\left.\left.\left.+\text{KL}(Q_{i}\parallel M_{%
i})\right]\right)\right)" class="ltx_math_unparsed" display="inline" id="S3.E3Xa.2.1.1.m1.1"><semantics id="S3.E3Xa.2.1.1.m1.1a"><mrow id="S3.E3Xa.2.1.1.m1.1b"><mrow id="S3.E3Xa.2.1.1.m1.1.1"><mrow id="S3.E3Xa.2.1.1.m1.1.1.1"><mo id="S3.E3Xa.2.1.1.m1.1.1.1.1">+</mo><mtext id="S3.E3Xa.2.1.1.m1.1.1.1.2">KL</mtext><mrow id="S3.E3Xa.2.1.1.m1.1.1.1.3"><mo id="S3.E3Xa.2.1.1.m1.1.1.1.3.1" stretchy="false">(</mo><msub id="S3.E3Xa.2.1.1.m1.1.1.1.3.2"><mi id="S3.E3Xa.2.1.1.m1.1.1.1.3.2.2">Q</mi><mi id="S3.E3Xa.2.1.1.m1.1.1.1.3.2.3">i</mi></msub><mo id="S3.E3Xa.2.1.1.m1.1.1.1.3.3" lspace="0em" rspace="0.167em">∥</mo><msub id="S3.E3Xa.2.1.1.m1.1.1.1.3.4"><mi id="S3.E3Xa.2.1.1.m1.1.1.1.3.4.2">M</mi><mi id="S3.E3Xa.2.1.1.m1.1.1.1.3.4.3">i</mi></msub><mo id="S3.E3Xa.2.1.1.m1.1.1.1.3.5" stretchy="false">)</mo></mrow><mo id="S3.E3Xa.2.1.1.m1.1.1.1.4">]</mo></mrow><mo id="S3.E3Xa.2.1.1.m1.1.1.2">)</mo></mrow><mo id="S3.E3Xa.2.1.1.m1.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.E3Xa.2.1.1.m1.1c">\displaystyle\qquad\qquad\qquad\left.\left.\left.+\text{KL}(Q_{i}\parallel M_{%
i})\right]\right)\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E3Xa.2.1.1.m1.1d">+ KL ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∥ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="S3.F2.g1" src="extracted/5875496/benchmark_figure3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Humanlikeness scores of three LLM families</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.T2.1">{tblr}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.T2.2">cell11 = r=2,
cell12 = r=2,
cell13 = c=2,
cell15 = c=2,
cell17 = c=2,
cell19 = c=2,
cell111 = c=2,
hline1,3,24 = -,
hline2 = 3-12,

Experiment &amp; Overall  Sound   Word   Meaning   Syntax   Discourse  
<br class="ltx_break"/>  E1  E2  E3  E4  E5  E6  E7  E8  E9  E10 
<br class="ltx_break"/>Meta-Llama-3.1-70B-Instruct  66.50  89  62  61  6  81  77  80  67  80  63 
<br class="ltx_break"/>Meta-Llama-3.1-8B-Instruct  65.89  73  65  60  12  84  78  79  74  79  56 
<br class="ltx_break"/>Phi-3-mini-4k-instruct  64.61  61  68  59  19  89  71  76  48  80  76 
<br class="ltx_break"/>Mistral-Nemo-Instruct-2407  63.69  74  63  56  10  84  77  52  75  79  68 
<br class="ltx_break"/>Llama-2-13b-chat-hf  63.15  57  57  51  25  75  67  74  72  76  79 
<br class="ltx_break"/>Mistral-7B-Instruct-v0.1  62.77  73  70  62  24  87  43  69  36  79  84 
<br class="ltx_break"/>CodeLlama-34b-Instruct-hf  62.18  79  64  60  23  82  53  58  63  79  61 
<br class="ltx_break"/>c4ai-command-r-plus  60.77  79  60  63  8  72  72  66  59  78  50 
<br class="ltx_break"/>Meta-Llama-3-8B-Instruct  60.65  69  53  57  14  79  78  59  66  77  54 
<br class="ltx_break"/>starchat2-15b-v0.1  60.57  58  70  58  25  87  73  62  36  75  62 
<br class="ltx_break"/>gpt-4o  58.58  60  63  68  2  71  77  47  61  75  62 
<br class="ltx_break"/>gpt-3.5-turbo  58.32  55  61  66  3  76  76  71  47  76  50 
<br class="ltx_break"/>Yi-1.5-34B-Chat  58.20  67  54  55  13  72  70  61  65  78  48 
<br class="ltx_break"/>Llama-2-7b-chat-hf  57.47  74  61  58  22  67  69  50  60  75  39 
<br class="ltx_break"/>zephyr-7b-alpha  56.96  57  62  47  23  85  29  44  73  76  75 
<br class="ltx_break"/>Meta-Llama-3-70B-Instruct  56.73  60  61  55  4  71  75  57  59  75  50 
<br class="ltx_break"/>gpt-4o-mini  56.21  56  58  62  3  70  75  46  58  75  57 
<br class="ltx_break"/>Mistral-8x7B-Instruct-v0.1  52.80  60  53  48  23  71  46  43  59  73  52 
<br class="ltx_break"/>Mistral-7B-Instruct-v0.3  52.45  53  58  47  25  75  38  49  59  73  47 
<br class="ltx_break"/>Mistral-7B-Instruct-v0.2  50.18  13  58  54  14  72  61  46  64  71  49 
<br class="ltx_break"/>zephyr-7b-beta  47.85  28  53  48  26  71  7  38  73  75  60</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The humanlikness score for models in different experiments.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Result</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The overall humanlikeness scores revealed notable variations in how well LLMs emulated human language use across the 10 psycholinguistic experiments. Here, we performed a concise analysis to explore the data.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">OpenAI’s models, including GPT-3.5-turbo, GPT-4o-mini, and GPT-4o, exhibited relatively stable performance across tasks, maintaining consistent humanlikeness scores. In contrast, the Llama family of models showed an overall increase in humanlikeness scores, with Meta-Llama-3.1-70B-Instruct achieving the highest performance among all Llama models. On the other hand, the Mistral family of models showed a slight decrease in humanlikeness, with Mistral-7B-Instruct-v0.3 scoring lower than its predecessors, indicating less alignment with human language use.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparative Analysis</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Statistical comparisons between model families (three models selected per model family) revealed significant differences in performance. Notably, Llama models significantly outperformed Mistral models in humanlikeness (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">t</span> = 10.44, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">p</span> &lt;.001) highlighting the substantial gap between these two families. Furthermore, Llama models also outperformed OpenAI models (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">t</span> = 3.13, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">p</span> =.002) although this difference was less pronounced compared to the Llama vs. Mistral comparison.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Within the Llama family, the transition from Meta-Llama-3-70B-Instruct to Meta-Llama-3.1-70B-Instruct showed a significant increase in humanlikeness (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">t</span> = -4.85, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">p</span> &lt; .001), indicating improvements in model performance. In contrast, no significant differences were observed between GPT-3.5-turbo and GPT-4o (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">t</span> = -0.93, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.4">p</span> = 0.352), suggesting that OpenAI’s models performed consistently across experiments. Interestingly, within the Mistral family, Mistral-7B-Instruct-v0.3 showed a significant decrease in humanlikeness compared to Mistral-7B-Instruct-v0.1 (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.5">t</span> = 5.45, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.6">p</span> &lt; .001).</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">These results underscore the varying abilities of different model families to approximate human language patterns, with Llama models demonstrating superior performance overall.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Case analysis</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">An in-depth analysis of individual experiments further highlights how LLMs’ performance varies in replicating human-like responses. Experiment 4, which tested word meaning priming, emerged as the most non-humanlike among the tasks, with substantial differences between human and LLMs’ responses (<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">t</span> = -116.32, <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">p</span> &lt; .001). In this experiment, we assessed whether humans and models tend to access, when reading an ambiguous word such as post, the meaning previously used in the prime of an ambiguous word. Human participants exhibited a modest priming effect, with 20% associating post with its job-related meaning after the word-meaning prime and 18% after the synonym prime. In contrast, the Llama-3.1-70B model demonstrated a significantly higher priming effect, with 52% responding to the word-meaning prime and 38% to the synonym prime, revealing a stark divergence from human patterns. This case study emphasizes the challenges LLMs face in aligning their semantic associations with human interpretations, particularly when processing ambiguous or polysemous words.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The results of this benchmark study highlight notable differences in how LLMs approximate human language use across various linguistic levels. The Llama family of models, particularly Meta-Llama-3.1-70B-Instruct, consistently outperformed both the OpenAI and Mistral models in terms of humanlikeness score. This finding suggests that recent advancements in the Llama models have led to more humanlike language behaviors, especially in terms of semantic and discourse processing.
The OpenAI models, including GPT-4o and GPT-3.5-turbo, showed relatively stable performance across tasks, with no significant differences between the models. This stability may reflect a plateau in the improvement of humanlikeness in these models, as compared to the more recent gains observed in the Llama family. On the other hand, the Mistral models demonstrated a decrease in humanlikeness scores, particularly in the transition to Mistral-7B-Instruct-v0.3. This suggests that certain training methods and data quality in Mistral may have reduced their alignment with human language patterns.
One of the key insights from this study is that models differ not only in their overall humanlikeness scores but also in how they handle specific linguistic phenomena. For instance, in Experiment 4 (word meaning priming), we observed a significant divergence in resposnes between humans and LLMs, with the latter showing a much larger priming effect. This over-priming suggests that while LLMs may excel in certain aspects of language generation, they often lack the subtle flexibility that humans display when processing ambiguous or context-dependent language.
A major strength of this study is its use of psycholinguistic experiments to evaluate LLMs, which goes beyond traditional NLP benchmarks that focus on task accuracy. By systematically probing various linguistic levels—sound, word, syntax, semantics, and discourse—this benchmark provides a more comprehensive understanding of how LLMs process and generate language.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced a novel benchmark for evaluating the humanlikeness of LLMs in language use based on psycholinguistic experiments. Our study evaluated 20 LLMs, including OpenAI’s GPT family, Meta’s Llama family, the Mistral family and others, across 10 experiments that spanned key linguistic aspects such as sound, word, syntax, semantics, and discourse. Using responses from over 2,000 human participants as a baseline, the results revealed significant differences in model performance, with Llama models consistently outperforming both OpenAI and Mistral models in terms of language use humanlikeness. These findings underscore the potential of psycholinguistic benchmarks to capture aspects of language that are often missed by traditional NLP evaluations.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">This benchmark provides a framework for future research on LLMs, offering a more meaningful and comprehensive way to evaluate their performance in real-world language use. It also highlights areas where current LLMs diverge from human language patterns, particularly in tasks involving semantic priming and ambiguity resolution. By identifying these gaps, this study offers critical insights for the next generation of LLM development, paving the way for models that more closely mirror the intricacies of human communication.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">However, there are several limitations to this study. First, while the benchmark covers a wide range of linguistic tasks, it may not encompass the full complexity of human language use. Some linguistic phenomena, such as pragmatic reasoning, were not explored in this study. Second, we did not manipulate models’ parameters, particularly the temperature or top k, to control the diversity of the generated responses. While using default parameters, particularly temperature, may seem limiting, this choice ensures that we evaluate models in their most typical and practical configurations. Default settings reflect how these models are commonly used in real-world applications, offering a fair and standardized comparison. Tuning parameters like temperature could introduce bias and variability across models, making it difficult to ensure consistent evaluation. By using default settings, we eliminate these concerns, allowing for a more reliable assessment of humanlikeness. Finally, while the study includes a large sample of human participants, the specific demographic characteristics (e.g., native English speakers from the UK and US) may not fully represent global language use patterns. Compared to previous benchmarks that focus on task-based performance, this study offers a more in-depth analysis of language models’ alignment with human linguistic behavior. Similar studies, such as Ettinger (2020), have used psycholinguistic principles to probe LLMs, but our study stands out by incorporating a broader range of linguistic levels and by using a large-scale dataset of human responses for direct comparison. The significant differences found between model families, such as the higher humanlikeness of Llama models, provide valuable insights for the ongoing development and fine-tuning of LLMs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Mistral AI. 2024.

</span>
<span class="ltx_bibblock">Mistral-7b-instruct-v0.3: An advanced instruction-based language model.

</span>
<span class="ltx_bibblock">Hugging Face Model Card.

</span>
<span class="ltx_bibblock">Released on May 22, 2024. Available at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3" title="">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altmann and Steedman (1988)</span>
<span class="ltx_bibblock">
Gerry Altmann and Mark Steedman. 1988.

</span>
<span class="ltx_bibblock">Interaction with context during human sentence processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Cognition</em>, 30(3):191–238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binz and Schulz (2023)</span>
<span class="ltx_bibblock">
Marcel Binz and Eric Schulz. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1073/pnas.2218523120" title="">Using cognitive psychology to understand GPT-3</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the National Academy of Sciences</em>, 120(6):e2218523120.

</span>
<span class="ltx_bibblock">Publisher: Proceedings of the National Academy of Sciences.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2024)</span>
<span class="ltx_bibblock">
Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, and Martin J. Pickering. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08014" title="">Do large language models resemble humans in language use?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2303.08014 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassidy et al. (1999)</span>
<span class="ltx_bibblock">
Kimberly Wright Cassidy, Michael H Kelly, and Lee’at J Sharoni. 1999.

</span>
<span class="ltx_bibblock">Inferring gender from name phonology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Journal of Experimental Psychology: General</em>, 128(3):362.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasgupta et al. (2023)</span>
<span class="ltx_bibblock">
Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2207.07051" title="">Language models show human-like content effects on reasoning tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2207.07051 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">del Rio-Chanona et al. (2024)</span>
<span class="ltx_bibblock">
R Maria del Rio-Chanona, Nadzeya Laurentsyeva, and Johannes Wachs. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1093/pnasnexus/pgae400" title="">Large language models reduce public knowledge sharing on online Q&amp;amp;A platforms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">PNAS Nexus</em>, page pgae400.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky et al. (2023)</span>
<span class="ltx_bibblock">
Dorottya Demszky, Diyi Yang, David S. Yeager, Christopher J. Bryan, Margarett Clapper, Susannah Chandhok, Johannes C. Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, Michaela Jones, Danielle Krettek-Cobb, Leslie Lai, Nirel JonesMitchell, Desmond C. Ong, Carol S. Dweck, James J. Gross, and James W. Pennebaker. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s44159-023-00241-5" title="">Using large language models in psychology</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Nature Reviews Psychology</em>, 2(11):688–701.

</span>
<span class="ltx_bibblock">Publisher: Nature Publishing Group.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erickson and Mattson (1981)</span>
<span class="ltx_bibblock">
TD Erickson and ME Mattson. 1981.

</span>
<span class="ltx_bibblock">From words to meaning: A semantic illusion. j verbal learn verbal behav 20 (5): 540–551.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ettinger (2020)</span>
<span class="ltx_bibblock">
Allyson Ettinger. 2020.

</span>
<span class="ltx_bibblock">What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Transactions of the Association for Computational Linguistics</em>, 8:34–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Futrell (2019)</span>
<span class="ltx_bibblock">
R Futrell. 2019.

</span>
<span class="ltx_bibblock">Neural language models as psycholinguistic subjects: Representations of syntactic state.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1903.03260</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garvey and Caramazza (1974)</span>
<span class="ltx_bibblock">
Catherine Garvey and Alfonso Caramazza. 1974.

</span>
<span class="ltx_bibblock">Implicit causality in verbs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Linguistic inquiry</em>, 5(3):459–464.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibson et al. (2013)</span>
<span class="ltx_bibblock">
Edward Gibson, Leon Bergen, and Steven T Piantadosi. 2013.

</span>
<span class="ltx_bibblock">Rational integration of noisy evidence and prior semantic expectations in sentence interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the National Academy of Sciences</em>, 110(20):8051–8056.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hagendorff (2023)</span>
<span class="ltx_bibblock">
Thilo Hagendorff. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.13988" title="">Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2303.13988 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2009.03300" title="">Measuring Massive Multitask Language Understanding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2009.03300 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang (2023)</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2212.10403" title="">Towards Reasoning in Large Language Models: A Survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2212.10403 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto, Christian Muxica, Grusha Prasad, Brian Dillon, and Tal Linzen. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.jml.2024.104510" title="">Large-scale benchmark yields no evidence that language model surprisal explains syntactic disambiguation difficulty</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Journal of Memory and Language</em>, 137:104510.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karra et al. (2023)</span>
<span class="ltx_bibblock">
Saketh Reddy Karra, Son The Nguyen, and Theja Tulabandhula. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2204.12000" title="">Estimating the Personality of White-Box Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2204.12000 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köhler (1967)</span>
<span class="ltx_bibblock">
Wolfgang Köhler. 1967.

</span>
<span class="ltx_bibblock">Gestalt psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Psychologische forschung</em>, 31(1):XVIII–XXX.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et al. (2022)</span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2206.14858" title="">Solving Quantitative Reasoning Problems with Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2206.14858 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahowald et al. (2013)</span>
<span class="ltx_bibblock">
Kyle Mahowald, Evelina Fedorenko, Steven T Piantadosi, and Edward Gibson. 2013.

</span>
<span class="ltx_bibblock">Info/information theory: Speakers choose shorter words in predictive contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Cognition</em>, 126(2):313–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et al. (2020)</span>
<span class="ltx_bibblock">
Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1073/pnas.1907367117" title="">Emergent linguistic structure in artificial neural networks trained by self-supervision</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the National Academy of Sciences</em>, 117(48):30046–30054.

</span>
<span class="ltx_bibblock">Publisher: Proceedings of the National Academy of Sciences.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michaelov and Bergen (2023)</span>
<span class="ltx_bibblock">
James A. Michaelov and Benjamin K. Bergen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.14681" title="">Emergent inabilities? Inverse scaling over the course of pretraining</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2305.14681 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miotto et al. (2022)</span>
<span class="ltx_bibblock">
Marilù Miotto, Nicola Rossberg, and Bennett Kleinberg. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2209.14338" title="">Who is GPT-3? An Exploration of Personality, Values and Demographics</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2209.14338 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo
Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan
Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08774" title="">GPT-4 Technical Report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2303.08774 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2024)</span>
<span class="ltx_bibblock">
Qiwei Peng, Yekun Chai, and Xuhong Li. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2402.16694" title="">HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2402.16694 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pickering and Branigan (1998)</span>
<span class="ltx_bibblock">
Martin J Pickering and Holly P Branigan. 1998.

</span>
<span class="ltx_bibblock">The representation of verbs: Evidence from syntactic priming in language production.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Journal of Memory and language</em>, 39(4):633–651.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al. (2023)</span>
<span class="ltx_bibblock">
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2212.09597" title="">Reasoning with Language Model Prompting: A Survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2212.09597 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2023)</span>
<span class="ltx_bibblock">
Zhuang Qiu, Xufeng Duan, and Zhenguang Garry Cai. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.31234/osf.io/qtbh9" title="">Pragmatic Implicature Processing in ChatGPT</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qualtrics (2024)</span>
<span class="ltx_bibblock">
Qualtrics. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.qualtrics.com" title="">Qualtrics and all other qualtrics product or service names are registered trademarks or trademarks of qualtrics</a>.

</span>
<span class="ltx_bibblock">Provo, UT, USA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodd et al. (2013)</span>
<span class="ltx_bibblock">
Jennifer M Rodd, Belen Lopez Cutrin, Hannah Kirsch, Alessandra Millar, and Matthew H Davis. 2013.

</span>
<span class="ltx_bibblock">Long-term priming of the meanings of ambiguous words.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Journal of Memory and Language</em>, 68(2):180–198.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2023)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2210.13312" title="">Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2210.13312 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et al. (2024)</span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024.

</span>
<span class="ltx_bibblock">Ai models collapse when trained on recursively generated data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Nature</em>, 631(8022):755–759.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer and Spear (2015)</span>
<span class="ltx_bibblock">
Murray Singer and Jackie Spear. 2015.

</span>
<span class="ltx_bibblock">Phantom recollection of bridging and elaborative inferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Discourse Processes</em>, 52(5-6):356–375.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.09288" title="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2307.09288 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trott et al. (2023)</span>
<span class="ltx_bibblock">
Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, and Benjamin Bergen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2209.01515" title="">Do Large Language Models know what humans know?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2209.01515 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsubota and Kano (2024)</span>
<span class="ltx_bibblock">
Yuka Tsubota and Yoshinobu Kano. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.aiwolfdial-1.2" title="">Text Generation Indistinguishable from Target Person by Prompting Few Examples Using LLM</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2nd International AIWolfDial Workshop</em>, pages 13–20, Tokyo, Japan. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019.

</span>
<span class="ltx_bibblock">Superglue: A stickier benchmark for general-purpose language understanding systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1804.07461</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1905.07830" title="">HellaSwag: Can a Machine Really Finish Your Sentence?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:1905.07830 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.07921" title="">Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2308.07921 [cs] version: 1.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">This section introduces the ten psycholinguistic experiments used to evaluate the humanlikeness of LLMs across multiple linguistic levels. Each experiment was designed to test a specific linguistic phenomenon and compare the performance of LLMs to human participants.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p2.1.1">Sounds: sound-shape association</span> People often associate specific sounds with certain shapes, a phenomenon known as sound symbolism. We tested whether LLMs, like humans, tend to link spiky-sounding words such as <span class="ltx_text ltx_font_italic" id="A1.p2.1.2">takete</span> or <span class="ltx_text ltx_font_italic" id="A1.p2.1.3">kiki</span> with spiky objects and round-sounding words like <span class="ltx_text ltx_font_italic" id="A1.p2.1.4">maluma</span> or <span class="ltx_text ltx_font_italic" id="A1.p2.1.5">bouba</span> with round objects.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p3.1.1">Sounds: sound-gender association</span> People can often guess if an unfamiliar name is male or female based on its sound. In English, women’s names more frequently end in vowels compared to men’s names. In this task, we asked participants to complete a preamble containing either a consonant-ending name (e.g., <span class="ltx_text ltx_font_italic" id="A1.p3.1.2">Pelcrad</span> in 1a) or a vowel-ending novel name (e.g., <span class="ltx_text ltx_font_italic" id="A1.p3.1.3">Pelcra</span> in 1b).</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">1a. Consonant-ending name: <span class="ltx_text ltx_font_italic" id="A1.p4.1.1">Although Pelcrad was sick…</span></p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1">1b. Vowel-ending name: <span class="ltx_text ltx_font_italic" id="A1.p5.1.1">Although Pelcra was sick…</span></p>
</div>
<div class="ltx_para" id="A1.p6">
<p class="ltx_p" id="A1.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p6.1.1">Words: word length and predictivity</span> Shorter words are suggested to make communication more efficient by carrying less information. If both humans and LLMs are sensitive to the relationship between word length and informativity, they should prefer shorter words over longer ones with nearly identical meanings when completing sentence preambles that predicted the meaning of the word (making it less informative; e.g., 2a), compared to neutral sentence preambles (e.g., 2b)</p>
</div>
<div class="ltx_para" id="A1.p7">
<p class="ltx_p" id="A1.p7.1">2a. Predictive context: <span class="ltx_text ltx_font_italic" id="A1.p7.1.1">Susan was very bad at algebra, so she hated… 1. math 2. mathematics</span></p>
</div>
<div class="ltx_para" id="A1.p8">
<p class="ltx_p" id="A1.p8.1">2b. Neutral context: <span class="ltx_text ltx_font_italic" id="A1.p8.1.1">Susan introduced herself to me as someone who loved… 1. math 2. mathematics</span></p>
</div>
<div class="ltx_para" id="A1.p9">
<p class="ltx_p" id="A1.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p9.1.1">Words: word meaning priming</span> Many words have multiple meanings; for instance, <span class="ltx_text ltx_font_italic" id="A1.p9.1.2">post</span> can refer to mail or a job. People update an ambiguous word’s meaning based on recent exposure. We tested whether humans and LLMs similarly demonstrate word meaning priming phennomenon: Participants associated post with its job-related meaning more frequently after reading sentences using that context rather than synonyms’ contexts (3a vs.3b).</p>
</div>
<div class="ltx_para" id="A1.p10">
<p class="ltx_p" id="A1.p10.1">3a. Word-meaning prime: <span class="ltx_text ltx_font_italic" id="A1.p10.1.1">The man accepted the post in the accountancy firm.</span></p>
</div>
<div class="ltx_para" id="A1.p11">
<p class="ltx_p" id="A1.p11.1">3b. Synonym prime: <span class="ltx_text ltx_font_italic" id="A1.p11.1.1">The man accepted the job in the accountancy firm.</span></p>
</div>
<div class="ltx_para" id="A1.p12">
<p class="ltx_p" id="A1.p12.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p12.1.1">Syntax: structural priming</span> In structural priming, people tend to repeat syntactic structures they’ve recently encountered. We had participants complete prime preambles designed for either PO (prepositional-object dative structure, e.g., <span class="ltx_text ltx_font_italic" id="A1.p12.1.2">The racing driver gave helpful mechanic wrench</span> to complete 4a) or DO (double-object dative structure, e.g., <span class="ltx_text ltx_font_italic" id="A1.p12.1.3">The racing driver gave torn overall his mechanic</span> to complete 4b). Participants then completed target preamble which could be continued as either DO/PO. If structural priming is demonstrated, participants replicate structure of the prime preamble.</p>
</div>
<div class="ltx_para" id="A1.p13">
<p class="ltx_p" id="A1.p13.1">4a. DO-inducing prime preamble: <span class="ltx_text ltx_font_italic" id="A1.p13.1.1">The racing driver showed the helpful mechanic …</span></p>
</div>
<div class="ltx_para" id="A1.p14">
<p class="ltx_p" id="A1.p14.1">4b. PO-inducing prime preamble: <span class="ltx_text ltx_font_italic" id="A1.p14.1.1">The racing driver showed the torn overall …</span></p>
</div>
<div class="ltx_para" id="A1.p15">
<p class="ltx_p" id="A1.p15.1">4c. Target preamble: <span class="ltx_text ltx_font_italic" id="A1.p15.1.1">The patient showed …</span></p>
</div>
<div class="ltx_para" id="A1.p16">
<p class="ltx_p" id="A1.p16.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p16.1.1">Syntax: syntactic ambiguity resolution</span> The way people parse words into syntactic structures has garnered significant attention in psycholinguistics. For instance, in VP/NP ambiguity (e.g., <span class="ltx_text ltx_font_italic" id="A1.p16.1.2">The ranger killed the poacher with the rifle</span>), people usually interpret the ambiguous prepositional phrase (PP, <span class="ltx_text ltx_font_italic" id="A1.p16.1.3">with the rifle</span>) as modifying the verb phrase (VP, <span class="ltx_text ltx_font_italic" id="A1.p16.1.4">killed the poacher</span>) rather than the noun phrase (NP, <span class="ltx_text ltx_font_italic" id="A1.p16.1.5">the poacher</span>). However, contextual information can modulate this resolution: People are more likely to interpret ambiguous PPs as modifying NPs when there are multiple possible referents (e.g., 5b) compared to when there is only a single referent (e.g., 5a). We examine how effectively LLMs use contextual information to resolve syntactic ambiguities and exhibit such modulation patterns.</p>
</div>
<div class="ltx_para" id="A1.p17">
<p class="ltx_p" id="A1.p17.1">5a. Single referent: <span class="ltx_text ltx_font_italic" id="A1.p17.1.1">There was a hunter and a poacher. The hunter killed the dangerous poacher with a rifle not long after sunset. Who had a rifle, the hunter or the poacher? </span></p>
</div>
<div class="ltx_para" id="A1.p18">
<p class="ltx_p" id="A1.p18.1">5b. Multiple referents: <span class="ltx_text ltx_font_italic" id="A1.p18.1.1">There was a hunter and two poachers. The hunter killed the dangerous poacher with a rifle not long after sunset. Who had a rifle, the hunter or the poacher?</span></p>
</div>
<div class="ltx_para" id="A1.p19">
<p class="ltx_p" id="A1.p19.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p19.1.1">Meaning: implausible sentence interpretation</span> Listeners often need to recover intended messages from noise-corrupted input. Errors in production or comprehension can make a plausible sentence implausible by omitting (e.g., <span class="ltx_text ltx_font_italic" id="A1.p19.1.2">to</span> omitted, 6a) or inserting words (e.g., <span class="ltx_text ltx_font_italic" id="A1.p19.1.3">to</span> inserted, 6b). People may interpret an implausible sentence nonliterally if they believe it is noise-corrupted. who found that people more frequently reinterpret implausible DO sentences than PO sentences due to the likelihood of omissions over insertions. We tested whether people and LLMs similarly assume that implausible sentences result from noise corruption, with omissions being more likely than insertions.</p>
</div>
<div class="ltx_para" id="A1.p20">
<p class="ltx_p" id="A1.p20.1">6a. Implausible DO: <span class="ltx_text ltx_font_italic" id="A1.p20.1.1">The mother gave the candle the daughter.</span></p>
</div>
<div class="ltx_para" id="A1.p21">
<p class="ltx_p" id="A1.p21.1">6b. Implausible PO: <span class="ltx_text ltx_font_italic" id="A1.p21.1.1">The mother gave the daughter to the candle.</span></p>
</div>
<div class="ltx_para" id="A1.p22">
<p class="ltx_p" id="A1.p22.1">6c. Question: <span class="ltx_text ltx_font_italic" id="A1.p22.1.1">Did the daughter receive something/someone?</span></p>
</div>
<div class="ltx_para" id="A1.p23">
<p class="ltx_p" id="A1.p23.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p23.1.1">Meaning: semantic illusions</span> People often overlook obvious errors in sentences. For instance, when asked (7a), many fail to notice that the question should refer to <span class="ltx_text ltx_font_italic" id="A1.p23.1.2">Noah</span> instead of <span class="ltx_text ltx_font_italic" id="A1.p23.1.3">Moses</span>. Such semantic illusions suggest that processing sentence meanings involves partial matches in semantic memory. We tested whether LLMs and people alike produce semantic illusions and are more likely to catch a weak imposter (e.g., <span class="ltx_text ltx_font_italic" id="A1.p23.1.4">Adam</span>, less similar to <span class="ltx_text ltx_font_italic" id="A1.p23.1.5">Noah</span>, 7b) than a strong imposter (e.g. <span class="ltx_text ltx_font_italic" id="A1.p23.1.6">Morse</span>, more similar to <span class="ltx_text ltx_font_italic" id="A1.p23.1.7">Noah</span>, 7a).</p>
</div>
<div class="ltx_para" id="A1.p24">
<p class="ltx_p" id="A1.p24.1">7a. Strong: <span class="ltx_text ltx_font_italic" id="A1.p24.1.1">During the Biblical flood, how many animals of each kind did Moses take on the ark? </span></p>
</div>
<div class="ltx_para" id="A1.p25">
<p class="ltx_p" id="A1.p25.1">7b. Weak: <span class="ltx_text ltx_font_italic" id="A1.p25.1.1">During the Biblical flood, how many animals of each kind did Adam take on the ark?</span></p>
</div>
<div class="ltx_para" id="A1.p26">
<p class="ltx_p" id="A1.p26.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p26.1.1">Discourse: implicit causality</span> Certain verbs prompt people to associate causality with either the subject or the object within a sentence. For instance, stimulus-experiencer verbs like scare typically lead people to attribute causality to the subject (e.g., completing 8a as <span class="ltx_text ltx_font_italic" id="A1.p26.1.2">Gary scared Anna because he was violent</span>), whereas experiencer-stimulus verbs like fear generally lead people to attribute causality to the object (e.g., completing 8b as <span class="ltx_text ltx_font_italic" id="A1.p26.1.3">Gary feared Anna because she was violent</span>). We assessed whether LLMs, like humans, show similar patterns of causal attribution based on verb type.</p>
</div>
<div class="ltx_para" id="A1.p27">
<p class="ltx_p" id="A1.p27.1">8a. Stimulus-experiencer verb: <span class="ltx_text ltx_font_italic" id="A1.p27.1.1">Gary scared Anna because…</span></p>
</div>
<div class="ltx_para" id="A1.p28">
<p class="ltx_p" id="A1.p28.1">8b. Experiencer-stimulus verb: <span class="ltx_text ltx_font_italic" id="A1.p28.1.1">Gary feared Anna because…</span></p>
</div>
<div class="ltx_para" id="A1.p29">
<p class="ltx_p" id="A1.p29.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A1.p29.1.1">Discourse: drawing inferences</span> People make bridging inferences more frequently than elaborative inferences. Bridging inferences connect two pieces of information (after reading 9a, people infer that Sharon cut her foot) while elaborative inferences extrapolate from a single piece of information (people are less likely to make this inference after reading 9b). We examined how well an LLM aligns with human patterns of inference by comparing the bridging and elaborative conditions.</p>
</div>
<div class="ltx_para" id="A1.p30">
<p class="ltx_p" id="A1.p30.1">9a. Bridging: <span class="ltx_text ltx_font_italic" id="A1.p30.1.1">While swimming in the shallow water near the rocks, Sharon stepped on a piece of glass. She called desperately for help, but there was no one around to hear her.</span></p>
</div>
<div class="ltx_para" id="A1.p31">
<p class="ltx_p" id="A1.p31.1">9b. Elaborative: <span class="ltx_text ltx_font_italic" id="A1.p31.1.1">While swimming in the shallow water near the rocks, Sharon stepped on a piece of glass. She had been looking for the watch that she misplaced while sitting on the rocks.</span></p>
</div>
<div class="ltx_para" id="A1.p32">
<p class="ltx_p" id="A1.p32.1">Question: <span class="ltx_text ltx_font_italic" id="A1.p32.1.1">Did she cut her foot?</span></p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 09:10:58 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
