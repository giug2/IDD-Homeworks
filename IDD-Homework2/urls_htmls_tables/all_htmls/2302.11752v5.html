<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.11752] VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witneâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.11752">

<!--Generated on Fri Mar  1 00:59:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ngan Luu-Thuy Nguyen<sup id="id11.11.id1" class="ltx_sup">1</sup>, Nghia Hieu Nguyen<sup id="id12.12.id2" class="ltx_sup">2</sup>, Duong T. D. Vo<sup id="id13.13.id3" class="ltx_sup">3</sup>, Khanh Quoc Tran<sup id="id14.14.id4" class="ltx_sup">4</sup>, 
<br class="ltx_break"><span id="id5.5.1" class="ltx_text ltx_font_bold">Kiet Van Nguyen<sup id="id5.5.1.1" class="ltx_sup"><span id="id5.5.1.1.1" class="ltx_text ltx_font_medium">5</span></sup>
<br class="ltx_break">Faculty of Information Science and Engineering, University of Information Technology, 
<br class="ltx_break">Ho Chi Minh City, Vietnam 
<br class="ltx_break">Vietnam National University, Ho Chi Minh City, Vietnam 
<br class="ltx_break"></span><span id="id7.7.3" class="ltx_text ltx_font_typewriter">{19520178<sup id="id7.7.3.1" class="ltx_sup"><span id="id7.7.3.1.1" class="ltx_text ltx_font_serif">2</span></sup>,19520483<sup id="id7.7.3.2" class="ltx_sup"><span id="id7.7.3.2.1" class="ltx_text ltx_font_serif">3</span></sup>}@gm.uit.edu.vn</span><span id="id15.15.id5" class="ltx_text ltx_font_bold">,
<br class="ltx_break"></span><span id="id10.10.6" class="ltx_text ltx_font_typewriter">{ngannlt<sup id="id10.10.6.1" class="ltx_sup"><span id="id10.10.6.1.1" class="ltx_text ltx_font_serif">1</span></sup>,khanhtq<sup id="id10.10.6.2" class="ltx_sup"><span id="id10.10.6.2.1" class="ltx_text ltx_font_serif">4</span></sup>,kietnv<sup id="id10.10.6.3" class="ltx_sup"><span id="id10.10.6.3.1" class="ltx_text ltx_font_serif">5</span></sup>}@uit.edu.vn</span><span id="id16.16.id6" class="ltx_text ltx_font_bold"> 
<br class="ltx_break"></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witnessed various developments in datasets and models for visual question answering. Visual question answering in other languages also would be developed for resources and models. In addition, there is no multilingual dataset targeting the visual content of a particular country with its own objects and cultural characteristics. To address the weakness, we provide the research community with a benchmark dataset named EVJVQA, including 33,000+ pairs of question-answer over three languages: Vietnamese, English, and Japanese, on approximately 5,000 images taken from Vietnam for evaluating multilingual VQA systems or models. EVJVQA is used as a benchmark dataset for the challenge of multilingual visual question answering at the 9th Workshop on Vietnamese Language and Speech Processing (VLSP 2022). This task attracted 62 participant teams from various universities and organizations. In this article, we present details of the organization of the challenge, an overview of the methods employed by shared-task participants, and the results. The highest performances are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The multilingual QA systems proposed by the top 2 teams use ViT for the pre-trained vision model and mT5 for the pre-trained language model, a powerful pre-trained language model based on the transformer architecture. EVJVQA is a challenging dataset that motivates NLP and CV researchers to further explore the multilingual models or systems for visual question answering systems. We released the challenge on the <a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/12274" title="" class="ltx_ref ltx_href">Codalab</a> evaluation system for further research.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual or Image-Based Question Answering is a challenging task that requires knowledge of two hot AI fields: natural language processing and computer vision. Specifically, querying the information of images through human-language questions is a friendly and natural approach to searching for information, meeting the needs of people extracting information in many domains such as life, education, work, etc. However, studies have mainly focused on resource-rich languages such as English. In this challenge, we aim to extend visual question answering to more languages, including rich and low-resource languages, including resources and models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">English has witnessed numerous benchmarks for evaluating and developing visual question answering models or systems. Recently, researchers designed datasets with goal-oriented evaluations. Firstly, VQA datasets <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>); Zhu etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2016</a>); Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>); Changpinyo etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2022a</a>)</cite> were created on general images. Soon after, the more complex dataset based on reasoning is discovered by <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. In addition, VQA <cite class="ltx_cite ltx_citemacro_cite">Gurari etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> is also geared towards support applications for seemingly-impaired and blind people. <cite class="ltx_cite ltx_citemacro_cite">Singh etÂ al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> showed that VQA has the ability to read text on photos. More challenging, VQA requires external, commonsense, or world knowledge to predict more correct answers <cite class="ltx_cite ltx_citemacro_cite">Marino etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>); Schwenk etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>. Besides, Changpinyo et al. <cite class="ltx_cite ltx_citemacro_cite">Changpinyo etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2022b</a>)</cite> proposed a multilingual dataset for visual question answering in 13 languages. However, this dataset is done automatically based on the auto-translation and verification method. In this paper, we presented a human-generation multilingual dataset, including three languages: English (resource-rich language), Japanese, and Vietnamese(resource-poor language).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we have three main contributions described as follows.</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Firstly, we constructed UIT-EVJVQA, a multilingual dataset for evaluating the visual question answering systems or models, which comprises 33.790 question-answer pairs in three languages: English, Vietnamese, and Japanese.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We organize the VLSP2022-EVJVQA Challenge for evaluating multilingual VQA models (Vietnamese, English, and Japanese) at the VLSP 2022. Our baseline system obtains 0.3346 in F1-score and 0.2275 in BLEU on the public and private test sets, respectively, and there are no models of participating teams that pass 0.44 (in F1-score) on the private test set, which indicates our dataset is challenging and requires the development of multilingual VQA models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">When combined with other VQA datasets for analysis, UIT-EVJVQA could potentially be a useful resource for multilingual research.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The following is how the rest of the article is organized. In Section 2, we provide a brief overview
of the background and relevant studies. We introduce the VLSP 2021-EVJVQA Challenge in Section 3. Our new dataset (UIT-EVJVQA) is presented in detail in Section 4. Section 5 presents the systems and results proposed by participating teams. In Section 6, we provide further analysis of the challenge results. Finally, Section 7 summarizes the findings of the VLSP 2022-EVJVQA Challenge and suggests future research directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Visual Question Answering (VQA) is a challenging task that has significant value not only in the research community but also in daily life. VQA task was first introduced by <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite>. The authors were successful in creating a novel dataset and fundamental English methodologies. Inspired by that success, various further studies have been created and implemented in a variety of languages <cite class="ltx_cite ltx_citemacro_cite">Gupta (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> including Chinese <cite class="ltx_cite ltx_citemacro_cite">Qi etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, Japanese <cite class="ltx_cite ltx_citemacro_cite">Shimizu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite>, and Vietnamese <cite class="ltx_cite ltx_citemacro_cite">Tran etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">VQA has gained more attention from researchers in recent years and has shown significant growth. Studies are currently being introduced not only in monolingual but also in multilingual applications <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Khan etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>); Liu etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>); Nooralahzadeh and Sennrich (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. This stage contributes significantly to the creation of multilingual VQA (mVQA) systems. Some typical research works in this approach can be mentioned such as <cite class="ltx_cite ltx_citemacro_cite">Gupta etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> with the study that the proposed model is capable of predicting responses from questions in Hindi, English, or Code-mixed (Hinglish: Hindi-English) languages; Changpinyo et al. <cite class="ltx_cite ltx_citemacro_cite">Changpinyo etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2022b</a>)</cite> with a translation-based mVQA dataset in 7 distinct languages; Gao et al. <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2015</a>)</cite> construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this study, we create the first dataset for the task of mVQA on English-Vietnamese-Japanese (EVJVQA). The EVJVQA dataset is expected to open up new research areas and aid in evaluating multilingual VQA models.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The VLSP 2022 - EVJVQA Challenge</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Definition</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2302.11752/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the multilingual visual question answering task.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This task aims to enable the ability of computers to understand images and answer relevant questions in different languages from users. The task is defined as below (Figure <a href="#S3.F1" title="Figure 1 â€£ 3.1 Task Definition â€£ 3 The VLSP 2022 - EVJVQA Challenge â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Input</span>: Given an image and a question that can be answerable.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Output</span>: An answer where can be a span related to the imageâ€™s content.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Language Selection</span>. Three languages are selected in which the main language is Vietnamese, and the other two popular other languages (English and Japanese) in the pictures taken from Vietnam.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this challenge, we use two evaluation metrics: F1 and BLUE. Based on <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar etÂ al. (<a href="#bib.bib25" title="" class="ltx_ref">2016</a>)</cite>, the F1-score of each answer is calculated based on tokens of the gold answer (GA) and tokens of the predicted answer (PA). The overall F1 is averaged across all questions of each set. For Vietnamese and English languages, we calculate F1 based on tokens, whereas F1 is calculated based on characters for Japanese.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.1" class="ltx_Math" alttext="Precision(P)=\frac{GA\cap PA}{PA}" display="block"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.2" xref="S3.Ex1.m1.1.2.cmml"><mrow id="S3.Ex1.m1.1.2.2" xref="S3.Ex1.m1.1.2.2.cmml"><mi id="S3.Ex1.m1.1.2.2.2" xref="S3.Ex1.m1.1.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.3" xref="S3.Ex1.m1.1.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1a" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.4" xref="S3.Ex1.m1.1.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1b" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.5" xref="S3.Ex1.m1.1.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1c" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.6" xref="S3.Ex1.m1.1.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1d" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.7" xref="S3.Ex1.m1.1.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1e" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.8" xref="S3.Ex1.m1.1.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1f" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.9" xref="S3.Ex1.m1.1.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1g" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.2.10" xref="S3.Ex1.m1.1.2.2.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.2.1h" xref="S3.Ex1.m1.1.2.2.1.cmml">â€‹</mo><mrow id="S3.Ex1.m1.1.2.2.11.2" xref="S3.Ex1.m1.1.2.2.cmml"><mo stretchy="false" id="S3.Ex1.m1.1.2.2.11.2.1" xref="S3.Ex1.m1.1.2.2.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">P</mi><mo stretchy="false" id="S3.Ex1.m1.1.2.2.11.2.2" xref="S3.Ex1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.2.1" xref="S3.Ex1.m1.1.2.1.cmml">=</mo><mfrac id="S3.Ex1.m1.1.2.3" xref="S3.Ex1.m1.1.2.3.cmml"><mrow id="S3.Ex1.m1.1.2.3.2" xref="S3.Ex1.m1.1.2.3.2.cmml"><mrow id="S3.Ex1.m1.1.2.3.2.2" xref="S3.Ex1.m1.1.2.3.2.2.cmml"><mi id="S3.Ex1.m1.1.2.3.2.2.2" xref="S3.Ex1.m1.1.2.3.2.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.3.2.2.1" xref="S3.Ex1.m1.1.2.3.2.2.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.3.2.2.3" xref="S3.Ex1.m1.1.2.3.2.2.3.cmml">A</mi></mrow><mo id="S3.Ex1.m1.1.2.3.2.1" xref="S3.Ex1.m1.1.2.3.2.1.cmml">âˆ©</mo><mrow id="S3.Ex1.m1.1.2.3.2.3" xref="S3.Ex1.m1.1.2.3.2.3.cmml"><mi id="S3.Ex1.m1.1.2.3.2.3.2" xref="S3.Ex1.m1.1.2.3.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.3.2.3.1" xref="S3.Ex1.m1.1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.3.2.3.3" xref="S3.Ex1.m1.1.2.3.2.3.3.cmml">A</mi></mrow></mrow><mrow id="S3.Ex1.m1.1.2.3.3" xref="S3.Ex1.m1.1.2.3.3.cmml"><mi id="S3.Ex1.m1.1.2.3.3.2" xref="S3.Ex1.m1.1.2.3.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.1.2.3.3.1" xref="S3.Ex1.m1.1.2.3.3.1.cmml">â€‹</mo><mi id="S3.Ex1.m1.1.2.3.3.3" xref="S3.Ex1.m1.1.2.3.3.3.cmml">A</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.2.cmml" xref="S3.Ex1.m1.1.2"><eq id="S3.Ex1.m1.1.2.1.cmml" xref="S3.Ex1.m1.1.2.1"></eq><apply id="S3.Ex1.m1.1.2.2.cmml" xref="S3.Ex1.m1.1.2.2"><times id="S3.Ex1.m1.1.2.2.1.cmml" xref="S3.Ex1.m1.1.2.2.1"></times><ci id="S3.Ex1.m1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.2.2.2">ğ‘ƒ</ci><ci id="S3.Ex1.m1.1.2.2.3.cmml" xref="S3.Ex1.m1.1.2.2.3">ğ‘Ÿ</ci><ci id="S3.Ex1.m1.1.2.2.4.cmml" xref="S3.Ex1.m1.1.2.2.4">ğ‘’</ci><ci id="S3.Ex1.m1.1.2.2.5.cmml" xref="S3.Ex1.m1.1.2.2.5">ğ‘</ci><ci id="S3.Ex1.m1.1.2.2.6.cmml" xref="S3.Ex1.m1.1.2.2.6">ğ‘–</ci><ci id="S3.Ex1.m1.1.2.2.7.cmml" xref="S3.Ex1.m1.1.2.2.7">ğ‘ </ci><ci id="S3.Ex1.m1.1.2.2.8.cmml" xref="S3.Ex1.m1.1.2.2.8">ğ‘–</ci><ci id="S3.Ex1.m1.1.2.2.9.cmml" xref="S3.Ex1.m1.1.2.2.9">ğ‘œ</ci><ci id="S3.Ex1.m1.1.2.2.10.cmml" xref="S3.Ex1.m1.1.2.2.10">ğ‘›</ci><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">ğ‘ƒ</ci></apply><apply id="S3.Ex1.m1.1.2.3.cmml" xref="S3.Ex1.m1.1.2.3"><divide id="S3.Ex1.m1.1.2.3.1.cmml" xref="S3.Ex1.m1.1.2.3"></divide><apply id="S3.Ex1.m1.1.2.3.2.cmml" xref="S3.Ex1.m1.1.2.3.2"><intersect id="S3.Ex1.m1.1.2.3.2.1.cmml" xref="S3.Ex1.m1.1.2.3.2.1"></intersect><apply id="S3.Ex1.m1.1.2.3.2.2.cmml" xref="S3.Ex1.m1.1.2.3.2.2"><times id="S3.Ex1.m1.1.2.3.2.2.1.cmml" xref="S3.Ex1.m1.1.2.3.2.2.1"></times><ci id="S3.Ex1.m1.1.2.3.2.2.2.cmml" xref="S3.Ex1.m1.1.2.3.2.2.2">ğº</ci><ci id="S3.Ex1.m1.1.2.3.2.2.3.cmml" xref="S3.Ex1.m1.1.2.3.2.2.3">ğ´</ci></apply><apply id="S3.Ex1.m1.1.2.3.2.3.cmml" xref="S3.Ex1.m1.1.2.3.2.3"><times id="S3.Ex1.m1.1.2.3.2.3.1.cmml" xref="S3.Ex1.m1.1.2.3.2.3.1"></times><ci id="S3.Ex1.m1.1.2.3.2.3.2.cmml" xref="S3.Ex1.m1.1.2.3.2.3.2">ğ‘ƒ</ci><ci id="S3.Ex1.m1.1.2.3.2.3.3.cmml" xref="S3.Ex1.m1.1.2.3.2.3.3">ğ´</ci></apply></apply><apply id="S3.Ex1.m1.1.2.3.3.cmml" xref="S3.Ex1.m1.1.2.3.3"><times id="S3.Ex1.m1.1.2.3.3.1.cmml" xref="S3.Ex1.m1.1.2.3.3.1"></times><ci id="S3.Ex1.m1.1.2.3.3.2.cmml" xref="S3.Ex1.m1.1.2.3.3.2">ğ‘ƒ</ci><ci id="S3.Ex1.m1.1.2.3.3.3.cmml" xref="S3.Ex1.m1.1.2.3.3.3">ğ´</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">Precision(P)=\frac{GA\cap PA}{PA}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="Recall(R)=\frac{GA\cap PA}{GA}" display="block"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.2" xref="S3.Ex2.m1.1.2.cmml"><mrow id="S3.Ex2.m1.1.2.2" xref="S3.Ex2.m1.1.2.2.cmml"><mi id="S3.Ex2.m1.1.2.2.2" xref="S3.Ex2.m1.1.2.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.2.3" xref="S3.Ex2.m1.1.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1a" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.2.4" xref="S3.Ex2.m1.1.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1b" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.2.5" xref="S3.Ex2.m1.1.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1c" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.2.6" xref="S3.Ex2.m1.1.2.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1d" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.2.7" xref="S3.Ex2.m1.1.2.2.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.2.1e" xref="S3.Ex2.m1.1.2.2.1.cmml">â€‹</mo><mrow id="S3.Ex2.m1.1.2.2.8.2" xref="S3.Ex2.m1.1.2.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.1.2.2.8.2.1" xref="S3.Ex2.m1.1.2.2.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">R</mi><mo stretchy="false" id="S3.Ex2.m1.1.2.2.8.2.2" xref="S3.Ex2.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.1.2.1" xref="S3.Ex2.m1.1.2.1.cmml">=</mo><mfrac id="S3.Ex2.m1.1.2.3" xref="S3.Ex2.m1.1.2.3.cmml"><mrow id="S3.Ex2.m1.1.2.3.2" xref="S3.Ex2.m1.1.2.3.2.cmml"><mrow id="S3.Ex2.m1.1.2.3.2.2" xref="S3.Ex2.m1.1.2.3.2.2.cmml"><mi id="S3.Ex2.m1.1.2.3.2.2.2" xref="S3.Ex2.m1.1.2.3.2.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.3.2.2.1" xref="S3.Ex2.m1.1.2.3.2.2.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.3.2.2.3" xref="S3.Ex2.m1.1.2.3.2.2.3.cmml">A</mi></mrow><mo id="S3.Ex2.m1.1.2.3.2.1" xref="S3.Ex2.m1.1.2.3.2.1.cmml">âˆ©</mo><mrow id="S3.Ex2.m1.1.2.3.2.3" xref="S3.Ex2.m1.1.2.3.2.3.cmml"><mi id="S3.Ex2.m1.1.2.3.2.3.2" xref="S3.Ex2.m1.1.2.3.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.3.2.3.1" xref="S3.Ex2.m1.1.2.3.2.3.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.3.2.3.3" xref="S3.Ex2.m1.1.2.3.2.3.3.cmml">A</mi></mrow></mrow><mrow id="S3.Ex2.m1.1.2.3.3" xref="S3.Ex2.m1.1.2.3.3.cmml"><mi id="S3.Ex2.m1.1.2.3.3.2" xref="S3.Ex2.m1.1.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.2.3.3.1" xref="S3.Ex2.m1.1.2.3.3.1.cmml">â€‹</mo><mi id="S3.Ex2.m1.1.2.3.3.3" xref="S3.Ex2.m1.1.2.3.3.3.cmml">A</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.2.cmml" xref="S3.Ex2.m1.1.2"><eq id="S3.Ex2.m1.1.2.1.cmml" xref="S3.Ex2.m1.1.2.1"></eq><apply id="S3.Ex2.m1.1.2.2.cmml" xref="S3.Ex2.m1.1.2.2"><times id="S3.Ex2.m1.1.2.2.1.cmml" xref="S3.Ex2.m1.1.2.2.1"></times><ci id="S3.Ex2.m1.1.2.2.2.cmml" xref="S3.Ex2.m1.1.2.2.2">ğ‘…</ci><ci id="S3.Ex2.m1.1.2.2.3.cmml" xref="S3.Ex2.m1.1.2.2.3">ğ‘’</ci><ci id="S3.Ex2.m1.1.2.2.4.cmml" xref="S3.Ex2.m1.1.2.2.4">ğ‘</ci><ci id="S3.Ex2.m1.1.2.2.5.cmml" xref="S3.Ex2.m1.1.2.2.5">ğ‘</ci><ci id="S3.Ex2.m1.1.2.2.6.cmml" xref="S3.Ex2.m1.1.2.2.6">ğ‘™</ci><ci id="S3.Ex2.m1.1.2.2.7.cmml" xref="S3.Ex2.m1.1.2.2.7">ğ‘™</ci><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">ğ‘…</ci></apply><apply id="S3.Ex2.m1.1.2.3.cmml" xref="S3.Ex2.m1.1.2.3"><divide id="S3.Ex2.m1.1.2.3.1.cmml" xref="S3.Ex2.m1.1.2.3"></divide><apply id="S3.Ex2.m1.1.2.3.2.cmml" xref="S3.Ex2.m1.1.2.3.2"><intersect id="S3.Ex2.m1.1.2.3.2.1.cmml" xref="S3.Ex2.m1.1.2.3.2.1"></intersect><apply id="S3.Ex2.m1.1.2.3.2.2.cmml" xref="S3.Ex2.m1.1.2.3.2.2"><times id="S3.Ex2.m1.1.2.3.2.2.1.cmml" xref="S3.Ex2.m1.1.2.3.2.2.1"></times><ci id="S3.Ex2.m1.1.2.3.2.2.2.cmml" xref="S3.Ex2.m1.1.2.3.2.2.2">ğº</ci><ci id="S3.Ex2.m1.1.2.3.2.2.3.cmml" xref="S3.Ex2.m1.1.2.3.2.2.3">ğ´</ci></apply><apply id="S3.Ex2.m1.1.2.3.2.3.cmml" xref="S3.Ex2.m1.1.2.3.2.3"><times id="S3.Ex2.m1.1.2.3.2.3.1.cmml" xref="S3.Ex2.m1.1.2.3.2.3.1"></times><ci id="S3.Ex2.m1.1.2.3.2.3.2.cmml" xref="S3.Ex2.m1.1.2.3.2.3.2">ğ‘ƒ</ci><ci id="S3.Ex2.m1.1.2.3.2.3.3.cmml" xref="S3.Ex2.m1.1.2.3.2.3.3">ğ´</ci></apply></apply><apply id="S3.Ex2.m1.1.2.3.3.cmml" xref="S3.Ex2.m1.1.2.3.3"><times id="S3.Ex2.m1.1.2.3.3.1.cmml" xref="S3.Ex2.m1.1.2.3.3.1"></times><ci id="S3.Ex2.m1.1.2.3.3.2.cmml" xref="S3.Ex2.m1.1.2.3.3.2">ğº</ci><ci id="S3.Ex2.m1.1.2.3.3.3.cmml" xref="S3.Ex2.m1.1.2.3.3.3">ğ´</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">Recall(R)=\frac{GA\cap PA}{GA}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.1" class="ltx_Math" alttext="F1=\frac{2PR}{P+R}" display="block"><semantics id="S3.Ex3.m1.1a"><mrow id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml"><mrow id="S3.Ex3.m1.1.1.2" xref="S3.Ex3.m1.1.1.2.cmml"><mi id="S3.Ex3.m1.1.1.2.2" xref="S3.Ex3.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.1.2.1" xref="S3.Ex3.m1.1.1.2.1.cmml">â€‹</mo><mn id="S3.Ex3.m1.1.1.2.3" xref="S3.Ex3.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.cmml">=</mo><mfrac id="S3.Ex3.m1.1.1.3" xref="S3.Ex3.m1.1.1.3.cmml"><mrow id="S3.Ex3.m1.1.1.3.2" xref="S3.Ex3.m1.1.1.3.2.cmml"><mn id="S3.Ex3.m1.1.1.3.2.2" xref="S3.Ex3.m1.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.1.3.2.1" xref="S3.Ex3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.Ex3.m1.1.1.3.2.3" xref="S3.Ex3.m1.1.1.3.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.1.1.3.2.1a" xref="S3.Ex3.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S3.Ex3.m1.1.1.3.2.4" xref="S3.Ex3.m1.1.1.3.2.4.cmml">R</mi></mrow><mrow id="S3.Ex3.m1.1.1.3.3" xref="S3.Ex3.m1.1.1.3.3.cmml"><mi id="S3.Ex3.m1.1.1.3.3.2" xref="S3.Ex3.m1.1.1.3.3.2.cmml">P</mi><mo id="S3.Ex3.m1.1.1.3.3.1" xref="S3.Ex3.m1.1.1.3.3.1.cmml">+</mo><mi id="S3.Ex3.m1.1.1.3.3.3" xref="S3.Ex3.m1.1.1.3.3.3.cmml">R</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1"><eq id="S3.Ex3.m1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1"></eq><apply id="S3.Ex3.m1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.2"><times id="S3.Ex3.m1.1.1.2.1.cmml" xref="S3.Ex3.m1.1.1.2.1"></times><ci id="S3.Ex3.m1.1.1.2.2.cmml" xref="S3.Ex3.m1.1.1.2.2">ğ¹</ci><cn type="integer" id="S3.Ex3.m1.1.1.2.3.cmml" xref="S3.Ex3.m1.1.1.2.3">1</cn></apply><apply id="S3.Ex3.m1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.3"><divide id="S3.Ex3.m1.1.1.3.1.cmml" xref="S3.Ex3.m1.1.1.3"></divide><apply id="S3.Ex3.m1.1.1.3.2.cmml" xref="S3.Ex3.m1.1.1.3.2"><times id="S3.Ex3.m1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.1.1.3.2.1"></times><cn type="integer" id="S3.Ex3.m1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.1.1.3.2.2">2</cn><ci id="S3.Ex3.m1.1.1.3.2.3.cmml" xref="S3.Ex3.m1.1.1.3.2.3">ğ‘ƒ</ci><ci id="S3.Ex3.m1.1.1.3.2.4.cmml" xref="S3.Ex3.m1.1.1.3.2.4">ğ‘…</ci></apply><apply id="S3.Ex3.m1.1.1.3.3.cmml" xref="S3.Ex3.m1.1.1.3.3"><plus id="S3.Ex3.m1.1.1.3.3.1.cmml" xref="S3.Ex3.m1.1.1.3.3.1"></plus><ci id="S3.Ex3.m1.1.1.3.3.2.cmml" xref="S3.Ex3.m1.1.1.3.3.2">ğ‘ƒ</ci><ci id="S3.Ex3.m1.1.1.3.3.3.cmml" xref="S3.Ex3.m1.1.1.3.3.3">ğ‘…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">F1=\frac{2PR}{P+R}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_cite">Papineni etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2002</a>)</cite>, the <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">B</span>ilingual <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_bold">E</span>valuation <span id="S3.SS2.p5.1.3" class="ltx_text ltx_font_bold">U</span>nderstudy (BLEU), a popular evaluation metric in machine translation, computes the n-gram co-occurrence between human-generation answers and system-generation answers. The best performances were estimated by averaged BLEU-based performances (BLEU-1, BLEU-2, BLEU-3, and BLEU-4) on the public test and private test sets. Both evaluation metrics ignore punctuations.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Schedule and Overview Summary</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 â€£ 3.3 Schedule and Overview Summary â€£ 3 The VLSP 2022 - EVJVQA Challenge â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows important dates of the VLSP 2022 - EVJVQA Challenge. It lasted for two months, during which the participating teams spent 39 days developing the multilingual visual question answering systems.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Schedule of the VLSP 2021 - ViMRC Challenge.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Time</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">Phase</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">October 1st</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Trial Data</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left">October 5th</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_left">Public test</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left">November 10th</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_left">Private test</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left">November 12th</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_left">Competition end</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_t">November 20th</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Submission deadline</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left">November 23th</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_left">Notification of acceptance</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_align_left ltx_border_b">November 25th</td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_align_left ltx_border_b">Camera-ready due</td>
</tr>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Besides, Table <a href="#S3.T2" title="Table 2 â€£ 3.3 Schedule and Overview Summary â€£ 3 The VLSP 2022 - EVJVQA Challenge â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents an overview of the participating teams who joined the VLSP2022-EVJVQA.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Participation summary of the VLSP 2022 - EVJVQA Challenge.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">#Registration Teams</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_right ltx_border_t">62</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_left">#Joined Teams</td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_right">57</td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_left">#Signed Data Agreements</td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_right">36</td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_left">#Submitted Teams</td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_right">8</td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_align_left ltx_border_b">#Paper Submissions</td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_align_right ltx_border_b">5</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Corpus Creation</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2302.11752/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall pipeline process for creating the UIT-EVJVQA dataset.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">A previous work <cite class="ltx_cite ltx_citemacro_cite">Tran etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> inherited assets from the well-known VQA benchmark in English and the COCO-QA <cite class="ltx_cite ltx_citemacro_cite">Ren etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2015a</a>)</cite>, then they proposed a semi-automatic annotating system by using machine translation to translate question-answer pairs from English to Vietnamese. On the other hand, we argue that the context in images captured in Vietnam is more complicated than in images coming from VQA benchmarks in English because of its crowded scene and â€out-of-commonâ€ objects, or in particular, objects that are not commonly used outside of Vietnam. Moreover, using such a machine translation system as <cite class="ltx_cite ltx_citemacro_cite">Tran etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> is hard to ensure the natural aspect of using language, which caused lots of confusion while evaluating VQA methods in Vietnamese. To overcome the above flaws and research and develop a VQA system, particularly for the Vietnamese, we constructed the novel dataset with images collected manually and relevant to daily life in Vietnam. In addition, to address and challenge the research community, we provided our dataset <span id="S4.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">multilingual</span> question-answer pairs to encourage the research community to explore and propose an effective system that can answer questions written in various languages.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Image collection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To build a VQA dataset in the Vietnamese context, we search for images with a diverse and open-domain set of keywords. We first selectively prepare various keywords which are relevant to Vietnamese locations and daily life activities or result in images that specifically contain targeted objects in Vietnamese scenes. For instance, the keywords can be Vietnamese streets, markets, sidewalk eateries, cultural sites, human outdoor activities, means of transport, or house interiors. Some keywords are appended with Vietnamese location names like Hanoi or Saigon for more variation in geological and cultural context. We then use these keywords to scrap images from Google Images. The image-scraping process is facilitated by the Octoparse tool.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">After collecting the images, we proceed with the filtering stage. The images originally came in various sizes. However, we must ensure the details in them are clearly visible. Therefore, we only keep images with widths and heights greater than 500 and 400, respectively. We also filter out GIF files and other file formats apart from JPEG and PNG. As a result, we obtain 4,909 images with their size varying in the range of 500 - 6000 pixels in terms of width and the range of 400 - 4032 pixels in terms of height.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Questions and answers creation process</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The questions and answers (QAs) of the UIT-EVJVQA dataset are first created in Vietnamese throughout the set of images. Afterward, these QAs are translated into English and Japanese. Both stages are conducted by the source of crowd workers. QAs in all three languages are then merged according to the images and eventually constitute the final corpus. The overall pipeline of the aforementioned process is visualized in Figure <a href="#S4.F2" title="Figure 2 â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Base Vietnamese QAs creation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We first employ five crowd workers for the Vietnamese questions and answers creation stage.
For each image, the workers are asked to formulate 3-5 question-answer pairs based on the <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">details</span> and objects that appear in the visible scene. The workers are required to conform to the following guideline:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Encourage using phrases or full sentences to give the answers.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Restrict the use of single words as answers.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">No selective question or yes/no question allowed.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Numbers must be typed in alphabet characters rather than numeric characters, and they must not be greater than 10.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">For colors, only use provided colors such as black, white, red, orange, yellow, green, blue, pink, purple, brown, and grey. If these colors cannot exactly describe the true color of the object, ignore such color property in the sentence.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">In the case of mentioning direction, if following that direction words is an object, then such direction is defined based on that object, else using the perspective of the annotator to define the direction.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Eventually, this stage yields 11,689 pairs of question and answer in Vietnamese for our dataset.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>English and Japanese QAs human-translation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">All the Vietnamese QAs are henceforth passed through the human-translation stages. These stages demand the employment of qualified crowd workers for the translation of questions and answers. For English translation, the workers must have at least IELTS certification with an overall band score of 6.5. Meanwhile, translators for the Japanese translation task must achieve an N3 proficiency level or above. Overall, there
were seven and nine translators working simultaneously for the English and Japanese translation process, respectively. The English and Japanese translation stages also have their corresponding guidelines.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">English translation guideline</span>: The English QAs are translated from the Vietnamese ones, with many entities and attributes in the sentences retained during the translation as possible. The translators are encouraged to use phrases or full sentences as the translated answers. Apostrophes in sentences are restricted. Thus, translators should use the uncontracted form or other valid grammar formulations.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p"><span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Japanese translation guideline</span>: The Japanese QAs are translated from the Vietnamese ones, with many entities and attributes in the sentences retained during the translation as possible. For transcribing Vietnamese proper nouns or other foreign words, the katakana syllabary is adopted. The polite form is utilized for writing translated questions and answers that contain verbs. In the case of complex Vietnamese questions and answers with multiple relevant information that may not be easily translated into continuous Japanese sentences, translators can use commas to split the sentences into smaller parts and then translate them subsequently. For example, the question â€What products does the woman wearing a helmet go to the store to buy?â€ can be translated to Japanese as â€ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆã‚’ã‹ã¶ã£ãŸå¥³æ€§ãŒåº—ã«è²·ã„ã«è¡Œã£ã¦ã€ã©ã†ã®å•†å“ã‚’è²·ã„ã¾ã™ã‹?â€, which literally means â€A woman wearing a helmet goes shopping at a store, what products does she buy?â€.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">After the human-translation process, we obtained 10,539 question-answer pairs in English and 11,562 question-answer pairs in Japanese.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pre-processing and splitting</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We normalize the Vietnamese and English QAs into lowercase. Latin characters in Japanese words, â€Tã‚·ãƒ£ãƒ„â€ (T-shirt) for instance, are also normalized similarly. After that slight pre-processing step, we merge all the QAs in all three languages into one dataset.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To prepare for the VLSP 2022 - EVJVQA Challenge, we split the dataset into training set, public test set, and private test set. We provide 3,763 images for the training set, while each test set comprises 558 images. Since no image is placed in multiple sets at once, we ensure that all QAs for a particular image only appear in the set to which that image belongs. The corresponding number of QAs in each language for each of the sets is shown in Table <a href="#S4.T3" title="Table 3 â€£ 4.3 Pre-processing and splitting â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:70.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.0pt,9.8pt) scale(0.783020461894594,0.783020461894594) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Training</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Public test</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Private test</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.1.2.1.1" class="ltx_text ltx_font_bold">Vietnamese</span></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">8,334</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1,685</td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">1,670</td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">11,689</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.3.1.1" class="ltx_text ltx_font_bold">English</span></td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center">7,189</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center">1,679</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center">1,671</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center">10,539</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T3.1.1.4.1.1" class="ltx_text ltx_font_bold">Japanese</span></td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center">8,262</td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center">1,651</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center">1,649</td>
<td id="S4.T3.1.1.4.5" class="ltx_td ltx_align_center">11,562</td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.1.1.5.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_b">23,785</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center ltx_border_b">5,015</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center ltx_border_b">4,990</td>
<td id="S4.T3.1.1.5.5" class="ltx_td ltx_align_center ltx_border_b">33,790</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Number of QAs in each language in our UIT-EVJVQA dataset.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Statistics</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">As our dataset was constructed from three languages: Vietnamese, English, and Japanese, we conducted statistics to deeply observe the characteristic of each language as well as gain clear insights into the three languages.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:74pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.5pt,8.0pt) scale(0.82194045573053,0.82194045573053) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_border_t" rowspan="2"></td>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Question</span></td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Answer</span></td>
</tr>
<tr id="S4.T4.1.1.2" class="ltx_tr">
<td id="S4.T4.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.1.1" class="ltx_text ltx_font_bold">Max.</span></td>
<td id="S4.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.2.1" class="ltx_text ltx_font_bold">Min.</span></td>
<td id="S4.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.3.1" class="ltx_text ltx_font_bold">Avg.</span></td>
<td id="S4.T4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.4.1" class="ltx_text ltx_font_bold">Max.</span></td>
<td id="S4.T4.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.5.1" class="ltx_text ltx_font_bold">Min.</span></td>
<td id="S4.T4.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.2.6.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="S4.T4.1.1.3" class="ltx_tr">
<td id="S4.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.1.3.1.1" class="ltx_text ltx_font_bold">Vietnamese</span></td>
<td id="S4.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">22</td>
<td id="S4.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">8.7</td>
<td id="S4.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S4.T4.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T4.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">7.2</td>
</tr>
<tr id="S4.T4.1.1.4" class="ltx_tr">
<td id="S4.T4.1.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T4.1.1.4.1.1" class="ltx_text ltx_font_bold">English</span></td>
<td id="S4.T4.1.1.4.2" class="ltx_td ltx_align_center">26</td>
<td id="S4.T4.1.1.4.3" class="ltx_td ltx_align_center">3</td>
<td id="S4.T4.1.1.4.4" class="ltx_td ltx_align_center">8.6</td>
<td id="S4.T4.1.1.4.5" class="ltx_td ltx_align_center">23</td>
<td id="S4.T4.1.1.4.6" class="ltx_td ltx_align_center">1</td>
<td id="S4.T4.1.1.4.7" class="ltx_td ltx_align_center">5.0</td>
</tr>
<tr id="S4.T4.1.1.5" class="ltx_tr">
<td id="S4.T4.1.1.5.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T4.1.1.5.1.1" class="ltx_text ltx_font_bold">Japanese</span></td>
<td id="S4.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_b">45</td>
<td id="S4.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_b">4</td>
<td id="S4.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_b">13.3</td>
<td id="S4.T4.1.1.5.5" class="ltx_td ltx_align_center ltx_border_b">23</td>
<td id="S4.T4.1.1.5.6" class="ltx_td ltx_align_center ltx_border_b">1</td>
<td id="S4.T4.1.1.5.7" class="ltx_td ltx_align_center ltx_border_b">5.9</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Statistic of question and answer in the UIT-EVJVQA dataset.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">To conduct statistics on Vietnamese, we use the word segmentation method from the VnCoreNLP <cite class="ltx_cite ltx_citemacro_cite">Vu etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite> as in Vietnamese, a word may have more than one token (for instance, â€cá»­a hÃ ng táº¡p hÃ³aâ€ is formed from two Vietnamese words â€cá»­a hÃ ngâ€ and â€táº¡p hÃ³aâ€, which is in turn formed from more than one token). For English QAs, we achieved tokens by splitting sentences using space. For Japanese QAs, like Vietnamese, Japanese uses hieroglyphs to form their word, hence each Japanese word may have more than one hieroglyph. We use the janome<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/mocobeta/janome</span></span></span> library to perform word segmentation on Japanese QAs.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/vi_qa_wordcloud.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Vietnamese</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/en_qa_wordcloud.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/jp_qa_wordcloud.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Japanese</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Word cloud of tokens in three language partitions of the UIT-EVJVQA dataset.</figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 â€£ 4.4 Statistics â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> indicate, with the same meaning, Japanese in general use more words to describe than Vietnamese and English. This implies another challenge for VQA method when tackling Japanese text beside the complexity of multilingualism in our dataset. Moreover, Vietnamese and English have the same distribution of length in terms of questions (according to Table <a href="#S4.T4" title="Table 4 â€£ 4.4 Statistics â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), while English has shorter answers compared with those in Vietnamese.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/vi_question_len.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="323" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/vi_answer_len.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Statistics of the length of QAs in Vietnamese partition of the UIT-EVJVQA dataset.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/en_question_len.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/en_answer_len.png" id="S4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Statistics of the length of QAs in English partition of the UIT-EVJVQA dataset.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/jp_question_len.png" id="S4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/jp_answer_len.png" id="S4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="317" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Statistics of the length of QAs in Japanese partition of the UIT-EVJVQA dataset.</figcaption>
</figure>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Answers of the three languages share the same characteristic where the most appearance of length is two. This indicates humans while giving answers, prefer saying in short statements, and this behavior leads to the classification approach on the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Teney etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>. However, such short answers are not always the case in our dataset as the context of images taken in Vietnam is complicated because of the crowded scenes, traffic jams, or street stalls, and short answers are not enough to answer questions enquiring about complex scenes. Moreover, we aim to emphasize the language aspect of the VQA task, which means we want to guide the community to research and propose a system that can give answers flexibly and naturally as a human does, not the way of â€selectingâ€ answers from a defined set as most approaches in the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>); Teney etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>. To this end, the answers given in our dataset are diverse in length and complicated in terms of level (word, phrase, or sentence level). Interestingly, while annotating answers, we found that giving a phrase or sentence as an answer is more fluent and human-like than giving only words or phrases as in the VQA dataset of <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">Another factor to consider while annotating the UIT-EVJVQA dataset is the language prior phenomenon <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>. This is the phenomenon where the VQA methods try to learn patterns between questions and answers, such as questions starting with â€how manyâ€ usually go with the answer â€twoâ€. As analyzed in <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, the language priors in VQA dataset is the result of the classification approach proposed for VQA task <cite class="ltx_cite ltx_citemacro_cite">Teney etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> and cause the model to learn the way of recognizing answer based on the question rather than the way of how to make use of the image to answer the given question. Therefore while constructing the guideline to annotate the UIT-EVJVQA dataset, we propose to give answers using words, phrases, or sentences. In this way, we can first eliminate the traditional classification approach proposed for the VQA task in English as well as avoid the language priors in our dataset.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Systems and Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The aim of the challenge is to evaluate the quality
of the teamsâ€™ approaches to multilingual visual question-answering systems.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baseline System</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Following Changpinyo et al. <cite class="ltx_cite ltx_citemacro_cite">Changpinyo etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2022c</a>)</cite>, we adopt transfer learning based on Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> for our baseline system. This work uses the training set for fine-tuning the pre-trained mBERT <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> model before generating answers. In addition, we use the ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> to extract the visual features of images (Figure <a href="#S5.F7" title="Figure 7 â€£ 5.1 Baseline System â€£ 5 Systems and Results â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Pre-trained ViT and mBERT models are initialized from HuggingFace checkpoints. We trained the baseline with a batch size of 64 and adapted the learning rate scheduler from Vaswani et al. <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite> to reduce the learning rate gradually after a number of iterations. The training process was interrupted automatically if the evaluation scores did not increase after 5 epochs.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2302.11752/assets/x3.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="216" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The baseline model architecture at the VLSP2022-EVJVQA Challenge.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Challenge Submission</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The competition was hosted offline where the organizers provide the training set and public test set to the participant teams to evaluate and fine-tune their methods. When the competition came into the private test phase, the submission policy allows each participant team to submit up to 3 <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">different</span> methods during a submission time lasting 3 days. After the three-day private test phase, we evaluated their submitted results and obtain their F1 score as well as avg. BLEU score on the private test set. The final score of each participant team is the highest score among their submitted models on the private test set and their rank was indicated based on the F1 score ((BLUE as a secondary score when there is a tie).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Experimental Results</h3>

<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:206.8pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.8pt,19.4pt) scale(0.841525365482142,0.841525365482142) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T5.1.1.1.1.1" class="ltx_text">No.</span></td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T5.1.1.1.2.1" class="ltx_text">Team name</span></td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S5.T5.1.1.1.3.1" class="ltx_text">Model type</span></td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T5.1.1.1.4.1" class="ltx_text">Models</span></td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Public Test</td>
<td id="S5.T5.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Private Test</td>
</tr>
<tr id="S5.T5.1.1.2" class="ltx_tr">
<td id="S5.T5.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td id="S5.T5.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">BLEU</td>
<td id="S5.T5.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td id="S5.T5.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">BLEU</td>
</tr>
<tr id="S5.T5.1.1.3" class="ltx_tr">
<td id="S5.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.1.3.1.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S5.T5.1.1.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.1.3.2.1" class="ltx_text ltx_font_bold">CIST AI</span></td>
<td id="S5.T5.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">Single</td>
<td id="S5.T5.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">ViT + mT5</td>
<td id="S5.T5.1.1.3.5" class="ltx_td ltx_align_left ltx_border_t">0.3491</td>
<td id="S5.T5.1.1.3.6" class="ltx_td ltx_align_left ltx_border_t">0.2508</td>
<td id="S5.T5.1.1.3.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.1.3.7.1" class="ltx_text ltx_font_bold">0.4392</span></td>
<td id="S5.T5.1.1.3.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.1.3.8.1" class="ltx_text ltx_font_bold">0.4009</span></td>
</tr>
<tr id="S5.T5.1.1.4" class="ltx_tr">
<td id="S5.T5.1.1.4.1" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.4.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S5.T5.1.1.4.2" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.4.2.1" class="ltx_text ltx_font_bold">OhYeah</span></td>
<td id="S5.T5.1.1.4.3" class="ltx_td ltx_align_center">Single</td>
<td id="S5.T5.1.1.4.4" class="ltx_td ltx_align_center">ViT + mT5</td>
<td id="S5.T5.1.1.4.5" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.4.5.1" class="ltx_text ltx_font_bold">0.5755</span></td>
<td id="S5.T5.1.1.4.6" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.4.6.1" class="ltx_text ltx_font_bold">0.4866</span></td>
<td id="S5.T5.1.1.4.7" class="ltx_td ltx_align_left">0.4349</td>
<td id="S5.T5.1.1.4.8" class="ltx_td ltx_align_left">0.3868</td>
</tr>
<tr id="S5.T5.1.1.5" class="ltx_tr">
<td id="S5.T5.1.1.5.1" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.5.1.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S5.T5.1.1.5.2" class="ltx_td ltx_align_left"><span id="S5.T5.1.1.5.2.1" class="ltx_text ltx_font_bold">DS-STBFL</span></td>
<td id="S5.T5.1.1.5.3" class="ltx_td ltx_align_center">Ensemble</td>
<td id="S5.T5.1.1.5.4" class="ltx_td ltx_align_center">CNN-Seq2Seq + ViT + OFA</td>
<td id="S5.T5.1.1.5.5" class="ltx_td ltx_align_left">0.3390</td>
<td id="S5.T5.1.1.5.6" class="ltx_td ltx_align_left">0.2156</td>
<td id="S5.T5.1.1.5.7" class="ltx_td ltx_align_left">0.4210</td>
<td id="S5.T5.1.1.5.8" class="ltx_td ltx_align_left">0.3482</td>
</tr>
<tr id="S5.T5.1.1.6" class="ltx_tr">
<td id="S5.T5.1.1.6.1" class="ltx_td ltx_align_left">4</td>
<td id="S5.T5.1.1.6.2" class="ltx_td ltx_align_left">FCoin</td>
<td id="S5.T5.1.1.6.3" class="ltx_td ltx_align_center">Single</td>
<td id="S5.T5.1.1.6.4" class="ltx_td ltx_align_center">ViT + mBERT</td>
<td id="S5.T5.1.1.6.5" class="ltx_td ltx_align_left">0.3355</td>
<td id="S5.T5.1.1.6.6" class="ltx_td ltx_align_left">0.2437</td>
<td id="S5.T5.1.1.6.7" class="ltx_td ltx_align_left">0.4103</td>
<td id="S5.T5.1.1.6.8" class="ltx_td ltx_align_left">0.3549</td>
</tr>
<tr id="S5.T5.1.1.7" class="ltx_tr">
<td id="S5.T5.1.1.7.1" class="ltx_td ltx_align_left">5</td>
<td id="S5.T5.1.1.7.2" class="ltx_td ltx_align_left">VL-UIT</td>
<td id="S5.T5.1.1.7.3" class="ltx_td ltx_align_center">Single</td>
<td id="S5.T5.1.1.7.4" class="ltx_td ltx_align_center">
<table id="S5.T5.1.1.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.1.1.7.4.1.1" class="ltx_tr">
<td id="S5.T5.1.1.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">BEiT + CLIP + Detectron-2</td>
</tr>
<tr id="S5.T5.1.1.7.4.1.2" class="ltx_tr">
<td id="S5.T5.1.1.7.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+ mBERT + BM25 + FastText</td>
</tr>
</table>
</td>
<td id="S5.T5.1.1.7.5" class="ltx_td ltx_align_left">0.3053</td>
<td id="S5.T5.1.1.7.6" class="ltx_td ltx_align_left">0.1878</td>
<td id="S5.T5.1.1.7.7" class="ltx_td ltx_align_left">0.3663</td>
<td id="S5.T5.1.1.7.8" class="ltx_td ltx_align_left">0.2743</td>
</tr>
<tr id="S5.T5.1.1.8" class="ltx_tr">
<td id="S5.T5.1.1.8.1" class="ltx_td ltx_align_left">6</td>
<td id="S5.T5.1.1.8.2" class="ltx_td ltx_align_left">BDboi</td>
<td id="S5.T5.1.1.8.3" class="ltx_td ltx_align_center">Ensemble</td>
<td id="S5.T5.1.1.8.4" class="ltx_td ltx_align_center">
<table id="S5.T5.1.1.8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.1.1.8.4.1.1" class="ltx_tr">
<td id="S5.T5.1.1.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ViT + BEiT + SwinTransformer</td>
</tr>
<tr id="S5.T5.1.1.8.4.1.2" class="ltx_tr">
<td id="S5.T5.1.1.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+ CLIP + OFA + BLIP</td>
</tr>
</table>
</td>
<td id="S5.T5.1.1.8.5" class="ltx_td ltx_align_left">0.3023</td>
<td id="S5.T5.1.1.8.6" class="ltx_td ltx_align_left">0.2183</td>
<td id="S5.T5.1.1.8.7" class="ltx_td ltx_align_left">0.3164</td>
<td id="S5.T5.1.1.8.8" class="ltx_td ltx_align_left">0.2649</td>
</tr>
<tr id="S5.T5.1.1.9" class="ltx_tr">
<td id="S5.T5.1.1.9.1" class="ltx_td ltx_align_left">7</td>
<td id="S5.T5.1.1.9.2" class="ltx_td ltx_align_left">UIT-squad</td>
<td id="S5.T5.1.1.9.3" class="ltx_td ltx_align_center">Ensemble</td>
<td id="S5.T5.1.1.9.4" class="ltx_td ltx_align_center">VinVL+mBERT</td>
<td id="S5.T5.1.1.9.5" class="ltx_td ltx_align_left">0.3224</td>
<td id="S5.T5.1.1.9.6" class="ltx_td ltx_align_left">0.2238</td>
<td id="S5.T5.1.1.9.7" class="ltx_td ltx_align_left">0.3024</td>
<td id="S5.T5.1.1.9.8" class="ltx_td ltx_align_left">0.1667</td>
</tr>
<tr id="S5.T5.1.1.10" class="ltx_tr">
<td id="S5.T5.1.1.10.1" class="ltx_td ltx_align_left">8</td>
<td id="S5.T5.1.1.10.2" class="ltx_td ltx_align_left">VC-Internship</td>
<td id="S5.T5.1.1.10.3" class="ltx_td ltx_align_center">Single</td>
<td id="S5.T5.1.1.10.4" class="ltx_td ltx_align_center">ResNet-152 + OFA</td>
<td id="S5.T5.1.1.10.5" class="ltx_td ltx_align_left">0.3017</td>
<td id="S5.T5.1.1.10.6" class="ltx_td ltx_align_left">0.1639</td>
<td id="S5.T5.1.1.10.7" class="ltx_td ltx_align_left">0.3007</td>
<td id="S5.T5.1.1.10.8" class="ltx_td ltx_align_left">0.1337</td>
</tr>
<tr id="S5.T5.1.1.11" class="ltx_tr">
<td id="S5.T5.1.1.11.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.1.1" class="ltx_text ltx_font_italic">9</span></td>
<td id="S5.T5.1.1.11.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.2.1" class="ltx_text ltx_font_italic">Baseline</span></td>
<td id="S5.T5.1.1.11.3" class="ltx_td ltx_align_center ltx_border_b">Single</td>
<td id="S5.T5.1.1.11.4" class="ltx_td ltx_align_center ltx_border_b">ViT + mBERT</td>
<td id="S5.T5.1.1.11.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.5.1" class="ltx_text ltx_font_italic">0.2924</span></td>
<td id="S5.T5.1.1.11.6" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.6.1" class="ltx_text ltx_font_italic">0.2183</span></td>
<td id="S5.T5.1.1.11.7" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.7.1" class="ltx_text ltx_font_italic">0.3346</span></td>
<td id="S5.T5.1.1.11.8" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.1.11.8.1" class="ltx_text ltx_font_italic">0.2275</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Final results of submitted methods.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">There are 8 participant teams submitted in total, and 5 of them have F1 scores higher than the baseline system.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Result Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We mainly analyze the results of methods coming from participating teams based on the length of questions and answers in each language. To alleviate the result analysis, we define the four different ranges for the length of questions and answers. In particular, we define the <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">short questions</span> (respective, <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">short answer</span>) are questions whose length is shorter or equal to 5 tokens, <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">medium questions</span> (respective, <span id="S6.p1.1.4" class="ltx_text ltx_font_italic">medium answer</span>) questions whose length is between 6 to 10 tokens, <span id="S6.p1.1.5" class="ltx_text ltx_font_italic">long questions</span> (respective, <span id="S6.p1.1.6" class="ltx_text ltx_font_italic">long answer</span>) are questions whose length is between 11 to 15 tokens, and finally, <span id="S6.p1.1.7" class="ltx_text ltx_font_italic">very long questions</span> (respective, <span id="S6.p1.1.8" class="ltx_text ltx_font_italic">very long answer</span>) are questions whose length is greater than 15 tokens. Tokens of questions and answers are defined in the same manner when we do statistics in Section 4.4. We sequentially report the results of the top-5 models in terms of scores and visualization in quantitative analysis and qualitative analysis, respectively, to support our statements as well as indicate what the research community should pay attention to when constructing the novel open-ended VQA dataset.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:174.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.0pt,2.8pt) scale(0.968680741184076,0.968680741184076) ;">
<table id="S6.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T6.1.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1.1" class="ltx_td ltx_border_t" colspan="2" rowspan="2"></td>
<td id="S6.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Top 1</span></td>
<td id="S6.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Top 2</span></td>
<td id="S6.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Top 3</span></td>
<td id="S6.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T6.1.1.1.5.1" class="ltx_text ltx_font_bold">Top 4</span></td>
<td id="S6.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">Top 5</span></td>
</tr>
<tr id="S6.T6.1.1.2" class="ltx_tr">
<td id="S6.T6.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.1.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T6.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.2.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T6.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.3.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T6.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.4.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T6.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.5.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T6.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.6.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T6.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.7.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T6.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.8.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T6.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.9.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T6.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.1.1.2.10.1" class="ltx_text ltx_font_bold">BLEU</span></td>
</tr>
<tr id="S6.T6.1.1.3" class="ltx_tr">
<td id="S6.T6.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S6.T6.1.1.3.1.1" class="ltx_text">
<span id="S6.T6.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:38.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.9pt;transform:translate(-15.08pt,-14.11pt) rotate(-90deg) ;">
<span id="S6.T6.1.1.3.1.1.1.1" class="ltx_p"><span id="S6.T6.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span></span></span></td>
<td id="S6.T6.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T6.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">0.3900</td>
<td id="S6.T6.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">0.1867</td>
<td id="S6.T6.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.5000</td>
<td id="S6.T6.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">0.1250</td>
<td id="S6.T6.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">0.4261</td>
<td id="S6.T6.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">0.2022</td>
<td id="S6.T6.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">0.4308</td>
<td id="S6.T6.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">0.2184</td>
<td id="S6.T6.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">0.3728</td>
<td id="S6.T6.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">0.1535</td>
</tr>
<tr id="S6.T6.1.1.4" class="ltx_tr">
<td id="S6.T6.1.1.4.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T6.1.1.4.2" class="ltx_td ltx_align_center">0.3935</td>
<td id="S6.T6.1.1.4.3" class="ltx_td ltx_align_center">0.2218</td>
<td id="S6.T6.1.1.4.4" class="ltx_td ltx_align_center">0.4922</td>
<td id="S6.T6.1.1.4.5" class="ltx_td ltx_align_center">0.2961</td>
<td id="S6.T6.1.1.4.6" class="ltx_td ltx_align_center">0.3833</td>
<td id="S6.T6.1.1.4.7" class="ltx_td ltx_align_center">0.1889</td>
<td id="S6.T6.1.1.4.8" class="ltx_td ltx_align_center">0.3730</td>
<td id="S6.T6.1.1.4.9" class="ltx_td ltx_align_center">0.1909</td>
<td id="S6.T6.1.1.4.10" class="ltx_td ltx_align_center">0.3222</td>
<td id="S6.T6.1.1.4.11" class="ltx_td ltx_align_center">0.1329</td>
</tr>
<tr id="S6.T6.1.1.5" class="ltx_tr">
<td id="S6.T6.1.1.5.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T6.1.1.5.2" class="ltx_td ltx_align_center">0.3815</td>
<td id="S6.T6.1.1.5.3" class="ltx_td ltx_align_center">0.2044</td>
<td id="S6.T6.1.1.5.4" class="ltx_td ltx_align_center">0.4470</td>
<td id="S6.T6.1.1.5.5" class="ltx_td ltx_align_center">0.2832</td>
<td id="S6.T6.1.1.5.6" class="ltx_td ltx_align_center">0.3475</td>
<td id="S6.T6.1.1.5.7" class="ltx_td ltx_align_center">0.1628</td>
<td id="S6.T6.1.1.5.8" class="ltx_td ltx_align_center">0.3248</td>
<td id="S6.T6.1.1.5.9" class="ltx_td ltx_align_center">0.1549</td>
<td id="S6.T6.1.1.5.10" class="ltx_td ltx_align_center">0.2982</td>
<td id="S6.T6.1.1.5.11" class="ltx_td ltx_align_center">0.1093</td>
</tr>
<tr id="S6.T6.1.1.6" class="ltx_tr">
<td id="S6.T6.1.1.6.1" class="ltx_td ltx_align_center">XL</td>
<td id="S6.T6.1.1.6.2" class="ltx_td ltx_align_center">0.3681</td>
<td id="S6.T6.1.1.6.3" class="ltx_td ltx_align_center">0.2019</td>
<td id="S6.T6.1.1.6.4" class="ltx_td ltx_align_center">0.4368</td>
<td id="S6.T6.1.1.6.5" class="ltx_td ltx_align_center">0.2838</td>
<td id="S6.T6.1.1.6.6" class="ltx_td ltx_align_center">0.3059</td>
<td id="S6.T6.1.1.6.7" class="ltx_td ltx_align_center">0.1518</td>
<td id="S6.T6.1.1.6.8" class="ltx_td ltx_align_center">0.3137</td>
<td id="S6.T6.1.1.6.9" class="ltx_td ltx_align_center">0.1253</td>
<td id="S6.T6.1.1.6.10" class="ltx_td ltx_align_center">0.3030</td>
<td id="S6.T6.1.1.6.11" class="ltx_td ltx_align_center">0.1288</td>
</tr>
<tr id="S6.T6.1.1.7" class="ltx_tr">
<td id="S6.T6.1.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="4"><span id="S6.T6.1.1.7.1.1" class="ltx_text">
<span id="S6.T6.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-12.74pt,-12.74pt) rotate(-90deg) ;">
<span id="S6.T6.1.1.7.1.1.1.1" class="ltx_p"><span id="S6.T6.1.1.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer</span></span>
</span></span></span></td>
<td id="S6.T6.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T6.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t">0.3000</td>
<td id="S6.T6.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t">0.1308</td>
<td id="S6.T6.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t">0.2429</td>
<td id="S6.T6.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t">0.0975</td>
<td id="S6.T6.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t">0.2999</td>
<td id="S6.T6.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t">0.1209</td>
<td id="S6.T6.1.1.7.9" class="ltx_td ltx_align_center ltx_border_t">0.3004</td>
<td id="S6.T6.1.1.7.10" class="ltx_td ltx_align_center ltx_border_t">0.1282</td>
<td id="S6.T6.1.1.7.11" class="ltx_td ltx_align_center ltx_border_t">0.2498</td>
<td id="S6.T6.1.1.7.12" class="ltx_td ltx_align_center ltx_border_t">0.0912</td>
</tr>
<tr id="S6.T6.1.1.8" class="ltx_tr">
<td id="S6.T6.1.1.8.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T6.1.1.8.2" class="ltx_td ltx_align_center">0.4751</td>
<td id="S6.T6.1.1.8.3" class="ltx_td ltx_align_center">0.2912</td>
<td id="S6.T6.1.1.8.4" class="ltx_td ltx_align_center">0.3478</td>
<td id="S6.T6.1.1.8.5" class="ltx_td ltx_align_center">0.1994</td>
<td id="S6.T6.1.1.8.6" class="ltx_td ltx_align_center">0.4603</td>
<td id="S6.T6.1.1.8.7" class="ltx_td ltx_align_center">0.2530</td>
<td id="S6.T6.1.1.8.8" class="ltx_td ltx_align_center">0.4502</td>
<td id="S6.T6.1.1.8.9" class="ltx_td ltx_align_center">0.2553</td>
<td id="S6.T6.1.1.8.10" class="ltx_td ltx_align_center">0.4013</td>
<td id="S6.T6.1.1.8.11" class="ltx_td ltx_align_center">0.1737</td>
</tr>
<tr id="S6.T6.1.1.9" class="ltx_tr">
<td id="S6.T6.1.1.9.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T6.1.1.9.2" class="ltx_td ltx_align_center">0.4742</td>
<td id="S6.T6.1.1.9.3" class="ltx_td ltx_align_center">0.2948</td>
<td id="S6.T6.1.1.9.4" class="ltx_td ltx_align_center">0.5063</td>
<td id="S6.T6.1.1.9.5" class="ltx_td ltx_align_center">0.3522</td>
<td id="S6.T6.1.1.9.6" class="ltx_td ltx_align_center">0.4473</td>
<td id="S6.T6.1.1.9.7" class="ltx_td ltx_align_center">0.2237</td>
<td id="S6.T6.1.1.9.8" class="ltx_td ltx_align_center">0.3837</td>
<td id="S6.T6.1.1.9.9" class="ltx_td ltx_align_center">0.1897</td>
<td id="S6.T6.1.1.9.10" class="ltx_td ltx_align_center">0.3642</td>
<td id="S6.T6.1.1.9.11" class="ltx_td ltx_align_center">0.1517</td>
</tr>
<tr id="S6.T6.1.1.10" class="ltx_tr">
<td id="S6.T6.1.1.10.1" class="ltx_td ltx_align_center ltx_border_b">XL</td>
<td id="S6.T6.1.1.10.2" class="ltx_td ltx_align_center ltx_border_b">0.4456</td>
<td id="S6.T6.1.1.10.3" class="ltx_td ltx_align_center ltx_border_b">0.1580</td>
<td id="S6.T6.1.1.10.4" class="ltx_td ltx_align_center ltx_border_b">0.5709</td>
<td id="S6.T6.1.1.10.5" class="ltx_td ltx_align_center ltx_border_b">0.4029</td>
<td id="S6.T6.1.1.10.6" class="ltx_td ltx_align_center ltx_border_b">0.3882</td>
<td id="S6.T6.1.1.10.7" class="ltx_td ltx_align_center ltx_border_b">0.1049</td>
<td id="S6.T6.1.1.10.8" class="ltx_td ltx_align_center ltx_border_b">0.3798</td>
<td id="S6.T6.1.1.10.9" class="ltx_td ltx_align_center ltx_border_b">0.1175</td>
<td id="S6.T6.1.1.10.10" class="ltx_td ltx_align_center ltx_border_b">0.4219</td>
<td id="S6.T6.1.1.10.11" class="ltx_td ltx_align_center ltx_border_b">0.1392</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of the top-5 methods in English part on the private test set. S stands for short, M stands for medium, L stands for long, and XL stands for very long.</figcaption>
</figure>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">From Table <a href="#S6.T6" title="Table 6 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we have the top-5 models share the same characteristic on the English part of the UIT-EVJVQA dataset. Particularly, when we observe the results based on the question length, we can see that all submitted models have the same behavior when they give a higher performance on short questions and medium questions, while they yield a few drawbacks on long questions and very long questions. Turn the attention to the last four rows, which are the results based on answer length. We have different behavior. The top-5 models give better results on medium and long answers while yielding worse results on short and very long answers. Come from Figure <a href="#S4.F5" title="Figure 5 â€£ 4.4 Statistics â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we can see that most English answers are short answers, which means models have more short answers to learning hence logically, they should have better performance on short answers rather than medium answers and long answers. To answer this weird insight, we showed all answers given by the top-5 models, and we found out that all top-5 models tend to give medium answers, even for questions having short answers. The most exciting thing here is how models give medium or lengthy answers: they repeat some tokens from the questions, and this is the primary way our annotators give medium and lengthy answers to questions. For questions with medium or long answers, answers from top-5 models mostly repeat some tokens from questions, and the gold medium or gold answers also repeat some tokens from questions. Thanks to these matched tokens with questions, F1 scores, and avg. BLEU scores of the answers from top-5 models are pretty high, while the crucial tokens, which determine whether or not the information in these answers is correct or not, usually have a length of 2 or 3 tokens; in case of totally wrong, they still do not affect the overall scores significantly. To gain a better understanding, we show some samples in the Appendix for an intuitive explanation.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<div id="S6.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:174.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.0pt,2.8pt) scale(0.968680741184076,0.968680741184076) ;">
<table id="S6.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T7.1.1.1" class="ltx_tr">
<td id="S6.T7.1.1.1.1" class="ltx_td ltx_border_t" colspan="2" rowspan="2"></td>
<td id="S6.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T7.1.1.1.2.1" class="ltx_text ltx_font_bold">Top 1</span></td>
<td id="S6.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">Top 2</span></td>
<td id="S6.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T7.1.1.1.4.1" class="ltx_text ltx_font_bold">Top 3</span></td>
<td id="S6.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T7.1.1.1.5.1" class="ltx_text ltx_font_bold">Top 4</span></td>
<td id="S6.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T7.1.1.1.6.1" class="ltx_text ltx_font_bold">Top 5</span></td>
</tr>
<tr id="S6.T7.1.1.2" class="ltx_tr">
<td id="S6.T7.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.1.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T7.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.2.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T7.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.3.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T7.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.4.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T7.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.5.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T7.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.6.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T7.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.7.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T7.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.8.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T7.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.9.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T7.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T7.1.1.2.10.1" class="ltx_text ltx_font_bold">BLEU</span></td>
</tr>
<tr id="S6.T7.1.1.3" class="ltx_tr">
<td id="S6.T7.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S6.T7.1.1.3.1.1" class="ltx_text">
<span id="S6.T7.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:38.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.9pt;transform:translate(-15.08pt,-14.11pt) rotate(-90deg) ;">
<span id="S6.T7.1.1.3.1.1.1.1" class="ltx_p"><span id="S6.T7.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span></span></span></td>
<td id="S6.T7.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T7.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">0.4143</td>
<td id="S6.T7.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">0.2088</td>
<td id="S6.T7.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.4370</td>
<td id="S6.T7.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">0.2412</td>
<td id="S6.T7.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">0.4582</td>
<td id="S6.T7.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">0.2482</td>
<td id="S6.T7.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">0.4894</td>
<td id="S6.T7.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">0.2768</td>
<td id="S6.T7.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">0.4116</td>
<td id="S6.T7.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">0.1909</td>
</tr>
<tr id="S6.T7.1.1.4" class="ltx_tr">
<td id="S6.T7.1.1.4.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T7.1.1.4.2" class="ltx_td ltx_align_center">0.4997</td>
<td id="S6.T7.1.1.4.3" class="ltx_td ltx_align_center">0.3259</td>
<td id="S6.T7.1.1.4.4" class="ltx_td ltx_align_center">0.4976</td>
<td id="S6.T7.1.1.4.5" class="ltx_td ltx_align_center">0.3150</td>
<td id="S6.T7.1.1.4.6" class="ltx_td ltx_align_center">0.5008</td>
<td id="S6.T7.1.1.4.7" class="ltx_td ltx_align_center">0.3111</td>
<td id="S6.T7.1.1.4.8" class="ltx_td ltx_align_center">0.4879</td>
<td id="S6.T7.1.1.4.9" class="ltx_td ltx_align_center">0.3016</td>
<td id="S6.T7.1.1.4.10" class="ltx_td ltx_align_center">0.4347</td>
<td id="S6.T7.1.1.4.11" class="ltx_td ltx_align_center">0.2271</td>
</tr>
<tr id="S6.T7.1.1.5" class="ltx_tr">
<td id="S6.T7.1.1.5.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T7.1.1.5.2" class="ltx_td ltx_align_center">0.4797</td>
<td id="S6.T7.1.1.5.3" class="ltx_td ltx_align_center">0.2954</td>
<td id="S6.T7.1.1.5.4" class="ltx_td ltx_align_center">0.4582</td>
<td id="S6.T7.1.1.5.5" class="ltx_td ltx_align_center">0.2582</td>
<td id="S6.T7.1.1.5.6" class="ltx_td ltx_align_center">0.4600</td>
<td id="S6.T7.1.1.5.7" class="ltx_td ltx_align_center">0.2687</td>
<td id="S6.T7.1.1.5.8" class="ltx_td ltx_align_center">0.4423</td>
<td id="S6.T7.1.1.5.9" class="ltx_td ltx_align_center">0.2545</td>
<td id="S6.T7.1.1.5.10" class="ltx_td ltx_align_center">0.4014</td>
<td id="S6.T7.1.1.5.11" class="ltx_td ltx_align_center">0.1835</td>
</tr>
<tr id="S6.T7.1.1.6" class="ltx_tr">
<td id="S6.T7.1.1.6.1" class="ltx_td ltx_align_center">XL</td>
<td id="S6.T7.1.1.6.2" class="ltx_td ltx_align_center">0.4197</td>
<td id="S6.T7.1.1.6.3" class="ltx_td ltx_align_center">0.2446</td>
<td id="S6.T7.1.1.6.4" class="ltx_td ltx_align_center">0.4349</td>
<td id="S6.T7.1.1.6.5" class="ltx_td ltx_align_center">0.2083</td>
<td id="S6.T7.1.1.6.6" class="ltx_td ltx_align_center">0.4430</td>
<td id="S6.T7.1.1.6.7" class="ltx_td ltx_align_center">0.2359</td>
<td id="S6.T7.1.1.6.8" class="ltx_td ltx_align_center">0.4121</td>
<td id="S6.T7.1.1.6.9" class="ltx_td ltx_align_center">0.2234</td>
<td id="S6.T7.1.1.6.10" class="ltx_td ltx_align_center">0.3154</td>
<td id="S6.T7.1.1.6.11" class="ltx_td ltx_align_center">0.1447</td>
</tr>
<tr id="S6.T7.1.1.7" class="ltx_tr">
<td id="S6.T7.1.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="4"><span id="S6.T7.1.1.7.1.1" class="ltx_text">
<span id="S6.T7.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-12.74pt,-12.74pt) rotate(-90deg) ;">
<span id="S6.T7.1.1.7.1.1.1.1" class="ltx_p"><span id="S6.T7.1.1.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer</span></span>
</span></span></span></td>
<td id="S6.T7.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T7.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t">0.3621</td>
<td id="S6.T7.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t">0.1659</td>
<td id="S6.T7.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t">0.3719</td>
<td id="S6.T7.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t">0.1700</td>
<td id="S6.T7.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t">0.3692</td>
<td id="S6.T7.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t">0.1666</td>
<td id="S6.T7.1.1.7.9" class="ltx_td ltx_align_center ltx_border_t">0.3576</td>
<td id="S6.T7.1.1.7.10" class="ltx_td ltx_align_center ltx_border_t">0.1656</td>
<td id="S6.T7.1.1.7.11" class="ltx_td ltx_align_center ltx_border_t">0.3493</td>
<td id="S6.T7.1.1.7.12" class="ltx_td ltx_align_center ltx_border_t">0.1481</td>
</tr>
<tr id="S6.T7.1.1.8" class="ltx_tr">
<td id="S6.T7.1.1.8.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T7.1.1.8.2" class="ltx_td ltx_align_center">0.5564</td>
<td id="S6.T7.1.1.8.3" class="ltx_td ltx_align_center">0.3874</td>
<td id="S6.T7.1.1.8.4" class="ltx_td ltx_align_center">0.5507</td>
<td id="S6.T7.1.1.8.5" class="ltx_td ltx_align_center">0.3743</td>
<td id="S6.T7.1.1.8.6" class="ltx_td ltx_align_center">0.5467</td>
<td id="S6.T7.1.1.8.7" class="ltx_td ltx_align_center">0.3626</td>
<td id="S6.T7.1.1.8.8" class="ltx_td ltx_align_center">0.5433</td>
<td id="S6.T7.1.1.8.9" class="ltx_td ltx_align_center">0.3609</td>
<td id="S6.T7.1.1.8.10" class="ltx_td ltx_align_center">0.4705</td>
<td id="S6.T7.1.1.8.11" class="ltx_td ltx_align_center">0.2604</td>
</tr>
<tr id="S6.T7.1.1.9" class="ltx_tr">
<td id="S6.T7.1.1.9.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T7.1.1.9.2" class="ltx_td ltx_align_center">0.5326</td>
<td id="S6.T7.1.1.9.3" class="ltx_td ltx_align_center">0.3623</td>
<td id="S6.T7.1.1.9.4" class="ltx_td ltx_align_center">0.4859</td>
<td id="S6.T7.1.1.9.5" class="ltx_td ltx_align_center">0.2849</td>
<td id="S6.T7.1.1.9.6" class="ltx_td ltx_align_center">0.5141</td>
<td id="S6.T7.1.1.9.7" class="ltx_td ltx_align_center">0.3244</td>
<td id="S6.T7.1.1.9.8" class="ltx_td ltx_align_center">0.4802</td>
<td id="S6.T7.1.1.9.9" class="ltx_td ltx_align_center">0.2851</td>
<td id="S6.T7.1.1.9.10" class="ltx_td ltx_align_center">0.4007</td>
<td id="S6.T7.1.1.9.11" class="ltx_td ltx_align_center">0.1760</td>
</tr>
<tr id="S6.T7.1.1.10" class="ltx_tr">
<td id="S6.T7.1.1.10.1" class="ltx_td ltx_align_center ltx_border_b">XL</td>
<td id="S6.T7.1.1.10.2" class="ltx_td ltx_align_center ltx_border_b">0.4898</td>
<td id="S6.T7.1.1.10.3" class="ltx_td ltx_align_center ltx_border_b">0.3172</td>
<td id="S6.T7.1.1.10.4" class="ltx_td ltx_align_center ltx_border_b">0.4627</td>
<td id="S6.T7.1.1.10.5" class="ltx_td ltx_align_center ltx_border_b">0.1959</td>
<td id="S6.T7.1.1.10.6" class="ltx_td ltx_align_center ltx_border_b">0.5328</td>
<td id="S6.T7.1.1.10.7" class="ltx_td ltx_align_center ltx_border_b">0.3253</td>
<td id="S6.T7.1.1.10.8" class="ltx_td ltx_align_center ltx_border_b">0.4142</td>
<td id="S6.T7.1.1.10.9" class="ltx_td ltx_align_center ltx_border_b">0.2243</td>
<td id="S6.T7.1.1.10.10" class="ltx_td ltx_align_center ltx_border_b">0.3430</td>
<td id="S6.T7.1.1.10.11" class="ltx_td ltx_align_center ltx_border_b">0.1278</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results of the top-5 methods in Vietnamese part on the private test set. S stands for short, M stands for medium, L stands for long, and XL stands for very long.</figcaption>
</figure>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Coming into the Vietnamese part of the UIT-EVJVQA dataset results, we have quite different behavior where most models perform better on medium and long questions. When we observe the length distribution of questions in Figure <a href="#S4.F4" title="Figure 4 â€£ 4.4 Statistics â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, most questions have a length range of around eight tokens, or most have medium length. While in English, most questions fell in the range of 5 tokens and seven tokens. Therefore the top-5 models have a good performance on short and medium questions in English, while they achieved better performance on medium and long answers in Vietnamese. For Vietnamese answers, as indicated in Table <a href="#S6.T7" title="Table 7 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, top-5 models share the same performance as the English answers: they perform better on medium and long answers than on short and very long answers. We also provided some samples in Appendix to better demonstrate the method with English answers.</p>
</div>
<figure id="S6.T8" class="ltx_table">
<div id="S6.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:174.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.0pt,2.8pt) scale(0.968680741184076,0.968680741184076) ;">
<table id="S6.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T8.1.1.1" class="ltx_tr">
<td id="S6.T8.1.1.1.1" class="ltx_td ltx_border_t" colspan="2" rowspan="2"></td>
<td id="S6.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Top 1</span></td>
<td id="S6.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">Top 2</span></td>
<td id="S6.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">Top 3</span></td>
<td id="S6.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">Top 4</span></td>
<td id="S6.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S6.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">Top 5</span></td>
</tr>
<tr id="S6.T8.1.1.2" class="ltx_tr">
<td id="S6.T8.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.1.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T8.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.2.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.3.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.4.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T8.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.5.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T8.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.6.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T8.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.7.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T8.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.8.1" class="ltx_text ltx_font_bold">BLEU</span></td>
<td id="S6.T8.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.9.1" class="ltx_text ltx_font_bold">F1</span></td>
<td id="S6.T8.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.1.1.2.10.1" class="ltx_text ltx_font_bold">BLEU</span></td>
</tr>
<tr id="S6.T8.1.1.3" class="ltx_tr">
<td id="S6.T8.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S6.T8.1.1.3.1.1" class="ltx_text">
<span id="S6.T8.1.1.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:38.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.9pt;transform:translate(-15.08pt,-14.11pt) rotate(-90deg) ;">
<span id="S6.T8.1.1.3.1.1.1.1" class="ltx_p"><span id="S6.T8.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span></span></span></td>
<td id="S6.T8.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T8.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">0.7500</td>
<td id="S6.T8.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">0.3125</td>
<td id="S6.T8.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.4060</td>
<td id="S6.T8.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">0.1917</td>
<td id="S6.T8.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">0.6667</td>
<td id="S6.T8.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">0.1850</td>
<td id="S6.T8.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">0.4444</td>
<td id="S6.T8.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">0.1250</td>
<td id="S6.T8.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">0.3333</td>
<td id="S6.T8.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">0.0460</td>
</tr>
<tr id="S6.T8.1.1.4" class="ltx_tr">
<td id="S6.T8.1.1.4.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T8.1.1.4.2" class="ltx_td ltx_align_center">0.4494</td>
<td id="S6.T8.1.1.4.3" class="ltx_td ltx_align_center">0.2004</td>
<td id="S6.T8.1.1.4.4" class="ltx_td ltx_align_center">0.3905</td>
<td id="S6.T8.1.1.4.5" class="ltx_td ltx_align_center">0.2164</td>
<td id="S6.T8.1.1.4.6" class="ltx_td ltx_align_center">0.4634</td>
<td id="S6.T8.1.1.4.7" class="ltx_td ltx_align_center">0.2984</td>
<td id="S6.T8.1.1.4.8" class="ltx_td ltx_align_center">0.4841</td>
<td id="S6.T8.1.1.4.9" class="ltx_td ltx_align_center">0.3118</td>
<td id="S6.T8.1.1.4.10" class="ltx_td ltx_align_center">0.4485</td>
<td id="S6.T8.1.1.4.11" class="ltx_td ltx_align_center">0.2447</td>
</tr>
<tr id="S6.T8.1.1.5" class="ltx_tr">
<td id="S6.T8.1.1.5.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T8.1.1.5.2" class="ltx_td ltx_align_center">0.4489</td>
<td id="S6.T8.1.1.5.3" class="ltx_td ltx_align_center">0.2841</td>
<td id="S6.T8.1.1.5.4" class="ltx_td ltx_align_center">0.3679</td>
<td id="S6.T8.1.1.5.5" class="ltx_td ltx_align_center">0.1974</td>
<td id="S6.T8.1.1.5.6" class="ltx_td ltx_align_center">0.4395</td>
<td id="S6.T8.1.1.5.7" class="ltx_td ltx_align_center">0.2720</td>
<td id="S6.T8.1.1.5.8" class="ltx_td ltx_align_center">0.4150</td>
<td id="S6.T8.1.1.5.9" class="ltx_td ltx_align_center">0.2741</td>
<td id="S6.T8.1.1.5.10" class="ltx_td ltx_align_center">0.4058</td>
<td id="S6.T8.1.1.5.11" class="ltx_td ltx_align_center">0.2333</td>
</tr>
<tr id="S6.T8.1.1.6" class="ltx_tr">
<td id="S6.T8.1.1.6.1" class="ltx_td ltx_align_center">XL</td>
<td id="S6.T8.1.1.6.2" class="ltx_td ltx_align_center">0.4399</td>
<td id="S6.T8.1.1.6.3" class="ltx_td ltx_align_center">0.2915</td>
<td id="S6.T8.1.1.6.4" class="ltx_td ltx_align_center">0.3456</td>
<td id="S6.T8.1.1.6.5" class="ltx_td ltx_align_center">0.1863</td>
<td id="S6.T8.1.1.6.6" class="ltx_td ltx_align_center">0.3896</td>
<td id="S6.T8.1.1.6.7" class="ltx_td ltx_align_center">0.2333</td>
<td id="S6.T8.1.1.6.8" class="ltx_td ltx_align_center">0.3834</td>
<td id="S6.T8.1.1.6.9" class="ltx_td ltx_align_center">0.2453</td>
<td id="S6.T8.1.1.6.10" class="ltx_td ltx_align_center">0.3452</td>
<td id="S6.T8.1.1.6.11" class="ltx_td ltx_align_center">0.1815</td>
</tr>
<tr id="S6.T8.1.1.7" class="ltx_tr">
<td id="S6.T8.1.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="4"><span id="S6.T8.1.1.7.1.1" class="ltx_text">
<span id="S6.T8.1.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:32.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.3pt;transform:translate(-12.74pt,-12.74pt) rotate(-90deg) ;">
<span id="S6.T8.1.1.7.1.1.1.1" class="ltx_p"><span id="S6.T8.1.1.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer</span></span>
</span></span></span></td>
<td id="S6.T8.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S6.T8.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t">0.1983</td>
<td id="S6.T8.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t">0.075</td>
<td id="S6.T8.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t">0.3101</td>
<td id="S6.T8.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t">0.1336</td>
<td id="S6.T8.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t">0.2057</td>
<td id="S6.T8.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t">0.0713</td>
<td id="S6.T8.1.1.7.9" class="ltx_td ltx_align_center ltx_border_t">0.1809</td>
<td id="S6.T8.1.1.7.10" class="ltx_td ltx_align_center ltx_border_t">0.0714</td>
<td id="S6.T8.1.1.7.11" class="ltx_td ltx_align_center ltx_border_t">0.1983</td>
<td id="S6.T8.1.1.7.12" class="ltx_td ltx_align_center ltx_border_t">0.0750</td>
</tr>
<tr id="S6.T8.1.1.8" class="ltx_tr">
<td id="S6.T8.1.1.8.1" class="ltx_td ltx_align_center">M</td>
<td id="S6.T8.1.1.8.2" class="ltx_td ltx_align_center">0.3503</td>
<td id="S6.T8.1.1.8.3" class="ltx_td ltx_align_center">0.1942</td>
<td id="S6.T8.1.1.8.4" class="ltx_td ltx_align_center">0.4654</td>
<td id="S6.T8.1.1.8.5" class="ltx_td ltx_align_center">0.2822</td>
<td id="S6.T8.1.1.8.6" class="ltx_td ltx_align_center">0.3236</td>
<td id="S6.T8.1.1.8.7" class="ltx_td ltx_align_center">0.1750</td>
<td id="S6.T8.1.1.8.8" class="ltx_td ltx_align_center">0.3443</td>
<td id="S6.T8.1.1.8.9" class="ltx_td ltx_align_center">0.2095</td>
<td id="S6.T8.1.1.8.10" class="ltx_td ltx_align_center">0.3503</td>
<td id="S6.T8.1.1.8.11" class="ltx_td ltx_align_center">0.1942</td>
</tr>
<tr id="S6.T8.1.1.9" class="ltx_tr">
<td id="S6.T8.1.1.9.1" class="ltx_td ltx_align_center">L</td>
<td id="S6.T8.1.1.9.2" class="ltx_td ltx_align_center">0.5242</td>
<td id="S6.T8.1.1.9.3" class="ltx_td ltx_align_center">0.3667</td>
<td id="S6.T8.1.1.9.4" class="ltx_td ltx_align_center">0.4436</td>
<td id="S6.T8.1.1.9.5" class="ltx_td ltx_align_center">0.2731</td>
<td id="S6.T8.1.1.9.6" class="ltx_td ltx_align_center">0.4734</td>
<td id="S6.T8.1.1.9.7" class="ltx_td ltx_align_center">0.3156</td>
<td id="S6.T8.1.1.9.8" class="ltx_td ltx_align_center">0.4609</td>
<td id="S6.T8.1.1.9.9" class="ltx_td ltx_align_center">0.3230</td>
<td id="S6.T8.1.1.9.10" class="ltx_td ltx_align_center">0.5242</td>
<td id="S6.T8.1.1.9.11" class="ltx_td ltx_align_center">0.3667</td>
</tr>
<tr id="S6.T8.1.1.10" class="ltx_tr">
<td id="S6.T8.1.1.10.1" class="ltx_td ltx_align_center ltx_border_b">XL</td>
<td id="S6.T8.1.1.10.2" class="ltx_td ltx_align_center ltx_border_b">0.5911</td>
<td id="S6.T8.1.1.10.3" class="ltx_td ltx_align_center ltx_border_b">0.4245</td>
<td id="S6.T8.1.1.10.4" class="ltx_td ltx_align_center ltx_border_b">0.3684</td>
<td id="S6.T8.1.1.10.5" class="ltx_td ltx_align_center ltx_border_b">0.1586</td>
<td id="S6.T8.1.1.10.6" class="ltx_td ltx_align_center ltx_border_b">0.5131</td>
<td id="S6.T8.1.1.10.7" class="ltx_td ltx_align_center ltx_border_b">0.3321</td>
<td id="S6.T8.1.1.10.8" class="ltx_td ltx_align_center ltx_border_b">0.4983</td>
<td id="S6.T8.1.1.10.9" class="ltx_td ltx_align_center ltx_border_b">0.3358</td>
<td id="S6.T8.1.1.10.10" class="ltx_td ltx_align_center ltx_border_b">0.5911</td>
<td id="S6.T8.1.1.10.11" class="ltx_td ltx_align_center ltx_border_b">0.4245</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Results of the top-5 methods in Japanese partition on the private test set in. S stands for short, M stands for medium, L stands for long, and XL stands for very long.</figcaption>
</figure>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">On the Japanese part of the UIT-EVJVQA dataset, the top-5 models have the same color on the English part when they give better results on medium and long questions. But unlike their performance in Vietnamese and English, where they achieved better scores for medium and long answers, the top-5 models have results as increase as the length of answers. From Figure <a href="#S4.F6" title="Figure 6 â€£ 4.4 Statistics â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, although the occurrence of short answers is the highest, the cumulative of medium and long answers are higher than the cumulative of short answers, which indicates the top-5 models not only have more medium and lengthy answers to learn but also tend to give medium or long answers to have the optimal loss on the training set. We also provided some samples in the Appendix for better demonstration.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Apart from the previous discussion, we can conclude on the UIT-EVJVQA dataset, most deep learning models tend to give lengthy answers to given questions with images, and the way they give lengthy answers is by repeatedly using some tokens of questions as starting point of the answers, and the main wrong parts of these answers are at the tokens indicating vital information to answer the questions such as objects, colors or side (see Appendix for more examples). Hence, to better understand how wrong the top-5 model gives predictions, we conduct analyses focusing on the usage of side, object, and color words in each language.</p>
</div>
<figure id="S6.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/vn_team_color_v.png" id="S6.F8.sf1.g1" class="ltx_graphics ltx_img_portrait" width="568" height="926" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Vietnamese</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/en_team_color_v.png" id="S6.F8.sf2.g1" class="ltx_graphics ltx_img_portrait" width="598" height="925" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>English</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/Image/jp_team_color_v.png" id="S6.F8.sf3.g1" class="ltx_graphics ltx_img_portrait" width="598" height="926" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Japanese</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Color words distribution in ground truth and predictions of each team in three languages.</figcaption>
</figure>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p"><span id="S6.p6.1.1" class="ltx_text ltx_font_bold">Side words</span>: One of the most confusing attributes while inferring the description of objects is the use of side or direction. This is the case where an object is observed on the left or right of another object or appears on the left or right side of the scene. In English, side words are the words â€leftâ€ and â€rightâ€ in our dataset. While in Japanese, they are â€å·¦â€ and â€å³â€. However, they are not simply â€trÃ¡iâ€ and â€pháº£iâ€ in Vietnamese, as â€trÃ¡iâ€ may not often be used to convey a side. Therefore, we broaden the set of side words
as â€bÃªn trÃ¡iâ€, â€bÃªn pháº£iâ€, â€tay trÃ¡iâ€ (left-hand side) and â€tay pháº£iâ€ (right-hand side) according to the observation of the dataset. To measure how well each submitted model uses those side words in each language, we adopt the F1-score to calculate the proportion of match in terms of side words between the predictions and the gold answers. According to the results in Table <a href="#S6.T9" title="Table 9 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, most models failed to indicate the side of objects while answering the questions.</p>
</div>
<figure id="S6.T9" class="ltx_table">
<table id="S6.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T9.1.1" class="ltx_tr">
<td id="S6.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Team</td>
<td id="S6.T9.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Vietnamese</td>
<td id="S6.T9.1.1.3" class="ltx_td ltx_align_center ltx_border_t">English</td>
<td id="S6.T9.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Japanese</td>
</tr>
<tr id="S6.T9.1.2" class="ltx_tr">
<td id="S6.T9.1.2.1" class="ltx_td ltx_align_left ltx_border_t">CIST AI</td>
<td id="S6.T9.1.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S6.T9.1.2.2.1" class="ltx_text ltx_font_bold">0.4948</span></td>
<td id="S6.T9.1.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S6.T9.1.2.3.1" class="ltx_text ltx_font_bold">0.3889</span></td>
<td id="S6.T9.1.2.4" class="ltx_td ltx_align_left ltx_border_t">0.3922</td>
</tr>
<tr id="S6.T9.1.3" class="ltx_tr">
<td id="S6.T9.1.3.1" class="ltx_td ltx_align_left">OhYeah</td>
<td id="S6.T9.1.3.2" class="ltx_td ltx_align_left">0.3814</td>
<td id="S6.T9.1.3.3" class="ltx_td ltx_align_left">0.3235</td>
<td id="S6.T9.1.3.4" class="ltx_td ltx_align_left"><span id="S6.T9.1.3.4.1" class="ltx_text ltx_font_bold">0.4085</span></td>
</tr>
<tr id="S6.T9.1.4" class="ltx_tr">
<td id="S6.T9.1.4.1" class="ltx_td ltx_align_left">DS-STBFL</td>
<td id="S6.T9.1.4.2" class="ltx_td ltx_align_left">0.4811</td>
<td id="S6.T9.1.4.3" class="ltx_td ltx_align_left">0.3137</td>
<td id="S6.T9.1.4.4" class="ltx_td ltx_align_left">0.3366</td>
</tr>
<tr id="S6.T9.1.5" class="ltx_tr">
<td id="S6.T9.1.5.1" class="ltx_td ltx_align_left">FCoin</td>
<td id="S6.T9.1.5.2" class="ltx_td ltx_align_left">0.4021</td>
<td id="S6.T9.1.5.3" class="ltx_td ltx_align_left">0.3039</td>
<td id="S6.T9.1.5.4" class="ltx_td ltx_align_left">0.3595</td>
</tr>
<tr id="S6.T9.1.6" class="ltx_tr">
<td id="S6.T9.1.6.1" class="ltx_td ltx_align_left ltx_border_b">VL-UIT</td>
<td id="S6.T9.1.6.2" class="ltx_td ltx_align_left ltx_border_b">0.2268</td>
<td id="S6.T9.1.6.3" class="ltx_td ltx_align_left ltx_border_b">0.2418</td>
<td id="S6.T9.1.6.4" class="ltx_td ltx_align_left ltx_border_b">0.1471</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>F1-score for side predictions of every team for each language</figcaption>
</figure>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p"><span id="S6.p7.1.1" class="ltx_text ltx_font_bold">Color words</span>: As an attribute frequently appears in the QAs of our dataset, color is worth being observed as part of the inference performance of the models. Here we measure the F1-score of color word matches between predicted and gold answers for every team. The color words have been defined in the guideline in Section <a href="#S4.SS2.SSS1" title="4.2.1 Base Vietnamese QAs creation â€£ 4.2 Questions and answers creation process â€£ 4 Corpus Creation â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>. However, there are some minor exceptions due to the crowdsourcing process.</p>
</div>
<figure id="S6.T10" class="ltx_table">
<table id="S6.T10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T10.1.1" class="ltx_tr">
<td id="S6.T10.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Team</td>
<td id="S6.T10.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Vietnamese</td>
<td id="S6.T10.1.1.3" class="ltx_td ltx_align_center ltx_border_t">English</td>
<td id="S6.T10.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Japanese</td>
</tr>
<tr id="S6.T10.1.2" class="ltx_tr">
<td id="S6.T10.1.2.1" class="ltx_td ltx_align_left ltx_border_t">CIST AI</td>
<td id="S6.T10.1.2.2" class="ltx_td ltx_align_left ltx_border_t">0.3230</td>
<td id="S6.T10.1.2.3" class="ltx_td ltx_align_left ltx_border_t">0.3434</td>
<td id="S6.T10.1.2.4" class="ltx_td ltx_align_left ltx_border_t">0.3450</td>
</tr>
<tr id="S6.T10.1.3" class="ltx_tr">
<td id="S6.T10.1.3.1" class="ltx_td ltx_align_left">OhYeah</td>
<td id="S6.T10.1.3.2" class="ltx_td ltx_align_left">0.2954</td>
<td id="S6.T10.1.3.3" class="ltx_td ltx_align_left">0.3326</td>
<td id="S6.T10.1.3.4" class="ltx_td ltx_align_left">0.3229</td>
</tr>
<tr id="S6.T10.1.4" class="ltx_tr">
<td id="S6.T10.1.4.1" class="ltx_td ltx_align_left">DS-STBFL</td>
<td id="S6.T10.1.4.2" class="ltx_td ltx_align_left"><span id="S6.T10.1.4.2.1" class="ltx_text ltx_font_bold">0.5161</span></td>
<td id="S6.T10.1.4.3" class="ltx_td ltx_align_left"><span id="S6.T10.1.4.3.1" class="ltx_text ltx_font_bold">0.4933</span></td>
<td id="S6.T10.1.4.4" class="ltx_td ltx_align_left"><span id="S6.T10.1.4.4.1" class="ltx_text ltx_font_bold">0.3934</span></td>
</tr>
<tr id="S6.T10.1.5" class="ltx_tr">
<td id="S6.T10.1.5.1" class="ltx_td ltx_align_left">FCoin</td>
<td id="S6.T10.1.5.2" class="ltx_td ltx_align_left">0.3619</td>
<td id="S6.T10.1.5.3" class="ltx_td ltx_align_left">0.3142</td>
<td id="S6.T10.1.5.4" class="ltx_td ltx_align_left">0.3234</td>
</tr>
<tr id="S6.T10.1.6" class="ltx_tr">
<td id="S6.T10.1.6.1" class="ltx_td ltx_align_left ltx_border_b">VL-UIT</td>
<td id="S6.T10.1.6.2" class="ltx_td ltx_align_left ltx_border_b">0.2716</td>
<td id="S6.T10.1.6.3" class="ltx_td ltx_align_left ltx_border_b">0.2420</td>
<td id="S6.T10.1.6.4" class="ltx_td ltx_align_left ltx_border_b">0.2355</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>F1-score for color predictions of every team for each language</figcaption>
</figure>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2302.11752/assets/x4.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Some examples for the performance of pre-trained image models. From top to bottom: origin images, object detection results of Faster-RCNN with Resnet101 <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite> as backbone, and object detection results of Cascade RCNN <cite class="ltx_cite ltx_citemacro_cite">Cai and Vasconcelos (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> with Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> as the backbone (Zooming out x4 times for better illustration as well as clearly observing the detected labels and their confident scores of each model).</figcaption>
</figure>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">We also visualize the color word distribution in the prediction of every team for each language. It is worth noting that in the Vietnamese QAs, there is a substantial quantity of â€xanhâ€ color words which are unclear to be whether green or blue, but rather depend on the context of the visual scene. As we can see in Figure <a href="#S6.F8.sf1" title="In Figure 8 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>, Figure <a href="#S6.F8.sf2" title="In Figure 8 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a> and Figure <a href="#S6.F8.sf3" title="In Figure 8 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(c)</span></a>, the ground truth color words are not evenly distributed because some colors such as white, black and red are used more frequently in the dataset, while the instances of brown, gray and purple are scant. This skewness is then emphasized through the overall inference of models. In Japanese, most submitted models clearly express bias as they intensively describe objects in white color, while less-appeared colors are poorly used. This kind of behavior is similar to the prior-language phenomenon pointed out in the previous work <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> as when being given questions about colors, top-5 models tend to use most-appearance colors despite the colors in images, and this phenomenon is also noticeable in other languages for some teams. For instance, the submitted model of team VL-UIT even ignores many of the less-appeared colors. As an attribute that is regularly used to describe an object, color also plays an important role in distinguishing between objects, which is an essential factor in visual question answering. Therefore, more efforts must be conducted to degrade the skewness in the color distribution in a way that helps the model infer better and more precisely describe objects out of images. We suggest future works should define a fixed range of colors and pay attention while asking and using colors to answer in order that we can ignore such prior-language phenomena on the VQA dataset.</p>
</div>
<div id="S6.p9" class="ltx_para">
<p id="S6.p9.1" class="ltx_p"><span id="S6.p9.1.1" class="ltx_text ltx_font_bold">Objects</span>: To get the distribution of objects in the UIT-EVJVQA dataset, we used the POS method from the previous work <cite class="ltx_cite ltx_citemacro_cite">Vu etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite> and achieved objects by collecting tokens tagged as nouns. We used the same manner as investigating the behavior of top-5 models on color words but we can not find such language-prior phenomenon on objects. To find out the reason why most of the top-5 models failed in recognizing objects in images, we observed the results of pre-trained image models. However, most of the top-5 modelâ€™s used pre-trained image models that output grid-based features <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> such as ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> or BEiT <cite class="ltx_cite ltx_citemacro_cite">Bao etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, which means it is hard to visualize and interpret the results of those image models. Nevertheless, we can interpret results of other similar pre-trained image models such as Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">Ren etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2015b</a>)</cite> used ResNet101 <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2016</a>)</cite> as the backbone or Swin Transformer for object detection <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> as these models were pre-trained on the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">Russakovsky etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2015</a>)</cite> as well before being fine-tuned on various tasks, hence we used the outcomes of these two models on the training set of the UIT-EVJVQA dataset as an approximate way to investigate the reason of failure in recognizing an object in images of the top-5 models. As depicted in Figure <a href="#S6.F9" title="Figure 9 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, these two image models accurately detect objects that they were trained, but lots of common objects in Vietnam such as non la or fan (the first column of three images in Figure <a href="#S6.F9" title="Figure 9 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> or some traditional products available in most of the markets in Vietnam (the last column of three images in Figure <a href="#S6.F9" title="Figure 9 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) are not detected, and some of the detected objects in Figure <a href="#S6.F9" title="Figure 9 â€£ 6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> are not correct. This result
implies the incorrect image understanding of pre-trained images models trained on images captured outside of Vietnam and indirectly affects the performance of VQA models (in case of using grid-based features, as region-based features <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> were achieved from grid-based features, e.g. from the backbone of Faster-RCNN, hence the failures indicated from region-based features indicates as well the failure in grid-based features). In conclusion available pre-trained image models are not relevant to scenes captured in Vietnam and the Computer Vision (CV) community in Vietnam should research and develop a better appropriate pre-trained image model, especially for images taken in Vietnam so that we can effectively tackle recent trending tasks where multi modeling task such as VQA is one of them.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The VLSP2022-EVJVQA Challenge on multilingual image-based question answering has been organized at the VLSP 2022. Even though 57 teams had legally signed up to get the training dataset, only eight teams submitted their results. Because several teams enrolled for many challenges at the VLSP 2022, the other teams may not have enough time to explore VQA models. The highest performances are 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The multilingual VQA systems proposed by the top 2 teams use ViT for the pre-trained vision model and mT5 for the pre-trained language model. EVJVQA is a challenging dataset including the training set, the development set (public test set), and the test set (private test set) that motivates NLP and CV researchers to further explore the multilingual models or systems in visual question answering.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">To increase performance in multilingual visual question answering, we intend to increase the amount and quality of annotated questions in the future. In addition, we also make human-adversarial questions based on findings proposed by the research work <cite class="ltx_cite ltx_citemacro_cite">Sheng etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to thank the efforts of annotators who have contributed to the construction of a high-quality resource for the natural language processing research community. The VLSP 2022 was supported by organizations: Aimsoft, Bee, INT2, and DAGORAS, and educational organizations: VNU-HCM University of Information Technology, VNU University of Science, VNU University of Engineering and Technology, Hanoi University of Science and Technology, Vietnam Lexicography Centre, University of Science and Technology of Hanoi, ThuyLoi University, and VAST Institute of Information Technology. This work is partially supported by Vingroup Innovation Foundation (VINIF) in project code VINIF.2020.DA14.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
CÂ Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425â€“2433.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hangbo Bao, LiÂ Dong, and Furu Wei. 2021.

</span>
<span class="ltx_bibblock">Beit: Bert pre-training of image transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.08254</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai and Vasconcelos (2018)</span>
<span class="ltx_bibblock">
Zhaowei Cai and Nuno Vasconcelos. 2018.

</span>
<span class="ltx_bibblock">Cascade r-cnn: Delving into high quality object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6154â€“6162.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, XiÂ Chen, Nan Ding, and
Radu Soricut. 2022a.

</span>
<span class="ltx_bibblock">All you may need for vqa are image captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01883</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Linting Xue, Idan Szpektor, AshishÂ V Thapliyal, Julien
Amelot, XiÂ Chen, and Radu Soricut. 2022b.

</span>
<span class="ltx_bibblock">Towards multi-lingual visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.05401</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo etÂ al. (2022c)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Linting Xue, Idan Szpektor, AshishÂ V. Thapliyal, Julien
Amelot, XiÂ Chen, and Radu Soricut. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2209.05401" title="" class="ltx_ref ltx_href">Towards
multi-lingual visual question answering</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171â€“4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, etÂ al. 2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2015)</span>
<span class="ltx_bibblock">
Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6904â€“6913.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta (2017)</span>
<span class="ltx_bibblock">
AkshayÂ Kumar Gupta. 2017.

</span>
<span class="ltx_bibblock">Survey of visual question answering: Datasets and techniques.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.03865</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. (2020)</span>
<span class="ltx_bibblock">
Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Pushpak Bhattacharyya. 2020.

</span>
<span class="ltx_bibblock">A unified framework for multilingual and code-mixed visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Conference of the Asia-Pacific
Chapter of the Association for Computational Linguistics and the 10th
International Joint Conference on Natural Language Processing</em>, pages
900â€“913.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari etÂ al. (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, AbigaleÂ J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and JeffreyÂ P Bigham. 2018.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 3608â€“3617.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 770â€“778.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
DrewÂ A Hudson and ChristopherÂ D Manning. 2019.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 6700â€“6709.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
Chen. 2020.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10267â€“10276.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan etÂ al. (2021)</span>
<span class="ltx_bibblock">
HumairÂ Raj Khan, Deepak Gupta, and Asif Ekbal. 2021.

</span>
<span class="ltx_bibblock">Towards developing a multilingual and code-mixed visual question
answering system by knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.04653</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vulic, and Iryna Gurevych. 2022.

</span>
<span class="ltx_bibblock">Delving deeper into cross-lingual visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07630</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
ZeÂ Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo. 2021.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 10012â€“10022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/cvf conference on computer vision
and pattern recognition</em>, pages 3195â€“3204.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nooralahzadeh and Sennrich (2022)</span>
<span class="ltx_bibblock">
Farhad Nooralahzadeh and Rico Sennrich. 2022.

</span>
<span class="ltx_bibblock">Improving the cross-lingual generalisation in visual question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.02982</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>, pages 311â€“318.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-MartinÂ O Steitz, Stefan
Roth, Ivan VuliÄ‡, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock">xgqa: Cross-lingual visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.06082</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi etÂ al. (2022)</span>
<span class="ltx_bibblock">
LeÂ Qi, Shangwen Lv, Hongyu Li, Jing Liu, YuÂ Zhang, Qiaoqiao She, Hua Wu,
Haifeng Wang, and Ting Liu. 2022.

</span>
<span class="ltx_bibblock">Dureadervis: A: A chinese dataset for open-domain document visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 1338â€“1351.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar etÂ al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2383â€“2392.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015a)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2015b)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015b.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky etÂ al. (2015)</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, etÂ al.
2015.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 115(3):211â€“252.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk etÂ al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock">A-okvqa: A benchmark for visual question answering using world
knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.01718</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Magana, Tristan Thrush,
Wojciech Galuba, Devi Parikh, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock">Human-adversarial visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
34:20346â€“20359.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shimizu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Nobuyuki Shimizu, NaÂ Rong, and Takashi Miyazaki. 2018.

</span>
<span class="ltx_bibblock">Visual question answering dataset for bilingual image understanding:
A study of cross-lingual transfer using attention maps.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th International Conference on
Computational Linguistics</em>, pages 1918â€“1928.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, YuÂ Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach. 2019.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 8317â€“8326.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney etÂ al. (2018)</span>
<span class="ltx_bibblock">
Damien Teney, Peter Anderson, Xiaodong He, and Anton Van DenÂ Hengel. 2018.

</span>
<span class="ltx_bibblock">Tips and tricks for visual question answering: Learnings from the
2017 challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4223â€“4232.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran etÂ al. (2021a)</span>
<span class="ltx_bibblock">
KhanhÂ Quoc Tran, AnÂ Trong Nguyen, AnÂ Tran-Hoai Le, and KietÂ Van Nguyen.
2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2021.paclic-1.72" title="" class="ltx_ref ltx_href">ViVQA:
Vietnamese visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th Pacific Asia Conference on Language,
Information and Computation</em>, pages 683â€“691, Shanghai, China. Association
for Computational Lingustics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran etÂ al. (2021b)</span>
<span class="ltx_bibblock">
KhanhÂ Quoc Tran, AnÂ Trong Nguyen, AnÂ Tran-Hoai Le, and Kiet VanÂ Nguyen.
2021b.

</span>
<span class="ltx_bibblock">Vivqa: Vietnamese visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th Pacific Asia Conference on Language,
Information and Computation</em>, pages 546â€“554.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Thanh Vu, DatÂ Quoc Nguyen, DaiÂ Quoc Nguyen, Mark Dras, and Mark Johnson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-5012" title="" class="ltx_ref ltx_href">VnCoreNLP: A
Vietnamese natural language processing toolkit</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Demonstrations</em>,
pages 56â€“60, New Orleans, Louisiana. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and LiÂ Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4995â€“5004.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section we provide extensively samples from top-5 models on the three languages for better demonstrating out observation in Section <a href="#S6" title="6 Result Analysis â€£ VLSP 2022 - EVJVQA Challenge: Multilingual Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="A1.F10" class="ltx_figure"><img src="/html/2302.11752/assets/x5.png" id="A1.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="476" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Results of top-5 models on medium answers in English of the UIT-EVJVQA dataset</figcaption>
</figure>
<figure id="A1.F11" class="ltx_figure"><img src="/html/2302.11752/assets/x6.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="397" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Results of top-5 models on long answers in English of the UIT-EVJVQA dataset</figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2302.11752/assets/x7.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="623" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Results of top-5 models on medium answers in Vietnamese of the UIT-EVJVQA dataset</figcaption>
</figure>
<figure id="A1.F13" class="ltx_figure"><img src="/html/2302.11752/assets/x8.png" id="A1.F13.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="406" height="548" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Results of top-5 models on long answers in Vietnamese of the UIT-EVJVQA dataset</figcaption>
</figure>
<figure id="A1.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/x9.png" id="A1.F14.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="291" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2302.11752/assets/x10.png" id="A1.F14.2.g1" class="ltx_graphics ltx_img_square" width="461" height="450" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Results of top-5 models on long answers and very long answers in Japanese of the UIT-EVJVQA dataset</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.11751" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.11752" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.11752">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.11752" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.11754" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 00:59:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
