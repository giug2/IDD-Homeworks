<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.11053] Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer</title><meta property="og:description" content="Visual question answering (VQA) in surgery is largely unexplored. Expert surgeons are scarce and are often overloaded with clinical and academic workloads. This overload often limits their time answering questionnaires…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.11053">

<!--Generated on Mon Mar 11 17:01:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
Dept. of Biomedical Engineering, National University of Singapore, Singapore. </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Biomedical Image Analysis Group, Imperial College London, UK. </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Dept. of ECE, National Institute of Technology, Tiruchirappalli, India. </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Dept. of Electronic Engineering, Chinese University of Hong Kong. </span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Shun Hing Institute of Advanced Engineering, Chinese University of Hong Kong.
<br class="ltx_break"><span id="id5.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>lalithkumar_s@u.nus.edu, m.islam20@imperial.ac.uk, 108118004@nitt.edu, ren@nus.edu.sg/hlren@ee.cuhk.edu.hk</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lalithkumar Seenivasan
</span><span class="ltx_author_notes">11 ⋆ ⋆
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-0103-1234" title="ORCID identifier" class="ltx_ref">0000-0002-0103-1234</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mobarakol Islam
</span><span class="ltx_author_notes">Lalithkumar Seenivasan and Mobarakol Islam are co-first authors.22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7162-2822" title="ORCID identifier" class="ltx_ref">0000-0002-7162-2822</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adithya K Krishna
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2284-703X" title="ORCID identifier" class="ltx_ref">0000-0002-2284-703X</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongliang Ren
</span><span class="ltx_author_notes">Corresponding author.11 4 4 5 5
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-6488-1551" title="ORCID identifier" class="ltx_ref">0000-0002-6488-1551</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual question answering (VQA) in surgery is largely unexplored. Expert surgeons are scarce and are often overloaded with clinical and academic workloads. This overload often limits their time answering questionnaires from patients, medical students or junior residents related to surgical procedures. At times, students and junior residents also refrain from asking too many questions during classes to reduce disruption. While computer-aided simulators and recording of past surgical procedures have been made available for them to observe and improve their skills, they still hugely rely on medical experts to answer their questions. Having a Surgical-VQA system as a reliable ‘second opinion’ could act as a backup and ease the load on the medical experts in answering these questions. The lack of annotated medical data and the presence of domain-specific terms has limited the exploration of VQA for surgical procedures. In this work, we design a Surgical-VQA task that answers questionnaires on surgical procedures based on the surgical scene. Extending the MICCAI endoscopic vision challenge 2018 dataset and workflow recognition dataset further, we introduce two Surgical-VQA datasets with classification and sentence-based answers. To perform Surgical-VQA, we employ vision-text transformers models. We further introduce a residual MLP-based VisualBert encoder model that enforces interaction between visual and text tokens, improving performance in classification-based answering. Furthermore, we study the influence of the number of input image patches and temporal visual features on the model performance in both classification and sentence-based answering.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Lack of medical domain-specific knowledge has left many patients, medical students and junior residents with questions lingering in their minds about medical diagnosis and surgical procedures. Many of these questions are left unanswered either because they assume these questions to be thoughtless, or students and junior residents refrain from raising too many questions to limit disruptions in lectures. The chances for them finding a medical expert to clarify their doubts are also slim due to the scarce number of medical experts who are often overloaded with clinical and academic works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. To assist students in sharpening their skills in surgical procedures, many computer-assisted techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and simulators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> have been proposed. Although the systems assist in improving their skills and help reduce the workloads on academic professionals, the systems don’t attempt to answer the student’s doubts. While students have also been known to learn by watching recorded surgical procedures, the tasks of answering their questions still fall upon the medical experts. In such cases, a computer-assisted system that can process both the medical data and the questionnaires and provide a reliable answer would greatly benefit the students and reduce the medical expert’s workload <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Surgical scenes are enriched with information that the system can exploit to answer questionnaires related to the defective tissue, surgical tool interaction and surgical procedures.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">With the potential to extract diverse information from a single visual feature just by varying the question, the computer vision domain has seen a recent influx of vision and natural language processing models for visual question answering (VQA) tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These models are either built based on the long short-term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> or attention modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In comparison to the computer vision domain, which is often complemented with massive annotated datasets, the medical domain suffers from the lack of annotated data, limiting the exploration of medical VQA. The presence of domain-specific medical terms also limits the use of transfer learning techniques to adapt pre-trained computer-vision VQA models for medical applications. While limited works have been recently reported on medical-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for medical diagnosis, VQA for surgical scenes remains largely unexplored.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2206.11053/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="78" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Surgical-VQA: Given a surgical scene, the model predicts answer related to surgical tools, their interactions and surgical procedures based on the questionnaires.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">(i)</span> we design a Surgical-VQA task to generate answers for questions related to surgical tools, their interaction with tissue and surgical procedures (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">(ii)</span> We exploit the surgical scene segmentation dataset from the MICCAI endoscopic vision challenge 2018 (EndoVis-18) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and workflow recognition challenge dataset (Cholec80) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and extend it further to introduce two novel datasets for Surgical-VQA tasks. <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">(iii)</span> We employ two vision-text attention-based transformer models to perform classification-based and sentence-based answering for the Surgical-VQA. <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">(iv)</span> We also introduce a residual MLP (ResMLP) based VisualBERT ResMLP encoder model that outperforms VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> in classification-based VQA. Inspired by ResMLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, cross-token and cross-channel sub-modules are introduced into the VisualBERT ResMLP model to enforce interaction among input visual and text tokens. <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">(v)</span> Finally, the effects on the model’s performance due to the varied number of input image patches and inclusion of temporal visual features are also studied.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Method</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Preliminaries</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.10" class="ltx_p"><span id="S2.SS1.p1.10.1" class="ltx_text ltx_font_bold">VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>:</span>
A multi-layer transformer encoder model that integrates BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> transformer model with object proposal models to perform vision-and-language tasks. BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> model primarily processes an input sentence as a series of tokens (subwords) for natural language processing. By mapping to a set of embeddings (<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">E</annotation></semantics></math>), each word token is embedded (<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="e\in E" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">e</mi><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">∈</mo><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝑒</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">e\in E</annotation></semantics></math>) based on token embedding <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="e_{t}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">e</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">𝑒</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">e_{t}</annotation></semantics></math>, segment embedding <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="e_{s}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">e</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝑒</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">e_{s}</annotation></semantics></math> and position embedding <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="e_{p}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">e</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑒</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">e_{p}</annotation></semantics></math>. Along with these input word tokens, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model processes visual inputs as unordered visual tokens that are generated using the visual features extracted from the object proposals. In addition to text embedding from BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, it performs visual embedding (<math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">F</annotation></semantics></math>), where, each visual token is embedded (<math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="f\in F" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">f</mi><mo id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">∈</mo><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><in id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1"></in><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝑓</ci><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">f\in F</annotation></semantics></math>) based on visual features <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="f_{o}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">f</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑓</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">f_{o}</annotation></semantics></math>, segment embedding <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><msub id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml">f</mi><mi id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2">𝑓</ci><ci id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">f_{s}</annotation></semantics></math> and position embedding <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="f_{p}" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><msub id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">f</mi><mi id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">𝑓</ci><ci id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">f_{p}</annotation></semantics></math>. Both text and visual embedding are then propagated through multiple encoder layers in the VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to allow rich interactions between both the text and visual tokens and establish joint representation. Each encoder layer consists of an (i) self-attention module that establishes relations between tokens, (ii) intermediate module and (iii) output module consisting of hidden linear layers to reason across channels. Finally, the encoder layer is followed by a pooler module.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2206.11053/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture: Given an input surgical scene and questions, its text and visual features are propagated through the vision-text encoder (VisualBERT ResMLP). <span id="S2.F2.3.1" class="ltx_text ltx_font_bold">(i) classification-based answer:</span> The encoder output is propagated through a prediction layer for answer classification. <span id="S2.F2.4.2" class="ltx_text ltx_font_bold">(ii) Sentence-based answer:</span> The encoder is combined with a transformer decoder to predict the answer sentence word-by-word (regressively).</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>VisualBERT ResMLP</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">In our proposed VisualBERT ResMLP encoder model, we aim to further boost the interaction between the input tokens for vision-and-language tasks. The VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model relies primarily on its self-attention module in the encoder layers to establish dependency relationships and allow interactions among tokens. Inspired by residual MLP (ResMLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the intermediate and output modules of the VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model are replaced by cross-token and cross-channel modules to further enforce interaction among tokens. In the cross-token module, the inputs word and visual tokens are transposed and propagated forward, allowing information exchange between tokens. The resultant is then transposed back to allow per-token forward propagation in the cross-channel module. Both cross-token and cross-channel modules are followed by element-wise summation with a skip-connection (residual-connection), which are layer-normalized. The cross-token output (<math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="X_{CT}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">X</mi><mrow id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.p1.1.m1.1.1.3.2" xref="S2.SS2.p1.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.1.3.1" xref="S2.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.1.m1.1.1.3.3" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">𝑋</ci><apply id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"><times id="S2.SS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3.1"></times><ci id="S2.SS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.p1.1.m1.1.1.3.2">𝐶</ci><ci id="S2.SS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">X_{CT}</annotation></semantics></math>) and cross-channel output(<math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="X_{CC}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">X</mi><mrow id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS2.p1.2.m2.1.1.3.2" xref="S2.SS2.p1.2.m2.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1.3.1" xref="S2.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p1.2.m2.1.1.3.3" xref="S2.SS2.p1.2.m2.1.1.3.3.cmml">C</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">𝑋</ci><apply id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3"><times id="S2.SS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3.1"></times><ci id="S2.SS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS2.p1.2.m2.1.1.3.2">𝐶</ci><ci id="S2.SS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">X_{CC}</annotation></semantics></math>) is theorised as:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle X_{CT}" display="inline"><semantics id="S2.E1.m1.1a"><msub id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">X</mi><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑋</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><times id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></times><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">𝐶</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle X_{CT}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2.1" class="ltx_Math" alttext="\displaystyle=Norm(X_{SA}+(A((X_{SA})^{T}))^{T})" display="inline"><semantics id="S2.E1.m2.1a"><mrow id="S2.E1.m2.1.1" xref="S2.E1.m2.1.1.cmml"><mi id="S2.E1.m2.1.1.3" xref="S2.E1.m2.1.1.3.cmml"></mi><mo id="S2.E1.m2.1.1.2" xref="S2.E1.m2.1.1.2.cmml">=</mo><mrow id="S2.E1.m2.1.1.1" xref="S2.E1.m2.1.1.1.cmml"><mi id="S2.E1.m2.1.1.1.3" xref="S2.E1.m2.1.1.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.2" xref="S2.E1.m2.1.1.1.2.cmml">​</mo><mi id="S2.E1.m2.1.1.1.4" xref="S2.E1.m2.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.2a" xref="S2.E1.m2.1.1.1.2.cmml">​</mo><mi id="S2.E1.m2.1.1.1.5" xref="S2.E1.m2.1.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.2b" xref="S2.E1.m2.1.1.1.2.cmml">​</mo><mi id="S2.E1.m2.1.1.1.6" xref="S2.E1.m2.1.1.1.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.2c" xref="S2.E1.m2.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m2.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m2.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.cmml"><msub id="S2.E1.m2.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m2.1.1.1.1.1.1.3.2" xref="S2.E1.m2.1.1.1.1.1.1.3.2.cmml">X</mi><mrow id="S2.E1.m2.1.1.1.1.1.1.3.3" xref="S2.E1.m2.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m2.1.1.1.1.1.1.3.3.2" xref="S2.E1.m2.1.1.1.1.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.1.1.1.3.3.1" xref="S2.E1.m2.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E1.m2.1.1.1.1.1.1.3.3.3" xref="S2.E1.m2.1.1.1.1.1.1.3.3.3.cmml">A</mi></mrow></msub><mo id="S2.E1.m2.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.2.cmml">+</mo><msup id="S2.E1.m2.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m2.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mrow id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">A</mi></mrow></msub><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">T</mi></msup><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S2.E1.m2.1.1.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.1.3.cmml">T</mi></msup></mrow><mo stretchy="false" id="S2.E1.m2.1.1.1.1.1.3" xref="S2.E1.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m2.1b"><apply id="S2.E1.m2.1.1.cmml" xref="S2.E1.m2.1.1"><eq id="S2.E1.m2.1.1.2.cmml" xref="S2.E1.m2.1.1.2"></eq><csymbol cd="latexml" id="S2.E1.m2.1.1.3.cmml" xref="S2.E1.m2.1.1.3">absent</csymbol><apply id="S2.E1.m2.1.1.1.cmml" xref="S2.E1.m2.1.1.1"><times id="S2.E1.m2.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.2"></times><ci id="S2.E1.m2.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.3">𝑁</ci><ci id="S2.E1.m2.1.1.1.4.cmml" xref="S2.E1.m2.1.1.1.4">𝑜</ci><ci id="S2.E1.m2.1.1.1.5.cmml" xref="S2.E1.m2.1.1.1.5">𝑟</ci><ci id="S2.E1.m2.1.1.1.6.cmml" xref="S2.E1.m2.1.1.1.6">𝑚</ci><apply id="S2.E1.m2.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1"><plus id="S2.E1.m2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.2"></plus><apply id="S2.E1.m2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m2.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3.2">𝑋</ci><apply id="S2.E1.m2.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3.3"><times id="S2.E1.m2.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E1.m2.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3.3.2">𝑆</ci><ci id="S2.E1.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.3.3.3">𝐴</ci></apply></apply><apply id="S2.E1.m2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1"><times id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.3">𝐴</ci><apply id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑋</ci><apply id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝐴</ci></apply></apply><ci id="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑇</ci></apply></apply><ci id="S2.E1.m2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.1.1.1.1.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.1c">\displaystyle=Norm(X_{SA}+(A((X_{SA})^{T}))^{T})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\displaystyle X_{CC}" display="inline"><semantics id="S2.E2.m1.1a"><msub id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mi id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">X</mi><mrow id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.3.3" xref="S2.E2.m1.1.1.3.3.cmml">C</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2">𝑋</ci><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><times id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1"></times><ci id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2">𝐶</ci><ci id="S2.E2.m1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle X_{CC}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m2.1" class="ltx_Math" alttext="\displaystyle=Norm(X_{CT}+C(GeLU(B(X_{CT}))))" display="inline"><semantics id="S2.E2.m2.1a"><mrow id="S2.E2.m2.1.1" xref="S2.E2.m2.1.1.cmml"><mi id="S2.E2.m2.1.1.3" xref="S2.E2.m2.1.1.3.cmml"></mi><mo id="S2.E2.m2.1.1.2" xref="S2.E2.m2.1.1.2.cmml">=</mo><mrow id="S2.E2.m2.1.1.1" xref="S2.E2.m2.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.3" xref="S2.E2.m2.1.1.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.2" xref="S2.E2.m2.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.4" xref="S2.E2.m2.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.2a" xref="S2.E2.m2.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.5" xref="S2.E2.m2.1.1.1.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.2b" xref="S2.E2.m2.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.6" xref="S2.E2.m2.1.1.1.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.2c" xref="S2.E2.m2.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m2.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m2.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.cmml"><msub id="S2.E2.m2.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.3.2" xref="S2.E2.m2.1.1.1.1.1.1.3.2.cmml">X</mi><mrow id="S2.E2.m2.1.1.1.1.1.1.3.3" xref="S2.E2.m2.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.3.3.2" xref="S2.E2.m2.1.1.1.1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.3.3.1" xref="S2.E2.m2.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E2.m2.1.1.1.1.1.1.3.3.3" xref="S2.E2.m2.1.1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msub><mo id="S2.E2.m2.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2a" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.5" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2b" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.6" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.6.cmml">U</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2c" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></mrow></msub><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E2.m2.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m2.1b"><apply id="S2.E2.m2.1.1.cmml" xref="S2.E2.m2.1.1"><eq id="S2.E2.m2.1.1.2.cmml" xref="S2.E2.m2.1.1.2"></eq><csymbol cd="latexml" id="S2.E2.m2.1.1.3.cmml" xref="S2.E2.m2.1.1.3">absent</csymbol><apply id="S2.E2.m2.1.1.1.cmml" xref="S2.E2.m2.1.1.1"><times id="S2.E2.m2.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.3">𝑁</ci><ci id="S2.E2.m2.1.1.1.4.cmml" xref="S2.E2.m2.1.1.1.4">𝑜</ci><ci id="S2.E2.m2.1.1.1.5.cmml" xref="S2.E2.m2.1.1.1.5">𝑟</ci><ci id="S2.E2.m2.1.1.1.6.cmml" xref="S2.E2.m2.1.1.1.6">𝑚</ci><apply id="S2.E2.m2.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1"><plus id="S2.E2.m2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.2"></plus><apply id="S2.E2.m2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m2.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3.2">𝑋</ci><apply id="S2.E2.m2.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3.3"><times id="S2.E2.m2.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E2.m2.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3.3.2">𝐶</ci><ci id="S2.E2.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3.3.3">𝑇</ci></apply></apply><apply id="S2.E2.m2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1"><times id="S2.E2.m2.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.3">𝐶</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.3">𝐺</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.4">𝑒</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.5">𝐿</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.6">𝑈</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝐵</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑋</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m2.1c">\displaystyle=Norm(X_{CT}+C(GeLU(B(X_{CT}))))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.5" class="ltx_p">where, <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="X_{SA}" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><msub id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">X</mi><mrow id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.m1.1.1.3.2" xref="S2.SS2.p3.1.m1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.1.1.3.1" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.p3.1.m1.1.1.3.3" xref="S2.SS2.p3.1.m1.1.1.3.3.cmml">A</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">𝑋</ci><apply id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3"><times id="S2.SS2.p3.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.m1.1.1.3.1"></times><ci id="S2.SS2.p3.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.m1.1.1.3.2">𝑆</ci><ci id="S2.SS2.p3.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">X_{SA}</annotation></semantics></math> is the self-attention module output, <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">A</annotation></semantics></math>, <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mi id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><ci id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">B</annotation></semantics></math> and <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">C</annotation></semantics></math> are the learnable linear layers, <math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="GeLU" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mrow id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml"><mi id="S2.SS2.p3.5.m5.1.1.2" xref="S2.SS2.p3.5.m5.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.1" xref="S2.SS2.p3.5.m5.1.1.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.3" xref="S2.SS2.p3.5.m5.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.1a" xref="S2.SS2.p3.5.m5.1.1.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.4" xref="S2.SS2.p3.5.m5.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.1b" xref="S2.SS2.p3.5.m5.1.1.1.cmml">​</mo><mi id="S2.SS2.p3.5.m5.1.1.5" xref="S2.SS2.p3.5.m5.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><apply id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1"><times id="S2.SS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1.1"></times><ci id="S2.SS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2">𝐺</ci><ci id="S2.SS2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3">𝑒</ci><ci id="S2.SS2.p3.5.m5.1.1.4.cmml" xref="S2.SS2.p3.5.m5.1.1.4">𝐿</ci><ci id="S2.SS2.p3.5.m5.1.1.5.cmml" xref="S2.SS2.p3.5.m5.1.1.5">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">GeLU</annotation></semantics></math> is the GeLU activation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and NORM is the layer-normalization.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>VisualBert ResMLP for Classification</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.4" class="ltx_p"><span id="S2.SS3.p1.4.1" class="ltx_text ltx_font_bold">Word and Visual Tokens:</span>
Each question is converted to a series of word tokens generated using BERT tokenizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Here, the BERT tokenizer is custom trained on the dataset to include surgical domain-specific words. The visual tokens are generated using the final convolution layer features of the feature extractor. A ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> model pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is used as the feature extractor. While the VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model uses the visual features extracted from object proposals, we bypass the need for object proposal networks by extracting the features from the entire image. By employing adaptive average pooling to the final convolution layer, the output shape (s) is resize to s = [batch size x <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">n</annotation></semantics></math> x <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">n</annotation></semantics></math> x <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mn id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><cn type="integer" id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">256</annotation></semantics></math>], thereby, restricting the number of visual tokens (patches) to <math id="S2.SS3.p1.4.m4.1" class="ltx_Math" alttext="n^{2}" display="inline"><semantics id="S2.SS3.p1.4.m4.1a"><msup id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml"><mi id="S2.SS3.p1.4.m4.1.1.2" xref="S2.SS3.p1.4.m4.1.1.2.cmml">n</mi><mn id="S2.SS3.p1.4.m4.1.1.3" xref="S2.SS3.p1.4.m4.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><apply id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.4.m4.1.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">superscript</csymbol><ci id="S2.SS3.p1.4.m4.1.1.2.cmml" xref="S2.SS3.p1.4.m4.1.1.2">𝑛</ci><cn type="integer" id="S2.SS3.p1.4.m4.1.1.3.cmml" xref="S2.SS3.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">n^{2}</annotation></semantics></math>.
<span id="S2.SS3.p1.4.2" class="ltx_text ltx_font_bold">VisualBERT ResMLP Module:</span>
The word and visual tokens are propagated through text and visual embedding layers, respectively, in the VisualBert ResMLP model. The embedded tokens are then propagated through 6 layers of encoders (comprising self-attention and ResMLP modules) and finally through the pooling module. Embedding size = 300 and hidden layer feature size = 2048 are set as VisualBert ResMLP model parameters.
<span id="S2.SS3.p1.4.3" class="ltx_text ltx_font_bold">Classification-based answer:</span>
The output of the pooling module is propagated through a prediction (linear) layer to predict the answer label.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>VisualBert ResMLP for Sentence</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p"><span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_bold">Word and Visual Tokens:</span> In addition to tokenizing the words in the questions and visual features in the image as stated in section <a href="#S2.SS3" title="2.3 VisualBert ResMLP for Classification ‣ 2 Proposed Method ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the target sentence-based answer is also converted to a series of word tokens using the BERT tokenizer.
<span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_bold">VisualBERT ResMLP encoder:</span>
The VisualBERT ResMLP model follows the same parameters and configuration as stated in section <a href="#S2.SS3" title="2.3 VisualBert ResMLP for Classification ‣ 2 Proposed Method ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
<span id="S2.SS4.p1.1.3" class="ltx_text ltx_font_bold">Sentence-based answer:</span> To generate the answer in a sentence structure, we propose to combine the vision-text encoder model (VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or VisualBERT ResMLP) with a multi-head attention-based transformer decoder (TD) model. Considering the sentence-based answer generation task to be similar to an image-caption generation task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, it is positioned as a next-word prediction task. Given the question features, visual features and the previous word (in answer sentence) token, the decoder model predicts the next word in the answer sentence. During the training stage, as shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1 Preliminaries ‣ 2 Proposed Method ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the target word tokens are shifted rights and, together with vision-text encoder output, are propagated through the transformer decoder layers to predict the corresponding next word. During the evaluation stage, the beam search <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> technique is employed to predict the sentence-based answer without using target word tokens. Taking the ‘[start]’ token as the first word, the visual-text encoder model and decoder is regressed on every predicted word to predict the subsequent word, until a ‘[end]’ token is predicted.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Med-VQA:</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">A public dataset from the ImageCLEF 2019 Med-VQA Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Three categories (C1: modality, C2: plane and C3: organ) of medical question-answer pairs from the dataset are used in this work. The C1, C2 and C3 pose a classification task (single-word answer) for 3825 images with a question. The C1, C2 and C3 consist of 45, 16 and 10 answer classes, respectively. The train and test set split follows the original implementation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>EndoVis-18-VQA:</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.11" class="ltx_p">A novel dataset was generated from <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">14</annotation></semantics></math> video sequences of robotic nephrectomy procedures from the MICCAI Endoscopic Vision Challenge 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset. Based on the tool, tissue, bounding box and interaction annotations used for tool-tissue interaction detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we generated two versions of question-answer pairs for each image frame. <span id="S3.SS1.SSS2.p1.11.1" class="ltx_text ltx_font_bold">(i) Classification (EndoVis-18-VQA (C)):</span> The answers are in single-word form. It consists of <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="26" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mn id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">26</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">26</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">26</annotation></semantics></math> distinct answer classes (one organ, <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mn id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><cn type="integer" id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">8</annotation></semantics></math> surgical tools, <math id="S3.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mn id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><cn type="integer" id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">13</annotation></semantics></math> tool interactions and <math id="S3.SS1.SSS2.p1.5.m5.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mn id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><cn type="integer" id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">4</annotation></semantics></math> tool locations). <span id="S3.SS1.SSS2.p1.11.2" class="ltx_text ltx_font_bold">(ii) Sentence (EndoVis-18-VQA (S)):</span> The answers are in a sentence form. In both versions, <math id="S3.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S3.SS1.SSS2.p1.6.m6.1a"><mn id="S3.SS1.SSS2.p1.6.m6.1.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.6.m6.1b"><cn type="integer" id="S3.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.6.m6.1c">11</annotation></semantics></math> sequences with <math id="S3.SS1.SSS2.p1.7.m7.1" class="ltx_Math" alttext="1560" display="inline"><semantics id="S3.SS1.SSS2.p1.7.m7.1a"><mn id="S3.SS1.SSS2.p1.7.m7.1.1" xref="S3.SS1.SSS2.p1.7.m7.1.1.cmml">1560</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.7.m7.1b"><cn type="integer" id="S3.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p1.7.m7.1.1">1560</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.7.m7.1c">1560</annotation></semantics></math> images and <math id="S3.SS1.SSS2.p1.8.m8.1" class="ltx_Math" alttext="9014" display="inline"><semantics id="S3.SS1.SSS2.p1.8.m8.1a"><mn id="S3.SS1.SSS2.p1.8.m8.1.1" xref="S3.SS1.SSS2.p1.8.m8.1.1.cmml">9014</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.8.m8.1b"><cn type="integer" id="S3.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS2.p1.8.m8.1.1">9014</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.8.m8.1c">9014</annotation></semantics></math> question-answer pairs are used as a training set and <math id="S3.SS1.SSS2.p1.9.m9.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS1.SSS2.p1.9.m9.1a"><mn id="S3.SS1.SSS2.p1.9.m9.1.1" xref="S3.SS1.SSS2.p1.9.m9.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.9.m9.1b"><cn type="integer" id="S3.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS2.p1.9.m9.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.9.m9.1c">3</annotation></semantics></math> sequences with <math id="S3.SS1.SSS2.p1.10.m10.1" class="ltx_Math" alttext="447" display="inline"><semantics id="S3.SS1.SSS2.p1.10.m10.1a"><mn id="S3.SS1.SSS2.p1.10.m10.1.1" xref="S3.SS1.SSS2.p1.10.m10.1.1.cmml">447</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.10.m10.1b"><cn type="integer" id="S3.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS2.p1.10.m10.1.1">447</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.10.m10.1c">447</annotation></semantics></math> images and <math id="S3.SS1.SSS2.p1.11.m11.1" class="ltx_Math" alttext="2769" display="inline"><semantics id="S3.SS1.SSS2.p1.11.m11.1a"><mn id="S3.SS1.SSS2.p1.11.m11.1.1" xref="S3.SS1.SSS2.p1.11.m11.1.1.cmml">2769</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.11.m11.1b"><cn type="integer" id="S3.SS1.SSS2.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS2.p1.11.m11.1.1">2769</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.11.m11.1c">2769</annotation></semantics></math> question-answers pairs are used as a test set. The train and test split follow the tool-tissue interaction detection task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Cholec80-VQA:</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.7" class="ltx_p">A novel dataset generated from <math id="S3.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mn id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">40</annotation></semantics></math> video sequences of the Cholec80 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. We sampled the video sequences at <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mn id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><cn type="float" id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">0.25</annotation></semantics></math> fps to generate the Cholec80-VQA dataset consisting of 21591 frames. Based on tool-operation and phase annotations provided in the Cholec80 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, two question-answer pairs are generated for each frame. <span id="S3.SS1.SSS3.p1.7.1" class="ltx_text ltx_font_bold">(i) Classification (Cholec80-VQA (C)):</span> <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><mn id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><cn type="integer" id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">14</annotation></semantics></math> distinct single-word answers (8 on surgical phase, two on tool state and 4 on number of Tools). <span id="S3.SS1.SSS3.p1.7.2" class="ltx_text ltx_font_bold">(ii) Sentence (Cholec80-VQA (S)):</span> The answers are in a sentence form. In both versions, <math id="S3.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="34" display="inline"><semantics id="S3.SS1.SSS3.p1.4.m4.1a"><mn id="S3.SS1.SSS3.p1.4.m4.1.1" xref="S3.SS1.SSS3.p1.4.m4.1.1.cmml">34</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.4.m4.1b"><cn type="integer" id="S3.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1">34</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.4.m4.1c">34</annotation></semantics></math>k question-answer pairs for <math id="S3.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="17" display="inline"><semantics id="S3.SS1.SSS3.p1.5.m5.1a"><mn id="S3.SS1.SSS3.p1.5.m5.1.1" xref="S3.SS1.SSS3.p1.5.m5.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.5.m5.1b"><cn type="integer" id="S3.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.5.m5.1c">17</annotation></semantics></math>k frames are used for the train set and <math id="S3.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="9K" display="inline"><semantics id="S3.SS1.SSS3.p1.6.m6.1a"><mrow id="S3.SS1.SSS3.p1.6.m6.1.1" xref="S3.SS1.SSS3.p1.6.m6.1.1.cmml"><mn id="S3.SS1.SSS3.p1.6.m6.1.1.2" xref="S3.SS1.SSS3.p1.6.m6.1.1.2.cmml">9</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS3.p1.6.m6.1.1.1" xref="S3.SS1.SSS3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS3.p1.6.m6.1.1.3" xref="S3.SS1.SSS3.p1.6.m6.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.6.m6.1b"><apply id="S3.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1"><times id="S3.SS1.SSS3.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1.1"></times><cn type="integer" id="S3.SS1.SSS3.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1.2">9</cn><ci id="S3.SS1.SSS3.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.6.m6.1c">9K</annotation></semantics></math> question-answer pairs for <math id="S3.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="4.5" display="inline"><semantics id="S3.SS1.SSS3.p1.7.m7.1a"><mn id="S3.SS1.SSS3.p1.7.m7.1.1" xref="S3.SS1.SSS3.p1.7.m7.1.1.cmml">4.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.7.m7.1b"><cn type="float" id="S3.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS3.p1.7.m7.1.1">4.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.7.m7.1c">4.5</annotation></semantics></math>k frames are used for the test set. The train and test set split follows the Cholec80 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation Details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.12" class="ltx_p">Both our classification and the sentence-based answer generation models<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/lalithjets/Surgical_VQA.git" title="" class="ltx_ref ltx_href">github.com/lalithjets/Surgical_VQA.git</a></span></span></span> are trained based on cross-entropy loss and optimized using the Adam optimizer. For Classification tasks, a batch size = 64 and epoch = 80 are used. A learning rate = <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">5</annotation></semantics></math>x<math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mo id="S3.SS2.p1.2.m2.1.1.3a" xref="S3.SS2.p1.2.m2.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">10</cn><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><minus id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">10^{-6}</annotation></semantics></math>, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1</annotation></semantics></math>x<math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mo id="S3.SS2.p1.4.m4.1.1.3a" xref="S3.SS2.p1.4.m4.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">10</cn><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><minus id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">10^{-5}</annotation></semantics></math> and <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn type="integer" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">5</annotation></semantics></math>x<math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><msup id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mn id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><mo id="S3.SS2.p1.6.m6.1.1.3a" xref="S3.SS2.p1.6.m6.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">10</cn><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><minus id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">10^{-6}</annotation></semantics></math> are used for Med-VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, EndoVis-18-VQA (C) and Cholec80-VQA (C) dataset, respectively. For the sentence-based answer generation tasks, a batch size = 50 is used. The models are trained for epoch = 50, 100 and 51, with a learning rate = <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><cn type="integer" id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">1</annotation></semantics></math>x<math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><msup id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mn id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml"><mo id="S3.SS2.p1.8.m8.1.1.3a" xref="S3.SS2.p1.8.m8.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.8.m8.1.1.3.2" xref="S3.SS2.p1.8.m8.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">10</cn><apply id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3"><minus id="S3.SS2.p1.8.m8.1.1.3.1.cmml" xref="S3.SS2.p1.8.m8.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.8.m8.1.1.3.2.cmml" xref="S3.SS2.p1.8.m8.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">10^{-4}</annotation></semantics></math>, <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mn id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><cn type="integer" id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">5</annotation></semantics></math>x<math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><msup id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml"><mn id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml"><mo id="S3.SS2.p1.10.m10.1.1.3a" xref="S3.SS2.p1.10.m10.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.10.m10.1.1.3.2" xref="S3.SS2.p1.10.m10.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">10</cn><apply id="S3.SS2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3"><minus id="S3.SS2.p1.10.m10.1.1.3.1.cmml" xref="S3.SS2.p1.10.m10.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.10.m10.1.1.3.2.cmml" xref="S3.SS2.p1.10.m10.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">10^{-5}</annotation></semantics></math> and <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mn id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><cn type="integer" id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">1</annotation></semantics></math>x<math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><msup id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml"><mn id="S3.SS2.p1.12.m12.1.1.2" xref="S3.SS2.p1.12.m12.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.12.m12.1.1.3" xref="S3.SS2.p1.12.m12.1.1.3.cmml"><mo id="S3.SS2.p1.12.m12.1.1.3a" xref="S3.SS2.p1.12.m12.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.12.m12.1.1.3.2" xref="S3.SS2.p1.12.m12.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><apply id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.1.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.12.m12.1.1.2.cmml" xref="S3.SS2.p1.12.m12.1.1.2">10</cn><apply id="S3.SS2.p1.12.m12.1.1.3.cmml" xref="S3.SS2.p1.12.m12.1.1.3"><minus id="S3.SS2.p1.12.m12.1.1.3.1.cmml" xref="S3.SS2.p1.12.m12.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.12.m12.1.1.3.2.cmml" xref="S3.SS2.p1.12.m12.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">10^{-6}</annotation></semantics></math> on Med-VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, EndoVis-18-VQA (S) and Cholec80-VQA (S) dataset, respectively.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.3" class="ltx_p">The performance of classification-based answering on the EndoVis-18-VQA (C), Cholec80-VQA (C) and Med-VQA (C1, C2 and C3) datasets are quantified based on the accuracy (Acc), recall and F-score in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It is observed that our proposed encoder (VisualBERT ResMLP) based model outperformed the current medical VQA state-of-the-art MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> model and marginally outperformed the base encoder (VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>) based model in almost all datasets. While the improvement in performance against the base model is marginal, a k-fold study (Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) on EndoVis-18-VQA (C) dataset proves that the improvement is consistent. Furthermore, our model (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="159.0" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">159.0</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="float" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">159.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">159.0</annotation></semantics></math>M) requires <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="13.64" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">13.64</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="float" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">13.64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">13.64</annotation></semantics></math>% lesser parameters compared to the base model (<math id="S4.p1.3.m3.1" class="ltx_Math" alttext="184.2" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">184.2</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="float" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">184.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">184.2</annotation></semantics></math>M).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of our VisualBERT ResMLP based model against MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> based model for classification-based answering.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:353.7pt;height:95.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.9pt,15.8pt) scale(0.75,0.75) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">VisualBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span></th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">VisualBert ResMLP</span></th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Fscore</span></th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Fscore</span></th>
<th id="S4.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.2.2.9.1" class="ltx_text ltx_font_bold">Fscore</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Med-VQA (C1)</span></th>
<td id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.754</td>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.224</td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.140</td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1.5.1" class="ltx_text ltx_font_bold">0.828</span></td>
<td id="S4.T1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1.6.1" class="ltx_text ltx_font_bold">0.617</span></td>
<td id="S4.T1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.3.1.7.1" class="ltx_text ltx_font_bold">0.582</span></td>
<td id="S4.T1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1.8.1" class="ltx_text ltx_font_bold">0.828</span></td>
<td id="S4.T1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">0.598</td>
<td id="S4.T1.1.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">0.543</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.1.4.2.1.1" class="ltx_text ltx_font_bold">Med-VQA (C2)</span></th>
<td id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_center">0.730</td>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_center">0.305</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">0.303</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.4.2.5.1" class="ltx_text ltx_font_bold">0.760</span></td>
<td id="S4.T1.1.1.4.2.6" class="ltx_td ltx_align_center">0.363</td>
<td id="S4.T1.1.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r">0.367</td>
<td id="S4.T1.1.1.4.2.8" class="ltx_td ltx_align_center">0.758</td>
<td id="S4.T1.1.1.4.2.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.4.2.9.1" class="ltx_text ltx_font_bold">0.399</span></td>
<td id="S4.T1.1.1.4.2.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.4.2.10.1" class="ltx_text ltx_font_bold">0.398</span></td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.1.5.3.1.1" class="ltx_text ltx_font_bold">Med-VQA (C3)</span></th>
<td id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_center">0.652</td>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_center">0.478</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">0.484</td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_center">0.734</td>
<td id="S4.T1.1.1.5.3.6" class="ltx_td ltx_align_center">0.587</td>
<td id="S4.T1.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r">0.595</td>
<td id="S4.T1.1.1.5.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.3.8.1" class="ltx_text ltx_font_bold">0.736</span></td>
<td id="S4.T1.1.1.5.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.3.9.1" class="ltx_text ltx_font_bold">0.609</span></td>
<td id="S4.T1.1.1.5.3.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.3.10.1" class="ltx_text ltx_font_bold">0.607</span></td>
</tr>
<tr id="S4.T1.1.1.6.4" class="ltx_tr">
<th id="S4.T1.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.1.6.4.1.1" class="ltx_text ltx_font_bold">EndoVis-18-VQA (C)</span></th>
<td id="S4.T1.1.1.6.4.2" class="ltx_td ltx_align_center">0.609</td>
<td id="S4.T1.1.1.6.4.3" class="ltx_td ltx_align_center">0.261</td>
<td id="S4.T1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">0.222</td>
<td id="S4.T1.1.1.6.4.5" class="ltx_td ltx_align_center">0.619</td>
<td id="S4.T1.1.1.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.6.4.6.1" class="ltx_text ltx_font_bold">0.412</span></td>
<td id="S4.T1.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r">0.334</td>
<td id="S4.T1.1.1.6.4.8" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.6.4.8.1" class="ltx_text ltx_font_bold">0.632</span></td>
<td id="S4.T1.1.1.6.4.9" class="ltx_td ltx_align_center">0.396</td>
<td id="S4.T1.1.1.6.4.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.6.4.10.1" class="ltx_text ltx_font_bold">0.336</span></td>
</tr>
<tr id="S4.T1.1.1.7.5" class="ltx_tr">
<th id="S4.T1.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T1.1.1.7.5.1.1" class="ltx_text ltx_font_bold">Cholec80-VQA (C)</span></th>
<td id="S4.T1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">0.861</td>
<td id="S4.T1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb">0.349</td>
<td id="S4.T1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.309</td>
<td id="S4.T1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb">0.897</td>
<td id="S4.T1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.7.5.6.1" class="ltx_text ltx_font_bold">0.629</span></td>
<td id="S4.T1.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.633</td>
<td id="S4.T1.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.7.5.8.1" class="ltx_text ltx_font_bold">0.898</span></td>
<td id="S4.T1.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_bb">0.627</td>
<td id="S4.T1.1.1.7.5.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.7.5.10.1" class="ltx_text ltx_font_bold">0.634</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>k-fold performance comparison of our VisualBERT ResMLP based model against VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> based model on EndoVis-18-VQA (C) dataset.</figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:295.6pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.3pt,9.0pt) scale(0.75,0.75) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.3.3.3.4.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">
<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="1^{st}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">1</mn><mrow id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.3.1" xref="S4.T2.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">1</cn><apply id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3"><times id="S4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.2">𝑠</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">1^{st}</annotation></semantics></math><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold"> Fold</span>
</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2">
<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="2^{nd}" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><msup id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mn id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml">2</mn><mrow id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.3.2" xref="S4.T2.2.2.2.2.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.2.m1.1.1.3.1" xref="S4.T2.2.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.2.2.2.2.m1.1.1.3.3" xref="S4.T2.2.2.2.2.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2">2</cn><apply id="S4.T2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3"><times id="S4.T2.2.2.2.2.m1.1.1.3.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3.1"></times><ci id="S4.T2.2.2.2.2.m1.1.1.3.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3.2">𝑛</ci><ci id="S4.T2.2.2.2.2.m1.1.1.3.3.cmml" xref="S4.T2.2.2.2.2.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">2^{nd}</annotation></semantics></math><span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_font_bold"> Fold</span>
</th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">
<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="3^{rd}" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><msup id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml"><mn id="S4.T2.3.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.3.m1.1.1.2.cmml">3</mn><mrow id="S4.T2.3.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.3.m1.1.1.3.cmml"><mi id="S4.T2.3.3.3.3.m1.1.1.3.2" xref="S4.T2.3.3.3.3.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.3.m1.1.1.3.1" xref="S4.T2.3.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S4.T2.3.3.3.3.m1.1.1.3.3" xref="S4.T2.3.3.3.3.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">superscript</csymbol><cn type="integer" id="S4.T2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.2">3</cn><apply id="S4.T2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3"><times id="S4.T2.3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.1"></times><ci id="S4.T2.3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.2">𝑟</ci><ci id="S4.T2.3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T2.3.3.3.3.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">3^{rd}</annotation></semantics></math><span id="S4.T2.3.3.3.3.1" class="ltx_text ltx_font_bold"> Fold</span>
</th>
</tr>
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<th id="S4.T2.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.3.3.4.1.2.1" class="ltx_text ltx_font_bold">Fscore</span></th>
<th id="S4.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.3.3.4.1.3.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.3.3.4.1.4.1" class="ltx_text ltx_font_bold">Fscore</span></th>
<th id="S4.T2.3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.3.3.4.1.5.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S4.T2.3.3.4.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.3.3.4.1.6.1" class="ltx_text ltx_font_bold">Fscore</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.5.1" class="ltx_tr">
<th id="S4.T2.3.3.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.3.3.5.1.1.1" class="ltx_text ltx_font_bold">VisualBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span></th>
<td id="S4.T2.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_t">0.619</td>
<td id="S4.T2.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.334</td>
<td id="S4.T2.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">0.605</td>
<td id="S4.T2.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.313</td>
<td id="S4.T2.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">0.578</td>
<td id="S4.T2.3.3.5.1.7" class="ltx_td ltx_align_center ltx_border_t">0.337</td>
</tr>
<tr id="S4.T2.3.3.6.2" class="ltx_tr">
<th id="S4.T2.3.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T2.3.3.6.2.1.1" class="ltx_text ltx_font_bold">VisualBert ResMLP</span></th>
<td id="S4.T2.3.3.6.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.6.2.2.1" class="ltx_text ltx_font_bold">0.632</span></td>
<td id="S4.T2.3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.3.3.6.2.3.1" class="ltx_text ltx_font_bold">0.336</span></td>
<td id="S4.T2.3.3.6.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.6.2.4.1" class="ltx_text ltx_font_bold">0.649</span></td>
<td id="S4.T2.3.3.6.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.3.3.6.2.5.1" class="ltx_text ltx_font_bold">0.347</span></td>
<td id="S4.T2.3.3.6.2.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.6.2.6.1" class="ltx_text ltx_font_bold">0.585</span></td>
<td id="S4.T2.3.3.6.2.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.6.2.7.1" class="ltx_text ltx_font_bold">0.373</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of transformer-based models ((i) VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> + TD and (ii) VisualBERT ResMLP + TD) against MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for sentence-based answering.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:439.2pt;height:67.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.2pt,11.7pt) scale(0.74,0.74) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">EndoVis-18-VQA (S)</span></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Cholec80-VQA (S)</span></th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">BLEU-3</span></th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.2.1" class="ltx_text ltx_font_bold">BLEU-4</span></th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.3.1" class="ltx_text ltx_font_bold">CIDEr</span></th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.4.1" class="ltx_text ltx_font_bold">METEOR</span></th>
<th id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.5.1" class="ltx_text ltx_font_bold">BLEU-3</span></th>
<th id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.6.1" class="ltx_text ltx_font_bold">BLEU-4</span></th>
<th id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.2.7.1" class="ltx_text ltx_font_bold">CIDEr</span></th>
<th id="S4.T3.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.1.1.2.2.8.1" class="ltx_text ltx_font_bold">METEOR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.212</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.165</td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.752</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.148</td>
<td id="S4.T3.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.378</td>
<td id="S4.T3.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.333</td>
<td id="S4.T3.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">1.250</td>
<td id="S4.T3.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">0.222</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.1.1.4.2.1.1" class="ltx_text ltx_font_bold">VisualBert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> + TD</span></th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.2.1" class="ltx_text ltx_font_bold">0.727</span></td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.3.1" class="ltx_text ltx_font_bold">0.694</span></td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_center">5.153</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.4.2.5.1" class="ltx_text ltx_font_bold">0.544</span></td>
<td id="S4.T3.1.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.6.1" class="ltx_text ltx_font_bold">0.963</span></td>
<td id="S4.T3.1.1.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.7.1" class="ltx_text ltx_font_bold">0.956</span></td>
<td id="S4.T3.1.1.4.2.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.8.1" class="ltx_text ltx_font_bold">8.802</span></td>
<td id="S4.T3.1.1.4.2.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.9.1" class="ltx_text ltx_font_bold">0.719</span></td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.1.1.5.3.1.1" class="ltx_text ltx_font_bold">VisualBert ResMLP + TD</span></th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.722</td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.691</td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.5.3.4.1" class="ltx_text ltx_font_bold">5.262</span></td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.543</td>
<td id="S4.T3.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.960</td>
<td id="S4.T3.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb">0.952</td>
<td id="S4.T3.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb">8.759</td>
<td id="S4.T3.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb">0.711</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2206.11053/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of sentence-based answer generation by MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> + TD and VisualBERT ResMLP + TD models.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">For the sentence-based answering, BLEU score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> are used for quantitative analysis. Compared to the LSTM-based MedFuse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> model, the two variants of our proposed Transformer-based model (VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> + TD and VisualBERT ResMLP + TD) performed better on both the EndoVis-18-VQA (S) and Cholec80-VQA (S) dataset (Table  <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). However, when compared within the two variants, the variant with the VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as its vision-text encoder performed marginally better. While the cross-patch sub-module in VisualBERT ResMLP encoder improves performance in classification-based answering, the marginal low performance could be attributed to its influence on the self-attention sub-module. To predict a sentence-based answer, in addition to the encoder’s overall output, the adaptive multi-head attention sub-module in the TD utilizes the encoder’s self-attention sub-module outputs. By enforcing interaction between tokens, the cross-patch sub-module could have affected the optimal training of the self-attention sub-module, thereby affecting the sentence generation performance. It is worth noting that the VisualBERT ResMLP encoder + TD model (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="184.7" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">184.7</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="float" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">184.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">184.7</annotation></semantics></math>M) requires 11.98% fewer parameters compared to the VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> + TD model (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="209.8" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">209.8</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="float" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">209.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">209.8</annotation></semantics></math>M) while maintaining similar performances. Fig. <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the qualitative performance of sentence-based answering.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Ablation Study</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2206.11053/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Ablation study: For classification-based answering (a) Performance comparison of VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> vs VisualBERT ResMLP based model; (b) Accuracy vs patch size (number of patches) study. For sentence-based answering: (c) BLEU-4 vs patch size study; (d) Performance of single frame vs temporal visual features.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Firstly, the performance of VisualBERT encoder and VisualBERT ResMLP encoder-based models for classification-based answering with a varying number of input image patches (patch size = 1, 4, 9, 16 and 25) is trained and studied. From Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Ablation Study ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a), it is observed that VisualBERT ResMLP encoder-based model generally performs better (in-line with observation in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) than VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> encoder-based model, even with varied number of input patches. Secondly, from Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Ablation Study ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b), it is also observed that performances generally improved with an increase in the number of patches for classification-based answering. However, the influence of the number of inputs patches on sentence-based answering remains inconclusive (Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Ablation Study ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c)). In most cases, the input of a single patch seems to offer the best or near best results.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Finally, the performance of models incorporated with temporal visual features is also studied. Here, a 3D ResNet50 pre-trained on the UCF101 dataset  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is employed as the feature extractor. With a temporal size = 3, the current and the past 2 frames are used as input to the feature extractor. The extracted features are then used as visual tokens for sentence-based answering. Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Ablation Study ‣ 4 Results ‣ Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (d) shows the model’s performance on a varied number of input patches (patch size = 1, 4, 9, 16 and 25) from single frames vs. temporal frames on EndoVis-18-VQA (S) and Cholec80-VQA (S). It is observed that both transformer-based models’ performance reduces when the temporal features are used.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We design a Surgical-VQA algorithm to answer questionnaires on surgical tools, their interactions and surgical procedures based on our two novel Surgical-VQA datasets evolved from two public datasets. To perform classification and sentence-based answering, vision-text attention-based transformer models are employed. A VisualBERT ResMLP transformer encoder model with lesser model parameters is also introduced that marginally outperforms the base vision-text attention encoder model by incorporating a cross-token sub-module. The influence of the number of input image patches and the inclusion of temporal visual features on the model’s performance is also reported. While our Surgical-VQA task answers to less-complex questions, from the application standpoint, it unfolds the possibility of incorporating open-ended questions where the model could be trained to answer surgery-specific complex questionnaires. From the model standpoint, future work could focus on introducing an asynchronous training regime to incorporate the benefits of the cross-patch sub-module without affecting the self-attention sub-module in sentence-based answer-generation tasks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We thank Ms. Xu Mengya, Dr. Preethiya Seenivasan and Dr. Sampoornam Thamizharasan for their valuable inputs during project discussions. This work is supported by Shun Hing Institute of Advanced Engineering (SHIAE project BME-p1-21, 8115064) at the Chinese University of Hong Kong (CUHK), Hong Kong Research Grants Council (RGC) Collaborative Research Fund (CRF C4026-21GF and CRF C4063-18G) and (GRS)#3110167.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abacha, A.B., Hasan, S.A., Datla, V.V., Liu, J., Demner-Fushman, D.,
Müller, H.: Vqa-med: Overview of the medical visual question answering
task at imageclef 2019. clef2019 working notes. In: CEUR Workshop
Proceedings. CEUR-WS. org&lt; http://ceur-ws. org&gt;. September. pp. 9–12

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Adams, L., Krybus, W., Meyer-Ebrecht, D., Rueger, R., Gilsbach, J.M., Moesges,
R., Schloendorff, G.: Computer-assisted surgery. IEEE Computer graphics and
applications <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">10</span>(3), 43–51 (1990)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Allan, M., Kondo, S., Bodenstedt, S., Leger, S., Kadkhodamohammadi, R., Luengo,
I., Fuentes, F., Flouty, E., Mohammed, A., Pedersen, M., et al.: 2018 robotic
scene segmentation challenge. arXiv preprint arXiv:2001.11190 (2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with
improved correlation with human judgments. In: Proceedings of the acl
workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. pp. 65–72 (2005)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Barra, S., Bisogni, C., De Marsico, M., Ricciardi, S.: Visual question
answering: which investigated applications? Pattern Recognition Letters
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">151</span>, 325–331 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bates, D.W., Gawande, A.A.: Error in medicine: what have we learned? (2000)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Cornia, M., Stefanini, M., Baraldi, L., Cucchiara, R.: Meshed-memory
transformer for image captioning. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 10578–10587 (2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database. In: 2009 IEEE conference on computer
vision and pattern recognition. pp. 248–255. Ieee (2009)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hara, K., Kataoka, H., Satoh, Y.: Learning spatio-temporal features with 3d
residual networks for action recognition. In: Proceedings of the IEEE
International Conference on Computer Vision Workshops. pp. 3154–3160 (2017)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770–778 (2016)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 (2016)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Islam, M., Seenivasan, L., Ming, L.C., Ren, H.: Learning and reasoning with the
graph structure representation in robotic surgery. In: International
Conference on Medical Image Computing and Computer-Assisted Intervention. pp.
627–636. Springer (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kneebone, R.: Simulation in surgical training: educational issues and practical
implications. Medical education <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">37</span>(3), 267–277 (2003)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple
and performant baseline for vision and language. arXiv preprint
arXiv:1908.03557 (2019)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th annual meeting
of the Association for Computational Linguistics. pp. 311–318 (2002)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Rogers, D.A., Yeh, K.A., Howdieshell, T.R.: Computer-assisted learning versus a
lecture and feedback seminar for teaching a basic surgical technical skill.
The American Journal of Surgery <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">175</span>(6), 508–510 (1998)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Sarker, S., Patel, B.: Simulation and surgical training. International journal
of clinical practice <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">61</span>(12), 2120–2125 (2007)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Seenivasan, L., Mitheran, S., Islam, M., Ren, H.: Global-reasoned multi-task
learning model for surgical scene understanding. IEEE Robotics and Automation
Letters (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Sharma, D., Purushotham, S., Reddy, C.K.: Medfusenet: An attention-based
multimodal deep learning model for visual question answering in the medical
domain. Scientific Reports <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">11</span>(1), 1–18 (2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sharma, H., Jalal, A.S.: Image captioning improved visual question answering.
Multimedia Tools and Applications pp. 1–22 (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sharma, H., Jalal, A.S.: Visual question answering model based on graph neural
network and contextual attention. Image and Vision Computing <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">110</span>,
104165 (2021)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sheng, S., Singh, A., Goswami, V., Magana, J., Thrush, T., Galuba, W., Parikh,
D., Kiela, D.: Human-adversarial visual question answering. Advances in
Neural Information Processing Systems <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">34</span> (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E.,
Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et al.: Resmlp:
Feedforward networks for image classification with data-efficient training.
arXiv preprint arXiv:2105.03404 (2021)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy,
N.: Endonet: a deep architecture for recognition tasks on laparoscopic
videos. IEEE transactions on medical imaging <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">36</span>(1), 86–97 (2016)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image
description evaluation. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 4566–4575 (2015)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Wang, Z., Yu, J., Yu, A.W., Dai, Z., Tsvetkov, Y., Cao, Y.: Simvlm: Simple
visual language model pretraining with weak supervision. arXiv preprint
arXiv:2108.10904 (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Wiseman, S., Rush, A.M.: Sequence-to-sequence learning as beam-search
optimization. arXiv preprint arXiv:1606.02960 (2016)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhang, S., Chen, M., Chen, J., Zou, F., Li, Y.F., Lu, P.: Multimodal
feature-wise co-attention method for visual question answering. Information
Fusion <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">73</span>, 1–10 (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.11052" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.11053" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.11053">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.11053" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.11054" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 17:01:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
