<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.15481] Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development</title><meta property="og:description" content="The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.15481">

<!--Generated on Fri Apr  5 14:04:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="artificial intelligence,  machine learning,  AI fairness,  AI practitioners,  interviews">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aastha Pant
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aastha.pant@monash.edu">aastha.pant@monash.edu</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Monash University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">Wellington Road</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Melbourne</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_state">Victoria</span><span id="id5.5.id5" class="ltx_text ltx_affiliation_country">Australia</span><span id="id6.6.id6" class="ltx_text ltx_affiliation_postcode">3800</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rashina Hoda
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Monash University</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_streetaddress">Wellington Road</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_city">Melbourne</span><span id="id10.4.id4" class="ltx_text ltx_affiliation_state">Victoria</span><span id="id11.5.id5" class="ltx_text ltx_affiliation_country">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rashina.hoda@monash.edu">rashina.hoda@monash.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chakkrit Tantithamthavorn
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id12.1.id1" class="ltx_text ltx_affiliation_institution">Monash University</span><span id="id13.2.id2" class="ltx_text ltx_affiliation_city">Melbourne</span><span id="id14.3.id3" class="ltx_text ltx_affiliation_country">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chakkrit@monash.edu">chakkrit@monash.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Burak Turhan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_affiliation_institution">University of Oulu</span><span id="id16.2.id2" class="ltx_text ltx_affiliation_city">Oulu</span><span id="id17.3.id3" class="ltx_text ltx_affiliation_country">Finland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:burak.turhan@oulu.fi">burak.turhan@oulu.fi</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018; 20 February 2007; 12 March 2009; 5 June 2009)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id18.id1" class="ltx_p">The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical studies focused on understanding the views and experiences of AI practitioners in developing a fair AI/ML. Understanding AI practitioners’ views and experiences on the fairness of AI/ML is important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring fairness in AI/ML. We conducted semi-structured interviews with 22 AI practitioners to investigate their <em id="id18.id1.1" class="ltx_emph ltx_font_italic">understanding</em> of what a ‘fair AI/ML’ is, the <em id="id18.id1.2" class="ltx_emph ltx_font_italic">challenges</em> they face in developing a fair AI/ML, the <em id="id18.id1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML, and the <em id="id18.id1.4" class="ltx_emph ltx_font_italic">strategies</em> they employ to ensure AI/ML fairness. We developed a framework showcasing the relationship between AI practitioners’ <em id="id18.id1.5" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’ and (i) their <em id="id18.id1.6" class="ltx_emph ltx_font_italic">challenges</em> in its development, (ii) the <em id="id18.id1.7" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML, and (iii) <em id="id18.id1.8" class="ltx_emph ltx_font_italic">strategies</em> used to ensure AI/ML fairness. Additionally, we also identify areas for further investigation and offer recommendations to aid AI practitioners and AI companies in navigating fairness.</p>
</div>
<div class="ltx_keywords">artificial intelligence, machine learning, AI fairness, AI practitioners, interviews
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>JACM</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Software and its engineering Software design engineering</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, the use of AI/ML has become widespread across various domains, including recruitment, legal proceedings, credit risk forecasting, admission processes, etc <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>.
The topic of ‘fairness’ has been a subject of study in Software Engineering (SE) research for some time, predating the recent surge in AI/ML applications <cite class="ltx_cite ltx_citemacro_citep">(Finkelstein et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2008</a>)</cite>. At the same time, the importance of ‘fairness’ of AI/ML-based systems has been highlighted by several real-world incidents in recent years <cite class="ltx_cite ltx_citemacro_citep">(Majumder et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. For example, there have been AI/ML fairness issues such as Google’s ML algorithm exhibiting gender bias against women by more frequently associating men with Science, Technology, Engineering, and Mathematics (STEM) careers <cite class="ltx_cite ltx_citemacro_citep">(Prates et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>; Amazon’s AI-powered recruitment tool that was gender-biased as it preferred male candidates over female candidates based on their resumes <cite class="ltx_cite ltx_citemacro_citep">(Martin, <a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite>; a risk score predicting algorithm exhibiting significant bias against African Americans, revealing a higher error rate in predicting future criminals <cite class="ltx_cite ltx_citemacro_citep">(Angwin et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>; gender bias in <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">Google</em> <cite class="ltx_cite ltx_citemacro_citep">(Caliskan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">Bing</em> translators <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Brun, <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>. Widespread cases of software displaying unfair behavior, particularly regarding protected attributes such as gender <cite class="ltx_cite ltx_citemacro_citep">(Caliskan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and race <cite class="ltx_cite ltx_citemacro_citep">(Angwin et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>)</cite>, underscore the necessity of prioritizing ‘fairness’ in the development of AI/ML, as these instances lead to unacceptable consequences disproportionately affecting users in minority or historically disadvantaged groups.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The widespread adoption of AI/ML across different domains has raised concerns about fairness, leading to increased research and the development of guidelines and policies. Major tech companies like Google <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, Microsoft <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>)</cite>, IBM <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, and various countries/ continents, including Australia <cite class="ltx_cite ltx_citemacro_citep">(Government, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> and Europe <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, have defined ‘fairness’ as a guiding principle for AI practitioners in developing a fair AI/ML. The essence of the ‘fairness’ principle for these countries/continents and tech companies is centered around developing an inclusive AI/ML that does not discriminate against any specific individuals, groups, or communities. Along with that, several software and tools have also been developed such as IBM’s AI Fairness 360 <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib30" title="" class="ltx_ref">2024b</a>)</cite>, LinkedIn’s Fairness Toolkit (LiFT) <cite class="ltx_cite ltx_citemacro_citep">(Vasudevan and Kenthapadi, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>, and fairness checklists like Deon <cite class="ltx_cite ltx_citemacro_citep">(DrivenData, <a href="#bib.bib14" title="" class="ltx_ref">2024</a>)</cite>, Microsoft’s AI Fairness Checklist <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib39" title="" class="ltx_ref">2024a</a>)</cite>, IBM’s AI FactSheets <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib29" title="" class="ltx_ref">2024a</a>)</cite> and many more to aid AI practitioners in developing a fair AI/ML. The extensive research in the field of AI/ML fairness covers various aspects, including the proposal of methods and frameworks <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Brun, <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite>, aimed at aiding AI practitioners in the design and development of a fair AI/ML or mitigating fairness-related issues in them. Despite the development of numerous tools, frameworks, guidelines, and policies for AI/ML fairness, issues persist. Our previous study (a survey with AI practitioners) also showed that the majority of AI practitioners who participated in our study faced challenges in developing <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">fair</span> AI/ML systems because of their own biased nature <cite class="ltx_cite ltx_citemacro_citep">(Pant et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The predominant focus has been on introducing guidelines, and policies, and developing tools for AI practitioners to enhance the development of a fair AI/ML. However, it is equally important to understand the perspectives and experiences of AI practitioners who are developing such systems. This deeper understanding can play a pivotal role in uncovering the real-world challenges during the development of a fair AI/ML and this awareness can help to devise solutions that can directly address the practical needs and concerns identified by practitioners. A recent study has also reported that most studies on AI/ML fairness are conceptual and focused on technical aspects, highlighting the importance and need for research on the social/human aspects of AI in the literature <cite class="ltx_cite ltx_citemacro_citep">(Xivuri and Twinomurinzi, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Therefore, considering the importance of understanding the overall perspectives and experiences of AI practitioners in the development of a fair AI/ML, as emphasized in the literature, and taking into account the identified research gap <cite class="ltx_cite ltx_citemacro_citep">(Xivuri and Twinomurinzi, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>, we were interested in addressing this gap by conducting an empirical study with AI practitioners<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The term ‘AI practitioners’ in our study includes AI/ML developers, AI engineers, AI/ML experts, and AI/ML/ data scientists involved in the design and development activities of AI/ML. The terms ‘AI practitioners’ and ‘practitioners’ are used interchangeably throughout our study.</span></span></span>. We conducted <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">semi-structured interviews</span> with 22 AI practitioners to explore four aspects: (i) AI practitioners’ understanding of ‘fair AI/ML’, (ii) their challenges in fair AI/ML development, (iii) consequences of developing an unfair AI/ML, and (iv) their strategies<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The term ‘strategy’ in our study refers to practical, day-to-day approaches aimed at ensuring the fairness of AI/ML, rather than encompassing a broader, overarching plan or approach intended for achieving long-term goals.</span></span></span> to ensure the fairness of an AI/ML. The study aims to answer the following four research questions (RQs):</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">RQ1. What do AI practitioners understand by ‘fair AI/ML’?
<br class="ltx_break"></span>To address RQ1, we explicitly asked AI practitioners about their understanding of ‘fair AI/ML’. This approach was chosen to investigate how ‘fairness’ is understood by AI practitioners in the context of AI/ML.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">RQ2. What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges?
<br class="ltx_break"></span>To address RQ2, we inquired with AI practitioners about the overall challenges they encounter in developing a fair AI/ML, drawing insights from their experiences. Additionally, we explored the underlying factors contributing to those challenges.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">RQ3. What do AI practitioners perceive as the consequences of developing an unfair AI/ML?
<br class="ltx_break"></span>To address RQ3, we asked AI practitioners to share their perceptions of the consequences associated with developing an unfair AI/ML. The question went beyond inquiring about their experiences, also seeking their overall perspective on the consequences of developing an unfair AI/ML.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">RQ4. What strategies do AI practitioners use in ensuring the fairness of an AI/ML?
<br class="ltx_break"></span>To address RQ4, we asked AI practitioners about their practical, day-to-day approaches derived from their experience in ensuring the fairness of the AI/ML they develop.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">We used <em id="S1.p9.1.1" class="ltx_emph ltx_font_italic">Socio-Technical Grounded Theory (STGT) for data analysis</em> <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> to analyze the qualitative data. The main contributions of this study are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We investigated what AI practitioners <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">understand</em> by ‘fair AI/ML’.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We identified the <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">challenges</em> faced by AI practitioners in developing a fair AI/ML and the factors leading to those challenges.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We identified the <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by AI practitioners.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We explored the <em id="S1.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">strategies</em> used by AI practitioners to ensure the fairness of AI/ML they developed.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">We developed a <em id="S1.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">framework</em> illustrating the relationship between AI practitioners’ <em id="S1.I1.i5.p1.1.2" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’ and (i) <em id="S1.I1.i5.p1.1.3" class="ltx_emph ltx_font_italic">challenges</em> faced in developing a fair AI/ML, (ii) the <em id="S1.I1.i5.p1.1.4" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML, and (iii) <em id="S1.I1.i5.p1.1.5" class="ltx_emph ltx_font_italic">strategies</em> for ensuring the fairness of an AI/ML.</p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p">We formulated a set of recommendations for AI practitioners and AI companies to assist them in the development of fair AI/ML based on the empirical findings.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Motivation</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Definition of ‘AI fairness’</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In recent years, the concept of ‘fairness’ in AI has gained significant attention. Leading software companies such as Microsoft, Google, and IBM have either outlined principles or recommended practices to guide practitioners in developing fair AI systems. For instance, Microsoft has defined ‘fairness’ as <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">“AI systems should treat all people fairly”</em> <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>)</cite>. Likewise, IBM emphasizes the importance of minimizing bias and promoting inclusive representation in AI development <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>. Meanwhile, Google recommends concrete steps for fair AI, including setting clear goals for fairness, using representative datasets, checking systems for unfair biases, and analyzing system performance <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>. In addition to companies, various countries and continents have their definitions of the term ‘fairness’ in the context of AI. For example, Australia’s AI Ethics Principles defined the ‘fairness’ principle as <em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">“AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups”</em> <cite class="ltx_cite ltx_citemacro_citep">(Government, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. Similarly, the European Commission defined ‘Diversity, non-discrimination and fairness’ in AI as, <em id="S2.SS1.p1.1.3" class="ltx_emph ltx_font_italic">“Unfair bias must be avoided, as it could have multiple negative implications, from the marginalization of vulnerable groups to the exacerbation of prejudice and discrimination. Fostering diversity, AI systems should be accessible to all, regardless of any disability, and involve relevant stakeholders throughout their entire life circle”</em> <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Along with that, over the last 13 years, there has been extensive research on AI/ML fairness <cite class="ltx_cite ltx_citemacro_citep">(Friedler et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, and different tools, techniques, and methods to measure and mitigate fairness issues in AI/ML have been developed and evaluated. Major tech companies like Microsoft, Google, and IBM have developed software tools and techniques to enhance the development of fair AI/ML systems such as AI Fairness 360 <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib30" title="" class="ltx_ref">2024b</a>)</cite>, LinkedIn’s Fairness Toolkit (LiFT) <cite class="ltx_cite ltx_citemacro_citep">(Vasudevan and Kenthapadi, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>, and fairness checklists like Deon <cite class="ltx_cite ltx_citemacro_citep">(DrivenData, <a href="#bib.bib14" title="" class="ltx_ref">2024</a>)</cite>, Microsoft’s AI Fairness Checklist <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib39" title="" class="ltx_ref">2024a</a>)</cite>, IBM’s AI FactSheets <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib29" title="" class="ltx_ref">2024a</a>)</cite>. Furthermore, researchers have developed a variety of methods and frameworks intending to enhance the development of fair AI/ML systems, including fairness checklists <cite class="ltx_cite ltx_citemacro_citep">(Madaio et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>, frameworks <cite class="ltx_cite ltx_citemacro_citep">(Vasudevan and Kenthapadi, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>; D’Amour et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, and fairness evaluation and comparison toolkit <cite class="ltx_cite ltx_citemacro_citep">(Johnson and Brun, <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Review studies on AI/ML fairness</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In addition to defining the concept of <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">fairness</em>, several review studies have been conducted in the area of <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">fairness</em> of the AI/ML. For example, studies have been conducted to explore and review the definition of <em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">fairness</em> focused on various aspects such as ML algorithmic classification <cite class="ltx_cite ltx_citemacro_citep">(Verma and Rubin, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite>, the widely used definition in ML <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>; Chouldechova and Roth, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> and political philosophy <cite class="ltx_cite ltx_citemacro_citep">(Binns, <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>. Likewise, studies have also been conducted to compare the historical and current perspectives of fairness in ML <cite class="ltx_cite ltx_citemacro_citep">(Hutchinson and Mitchell, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>. Studies have also focused on reviewing the challenges and methodologies related to AI fairness. For example, <cite class="ltx_cite ltx_citemacro_citet">Chen et al<span class="ltx_text">.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> conducted a literature review of 59 articles to explore the challenges in ensuring AI fairness and the strategies to improve fairness in AI systems. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Xivuri and Twinomurinzi (<a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> performed a systematic literature review (SLR) with 47 articles, examining AI algorithm fairness across research methods, practices, sectors, and locations. Their findings revealed a predominance of conceptual research, primarily emphasizing the technical aspects of narrow AI, and highlighted a notable gap in research, specifically the lack of research on the social and human aspects of AI. <cite class="ltx_cite ltx_citemacro_citet">Pessach and Shmueli (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> conducted a review study on ML fairness focusing on exploring the causes of algorithmic bias, common definition, and measures of fairness. <cite class="ltx_cite ltx_citemacro_citet">Caton and Haas (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> conducted a review study to provide an overview of different approaches used to increase the fairness of ML systems. <cite class="ltx_cite ltx_citemacro_citet">Pagano et al<span class="ltx_text">.</span> (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite> conducted a systematic review to explore various aspects like datasets, fairness metrics, tools, and identification and mitigation methods of mitigating bias and unfairness in ML systems. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Bacelar (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> provided an overview of various measurement methods of bias and fairness in ML models, in their review study. <cite class="ltx_cite ltx_citemacro_citet">Wan et al<span class="ltx_text">.</span> (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite> provided a review of the currently available mitigation techniques of in-procession fairness issues in ML models. Review studies have also been conducted to address the fairness issues, the causes of biases in AI, and their consequences in the medical domain <cite class="ltx_cite ltx_citemacro_citep">(Ueda et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>)</cite>. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite> conducted a review of 95 articles to explore similarities, and differences in the understanding of fairness, influencing factors, and potential solutions for fairness integration in medical AI.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The emphasis of major tech companies and nations has largely been on working to define the concept of ‘fairness’ and develop diverse tools and techniques to assist AI practitioners in enhancing the development of fair AI/ML systems. Despite these efforts, one of the findings of our previous study (a survey with AI practitioners) was that the majority of the participants reported the challenges in developing <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">fair</span> AI/ML systems due to their own biased nature <cite class="ltx_cite ltx_citemacro_citep">(Pant et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. Given that most studies on AI/ML fairness are conceptual and focused on technical aspects, and considering the highlighted importance and need for research on the social/human aspects of AI in the literature <cite class="ltx_cite ltx_citemacro_citep">(Xivuri and Twinomurinzi, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>, we were interested in exploring the perceptions and experiences of AI practitioners regarding fair AI/ML. Investigating AI practitioners’ perceptions and experiences in developing a fair AI/ML can assist in understanding the real-world challenges associated with fair AI/ML development. Furthermore, it can aid in devising solutions to address their practical needs and concerns in developing a fair AI/ML.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Research Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our study aimed to investigate the perspectives and experiences of AI practitioners in developing a fair AI/ML. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1. Study Design ‣ 3. Research Methodology ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overview of the research methodology of our study.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Study Design</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2403.15481/assets/methodology-7.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="382" height="71" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of the research methodology of our study</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We conducted a semi-structured interview-based study which commonly allows researchers to study the complexities of human behavior such as motivation, communication, understanding, etc. to obtain rich and informative results <cite class="ltx_cite ltx_citemacro_citep">(Seaman, <a href="#bib.bib48" title="" class="ltx_ref">1999</a>)</cite>. We conducted semi-structured interviews with AI practitioners to gather insights on their <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’, the <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">challenges</em> they face in developing a fair AI/ML, <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them, and the <em id="S3.SS1.p1.1.4" class="ltx_emph ltx_font_italic">strategies</em> they take to ensure fairness of an AI/ML. The interview protocol can be found in Appendix <a href="#A1" title="Appendix A Appendix A: Interview Protocol ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. Interview planning spanned from July 2023 to October 2023. Throughout this period, tasks included defining interview objectives, refining the interview protocol through iterative processes, and prioritizing crucial interview questions. Consequently, a semi-structured interview protocol with two sections was developed.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Participant Information</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The first section of the interview protocol was formulated to gather participants’ demographic information, including their name, email, gender, age, country of residence, and educational qualifications. Employment details such as job titles, and involvement in AI/ML development activities were also collected. We employed a survey research method to gather the participants’ demographic information. Participants were also asked to provide details of their work experience in the area of AI/ML development, and those without experience were not included in the study. Each participant included in our study has at least some experience in the area of AI/ML development. Using the <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Qualtrics</span> platform, we created the survey and advertised it as an anonymous survey link following the receipt of necessary ethics approval (Reference Number: 38991). The survey questions can be found in Appendix <a href="#A1" title="Appendix A Appendix A: Interview Protocol ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>- Section A.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Understanding Participants’ Perspectives and Experiences in Developing a Fair AI/ML</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The second section of the interview protocol was designed to gather insights into participants’ perspectives and experiences in the development of a fair AI/ML. Our focus was specifically on investigating AI practitioners’ <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">understanding</span> of ‘fair AI/ML’. We did not provide a predefined definition of ‘fairness’ to participants and explicitly inquired about their understanding, aiming to assess their perspectives independently. The two key reasons for this design choice include: (i) as mentioned in section <a href="#S2" title="2. Background and Motivation ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there is no universal definition of ‘fairness’ in AI—different countries and tech companies have their own definitions of ‘fairness’ and (ii) this approach aimed to evaluate participants’ natural interpretations, avoiding influence from a predetermined definition. We also aimed at identifying their <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">challenges</span> in developing fair AI/ML and the factors leading to those challenges, understanding the <span id="S3.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">consequences</span> of developing an unfair AI/ML from the participants’ viewpoint, and exploring the <span id="S3.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_italic">strategies</span> they employ to ensure fairness of an AI/ML. The interview questions can be found in Appendix <a href="#A1" title="Appendix A Appendix A: Interview Protocol ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>- Section B.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Pilot study</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">After designing the interview protocol, we executed a pilot study, engaging two AI practitioners—one from industry and another from academia—identified through our professional networks. The purpose was to confirm the clarity and understandability of the interview questions, assess the time required to complete the study and gather feedback for enhancing the interview process. Both participants possessed expertise in AI/ML development. Taking into account their feedback, we made slight modifications to the interview questions to enhance clarity, ultimately finalizing the interview protocol.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Interview Sampling and Data Collection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We used purposive sampling in our study to select the participants <cite class="ltx_cite ltx_citemacro_citep">(Baltes and Ralph, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. By using this method, we were able to specifically target our desired group of participants, namely AI practitioners involved in AI/ML development activities.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We conducted data collection in two rounds. In the first round, participation was voluntary. After we got the ethics approval, we advertised our study on social media platforms such as LinkedIn and Twitter, as well as within our professional networks. We specifically targeted AI practitioners engaged in AI/ML development activities. In the first round, we received interest from only 3 candidates for participating in our study. So, after obtaining ethics approval, we decided to conduct a second round of data collection and introduced a reward— an AUD 50 gift card voucher— to incentivize participation. The second round, advertised again on social media like LinkedIn and Twitter with mention of the reward, resulted in responses from 19 suitable candidates, bringing the total number of participants to 22. Since our goal was to recruit participants with some experience in AI/ML development, we incorporated two employment-related questions, inquiring about their years of experience in the field and their level of involvement in various job responsibilities. Participants received a reward of AUD 50 upon the completion of data collection. Since we advertised our study on social media, we obtained responses from various countries worldwide, as illustrated in Table <a href="#S4.T1" title="Table 1 ‣ 4.1. Participants’ Demographics ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Since we did not favour specific countries, the responses were spread out across different regions. We obtained a majority of the responses from Australia (13), followed by the responses from other countries like Nepal (3), Israel (1), Japan (1), USA (1) etc. We present an in-depth analysis of the participants’ demographics in Section <a href="#S4.SS1" title="4.1. Participants’ Demographics ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We gathered qualitative data through semi-structured interviews with 22 AI practitioners experienced in AI/ML development. All interviews were conducted online using Zoom and were audio-recorded. Each interview lasted between 40 and 45 minutes.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Data Analysis</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In our study, qualitative data was gathered via semi-structured interviews, and consequently, a qualitative approach was employed for data analysis. <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Socio-Technical Grounded Theory (STGT) for data analysis</em> was used to analyze the data, as it is particularly suitable for analyzing open-ended data and gaining insights within socio-technical contexts <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. After obtaining consent from each participant, we transcribed the data. The data collection and analysis phases involved an iterative process as shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1. Study Design ‣ 3. Research Methodology ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Initially, we analyzed the data from 13 participants. Building on the primary findings from this analysis, we then proceeded to collect data from the remaining 9 participants, concentrating on the key insights derived from the first round of data analysis, and analyzed them.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We performed <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">open coding</em> to develop concepts and categories, involving constant comparison of diverse open-text responses <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. We performed inductive open coding within the RQs. For example, to answer our RQ2 which is, <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_italic">What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges?</span>, we gathered qualitative data from 22 participants by asking them, <em id="S3.SS3.p2.1.3" class="ltx_emph ltx_font_italic">“Do you face any challenges in developing a fair AI/ML? If yes, what challenges do you face? What do you think are the factors leading to those challenges?”</em> We developed codes using the open-coding approach in open-text answers as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3. Data Analysis ‣ 3. Research Methodology ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For instance, codes like <em id="S3.SS3.p2.1.4" class="ltx_emph ltx_font_italic">‘access to limited data’</em> and <em id="S3.SS3.p2.1.5" class="ltx_emph ltx_font_italic">‘lack of data access’</em> were identified through open coding. Subsequently, we engaged in constant comparison of these codes to continually compare them, leading to the recognition of patterns among them. For instance, upon reviewing the codes mentioned above, we identified a common pattern related to the challenge of accessing datasets required in the development of a fair AI/ML. We combined these two codes to develop a concept of <em id="S3.SS3.p2.1.6" class="ltx_emph ltx_font_italic">‘gaining access to datasets’</em>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.15481/assets/stgt3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="331" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Examples of STGT analysis <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> applied to qualitative data on the <em id="S3.F2.2.1" class="ltx_emph ltx_font_italic">challenges</em> in developing a fair AI/ML.</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Using the same constant comparison approach for other codes, we derived concepts such as <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">‘balancing ideal vs real’</em>, <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">‘handling data-related issues’</em>, and <em id="S3.SS3.p3.1.3" class="ltx_emph ltx_font_italic">‘following policies and regulations’</em>. We again constantly compared these concepts with one another and developed distinct categories. In this context, these four concepts shared a challenge associated with the <em id="S3.SS3.p3.1.4" class="ltx_emph ltx_font_italic">process</em> of developing a fair AI/ML, leading us to establish a category known as <em id="S3.SS3.p3.1.5" class="ltx_emph ltx_font_italic">‘process-related challenges’</em>.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Likewise, we identified multiple codes and concepts addressing the challenges associated with the resources required for developing a fair AI/ML. This process led to the development of another high-level category, namely, <em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">‘resource-related challenges’</em>. In this way, we established a total of three categories encapsulating the challenges faced by AI practitioners in developing a fair AI/ML, namely, <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_bold">process-related challenges</span>, <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_bold">resource-related challenges</span>, and <span id="S3.SS3.p4.1.4" class="ltx_text ltx_font_bold">team-related challenges</span>. Detailed information on these challenges is provided in Section <a href="#S4" title="4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">All four authors were involved in designing the interview questionnaire. However, the first author led the data analysis with detailed feedback from the second author and regular feedback from the third and fourth authors. After the qualitative data were analyzed, the results, including codes, concepts, and categories, were shared and discussed among all authors, who collectively contributed to presenting the findings.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">The <em id="S3.SS3.p6.1.1" class="ltx_emph ltx_font_italic">STGT for data analysis</em> encompasses steps of open coding, constant comparison, and memoing. <em id="S3.SS3.p6.1.2" class="ltx_emph ltx_font_italic">“Basic memoing is the process of documenting the researcher’s thoughts, ideas, and reflections on emerging concepts and (sub)categories and evidence-based conjectures on possible links between them”</em> <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. Consequently, we wrote memos to record significant insights and reflections discovered during the open coding activities. An illustration of a memo created for AI practitioners’ description of ‘fair AI/ML’, specifically ‘in terms of the absence of <em id="S3.SS3.p6.1.3" class="ltx_emph ltx_font_italic">bias</em>’ and ‘in terms of the presence of desirable <em id="S3.SS3.p6.1.4" class="ltx_emph ltx_font_italic">attributes</em>’, is provided in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3. Data Analysis ‣ 3. Research Methodology ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The discussion on the key insights derived from memoing is presented in Section <a href="#S5.SS5" title="5.5. Insights ‣ 5. Discussion ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2403.15481/assets/memo4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="335" height="116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>An example of a memo on AI practitioners’ understanding of ‘fair AI/ML’.”</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Findings</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Participants’ Demographics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We present the demographic information of the participants in this section. Table <a href="#S4.T1" title="Table 1 ‣ 4.1. Participants’ Demographics ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an overview of the participants’ demographics based on their age, gender, country, education, work experience in AI/ML development activities, and job title. We used identifiers such as P1, P2, P3, and so forth to represent the participants in our study.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Demographics of the Interview Participants</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">P_Id</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Age Range (years)</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Gender</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.1.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Country</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.1.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Education</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.1.1.6.1.1.1" class="ltx_text" style="font-size:70%;">Exp. in AI/ML (years)</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.1.1.7.1.1.1" class="ltx_text" style="font-size:70%;">Job Title</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">P1</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:70%;">20-25</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.2.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.2.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Nepal</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.2.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Bachelor</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.2.1.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.2.1.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.3.2.1.1.1.1" class="ltx_text" style="font-size:70%;">P2</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:70%;">20-25</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.3.2.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.3.2.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.3.2.5.1.1.1" class="ltx_text" style="font-size:70%;">Bachelor</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.3.2.6.1.1.1" class="ltx_text" style="font-size:70%;">0-1</span></span>
</span>
</td>
<td id="S4.T1.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.3.2.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">P3</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.4.3.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.4.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.4.3.4.1.1.1" class="ltx_text" style="font-size:70%;">Thailand</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.4.3.5.1.1.1" class="ltx_text" style="font-size:70%;">Ph.D. or higher</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.4.3.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.4.3.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Research Scientist</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">P4</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.5.4.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.5.4.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.5.4.4.1.1.1" class="ltx_text" style="font-size:70%;">India</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.5.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Bachelor</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.5.4.6.1.1.1" class="ltx_text" style="font-size:70%;">More than 5</span></span>
</span>
</td>
<td id="S4.T1.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.5.4.7.1.1.1" class="ltx_text" style="font-size:70%;">Data Scientist</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.6.5.1.1.1.1" class="ltx_text" style="font-size:70%;">P5</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.6.5.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.6.5.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.6.5.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.6.5.6.1.1.1" class="ltx_text" style="font-size:70%;">More than 5</span></span>
</span>
</td>
<td id="S4.T1.1.6.5.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.6.5.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.7.6.1.1.1.1" class="ltx_text" style="font-size:70%;">P6</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.7.6.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.7.6.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.7.6.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.7.6.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.7.6.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.7.6.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.7.6.7.1.1.1" class="ltx_text" style="font-size:70%;">Data Scientist</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.8.7.1.1.1.1" class="ltx_text" style="font-size:70%;">P7</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.8.7.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.8.7.3.1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.8.7.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.8.7.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.8.7.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.8.7.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.8.7.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.9.8.1.1.1.1" class="ltx_text" style="font-size:70%;">P8</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.9.8.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.9.8.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.9.8.4.1.1.1" class="ltx_text" style="font-size:70%;">Nepal</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.9.8.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.9.8.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.9.8.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.9.8.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.10.9.1.1.1.1" class="ltx_text" style="font-size:70%;">P9</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.10.9.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.10.9.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.10.9.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.10.9.5.1.1.1" class="ltx_text" style="font-size:70%;">Ph.D. or higher</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.10.9.6.1.1.1" class="ltx_text" style="font-size:70%;">More than 5</span></span>
</span>
</td>
<td id="S4.T1.1.10.9.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.10.9.7.1.1.1" class="ltx_text" style="font-size:70%;">Data Scientist</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.11.10.1.1.1.1" class="ltx_text" style="font-size:70%;">P10</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.11.10.2.1.1.1" class="ltx_text" style="font-size:70%;">46-50</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.11.10.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.11.10.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.11.10.5.1.1.1" class="ltx_text" style="font-size:70%;">Ph.D. or higher</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.11.10.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.11.10.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.11.10.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.12.11" class="ltx_tr">
<td id="S4.T1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.12.11.1.1.1.1" class="ltx_text" style="font-size:70%;">P11</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.12.11.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.12.11.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.12.11.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.12.11.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.12.11.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.12.11.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.12.11.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.13.12" class="ltx_tr">
<td id="S4.T1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.13.12.1.1.1.1" class="ltx_text" style="font-size:70%;">P12</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.13.12.2.1.1.1" class="ltx_text" style="font-size:70%;">20-25</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.13.12.3.1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.13.12.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.13.12.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.13.12.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.13.12.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.13.12.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.14.13" class="ltx_tr">
<td id="S4.T1.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.14.13.1.1.1.1" class="ltx_text" style="font-size:70%;">P13</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.14.13.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.14.13.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.14.13.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.14.13.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.14.13.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.14.13.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.14.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.14.13.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.15.14" class="ltx_tr">
<td id="S4.T1.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.15.14.1.1.1.1" class="ltx_text" style="font-size:70%;">P14</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.15.14.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.15.14.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.15.14.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.15.14.5.1.1.1" class="ltx_text" style="font-size:70%;">Ph.D. or higher</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.15.14.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.15.14.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.15.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.15.14.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.15.14.7.1.1.1" class="ltx_text" style="font-size:70%;">Data Scientist</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.16.15" class="ltx_tr">
<td id="S4.T1.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.16.15.1.1.1.1" class="ltx_text" style="font-size:70%;">P15</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.16.15.2.1.1.1" class="ltx_text" style="font-size:70%;">46-50</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.16.15.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.16.15.4.1.1.1" class="ltx_text" style="font-size:70%;">Japan</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.16.15.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.16.15.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.16.15.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.16.15.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.16.15.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.16.15.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.17.16" class="ltx_tr">
<td id="S4.T1.1.17.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.17.16.1.1.1.1" class="ltx_text" style="font-size:70%;">P16</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.17.16.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.17.16.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.17.16.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.17.16.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.17.16.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.17.16.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.17.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.17.16.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.17.16.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.18.17" class="ltx_tr">
<td id="S4.T1.1.18.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.18.17.1.1.1.1" class="ltx_text" style="font-size:70%;">P17</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.18.17.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.18.17.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.18.17.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.18.17.5.1.1.1" class="ltx_text" style="font-size:70%;">Bachelor</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.18.17.6.1.1.1" class="ltx_text" style="font-size:70%;">1-2</span></span>
</span>
</td>
<td id="S4.T1.1.18.17.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.18.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.18.17.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.18.17.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.19.18" class="ltx_tr">
<td id="S4.T1.1.19.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.19.18.1.1.1.1" class="ltx_text" style="font-size:70%;">P18</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.19.18.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.19.18.3.1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.19.18.4.1.1.1" class="ltx_text" style="font-size:70%;">Australia</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.19.18.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.19.18.6.1.1.1" class="ltx_text" style="font-size:70%;">0-1</span></span>
</span>
</td>
<td id="S4.T1.1.19.18.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.19.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.19.18.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.19.18.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.20.19" class="ltx_tr">
<td id="S4.T1.1.20.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.20.19.1.1.1.1" class="ltx_text" style="font-size:70%;">P19</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.20.19.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.20.19.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.20.19.4.1.1.1" class="ltx_text" style="font-size:70%;">Vietnam</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.20.19.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.20.19.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.20.19.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.20.19.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.20.19.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.20.19.7.1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.21.20" class="ltx_tr">
<td id="S4.T1.1.21.20.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.21.20.1.1.1.1" class="ltx_text" style="font-size:70%;">P20</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.21.20.2.1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.21.20.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.21.20.4.1.1.1" class="ltx_text" style="font-size:70%;">Israel</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.21.20.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.21.20.6.1.1.1" class="ltx_text" style="font-size:70%;">More than 5</span></span>
</span>
</td>
<td id="S4.T1.1.21.20.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.21.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.21.20.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.21.20.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Expert</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.22.21" class="ltx_tr">
<td id="S4.T1.1.22.21.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.22.21.1.1.1.1" class="ltx_text" style="font-size:70%;">P21</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.22.21.2.1.1.1" class="ltx_text" style="font-size:70%;">20-25</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.22.21.3.1.1.1" class="ltx_text" style="font-size:70%;">Man</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.22.21.4.1.1.1" class="ltx_text" style="font-size:70%;">Nepal</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.22.21.5.1.1.1" class="ltx_text" style="font-size:70%;">Bachelor</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.22.21.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.22.21.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.22.21.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.22.21.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.22.21.7.1.1.1" class="ltx_text" style="font-size:70%;">ML Engineer</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.23.22" class="ltx_tr">
<td id="S4.T1.1.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T1.1.23.22.1.1.1.1" class="ltx_text" style="font-size:70%;">P22</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.2.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.23.22.2.1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.23.22.3.1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.23.22.4.1.1.1" class="ltx_text" style="font-size:70%;">USA</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S4.T1.1.23.22.5.1.1.1" class="ltx_text" style="font-size:70%;">Master</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.6.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T1.1.23.22.6.1.1.1" class="ltx_text" style="font-size:70%;">3-5</span></span>
</span>
</td>
<td id="S4.T1.1.23.22.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T1.1.23.22.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.23.22.7.1.1" class="ltx_p" style="width:62.6pt;"><span id="S4.T1.1.23.22.7.1.1.1" class="ltx_text" style="font-size:70%;">Data Scientist</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">A total of 22 AI practitioners took part in our study including 17 men and 5 women. Moreover, the majority of participants (8 out of 22) fell into the age group of 26-30 years and 31-35 years each, while only 2 belonged to the age group of 46 to 50 years. In terms of experience, the majority (9 participants) had 3-5 years of experience in AI/ML development activities, whereas only 2 participants had up to 1 year of experience. The geographical distribution indicated that the majority were from Australia (13 participants), with 3 participants from Nepal and 1 each from India, Japan, USA, Israel, Thailand, and Vietnam. Similarly, we inquired about the participants’ job titles or roles within their companies. The majority of participants held the title of ‘ML Engineer’ (9 out of 22), followed by ‘AI Engineer’ (6 out of 22), ‘Data Scientist’ (5 out of 22), and one participant each for ‘AI Research Scientist’ and ‘ML Expert’. As our target interview participants were practitioners involved in AI/ML development activities, we wanted to know the major AI/ML development-related activities they were involved in. Among the 22 participants, the majority engaged in ‘Data cleaning’ (19 participants), followed by ‘Model requirements’, ‘Data collection’, ‘Model training’, ‘Model evaluation’, and ‘Model deployment’ activities, each having 17 participants out of 22. 5 out of 22 participants chose the ‘Other’ option and elaborated on activities they engaged in through open-ended answers— activities that were not initially listed in the survey question. Some of the mentioned activities included ‘system design’, ‘data pipelines’, ‘business benefit monitoring and reporting’, ‘model integration developed by the research team into the pipelines’, and ‘pipeline deployment’.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>RQ 1- What do AI practitioners <em id="S4.SS2.1.1" class="ltx_emph ltx_font_italic">understand</em> by ‘fair AI/ML’?</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Based on the responses, we grouped the participants’ understanding of ‘fair AI/ML’ into two categories including, (i) In terms of absence of <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">bias</em> and (ii) In terms of presence of desirable <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">attributes</em>, which are explained in detail below. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2. RQ 1- What do AI practitioners understand by ‘fair AI/ML’? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the overview of the participant’s understanding of ‘fair AI/ML’.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.15481/assets/understanding3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="266" height="74" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Overview of the participant’s understanding of ‘fair AI/ML’</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>In terms of absence of bias</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">When a person is tasked with comprehending the concept of ‘fairness’, it is quite probable that their understanding will revolve around the absence of biases as defined by tech companies such as Google <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, IBM <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, and countries like Australia <cite class="ltx_cite ltx_citemacro_citep">(Government, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. When the participants were asked to share their understanding of ‘fair AI/ML’, [P2, P9, P10, P11, P13, P14, P15, P18, P19, and P20] described ‘fair AI/ML’ in terms of absence of <em id="S4.SS2.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">bias</em> in the AI/ML system. For example, participants [P9] and [P11] said:</p>
<blockquote id="S4.SS2.SSS1.p1.2" class="ltx_quote">
<span id="S4.SS2.SSS1.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS2.SSS1.p1.2.2" class="ltx_p"><em id="S4.SS2.SSS1.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“A fair model is a model which is not skewed and not biased.”</em><span id="S4.SS2.SSS1.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P9]
<br class="ltx_break"><span id="S4.SS2.SSS1.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS2.SSS1.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“So in my opinion, I guess like a fair model should be something which decreases the bias, as I mean, there should be very less bias.”</em> - [P11]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>In terms of the presence of desirable attributes:</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">[P1, P3, P4, P5, P6, P7, P8, P12, P14, P16, P17, P21 and P22] described ‘fair AI/ML’ in terms of its features or attributes. The participants framed it as the necessary elements an AI/ML must possess to be considered as <em id="S4.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">fair</em>. For example, the participants said that the AI/ML should be reproducible [P3], transparent and explainable [P4, P12, P17, P21], interpretable [P21], and accurate [P1, P7, P8, P16, P22]. Some participants also mentioned that a fair AI/ML should use a good amount of data [P6] and have proper algorithms [P14]. For example, [P3], [P4], [P7], and [P14] said,</p>
<blockquote id="S4.SS2.SSS2.p1.2" class="ltx_quote">
<span id="S4.SS2.SSS2.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS2.SSS2.p1.2.2" class="ltx_p"><em id="S4.SS2.SSS2.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“But to me fairness is more about whether or not it is reproducible. It’s something that can be tested and checked and improved from that again.”</em><span id="S4.SS2.SSS2.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P3] 
<br class="ltx_break"><span id="S4.SS2.SSS2.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS2.SSS2.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“There should be transparency in any fair model that you build, right? So it should explain why it throws a certain outcome.”</em> - [P4] 
<br class="ltx_break"><span id="S4.SS2.SSS2.p1.2.2.2.3" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS2.SSS2.p1.2.2.2.4" class="ltx_emph ltx_font_italic">“Fair model my understanding that could work with different data. It should still like give proper accurate results. There shouldn’t be a huge difference between the seen data and unseen data.”</em> - [P7]
<br class="ltx_break"><span id="S4.SS2.SSS2.p1.2.2.2.5" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS2.SSS2.p1.2.2.2.6" class="ltx_emph ltx_font_italic">“A fair model should have proper algorithms which support you to treat your groups fairly and maybe post-processing stage where you when you’re applying business logic.”</em> - [P14] 
<br class="ltx_break"></span></p>
</blockquote>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>RQ 2- What <em id="S4.SS3.1.1" class="ltx_emph ltx_font_italic">challenges</em> do AI practitioners face in developing a fair AI/ML and what are the <em id="S4.SS3.2.2" class="ltx_emph ltx_font_italic">factors</em> that lead to those challenges?</h3>

<figure id="S4.F5" class="ltx_figure"><img src="/html/2403.15481/assets/challenge10.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="774" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Overview of the participants’ challenges in developing a fair AI/ML</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We also asked the participants about the challenges they face in developing a fair AI/ML through an open-ended question. We categorized the challenges of AI practitioners into three categories which are, (i) Process-related challenges, (ii) Resource-related challenges, and (iii) Team-related challenges, based on the responses of the participants. Each category is underpinned by multiple concepts and codes which are explained in detail below. Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the overview of the challenges faced by AI practitioners in developing a fair AI/ML.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Once we inquired with the participants regarding the challenges they encountered in developing a fair AI/ML, we delved further to understand the <em id="S4.SS3.p2.1.1" class="ltx_emph ltx_font_italic">factors</em> leading to those challenges. Gaining insights into the factors leading to the challenges could contribute to devising more effective strategies to assist AI practitioners in overcoming those challenges. Additionally, we have highlighted the <em id="S4.SS3.p2.1.2" class="ltx_emph ltx_font_italic">factors</em> leading to each challenge within the quotes of the participants.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Process-related challenges</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">The participants shared the challenges they encountered in developing fair AI/ML, specifically about the <em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">process</em> of developing such systems. The participants reported four key challenges (concepts) under this category which include, (i) Gaining access to datasets, (ii) Balancing ideal vs real, (iii) Handling data-related issues, and (iv) Following policies and regulations. Each of these concepts is underpinned by multiple codes which are discussed below.</p>
</div>
</section>
<section id="S4.SS3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Gaining access to datasets</h4>

<div id="S4.SS3.SSSx1.p1" class="ltx_para">
<p id="S4.SS3.SSSx1.p1.1" class="ltx_p">AI practitioners involved in the development of AI/ML might face limitations in accessing vital resources for their work. Factors like adherence to company rules and regulations can impede their access to necessary resources, leading to challenges that, in turn, may contribute to the development of an unfair AI/ML. In our study, [P4, P8, P12, and P14] reported the challenge of gaining access to the datasets they require to train a AI/ML model. The factors that led to the challenge of gaining access to the datasets include the size of the organization and the data confidentiality policy. For example, participants [P4] and [P8] said,</p>
<blockquote id="S4.SS3.SSSx1.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx1.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx1.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx1.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“Like as a data scientist, I would have access to a very certain amount of data which I can pick. For example, some of the data would be from the external side or something you would not have access to that team data. Sometimes when you work due to <span id="S4.SS3.SSSx1.p1.2.2.1.1" class="ltx_text ltx_font_bold">data confidentiality</span> policy, you will not have access to most of your data points.”</em><span id="S4.SS3.SSSx1.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P4] 
<br class="ltx_break"><span id="S4.SS3.SSSx1.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx1.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Data is the one thing which models are built on but they are not available for public access, right? Like the open AI is things.. that language models are trained on the data sets, but data that are not available for <span id="S4.SS3.SSSx1.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">our cases (small companies)</span>.”</em> - [P8]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Balancing ideal vs real</h4>

<div id="S4.SS3.SSSx2.p1" class="ltx_para">
<p id="S4.SS3.SSSx2.p1.1" class="ltx_p">#Real-world data vs training data: The desire to develop a perfect AI/ML is different from the ability to develop it. Training an AI/ML with an extensive array of real-world data can be impractical. AI practitioners must rely on initial training data, and the system subsequently interacts with real-world data after deployment. Consequently, achieving fairness in AI/ML requires AI practitioners to navigate a balance between the training data and real-world data. This equilibrium can prove challenging at times due to a variety of limiting factors. In our study, [P8, P9, P10, P12, P15, P16, and P19] reported the challenge of striking a balance between the real-world data and the training data that they use in the development of AI/ML. The factors that led to the challenge of striking a balance between the real-world data and the training datasets include gaps existing between the real-world data and training data and negligence of the AI practitioners. For example, participants [P10] and [P15] said,</p>
<blockquote id="S4.SS3.SSSx2.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx2.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx2.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx2.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“Because the collected data is only a <span id="S4.SS3.SSSx2.p1.2.2.1.1" class="ltx_text ltx_font_bold">subset of the data in real world</span>. So, the distribution of collected data is not identical to the real-world distribution, even if we do some data augmentation such as oversampling or other generative technique, we cannot ensure the data distribution of augmented data are identical to the distribution in real world. So we can just assume it is approximately identical, but they are not perfectly, identical. So it can be still a huge, huge challenge.”</em><span id="S4.SS3.SSSx2.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P10]
<br class="ltx_break"><span id="S4.SS3.SSSx2.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx2.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Originally, in the project we made, we were trying to do some stuff on human photos. And so we had to augment our data, but the way that the <span id="S4.SS3.SSSx2.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">data was created was not correct</span>, it didn’t actually match the real data, like real photos. And so the AI actually learned to tell the difference between the synthetic data and the real data and could tell the difference between a fake photo and a real photo. And so that was like a kind of an eye opener for us that there was a bias from introduced from the synthetic data.”</em> - [P15] 
<br class="ltx_break"></span></p>
</blockquote>
<p id="S4.SS3.SSSx2.p1.3" class="ltx_p">#Fairness requirements vs technical constraints: Developing an AI/ML may appear straightforward, but it is a highly intricate undertaking. AI practitioners may face considerable challenges when trying to balance their envisioned ideal AI/ML system with the challenges of the real world. In our study, [P4, P11, P14, P16, P17, and P19] mentioned the challenge they face in maintaining a balance between their requirements for developing a fair AI/ML with the technical constraints they encounter. They discussed factors such as complex nature of AI and lack of time as contributing to this challenge. For example, [P16] and [P19] said:</p>
<blockquote id="S4.SS3.SSSx2.p1.4" class="ltx_quote">
<span id="S4.SS3.SSSx2.p1.4.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx2.p1.4.2" class="ltx_p"><em id="S4.SS3.SSSx2.p1.4.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“And actually, at first, we do not talk about the machine learning bias. At first, we talk about the production because we are practitioners and it is more important, I mean, the <span id="S4.SS3.SSSx2.p1.4.2.1.1" class="ltx_text ltx_font_bold">working version is the most important</span>.”</em><span id="S4.SS3.SSSx2.p1.4.2.2" class="ltx_text" style="font-size:90%;"> - [P16]
<br class="ltx_break"><span id="S4.SS3.SSSx2.p1.4.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx2.p1.4.2.2.2" class="ltx_emph ltx_font_italic">“So as I mentioned, we tend to have like the model that works first, then we’ll look at the virus later. In the industry here, every project has a <span id="S4.SS3.SSSx2.p1.4.2.2.2.1" class="ltx_text ltx_font_bold">deadline and lifetime</span>. So if we don’t launch the products, the stakeholders might not be happy and then they could find some other people who can do that.”</em> - [P19]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Handling data-related issues</h4>

<div id="S4.SS3.SSSx3.p1" class="ltx_para">
<p id="S4.SS3.SSSx3.p1.1" class="ltx_p">#Detecting data bias: One of the main aspects of developing any AI/ML is data, presenting a significant challenge for AI practitioners in addressing data-related issues during the development of fair AI/ML. Effectively managing biases in the data necessitates the initial step of detection, and corrective measures can only be taken once the biases are identified. The detection of data bias may pose a formidable challenge for AI practitioners, influenced by various factors. In our study, [P3, P6, P9, P10, P13, P14, and P15] reported the challenge of detecting data bias during the development of a fair AI/ML. This challenge was led by factors like the nature of data bias, lack of time, and lack of software/tools/methods. For example, [P9] and [P13] said:</p>
<blockquote id="S4.SS3.SSSx3.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx3.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx3.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx3.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“You’re working on a <span id="S4.SS3.SSSx3.p1.2.2.1.1" class="ltx_text ltx_font_bold">limited timeline project</span>. So your priority is to have a working model. And sometimes you might not be able to discover such data biases.”</em><span id="S4.SS3.SSSx3.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P9]
<br class="ltx_break"><span id="S4.SS3.SSSx3.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx3.p1.2.2.2.2" class="ltx_emph ltx_font_italic">If we have <span id="S4.SS3.SSSx3.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">data bias checking tool</span> to detect biases automatically, it would be great.”</em> - [P13]</span></p>
</blockquote>
</div>
<div id="S4.SS3.SSSx3.p2" class="ltx_para">
<p id="S4.SS3.SSSx3.p2.1" class="ltx_p">#Addressing data bias: Simply identifying data bias is not sufficient for developing a fair AI/ML; it is crucial to actively address and rectify these biases. Dealing with identified bias in the data becomes the subsequent step and it can be challenging for AI practitioners due to several factors. In our study, [P9, P11, P14, and P15] discussed that they feel it is challenging to address (mitigate and/or remove) biases from the data during the development of AI/ML. The factors leading to the challenge of addressing data bias include a lack of software/tools/methods and the biased nature of team members. For example, [P11] said,</p>
<blockquote id="S4.SS3.SSSx3.p2.2" class="ltx_quote">
<span id="S4.SS3.SSSx3.p2.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx3.p2.2.2" class="ltx_p"><em id="S4.SS3.SSSx3.p2.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“<span id="S4.SS3.SSSx3.p2.2.2.1.1" class="ltx_text ltx_font_bold">If there was some kind of tool</span> which can let the person who is training the model know, maybe you need to remove this data, or else maybe you need to do these kinds of operations on your data, or maybe you need to do something, obviously, anyone wants that kind of tool as removing bias from the data is hard.”</em><span id="S4.SS3.SSSx3.p2.2.2.2" class="ltx_text" style="font-size:90%;"> - [P11]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Following policies and regulations</h4>

<div id="S4.SS3.SSSx4.p1" class="ltx_para">
<p id="S4.SS3.SSSx4.p1.1" class="ltx_p">AI practitioners are required to adhere to various policies and regulations, which serve as guiding principles in developing AI/ML. Nevertheless, if these policies and regulations fall short, they can pose challenges to AI practitioners and can impede the development of a fair AI/ML. Likewise, in our study, [P4, P5, P7, P9, and P12] expressed challenges in adhering to policies and regulations concerning AI ethics while working on the development of AI/ML. This challenge arose due to factors like lack of policies and lack of implementation of the policies. For example, [P5] and [P9] quoted:</p>
<blockquote id="S4.SS3.SSSx4.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx4.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx4.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx4.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“I don’t think Australia has any <span id="S4.SS3.SSSx4.p1.2.2.1.1" class="ltx_text ltx_font_bold">updated AI ethics policies</span> and stuff, maybe, they need to update the policies based on how to follow the current trend and to follow the current technologies, something like that.”</em><span id="S4.SS3.SSSx4.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P5] 
<br class="ltx_break"><span id="S4.SS3.SSSx4.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx4.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“The European Alliance introduced a responsible AI framework but <span id="S4.SS3.SSSx4.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">not sure if Australia has adapted</span> such things in organisations. I know that companies like Facebook, are currently adapting. When it comes to small companies, small organizations, I’m not quite sure.”</em> - [P9]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Resource-related challenges</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Participants encountering challenges in developing a fair AI/ML face issues primarily associated with the <em id="S4.SS3.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">resources</em> used in the development process, constituting the second category of challenges. The participants reported two key challenges (concepts) under this category which include, (i) Obtaining required datasets and (ii) Obtaining other resources. Each of these concepts is underpinned by multiple codes which are discussed below.</p>
</div>
</section>
<section id="S4.SS3.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Obtaining required datasets</h4>

<div id="S4.SS3.SSSx5.p1" class="ltx_para">
<p id="S4.SS3.SSSx5.p1.1" class="ltx_p">As previously mentioned, data plays a crucial role in the development of AI/ML. In our study, the majority of the participants [P1, P3, P4, P5, P8, P9, P11, P13, P14, P15, P18, P20, P21, and P22] pointed out that obtaining the required datasets to develop a fair AI/ML is challenging. The participants noted that they often acquire imperfect (non-representative) datasets or incomplete datasets for developing AI/ML, presenting a challenge in ensuring the fairness of such systems. The factors leading to this challenge include a lack of representative datasets, lack of cost, lack of software/tools/methods, lack of control over data collection, and negligence of AI practitioners. For example, [P3], [P11] and [P20] said:</p>
<blockquote id="S4.SS3.SSSx5.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx5.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx5.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx5.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“I think bias could happen since the first step because sometimes we have <span id="S4.SS3.SSSx5.p1.2.2.1.1" class="ltx_text ltx_font_bold">other teams to collect the data</span>, yes, we don’t know what they actually provide to us.”</em><span id="S4.SS3.SSSx5.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P3]
<br class="ltx_break"><span id="S4.SS3.SSSx5.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx5.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“It has got something to do with the data, but then from the very beginning, like black people are discriminated in the world. So, I feel like the reason might be because, in the world, we have <span id="S4.SS3.SSSx5.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">more white people images than black people images</span>. So the data itself is less and it is a proven thing.”</em> - [P11] 
<br class="ltx_break"><span id="S4.SS3.SSSx5.p1.2.2.2.3" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx5.p1.2.2.2.4" class="ltx_emph ltx_font_italic">“Should I buy this data? Or not? It’s money. In most cases, you don’t use data for free. You know, sometimes you need to actually <span id="S4.SS3.SSSx5.p1.2.2.2.4.1" class="ltx_text ltx_font_bold">pay for this in some way</span>.”</em> - [P20]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSSx6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Obtaining other resources</h4>

<div id="S4.SS3.SSSx6.p1" class="ltx_para">
<p id="S4.SS3.SSSx6.p1.1" class="ltx_p">#Technological requirements: In addition to datasets, AI practitioners have diverse technological needs, including high-quality hardware, to augment the development of a fair AI/ML. Technology plays a vital role in assisting AI practitioners in the development process. However, acquiring the necessary technology can present a challenge, potentially impeding the development of a fair AI/ML. Only [P8, P11, and P12] indicated that they encounter challenges in developing fair AI/ML because they lack the necessary technology needed for the development process. The participants discussed the lack of cost as a factor leading to this challenge. For example, [P11] said:</p>
<blockquote id="S4.SS3.SSSx6.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx6.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx6.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx6.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“And resources because some huge models require huge GPUs. So in our company, we do not have GPUs because they are <span id="S4.SS3.SSSx6.p1.2.2.1.1" class="ltx_text ltx_font_bold">expensive</span>. So yeah, lack of such resources of course is a challenge.”</em><span id="S4.SS3.SSSx6.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P11]</span></p>
</blockquote>
</div>
<div id="S4.SS3.SSSx6.p2" class="ltx_para">
<p id="S4.SS3.SSSx6.p2.1" class="ltx_p">#Human-related requirements: Developing any software is a collaborative effort, involving multiple teams within a company dedicated to specific tasks. Collaborating with various team members offers advantages, such as diverse assistance in different aspects. Nonetheless, not all companies may incorporate multiple members in their development teams, posing a challenge for AI practitioners striving to develop a fair AI/ML. In our study, [P12 and P15] reported that not having multiple people on the team is a challenge for them in developing a fair AI/ML, and the lack of cost is the factor contributing to this challenge. For example, [P15] said:</p>
<blockquote id="S4.SS3.SSSx6.p2.2" class="ltx_quote">
<span id="S4.SS3.SSSx6.p2.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx6.p2.2.2" class="ltx_p"><em id="S4.SS3.SSSx6.p2.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“So even an individual (AI practitioner) can introduce bias into their model. And what we try to do is have the evaluation code be performed by a different person than the trainer. But, usually, <span id="S4.SS3.SSSx6.p2.2.2.1.1" class="ltx_text ltx_font_bold">companies can’t really afford</span> to do that which is challenging.”</em><span id="S4.SS3.SSSx6.p2.2.2.2" class="ltx_text" style="font-size:90%;"> - [P15]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Team-related challenges</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">The majority of participants also encountered challenges pertaining to their knowledge and understanding of different aspects, which we have classified as <em id="S4.SS3.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">team</em>-related challenges in developing a fair AI/ML. The participants discussed two key challenges under this category which are the challenges in (i) Having knowledge of bias/fairness and (ii) Having knowledge of AI. These two key concepts are underpinned by multiple codes which are explained in detail below:</p>
</div>
</section>
<section id="S4.SS3.SSSx7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Having knowledge of bias/fairness</h4>

<div id="S4.SS3.SSSx7.p1" class="ltx_para">
<p id="S4.SS3.SSSx7.p1.1" class="ltx_p">It is important that AI practitioners possess good knowledge and understanding of key concepts such as ‘bias’ and ‘fairness’ when aiming to develop a fair AI/ML. However, AI practitioners might face difficulties in grasping the concepts of ‘bias’ and ‘fairness’ due to reasons such as the subjective nature of these concepts. In our study, [P1, P2, P4, P5, P7, P9, P10, P14, P15, and P19] reported that they are unable to understand the concept of ‘bias’ or ‘fairness’ which poses a challenge in developing a fair AI/ML. The factors discussed by the participants that contributed to this challenge include a lack of domain knowledge, a lack of AI practitioners’ common approach, and a lack of awareness. For example, [P4], [P9] and [P19] said,</p>
<blockquote id="S4.SS3.SSSx7.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx7.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx7.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx7.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“And again, now all humans’ thoughts and the way <span id="S4.SS3.SSSx7.p1.2.2.1.1" class="ltx_text ltx_font_bold">they approach a problem would be different</span>. So that is one more reason for not understanding the bias problems in the systems we develop.”</em><span id="S4.SS3.SSSx7.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P4] 
<br class="ltx_break"><span id="S4.SS3.SSSx7.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx7.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Most of our data is from sensors. So we might not have like a very <span id="S4.SS3.SSSx7.p1.2.2.2.2.1" class="ltx_text ltx_font_bold">clear view of biases</span> like the people who deal with NLP and stuff.”</em> - [P9]
<br class="ltx_break"><span id="S4.SS3.SSSx7.p1.2.2.2.3" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS3.SSSx7.p1.2.2.2.4" class="ltx_emph ltx_font_italic">“It’s hard to understand fairness. I think I’m not intellectual enough to be in a position to define fairness. <span id="S4.SS3.SSSx7.p1.2.2.2.4.1" class="ltx_text ltx_font_bold">My definition of fairness could be different from other people’s</span> point of view.”</em> - [P19]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS3.SSSx8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Having knowledge of AI</h4>

<div id="S4.SS3.SSSx8.p1" class="ltx_para">
<p id="S4.SS3.SSSx8.p1.1" class="ltx_p">The rapid growth of AI is making it increasingly challenging for everyone to keep updated with its advancements and understand its outcomes <cite class="ltx_cite ltx_citemacro_citep">(Pant et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. AI practitioners might face such challenges that can negatively impact the development of a fair AI/ML. In our study, only [P11 and P15] reported that understanding AI outcomes is challenging due to its complex nature, negatively affecting the development of fair systems. For example, [P15] said,</p>
<blockquote id="S4.SS3.SSSx8.p1.2" class="ltx_quote">
<span id="S4.SS3.SSSx8.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS3.SSSx8.p1.2.2" class="ltx_p"><em id="S4.SS3.SSSx8.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“And then AI models, sometimes you don’t know what it is actually deciding on and what it is actually measuring as it is too <span id="S4.SS3.SSSx8.p1.2.2.1.1" class="ltx_text ltx_font_bold">complex</span>. So we had other cases where the AI kind of learns funny things that you don’t anticipate.”</em><span id="S4.SS3.SSSx8.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P15]</span></p>
</blockquote>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>RQ 3- What do AI practitioners perceive as the <em id="S4.SS4.1.1" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML?</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2403.15481/assets/consequence5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="348" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Overview of the consequences of developing an unfair AI/ML</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We posed an open-ended query to the participants regarding the consequences of developing an unfair AI/ML. The participants explored three distinct categories of negative consequences: (i) Impact on organizations, (ii) Impact on users, and (iii) Impact on practitioners. Each of these three categories is underpinned by multiple concepts and codes explained in detail below. Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4. RQ 3- What do AI practitioners perceive as the consequences of developing an unfair AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows an overview of the consequences of developing an unfair AI/ML.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Impact on organizations</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">The majority of participants delved into the adverse impacts on organizations resulting from the failure to develop a fair AI/ML. The participants delved into two key facets (concepts) of the impacts on organizations namely: (i) Financial losses and (ii) Reputational repercussions.</p>
</div>
</section>
<section id="S4.SS4.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Financial losses</h4>

<div id="S4.SS4.SSSx1.p1" class="ltx_para">
<p id="S4.SS4.SSSx1.p1.1" class="ltx_p">Developing AI/ML is a complex process that demands various resources like time and money <cite class="ltx_cite ltx_citemacro_citep">(Pant et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. If the systems turn out to be unfair or fail to meet goals, it not only affects the project but also leads to financial setbacks for the organization. Many participants [P1, P4, P6, P8, P9, P11, P12, P14, P17, P19, and P21] in our study reported that the development of an unfair AI/ML system leads to financial losses to organizations. For example, participants [P4] and [P6] said:</p>
<blockquote id="S4.SS4.SSSx1.p1.2" class="ltx_quote">
<span id="S4.SS4.SSSx1.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS4.SSSx1.p1.2.2" class="ltx_p"><em id="S4.SS4.SSSx1.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“Because the one that you’re going to deploy would definitely have an impact on your business and in such cases, any small bias can lead to a huge financial loss.”</em><span id="S4.SS4.SSSx1.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P4] 
<br class="ltx_break"><span id="S4.SS4.SSSx1.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS4.SSSx1.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Yeah, it also constitutes money loss to the organization.”</em> - [P6]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS4.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Reputational repercussions</h4>

<div id="S4.SS4.SSSx2.p1" class="ltx_para">
<p id="S4.SS4.SSSx2.p1.1" class="ltx_p">In today’s tech-driven era, organizations are in fierce competition to enhance their software systems <cite class="ltx_cite ltx_citemacro_citep">(Hua and Belfield, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. The constant race for improvement means even a minor flaw can tarnish an organization’s reputation. Developing an unfair AI/ML poses a significant risk, as it can lead to severe reputational repercussions for organizations in this highly competitive landscape. In our study, only [P5, P7, and P9] provided insights into the consequences associated with the organization’s reputation when an unfair AI/ML is developed. For example, participants [P5] and [P9] said:</p>
<blockquote id="S4.SS4.SSSx2.p1.2" class="ltx_quote">
<span id="S4.SS4.SSSx2.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS4.SSSx2.p1.2.2" class="ltx_p"><em id="S4.SS4.SSSx2.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“If they are collecting the data, then there shouldn’t be, any bias in the data. If bias is there, different kinds of controversies will rise in, and legal issues will arise.”</em><span id="S4.SS4.SSSx2.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P5]
<br class="ltx_break"><span id="S4.SS4.SSSx2.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS4.SSSx2.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“But to give you another perspective, like Twitter, such models, are exposed to a large number of people and a large number of datasets and are heavily used worldwide. Such organizations need to ensure they do not have such biases in their datasets, or in finally at their models, because then it would create controversies, and then it could finally tarnish the image of these organizations as well.”</em> - [P9]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Impact on users</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Many participants mentioned the negative impacts on users due to the development of an unfair AI/ML. The participants discussed two key aspects (concepts) of the impacts on people including (i) Obtaining flawed product and (ii) Emotional distress and discrimination.</p>
</div>
</section>
<section id="S4.SS4.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Obtaining flawed product</h4>

<div id="S4.SS4.SSSx3.p1" class="ltx_para">
<p id="S4.SS4.SSSx3.p1.1" class="ltx_p">The primary objective of developing any AI/ML systems is to aid users in various domains, be it healthcare, technology, education etc. When AI/ML systems are developed unfairly, users receive flawed products, which undermines their core purpose. In our study, [P1, P2, P3, P8, P9, P10, P11, P15, P16, and P18] emphasized that the primary detriment to users resulting from the development of an unfair AI/ML is the receipt of defective products, leading to inaccurate predictions. For example, [P1] and [P2] said:</p>
<blockquote id="S4.SS4.SSSx3.p1.2" class="ltx_quote">
<span id="S4.SS4.SSSx3.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS4.SSSx3.p1.2.2" class="ltx_p"><em id="S4.SS4.SSSx3.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“It is important to create a fair ML model because the main reason is that if the ML model is biased, users won’t be able to achieve our end goal. If the model is biased, it won’t give the result that it is required to give.”</em><span id="S4.SS4.SSSx3.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P1] 
<br class="ltx_break"><span id="S4.SS4.SSSx3.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS4.SSSx3.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Users will be getting a great product from the company if it is fair, otherwise not.”</em> - [P2]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS4.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Emotional distress and discrimination</h4>

<div id="S4.SS4.SSSx4.p1" class="ltx_para">
<p id="S4.SS4.SSSx4.p1.1" class="ltx_p">When an unfair AI/ML system is developed, most likely the users who are using those systems are impacted negatively <cite class="ltx_cite ltx_citemacro_citep">(Martin, <a href="#bib.bib36" title="" class="ltx_ref">2018</a>; Prates et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>. The primary impact could manifest as emotional distress and discrimination towards these users. Only [P1, P7, P20, and P22] highlighted that users experience emotional distress and discrimination when using unfair AI/ML systems, leading to hurt sentiments. For example, a participant [P1] said:</p>
<blockquote id="S4.SS4.SSSx4.p1.2" class="ltx_quote">
<span id="S4.SS4.SSSx4.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS4.SSSx4.p1.2.2" class="ltx_p"><em id="S4.SS4.SSSx4.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“The second reason is that the sentiments of the people can be hurt if the model is biased.”</em><span id="S4.SS4.SSSx4.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P1]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Impact on practitioners</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">A very few participants mentioned the negative impacts of developing an unfair AI/ML on the practitioners responsible for their development. We classified these negative consequences into a specific aspect: (i) No professional empowerment.</p>
</div>
</section>
<section id="S4.SS4.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">No professional empowerment</h4>

<div id="S4.SS4.SSSx5.p1" class="ltx_para">
<p id="S4.SS4.SSSx5.p1.1" class="ltx_p">AI practitioners gain valuable insights through hands-on experience in developing AI/ML, complementing their theoretical knowledge. The development of unfair systems, however, could have a negative impact on these practitioners, affecting their learning experiences in the field. Only [P11 and P13] highlighted that the development of unfair AI/ML systems hinders the professional empowerment of AI practitioners, causing a decline in confidence and knowledge. For example, participants [P11] and [P13] said,</p>
<blockquote id="S4.SS4.SSSx5.p1.2" class="ltx_quote">
<span id="S4.SS4.SSSx5.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS4.SSSx5.p1.2.2" class="ltx_p"><em id="S4.SS4.SSSx5.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“When they develop an unfair model, they will not learn something new from that.. like, in terms of data augmentation, or in terms of algorithmic change, or in terms of data collection.”</em><span id="S4.SS4.SSSx5.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P11]
<br class="ltx_break"><span id="S4.SS4.SSSx5.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS4.SSSx5.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“Yeah, if the research team or the model training team make the unfair model and we lack the confidence to use the model directly in our product.”</em> - [P13]</span></p>
</blockquote>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>RQ 4- What <em id="S4.SS5.1.1" class="ltx_emph ltx_font_italic">strategies</em> do AI practitioners use in ensuring the fairness of an AI/ML?</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2403.15481/assets/strategy9.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="451" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Overview of the participants’ strategies in ensuring the fairness of an AI/ML</figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We asked the participants about the strategies that they use to ensure the fairness of AI/ML systems.
The participants discussed two categories of strategies that they use to ensure the fairness of AI/ML which are (i) Bias-related strategies and (ii) Performance-related strategies. Each of these two categories is underpinned by multiple concepts and codes explained in detail below. Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the overview of AI practitioners’ strategies to ensure the fairness of AI/ML they develop.</p>
</div>
<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1. </span>Bias-related strategies</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">The majority of the participants discussed the strategies they used to address the bias-related issues when developing a fair AI/ML. The participants reported two key strategies (concepts) they used to address bias-related issues including (i) Detecting bias and (ii) Mitigating bias. Each of these concepts is discussed in detail below.</p>
</div>
</section>
<section id="S4.SS5.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Detecting bias</h4>

<div id="S4.SS5.SSSx1.p1" class="ltx_para">
<p id="S4.SS5.SSSx1.p1.1" class="ltx_p">Addressing any concern starts with its detection; without identification, mitigation is impossible. In our study also, participants outlined their strategies for ensuring the fairness of AI/ML they developed, emphasizing the initial step of detecting bias. [P1, P3, P4, P5, P6, P7, P8, P9, P11, and P18] mentioned that they rely on testing the system with test datasets as their strategy for identifying biases in the system. For example, participants [P1] and [P8] said:</p>
<blockquote id="S4.SS5.SSSx1.p1.2" class="ltx_quote">
<span id="S4.SS5.SSSx1.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS5.SSSx1.p1.2.2" class="ltx_p"><em id="S4.SS5.SSSx1.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“ So in the beginning, we categorize our whole dataset as train, test, and validation dataset and use the testing data to test the model. The testing phase is mandatory, otherwise, we won’t know if the model we created has biases.”</em><span id="S4.SS5.SSSx1.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P1]
<br class="ltx_break"><span id="S4.SS5.SSSx1.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS5.SSSx1.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“After building the model, we use some test case scenarios and we have been doing this post process like how well the model is doing in the test data set to find out any biases.”</em> - [P8]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS5.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Mitigating bias</h4>

<div id="S4.SS5.SSSx2.p1" class="ltx_para">
<p id="S4.SS5.SSSx2.p1.1" class="ltx_p">#By balancing datasets: Data holds significance in the development of AI/ML. Using well-balanced datasets for training is important to ensure the system generates an unbiased prediction. Likewise, in our study, the majority of the participants [P1, P3, P5, P7, P8, P9, P10, P11, P13, P14, P15, P16, P18, P19, P20, P21, and P22] mentioned using the data augmentation technique to balance training datasets during the development phase to mitigate biases in their systems. For example, participants [P14] and [P22] said:</p>
<blockquote id="S4.SS5.SSSx2.p1.2" class="ltx_quote">
<span id="S4.SS5.SSSx2.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS5.SSSx2.p1.2.2" class="ltx_p"><em id="S4.SS5.SSSx2.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“I try to identify whether there are any segments where these metrics are up or down, and then I would go back to the data and see if it is because we don’t have enough data for those regions. And then if that is the case, I’ll try to maybe augment the data.”</em><span id="S4.SS5.SSSx2.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P14]
<br class="ltx_break"><span id="S4.SS5.SSSx2.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS5.SSSx2.p1.2.2.2.2" class="ltx_emph ltx_font_italic">I tried to increase for those that did not have enough by strategically doing some kind of artificial, like stretching the existing data or compressing data because I was working with audio data.”</em> - [P22]</span></p>
</blockquote>
</div>
<div id="S4.SS5.SSSx2.p2" class="ltx_para">
<p id="S4.SS5.SSSx2.p2.1" class="ltx_p">#By involving multiple people: Collaborating in a team with multiple members offers numerous advantages in software development, fostering the exchange of knowledge and facilitating mutual learning <cite class="ltx_cite ltx_citemacro_citep">(Augustin et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2002</a>)</cite>. This collaborative dynamic can also prove particularly beneficial in the context of developing AI/ML systems. In our study, [P2, P7, and P12] mentioned that they get input from multiple people and this collaborative approach aids them in mitigating data biases within the system they develop. For example, [P7] said:</p>
<blockquote id="S4.SS5.SSSx2.p2.2" class="ltx_quote">
<span id="S4.SS5.SSSx2.p2.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS5.SSSx2.p2.2.2" class="ltx_p"><em id="S4.SS5.SSSx2.p2.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“I don’t have any medical background. Sometimes, some factors or features I never thought about could cause bias. So we have other advisors, like from another university, doctors, and professors. So yeah, we have that on the system to ask them (domain experts) if we have any doubts.”</em><span id="S4.SS5.SSSx2.p2.2.2.2" class="ltx_text" style="font-size:90%;"> - [P7]</span></p>
</blockquote>
</div>
<div id="S4.SS5.SSSx2.p3" class="ltx_para">
<p id="S4.SS5.SSSx2.p3.1" class="ltx_p">#By focusing on practices: In addition to securing the necessary resources, adhering to best practices can be important in the development of an AI/ML. In our study, [P1, P2, P4, P5, and P12] reported that focusing on the practices of developing a fair AI/ML is the strategy they take in mitigating biases from the system. For example, [P2] and [P4] quoted:</p>
<blockquote id="S4.SS5.SSSx2.p3.2" class="ltx_quote">
<span id="S4.SS5.SSSx2.p3.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS5.SSSx2.p3.2.2" class="ltx_p"><em id="S4.SS5.SSSx2.p3.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“I just become conscious about unconscious biases during the development process.”</em><span id="S4.SS5.SSSx2.p3.2.2.2" class="ltx_text" style="font-size:90%;"> - [P2]
<br class="ltx_break"><span id="S4.SS5.SSSx2.p3.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S4.SS5.SSSx2.p3.2.2.2.2" class="ltx_emph ltx_font_italic">“I would also believe in the feedback mechanism out there, not just seeing your results on the test set and then going and deploying it, but rather enabling a feedback mechanism. And whenever the system goes a little off in terms of prediction, immediately, the feedback loop is getting connected there. So that is one way that I generally rectify my bias.”</em> - [P4]</span></p>
</blockquote>
</div>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2. </span>Performance-related strategies</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">A few participants elaborated on strategies for the performance of the AI/ML they developed to ensure fairness. Within this category, participants deliberated on a specific strategy (concept), namely, (i) Detecting inaccuracy, which is explained below.</p>
</div>
</section>
<section id="S4.SS5.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Detecting inaccuracy</h4>

<div id="S4.SS5.SSSx3.p1" class="ltx_para">
<p id="S4.SS5.SSSx3.p1.1" class="ltx_p">In our study, [P5, P7, P8, P9, P10, P15, P17, and P21] mentioned that they detected the inaccuracy of the AI/ML by using evaluation metrics. This helps them gauge the system’s performance and ensure its fairness. For example, [P8] said:</p>
<blockquote id="S4.SS5.SSSx3.p1.2" class="ltx_quote">
<span id="S4.SS5.SSSx3.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S4.SS5.SSSx3.p1.2.2" class="ltx_p"><em id="S4.SS5.SSSx3.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“Fair model should be 100% accurate. 100% accuracy is good for our scenario, but we also have other case scenarios like the loss and other things to notice in the evaluation metrics. When we have to check what is the loss of the models we have to go for the minimum loss.”</em><span id="S4.SS5.SSSx3.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P8] 
<br class="ltx_break"></span></p>
</blockquote>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Summary of Key Findings</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">This study focuses on exploring AI practitioners’ <em id="S4.SS6.p1.1.1" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’, exploring the <em id="S4.SS6.p1.1.2" class="ltx_emph ltx_font_italic">challenges</em> they encounter during the development of a fair AI/ML, understanding the <em id="S4.SS6.p1.1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them, and investigating the <em id="S4.SS6.p1.1.4" class="ltx_emph ltx_font_italic">strategies</em> they employ to ensure fairness in the AI/ML systems they develop. Table <a href="#S4.T2" title="Table 2 ‣ 4.6. Summary of Key Findings ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the summary of the key findings of our study.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Key Findings (KF) of the study.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Key Findings (KF)</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Section</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">KF1</span></span>
</span>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">AI practitioners’ understanding of ‘fair AI/ML’</span></span>
</span>
</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.2.1.3.1.1.1" class="ltx_text" style="font-size:70%;">4.2</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.3.2.2.1.1.1" class="ltx_text" style="font-size:70%;">(i) In terms of the absence of </span><em id="S4.T2.1.3.2.2.1.1.2" class="ltx_emph ltx_align_left ltx_font_italic" style="font-size:70%;">bias</em></span>
</span>
</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.4.3.2.1.1.1" class="ltx_text" style="font-size:70%;">(ii) In terms of the presence of desirable </span><em id="S4.T2.1.4.3.2.1.1.2" class="ltx_emph ltx_align_left ltx_font_italic" style="font-size:70%;">attributes</em><span id="S4.T2.1.4.3.2.1.1.3" class="ltx_text" style="font-size:70%;"> (transparency, accuracy, interpretability etc.)</span></span>
</span>
</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.5.4.1.1.1.1" class="ltx_text" style="font-size:70%;">KF2</span></span>
</span>
</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.5.4.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">AI practitioners’ challenges in developing a fair AI/ML</span></span>
</span>
</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.5.4.3.1.1.1" class="ltx_text" style="font-size:70%;">4.3</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.6.5.2.1.1.1" class="ltx_text" style="font-size:70%;">(i) Process-related challenges: gaining access to datasets, balancing ideal vs real, handling data-related issues, and following policies and regulations</span></span>
</span>
</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.6.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.6.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.7.6.2.1.1.1" class="ltx_text" style="font-size:70%;">(ii) Resource-related challenges: obtaining required datasets and obtaining other resources</span></span>
</span>
</td>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.6.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<td id="S4.T2.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.7.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.7.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.8.7.2.1.1.1" class="ltx_text" style="font-size:70%;">(iii) Team-related challenges: having knowledge of bias/fairness and having knowledge of AI</span></span>
</span>
</td>
<td id="S4.T2.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.8.7.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.9.8" class="ltx_tr">
<td id="S4.T2.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.8.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.9.8.1.1.1.1" class="ltx_text" style="font-size:70%;">KF3</span></span>
</span>
</td>
<td id="S4.T2.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.8.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.9.8.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Consequences of developing an unfair AI/ML perceived by AI practitioners</span></span>
</span>
</td>
<td id="S4.T2.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.9.8.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.9.8.3.1.1.1" class="ltx_text" style="font-size:70%;">4.4</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.10.9" class="ltx_tr">
<td id="S4.T2.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.10.9.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.10.9.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.10.9.2.1.1.1" class="ltx_text" style="font-size:70%;">(i) Impact on organizations: financial losses and reputational repercussions</span></span>
</span>
</td>
<td id="S4.T2.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.10.9.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.11.10" class="ltx_tr">
<td id="S4.T2.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.11.10.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.11.10.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.11.10.2.1.1.1" class="ltx_text" style="font-size:70%;">(ii) Impact on users: obtaining flawed product and emotional distress and discrimination</span></span>
</span>
</td>
<td id="S4.T2.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.11.10.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.12.11" class="ltx_tr">
<td id="S4.T2.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.12.11.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.12.11.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.12.11.2.1.1.1" class="ltx_text" style="font-size:70%;">(iii) Impact on practitioners: no professional empowerment</span></span>
</span>
</td>
<td id="S4.T2.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.12.11.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.13.12" class="ltx_tr">
<td id="S4.T2.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.13.12.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.13.12.1.1.1.1" class="ltx_text" style="font-size:70%;">KF4</span></span>
</span>
</td>
<td id="S4.T2.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.13.12.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.13.12.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">AI practitioners’ strategies to ensure the fairness of an AI/ML</span></span>
</span>
</td>
<td id="S4.T2.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.13.12.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.13.12.3.1.1.1" class="ltx_text" style="font-size:70%;">4.5</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.14.13" class="ltx_tr">
<td id="S4.T2.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.14.13.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.14.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.14.13.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.14.13.2.1.1.1" class="ltx_text" style="font-size:70%;">(i) Bias-related strategies: detecting bias and mitigating bias</span></span>
</span>
</td>
<td id="S4.T2.1.14.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.14.13.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.15.14" class="ltx_tr">
<td id="S4.T2.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.15.14.1.1.1" class="ltx_p" style="width:14.2pt;"></span>
</span>
</td>
<td id="S4.T2.1.15.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.15.14.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.15.14.2.1.1.1" class="ltx_text" style="font-size:70%;">(ii) Performance-related strategies: detecting inaccuracy</span></span>
</span>
</td>
<td id="S4.T2.1.15.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.15.14.3.1.1" class="ltx_p" style="width:17.1pt;"></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.16.15" class="ltx_tr">
<td id="S4.T2.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.16.15.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.16.15.1.1.1.1" class="ltx_text" style="font-size:70%;">KF5</span></span>
</span>
</td>
<td id="S4.T2.1.16.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.16.15.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.16.15.2.1.1.1" class="ltx_text" style="font-size:70%;">Despite differing understandings of ‘fair AI/ML’ among practitioners, a common challenge they report facing is obtaining the required datasets for model training during AI/ML development.</span></span>
</span>
</td>
<td id="S4.T2.1.16.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.16.15.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.16.15.3.1.1.1" class="ltx_text" style="font-size:70%;">4.7.1</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.17.16" class="ltx_tr">
<td id="S4.T2.1.17.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.17.16.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.17.16.1.1.1.1" class="ltx_text" style="font-size:70%;">KF6</span></span>
</span>
</td>
<td id="S4.T2.1.17.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.17.16.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.17.16.2.1.1.1" class="ltx_text" style="font-size:70%;">Some AI practitioners describing ‘fair AI/ML’ in terms of absence of </span><em id="S4.T2.1.17.16.2.1.1.2" class="ltx_emph ltx_align_left ltx_font_italic" style="font-size:70%;">bias</em><span id="S4.T2.1.17.16.2.1.1.3" class="ltx_text" style="font-size:70%;"> seemed to have broader understanding of negative consequences on practitioners due to the development of unfair AI/ML compared to those emphasizing the presence of desirable </span><em id="S4.T2.1.17.16.2.1.1.4" class="ltx_emph ltx_align_left ltx_font_italic" style="font-size:70%;">attributes</em><span id="S4.T2.1.17.16.2.1.1.5" class="ltx_text" style="font-size:70%;">.</span></span>
</span>
</td>
<td id="S4.T2.1.17.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.1.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.17.16.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.17.16.3.1.1.1" class="ltx_text" style="font-size:70%;">4.7.2</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.1.18.17" class="ltx_tr">
<td id="S4.T2.1.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T2.1.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.18.17.1.1.1" class="ltx_p" style="width:14.2pt;"><span id="S4.T2.1.18.17.1.1.1.1" class="ltx_text" style="font-size:70%;">KF7</span></span>
</span>
</td>
<td id="S4.T2.1.18.17.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T2.1.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.18.17.2.1.1" class="ltx_p" style="width:270.3pt;"><span id="S4.T2.1.18.17.2.1.1.1" class="ltx_text" style="font-size:70%;">Despite differing understandings of ‘fair AI/ML’ among practitioners, a common strategy they report using to ensure fairness in AI/ML is implementing bias-mitigation strategies.</span></span>
</span>
</td>
<td id="S4.T2.1.18.17.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T2.1.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.18.17.3.1.1" class="ltx_p" style="width:17.1pt;"><span id="S4.T2.1.18.17.3.1.1.1" class="ltx_text" style="font-size:70%;">4.7.3</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7. </span>Framework showing the relationship between the aspects</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">In this section, we discuss the relationship between AI practitioners’ <em id="S4.SS7.p1.1.1" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’ with three other aspects including, (i) the <em id="S4.SS7.p1.1.2" class="ltx_emph ltx_font_italic">challenges</em> encountered in the development of a fair AI/ML, (ii) the <em id="S4.SS7.p1.1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML, and (iii) <em id="S4.SS7.p1.1.4" class="ltx_emph ltx_font_italic">strategies</em> used to ensure the fairness of an AI/ML. Every participant [P1 to P22] in our study described ‘fair AI/ML’ either in terms of the absence of <em id="S4.SS7.p1.1.5" class="ltx_emph ltx_font_italic">bias</em> or in terms of the presence of desirable <em id="S4.SS7.p1.1.6" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML. The exception was the participant [P14], who described it in terms of the absence of <em id="S4.SS7.p1.1.7" class="ltx_emph ltx_font_italic">bias</em>, as well as in terms of the presence of desirable <em id="S4.SS7.p1.1.8" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML. To illustrate the relationship between these aspects, we developed a framework, which is shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2403.15481/assets/relationship7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="394" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>A framework showing the relationship between AI practitioners’ <em id="S4.F8.5.1" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’, and their <em id="S4.F8.6.2" class="ltx_emph ltx_font_italic">challenges</em>, <em id="S4.F8.7.3" class="ltx_emph ltx_font_italic">consequences</em>, and <em id="S4.F8.8.4" class="ltx_emph ltx_font_italic">strategies</em></figcaption>
</figure>
<section id="S4.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.7.1. </span>Relationship between AI practitioners’ <span id="S4.SS7.SSS1.1.1" class="ltx_text ltx_font_bold">understanding</span> and their <span id="S4.SS7.SSS1.2.2" class="ltx_text ltx_font_bold">challenges</span>
</h4>

</section>
<section id="S4.SS7.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the absence of bias vs challenges</h4>

<div id="S4.SS7.SSSx1.p1" class="ltx_para">
<p id="S4.SS7.SSSx1.p1.1" class="ltx_p">The interview indicates that the participants who described ‘fair AI/ML’ in terms of the absence of <em id="S4.SS7.SSSx1.p1.1.1" class="ltx_emph ltx_font_italic">bias</em> in an AI/ML reported challenges in all three categories: (i) process-related, (ii) resource-related, and (iii) team-related challenges that they face during AI/ML development, as shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Specifically, the majority of the participants reported facing challenges in obtaining required datasets (resource-related challenge), followed by the challenge of handling data-related issues (process-related challenge). We see that AI practitioners who described ‘fair AI/ML’ in terms of the absence of <em id="S4.SS7.SSSx1.p1.1.2" class="ltx_emph ltx_font_italic">bias</em> in AI/ML also reported facing substantial challenges related to the datasets used in development. These challenges primarily revolve around resource availability, specifically in obtaining the necessary datasets and handling data-related issues.</p>
</div>
</section>
<section id="S4.SS7.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the presence of desirable attributes vs challenges</h4>

<div id="S4.SS7.SSSx2.p1" class="ltx_para">
<p id="S4.SS7.SSSx2.p1.1" class="ltx_p">The interview shows that the participants who described ‘fair AI/ML’ in terms of the presence of desirable <em id="S4.SS7.SSSx2.p1.1.1" class="ltx_emph ltx_font_italic">attributes</em> reported almost all the challenges across all three categories— (i) process-related, (ii) resource-related, and (iii) team-related challenges that they encounter during the development of an AI/ML, as illustrated in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Notably, none of the participants reported the challenge of lacking knowledge of AI (team-related challenge), whereas, most of the participants reported facing the challenge of obtaining required datasets (resource-related challenge) during AI/ML development.</p>
</div>
<div id="S4.SS7.SSSx2.p2" class="ltx_para">
<p id="S4.SS7.SSSx2.p2.1" class="ltx_p">In summary, regardless of how AI practitioners described ‘fair AI/ML’, a common challenge they faced is obtaining the required datasets to train a model during AI/ML development.</p>
</div>
</section>
<section id="S4.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.7.2. </span>Relationship between AI practitioners’ <span id="S4.SS7.SSS2.1.1" class="ltx_text ltx_font_bold">understanding</span> and the <span id="S4.SS7.SSS2.2.2" class="ltx_text ltx_font_bold">consequences</span>
</h4>

</section>
<section id="S4.SS7.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the absence of bias vs consequences</h4>

<div id="S4.SS7.SSSx3.p1" class="ltx_para">
<p id="S4.SS7.SSSx3.p1.1" class="ltx_p">The interview indicates that the participants in our study, describing ‘fair AI/ML’ in terms of the absence of <em id="S4.SS7.SSSx3.p1.1.1" class="ltx_emph ltx_font_italic">bias</em> in AI/ML, perceived the negative consequences of developing an unfair AI/ML across all three categories including, (i) impact on organizations, (ii) impact of users and (iii) impact on practitioners, as shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Notably, the majority of participants perceived the acquisition of flawed products by users as a negative consequence of developing an unfair AI/ML. However, the majority did not explicitly mention emotional distress and discrimination for users, as well as reputational repercussions for organizations, as significant negative consequences. These two specific concerns were expressed by only one participant each.</p>
</div>
</section>
<section id="S4.SS7.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the presence of desirable attributes vs consequences</h4>

<div id="S4.SS7.SSSx4.p1" class="ltx_para">
<p id="S4.SS7.SSSx4.p1.1" class="ltx_p">The interview shows that the participants in our study, describing ‘fair AI/ML’ in terms of the presence of desirable <em id="S4.SS7.SSSx4.p1.1.1" class="ltx_emph ltx_font_italic">attributes</em> perceived the negative consequences of developing an unfair AI/ML across only two categories including, (i) impact on organizations, and (ii) impact on users. According to the data, most participants talked about the financial loss to organizations as a negative consequence of developing an unfair AI/ML, as shown in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Notably, no participants in our study mentioned any negative consequences of developing an unfair AI/ML for the practitioners engaged in their development.</p>
</div>
<div id="S4.SS7.SSSx4.p2" class="ltx_para">
<p id="S4.SS7.SSSx4.p2.1" class="ltx_p">In summary, of the ones who described ‘fair AI/ML’ in terms of absence of <em id="S4.SS7.SSSx4.p2.1.1" class="ltx_emph ltx_font_italic">bias</em>, two of them discussed the negative consequence of developing an unfair AI/ML on practitioners involved in the development. While those who described it in terms of the presence of desirable <em id="S4.SS7.SSSx4.p2.1.2" class="ltx_emph ltx_font_italic">attributes</em> did not discuss the negative consequences of developing an unfair AI/ML for practitioners at all. It looks like the former were able to acknowledge it, and they seem to have a broader understanding of the negative consequences associated with developing unfair AI/ML systems.</p>
</div>
</section>
<section id="S4.SS7.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.7.3. </span>Relationship between AI practitioners’ <span id="S4.SS7.SSS3.1.1" class="ltx_text ltx_font_bold">understanding</span> and their <span id="S4.SS7.SSS3.2.2" class="ltx_text ltx_font_bold">strategies</span>
</h4>

</section>
<section id="S4.SS7.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the absence of bias vs strategies</h4>

<div id="S4.SS7.SSSx5.p1" class="ltx_para">
<p id="S4.SS7.SSSx5.p1.1" class="ltx_p">According to the interview, the participants in our study who described ‘fair AI/ML’ in terms of the absence of <em id="S4.SS7.SSSx5.p1.1.1" class="ltx_emph ltx_font_italic">bias</em> in AI/ML discussed strategies falling into both categories, including (i) bias-related strategies and (ii) performance-related strategies. It shows that most participants discussed the strategies to mitigate bias (bias-related strategies) from AI/ML to ensure its fairness. In contrast, only a small number of participants reported the strategies of detecting bias (bias-related strategies) and detecting inaccuracy (performance-related strategies) to ensure the fairness of AI/ML.</p>
</div>
</section>
<section id="S4.SS7.SSSx6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">AI practitioners’ understanding in terms of the presence of desirable attributes vs strategies</h4>

<div id="S4.SS7.SSSx6.p1" class="ltx_para">
<p id="S4.SS7.SSSx6.p1.1" class="ltx_p">The interview shows that the participants in our study who described ‘fair AI/ML’ in terms of the presence of desirable <em id="S4.SS7.SSSx6.p1.1.1" class="ltx_emph ltx_font_italic">attributes</em> discussed strategies falling into both categories, including (i) bias-related strategies and (ii) performance-related strategies. It shows that a slightly higher number of participants discussed the strategy of mitigating bias (bias-related strategies) from AI/ML to ensure its fairness as compared to other strategies like detecting bias (bias-related strategies) and detecting inaccuracy (performance-related strategies). In contrast, an almost equal number of participants discussed strategies like detecting bias (bias-related strategies) or detecting inaccuracy (performance-related strategies) to ensure AI/ML fairness.</p>
</div>
<div id="S4.SS7.SSSx6.p2" class="ltx_para">
<p id="S4.SS7.SSSx6.p2.1" class="ltx_p">In summary, the interview shows a consistent trend among participants discussing the strategies that they used to ensure the fairness of AI/ML, regardless of how they described ‘fair AI/ML’. The majority of the participants who described ‘fair AI/ML’ either in terms of the absence of <em id="S4.SS7.SSSx6.p2.1.1" class="ltx_emph ltx_font_italic">bias</em> as well as in terms of the presence of desirable <em id="S4.SS7.SSSx6.p2.1.2" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML reported the use of the common strategy of mitigating bias (bias-related strategies) to ensure the fairness of the AI/ML they developed.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we discuss and compare our findings in light of the related works.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Definition/understanding of AI/ML fairness</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In recent years, major players in the tech industry, such as Google, Microsoft, and IBM, have delved deeply into the concept of <em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">fairness</em> in AI. Their consensus on the ‘fairness’ principle revolves around minimizing bias and fostering inclusive representation in the development of AI <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Microsoft, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>; IBM, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Various experiments, including those by <cite class="ltx_cite ltx_citemacro_citet">Harrison et al<span class="ltx_text">.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Srivastava et al<span class="ltx_text">.</span> (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite>, have explored user perspectives on AI/ML fairness. The study conducted by <cite class="ltx_cite ltx_citemacro_citet">Harrison et al<span class="ltx_text">.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> with non-technical users on Amazon Mechanical Turk (AMT) revealed that users reported unbiased models might not be automatically perceived as fair. This finding does not align with our study as some participants described ‘fair AI/ML’ in terms of absence of <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">bias</em>. On the other hand, the experiment by <cite class="ltx_cite ltx_citemacro_citet">Srivastava et al<span class="ltx_text">.</span> (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite> on AMT found users defining fairness technically, focusing on accuracy and demographic parity, mirroring our study where AI practitioners also described ‘fair AI/ML’ in terms of accuracy (the presence of desirable <em id="S5.SS1.p2.1.2" class="ltx_emph ltx_font_italic">attributes</em>) of the system. Importantly, our study involved AI practitioners, while the mentioned studies focused on the general users of AI/ML systems.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Due to the lack of research that focuses on investigating AI practitioners’ understanding of ‘fair AI/ML’, we conducted an empirical study with 22 AI practitioners to investigate their understanding of what a ‘fair AI/ML’ is. In our study, AI practitioners described ‘fair AI/ML’ in terms of the absence of <em id="S5.SS1.p3.1.1" class="ltx_emph ltx_font_italic">bias</em> and in terms of the presence of desirable <em id="S5.SS1.p3.1.2" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML. <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> in their empirical study, found that participants when discussing the term ‘fairness’, commonly focused on preventing biased decisions of ML systems. This aligns with our findings as several AI practitioners in our study also described ‘fair AI/ML’ in terms of the absence of <em id="S5.SS1.p3.1.3" class="ltx_emph ltx_font_italic">bias</em>. It is important to note that both academic and industry professionals in the fields of Human-Computer Interaction (HCI) and ML participated in <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>’s study. Similarly, aligning with the definition of <em id="S5.SS1.p3.1.4" class="ltx_emph ltx_font_italic">‘fairness’</em> introduced by tech companies like Google <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> and Microsoft <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>)</cite>, some AI practitioners in our study described ‘fair AI/ML’ in terms of the absence of <em id="S5.SS1.p3.1.5" class="ltx_emph ltx_font_italic">bias</em>. However, when describing in terms of the presence of desirable <em id="S5.SS1.p3.1.6" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML, AI practitioners in our study specified features such as interpretability, transparency, and explainability that an AI/ML should possess to be deemed <em id="S5.SS1.p3.1.7" class="ltx_emph ltx_font_italic">fair</em>. <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> also highlighted that a few participants mentioned that a system needs to be transparent to be considered fair. Notably, principles like ‘explainability’ and ‘transparency’ are outlined separately in the AI ethics principles listed by tech companies such as Google <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, IBM <cite class="ltx_cite ltx_citemacro_citep">(IBM, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, Microsoft <cite class="ltx_cite ltx_citemacro_citep">(Microsoft, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>)</cite>, and countries such as Australia <cite class="ltx_cite ltx_citemacro_citep">(Government, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> and countries in Europe <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. This suggests a lack of alignment between how AI practitioners understand ‘fair AI/ML’ and the definitions set forth by tech companies, countries, and continents. This misalignment may hinder the development of universally accepted principles for fair AI/ML, potentially resulting in disparate approaches and interpretations within the AI community.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">In a similar vein, <em id="S5.SS1.p4.1.1" class="ltx_emph ltx_font_italic">accuracy</em> and <em id="S5.SS1.p4.1.2" class="ltx_emph ltx_font_italic">fairness</em> are categorized as two different non-functional requirements of an ML system <cite class="ltx_cite ltx_citemacro_citep">(Habibullah et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. The <em id="S5.SS1.p4.1.3" class="ltx_emph ltx_font_italic">accuracy</em> of an ML system has been categorized as a non-functional requirement, which can be measured using ML-specific or standard measures, whereas <em id="S5.SS1.p4.1.4" class="ltx_emph ltx_font_italic">fairness</em> has been categorized as a non-functional requirement that cannot be measured and is non- quantifiable <cite class="ltx_cite ltx_citemacro_citep">(Habibullah et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. However, in our study, we found that the participants [P1, P7, P8, P16, and P22] described ‘fair AI/ML’ in terms of <em id="S5.SS1.p4.1.5" class="ltx_emph ltx_font_italic">accuracy</em> of the AI/ML system (Section <a href="#S4.SS2" title="4.2. RQ 1- What do AI practitioners understand by ‘fair AI/ML’? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Along with that, when asked about the strategies to ensure AI/ML fairness, some participants [P5, P7, P8, P9, P10, P15, P17, and P21] reported that they focus on detecting the inaccuracy of the system (Section <a href="#S4.SS5" title="4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>). The participants in our study considered <span id="S5.SS1.p4.1.6" class="ltx_text ltx_font_italic">accuracy</span> as a requirement to develop a fair AI/ML. This also shows that the way AI practitioners in our study described ‘fair AI/ML’ is different from the definitions of <em id="S5.SS1.p4.1.7" class="ltx_emph ltx_font_italic">fairness</em> provided by tech companies like Google, IBM, etc, and different countries/continents like the USA, China, Australia, Europe, etc. Notably, these definitions do not include considerations regarding the <em id="S5.SS1.p4.1.8" class="ltx_emph ltx_font_italic">accuracy</em> of AI/ML systems.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Challenges in developing a fair AI/ML</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Studies have highlighted challenges for AI practitioners in developing a fair AI/ML across various domains and phases of development. Our study specifically focused on investigating the overall challenges of AI practitioners in developing a fair AI/ML through semi-structured interviews. A majority of participants in the studies by <cite class="ltx_cite ltx_citemacro_citet">Holstein et al<span class="ltx_text">.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>); Fenu et al<span class="ltx_text">.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> faced challenges related to limited control over data collection, as well as challenges in obtaining balanced and representative datasets for model training due to a lack of methods supporting data collection and curation <cite class="ltx_cite ltx_citemacro_citep">(Holstein et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. These findings align with our study, where participants reported similar challenges in obtaining necessary datasets, attributing them to a lack of control over data collection, and expressed difficulties in obtaining balanced datasets due to a lack of methods for data collection and curation (Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>). Most participants in <cite class="ltx_cite ltx_citemacro_citet">Holstein et al<span class="ltx_text">.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>’s study reported challenges in detecting biases in the ML system, due to a lack of support and challenges in developing their own solutions due to limited time. Our findings align with these, as participants in our study also highlighted how constraints, such as lack of support and time, pose challenges in detecting biases in systems and developing their envisioned ideal system (Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>).
Similarly, <cite class="ltx_cite ltx_citemacro_citet">Madaio et al<span class="ltx_text">.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> identified collecting datasets as a challenge for AI practitioners during the development of AI systems, primarily due to the need to safeguard the personal information of user data. Our study’s findings align, as participants also reported facing challenges in accessing necessary datasets due to data confidentiality concerns (Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>). <cite class="ltx_cite ltx_citemacro_citet">Madaio et al<span class="ltx_text">.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> found that participants reported challenges related to the resources required to develop a fair AI system, which aligns with one of our findings. Participants in our study also reported challenges in obtaining resources such as datasets, technology, and human resources, as discussed in Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>. Both studies identified funding issues as a contributing factor to this challenge. <cite class="ltx_cite ltx_citemacro_citet">Fenu et al<span class="ltx_text">.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> reported that participants faced challenges in collecting data for training an AI system due to a lack of datasets representing the diversity of the population, which aligns with our findings (Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>).
<cite class="ltx_cite ltx_citemacro_citet">Fenu et al<span class="ltx_text">.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> highlighted that adhering to regulations related to the fairness of an AI system was a reported challenge. Some participants in our study also emphasized challenges in following policies and regulations, citing reasons such as the lack of policies and inadequate implementation as discussed in Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. Likewise, <cite class="ltx_cite ltx_citemacro_citet">Hopkins and Booth (<a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> in their empirical study reported challenges faced by ML practitioners in detecting bias in ML, attributed to biased data or insufficient model testing, as discussed in Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. The participants in our study also emphasized the same challenge, citing factors such as the nature of data bias, lack of time, and a lack of software/tools/methods to assist in detecting system biases. <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> in their empirical study highlighted challenges anticipated by HCI experts and ML experts in developing a fair AI which includes obtaining high-quality data to develop and evaluate a model, which aligns with our findings (Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>). However, <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> identified additional challenges, including the importance of clarity regarding the model’s context and credibility, as well as the difficulty in aligning the mathematical definition of fairness with the accuracy of the model, which does not align with our findings.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">While some findings in our study align with previous research, there are unique contributions, particularly in uncovering team-related challenges faced by AI practitioners in developing a fair AI/ML. Our study uncovered challenges specific to the development team, such as having knowledge of bias/fairness and knowledge of AI, as discussed in Section <a href="#S4.SS3.SSS3" title="4.3.3. Team-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.3</span></a>. Understanding these challenges is crucial, given that AI practitioners play a pivotal role in designing systems that have substantial societal impact and it fosters responsible and effective AI development <cite class="ltx_cite ltx_citemacro_citep">(Orr and Davis, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>. Similarly, our study uncovered challenges faced by AI practitioners in balancing real-world data with training data in AI/ML system development (Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>), a finding not reported in previous studies. Additionally, we identified challenges related to obtaining various resources, including technological and human-related resources, which impact the development of a fair AI/ML (Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>). These insights contribute new dimensions to the existing understanding of challenges in this domain.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Consequences of developing an unfair AI/ML</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Studies have explored the consequences of developing an unfair AI/ML from the perspectives of different stakeholders <cite class="ltx_cite ltx_citemacro_citep">(Marcinkowski et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>; Shin and Park, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Woodruff et al<span class="ltx_text">.</span> (<a href="#bib.bib57" title="" class="ltx_ref">2018</a>)</cite> identified that users reported the potential negative consequences of algorithmic unfairness which include racial discrimination and stereotyping and loss of opportunities for personal advancement.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Given the limited research on AI practitioners’ perspectives on the consequences of developing an unfair AI/ML, we conducted semi-structured interviews with 22 practitioners. Our study revealed new insights, identifying three main negative consequences perceived by AI practitioners: those affecting organizations, users, and the practitioners themselves, as discussed in Section <a href="#S4.SS4" title="4.4. RQ 3- What do AI practitioners perceive as the consequences of developing an unfair AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. Understanding the consequences of developing an unfair AI/ML may facilitate the development of specific mitigation strategies. Addressing issues at the organizational, user, and practitioner levels may contribute to more effective and comprehensive solutions in tackling unfairness in AI/ML.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Strategies in ensuring fairness in AI/ML</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">In recent years, numerous studies have been conducted in the area of exploring strategies and approaches to AI/ML fairness. Several qualitative studies, such as those by <cite class="ltx_cite ltx_citemacro_citet">Deng et al<span class="ltx_text">.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Richardson et al<span class="ltx_text">.</span> (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Balayn et al<span class="ltx_text">.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, have explored AI practitioners’ experiences and perspectives on specific fairness toolkits. These studies conducted semi-structured interviews with AI/ML practitioners to understand their practices in using different fairness toolkits. However, as our study focuses on general strategies employed by AI practitioners to ensure fairness in AI/ML systems, the findings from these studies do not align with our study.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">On the other hand, <cite class="ltx_cite ltx_citemacro_citet">Madaio et al<span class="ltx_text">.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> identified AI practitioners’ processes for recognizing and addressing fairness issues in AI systems, emphasizing understanding fairness as a personal priority and adhering to ad-hoc processes. However, these findings diverge from our study, which concentrates on tactical approaches or strategies used by AI practitioners in their day-to-day life to ensure fairness of AI/ML systems. <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> noted that the common approach used by ML and HCI experts when addressing fairness was associated with data used in an AI system. This finding aligns with our study as most of the participants in our study also discussed the bias mitigation strategy in the AI/ML by balancing datasets (Section <a href="#S4.SS5.SSS1" title="4.5.1. Bias-related strategies ‣ 4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>). Similarly, in the study by <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, a participant mentioned comparing model accuracy across demographic groups to assess fairness. This corresponds with our findings, where several participants also identified inaccuracies in AI/ML through the use of evaluation metrics. However, participants in <cite class="ltx_cite ltx_citemacro_citet">Ryan et al<span class="ltx_text">.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>’s study mentioned not considering fairness in the AI systems they develop, contrasting with our findings. In our study, each participant reported employing at least one strategy to ensure fairness in AI/ML.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Our study presents unique contributions, notably in revealing strategies employed by AI practitioners to detect bias for ensuring fairness in AI/ML, as discussed in Section <a href="#S4.SS5.SSS1" title="4.5.1. Bias-related strategies ‣ 4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>. These insights, including collaboration with team members to mitigate data biases and a focus on individual practices during AI/ML development, are novel findings which have not been reported in previous research.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Insights</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Based on the memos written for the study, we uncovered several interesting insights and reflections. Research recommendations can be made based on these findings and our insights.</p>
</div>
<section id="S5.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.1. </span>‘No bias’- necessary but not sufficient to make a fair AI/ML</h4>

<div id="S5.SS5.SSS1.p1" class="ltx_para">
<p id="S5.SS5.SSS1.p1.1" class="ltx_p">Most ethical guidelines in AI stress the importance of ensuring <em id="S5.SS5.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">fairness</em>, aiming to eliminate bias and discrimination within AI systems. For example, Australia’s AI Ethics Principles defined <em id="S5.SS5.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">‘fairness’</em> as <em id="S5.SS5.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">“AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups”</em> <cite class="ltx_cite ltx_citemacro_citep">(Government, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. Similarly, the European Commission defined ‘Diversity, non-discrimination and fairness’ as, <em id="S5.SS5.SSS1.p1.1.4" class="ltx_emph ltx_font_italic">“Unfair bias must be avoided, as it could have multiple negative implications, from the marginalization of vulnerable groups to the exacerbation of prejudice and discrimination. Fostering diversity, AI systems should be accessible to all, regardless of any disability, and involve relevant stakeholders throughout their entire life circle”</em> <cite class="ltx_cite ltx_citemacro_citep">(Group, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. However, based on the interview with AI practitioners, we identified that ‘no bias’ is a crucial element in developing a fair AI/ML, but it alone is not adequate. For example, when participants were asked to share their understanding of ‘fair AI/ML’, they noted that it must not only be accurate but also exhibit attributes such as transparency and reproducibility to qualify as a fair AI/ML, as discussed in Section <a href="#S4.SS2" title="4.2. RQ 1- What do AI practitioners understand by ‘fair AI/ML’? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Some research has also discussed similar notions. For example, <cite class="ltx_cite ltx_citemacro_citet">Silberg and Manyika (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite> reported that the absence of unwanted bias is not enough to infer that the AI system is ‘fair’. Similarly, participants in an experiment perceived that an unbiased AI/ML does not necessarily mean it is perceived as fair, as they often found certain systems unfair despite being unbiased, especially when errors were distributed unevenly among different racial groups <cite class="ltx_cite ltx_citemacro_citep">(Harrison et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S5.SS5.SSS1.p2" class="ltx_para">
<p id="S5.SS5.SSS1.p2.1" class="ltx_p">Even though participants had broader ideas about what constitutes a ‘fair AI/ML’, it was interesting to observe that their discussions predominantly revolved around the concept of ‘bias’ when responding to various questions. For instance, when queried about <em id="S5.SS5.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">strategies</em> employed to ensure the fairness of an AI/ML system, the majority focused on detecting or addressing biases in AI/ML. In contrast, only a small number delved into strategies related to optimizing the system’s performance for fairness, as outlined in Section <a href="#S4.SS5" title="4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>. Similarly, in discussions about the <em id="S5.SS5.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML, nearly all participants used the term ‘bias’ and elaborated on the consequences of developing a ‘biased’ system. Hence, while achieving a ‘no bias’ is crucial for developing a fair AI/ML, it is essential to recognize that it alone is not adequate; nevertheless, it remains a significant aspect in the development of a fair AI/ML.</p>
</div>
</section>
<section id="S5.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.2. </span>Data bias vs other biases in AI/ML</h4>

<div id="S5.SS5.SSS2.p1" class="ltx_para">
<p id="S5.SS5.SSS2.p1.1" class="ltx_p">As previously discussed, we observed that participants primarily centered their discussions on the notion of ‘bias’ when working towards a fair AI/ML. Machine learning (ML) can be prone to various biases, including biases from data to algorithm, algorithm to user, and user to data. Moreover, each of these categories encompasses different sub-types of biases <cite class="ltx_cite ltx_citemacro_citep">(Mehrabi et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite>. However, in our case, even within the discourse on ‘bias’, a majority of participants specifically addressed the concept of ‘data bias’. For example, when the participants were asked about the challenges encountered in developing a fair AI/ML, one of the aspects (concepts) they highlighted pertained to handling data-related issues, as detailed in Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. Likewise, the majority of the participants discussed the strategies related to detecting bias in the data and mitigating data biases when they were asked about the strategies to ensure the fairness of AI/ML as discussed in Section <a href="#S4.SS5.SSS1" title="4.5.1. Bias-related strategies ‣ 4.5. RQ 4- What strategies do AI practitioners use in ensuring the fairness of an AI/ML? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>. Additionally, participants discussed the factors leading to their challenges to develop a fair AI/ML, mainly related to data bias, such as the use of biased training data, lack of tool to check data bias, etc. as discussed in Section <a href="#S4.SS3" title="4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>. This indicates that among various biases, data bias stands out as particularly prevalent. Effectively addressing data bias is important while developing a fair AI/ML.</p>
</div>
</section>
<section id="S5.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.3. </span>Can a fair AI/ML ever be developed?</h4>

<div id="S5.SS5.SSS3.p1" class="ltx_para">
<p id="S5.SS5.SSS3.p1.1" class="ltx_p">As per the participants, a challenge they encountered in developing a fair AI/ML revolved around <em id="S5.SS5.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">obtaining the required datasets</em> for training the model. Several factors like ‘lack of cost’, ‘lack of software/tools/methods’, ‘lack of representative datasets’, and ‘lack of control over data collection’ were reported that led to this challenge as discussed in Section <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>. While factors like ‘lack of cost’, ‘lack of software/tools/methods’, and ‘lack of control over data collection’ could be addressable to improve the development of a fair AI/ML, the majority of the participants reported the ‘lack of representative datasets’ in the real world as one of the factors leading to the challenge of obtaining required datasets. For example, participants [P1] and [P15] said,</p>
<blockquote id="S5.SS5.SSS3.p1.2" class="ltx_quote">
<span id="S5.SS5.SSS3.p1.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S5.SS5.SSS3.p1.2.2" class="ltx_p"><em id="S5.SS5.SSS3.p1.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“In the real world, normally, we don’t get perfect data to train the model.</em><span id="S5.SS5.SSS3.p1.2.2.2" class="ltx_text" style="font-size:90%;"> - [P1]
<br class="ltx_break"><span id="S5.SS5.SSS3.p1.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S5.SS5.SSS3.p1.2.2.2.2" class="ltx_emph ltx_font_italic">“So for example, in our human body projects, a lot of things are on bell curves in terms of like weight and height. And so it’s often very difficult to get those data at the edges of the bell curve, you don’t usually have a lot of very, very, like obese people.”</em> - [P15]</span></p>
</blockquote>
</div>
<div id="S5.SS5.SSS3.p2" class="ltx_para">
<p id="S5.SS5.SSS3.p2.1" class="ltx_p">This particular factor, ‘the lack of representative datasets’, appears to be more persistent because we cannot change real-world data, and it may pose a greater challenge that is not easily overcome. It was intriguing to learn that some participants believe developing a fair AI/ML is not possible, asserting that while bias can be minimized, it cannot be entirely eradicated from the systems. For example, participants [P9] and [P11] said:</p>
<blockquote id="S5.SS5.SSS3.p2.2" class="ltx_quote">
<span id="S5.SS5.SSS3.p2.2.1" class="ltx_ERROR undefined">\faCommenting</span>
<p id="S5.SS5.SSS3.p2.2.2" class="ltx_p"><em id="S5.SS5.SSS3.p2.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">“The real world is not perfect so you don’t have all the datasets you need. So you won’t be able to remove some sort of biases from them. You will have some sort of biases, the only thing that we can do is reduce it to a certain acceptable level. We won’t be getting a perfect fair model. It’s not there. Model reflects the data. Real data is not perfect. So you cannot expect a perfect model with it.”</em><span id="S5.SS5.SSS3.p2.2.2.2" class="ltx_text" style="font-size:90%;"> - [P9] 
<br class="ltx_break"><span id="S5.SS5.SSS3.p2.2.2.2.1" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S5.SS5.SSS3.p2.2.2.2.2" class="ltx_emph ltx_font_italic">“”In my view, because we are talking about bias here, like, there is no model I mean, even in AI, as far as I know, there is no perfect machine or model when prediction is involved. So in my opinion, I guess like a fair model should be something that decreases the bias, as you know, as I mean, it should decrease it. It should decrease it to be very less. But then I don’t think there can be any model, which is not biased.”</em> - [P11]</span></p>
</blockquote>
</div>
<div id="S5.SS5.SSS3.p3" class="ltx_para">
<p id="S5.SS5.SSS3.p3.1" class="ltx_p">Because only a small number of participants in our study talked about this subject, there is room for further investigation into why AI practitioners believe developing a perfectly fair (bias-free) AI/ML system is not feasible.</p>
</div>
</section>
<section id="S5.SS5.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.5.4. </span>Organizational Impact vs. User Well-being</h4>

<div id="S5.SS5.SSS4.p1" class="ltx_para">
<p id="S5.SS5.SSS4.p1.1" class="ltx_p">We found that participants in our study believed that the repercussions of developing an unfair AI/ML impact organizations more significantly than the users who interact with it. When asked about the consequences of developing an unfair AI/ML, the majority highlighted potential financial losses and damage to the organization’s reputation. Participants expressed opinions such as <span id="S5.SS5.SSS4.p1.1.1" class="ltx_ERROR undefined">\faCommenting</span><span id="S5.SS5.SSS4.p1.1.2" class="ltx_text" style="font-size:90%;">  <em id="S5.SS5.SSS4.p1.1.2.1" class="ltx_emph ltx_font_italic">“it could ultimately tarnish the image of these organizations (Twitter)”</em> and <span id="S5.SS5.SSS4.p1.1.2.2" class="ltx_ERROR undefined">\faCommenting</span>  <em id="S5.SS5.SSS4.p1.1.2.3" class="ltx_emph ltx_font_italic">“incur a substantial loss for the company”</em>. </span>Only a small number of participants mentioned that users may experience emotional distress and discrimination when unfair AI/ML systems are developed. Participants quoted, <span id="S5.SS5.SSS4.p1.1.3" class="ltx_ERROR undefined">\faCommenting</span><span id="S5.SS5.SSS4.p1.1.4" class="ltx_text" style="font-size:90%;">  <em id="S5.SS5.SSS4.p1.1.4.1" class="ltx_emph ltx_font_italic">“the sentiments of the people can be hurt”</em>. </span>The limited acknowledgment of potential user experiences, such as emotional distress and discrimination, suggests a potential gap in awareness or consideration of the individual implications of unfair AI/ML systems. It may highlight a tendency to prioritize the broader consequences for organizations over the direct effects on the individuals interacting with these systems.</p>
</div>
</section>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>Implications</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">This section outlines implications for researchers and AI practitioners involved in AI/ML development, derived from our study findings. Additionally, we offer recommendations for AI practitioners and AI companies to assist in the development of fair AI/ML systems.</p>
</div>
<section id="S5.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.1. </span>Implications for Research and Future Work</h4>

<div id="S5.SS6.SSS1.p1" class="ltx_para">
<p id="S5.SS6.SSS1.p1.1" class="ltx_p">We developed a framework to show the relationship between AI practitioners’ <em id="S5.SS6.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’ and the associated <em id="S5.SS6.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">challenges</em> in developing a fair AI/ML, the <em id="S5.SS6.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them, and the <em id="S5.SS6.SSS1.p1.1.4" class="ltx_emph ltx_font_italic">strategies</em> employed to ensure fairness in AI/ML systems (Figure <a href="#S4.F8" title="Figure 8 ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) based on empirical findings. This framework can be used to identify patterns, and potential areas for intervention, ultimately contributing to a more nuanced understanding of how to enhance fairness in AI/ML. The insights drawn from this framework can inform future studies, shaping the direction of research in the field. Researchers can use our findings for future research in the following areas:</p>
</div>
<div id="S5.SS6.SSS1.p2" class="ltx_para">
<span id="S5.SS6.SSS1.p2.1" class="ltx_ERROR undefined">\faHandORight</span>
<p id="S5.SS6.SSS1.p2.2" class="ltx_p"><span id="S5.SS6.SSS1.p2.2.1" class="ltx_text ltx_font_bold">Investigating factors and solutions for the challenge of obtaining required datasets:</span> Our findings reveal that a common challenge faced by AI practitioners who shared their understanding of ‘fair AI/ML’ either in terms of the absence of <em id="S5.SS6.SSS1.p2.2.2" class="ltx_emph ltx_font_italic">bias</em> or in terms of the presence of desirable <em id="S5.SS6.SSS1.p2.2.3" class="ltx_emph ltx_font_italic">attributes</em> in AI/ML such as transparency, interpretability, accuracy etc. was obtaining necessary datasets during AI/ML development, as discussed in Section <a href="#S4.SS7.SSS1" title="4.7.1. Relationship between AI practitioners’ understanding and their challenges ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7.1</span></a>. Future work can focus on addressing more factors leading to this challenge and investigating approaches to mitigate them, which can contribute to the development of a fair AI/ML.</p>
</div>
<div id="S5.SS6.SSS1.p3" class="ltx_para">
<span id="S5.SS6.SSS1.p3.1" class="ltx_ERROR undefined">\faHandORight</span>
<p id="S5.SS6.SSS1.p3.2" class="ltx_p"><span id="S5.SS6.SSS1.p3.2.1" class="ltx_text ltx_font_bold">Mapping countries’/companies’ definitions of ‘AI fairness’ with practitioners’ understanding:</span> Our findings reveal variations in AI practitioners’ understanding of ‘fair AI/ML’ compared to definitions set by different countries and tech companies on ‘AI fairness’ (section <a href="#S5.SS1" title="5.1. Definition/understanding of AI/ML fairness ‣ 5. Discussion ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Future research could explore alignment between these perspectives through a mapping exercise.</p>
</div>
<div id="S5.SS6.SSS1.p4" class="ltx_para">
<span id="S5.SS6.SSS1.p4.1" class="ltx_ERROR undefined">\faHandORight</span>
<p id="S5.SS6.SSS1.p4.2" class="ltx_p"><span id="S5.SS6.SSS1.p4.2.1" class="ltx_text ltx_font_bold">Delving deeper into strategies for ensuring fairness in AI/ML:</span> Our findings show that AI practitioners commonly use mitigating bias (bias-related strategy) to ensure fairness in AI/ML, regardless of how they describe ‘fair AI/ML’, as discussed in Section <a href="#S4.SS7.SSS3" title="4.7.3. Relationship between AI practitioners’ understanding and their strategies ‣ 4.7. Framework showing the relationship between the aspects ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7.3</span></a>. Similarly, the participants discussed the strategy of detecting inaccuracy to ensure fairness in AI/ML; however, there is no mention of strategies to address accuracy-related issues for ensuring fairness. Future research can delve into why mitigating bias is the predominant strategy and explore if practitioners employ strategies to address accuracy-related issues in the system to ensure fairness. This may help to inform the development of comprehensive strategies to address both bias and accuracy concerns, leading to more robust and fair AI/ML systems.</p>
</div>
<div id="S5.SS6.SSS1.p5" class="ltx_para">
<span id="S5.SS6.SSS1.p5.1" class="ltx_ERROR undefined">\faHandORight</span>
<p id="S5.SS6.SSS1.p5.2" class="ltx_p"><span id="S5.SS6.SSS1.p5.2.1" class="ltx_text ltx_font_bold">Exploring links between various aspects:</span> In our study, we explored the link between what AI practitioners <em id="S5.SS6.SSS1.p5.2.2" class="ltx_emph ltx_font_italic">understand</em> by ‘fair AI/ML’ and the <em id="S5.SS6.SSS1.p5.2.3" class="ltx_emph ltx_font_italic">challenges</em> they face in development, the <em id="S5.SS6.SSS1.p5.2.4" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them, and the <em id="S5.SS6.SSS1.p5.2.5" class="ltx_emph ltx_font_italic">strategies</em> they employed to ensure fairness of an AI/ML. In the future, researchers can delve into exploring connections between other aspects, such as the challenges encountered in developing fair AI/ML and the consequences of developing an unfair AI/ML perceived by AI practitioners. This may help to uncover deeper insights and connections within the complex landscape of developing a fair AI/ML, guiding researchers in refining methodologies, devising more effective strategies, and advancing fair and ethical practices in AI/ML development.</p>
</div>
</section>
<section id="S5.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.2. </span>Implications for Practice and Recommendations</h4>

<div id="S5.SS6.SSS2.p1" class="ltx_para">
<p id="S5.SS6.SSS2.p1.1" class="ltx_p">Our study focuses on investigating AI practitioners’ experiences and perceptions about various aspects related to the development of a fair AI/ML. We conducted semi-structured interviews with 22 AI practitioners, exploring their <span id="S5.SS6.SSS2.p1.1.1" class="ltx_text ltx_font_bold">understanding</span> of ‘fair AI/ML’, the <span id="S5.SS6.SSS2.p1.1.2" class="ltx_text ltx_font_bold">challenges</span> encountered in its development, <span id="S5.SS6.SSS2.p1.1.3" class="ltx_text ltx_font_bold">consequences</span> of developing an unfair AI/ML, and the <span id="S5.SS6.SSS2.p1.1.4" class="ltx_text ltx_font_bold">strategies</span> employed to ensure the fairness of an AI/ML. Our findings provide AI practitioners with valuable insights, into how people in the same field understand a ‘fair AI/ML’, the challenges they encounter, the consequences of developing an unfair AI/ML, and the strategies they employ to ensure fairness in an AI/ML. This comprehensive understanding, derived from real-world experiences, can inform practitioners’ approaches, enhance decision-making, and contribute to the use of more effective strategies for developing a fair AI/ML. It may provide a practical and grounded perspective that can guide practitioners in navigating the complexities of fairness in their AI development processes.</p>
</div>
<div id="S5.SS6.SSS2.p2" class="ltx_para">
<p id="S5.SS6.SSS2.p2.1" class="ltx_p">Drawing from our study’s findings, we present some recommendations for AI practitioners and AI companies to support the development of a fair AI/ML, as detailed below.</p>
</div>
<div id="S5.SS6.SSS2.p3" class="ltx_para">
<span id="S5.SS6.SSS2.p3.1" class="ltx_ERROR undefined">\faLightbulbO</span>
<p id="S5.SS6.SSS2.p3.2" class="ltx_p"><span id="S5.SS6.SSS2.p3.2.1" class="ltx_text ltx_font_bold">Recommendation 1: Striking a balance between the fairness of a system and its working version:</span> Several participants in our study highlighted the challenge of developing their envisioned ideal system, attributing it to factors like a shortage of time. Consequently, they prioritize creating a functional system over ensuring its fairness, as discussed in Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. AI managers can help AI practitioners by fostering a culture that values and prioritizes fairness in AI/ML development. They can allocate resources, both in terms of time and support, to enable practitioners to strike a balance between developing a working system and ensuring its fairness.</p>
</div>
<div id="S5.SS6.SSS2.p4" class="ltx_para">
<span id="S5.SS6.SSS2.p4.1" class="ltx_ERROR undefined">\faLightbulbO</span>
<p id="S5.SS6.SSS2.p4.2" class="ltx_p"><span id="S5.SS6.SSS2.p4.2.1" class="ltx_text ltx_font_bold">Recommendation 2: Providing AI practitioners with necessary software/tools/methods:</span> Many participants in our study emphasized the challenges of developing a fair AI/ML, citing a lack of software, tools, or methods as discussed in Sections <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a> and <a href="#S4.SS3.SSS2" title="4.3.2. Resource-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>. They specifically pointed out challenges in detecting and addressing data bias and obtaining necessary datasets due to the absence of adequate tools. AI companies can provide substantial support by investing in the development and provision of specialized software, tools, and methods aimed at addressing the challenges highlighted by participants <cite class="ltx_cite ltx_citemacro_citep">(Holstein et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S5.SS6.SSS2.p5" class="ltx_para">
<span id="S5.SS6.SSS2.p5.1" class="ltx_ERROR undefined">\faLightbulbO</span>
<p id="S5.SS6.SSS2.p5.2" class="ltx_p"><span id="S5.SS6.SSS2.p5.2.1" class="ltx_text ltx_font_bold">Recommendation 3: Focusing on enhancing own knowledge and awareness of different concepts:</span> The majority of participants in our study acknowledged facing challenges in grasping the concepts of ‘bias’ and ‘fairness’ as discussed in Section <a href="#S4.SS3.SSS3" title="4.3.3. Team-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.3</span></a>. They attributed this challenge to a lack of awareness and knowledge about these concepts, as well as a deficit in understanding the domain they work in. AI practitioners can take proactive steps such as seeking additional training or education on the concepts of ‘bias’ and ‘fairness.’ Engaging in domain-specific learning to enhance their understanding of the context they work in might also be beneficial. Staying informed about the latest developments and best practices in AI fairness can contribute to a more comprehensive understanding of these concepts.</p>
</div>
<div id="S5.SS6.SSS2.p6" class="ltx_para">
<span id="S5.SS6.SSS2.p6.1" class="ltx_ERROR undefined">\faLightbulbO</span>
<p id="S5.SS6.SSS2.p6.2" class="ltx_p"><span id="S5.SS6.SSS2.p6.2.1" class="ltx_text ltx_font_bold">Recommendation 4: Prioritizing users in AI/ML development:</span>
In discussions about the consequences of developing unfair AI/ML systems perceived by the participants, most participants focused on the negative impacts on organizations, including financial losses and reputational repercussions (Section <a href="#S5" title="5. Discussion ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Interestingly, only a small number recognized the potential emotional distress and discrimination experienced by end users as a consequence of such systems. AI practitioners can make a conscious effort to shift the focus from solely considering organizational consequences to understanding the direct impact on individuals. This might allow them to identify and address potential biases and discriminatory outcomes, contributing to the development of fair and inclusive AI/ML systems that treat users equitably.</p>
</div>
<div id="S5.SS6.SSS2.p7" class="ltx_para">
<span id="S5.SS6.SSS2.p7.1" class="ltx_ERROR undefined">\faLightbulbO</span>
<p id="S5.SS6.SSS2.p7.2" class="ltx_p"><span id="S5.SS6.SSS2.p7.2.1" class="ltx_text ltx_font_bold">Recommendation 5: Updating and adapting AI ethics policies in organizations:</span>
Participants in our study identified challenges in adhering to policies and regulations within their organizations, citing outdated AI ethics policies and a lack of adaptation as the factors leading to those challenges, as discussed in Section <a href="#S4.SS3.SSS1" title="4.3.1. Process-related challenges ‣ 4.3. RQ 2- What challenges do AI practitioners face in developing a fair AI/ML and what are the factors that lead to those challenges? ‣ 4. Findings ‣ Navigating Fairness: Practitioners’ Understanding, Challenges, and Strategies in AI/ML Development" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>. To address this, AI companies can prioritize updating and adapting their AI ethics policies, ensuring strict adherence by practitioners. This proactive approach can help ensure that AI practitioners are equipped with the latest guidelines to navigate complex ethical challenges, promoting responsible AI development.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Limitations and Threats to Validity</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While we advertised our study on platforms such as LinkedIn and Twitter to attract participants globally, our data collection lacks an even distribution of participants worldwide. The majority of study participants are based in Australia. The findings of our study hold the most relevance for participants’ organizations and their respective countries, potentially extending to similar contexts. However, generalizing these findings to the entire global software engineering community is deemed impractical in practice <cite class="ltx_cite ltx_citemacro_citep">(Masood et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Likewise, our main interview guide was developed after conducting two pilot interviews. The interview recordings underwent automatic transcription, and any errors introduced during this process were manually rectified by listening to each audio recording during the coding phase. In the interviews, there could be a possibility of misalignment between our intended questions and participants’ understanding, leading to potential misinterpretations or misunderstandings. To address this, we employed follow-up questions to ensure clarity on the participants’ statements.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">All four authors were involved in designing the interview guide, with the initial coding primarily handled by the first author. However, all authors actively participated in refining and finalizing the codes, concepts, and categories through collaborative discussions. We have also included various interview quotes as examples, aiming to minimize any potential reporting biases in the study.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In addition, there could be a potential risk to the research’s internal validity when using the payment for the second round of data collection. As a way of mitigating this risk, we initially provided the candidates with an anonymous survey asking them for their years of experience in AI/ML development. Using this information, we selected participants for interviews, and approval for payment was granted only after confirming alignment with our predetermined participation criteria. The process was carried out with ethics approval. Candidates with no experience in AI/ML development were not selected for the interview.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This study aimed to investigate AI practitioners’ perspectives and experiences in developing a fair AI/ML, recognizing their pivotal role in development and deployment. The study contributes to gaining insights into the industry’s standpoint on the <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">understanding</em> of a ‘fair AI/ML’, the <em id="S7.p1.1.2" class="ltx_emph ltx_font_italic">challenges</em> involved in its development, the <em id="S7.p1.1.3" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them, and the <em id="S7.p1.1.4" class="ltx_emph ltx_font_italic">strategies</em> they employed to ensure fairness of an AI/ML system.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We conducted semi-structured interviews with 22 AI practitioners to fulfill the objective of our study and analyzed the qualitative data using the <em id="S7.p2.1.1" class="ltx_emph ltx_font_italic">STGT for data analysis</em> <cite class="ltx_cite ltx_citemacro_citep">(Hoda, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. The analysis revealed two categories of AI practitioners’ <span id="S7.p2.1.2" class="ltx_text ltx_font_bold">understanding</span> of ‘fair AI/ML’ including, (i) <em id="S7.p2.1.3" class="ltx_emph ltx_font_italic">In terms of the absence of bias</em> and (ii) <em id="S7.p2.1.4" class="ltx_emph ltx_font_italic">In terms of the presence of desirable attributes</em> in AI/ML. We also categorized the <span id="S7.p2.1.5" class="ltx_text ltx_font_bold">challenges</span> of the participants in developing a fair AI/ML into three sections including, (i) <em id="S7.p2.1.6" class="ltx_emph ltx_font_italic">Process-related challenges</em>, (ii) <em id="S7.p2.1.7" class="ltx_emph ltx_font_italic">Resource-related challenges</em>, and (iii) <em id="S7.p2.1.8" class="ltx_emph ltx_font_italic">Team-related challenges</em>. Similarly, our analysis showed three categories of negative <span id="S7.p2.1.9" class="ltx_text ltx_font_bold">consequences</span> perceived by participants in developing an unfair AI/ML: (i) <em id="S7.p2.1.10" class="ltx_emph ltx_font_italic">Impact on organizations</em>, (ii) <em id="S7.p2.1.11" class="ltx_emph ltx_font_italic">Impact on users</em>, and (iii) <em id="S7.p2.1.12" class="ltx_emph ltx_font_italic">Impact on practitioners</em>. We also classified the <span id="S7.p2.1.13" class="ltx_text ltx_font_bold">strategies</span> employed by participants to ensure the fairness of an AI/ML into two categories: (i) <em id="S7.p2.1.14" class="ltx_emph ltx_font_italic">Bias-related strategies</em> and (ii) <em id="S7.p2.1.15" class="ltx_emph ltx_font_italic">Performance-related strategies</em>. Based on the findings, we also developed a framework to show the relationship between AI practitioners’ <em id="S7.p2.1.16" class="ltx_emph ltx_font_italic">understanding</em> of ‘fair AI/ML’ and three other aspects including, (i) their <em id="S7.p2.1.17" class="ltx_emph ltx_font_italic">challenges</em> in developing a fair AI/ML, (ii) the <em id="S7.p2.1.18" class="ltx_emph ltx_font_italic">consequences</em> of developing an unfair AI/ML perceived by them and (iii) their <em id="S7.p2.1.19" class="ltx_emph ltx_font_italic">strategies</em> to ensure the fairness of an AI/ML.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Our findings offer valuable insights into the industry’s perspective and experiences in developing a fair AI/ML, aiding the AI research community in better understanding how AI practitioners perceive and experience this process. We also identified areas that need further investigation within the AI research community, enabling researchers to make more informed decisions about the direction of their studies. This might ensure that their efforts address the critical areas identified by the study for further exploration. We also offered recommendations to AI practitioners and AI companies, aiming to assist in enhancing the development of a fair AI/ML.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Acknowledgments</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Aastha Pant is supported by the Faculty of IT Ph.D. scholarship from Monash University. C. Tantithamthavorn is partially supported by the Australian Research Council’s
Discovery Early Career Researcher Award (DECRA) funding scheme (DE200100941). We would like to thank all the interviewees for their participation in our study.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angwin et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016.

</span>
<span class="ltx_bibblock">Machine Bias.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Augustin et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Larry Augustin, Dan Bressler, and Guy Smith. 2002.

</span>
<span class="ltx_bibblock">Accelerating software development through collaboration. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th International Conference on Software Engineering</em>. 559–563.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/581339.581409" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/581339.581409</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bacelar (2021)</span>
<span class="ltx_bibblock">
Marley Bacelar. 2021.

</span>
<span class="ltx_bibblock">Monitoring bias and fairness in machine learning models: A review.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ScienceOpen Preprints</em> (2021).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.14293/S2199-1006.1.SOR-.PP59WRH.v1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.14293/S2199-1006.1.SOR-.PP59WRH.v1</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balayn et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Agathe Balayn, Mireia Yurrita, Jie Yang, and Ujwal Gadiraju. 2023.

</span>
<span class="ltx_bibblock">“Fairness toolkits, A checkbox culture?” On the factors that fragment developer practices in handling algorithmic harms. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society</em>. 482–495.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3600211.3604674" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3600211.3604674</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baltes and Ralph (2022)</span>
<span class="ltx_bibblock">
Sebastian Baltes and Paul Ralph. 2022.

</span>
<span class="ltx_bibblock">Sampling in software engineering research: A critical review and guidelines.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Empirical Software Engineering</em> 27, 4 (2022), 94.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s10664-021-10072-8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10664-021-10072-8</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binns (2018)</span>
<span class="ltx_bibblock">
Reuben Binns. 2018.

</span>
<span class="ltx_bibblock">Fairness in machine learning: Lessons from political philosophy. In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Conference on Fairness, Accountability and Transparency</em>. PMLR, 149–159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caliskan et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language corpora contain human-like biases.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Science</em> 356, 6334 (2017), 183–186.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1126/science.aal42" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1126/science.aal42</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caton and Haas (2020)</span>
<span class="ltx_bibblock">
Simon Caton and Christian Haas. 2020.

</span>
<span class="ltx_bibblock">Fairness in machine learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Comput. Surveys</em> (2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3616865" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3616865</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Pu Chen, Linna Wu, and Lei Wang. 2023.

</span>
<span class="ltx_bibblock">AI fairness in data management and analytics: A review on challenges, methodologies and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 13, 18 (2023), 10258.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3390/app131810258" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/app131810258</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chouldechova and Roth (2018)</span>
<span class="ltx_bibblock">
Alexandra Chouldechova and Aaron Roth. 2018.

</span>
<span class="ltx_bibblock">The frontiers of fairness in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.08810</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’Amour et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, David Sculley, and Yoni Halpern. 2020.

</span>
<span class="ltx_bibblock">Fairness is not static: Deeper understanding of long term fairness via simulation studies. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>. 525–534.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3351095.3372878" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3351095.3372878</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Wesley Hanwen Deng, Manish Nagireddy, Michelle Seng Ah Lee, Jatinder Singh, Zhiwei Steven Wu, Kenneth Holstein, and Haiyi Zhu. 2022.

</span>
<span class="ltx_bibblock">Exploring how machine learning practitioners (try to) use fairness toolkits. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. 473–484.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3531146.3533113" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3531146.3533113</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DrivenData (2024)</span>
<span class="ltx_bibblock">
DrivenData. 2024.

</span>
<span class="ltx_bibblock">Deon.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://deon.drivendata.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://deon.drivendata.org/</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fenu et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Gianni Fenu, Roberta Galici, and Mirko Marras. 2022.

</span>
<span class="ltx_bibblock">Experts’ view on challenges and needs for fairness in artificial intelligence for education. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence in Education</em>. Springer, 243–255.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-3-031-11644-5_20" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-031-11644-5_20</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finkelstein et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Anthony Finkelstein, Mark Harman, S Afshin Mansouri, Jian Ren, and Yuanyuan Zhang. 2008.

</span>
<span class="ltx_bibblock">“Fairness analysis” in requirements assignments. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">2008 16th IEEE International Requirements Engineering Conference</em>. IEEE, 115–124.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/RE.2008.61" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/RE.2008.61</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedler et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P Hamilton, and Derek Roth. 2019.

</span>
<span class="ltx_bibblock">A comparative study of fairness-enhancing interventions in machine learning. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability, and transparency</em>. 329–338.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3287560.3287589" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3287560.3287589</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2022)</span>
<span class="ltx_bibblock">
Google. 2022.

</span>
<span class="ltx_bibblock">Responsible AI practices.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://ai.google/responsibility/responsible-ai-practices/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/responsibility/responsible-ai-practices/</a>

</span>
<span class="ltx_bibblock">Accessed 10 January 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Government (2019)</span>
<span class="ltx_bibblock">
Australian Government. 2019.

</span>
<span class="ltx_bibblock">Australia’s AI Ethics Principles.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles</a>

</span>
<span class="ltx_bibblock">Accessed 10 January 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Group (2019)</span>
<span class="ltx_bibblock">
High-Level Expert Group. 2019.

</span>
<span class="ltx_bibblock">Ethics guidelines for trustworthy AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</a>

</span>
<span class="ltx_bibblock">Accessed 10 January 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Habibullah et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Khan Mohammad Habibullah, Gregory Gay, and Jennifer Horkoff. 2023.

</span>
<span class="ltx_bibblock">Non-functional requirements for machine learning: Understanding current use and challenges among practitioners.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Requirements Engineering</em> 28, 2 (2023), 283–316.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s00766-022-00395-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s00766-022-00395-3</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harrison et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Galen Harrison, Julia Hanson, Christine Jacinto, Julio Ramirez, and Blase Ur. 2020.

</span>
<span class="ltx_bibblock">An empirical study on the perceived fairness of realistic, imperfect machine learning models. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>. 392–402.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3351095.3372831" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3351095.3372831</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoda (2021)</span>
<span class="ltx_bibblock">
Rashina Hoda. 2021.

</span>
<span class="ltx_bibblock">Socio-technical grounded theory for software engineering.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em> 48, 10 (2021), 3808–3832.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TSE.2021.3106280" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TSE.2021.3106280</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holstein et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach. 2019.

</span>
<span class="ltx_bibblock">Improving fairness in machine learning systems: What do industry practitioners need?. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1–16.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3290605.3300830" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3290605.3300830</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hopkins and Booth (2021)</span>
<span class="ltx_bibblock">
Aspen Hopkins and Serena Booth. 2021.

</span>
<span class="ltx_bibblock">Machine learning practices outside big tech: How resource constraints challenge responsible development. In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</em>. ACM New York, United States, 134–145.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3461702.3462527" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3461702.3462527</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua and Belfield (2020)</span>
<span class="ltx_bibblock">
Shin-Shin Hua and Haydn Belfield. 2020.

</span>
<span class="ltx_bibblock">AI &amp; Antitrust: Reconciling tensions between competition law and cooperative AI development.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Yale JL &amp; Tech.</em> 23 (2020), 415.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchinson and Mitchell (2019)</span>
<span class="ltx_bibblock">
Ben Hutchinson and Margaret Mitchell. 2019.

</span>
<span class="ltx_bibblock">50 years of test (un) fairness: Lessons for machine learning. In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Fairness, Accountability, and Transparency</em>. ACM New York, USA, 49–58.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3287560.3287600" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3287560.3287600</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IBM (2022)</span>
<span class="ltx_bibblock">
IBM. 2022.

</span>
<span class="ltx_bibblock">Everyday ethics for AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.ibm.com/design/ai/ethics/everyday-ethics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ibm.com/design/ai/ethics/everyday-ethics</a>

</span>
<span class="ltx_bibblock">Accessed 10 January 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IBM (2024a)</span>
<span class="ltx_bibblock">
IBM. 2024a.

</span>
<span class="ltx_bibblock">AI FactSheets.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/factsheets-model-inventory.html?context=cpdaas" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/factsheets-model-inventory.html?context=cpdaas</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IBM (2024b)</span>
<span class="ltx_bibblock">
IBM. 2024b.

</span>
<span class="ltx_bibblock">AI Fairness 360.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.ibm.com/opensource/open/projects/ai-fairness-360/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ibm.com/opensource/open/projects/ai-fairness-360/</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Brun (2022)</span>
<span class="ltx_bibblock">
Brittany Johnson and Yuriy Brun. 2022.

</span>
<span class="ltx_bibblock">Fairkit-learn: A fairness evaluation and comparison toolkit. In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings</em>. 70–74.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3510454.3516830" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3510454.3516830</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaio et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Michael Madaio, Lisa Egede, Hariharan Subramonyam, Jennifer Wortman Vaughan, and Hanna Wallach. 2022.

</span>
<span class="ltx_bibblock">Assessing the fairness of AI systems: AI practitioners’ processes, challenges, and needs for support.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer Interaction</em> 6, CSCW1 (2022), 1–26.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3512899" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3512899</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaio et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020.

</span>
<span class="ltx_bibblock">Co-designing checklists to understand organizational challenges and opportunities around fairness in AI. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>. ACM New York, USA, 1–14.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3313831.3376445" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3313831.3376445</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumder et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Suvodeep Majumder, Joymallya Chakraborty, Gina R Bai, Kathryn T Stolee, and Tim Menzies. 2023.

</span>
<span class="ltx_bibblock">Fair enough: Searching for sufficient measures of fairness.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Software Engineering and Methodology</em> 32, 6 (2023), 1–22.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3585006" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3585006</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcinkowski et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Frank Marcinkowski, Kimon Kieslich, Christopher Starke, and Marco Lünich. 2020.

</span>
<span class="ltx_bibblock">Implications of AI (un-) fairness in higher education admissions: The effects of perceived AI (un-) fairness on exit, voice and organizational reputation. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>. ACM New York, USA, 122–130.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3351095.3372867" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3351095.3372867</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin (2018)</span>
<span class="ltx_bibblock">
Nicole Martin. 2018.

</span>
<span class="ltx_bibblock">Are AI hiring programs eliminating bias or making it worse?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.forbes.com/sites/nicolemartin1/2018/12/13/are-ai-hiring-programs-eliminating-bias-or-making-it-worse/?sh=552bb0cc22b8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.forbes.com/sites/nicolemartin1/2018/12/13/are-ai-hiring-programs-eliminating-bias-or-making-it-worse/?sh=552bb0cc22b8</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masood et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zainab Masood, Rashina Hoda, and Kelly Blincoe. 2020.

</span>
<span class="ltx_bibblock">How agile teams make self-assignment work: A grounded theory study.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Empirical Software Engineering</em> 25 (2020), 4962–5005.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s10664-020-09876-x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10664-020-09876-x</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabi et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021.

</span>
<span class="ltx_bibblock">A survey on bias and fairness in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em> 54, 6 (2021), 1–35.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3457607" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3457607</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft (2024a)</span>
<span class="ltx_bibblock">
Microsoft. 2024a.

</span>
<span class="ltx_bibblock">AI Fairness Checklist.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/</a>

</span>
<span class="ltx_bibblock">Accessed 17 January 2024.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft (2024b)</span>
<span class="ltx_bibblock">
Microsoft. 2024b.

</span>
<span class="ltx_bibblock">Microsoft Responsible AI Standard.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6</a>

</span>
<span class="ltx_bibblock">Accessed 10 January 2024.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orr and Davis (2020)</span>
<span class="ltx_bibblock">
Will Orr and Jenny L Davis. 2020.

</span>
<span class="ltx_bibblock">Attributions of ethical responsibility by artificial intelligence practitioners.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Information, Communication &amp; Society</em> 23, 5 (2020), 719–735.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1080/1369118X.2020.1713842" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/1369118X.2020.1713842</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pagano et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tiago P Pagano, Rafael B Loureiro, Fernanda VN Lisboa, Rodrigo M Peixoto, Guilherme AS Guimarães, Gustavo OR Cruz, Maira M Araujo, Lucas L Santos, Marco AS Cruz, Ewerton LS Oliveira, et al<span id="bib.bib42.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Bias and unfairness in machine learning models: A systematic review on datasets, tools, fairness metrics, and identification and mitigation methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.4.1" class="ltx_emph ltx_font_italic">Big Data and Cognitive Computing</em> 7, 1 (2023), 15.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3390/bdcc7010015" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/bdcc7010015</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pant et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Aastha Pant, Rashina Hoda, Simone V Spiegler, Chakkrit Tantithamthavorn, and Burak Turhan. 2023.

</span>
<span class="ltx_bibblock">Ethics in the age of AI: An analysis of AI practitioners’ awareness and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Software Engineering and Methodology</em> (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3635715" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3635715</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pessach and Shmueli (2022)</span>
<span class="ltx_bibblock">
Dana Pessach and Erez Shmueli. 2022.

</span>
<span class="ltx_bibblock">A review on fairness in machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em> 55, 3 (2022), 1–44.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3494672" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3494672</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prates et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Marcelo OR Prates, Pedro H Avelar, and Luís C Lamb. 2020.

</span>
<span class="ltx_bibblock">Assessing gender bias in machine translation: A case study with Google translate.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em> 32 (2020), 6363–6381.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s00521-019-04144-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s00521-019-04144-6</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Brianna Richardson, Jean Garcia-Gathright, Samuel F Way, Jennifer Thom, and Henriette Cramer. 2021.

</span>
<span class="ltx_bibblock">Towards fairness in practice: A practitioner-oriented rubric for evaluating fair ML toolkits. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. ACM New York, USA, 1–13.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3411764.3445604" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411764.3445604</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryan et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Seamus Ryan, Camille Nadal, and Gavin Doherty. 2023.

</span>
<span class="ltx_bibblock">Integrating fairness in the software design Process: An interview study with HCI and ML experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 11 (2023), 29296–29313.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ACCESS.2023.3260639" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2023.3260639</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seaman (1999)</span>
<span class="ltx_bibblock">
Carolyn B. Seaman. 1999.

</span>
<span class="ltx_bibblock">Qualitative methods in empirical studies of software engineering.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em> 25, 4 (1999), 557–572.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/32.799955" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/32.799955</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin and Park (2019)</span>
<span class="ltx_bibblock">
Donghee Shin and Yong Jin Park. 2019.

</span>
<span class="ltx_bibblock">Role of fairness, accountability, and transparency in algorithmic affordance.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em> 98 (2019), 277–284.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/j.chb.2019.04.019" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.chb.2019.04.019</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silberg and Manyika (2019)</span>
<span class="ltx_bibblock">
Jake Silberg and James Manyika. 2019.

</span>
<span class="ltx_bibblock">Notes from the AI frontier: Tackling bias in AI (and in humans).

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">McKinsey Global Institute</em> 1, 6 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019.

</span>
<span class="ltx_bibblock">Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. ACM New York, USA, 2459–2468.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3292500.3330664" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3292500.3330664</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ueda et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Daiju Ueda, Taichi Kakinuma, Shohei Fujita, Koji Kamagata, Yasutaka Fushimi, Rintaro Ito, Yusuke Matsui, Taiki Nozaki, Takeshi Nakaura, Noriyuki Fujima, et al<span id="bib.bib52.3.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">Fairness of artificial intelligence in healthcare: Review and recommendations.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.4.1" class="ltx_emph ltx_font_italic">Japanese Journal of Radiology</em> 42, 1 (2024), 3–15.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s11604-023-01474-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11604-023-01474-3</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasudevan and Kenthapadi (2020)</span>
<span class="ltx_bibblock">
Sriram Vasudevan and Krishnaram Kenthapadi. 2020.

</span>
<span class="ltx_bibblock">Lift: A scalable framework for measuring fairness in ML applications. In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. ACM New York, USA, 2773–2780.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3340531.3412705" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3340531.3412705</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verma and Rubin (2018)</span>
<span class="ltx_bibblock">
Sahil Verma and Julia Rubin. 2018.

</span>
<span class="ltx_bibblock">Fairness definitions explained. In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Workshop on Software Fairness</em>. ACM New York, USA, 1–7.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3194770.3194776" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3194770.3194776</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Mingyang Wan, Daochen Zha, Ninghao Liu, and Na Zou. 2023.

</span>
<span class="ltx_bibblock">In-processing modeling techniques for machine learning fairness: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Knowledge Discovery from Data</em> 17, 3 (2023), 1–27.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3551390" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3551390</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yue Wang, Yaxin Song, Zhuo Ma, and Xiaoxue Han. 2023.

</span>
<span class="ltx_bibblock">Multidisciplinary considerations of fairness in medical AI: A scoping review.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">International Journal of Medical Informatics</em> 178 (2023), 105175.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1016/j.ijmedinf.2023.105175" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.ijmedinf.2023.105175</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woodruff et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Allison Woodruff, Sarah E Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018.

</span>
<span class="ltx_bibblock">A qualitative exploration of perceptions of algorithmic fairness. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. ACM New York, USA, 1–14.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3173574.3174230" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3173574.3174230</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xivuri and Twinomurinzi (2021)</span>
<span class="ltx_bibblock">
Khensani Xivuri and Hossana Twinomurinzi. 2021.

</span>
<span class="ltx_bibblock">A systematic review of fairness in artificial intelligence algorithms. In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Responsible AI and Analytics for an Ethical and Inclusive Digitized Society</em>, Vol. 12896. Springer, 271–284.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-3-030-85447-8_24" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-85447-8_24</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiehuang Zhang, Ying Shu, and Han Yu. 2023.

</span>
<span class="ltx_bibblock">Fairness in design: A framework for facilitating ethical artificial intelligence designs.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">International Journal of Crowd Science</em> 7, 1 (2023), 32–39.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.26599/IJCS.2022.9100033" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.26599/IJCS.2022.9100033</a>

</span>
</li>
</ul>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Appendices</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:70%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix A: Interview Protocol</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Section A: Demographic Information (via <span id="A1.p1.1.1.1" class="ltx_text ltx_font_italic">Qualtrics</span>)</span><span id="A1.p1.1.2" class="ltx_text" style="font-size:70%;"></span></p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text" style="font-size:70%;">1. Your full name:</span></p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text" style="font-size:70%;">2. Please enter your email address so the researcher can contact you to schedule a time for an interview:</span></p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p"><span id="A1.p4.1.1" class="ltx_text" style="font-size:70%;">3. What is your current job title?</span></p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">AI Engineer</span></p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text" style="font-size:70%;">AI/ML/Data Scientist</span></p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text" style="font-size:70%;">AI/ML Expert</span></p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text" style="font-size:70%;">AI/ML Practitioner</span></p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text" style="font-size:70%;">AI/ML Developer</span></p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p"><span id="A1.I1.i6.p1.1.1" class="ltx_text" style="font-size:70%;">Other:</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.1" class="ltx_p"><span id="A1.p5.1.1" class="ltx_text" style="font-size:70%;">4. How many years of experience do you have in the area of AI/ML development?</span></p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p"><span id="A1.I2.i1.p1.1.1" class="ltx_text" style="font-size:70%;">No Experience</span></p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p"><span id="A1.I2.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Less than 1 year</span></p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p"><span id="A1.I2.i3.p1.1.1" class="ltx_text" style="font-size:70%;">Between 1 to 2 years</span></p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p"><span id="A1.I2.i4.p1.1.1" class="ltx_text" style="font-size:70%;">Between 3 to 5 years</span></p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p id="A1.I2.i5.p1.1" class="ltx_p"><span id="A1.I2.i5.p1.1.1" class="ltx_text" style="font-size:70%;">More than 5 years</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p6" class="ltx_para">
<p id="A1.p6.1" class="ltx_p"><span id="A1.p6.1.1" class="ltx_text" style="font-size:70%;">5. How old are you?</span></p>
<ul id="A1.I3" class="ltx_itemize">
<li id="A1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i1.p1" class="ltx_para">
<p id="A1.I3.i1.p1.1" class="ltx_p"><span id="A1.I3.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Below 20</span></p>
</div>
</li>
<li id="A1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i2.p1" class="ltx_para">
<p id="A1.I3.i2.p1.1" class="ltx_p"><span id="A1.I3.i2.p1.1.1" class="ltx_text" style="font-size:70%;">20-25</span></p>
</div>
</li>
<li id="A1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i3.p1" class="ltx_para">
<p id="A1.I3.i3.p1.1" class="ltx_p"><span id="A1.I3.i3.p1.1.1" class="ltx_text" style="font-size:70%;">26-30</span></p>
</div>
</li>
<li id="A1.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i4.p1" class="ltx_para">
<p id="A1.I3.i4.p1.1" class="ltx_p"><span id="A1.I3.i4.p1.1.1" class="ltx_text" style="font-size:70%;">31-35</span></p>
</div>
</li>
<li id="A1.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i5.p1" class="ltx_para">
<p id="A1.I3.i5.p1.1" class="ltx_p"><span id="A1.I3.i5.p1.1.1" class="ltx_text" style="font-size:70%;">36-40</span></p>
</div>
</li>
<li id="A1.I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i6.p1" class="ltx_para">
<p id="A1.I3.i6.p1.1" class="ltx_p"><span id="A1.I3.i6.p1.1.1" class="ltx_text" style="font-size:70%;">41-45</span></p>
</div>
</li>
<li id="A1.I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i7.p1" class="ltx_para">
<p id="A1.I3.i7.p1.1" class="ltx_p"><span id="A1.I3.i7.p1.1.1" class="ltx_text" style="font-size:70%;">46-50</span></p>
</div>
</li>
<li id="A1.I3.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i8.p1" class="ltx_para">
<p id="A1.I3.i8.p1.1" class="ltx_p"><span id="A1.I3.i8.p1.1.1" class="ltx_text" style="font-size:70%;">50</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p7" class="ltx_para">
<p id="A1.p7.1" class="ltx_p"><span id="A1.p7.1.1" class="ltx_text" style="font-size:70%;">6. How would you describe your gender?</span></p>
<ul id="A1.I4" class="ltx_itemize">
<li id="A1.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i1.p1" class="ltx_para">
<p id="A1.I4.i1.p1.1" class="ltx_p"><span id="A1.I4.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Woman</span></p>
</div>
</li>
<li id="A1.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i2.p1" class="ltx_para">
<p id="A1.I4.i2.p1.1" class="ltx_p"><span id="A1.I4.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Man</span></p>
</div>
</li>
<li id="A1.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i3.p1" class="ltx_para">
<p id="A1.I4.i3.p1.1" class="ltx_p"><span id="A1.I4.i3.p1.1.1" class="ltx_text" style="font-size:70%;">Non-binary/ gender diverse</span></p>
</div>
</li>
<li id="A1.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i4.p1" class="ltx_para">
<p id="A1.I4.i4.p1.1" class="ltx_p"><span id="A1.I4.i4.p1.1.1" class="ltx_text" style="font-size:70%;">My gender identity isn’t listed. I identify as:</span></p>
</div>
</li>
<li id="A1.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i5.p1" class="ltx_para">
<p id="A1.I4.i5.p1.1" class="ltx_p"><span id="A1.I4.i5.p1.1.1" class="ltx_text" style="font-size:70%;">Prefer not to say</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p8" class="ltx_para">
<p id="A1.p8.1" class="ltx_p"><span id="A1.p8.1.1" class="ltx_text" style="font-size:70%;">7. What is your country of residence?</span></p>
</div>
<div id="A1.p9" class="ltx_para">
<p id="A1.p9.1" class="ltx_p"><span id="A1.p9.1.1" class="ltx_text" style="font-size:70%;">8. What is the highest degree or level of education you have completed?</span></p>
<ul id="A1.I5" class="ltx_itemize">
<li id="A1.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i1.p1" class="ltx_para">
<p id="A1.I5.i1.p1.1" class="ltx_p"><span id="A1.I5.i1.p1.1.1" class="ltx_text" style="font-size:70%;">High School</span></p>
</div>
</li>
<li id="A1.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i2.p1" class="ltx_para">
<p id="A1.I5.i2.p1.1" class="ltx_p"><span id="A1.I5.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Bachelor degree</span></p>
</div>
</li>
<li id="A1.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i3.p1" class="ltx_para">
<p id="A1.I5.i3.p1.1" class="ltx_p"><span id="A1.I5.i3.p1.1.1" class="ltx_text" style="font-size:70%;">Master degree</span></p>
</div>
</li>
<li id="A1.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i4.p1" class="ltx_para">
<p id="A1.I5.i4.p1.1" class="ltx_p"><span id="A1.I5.i4.p1.1.1" class="ltx_text" style="font-size:70%;">Ph.D. or Higher</span></p>
</div>
</li>
<li id="A1.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i5.p1" class="ltx_para">
<p id="A1.I5.i5.p1.1" class="ltx_p"><span id="A1.I5.i5.p1.1.1" class="ltx_text" style="font-size:70%;">Prefer not to answer</span></p>
</div>
</li>
<li id="A1.I5.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I5.i6.p1" class="ltx_para">
<p id="A1.I5.i6.p1.1" class="ltx_p"><span id="A1.I5.i6.p1.1.1" class="ltx_text" style="font-size:70%;">Other:</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p10" class="ltx_para">
<p id="A1.p10.1" class="ltx_p"><span id="A1.p10.1.1" class="ltx_text" style="font-size:70%;">9. What activities are you involved in? Select </span><span id="A1.p10.1.2" class="ltx_text ltx_font_bold" style="font-size:70%;">all</span><span id="A1.p10.1.3" class="ltx_text" style="font-size:70%;"> that apply.</span></p>
<ul id="A1.I6" class="ltx_itemize">
<li id="A1.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i1.p1" class="ltx_para">
<p id="A1.I6.i1.p1.1" class="ltx_p"><span id="A1.I6.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Model requirements</span></p>
</div>
</li>
<li id="A1.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i2.p1" class="ltx_para">
<p id="A1.I6.i2.p1.1" class="ltx_p"><span id="A1.I6.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Data collection</span></p>
</div>
</li>
<li id="A1.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i3.p1" class="ltx_para">
<p id="A1.I6.i3.p1.1" class="ltx_p"><span id="A1.I6.i3.p1.1.1" class="ltx_text" style="font-size:70%;">Data cleaning</span></p>
</div>
</li>
<li id="A1.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i4.p1" class="ltx_para">
<p id="A1.I6.i4.p1.1" class="ltx_p"><span id="A1.I6.i4.p1.1.1" class="ltx_text" style="font-size:70%;">Data labeling</span></p>
</div>
</li>
<li id="A1.I6.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i5.p1" class="ltx_para">
<p id="A1.I6.i5.p1.1" class="ltx_p"><span id="A1.I6.i5.p1.1.1" class="ltx_text" style="font-size:70%;">Feature engineering</span></p>
</div>
</li>
<li id="A1.I6.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i6.p1" class="ltx_para">
<p id="A1.I6.i6.p1.1" class="ltx_p"><span id="A1.I6.i6.p1.1.1" class="ltx_text" style="font-size:70%;">Model training</span></p>
</div>
</li>
<li id="A1.I6.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i7.p1" class="ltx_para">
<p id="A1.I6.i7.p1.1" class="ltx_p"><span id="A1.I6.i7.p1.1.1" class="ltx_text" style="font-size:70%;">Model evaluation</span></p>
</div>
</li>
<li id="A1.I6.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i8.p1" class="ltx_para">
<p id="A1.I6.i8.p1.1" class="ltx_p"><span id="A1.I6.i8.p1.1.1" class="ltx_text" style="font-size:70%;">Model deployment</span></p>
</div>
</li>
<li id="A1.I6.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i9.p1" class="ltx_para">
<p id="A1.I6.i9.p1.1" class="ltx_p"><span id="A1.I6.i9.p1.1.1" class="ltx_text" style="font-size:70%;">Model monitoring</span></p>
</div>
</li>
<li id="A1.I6.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I6.i10.p1" class="ltx_para">
<p id="A1.I6.i10.p1.1" class="ltx_p"><span id="A1.I6.i10.p1.1.1" class="ltx_text" style="font-size:70%;">Other:</span></p>
</div>
</li>
</ul>
</div>
<div id="A1.p11" class="ltx_para">
<p id="A1.p11.1" class="ltx_p"><span id="A1.p11.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Section B: Practitioners’ Perception and Experiences on Developing a Fair AI/ML (via semi-structured interviews)</span><span id="A1.p11.1.2" class="ltx_text" style="font-size:70%;"></span></p>
</div>
<div id="A1.p12" class="ltx_para">
<ol id="A1.I7" class="ltx_enumerate">
<li id="A1.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="A1.I7.i1.p1" class="ltx_para">
<p id="A1.I7.i1.p1.1" class="ltx_p"><span id="A1.I7.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Can you briefly tell me about your professional background and current role?</span></p>
</div>
</li>
<li id="A1.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="A1.I7.i2.p1" class="ltx_para">
<p id="A1.I7.i2.p1.1" class="ltx_p"><span id="A1.I7.i2.p1.1.1" class="ltx_text" style="font-size:70%;">Are you aware of the term ‘fair AI/ML’?</span></p>
</div>
<div id="A1.I7.i2.p2" class="ltx_para">
<ol id="A1.I7.i2.I1" class="ltx_enumerate">
<li id="A1.I7.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i2.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i2.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i2.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">If yes, what would you consider as fair? Can you give an example?</span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="A1.I7.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="A1.I7.i3.p1" class="ltx_para">
<p id="A1.I7.i3.p1.1" class="ltx_p"><span id="A1.I7.i3.p1.1.1" class="ltx_text" style="font-size:70%;">In your view, do you think it is important to create a fair AI/ML?</span></p>
<ol id="A1.I7.i3.I1" class="ltx_enumerate">
<li id="A1.I7.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i3.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i3.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i3.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Why do you think it is important for the AI/ML to be fair?</span></p>
</div>
</li>
<li id="A1.I7.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A1.I7.i3.I1.i2.p1" class="ltx_para">
<p id="A1.I7.i3.I1.i2.p1.1" class="ltx_p"><span id="A1.I7.i3.I1.i2.p1.1.1" class="ltx_text" style="font-size:70%;">If not, why do you think it is not important to create fair a AI/ML?</span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="A1.I7.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="A1.I7.i4.p1" class="ltx_para">
<p id="A1.I7.i4.p1.1" class="ltx_p"><span id="A1.I7.i4.p1.1.1" class="ltx_text" style="font-size:70%;">How do you know the AI/ML that you developed is fair? Do you use any strategies to ensure its fairness?</span></p>
<ol id="A1.I7.i4.I1" class="ltx_enumerate">
<li id="A1.I7.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i4.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i4.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i4.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">If yes, what strategies/ tools/ techniques do you use?</span></p>
</div>
</li>
<li id="A1.I7.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A1.I7.i4.I1.i2.p1" class="ltx_para">
<p id="A1.I7.i4.I1.i2.p1.1" class="ltx_p"><span id="A1.I7.i4.I1.i2.p1.1.1" class="ltx_text" style="font-size:70%;">If not, is it not mandatory to use tools/strategies/techniques?</span></p>
</div>
</li>
<li id="A1.I7.i4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="A1.I7.i4.I1.i3.p1" class="ltx_para">
<p id="A1.I7.i4.I1.i3.p1.1" class="ltx_p"><span id="A1.I7.i4.I1.i3.p1.1.1" class="ltx_text" style="font-size:70%;">Why is it not mandatory?</span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="A1.I7.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="A1.I7.i5.p1" class="ltx_para">
<p id="A1.I7.i5.p1.1" class="ltx_p"><span id="A1.I7.i5.p1.1.1" class="ltx_text" style="font-size:70%;">Do you face any challenges in developing a fair AI/ML?</span></p>
<ol id="A1.I7.i5.I1" class="ltx_enumerate">
<li id="A1.I7.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i5.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i5.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i5.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">If yes, what challenges do you face?</span></p>
</div>
</li>
<li id="A1.I7.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A1.I7.i5.I1.i2.p1" class="ltx_para">
<p id="A1.I7.i5.I1.i2.p1.1" class="ltx_p"><span id="A1.I7.i5.I1.i2.p1.1.1" class="ltx_text" style="font-size:70%;">What do you think are the factors leading to those challenges?</span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="A1.I7.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="A1.I7.i6.p1" class="ltx_para">
<p id="A1.I7.i6.p1.1" class="ltx_p"><span id="A1.I7.i6.p1.1.1" class="ltx_text" style="font-size:70%;">In your view, what does it take to create AI/ML systems that are fair?</span></p>
</div>
<div id="A1.I7.i6.p2" class="ltx_para">
<ol id="A1.I7.i6.I1" class="ltx_enumerate">
<li id="A1.I7.i6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i6.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i6.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i6.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">Why do you think so?</span></p>
</div>
</li>
</ol>
</div>
</li>
<li id="A1.I7.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(7)</span> 
<div id="A1.I7.i7.p1" class="ltx_para">
<p id="A1.I7.i7.p1.1" class="ltx_p"><span id="A1.I7.i7.p1.1.1" class="ltx_text" style="font-size:70%;">If an AI/ML is unfair, who does it impact, according to you?</span></p>
<ol id="A1.I7.i7.I1" class="ltx_enumerate">
<li id="A1.I7.i7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A1.I7.i7.I1.i1.p1" class="ltx_para">
<p id="A1.I7.i7.I1.i1.p1.1" class="ltx_p"><span id="A1.I7.i7.I1.i1.p1.1.1" class="ltx_text" style="font-size:70%;">If yes, how does it impact them? Can you give an example?</span></p>
</div>
</li>
<li id="A1.I7.i7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A1.I7.i7.I1.i2.p1" class="ltx_para">
<p id="A1.I7.i7.I1.i2.p1.1" class="ltx_p"><span id="A1.I7.i7.I1.i2.p1.1.1" class="ltx_text" style="font-size:70%;">If not, why not? Can you give an example?</span></p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.15480" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.15481" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.15481">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.15481" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.15482" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:04:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
