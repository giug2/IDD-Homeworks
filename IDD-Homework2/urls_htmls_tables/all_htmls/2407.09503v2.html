<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos</title>
<!--Generated on Thu Jul 25 13:30:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.09503v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S1" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S2" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Human Computer Interaction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Machine Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The PARSE-Ego4D Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS1" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>The Ego4D dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS2" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Available actions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS3" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic LLM annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS4" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Human annotation study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS5" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Subjective user study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS6" title="In 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Participants</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S4" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>The PARSE-Ego4D Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S4.SS1" title="In 4 The PARSE-Ego4D Benchmark ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task 1: Explicit Query-to-Action</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S4.SS2" title="In 4 The PARSE-Ego4D Benchmark ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Task 2: Implicit Query-to-Action</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5.SS0.SSS0.Px1" title="In 5 Discussion and Limitations ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Context only as textual narrations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5.SS0.SSS0.Px2" title="In 5 Discussion and Limitations ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Efficient ML systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5.SS0.SSS0.Px3" title="In 5 Discussion and Limitations ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Moving beyond human annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5.SS0.SSS0.Px4" title="In 5 Discussion and Limitations ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Multi-turn suggestions and bespoke UI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S5.SS0.SSS0.Px5" title="In 5 Discussion and Limitations ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title">Advanced LLM reasoning techniques.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S6" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Broader Impacts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1" title="In PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.SS1" title="In Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Dataset Availability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.SS2" title="In Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Human Annotation Demographics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.SS3" title="In Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Annotation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.SS4" title="In Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Annotation Interface Screenshots</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Abreu<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">1,2</span></sup>  Tiffany D. Do<sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">1,3</span></sup>  Karan Ahuja<sup class="ltx_sup" id="id13.13.id3">1</sup>  Eric J. Gonzalez<sup class="ltx_sup" id="id14.14.id4">1</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id7.7.3">Lee Payne<sup class="ltx_sup" id="id7.7.3.1"><span class="ltx_text ltx_font_medium" id="id7.7.3.1.1">1</span></sup>  Daniel McDuff <sup class="ltx_sup" id="id7.7.3.2"><span class="ltx_text ltx_font_medium" id="id7.7.3.2.1">1</span></sup>  Mar Gonzalez-Franco <sup class="ltx_sup" id="id7.7.3.3"><span class="ltx_text ltx_font_medium" id="id7.7.3.3.1">1</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id5">1</sup>Google  <sup class="ltx_sup" id="id16.16.id6">2</sup>University of Groningen <sup class="ltx_sup" id="id17.17.id7">3</sup>University of Central Florida
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id18.18.id8">s.abreu@rug.nl, tiffany.do@ucf.edu</span>, 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id19.19.id9">{karanahuja,ejgonz,leepayne,dmcduff,margon}@google.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">Intelligent assistance involves not only understanding but also action. Existing ego-centric video datasets contain rich annotations of the videos, but not of actions that an intelligent assistant could perform in the moment. To address this gap, we release <span class="ltx_text ltx_font_bold" id="id20.id1.1">PARSE-Ego4D</span>, a new set of personal action recommendation annotations for the Ego4D dataset. We take a multi-stage approach to generating and evaluating these annotations.
First, we used a prompt-engineered large language model (LLM) to generate context-aware action suggestions and identified over 18,000 action suggestions. While these synthetic action suggestions are valuable, the inherent limitations of LLMs necessitate human evaluation. To ensure high-quality and user-centered recommendations, we conducted a large-scale human annotation study that provides grounding in human preferences for all of PARSE-Ego4D.
We analyze the inter-rater agreement and evaluate subjective preferences of participants.
Based on our synthetic dataset and complete human annotations, we propose several new tasks for action suggestions based on ego-centric videos.
We encourage novel solutions that improve latency and energy requirements.
The annotations in PARSE-Ego4D will support researchers and developers who are working on building action recommendation systems for augmented and virtual reality systems.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="441" id="S0.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Examples of action suggestions for different videos in the PARSE-Ego4D dataset.
</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Egocentric perception, the ability to capture and understanding of the world from a first-person perspective is gaining significant traction with the adoption of Augmented Reality (AR) and Head-Mounted Displays. Recent advancements in egocentric video understanding have opened new opportunities for research and application, including activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib30" title="">30</a>]</cite>, object interaction analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib52" title="">52</a>]</cite>, and social interaction modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib20" title="">20</a>]</cite>. However, a fundamental limitation of most existing systems is their reactive nature, driven by explicit user queries. We argue that the ability to take bespoke, proactive actions that anticipate a user’s needs is a core component of intelligent behavior without which these systems will be limited in their practical applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Public datasets have been highly consequential in the advancement of machine learning and artificial intelligence. However, older datasets, particularly in the field of computer vision, often included static, context agnostic, unimodal repositories of labeled data, <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">e.g.</span>, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib28" title="">28</a>]</cite> or Imagenet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib41" title="">41</a>]</cite>. As ambitions in AI have become more complex and situated in the context of specific human-computer interaction scenarios, there has been a movement toward datasets that contain temporal, ecologically valid and multimodal data. This paradigm shift is exemplified in new datasets such as Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite> or Ego-Exo4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib15" title="">15</a>]</cite> which include thousands of hours of egocentric video streams.
Several existing egocentric vision datasets provide rich annotations for tasks like activity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib10" title="">10</a>]</cite>, object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib47" title="">47</a>]</cite>, and for the analysis of interactions with other humans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib40" title="">40</a>]</cite> and with the environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib37" title="">37</a>]</cite>. These datasets play a crucial role in advancing research on egocentric perception.
However, previous work focucses primarily on understanding and classifying video content. While valuable, such annotations don’t address how an intelligent system could suggest and take actions in the real or virtual world to assist the user.
This ability to take appropriate action is a core component of intelligent behavior. Without this capability, systems can simply observe the world but have limited practical application as they rely on explicit user queries, as in existing work in visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib12" title="">12</a>]</cite> and visual query localization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib26" title="">26</a>]</cite>. The ability to generate bespoke or proactive actions, which could further our exploration of the environment, is currently missing.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address this limitation and empower the development of proactive AI assistants, we release <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">PARSE-Ego4D</span>, a novel dataset designed to provide personal action recommendation annotations for egocentric videos. Herein, we consider personal suggestions that are context-dependent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib13" title="">13</a>]</cite>.
Our dataset is built upon the extensive Ego4D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite>, which contains 3,670 hours of first-person video recordings of a wide range of everyday activities.
We leverage a two-stage annotation process, combining automated suggestions generated by a state-of-the-art large language model (Gemini Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib48" title="">48</a>]</cite>) with meticulous human evaluation, to ensure the quality, relevance, and usefulness of the action recommendations. These annotations identify moments in the Ego4D video sequence when an assistant may be able to suggest a useful action (see more details in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3" title="3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3</span></a>), creating a total of 18,360 possible action recommendations, which we call the <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">synthetic</em> dataset for it was created by an LLM and not yet grounded in human preferences. While the AI-assisted nature of these annotations allowed us to generate them at scale, the quality can be called into question. Consequently, we performed a large-scale human validation study that provides the necessary grounding in human preferences.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using a 5-point Likert scale for human ratings, we found that 65% of all synthetically generated action suggestions were annotated with average scores above 3, and 42% were annotated with average scores above 4. Considering that our dataset aims at providing a footing to fine tune existing agents so they can provide better actions and personalized queries on-the-fly using real-time multi-modal data, the relatively high scoring validates our automatic captioning and annotation approach.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our first study took 20 samples from our newly generated PARSE-Ego4D dataset and requested 20 human participants to evaluate our AI-generated queries and action suggestions with respect to five axes: (1) whether the query was <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.1">sensible</span> at all (to filter out hallucinations and mistakes from the LLM), (2) whether the suggestion would be helpful as an <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.2">implicit</span> suggestion if it was presented unsolicited to the user, (3) whether the action suggestion was <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.3">valuable</span> to the user (<span class="ltx_text ltx_font_italic" id="S1.p5.1.4">e.g.</span>, by saving them time), (4) whether the suggested action was the <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.5">correct</span> action to take in response to the query, and (5) if the participant would personally be <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.6">likely</span> to take the presented action on their AR glasses (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F4" title="Figure 4 ‣ 3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">4</span></a>).
In the large-scale annotation study, we requested 20% of the PARSE-Ego4D dataset to be annotated by 5 human raters, and the remaining 80% of the PARSE-Ego4D dataset to be annotated by 1 human rater. For the annotation study, we only evaluated the (1) sensibleness, (2) the helpfulness as an implicit action suggestion, and (3) the correctness of the action.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The current <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">PARSE-Ego4D</span> dataset aims at providing a basis for fine-tuning existing agents so they can provide better actions and queries on the fly using real-time multimodal data. Annotation, code and model responses can be found at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://parse-ego4d.github.io" title="">https://parse-ego4d.github.io</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Human Computer Interaction</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Within the realm of Human-Computer Interaction (HCI), research on action recommendations has primarily focused on enhancing user experience and task efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib1" title="">1</a>]</cite>. Prior work has identified several key motivations for providing action suggestions in user interfaces (UIs): saving time by streamlining interactions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib52" title="">52</a>]</cite>, improving discoverability of features and functionalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib20" title="">20</a>]</cite>, and enabling discrete interactions without explicit user input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib42" title="">42</a>]</cite> – an aspect that is particularly relevant for AR glasses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">Research on spatial UI transitions in AR has explored the balance between automation and user control in placing and manipulating UI elements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib32" title="">32</a>]</cite>, emphasizing the importance of user agency and control for a positive user experience. This underscores the need for easy error recovery mechanisms to mitigate the negative impact of incorrect predictions or actions.
Explainability has emerged as a crucial aspect of action recommendations, particularly in the context of augmented reality (AR) systems. Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib55" title="">55</a>]</cite> introduced the XAIR framework, emphasizing the importance of providing clear and understandable explanations for AI-generated suggestions in AR environments. Their findings highlight that users prefer personalized explanations and that the timing, content, and modality of explanations should be carefully tailored to the user’s context and goals.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Machine Learning</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">The increasing traction of egocentric devices
through smart glasses, like Snap’s Spectacles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib23" title="">23</a>]</cite> and Meta’s Ray-Ban Stories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib36" title="">36</a>]</cite>, and mixed reality head-mounted displays, like Apple’s Vision Pro <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib22" title="">22</a>]</cite> and Meta’s Quest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib35" title="">35</a>]</cite>,
has spurred significant advancements in egocentric video <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite> and user understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib45" title="">45</a>]</cite>. These devices provide a unique perspective on the user’s environment and activities, making them ideal platforms for personalized and context-aware AI assistants. The recent surge in multi-modal Large Language Models (M-LLMs) such as Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib48" title="">48</a>]</cite> and ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib39" title="">39</a>]</cite> has further propelled research in this area, particularly in the realm of visual perception and question answering. In the realm of egocentric video understanding, works like EgoOnly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib51" title="">51</a>]</cite> have explored action detection without relying on exocentric (third-person) data, demonstrating the potential of understanding actions from a first-person perspective as a prerequisite for generating relevant action suggestions. Additionally, research in intent classification, such as IntentCapsNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib54" title="">54</a>]</cite>, aims to discern user needs and preferences from egocentric videos, which can inform the generation of personalized suggestions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Recent research has also focused on developing agents that can understand and execute instructions in interactive environments. In robotics, Instruct2Act <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib19" title="">19</a>]</cite> leverages LLMs to generate code that controls a robotic arm to manipulate objects based on multi-modal instructions. In UI interaction, approaches like CogAgent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib17" title="">17</a>]</cite> have shown promising results in mapping natural language instructions to sequences of actions on mobile devices. Similarly, a plethora of LLM-based action agents are aiding in tasks such as knowledge discovery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib38" title="">38</a>]</cite>, web navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib29" title="">29</a>]</cite>, and shopping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib56" title="">56</a>]</cite>, among others.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p3.1">Despite these advancements in understanding actions and executing instructions, there remains a gap in the development of proactive AI assistants for egocentric devices. Existing datasets like Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite> and EPIC-Kitchens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib8" title="">8</a>]</cite> provide rich annotations for understanding activities and objects but do not offer a direct mapping to actionable recommendations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p4.1">The form factor and resource limitations of AR/VR devices, impose unique challenges on the machine learning models used in these systems. Energy efficiency, latency, and memory footprint are critical concerns for ensuring a positive user experience in these battery-powered and often mobile environments. Lightweight LLM models like Gemini XXS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib48" title="">48</a>]</cite> are moving towards deployment on resource-constrained devices. Moreover, model compression techniques like quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib21" title="">21</a>]</cite> have been applied to transformer architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib33" title="">33</a>]</cite> as well as pruning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib34" title="">34</a>]</cite>. Furthermore, more efficient architectures are being developed that compete with transformers and offer better scaling with sequence length <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib9" title="">9</a>]</cite>. Model compression techniques and novel architectures for sequence modeling may provide a path towards efficient always-on foundation models running on resource-constrained AR/VR devices.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The PARSE-Ego4D Dataset</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The PARSE-Ego4D dataset builds on top of the Ego4D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite> and provides action suggestions that draw from the specification of available actions given in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS2" title="3.2 Available actions ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.2</span></a>. After generating synthetic action suggestions using an LLM (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS3" title="3.3 Synthetic LLM annotation ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.3</span></a>), all action suggestions are rated through in a human annotation study (Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS4" title="3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The Ego4D dataset</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">The Ego4D dataset is a massive ego-centric video dataset containing 3,670 hours of daily-life activity video from over 900 people across 74 locations and 9 countries. The data is split into <math alttext="\approx" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mo id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><approx id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">≈</annotation></semantics></math>9,600 videos with an average duration of 15-30 minutes and contains video streams from a head-mounted camera, as well as IMU and gaze data.
The Ego4D dataset further contains rich annotations. All videos have dense written narrations in English for intervals of <math alttext="\approx" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mo id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><approx id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">≈</annotation></semantics></math>10 seconds, as well as a summary for the whole video clip. Additionally, transcriptions, speech segmentation, user attention, speech target classification, speaker labeling, and episodic memory annotations are also provided for parts, or all, of the Ego4D dataset. We make use of the egocentric videos as well as the complete textual narrations from the Ego4D dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Adding additional annotations and expanding the utility of such a dataset that already been collected is better than collecting a new dataset for two reasons.
<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">(1)</span> It enables us to focus on the action suggestions without having to dedicate additional compute to labeling the narrations and captioning and labeling a whole new dataset.
<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.2">(2)</span> Given the substantial investment made into this dataset, we can build on top of other projects that also have augmented the existing Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib47" title="">47</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Available actions</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To create a dataset with action suggestions, we first identify a set of possible actions that can be invoked from the AR/VR device, considering applications that future AR/VR devices are expected to support, such as:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Search</span>: an application that can take in the current camera input and a query (written or spoken) to run a multimodal search, and provide a written and/or spoken response.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Assistant search</span>: the AI assistant for the device, with access to system apps like “notes”, “timer”, “stopwatch”, “alarm”, “email”, “music”, “phone”, “contacts”, “messages”, “settings”, “calculator” and potentially more such as smart home access, notification access, and more.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Assistant local</span>: an application that can explicitly store memories and retrieve them later. Memories may be enrolled manually and explicitly, but they may also be enrolled passively and automatically as in the episodic memory tasks from the Ego4D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib14" title="">14</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Language</span>: an application that can either transcribe what the user is hearing right now, translate what the user is reading or hearing, or determine what language is spoken.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Directions</span>: find relevant places nearby, plan routes, estimate distances and navigate to places.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">Assistant guide</span>: an application that can give detailed and step-by-step instructions to the user.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i7.p1">
<p class="ltx_p" id="S3.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i7.p1.1.1">Others</span>: For open-ended exploration, we also define the option to suggest actions that do not belong to the categories mentioned above. This may allow the LLM to come up with novel, creative use cases for AR glasses that are not covered by the available applications listed above. Actions that fall into this category are not included in the human annotation study.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.2.1">PARSE-Ego4D</span> - We curated, annotated and open-source over 11,000 action suggestions for the Ego4D dataset. These annotations support researchers and developers who are working on building personalized action recommendation systems for augmented and virtual reality systems.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic LLM annotation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In order to generate samples for action suggestions we used a prompt-engineered LLM, the Gemini Pro model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib48" title="">48</a>]</cite>. We use prompt engineering for the LLM to use in-context learning to learn the annotation task. We pass textual narration sentences from the Ego4D annotations as input to the LLM, and request a JSON-formatted output in response. The process is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F2" title="Figure 2 ‣ 3.2 Available actions ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">2</span></a>.
The system prompt to the LLM contains:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Task explanation</span>: the LLM is prompted to behave as a user experience researcher, helping to collect a dataset for useful interactions with AR glasses.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Input format</span>: the input format of the narrations is explained and an example is presented.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Available actions</span>: the set of available actions described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS2" title="3.2 Available actions ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.2</span></a> is listed with example queries and the expected API format (this API format is not used for the annotation study).</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Output format</span>: the expected JSON output format is described. The LLM is expected to return its <span class="ltx_text ltx_font_typewriter" id="S3.I2.i4.p1.1.2">thoughts</span> to assess the situation and develop a rationale for the suggestion that it will return, the <span class="ltx_text ltx_font_typewriter" id="S3.I2.i4.p1.1.3">query</span> that the user would ask along with the timestamp when this would be asked, and the corresponding <span class="ltx_text ltx_font_typewriter" id="S3.I2.i4.p1.1.4">action</span> that the system should take in response to the query.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS3.p1.2">For every video clip in the Ego4D dataset, we split the entire video into batches of 200 narration sentences and pass these batches into the LLM. We drop 1897 short videos that have fewer than 50 sentences of narrations and do not generate any action suggestions for these.
If the response of the LLM is not in valid JSON format, we ask the LLM to re-generate it to be valid. Once the LLM has generated a valid suggestion, we ask it to generate one more suggestion for the same input data. The complete system prompt is given in the Supplementary Materials.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The resulting dataset of synthetically generated action suggestions contains 32,155 action suggestions. After removing 7,491 duplicates (where the same batch of narrations gives the same query and action), we also remove 2,575 approximate duplicates. We classify a suggestion to be an approximate duplicate if it has an embedding distance <math alttext="f(x_{1},x_{2})&gt;0.9" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml"><mrow id="S3.SS3.p2.1.m1.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.4.cmml">f</mi><mo id="S3.SS3.p2.1.m1.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.p2.1.m1.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.3.cmml"><mo id="S3.SS3.p2.1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS3.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p2.1.m1.2.2.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.1.m1.2.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.2.cmml">x</mi><mn id="S3.SS3.p2.1.m1.2.2.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p2.1.m1.2.2.3" xref="S3.SS3.p2.1.m1.2.2.3.cmml">&gt;</mo><mn id="S3.SS3.p2.1.m1.2.2.4" xref="S3.SS3.p2.1.m1.2.2.4.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2"><gt id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.3"></gt><apply id="S3.SS3.p2.1.m1.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2"><times id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.3"></times><ci id="S3.SS3.p2.1.m1.2.2.2.4.cmml" xref="S3.SS3.p2.1.m1.2.2.2.4">𝑓</ci><interval closure="open" id="S3.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.2">𝑥</ci><cn id="S3.SS3.p2.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.2">𝑥</ci><cn id="S3.SS3.p2.1.m1.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.3">2</cn></apply></interval></apply><cn id="S3.SS3.p2.1.m1.2.2.4.cmml" type="float" xref="S3.SS3.p2.1.m1.2.2.4">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">f(x_{1},x_{2})&gt;0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">italic_f ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) &gt; 0.9</annotation></semantics></math> using the normalized Gemini text embeddings from the Gemini API<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding" title="">ai.google.dev/gemini-api/docs/models/gemini#text-embedding</a></span></span></span>. This leaves 19,255 suggestions in our synthetic dataset, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F3" title="Figure 3 ‣ 3.3 Synthetic LLM annotation ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3</span></a> (left).</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.3.1">Left</span>: Suggested actions by type. <span class="ltx_text ltx_font_bold" id="S3.F3.4.2">Right</span>: Score distribution for different questions in the human annotation study, showing that there are more valid explicit suggestions than implicit suggestions.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Every sample in the PARSE-Ego4D dataset contains
a reference to the Ego4D video,
a time range that corresponds to the narration sentence during which the action suggestion is invoked,
the suggestion in the form of a (query, action) tuple,
the name of the LLM that was used to generate the suggesion.
Additionally, each sample also contains a parameter JSON that provides structured information that the suggested application may use.
Furthermore, the dataset contains a rationale for each sample that was generated by the LLM as a form of chain-of-thought reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib53" title="">53</a>]</cite>.
We do not include the action parameters or rationale in the human annotation study, but still provide them as part of the PARSE-Ego4D dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Human annotation study</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Sketch of the survey that participants filled out in the human annotation study in order to verify the synthetically generated action suggestions in PARSE-Ego4D.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We annotate 20% of the synthetic action suggestion dataset gathered in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS3" title="3.3 Synthetic LLM annotation ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.3</span></a> with 5 human raters which will be used as the test split. We annotate the remaining 80% of the dataset with 1 human rater each–of which 75% will be used as the train set and the other 5% as the validation set. In total, we received 36,171 annotations for 18,360 suggestions.
The originally published benchmarks for the Ego4D dataset come with several different train/test/validation splits. However, these data splits are either based on subsets of the entire dataset, or based on specific scenarios, <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">e.g.</span>, hand-object interactions. As we are using the entirety of the Ego4D dataset, we chose a new random train/test/validation split.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">The survey for participants of the annotation study is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F4" title="Figure 4 ‣ 3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">4</span></a>. In the large-scale annotation study, each sample is evaluated with three separate questions that each verify one dimension of the PARSE-Ego4D dataset. First, the sample is evaluated on being <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.1">sensible</span> to verify that the query makes sense in the given context. Second, query is being evaluated on being helpful as an <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.2">implicit</span> (or proactive) action suggestion. We expect that not all samples that score high on the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.3">sensible</span> rating will also score highly on the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.4">implicit</span> rating because we would expect users to have higher standards for implicit, proactive suggestions where false positives are disturbing or even annoying. Indeed, results from our annotation study confirm this, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F3" title="Figure 3 ‣ 3.3 Synthetic LLM annotation ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3</span></a>. Third, the action is evaluated for being <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.5">correct</span> given the query and context.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.2">The release the PARSE-Ego4D dataset with all suggestions and their corresponding ratings from human annotators. For all downstream experiments, we filter the dataset to keep only suggestions that have (mean) ratings <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.2.1">sensible &gt;= 4</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.2.2">correct &gt;= 4</span> to use only verified, high-quality suggestions. If only the queries are used and actions are discarded, we suggest filtering for <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.2.3">sensible &gt;= 4</span>. For implicit, proactive suggestions we additionally filter for <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.2.4">implicit &gt;= 4</span>. Optionally, the cutoff for mean ratings can also be set at <math alttext="\mu=3" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">μ</mi><mo id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><eq id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></eq><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝜇</ci><cn id="S3.SS4.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mu=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_μ = 3</annotation></semantics></math> instead of <math alttext="\mu=4" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mrow id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">μ</mi><mo id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><eq id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1"></eq><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝜇</ci><cn id="S3.SS4.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.SS4.p3.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\mu=4</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_μ = 4</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Subjective user study</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.4">In addition to providing annotations to verify and ground our synthetic action suggestions in human preferences, we ran two extended surveys for participants to assess their subjective preferences for different action suggestions. We ran one study with <math alttext="N=10" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mrow id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">N</mi><mo id="S3.SS5.p1.1.m1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><eq id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1"></eq><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝑁</ci><cn id="S3.SS5.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS5.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">N=10</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_N = 10</annotation></semantics></math> participants and <math alttext="M=10" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">M</mi><mo id="S3.SS5.p1.2.m2.1.1.1" xref="S3.SS5.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><eq id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></eq><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑀</ci><cn id="S3.SS5.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS5.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">M=10</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_M = 10</annotation></semantics></math> samples, and one study with <math alttext="N=20" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><mrow id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS5.p1.3.m3.1.1.1" xref="S3.SS5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><eq id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1.1"></eq><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝑁</ci><cn id="S3.SS5.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS5.p1.3.m3.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">N=20</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_N = 20</annotation></semantics></math> participants and <math alttext="M=20" class="ltx_Math" display="inline" id="S3.SS5.p1.4.m4.1"><semantics id="S3.SS5.p1.4.m4.1a"><mrow id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml"><mi id="S3.SS5.p1.4.m4.1.1.2" xref="S3.SS5.p1.4.m4.1.1.2.cmml">M</mi><mo id="S3.SS5.p1.4.m4.1.1.1" xref="S3.SS5.p1.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.4.m4.1.1.3" xref="S3.SS5.p1.4.m4.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><apply id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1"><eq id="S3.SS5.p1.4.m4.1.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1.1"></eq><ci id="S3.SS5.p1.4.m4.1.1.2.cmml" xref="S3.SS5.p1.4.m4.1.1.2">𝑀</ci><cn id="S3.SS5.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.SS5.p1.4.m4.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">M=20</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.4.m4.1d">italic_M = 20</annotation></semantics></math> samples per participant. In these smaller subjective user studies, each participant is requested to answer all questions from the annotation survey shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F4" title="Figure 4 ‣ 3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.
In addition to the questions outlined in the previous section, participants of the subjective user study were also asked to evaluate how <span class="ltx_text ltx_font_typewriter" id="S3.SS5.p1.4.1">likely</span> they would personally be to ask the given query to their AR glasses, and how much <span class="ltx_text ltx_font_typewriter" id="S3.SS5.p1.4.2">value</span> they think an AI assistant would add in the given scenario.</p>
</div>
<figure class="ltx_table ltx_align_floatleft" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Intraclass Correlation Coefficients (ICC) for the Annotation Questions.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Rating</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">ICC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.1.1">Sensible</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.2">0.87</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T1.1.3.2.1">Helpful</th>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.2">0.73</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T1.1.4.3.1">Value</th>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.2">0.88</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T1.1.5.4.1">Likely</th>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.2">0.90</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.6.5.1">Correct</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.6.5.2">0.81</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">With these questions, we aim to better understand what kind of interactions different users value and to assess if there is a need for personalization in action recommendation systems based on our proposed action specification.
Our results show that intraclass correlation coefficients (ICC) for the five annotation questions were above 0.7 for all questions and above 0.8 for all non-subjective questions from the study, thus showing high inter-rater agreement (see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.T1" title="Table 1 ‣ 3.5 Subjective user study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Although the ICC for the personal <span class="ltx_text ltx_font_typewriter" id="S3.SS5.p3.1.1">helpful</span> question is lower that for other questions, the inter-rater agreement is still considerably high. We thus conclude that personalization may not be very important for building useful and valuable action recommendation systems of the sort that are described in this paper. However, we acknowledge that our user study was small and that the actions used in the annotations studies do not allow for the kind of personal data to be used that would be available to a real-world assistant on augmented and virtual reality systems. We hypothesize that expanding the set of available actions and giving the AI assistant access to personal user data would strengthen the need for personalization in action suggestion systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Participants</h3>
<div class="ltx_para ltx_noindent" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Participants for both the subjective and annotation studies were recruited from Prolific, an online platform for crowdworkers, and were pre-screened for English fluency. For the larger subjective user study, we recruited 20 participants (10 male, 10 female) with an average age of 27.47 (SD=7.80). Participants were geographically diverse, residing in Poland (7), Portugal (6), Hungary (2), South Africa (2), Germany (1), Italy (1), Spain (1), and New Zealand (1).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">The annotation study involved 1496 participants (749 male, 747 female), with an average age of 29.83 (SD=9.15). Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.F5" title="Figure 5 ‣ A.2 Human Annotation Demographics ‣ Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">5</span></a> presents a demographic breakdown of our participants, including gender, race, age, and country of residence. Participants annotated up to 20 samples each and were compensated through Prolific with US$0.13 per annotation for an average hourly wage of US$8.79.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>The PARSE-Ego4D Benchmark</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We propose two tasks for action recommendation based on the PARSE-Ego4D dataset. Each task aims to build action recommendation systems either for (1) explicit user queries or (2) implicit user queries for proactive action suggestions, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S0.F1" title="Figure 1 ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. Both tasks work towards building real-world action recommendation systems for augmented and virtual reality systems.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task 1: Explicit Query-to-Action</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.6">Given the query from the PARSE-Ego4D dataset and the corresponding context from the Ego4D dataset, the task is to predict the action that the system should call on in order handle the user query. The PARSE-Ego4D dataset provides human annotations for six kinds of actions, thus making this a classification task with <math alttext="C=6" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">C</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝐶</ci><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">C=6</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_C = 6</annotation></semantics></math> classes. Formally, the task is to approximate the function <math alttext="f:(c,q)\mapsto a" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.2"><semantics id="S4.SS1.p1.2.m2.2a"><mrow id="S4.SS1.p1.2.m2.2.3" xref="S4.SS1.p1.2.m2.2.3.cmml"><mi id="S4.SS1.p1.2.m2.2.3.2" xref="S4.SS1.p1.2.m2.2.3.2.cmml">f</mi><mo id="S4.SS1.p1.2.m2.2.3.1" lspace="0.278em" rspace="0.278em" xref="S4.SS1.p1.2.m2.2.3.1.cmml">:</mo><mrow id="S4.SS1.p1.2.m2.2.3.3" xref="S4.SS1.p1.2.m2.2.3.3.cmml"><mrow id="S4.SS1.p1.2.m2.2.3.3.2.2" xref="S4.SS1.p1.2.m2.2.3.3.2.1.cmml"><mo id="S4.SS1.p1.2.m2.2.3.3.2.2.1" stretchy="false" xref="S4.SS1.p1.2.m2.2.3.3.2.1.cmml">(</mo><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">c</mi><mo id="S4.SS1.p1.2.m2.2.3.3.2.2.2" xref="S4.SS1.p1.2.m2.2.3.3.2.1.cmml">,</mo><mi id="S4.SS1.p1.2.m2.2.2" xref="S4.SS1.p1.2.m2.2.2.cmml">q</mi><mo id="S4.SS1.p1.2.m2.2.3.3.2.2.3" stretchy="false" xref="S4.SS1.p1.2.m2.2.3.3.2.1.cmml">)</mo></mrow><mo id="S4.SS1.p1.2.m2.2.3.3.1" stretchy="false" xref="S4.SS1.p1.2.m2.2.3.3.1.cmml">↦</mo><mi id="S4.SS1.p1.2.m2.2.3.3.3" xref="S4.SS1.p1.2.m2.2.3.3.3.cmml">a</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.2b"><apply id="S4.SS1.p1.2.m2.2.3.cmml" xref="S4.SS1.p1.2.m2.2.3"><ci id="S4.SS1.p1.2.m2.2.3.1.cmml" xref="S4.SS1.p1.2.m2.2.3.1">:</ci><ci id="S4.SS1.p1.2.m2.2.3.2.cmml" xref="S4.SS1.p1.2.m2.2.3.2">𝑓</ci><apply id="S4.SS1.p1.2.m2.2.3.3.cmml" xref="S4.SS1.p1.2.m2.2.3.3"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.2.3.3.1.cmml" xref="S4.SS1.p1.2.m2.2.3.3.1">maps-to</csymbol><interval closure="open" id="S4.SS1.p1.2.m2.2.3.3.2.1.cmml" xref="S4.SS1.p1.2.m2.2.3.3.2.2"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑐</ci><ci id="S4.SS1.p1.2.m2.2.2.cmml" xref="S4.SS1.p1.2.m2.2.2">𝑞</ci></interval><ci id="S4.SS1.p1.2.m2.2.3.3.3.cmml" xref="S4.SS1.p1.2.m2.2.3.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.2c">f:(c,q)\mapsto a</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.2d">italic_f : ( italic_c , italic_q ) ↦ italic_a</annotation></semantics></math> where <math alttext="a\in\{1,\ldots C\}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.2"><semantics id="S4.SS1.p1.3.m3.2a"><mrow id="S4.SS1.p1.3.m3.2.2" xref="S4.SS1.p1.3.m3.2.2.cmml"><mi id="S4.SS1.p1.3.m3.2.2.3" xref="S4.SS1.p1.3.m3.2.2.3.cmml">a</mi><mo id="S4.SS1.p1.3.m3.2.2.2" xref="S4.SS1.p1.3.m3.2.2.2.cmml">∈</mo><mrow id="S4.SS1.p1.3.m3.2.2.1.1" xref="S4.SS1.p1.3.m3.2.2.1.2.cmml"><mo id="S4.SS1.p1.3.m3.2.2.1.1.2" stretchy="false" xref="S4.SS1.p1.3.m3.2.2.1.2.cmml">{</mo><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">1</mn><mo id="S4.SS1.p1.3.m3.2.2.1.1.3" xref="S4.SS1.p1.3.m3.2.2.1.2.cmml">,</mo><mrow id="S4.SS1.p1.3.m3.2.2.1.1.1" xref="S4.SS1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.3.m3.2.2.1.1.1.2" mathvariant="normal" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml">…</mi><mo id="S4.SS1.p1.3.m3.2.2.1.1.1.1" xref="S4.SS1.p1.3.m3.2.2.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.3.m3.2.2.1.1.1.3" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml">C</mi></mrow><mo id="S4.SS1.p1.3.m3.2.2.1.1.4" stretchy="false" xref="S4.SS1.p1.3.m3.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.2b"><apply id="S4.SS1.p1.3.m3.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2"><in id="S4.SS1.p1.3.m3.2.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2.2"></in><ci id="S4.SS1.p1.3.m3.2.2.3.cmml" xref="S4.SS1.p1.3.m3.2.2.3">𝑎</ci><set id="S4.SS1.p1.3.m3.2.2.1.2.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1"><cn id="S4.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS1.p1.3.m3.1.1">1</cn><apply id="S4.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1"><times id="S4.SS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1.1"></times><ci id="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2">…</ci><ci id="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3">𝐶</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.2c">a\in\{1,\ldots C\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.2d">italic_a ∈ { 1 , … italic_C }</annotation></semantics></math> is the action, <math alttext="c" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_c</annotation></semantics></math> is the context, and <math alttext="q" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_q</annotation></semantics></math> is the text describing the user query. The task can be solved in language-only mode by using the textual narration from the Ego4D dataset, or in multimodal mode by using the raw videos from the Ego4D dataset. Thus, the context <math alttext="c" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">italic_c</annotation></semantics></math> can be either text (as the narrations), or a video stream, or a combination of multiple modalities.
We report the baseline performance of this task with a prompt engineered Gemini Pro model used in zero-shot manner. The system prompt for this task is presented in the Supplementary Material.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task 2: Implicit Query-to-Action</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For increased autonomy of the AI assistant and easier interfacing for users of AR and VR systems, we further propose a new benchmark task to evaluate a system’s capability to make action suggestions without an explicit user query. Instead, the model only receives a signal of intent from the user, for example the press of an action button or the invocation of a hot word without an explicit query that specifies the user’s intent. The present dataset inherently contains such intent signals - which are the timesteps in the Ego4D data for which the PARSE-Ego4D dataset contains verified <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">sensible</span> suggestions.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Baseline results for the PARSE-Ego4D benchmark tasks. All models use language-based input through the narrations provided by the Ego4D dataset annotations. The Gemini model is used in zero-shot manner.
<span class="ltx_text ltx_font_bold" id="S4.T2.3.1">Left</span>: shows the Explicit-Query-to-Action task where classification accuracy is reported. The constant model always predicts the most frequent action in the training dataset.
<span class="ltx_text ltx_font_bold" id="S4.T2.4.2">Right</span>: shows the Implicit Query-to-Action task where negative log likelihood is reported, see main text for explanation. The first random model randomly predicts one of the top-500 (query, action) pairs, the second random model chooses from all (query, action) pairs in the training dataset.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T2.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.5.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.5.1.1.2">Train</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.5.1.1.3">Val</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.5.1.1.4">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.5.2.1.1">Gemini Pro</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.1.2">55.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.1.3">54.43%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.2.1.4">63.57%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.5.3.2.1">Constant</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.3.2.2">42.75%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.3.2.3">42.75%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.3.2.4">42.75%</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S4.T2.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.6.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.1.1.2">Train</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.1.1.3">Val</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.6.1.1.4">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.6.2.1.1">Gemini Pro</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.2">-43.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.3">-43.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.2.1.4">-42.50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.6.3.2.1">Random (top)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.2">-44.77</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.3">-45.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.3.2.4">-44.80</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.6.4.3.1">Random (all)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.4.3.2">-53.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.4.3.3">-53.97</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.4.3.4">-53.39</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.3">The input is the context at a given point in time from the Ego4D dataset where the time is taken from the PARSE-Ego4D dataset, filtered as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.SS4" title="3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3.4</span></a>. As with the previous task, the context can be ingested either in language form, as narrations from the Ego4D dataset annotations, or in raw video form. We present baseline results for the language-based narrations as input.
The output for this task can be an action suggestion, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S0.F1" title="Figure 1 ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. However, it is evident that all necessary information about such an action suggestion is also contained in the (query, action) pair that is provided in the PARSE-Ego4D dataset. As such, we propose to solve this task by learning the function <math alttext="f:c\mapsto(q,a)" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.2"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.3" xref="S4.SS2.p2.1.m1.2.3.cmml"><mi id="S4.SS2.p2.1.m1.2.3.2" xref="S4.SS2.p2.1.m1.2.3.2.cmml">f</mi><mo id="S4.SS2.p2.1.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S4.SS2.p2.1.m1.2.3.1.cmml">:</mo><mrow id="S4.SS2.p2.1.m1.2.3.3" xref="S4.SS2.p2.1.m1.2.3.3.cmml"><mi id="S4.SS2.p2.1.m1.2.3.3.2" xref="S4.SS2.p2.1.m1.2.3.3.2.cmml">c</mi><mo id="S4.SS2.p2.1.m1.2.3.3.1" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.1.cmml">↦</mo><mrow id="S4.SS2.p2.1.m1.2.3.3.3.2" xref="S4.SS2.p2.1.m1.2.3.3.3.1.cmml"><mo id="S4.SS2.p2.1.m1.2.3.3.3.2.1" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.3.1.cmml">(</mo><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">q</mi><mo id="S4.SS2.p2.1.m1.2.3.3.3.2.2" xref="S4.SS2.p2.1.m1.2.3.3.3.1.cmml">,</mo><mi id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S4.SS2.p2.1.m1.2.3.3.3.2.3" stretchy="false" xref="S4.SS2.p2.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><apply id="S4.SS2.p2.1.m1.2.3.cmml" xref="S4.SS2.p2.1.m1.2.3"><ci id="S4.SS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.1">:</ci><ci id="S4.SS2.p2.1.m1.2.3.2.cmml" xref="S4.SS2.p2.1.m1.2.3.2">𝑓</ci><apply id="S4.SS2.p2.1.m1.2.3.3.cmml" xref="S4.SS2.p2.1.m1.2.3.3"><csymbol cd="latexml" id="S4.SS2.p2.1.m1.2.3.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.3.1">maps-to</csymbol><ci id="S4.SS2.p2.1.m1.2.3.3.2.cmml" xref="S4.SS2.p2.1.m1.2.3.3.2">𝑐</ci><interval closure="open" id="S4.SS2.p2.1.m1.2.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.3.3.2"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑞</ci><ci id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">𝑎</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">f:c\mapsto(q,a)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.2d">italic_f : italic_c ↦ ( italic_q , italic_a )</annotation></semantics></math> where <math alttext="c" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_c</annotation></semantics></math> is the context from the Ego4D dataset, and <math alttext="(q,a)" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.2"><semantics id="S4.SS2.p2.3.m3.2a"><mrow id="S4.SS2.p2.3.m3.2.3.2" xref="S4.SS2.p2.3.m3.2.3.1.cmml"><mo id="S4.SS2.p2.3.m3.2.3.2.1" stretchy="false" xref="S4.SS2.p2.3.m3.2.3.1.cmml">(</mo><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">q</mi><mo id="S4.SS2.p2.3.m3.2.3.2.2" xref="S4.SS2.p2.3.m3.2.3.1.cmml">,</mo><mi id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">a</mi><mo id="S4.SS2.p2.3.m3.2.3.2.3" stretchy="false" xref="S4.SS2.p2.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.2b"><interval closure="open" id="S4.SS2.p2.3.m3.2.3.1.cmml" xref="S4.SS2.p2.3.m3.2.3.2"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝑞</ci><ci id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">𝑎</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.2c">(q,a)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.2d">( italic_q , italic_a )</annotation></semantics></math> is the (query, action) tuple.
As this is an open-ended task with the final output being in natural language, we propose the use of the negative log-likelihood of the language model’s output on the (query, action) pair from the PARSE-Ego4D dataset, given the Ego4D context as input. We report the performance of a baseline LLM model on text-based narrations as context input, and provide two naive baseline methods for comparison, see Table <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S4.T2" title="Table 2 ‣ 4.2 Task 2: Implicit Query-to-Action ‣ 4 The PARSE-Ego4D Benchmark ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">2</span></a>. The system prompt for the LLM on this task is presented in the Supplementary Material.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Limitations</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Context only as textual narrations</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">We generated the presented dataset based only on textual narrations from the Ego4D dataset that were provided by human annotators. Using a the few-shot learning ability of foundation models would, at the present time, be too computationally expensive on video data directly. However, it is conceivable to pass one, or a few, images from the video stream into the model, along with the textual narrations. It may also be advantageous to train a video-to-text model directly or fine-tune an existing model using our proposed dataset. Experiments using multimodal LLMs on our proposed benchmark tasks remain to be explored.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Efficient ML systems</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Our proposed experimental baselines use either naive methods or a state-of-the-art LLM that is too large to be deployed on AR/VR devices. We encourage future work to explore the tradeoffs between performance on the proposed tasks and the efficiency of the suggestion model. Novel efficient architectures for sequence modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib16" title="">16</a>]</cite> may provide a path towards efficient AI assistants running on-device in resource-constrained environments such as those faced by AR/VR systems.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Moving beyond human annotations</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">Despite in our approach we use LLMs to create the dataset through prompt engineering on the narration of videos, we still require a certain level of human annotation to evaluate the quality of the dataset. This is inline with current recommendations that test the limits of how far can synthetic user experiences go <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib27" title="">27</a>]</cite>. It remains to be explored if new advances in self-training LLMs based on automated scalar feedback <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib43" title="">43</a>]</cite> or self-consistency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib18" title="">18</a>]</cite> can be applied to our dataset to improve the performance of LLMs on our proposed tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Multi-turn suggestions and bespoke UI</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">The development of personalized action recommendation systems in egocentric video presents a unique challenge in the design of user interfaces (UI). Traditional Assistants rely on queries by user, often optimized for general use, may not be suitable for presenting contextually relevant suggestions unless users start doing multi-turn interactions. This necessitates the exploration of shortcuts and bespoke UI designs that can seamlessly integrate with the user’s context. In our research we propose implicit queries that can actually reduce the number of multi-turn queries or UI interactions needed.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Advanced LLM reasoning techniques.</h4>
<div class="ltx_para ltx_noindent" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">The creation of our PARSE-Ego4D dataset aligns with and could benefit from advancements in Large Language Model (LLM) reasoning techniques, specifically Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib53" title="">53</a>]</cite>, Tree-of-Thought (ToT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib57" title="">57</a>]</cite>, and self-reflection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#bib.bib24" title="">24</a>]</cite>. These techniques hold the potential to enhance both the generation and evaluation of action suggestions, moving us closer to truly personalized AI assistants.
CoT prompting encourages LLMs to generate intermediate reasoning steps before reaching a conclusion. This approach can be applied to action suggestions by prompting the LLM to explicitly consider the user’s context, goals, and preferences before recommending an action. For example, instead of directly suggesting “Turn on the lights”, the LLM might first reason about the time of day, the user’s location, and their recent activities. This could lead to more nuanced suggestions like, “It’s getting dark in the kitchen, would you like me to turn on the lights?”.
ToT extends CoT by allowing the LLM to explore multiple reasoning paths in parallel. This could be beneficial for generating a wider range of action suggestions and evaluating their potential impact on the user. For instance, the LLM could consider different options for completing a task, weigh their pros and cons, and present the most suitable one to the user.
Self-reflection enables LLMs to evaluate their own outputs and identify potential errors or biases. In the context of action suggestions, this could involve the LLM assessing the confidence of its recommendations and providing explanations to the user. This could increase user trust and allow them to understand the reasoning behind the suggestions.
As LLM reasoning techniques advance, they will also open up new research avenues, such as developing LLM-based agents that can learn user preferences and adapt their suggestions over time. CoT and ToT prompting could be used in real-time to refine the LLM-generated action suggestions in PARSE-Ego4D, making them more contextually relevant, time bonded, and personalized.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Broader Impacts</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we have introduced PARSE-Ego4D, a novel dataset that expands upon the existing Ego4D dataset by incorporating context-aware personal action recommendation annotations. By leveraging a two-stage annotation process combining automated suggestions from a large language model (Gemini Pro) and human evaluation, we have ensured the quality, relevance, and usefulness of these recommendations. Our comprehensive human evaluation not only validates the efficacy of the LLM-generated suggestions but also reveals insights into the nuances of user preferences in real-world scenarios, for example proposing a difference between implicit and explicit types of queries. Through this dataset, we aim to empower researchers and developers to build intelligent assistants capable of anticipating user needs and proactively offering personalized action suggestions, ultimately enhancing the user experience in egocentric video applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our dataset also is free of personally identifiable information and given the very tailored prompt engineering eliminates the appearance of offensive content. Both aspects are also further enhanced by the reliance on the original Ego4D dataset. The annotations in PARSE-Ego4D will support future research on a variety of tasks, such as intent to action mapping, personalized suggestion learning, and user modeling. We believe that the release of this dataset will significantly advance the field of proactive AI assistance in egocentric video and contribute to the development of more intelligent and intuitive user experiences.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,
Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al.

</span>
<span class="ltx_bibblock">Guidelines for human-ai interaction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 chi conference on human factors in
computing systems</span>, pages 1–13, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Dan Bohus, Sean Andrist, Ashley Feniello, Nick Saw, Mihai Jalobeanu, Patrick
Sweeney, Anne Loomis Thompson, and Eric Horvitz.

</span>
<span class="ltx_bibblock">Platform for situated intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2103.15975</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Dan Bohus, Sean Andrist, Nick Saw, Ann Paradiso, Ishani Chakraborty, and Mahdi
Rad.

</span>
<span class="ltx_bibblock">Sigma: An open-source interactive system for mixed-reality task
assistance research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2405.13035</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian
Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa,
Robert Dadashi, et al.

</span>
<span class="ltx_bibblock">Recurrentgemma: Moving past transformers for efficient open language
models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2404.07839</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Matthew Chang, Aditya Prakash, and Saurabh Gupta.

</span>
<span class="ltx_bibblock">Look ma, no hands! agent-environment factorization of egocentric
videos.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 21466–21486. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dibyadip Chatterjee, Fadime Sener, Shugao Ma, and Angela Yao.

</span>
<span class="ltx_bibblock">Opening the vocabulary of egocentric actions.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 33174–33187. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jihoon Chung, Yu Wu, and Olga Russakovsky.

</span>
<span class="ltx_bibblock">Enabling detailed action recognition evaluation through video dataset
augmentation.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</span>, volume 35,
pages 39020–39033. Curran Associates, Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino
Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett,
Will Price, et al.

</span>
<span class="ltx_bibblock">The epic-kitchens dataset: Collection, challenges and baselines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
43(11):4125–4141, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tri Dao and Albert Gu.

</span>
<span class="ltx_bibblock">Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2405.21060</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Joseph DelPreto, Chao Liu, Yiyue Luo, Michael Foshey, Yunzhu Li, Antonio
Torralba, Wojciech Matusik, and Daniela Rus.

</span>
<span class="ltx_bibblock">Actionsense: A multimodal dataset and recording framework for human
activities using wearable sensors in a kitchen environment.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</span>, volume 35,
pages 13800–13813. Curran Associates, Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Mustafa Doga Dogan, Eric J Gonzalez, Andrea Colaco, Karan Ahuja, Ruofei Du,
Johnny Lee, Mar Gonzalez-Franco, and David Kim.

</span>
<span class="ltx_bibblock">Augmented object intelligence: Making the analog world interactable
with xr-objects.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2404.13274</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chenyou Fan.

</span>
<span class="ltx_bibblock">Egovqa - an egocentric video question answering benchmark dataset.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) Workshops</span>, Oct 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Giuseppe Ghiani, Marco Manca, Fabio Paternò, and Carmen Santoro.

</span>
<span class="ltx_bibblock">Personalization of context-dependent applications through
trigger-action rules.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">ACM Trans. Comput.-Hum. Interact.</span>, 24(2), apr 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
et al.

</span>
<span class="ltx_bibblock">Ego4d: Around the world in 3,000 hours of egocentric video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 18995–19012, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra
Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal,
Bikram Boote, et al.

</span>
<span class="ltx_bibblock">Ego-exo4d: Understanding skilled human activity from first-and
third-person perspectives.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2311.18259</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2312.00752</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan
Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al.

</span>
<span class="ltx_bibblock">Cogagent: A visual language model for gui agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2312.08914</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,
and Jiawei Han.

</span>
<span class="ltx_bibblock">Large language models can self-improve, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Instruct2act: Mapping multi-modality instructions to robotic actions
with large language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2305.11176</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie
Zhang, Lu Dong, Yali Wang, Limin Wang, et al.

</span>
<span class="ltx_bibblock">Egoexolearn: A dataset for bridging asynchronous ego-and exo-centric
view of procedural activities in real world.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2403.16182</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio.

</span>
<span class="ltx_bibblock">Quantized neural networks: Training neural networks with low
precision weights and activations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Journal of Machine Learning Research</span>, 18(187):1–30, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Apple Inc.

</span>
<span class="ltx_bibblock">Apple vision pro.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.apple.com/apple-vision-pro/" title="">https://www.apple.com/apple-vision-pro/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Snap Inc.

</span>
<span class="ltx_bibblock">Snap spectacles.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://spectacles.com/" title="">https://spectacles.com/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.

</span>
<span class="ltx_bibblock">Towards mitigating llm hallucination via self reflection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2023</span>, pages 1827–1843, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang.

</span>
<span class="ltx_bibblock">Egotaskqa: Understanding human tasks in egocentric videos.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Advances in Neural Information Processing Systems</span>, volume 35,
pages 3343–3360. Curran Associates, Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hanwen Jiang, Santhosh Kumar Ramakrishnan, and Kristen Grauman.

</span>
<span class="ltx_bibblock">Single-stage visual query localization in egocentric videos.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 24143–24157. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jie Li.

</span>
<span class="ltx_bibblock">How far can we go with synthetic user experience research?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Interactions</span>, 31(3):26–29, may 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span>, pages 740–755.
Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jiarun Liu, Wentao Hu, and Chunhong Zhang.

</span>
<span class="ltx_bibblock">Alltogether: Investigating the efficacy of spliced prompt for web
navigation using large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2310.18331</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
XingyuBruce Liu, JiahaoNick Li, David Kim, Xiang’Anthony’ Chen, and Ruofei Du.

</span>
<span class="ltx_bibblock">Human I/O: Towards a Unified Approach to Detecting Situational
Impairments.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2024 CHI Conference on Human Factors in
Computing Systems</span>, CHI. ACM, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Jieyi Long.

</span>
<span class="ltx_bibblock">Large language model guided tree-of-thought.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2305.08291</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Feiyu Lu and Yan Xu.

</span>
<span class="ltx_bibblock">Exploring spatial ui transition mechanisms with head-worn augmented
reality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2022 CHI Conference on Human Factors in
Computing Systems</span>, pages 1–16, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang,
Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei.

</span>
<span class="ltx_bibblock">The era of 1-bit llms: All large language models are in 1.58 bits,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Xinyin Ma, Gongfan Fang, and Xinchao Wang.

</span>
<span class="ltx_bibblock">Llm-pruner: On the structural pruning of large language models.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 21702–21720. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Inc. Meta Platforms.

</span>
<span class="ltx_bibblock">Meta quest virtual reality headset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.meta.com/quest/" title="">https://www.meta.com/quest/</a>, 2020.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Inc. Meta Platforms and EssilorLuxottica.

</span>
<span class="ltx_bibblock">Ray-ban stories smart glasses.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.meta.com/smart-glasses/" title="">https://www.meta.com/smart-glasses/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Tushar Nagarajan, Santhosh Kumar Ramakrishnan, Ruta Desai, James Hillis, and
Kristen Grauman.

</span>
<span class="ltx_bibblock">Egoenv: Human-centric environment representations from egocentric
video.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 60130–60143. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
et al.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2112.09332</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a>, 2022.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Chirag Raman, Jose Vargas Quiros, Stephanie Tan, Ashraful Islam, Ekin Gedik,
and Hayley Hung.

</span>
<span class="ltx_bibblock">Conflab: A data collection concept, dataset, and benchmark for
machine analysis of free-standing social interactions in the wild.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</span>, volume 35,
pages 23701–23715. Curran Associates, Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">International journal of computer vision</span>, 115:211–252, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Albrecht Schmidt.

</span>
<span class="ltx_bibblock">Implicit human computer interaction through context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Personal technologies</span>, 4:191–199, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil,
Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al.

</span>
<span class="ltx_bibblock">Beyond human data: Scaling self-training for problem-solving with
language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2312.06585</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Yale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang, Miguel Martin, and
Lorenzo Torresani.

</span>
<span class="ltx_bibblock">Ego4d goal-step: Toward hierarchical understanding of procedural
activities.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 38863–38886. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Yale Song, Eugene Byrne, Tushar Nagarajan, Huiyu Wang, Miguel Martin, and
Lorenzo Torresani.

</span>
<span class="ltx_bibblock">Ego4d goal-step: Toward hierarchical understanding of procedural
activities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia.

</span>
<span class="ltx_bibblock">Sensecape: Enabling multilevel exploration and sensemaking with large
language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 36th Annual ACM Symposium on User
Interface Software and Technology</span>, pages 1–18, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Hao Tang, Kevin J Liang, Kristen Grauman, Matt Feiszli, and Weiyao Wang.

</span>
<span class="ltx_bibblock">Egotracks: A long-term egocentric visual object tracking dataset.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Advances in Neural Information Processing Systems</span>,
volume 36, pages 75716–75739. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Daniel Vogel and Ravin Balakrishnan.

</span>
<span class="ltx_bibblock">Interactive public ambient displays: transitioning from implicit to
explicit, public to personal, interaction with multiple users.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 17th annual ACM symposium on User
interface software and technology</span>, pages 137–146, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan
Yang, Ruiping Wang, Yi Wu, and Furu Wei.

</span>
<span class="ltx_bibblock">Bitnet: Scaling 1-bit transformers for large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Huiyu Wang, Mitesh Kumar Singh, and Lorenzo Torresani.

</span>
<span class="ltx_bibblock">Ego-only: Egocentric action detection without exocentric
transferring.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 5250–5261, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist,
Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al.

</span>
<span class="ltx_bibblock">Holoassist: an egocentric human interaction dataset for interactive
ai assistants in the real world.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 20270–20281, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
Ed H. Chi, Quoc V Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock">In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Advances in Neural Information Processing Systems</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Congying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang, and Philip S Yu.

</span>
<span class="ltx_bibblock">Zero-shot user intent detection via capsule neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1809.00385</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Xuhai Xu, Anna Yu, Tanya R Jonker, Kashyap Todi, Feiyu Lu, Xun Qian,
João Marcelo Evangelista Belo, Tianyi Wang, Michelle Li, Aran Mun, et al.

</span>
<span class="ltx_bibblock">Xair: A framework of explainable ai in augmented reality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems</span>, pages 1–30, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded
language agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Advances in Neural Information Processing Systems</span>,
35:20744–20757, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and
Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language
models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Checklist</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<ol class="ltx_enumerate" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">For all authors…</p>
<ol class="ltx_enumerate" id="Sx1.I1.i1.I1">
<li class="ltx_item" id="Sx1.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx1.I1.i1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.I1.i1.p1.1">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope?
<span class="ltx_text" id="Sx1.I1.i1.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx1.I1.i1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i1.I1.i2.p1.1">Did you describe the limitations of your work?
<span class="ltx_text" id="Sx1.I1.i1.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span>Limitations are discussed in Section 5.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx1.I1.i1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i1.I1.i3.p1.1">Did you discuss any potential negative societal impacts of your work?
<span class="ltx_text" id="Sx1.I1.i1.I1.i3.p1.1.1" style="color:#0000FF;">[Yes] </span>The broader impacts of our work are discussed in Section 7.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i1.I1.i4.p1.1">Have you read the ethics review guidelines and ensured that your paper conforms to them?
<span class="ltx_text" id="Sx1.I1.i1.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">If you are including theoretical results…</p>
<ol class="ltx_enumerate" id="Sx1.I1.i2.I1">
<li class="ltx_item" id="Sx1.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx1.I1.i2.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i2.I1.i1.p1.1">Did you state the full set of assumptions of all theoretical results?
<span class="ltx_text" id="Sx1.I1.i2.I1.i1.p1.1.1" style="color:#808080;">[N/A] </span>No theoretical results are included.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i2.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.I1.i2.p1.1">Did you include complete proofs of all theoretical results?
<span class="ltx_text" id="Sx1.I1.i2.I1.i2.p1.1.1" style="color:#808080;">[N/A] </span>No theoretical results are included.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">If you ran experiments (e.g. for benchmarks)…</p>
<ol class="ltx_enumerate" id="Sx1.I1.i3.I1">
<li class="ltx_item" id="Sx1.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx1.I1.i3.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i3.I1.i1.p1.1">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
<span class="ltx_text" id="Sx1.I1.i3.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span>Code, data and instructions for reproducing the experimental results can be found on our project page.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx1.I1.i3.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i3.I1.i2.p1.1">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
<span class="ltx_text" id="Sx1.I1.i3.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx1.I1.i3.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.I1.i3.p1.1">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
<span class="ltx_text" id="Sx1.I1.i3.I1.i3.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i3.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i3.I1.i4.p1.1">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
<span class="ltx_text" id="Sx1.I1.i3.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets…</p>
<ol class="ltx_enumerate" id="Sx1.I1.i4.I1">
<li class="ltx_item" id="Sx1.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx1.I1.i4.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i4.I1.i1.p1.1">If your work uses existing assets, did you cite the creators?
<span class="ltx_text" id="Sx1.I1.i4.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span>Our work builds on and contributes to the Ego4D dataset which we have cited.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx1.I1.i4.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i4.I1.i2.p1.1">Did you mention the license of the assets?
<span class="ltx_text" id="Sx1.I1.i4.I1.i2.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="Sx1.I1.i4.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i4.I1.i3.p1.1">Did you include any new assets either in the supplemental material or as a URL?
<span class="ltx_text" id="Sx1.I1.i4.I1.i3.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span>
<div class="ltx_para" id="Sx1.I1.i4.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.I1.i4.p1.1">Did you discuss whether and how consent was obtained from people whose data you’re using/curating?
<span class="ltx_text" id="Sx1.I1.i4.I1.i4.p1.1.1" style="color:#0000FF;">[Yes] </span>We have provided detailed information about the instructions and information given to annotators recruited for this work.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(e)</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i4.I1.i5.p1">
<p class="ltx_p" id="Sx1.I1.i4.I1.i5.p1.1">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
<span class="ltx_text" id="Sx1.I1.i4.I1.i5.p1.1.1" style="color:#0000FF;">[Yes] </span></p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i5.p1">
<p class="ltx_p" id="Sx1.I1.i5.p1.1">If you used crowdsourcing or conducted research with human subjects…</p>
<ol class="ltx_enumerate" id="Sx1.I1.i5.I1">
<li class="ltx_item" id="Sx1.I1.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="Sx1.I1.i5.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i5.I1.i1.p1.1">Did you include the full text of instructions given to participants and screenshots, if applicable?
<span class="ltx_text" id="Sx1.I1.i5.I1.i1.p1.1.1" style="color:#0000FF;">[Yes] </span>Yes, we have included details of the instructions and screenshots of the annotation interface.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="Sx1.I1.i5.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i5.I1.i2.p1.1">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
<span class="ltx_text" id="Sx1.I1.i5.I1.i2.p1.1.1" style="color:#808080;">[N/A] </span>Risks associated with the annotation task were deemed very minimal.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i5.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i5.I1.i3.p1.1">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
<span class="ltx_text" id="Sx1.I1.i5.I1.i3.p1.1.1" style="color:#0000FF;">[Yes] </span>Details of the tasks and compensation are included in Section 3.6 "Participants".</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset Availability</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The dataset is available on the PARSE-Ego4D GitHub repository on: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/parse-ego4d/parse-ego4d.github.io/tree/main/dataset/" title="">https://github.com/parse-ego4d/parse-ego4d.github.io/tree/main/dataset/</a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Human Annotation Demographics</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">A visualization of the demographics from our human annotation study is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.F5" title="Figure 5 ‣ A.2 Human Annotation Demographics ‣ Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="390" id="A1.F5.g1" src="extracted/5754951/figures/demographics.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>A demographic breakdown of our participants in the annotation study, including ethnicity, gender, and age. Countries with fewer than 15 participants are listed in "Other".</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Annotation Metrics</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">The human annotations are used to filter the suggestions in PARSE-Ego4D so that samples above a certain mean rating for each question are accepted. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.T3" title="Table 3 ‣ A.3 Annotation Metrics ‣ Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">3</span></a> shows an overview of how many samples are accepted at different mean ratings.</p>
</div>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T3.15">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T3.15.16.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T3.15.16.1.1">Filter</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T3.15.16.1.2">Percentage</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T3.15.16.1.3">Number of Suggestions</td>
</tr>
<tr class="ltx_tr" id="A1.T3.15.17.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.15.17.2.1">All samples</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.15.17.2.2">100.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.15.17.2.3">18,360</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.1.1.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.1.1.1.1">sensible</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.1.1.1.m1.1"><semantics id="A1.T3.1.1.1.m1.1a"><mo id="A1.T3.1.1.1.m1.1.1" xref="A1.T3.1.1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.1.1.1.m1.1b"><geq id="A1.T3.1.1.1.m1.1.1.cmml" xref="A1.T3.1.1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.1.1.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.1.1.1.m1.1d">≥</annotation></semantics></math> 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.1.1.2">78.10%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.1.3">14,340</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.2.2.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.2.2.1.1">sensible</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.2.2.1.m1.1"><semantics id="A1.T3.2.2.1.m1.1a"><mo id="A1.T3.2.2.1.m1.1.1" xref="A1.T3.2.2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.2.2.1.m1.1b"><geq id="A1.T3.2.2.1.m1.1.1.cmml" xref="A1.T3.2.2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.2.2.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.2.2.1.m1.1d">≥</annotation></semantics></math> 3.5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.2.2">63.10%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.2.2.3">11,586</td>
</tr>
<tr class="ltx_tr" id="A1.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.3.3.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.3.3.1.1">sensible</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.3.3.1.m1.1"><semantics id="A1.T3.3.3.1.m1.1a"><mo id="A1.T3.3.3.1.m1.1.1" xref="A1.T3.3.3.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.3.3.1.m1.1b"><geq id="A1.T3.3.3.1.m1.1.1.cmml" xref="A1.T3.3.3.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.3.3.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.3.3.1.m1.1d">≥</annotation></semantics></math> 4</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.3.3.2">58.31%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.3.3.3">10,705</td>
</tr>
<tr class="ltx_tr" id="A1.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.4.4.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.4.4.1.1">correct</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.4.4.1.m1.1"><semantics id="A1.T3.4.4.1.m1.1a"><mo id="A1.T3.4.4.1.m1.1.1" xref="A1.T3.4.4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.4.4.1.m1.1b"><geq id="A1.T3.4.4.1.m1.1.1.cmml" xref="A1.T3.4.4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.4.4.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.4.4.1.m1.1d">≥</annotation></semantics></math> 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.4.4.2">74.56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.4.4.3">13,689</td>
</tr>
<tr class="ltx_tr" id="A1.T3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.5.5.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.5.5.1.1">correct</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.5.5.1.m1.1"><semantics id="A1.T3.5.5.1.m1.1a"><mo id="A1.T3.5.5.1.m1.1.1" xref="A1.T3.5.5.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.5.5.1.m1.1b"><geq id="A1.T3.5.5.1.m1.1.1.cmml" xref="A1.T3.5.5.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.5.5.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.5.5.1.m1.1d">≥</annotation></semantics></math> 3.5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.5.5.2">59.54%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.5.5.3">10,932</td>
</tr>
<tr class="ltx_tr" id="A1.T3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.6.6.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.6.6.1.1">correct</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.6.6.1.m1.1"><semantics id="A1.T3.6.6.1.m1.1a"><mo id="A1.T3.6.6.1.m1.1.1" xref="A1.T3.6.6.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.6.6.1.m1.1b"><geq id="A1.T3.6.6.1.m1.1.1.cmml" xref="A1.T3.6.6.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.6.6.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.6.6.1.m1.1d">≥</annotation></semantics></math> 4</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.6.6.2">54.80%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.6.6.3">10,061</td>
</tr>
<tr class="ltx_tr" id="A1.T3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.7.7.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.7.7.1.1">implicit</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.7.7.1.m1.1"><semantics id="A1.T3.7.7.1.m1.1a"><mo id="A1.T3.7.7.1.m1.1.1" xref="A1.T3.7.7.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.7.7.1.m1.1b"><geq id="A1.T3.7.7.1.m1.1.1.cmml" xref="A1.T3.7.7.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.7.7.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.7.7.1.m1.1d">≥</annotation></semantics></math> 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.7.7.2">59.38%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.7.7.3">10,903</td>
</tr>
<tr class="ltx_tr" id="A1.T3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.8.8.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.8.8.1.1">implicit</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.8.8.1.m1.1"><semantics id="A1.T3.8.8.1.m1.1a"><mo id="A1.T3.8.8.1.m1.1.1" xref="A1.T3.8.8.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.8.8.1.m1.1b"><geq id="A1.T3.8.8.1.m1.1.1.cmml" xref="A1.T3.8.8.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.8.8.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.8.8.1.m1.1d">≥</annotation></semantics></math> 3.5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.8.8.2">37.61%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.8.8.3">6,905</td>
</tr>
<tr class="ltx_tr" id="A1.T3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.9.9.1">
<span class="ltx_text ltx_font_typewriter" id="A1.T3.9.9.1.1">implicit</span> <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.9.9.1.m1.1"><semantics id="A1.T3.9.9.1.m1.1a"><mo id="A1.T3.9.9.1.m1.1.1" xref="A1.T3.9.9.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.9.9.1.m1.1b"><geq id="A1.T3.9.9.1.m1.1.1.cmml" xref="A1.T3.9.9.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.9.9.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.9.9.1.m1.1d">≥</annotation></semantics></math> 4</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.9.9.2">33.26%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.9.9.3">6,107</td>
</tr>
<tr class="ltx_tr" id="A1.T3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.10.10.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.10.10.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.10.10.1.2">correct</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.10.10.1.m1.1"><semantics id="A1.T3.10.10.1.m1.1a"><mo id="A1.T3.10.10.1.m1.1.1" xref="A1.T3.10.10.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.10.10.1.m1.1b"><geq id="A1.T3.10.10.1.m1.1.1.cmml" xref="A1.T3.10.10.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.10.10.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.10.10.1.m1.1d">≥</annotation></semantics></math> 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.10.10.2">65.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.10.10.3">11,934</td>
</tr>
<tr class="ltx_tr" id="A1.T3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.11.11.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.11.11.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.11.11.1.2">correct</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.11.11.1.m1.1"><semantics id="A1.T3.11.11.1.m1.1a"><mo id="A1.T3.11.11.1.m1.1.1" xref="A1.T3.11.11.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.11.11.1.m1.1b"><geq id="A1.T3.11.11.1.m1.1.1.cmml" xref="A1.T3.11.11.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.11.11.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.11.11.1.m1.1d">≥</annotation></semantics></math> 3.5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.11.11.2">47.17%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.11.11.3">8,660</td>
</tr>
<tr class="ltx_tr" id="A1.T3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.12.12.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.12.12.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.12.12.1.2">correct</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.12.12.1.m1.1"><semantics id="A1.T3.12.12.1.m1.1a"><mo id="A1.T3.12.12.1.m1.1.1" xref="A1.T3.12.12.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.12.12.1.m1.1b"><geq id="A1.T3.12.12.1.m1.1.1.cmml" xref="A1.T3.12.12.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.12.12.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.12.12.1.m1.1d">≥</annotation></semantics></math> 4</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.12.12.2">42.32%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.12.12.3">7,770</td>
</tr>
<tr class="ltx_tr" id="A1.T3.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T3.13.13.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.13.13.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.13.13.1.2">correct</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.13.13.1.3">implicit</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.13.13.1.m1.1"><semantics id="A1.T3.13.13.1.m1.1a"><mo id="A1.T3.13.13.1.m1.1.1" xref="A1.T3.13.13.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.13.13.1.m1.1b"><geq id="A1.T3.13.13.1.m1.1.1.cmml" xref="A1.T3.13.13.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.13.13.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.13.13.1.m1.1d">≥</annotation></semantics></math> 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.13.13.2">48.22%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.13.13.3">8,854</td>
</tr>
<tr class="ltx_tr" id="A1.T3.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T3.14.14.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.14.14.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.14.14.1.2">correct</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.14.14.1.3">implicit</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.14.14.1.m1.1"><semantics id="A1.T3.14.14.1.m1.1a"><mo id="A1.T3.14.14.1.m1.1.1" xref="A1.T3.14.14.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.14.14.1.m1.1b"><geq id="A1.T3.14.14.1.m1.1.1.cmml" xref="A1.T3.14.14.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.14.14.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.14.14.1.m1.1d">≥</annotation></semantics></math> 3.5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.14.14.2">27.65%</td>
<td class="ltx_td ltx_align_center" id="A1.T3.14.14.3">5,076</td>
</tr>
<tr class="ltx_tr" id="A1.T3.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T3.15.15.1">{<span class="ltx_text ltx_font_typewriter" id="A1.T3.15.15.1.1">sensible</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.15.15.1.2">correct</span>, <span class="ltx_text ltx_font_typewriter" id="A1.T3.15.15.1.3">implicit</span>} <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T3.15.15.1.m1.1"><semantics id="A1.T3.15.15.1.m1.1a"><mo id="A1.T3.15.15.1.m1.1.1" xref="A1.T3.15.15.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.T3.15.15.1.m1.1b"><geq id="A1.T3.15.15.1.m1.1.1.cmml" xref="A1.T3.15.15.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.T3.15.15.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="A1.T3.15.15.1.m1.1d">≥</annotation></semantics></math> 4</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T3.15.15.2">24.02%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T3.15.15.3">4,410</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Number of suggestions in PARSE-Ego4D above a mean rating for different metrics. The filter <span class="ltx_text ltx_font_typewriter" id="A1.T3.18.1">{sensible,correct}</span> is applied for Task 1, whereas the <span class="ltx_text ltx_font_typewriter" id="A1.T3.19.2">{sensible,correct,implicit}</span> filter is applied for Task 2.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Annotation Interface Screenshots</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">The human annotation study was run using Prolific, with participants filling out the survey on Qualtrics. The survey design is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#S3.F4" title="Figure 4 ‣ 3.4 Human annotation study ‣ 3 The PARSE-Ego4D Dataset ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">4</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.09503v2#A1.F6" title="Figure 6 ‣ A.4 Annotation Interface Screenshots ‣ Appendix A Appendix ‣ PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos"><span class="ltx_text ltx_ref_tag">6</span></a> shows screenshots of the survey that human participants filled out.</p>
</div>
<figure class="ltx_figure" id="A1.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="A1.F6.sf1.g1" src="extracted/5754951/figures/Survey_1.png" width="352"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Introductory instructions.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="A1.F6.sf2.g1" src="extracted/5754951/figures/Survey_2.png" width="352"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Example video.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="A1.F6.sf3.g1" src="extracted/5754951/figures/Survey_3.png" width="352"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Task video.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F6.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="A1.F6.sf4.g1" src="extracted/5754951/figures/Survey_4.png" width="352"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Task questions.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Screenshots of the human annotation task.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jul 25 13:30:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
