<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting</title>
<!--Generated on Sun Sep 15 15:22:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09766v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S1" title="In Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2" title="In Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS1" title="In 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data and preprocessing</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS1.SSS1" title="In 2.1 Data and preprocessing ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Datasets for lesion segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS1.SSS2" title="In 2.1 Data and preprocessing ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Datasets for image classification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS2" title="In 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model architecture and training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS2.SSS1" title="In 2.2 Model architecture and training ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>YOLO model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.SS2.SSS2" title="In 2.2 Model architecture and training ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>nnU-Net model</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S3" title="In Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S4" title="In Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\doparttoc</span><span class="ltx_ERROR undefined" id="p1.2">\faketableofcontents</span>
</div>
<h1 class="ltx_title ltx_title_document">Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\name</span>Qiaoyi¬†Xue *  <span class="ltx_ERROR undefined" id="id2.2.id2">\email</span>qiaoyi.xue@cri-united-imaging.com 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\addr</span>Shanghai United Imaging Healthcare Advanced Technology 
<br class="ltx_break"/>Research Institute Co., Ltd. 
<br class="ltx_break"/>Shanghai 201807, China
<span class="ltx_ERROR undefined" id="id4.4.id4">\AND</span><span class="ltx_ERROR undefined" id="id5.5.id5">\name</span>Youdan¬†Feng * <span class="ltx_ERROR undefined" id="id6.6.id6">\email</span>youdan.feng@cri-united-imaging.com 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id7.7.id7">\addr</span>Shanghai United Imaging Healthcare Advanced Technology 
<br class="ltx_break"/>Research Institute Co., Ltd. 
<br class="ltx_break"/>Shanghai 201807, China
<span class="ltx_ERROR undefined" id="id8.8.id8">\AND</span><span class="ltx_ERROR undefined" id="id9.9.id9">\name</span>Jiayi¬†Liu * <span class="ltx_ERROR undefined" id="id10.10.id10">\email</span>jiayi.liu01@cri-united-imaging.com 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id11.11.id11">\addr</span>Shanghai United Imaging Healthcare Advanced Technology 
<br class="ltx_break"/>Research Institute Co., Ltd. 
<br class="ltx_break"/>Shanghai 201807, China
<span class="ltx_ERROR undefined" id="id12.12.id12">\AND</span><span class="ltx_ERROR undefined" id="id13.13.id13">\name</span>Tianming¬†Xu <span class="ltx_ERROR undefined" id="id14.14.id14">\email</span>cecilia_xtm@sjtu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id15.15.id15">\addr</span>Global Institute of Future Technology 
<br class="ltx_break"/>Shanghai Jiao Tong University 
<br class="ltx_break"/>Shanghai 200240, China
<span class="ltx_ERROR undefined" id="id16.16.id16">\AND</span><span class="ltx_ERROR undefined" id="id17.17.id17">\name</span>Kaixin¬†Shen <span class="ltx_ERROR undefined" id="id18.18.id18">\email</span>812852899@sjtu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id19.19.id19">\addr</span>Global Institute of Future Technology 
<br class="ltx_break"/>Shanghai Jiao Tong University 
<br class="ltx_break"/>Shanghai 200240, China
<span class="ltx_ERROR undefined" id="id20.20.id20">\AND</span><span class="ltx_ERROR undefined" id="id21.21.id21">\name</span>Chuyun¬†Shen <span class="ltx_ERROR undefined" id="id22.22.id22">\email</span>cyshen@stu.ecnu.edu.cn 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id23.23.id23">\addr</span>School of Computer Science and Technology 
<br class="ltx_break"/>East China Normal University
<br class="ltx_break"/>Shanghai 200062, China
<span class="ltx_ERROR undefined" id="id24.24.id24">\AND</span><span class="ltx_ERROR undefined" id="id25.25.id25">\name</span>Yuhang¬†Shi <span class="ltx_ERROR undefined" id="id26.26.id26">\email</span>yuhang.shi@cri-united-imaging.com 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id27.27.id27">\addr</span>Shanghai United Imaging Healthcare Advanced Technology 
<br class="ltx_break"/>Research Institute Co., Ltd. 
<br class="ltx_break"/>Shanghai 201807, China
</span><span class="ltx_author_notes">These authors contributed to the work equally and should be regarded as co-first authors.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id28.id1">This study explores a workflow for automated segmentation of lesions in FDG and PSMA PET/CT images. Due to the substantial differences in image characteristics between FDG and PSMA, specialized preprocessing steps are required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are preprocessed separately before feeding them into the segmentation models, aiming to improve lesion segmentation accuracy. The study focuses on evaluating the performance of automated segmentation workflow for multitracer PET images. The findings are expected to provide critical insights for enhancing diagnostic workflows and patient-specific treatment plans.
Our code will be open-sourced and available at <a class="ltx_ref ltx_href" href="https://github.com/jiayiliu-pku/AP2024" title="">https://github.com/jiayiliu-pku/AP2024</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The increasing global cancer incidence demands advanced diagnostic and therapeutic technologies to enhance precision and personalization in cancer management. Molecular theranostics, which integrates diagnostic imaging with targeted therapy, exemplifies this trend by offering personalized treatment opportunities with unprecedented accuracy. Positron emission tomography (PET) combined with computed tomography (CT) plays a pivotal role in oncological diagnostics, utilizing radiotracers such as Fluorodeoxyglucose (FDG) and prostate-specific membrane antigen (PSMA) to effectively detect and manage various cancers. FDG is particularly effective in highlighting metabolically active cancer cells, facilitating the evaluation of multiple cancer types¬†<cite class="ltx_cite ltx_citemacro_citep">(Dholakia et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib3" title="">2014</a>)</cite>. PSMA, highly expressed in prostate cancer cells, is essential for diagnosing and treating prostate cancer, serving as a valuable target for both imaging and therapeutic interventions¬†<cite class="ltx_cite ltx_citemacro_citep">(Nickols et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib7" title="">2021</a>; Zhao et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib10" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For FDG PET/CT scans, the adoption of the deep learning methods improve lesion segmentation accuracy and overcomes the challenges associated with differentiating pathological changes from physiological uptake in organs like the liver and brain¬†<cite class="ltx_cite ltx_citemacro_citep">(Im et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib5" title="">2017</a>)</cite>. Advances in multi-label segmentation techniques enable simultaneous delineation of lesions and high-uptake organs, further improving segmentation accuracy¬†<cite class="ltx_cite ltx_citemacro_citep">(Weisman et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib9" title="">2020</a>; Barrington et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib2" title="">2020</a>)</cite>. As PSMA PET imaging has become increasingly vital for early detection of lymph node metastases and monitoring treatment responses, recent research also shows the superiority of using the deep learning methods in segmenting lesion of the PSMA PET images ¬†<cite class="ltx_cite ltx_citemacro_citep">(Fr√ºh et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib4" title="">2021</a>; Anttinen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib1" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, a significant challenge lies in the differences between FDG and PSMA PET images, which necessitate specific and targeted preprocessing steps to handle their unique properties. This study aims to develop a lesion segmentation workflow that can effectively manage both FDG and PSMA PET/CT images. Specifically, the study utilizes YOLOv8 to classify FDG and PSMA data and subsequently applies tailored preprocessing techniques before inputting the classified data into dedicated segmentation models for each tracer ¬†<cite class="ltx_cite ltx_citemacro_citep">(Varghese and M., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib8" title="">2024</a>)</cite>. By evaluating the impact of organ-specific labeling and preprocessing strategies on model performance, this research seeks to optimize PET/CT imaging for broader oncological applications, particularly for individualized prostate cancer interventions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The automated lesion segmentation process for FDG and PSMA PET images consists of two steps. First, a classification model was trained for distinguishing FDG-PET and PSMA-PET medical images. Second, two 3D Unets were trained independently with FDG or PSMA data for the organ and lesion segmentation (shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.F1" title="Figure 1 ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="221" id="S2.F1.g1" src="extracted/5855673/imgs/Workflow.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The workflow of automated lesion segmentation of FDG and PSMA PET images. </figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data and preprocessing</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Datasets for lesion segmentation</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">The training of the FDG lesion segmentation models was conducted using whole-body FDG PET/CT data from a cohort of 900 patients, encompassing 1014 studies supplied by the AutoPET challenge III in 2024. The challenge consists of patients with malignant melanoma, lymphoma, lung cancer and negative control patients. The data was split into a training set of 811 cases and a testing set of 203 cases. For the PSMA model, 600 PSMA -PET/CT data supplied by the AutoPET challenge III was split into a training set of 500 cases and a testing set of 100 cases. Lesion numbers and patient meta info were taken into consideration to ensure that both the training and testing subsets exhibited equitable distributions of lesion counts.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">In the label preprocessing phase, both CT and PET images were concurrently utilized for organ segmentation. The segmentation of bone structures was achieved using open-source framework Totalsegmentator. The segmentation of high-uptake organs was conducted on PET images using an in-house developed model based on nnU-Net. This approach was meticulously selected to mitigate the potential for mismatch between PET and CT data. Such discrepancies are often attributable to the distinct respiratory phases of abdominal organs during PET/CT scanning; specifically, CT scans are typically acquired during breath-hold periods, whereas PET scans are acquired over several minutes, capturing an average representation of the free-breathing state. This phenomenon is particularly pronounced in the case of the liver and lungs. The bone and organ segmentation labels (liver, kidneys, urinary bladder, spleen, lung, brain, heart, femur, stomach and prostate) were subsequently integrated with lesion labels, which were provided as part of the AutoPET challenge dataset (shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S2.F2" title="Figure 2 ‚Ä£ 2.1.1 Datasets for lesion segmentation ‚Ä£ 2.1 Data and preprocessing ‚Ä£ 2 Methods ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S2.F2.g1" src="extracted/5855673/imgs/label_preprocess.png" width="359"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Organ labelling pipeline.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">The data preprocessing procedures were integrated within the nnU-Net pipeline. In brief, the images underwent resampling to achieve uniform voxel spacing, followed by intensity normalization through the computation of the z-score. This standardized preprocessing ensures that the data is primed for robust and consistent analysis within the framework of the nnU-Net neural network.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Datasets for image classification</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">It is observed that PSMA-PET shows higher uptake than FDG in submandibular glands, kidneys, liver, spleen and bladder. Based on the observation, two steps were conducted to classify FDG-PET and PSMA-PET images. The maximum-intensity projection (MIP) images were generated by projecting the voxel with the highest FDG uptake value on coronal view throughout the volume onto a 2D image. Besides the Data supplied by the AutoPET challenge III, additional in-house FDG and PSMA PET MIP images are collected. The training dataset comprised of 1378 FDG and 1345 MIP PSMA images, while the testing dataset include 474 FDG and 539 PSMA MIP images. Images were preprocessed by resizing to 640x640 pixels and normalizing pixel values.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model architecture and training</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>YOLO model</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The YOLOv8 architecture was adapted for classification of PSMA-PET and FDG-PET. The model was trained with hyperparameters optimized: initial learning rate set to 0.0001 and batch size to 16. Training spanned 200 epochs, incorporating data augmentation techniques like flipping to enhance model robustness and mitigate overfitting.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>nnU-Net model</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">The models were trained based on the nnU-Net framework to segment multiple organs and lesions ¬†<cite class="ltx_cite ltx_citemacro_citep">(Isensee et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#bib.bib6" title="">2021</a>)</cite>. 3D nnU-Net was used with the ResNet18 backbone structure. The input patch size of the 3D U-Net was set to 160x160x160. The loss function is set to a combination of the Dice loss and focal loss to combat overfitting.</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Dice Loss</span>:
The Dice Loss is based on the Dice coefficient, which is a measure of overlap between two sets. The formula for Dice Loss is:</p>
</div>
<div class="ltx_para" id="S2.I1.i1.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Dice Loss}=1-\frac{2\sum_{i}p_{i}g_{i}}{\sum_{i}p_{i}^{2}+\sum_{i}g_{i}^%
{2}}" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mtext id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2a.cmml">Dice Loss</mtext><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">1</mn><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">‚àí</mo><mfrac id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mrow id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml"><mn id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">2</mn><mo id="S2.E1.m1.1.1.3.3.2.1" xref="S2.E1.m1.1.1.3.3.2.1.cmml">‚Å¢</mo><mrow id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.cmml"><msub id="S2.E1.m1.1.1.3.3.2.3.1" xref="S2.E1.m1.1.1.3.3.2.3.1.cmml"><mo id="S2.E1.m1.1.1.3.3.2.3.1.2" xref="S2.E1.m1.1.1.3.3.2.3.1.2.cmml">‚àë</mo><mi id="S2.E1.m1.1.1.3.3.2.3.1.3" xref="S2.E1.m1.1.1.3.3.2.3.1.3.cmml">i</mi></msub><mrow id="S2.E1.m1.1.1.3.3.2.3.2" xref="S2.E1.m1.1.1.3.3.2.3.2.cmml"><msub id="S2.E1.m1.1.1.3.3.2.3.2.2" xref="S2.E1.m1.1.1.3.3.2.3.2.2.cmml"><mi id="S2.E1.m1.1.1.3.3.2.3.2.2.2" xref="S2.E1.m1.1.1.3.3.2.3.2.2.2.cmml">p</mi><mi id="S2.E1.m1.1.1.3.3.2.3.2.2.3" xref="S2.E1.m1.1.1.3.3.2.3.2.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.1.1.3.3.2.3.2.1" xref="S2.E1.m1.1.1.3.3.2.3.2.1.cmml">‚Å¢</mo><msub id="S2.E1.m1.1.1.3.3.2.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2.3.2.3.2" xref="S2.E1.m1.1.1.3.3.2.3.2.3.2.cmml">g</mi><mi id="S2.E1.m1.1.1.3.3.2.3.2.3.3" xref="S2.E1.m1.1.1.3.3.2.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mrow id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mrow id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml"><msub id="S2.E1.m1.1.1.3.3.3.2.1" xref="S2.E1.m1.1.1.3.3.3.2.1.cmml"><mo id="S2.E1.m1.1.1.3.3.3.2.1.2" xref="S2.E1.m1.1.1.3.3.3.2.1.2.cmml">‚àë</mo><mi id="S2.E1.m1.1.1.3.3.3.2.1.3" xref="S2.E1.m1.1.1.3.3.3.2.1.3.cmml">i</mi></msub><msubsup id="S2.E1.m1.1.1.3.3.3.2.2" xref="S2.E1.m1.1.1.3.3.3.2.2.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2.2.2.2" xref="S2.E1.m1.1.1.3.3.3.2.2.2.2.cmml">p</mi><mi id="S2.E1.m1.1.1.3.3.3.2.2.2.3" xref="S2.E1.m1.1.1.3.3.3.2.2.2.3.cmml">i</mi><mn id="S2.E1.m1.1.1.3.3.3.2.2.3" xref="S2.E1.m1.1.1.3.3.3.2.2.3.cmml">2</mn></msubsup></mrow><mo id="S2.E1.m1.1.1.3.3.3.1" rspace="0.055em" xref="S2.E1.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.cmml"><msub id="S2.E1.m1.1.1.3.3.3.3.1" xref="S2.E1.m1.1.1.3.3.3.3.1.cmml"><mo id="S2.E1.m1.1.1.3.3.3.3.1.2" xref="S2.E1.m1.1.1.3.3.3.3.1.2.cmml">‚àë</mo><mi id="S2.E1.m1.1.1.3.3.3.3.1.3" xref="S2.E1.m1.1.1.3.3.3.3.1.3.cmml">i</mi></msub><msubsup id="S2.E1.m1.1.1.3.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.3.2.cmml"><mi id="S2.E1.m1.1.1.3.3.3.3.2.2.2" xref="S2.E1.m1.1.1.3.3.3.3.2.2.2.cmml">g</mi><mi id="S2.E1.m1.1.1.3.3.3.3.2.2.3" xref="S2.E1.m1.1.1.3.3.3.3.2.2.3.cmml">i</mi><mn id="S2.E1.m1.1.1.3.3.3.3.2.3" xref="S2.E1.m1.1.1.3.3.3.3.2.3.cmml">2</mn></msubsup></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2a.cmml" xref="S2.E1.m1.1.1.2"><mtext id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">Dice Loss</mtext></ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><minus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></minus><cn id="S2.E1.m1.1.1.3.2.cmml" type="integer" xref="S2.E1.m1.1.1.3.2">1</cn><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><divide id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3"></divide><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2"><times id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2.1"></times><cn id="S2.E1.m1.1.1.3.3.2.2.cmml" type="integer" xref="S2.E1.m1.1.1.3.3.2.2">2</cn><apply id="S2.E1.m1.1.1.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3"><apply id="S2.E1.m1.1.1.3.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.3.2.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.3.1.1.cmml" xref="S2.E1.m1.1.1.3.3.2.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.3.2.3.1.2.cmml" xref="S2.E1.m1.1.1.3.3.2.3.1.2"></sum><ci id="S2.E1.m1.1.1.3.3.2.3.1.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3.1.3">ùëñ</ci></apply><apply id="S2.E1.m1.1.1.3.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2"><times id="S2.E1.m1.1.1.3.3.2.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.1"></times><apply id="S2.E1.m1.1.1.3.3.2.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.2.2">ùëù</ci><ci id="S2.E1.m1.1.1.3.3.2.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.2.3">ùëñ</ci></apply><apply id="S2.E1.m1.1.1.3.3.2.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.3.2">ùëî</ci><ci id="S2.E1.m1.1.1.3.3.2.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3.2.3.3">ùëñ</ci></apply></apply></apply></apply><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><plus id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1"></plus><apply id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2"><apply id="S2.E1.m1.1.1.3.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.2.1.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.3.3.2.1.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.1.2"></sum><ci id="S2.E1.m1.1.1.3.3.3.2.1.3.cmml" xref="S2.E1.m1.1.1.3.3.3.2.1.3">ùëñ</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2">superscript</csymbol><apply id="S2.E1.m1.1.1.3.3.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.2.2.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.3.2.2.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2.2.2">ùëù</ci><ci id="S2.E1.m1.1.1.3.3.3.2.2.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.2.2.2.3">ùëñ</ci></apply><cn id="S2.E1.m1.1.1.3.3.3.2.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.3.3.3.2.2.3">2</cn></apply></apply><apply id="S2.E1.m1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3"><apply id="S2.E1.m1.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.3.1.1.cmml" xref="S2.E1.m1.1.1.3.3.3.3.1">subscript</csymbol><sum id="S2.E1.m1.1.1.3.3.3.3.1.2.cmml" xref="S2.E1.m1.1.1.3.3.3.3.1.2"></sum><ci id="S2.E1.m1.1.1.3.3.3.3.1.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3.1.3">ùëñ</ci></apply><apply id="S2.E1.m1.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2">superscript</csymbol><apply id="S2.E1.m1.1.1.3.3.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.3.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.3.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2.2.2">ùëî</ci><ci id="S2.E1.m1.1.1.3.3.3.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3.2.2.3">ùëñ</ci></apply><cn id="S2.E1.m1.1.1.3.3.3.3.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.3.3.3.3.2.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{Dice Loss}=1-\frac{2\sum_{i}p_{i}g_{i}}{\sum_{i}p_{i}^{2}+\sum_{i}g_{i}^%
{2}}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">Dice Loss = 1 - divide start_ARG 2 ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.I1.i1.p3">
<p class="ltx_p" id="S2.I1.i1.p3.4">where <math alttext="p_{i}" class="ltx_Math" display="inline" id="S2.I1.i1.p3.1.m1.1"><semantics id="S2.I1.i1.p3.1.m1.1a"><msub id="S2.I1.i1.p3.1.m1.1.1" xref="S2.I1.i1.p3.1.m1.1.1.cmml"><mi id="S2.I1.i1.p3.1.m1.1.1.2" xref="S2.I1.i1.p3.1.m1.1.1.2.cmml">p</mi><mi id="S2.I1.i1.p3.1.m1.1.1.3" xref="S2.I1.i1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p3.1.m1.1b"><apply id="S2.I1.i1.p3.1.m1.1.1.cmml" xref="S2.I1.i1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p3.1.m1.1.1.1.cmml" xref="S2.I1.i1.p3.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p3.1.m1.1.1.2.cmml" xref="S2.I1.i1.p3.1.m1.1.1.2">ùëù</ci><ci id="S2.I1.i1.p3.1.m1.1.1.3.cmml" xref="S2.I1.i1.p3.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p3.1.m1.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p3.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted probability for pixel <math alttext="i" class="ltx_Math" display="inline" id="S2.I1.i1.p3.2.m2.1"><semantics id="S2.I1.i1.p3.2.m2.1a"><mi id="S2.I1.i1.p3.2.m2.1.1" xref="S2.I1.i1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p3.2.m2.1b"><ci id="S2.I1.i1.p3.2.m2.1.1.cmml" xref="S2.I1.i1.p3.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p3.2.m2.1d">italic_i</annotation></semantics></math>, <math alttext="g_{i}" class="ltx_Math" display="inline" id="S2.I1.i1.p3.3.m3.1"><semantics id="S2.I1.i1.p3.3.m3.1a"><msub id="S2.I1.i1.p3.3.m3.1.1" xref="S2.I1.i1.p3.3.m3.1.1.cmml"><mi id="S2.I1.i1.p3.3.m3.1.1.2" xref="S2.I1.i1.p3.3.m3.1.1.2.cmml">g</mi><mi id="S2.I1.i1.p3.3.m3.1.1.3" xref="S2.I1.i1.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p3.3.m3.1b"><apply id="S2.I1.i1.p3.3.m3.1.1.cmml" xref="S2.I1.i1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p3.3.m3.1.1.1.cmml" xref="S2.I1.i1.p3.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i1.p3.3.m3.1.1.2.cmml" xref="S2.I1.i1.p3.3.m3.1.1.2">ùëî</ci><ci id="S2.I1.i1.p3.3.m3.1.1.3.cmml" xref="S2.I1.i1.p3.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p3.3.m3.1c">g_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p3.3.m3.1d">italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the ground truth label for pixel <math alttext="i" class="ltx_Math" display="inline" id="S2.I1.i1.p3.4.m4.1"><semantics id="S2.I1.i1.p3.4.m4.1a"><mi id="S2.I1.i1.p3.4.m4.1.1" xref="S2.I1.i1.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p3.4.m4.1b"><ci id="S2.I1.i1.p3.4.m4.1.1.cmml" xref="S2.I1.i1.p3.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p3.4.m4.1d">italic_i</annotation></semantics></math>. A higher Dice coefficient indicates a greater overlap between the predicted segmentation and the ground truth, reflecting a more accurate segmentation result.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.4"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.4.1">Focal Loss</span>:
The Focal Loss is designed to address class imbalance by down-weighting the loss assigned to well-classified examples. The Focal Loss is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Focal Loss}(p_{t})=-\alpha_{t}(1-p_{t})^{\gamma}\log(p_{t})" class="ltx_Math" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.cmml"><mtext id="S2.E2.m1.2.2.1.3" xref="S2.E2.m1.2.2.1.3a.cmml">Focal Loss</mtext><mo id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.2.cmml">‚Å¢</mo><mrow id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.cmml"><mo id="S2.E2.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E2.m1.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.2.cmml">p</mi><mi id="S2.E2.m1.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.2.2.1.1.1.3" stretchy="false" xref="S2.E2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.4" xref="S2.E2.m1.4.4.4.cmml">=</mo><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><mo id="S2.E2.m1.4.4.3a" xref="S2.E2.m1.4.4.3.cmml">‚àí</mo><mrow id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><msub id="S2.E2.m1.4.4.3.2.4" xref="S2.E2.m1.4.4.3.2.4.cmml"><mi id="S2.E2.m1.4.4.3.2.4.2" xref="S2.E2.m1.4.4.3.2.4.2.cmml">Œ±</mi><mi id="S2.E2.m1.4.4.3.2.4.3" xref="S2.E2.m1.4.4.3.2.4.3.cmml">t</mi></msub><mo id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3.cmml">‚Å¢</mo><msup id="S2.E2.m1.3.3.2.1.1" xref="S2.E2.m1.3.3.2.1.1.cmml"><mrow id="S2.E2.m1.3.3.2.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.2.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.3.3.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.2.1.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.1.cmml"><mn id="S2.E2.m1.3.3.2.1.1.1.1.1.2" xref="S2.E2.m1.3.3.2.1.1.1.1.1.2.cmml">1</mn><mo id="S2.E2.m1.3.3.2.1.1.1.1.1.1" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S2.E2.m1.3.3.2.1.1.1.1.1.3" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.2.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.E2.m1.3.3.2.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo id="S2.E2.m1.3.3.2.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.3.3.2.1.1.1.1.1.cmml">)</mo></mrow><mi id="S2.E2.m1.3.3.2.1.1.3" xref="S2.E2.m1.3.3.2.1.1.3.cmml">Œ≥</mi></msup><mo id="S2.E2.m1.4.4.3.2.3a" lspace="0.167em" xref="S2.E2.m1.4.4.3.2.3.cmml">‚Å¢</mo><mrow id="S2.E2.m1.4.4.3.2.2.1" xref="S2.E2.m1.4.4.3.2.2.2.cmml"><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">log</mi><mo id="S2.E2.m1.4.4.3.2.2.1a" xref="S2.E2.m1.4.4.3.2.2.2.cmml">‚Å°</mo><mrow id="S2.E2.m1.4.4.3.2.2.1.1" xref="S2.E2.m1.4.4.3.2.2.2.cmml"><mo id="S2.E2.m1.4.4.3.2.2.1.1.2" stretchy="false" xref="S2.E2.m1.4.4.3.2.2.2.cmml">(</mo><msub id="S2.E2.m1.4.4.3.2.2.1.1.1" xref="S2.E2.m1.4.4.3.2.2.1.1.1.cmml"><mi id="S2.E2.m1.4.4.3.2.2.1.1.1.2" xref="S2.E2.m1.4.4.3.2.2.1.1.1.2.cmml">p</mi><mi id="S2.E2.m1.4.4.3.2.2.1.1.1.3" xref="S2.E2.m1.4.4.3.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.4.4.3.2.2.1.1.3" stretchy="false" xref="S2.E2.m1.4.4.3.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.4.cmml" xref="S2.E2.m1.4.4.4"></eq><apply id="S2.E2.m1.2.2.1.cmml" xref="S2.E2.m1.2.2.1"><times id="S2.E2.m1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.2"></times><ci id="S2.E2.m1.2.2.1.3a.cmml" xref="S2.E2.m1.2.2.1.3"><mtext id="S2.E2.m1.2.2.1.3.cmml" xref="S2.E2.m1.2.2.1.3">Focal Loss</mtext></ci><apply id="S2.E2.m1.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.2">ùëù</ci><ci id="S2.E2.m1.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3">ùë°</ci></apply></apply><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><minus id="S2.E2.m1.4.4.3.3.cmml" xref="S2.E2.m1.4.4.3"></minus><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><times id="S2.E2.m1.4.4.3.2.3.cmml" xref="S2.E2.m1.4.4.3.2.3"></times><apply id="S2.E2.m1.4.4.3.2.4.cmml" xref="S2.E2.m1.4.4.3.2.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.4.1.cmml" xref="S2.E2.m1.4.4.3.2.4">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.4.2.cmml" xref="S2.E2.m1.4.4.3.2.4.2">ùõº</ci><ci id="S2.E2.m1.4.4.3.2.4.3.cmml" xref="S2.E2.m1.4.4.3.2.4.3">ùë°</ci></apply><apply id="S2.E2.m1.3.3.2.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.2.1.1.2.cmml" xref="S2.E2.m1.3.3.2.1.1">superscript</csymbol><apply id="S2.E2.m1.3.3.2.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1"><minus id="S2.E2.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.1"></minus><cn id="S2.E2.m1.3.3.2.1.1.1.1.1.2.cmml" type="integer" xref="S2.E2.m1.3.3.2.1.1.1.1.1.2">1</cn><apply id="S2.E2.m1.3.3.2.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.2.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.2.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3.2">ùëù</ci><ci id="S2.E2.m1.3.3.2.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.2.1.1.1.1.1.3.3">ùë°</ci></apply></apply><ci id="S2.E2.m1.3.3.2.1.1.3.cmml" xref="S2.E2.m1.3.3.2.1.1.3">ùõæ</ci></apply><apply id="S2.E2.m1.4.4.3.2.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2.1"><log id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"></log><apply id="S2.E2.m1.4.4.3.2.2.1.1.1.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.2.1.1.1.1.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2.1.1.1.2.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.1.2">ùëù</ci><ci id="S2.E2.m1.4.4.3.2.2.1.1.1.3.cmml" xref="S2.E2.m1.4.4.3.2.2.1.1.1.3">ùë°</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\text{Focal Loss}(p_{t})=-\alpha_{t}(1-p_{t})^{\gamma}\log(p_{t})</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">Focal Loss ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( 1 - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_Œ≥ end_POSTSUPERSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.I1.i2.p1.3">where <math alttext="p_{t}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">p</mi><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">ùëù</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">p_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted probability of the true class. <math alttext="\alpha_{t}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><msub id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml">Œ±</mi><mi id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">ùõº</ci><ci id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">\alpha_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is a weighting factor for class imbalance. <math alttext="\gamma" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3.1"><semantics id="S2.I1.i2.p1.3.m3.1a"><mi id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><ci id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.1d">italic_Œ≥</annotation></semantics></math> is the focusing parameter that controls the rate at which easy examples are down-weighted.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">The models are trained with 1000 epochs and a batch size of 4, using the SGD optimizer and an initial learning rate of 0.01. This model was then formatted in docker and submitted to the challenge portal for testing and benchmarking.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1">The evaluation of model performance is conducted using metrics such as the Dice score, false positive volume (FPvol) and false negative volume (FNvol), which provide a comprehensive assessment of the segmentation methodologies.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The PET model achieved a classification accuracy of 99.85% which showed superior performance in differentiating FDG and PSMA PET images.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The evaluation of our lesion segmentation models for FDG and PSMA PET/CT images produced the following outcomes (shown in Table.<a class="ltx_ref" href="https://arxiv.org/html/2409.09766v1#S3.T1" title="Table 1 ‚Ä£ 3 Results and Discussion ‚Ä£ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting"><span class="ltx_text ltx_ref_tag">1</span></a>): the Dice coefficients were 0.8408 for FDG and 0.7385 for PSMA. False Positive volumes (FPvol) were 1.7979 for FDG and 9.3574 for PSMA, while False Negative volumes (FNvol) were 2.3625 for FDG and 5.0745 for PSMA. These results indicate differences in segmentation performance between the two imaging modalities.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of lesion segmentation models for PSMA and FDG PET images.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:260.2pt;height:71.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(32.4pt,-8.9pt) scale(1.33100303009416,1.33100303009416) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1">Dice</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1">FPvol</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1">FNvol</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.1">FDG nnU-Net</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.2">0.8408</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.3">1.7979</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.4">2.3625</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T1.1.1.3.2.1">PSMA nnU-Net</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.1.3.2.2">0.7385</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.1.3.2.3">9.3574</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.1.1.3.2.4">5.0745</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In conclusion, this study demonstrated the feasibility of the proposed lesion segmentation workflow for both FDG and PSMA PET/CT images. YOLOv8 demonstrated its superior performance in classifying the PSMA and FDG PET image which allows for using tailored preprocessing techniques in segmenting the lesion in PET image with different tracers.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anttinen et¬†al. (2021)</span>
<span class="ltx_bibblock">
Mikael Anttinen, Otto Ettala, Simona Malaspina, Ivan Jambor, Minna Sandell, Sami Kajander, Irina Rinta-Kiikka, Jukka Schildt, Ekaterina Saukko, Pentti Rautio, Kirsi¬†L. Timonen, Tuomas Matikainen, Tommi Noponen, Jani Saunavaara, Eliisa L√∂yttyniemi, Pekka Taimen, Jukka Kemppainen, Peter¬†B. Dean, Roberto Blanco¬†Sequeiros, Hannu¬†J. Aronen, Marko Sepp√§nen, and Peter¬†J. Bostr√∂m.

</span>
<span class="ltx_bibblock">A prospective comparison of 18f-prostate-specific membrane antigen-1007 positron emission tomography computed tomography, whole-body 1.5 t magnetic resonance imaging with diffusion-weighted imaging, and single-photon emission computed tomography/computed tomography with traditional imaging in primary distant metastasis staging of prostate cancer (prostage).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">European Urology Oncology</em>, 4(4):635‚Äì644, August 2021.

</span>
<span class="ltx_bibblock">ISSN 2588-9311.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.euo.2020.06.012</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.euo.2020.06.012" title="">http://dx.doi.org/10.1016/j.euo.2020.06.012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrington et¬†al. (2020)</span>
<span class="ltx_bibblock">
Sally¬†F. Barrington, Ben¬†G.J.C. Zwezerijnen, Henrica¬†C.W. de¬†Vet, Martijn¬†W. Heymans, N.¬†George Mikhaeel, Coreline¬†N. Burggraaff, Jakoba¬†J. Eertink, Lucy¬†C. Pike, Otto¬†S. Hoekstra, Jos√©e¬†M. Zijlstra, and Ronald Boellaard.

</span>
<span class="ltx_bibblock">Automated segmentation of baseline metabolic total tumor burden in diffuse large b-cell lymphoma: Which method is most successful? a study on behalf of the petra consortium.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Journal of Nuclear Medicine</em>, 62(3):332‚Äì337, July 2020.

</span>
<span class="ltx_bibblock">ISSN 2159-662X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.2967/jnumed.119.238923</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.2967/jnumed.119.238923" title="">http://dx.doi.org/10.2967/jnumed.119.238923</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dholakia et¬†al. (2014)</span>
<span class="ltx_bibblock">
Avani¬†S. Dholakia, Muhammad Chaudhry, Jeffrey¬†P. Leal, Daniel¬†T. Chang, Siva¬†P. Raman, Amy Hacker-Prietz, Zheng Su, Jonathan Pai, Katharine¬†E. Oteiza, Mary¬†E. Griffith, Richard¬†L. Wahl, Erik Tryggestad, Timothy Pawlik, Daniel¬†A. Laheru, Christopher¬†L. Wolfgang, Albert¬†C. Koong, and Joseph¬†M. Herman.

</span>
<span class="ltx_bibblock">Baseline metabolic tumor volume and total lesion glycolysis are associated with survival outcomes in patients with locally advanced pancreatic cancer receiving stereotactic body radiation therapy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Journal of Radiation Oncology*Biology*Physics</em>, 89(3):539‚Äì546, July 2014.

</span>
<span class="ltx_bibblock">ISSN 0360-3016.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.ijrobp.2014.02.031</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1016/j.ijrobp.2014.02.031" title="">http://dx.doi.org/10.1016/j.ijrobp.2014.02.031</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fr√ºh et¬†al. (2021)</span>
<span class="ltx_bibblock">
Marcel Fr√ºh, Marc Fischer, Andreas Schilling, Sergios Gatidis, and Tobias Hepp.

</span>
<span class="ltx_bibblock">Weakly supervised segmentation of tumor lesions in pet-ct hybrid imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Medical Imaging</em>, 8(05), October 2021.

</span>
<span class="ltx_bibblock">ISSN 2329-4302.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1117/1.jmi.8.5.054003</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1117/1.Jmi.8.5.054003" title="">http://dx.doi.org/10.1117/1.Jmi.8.5.054003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Im et¬†al. (2017)</span>
<span class="ltx_bibblock">
Hyung-Jun Im, Tyler Bradshaw, Meiyappan Solaiyappan, and Steve¬†Y. Cho.

</span>
<span class="ltx_bibblock">Current methods to define metabolic tumor volume in positron emission tomography: Which one is better?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Nuclear Medicine and Molecular Imaging</em>, 52(1):5‚Äì15, September 2017.

</span>
<span class="ltx_bibblock">ISSN 1869-3482.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s13139-017-0493-6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s13139-017-0493-6" title="">http://dx.doi.org/10.1007/s13139-017-0493-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isensee et¬†al. (2021)</span>
<span class="ltx_bibblock">
Fabian Isensee, Paul¬†F Jaeger, Simon¬†AA Kohl, Jens Petersen, and Klaus¬†H Maier-Hein.

</span>
<span class="ltx_bibblock">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Nature Methods</em>, 18(2):203‚Äì211, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nickols et¬†al. (2021)</span>
<span class="ltx_bibblock">
Nicholas Nickols, Aseem Anand, Kerstin Johnsson, Johan Brynolfsson, Pablo Borreli, Neil Parikh, Jesus Juarez, Lida Jafari, Mattias Eiber, and Matthew Rettig.

</span>
<span class="ltx_bibblock">apromise: A novel automated promise platform to standardize evaluation of tumor burden in 18f-dcfpyl images of veterans with prostate cancer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of Nuclear Medicine</em>, 63(2):233‚Äì239, May 2021.

</span>
<span class="ltx_bibblock">ISSN 2159-662X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.2967/jnumed.120.261863</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.2967/jnumed.120.261863" title="">http://dx.doi.org/10.2967/jnumed.120.261863</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varghese and M. (2024)</span>
<span class="ltx_bibblock">
Rejin Varghese and Sambath M.

</span>
<span class="ltx_bibblock">Yolov8: A novel object detection algorithm with enhanced performance and robustness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)</em>. IEEE, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/adics58448.2024.10533619</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1109/adics58448.2024.10533619" title="">http://dx.doi.org/10.1109/adics58448.2024.10533619</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weisman et¬†al. (2020)</span>
<span class="ltx_bibblock">
Amy¬†J Weisman, Minnie¬†W Kieler, Scott Perlman, Martin Hutchings, Robert Jeraj, Lale Kostakoglu, and Tyler¬†J Bradshaw.

</span>
<span class="ltx_bibblock">Comparison of 11 automated pet segmentation methods in lymphoma.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Physics in Medicine Biology</em>, 65(23):235019, November 2020.

</span>
<span class="ltx_bibblock">ISSN 1361-6560.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1088/1361-6560/abb6bd</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1088/1361-6560/abb6bd" title="">http://dx.doi.org/10.1088/1361-6560/abb6bd</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et¬†al. (2019)</span>
<span class="ltx_bibblock">
Yu¬†Zhao, Andrei Gafita, Bernd Vollnberg, Giles Tetteh, Fabian Haupt, Ali Afshar-Oromieh, Bjoern Menze, Matthias Eiber, Axel Rominger, and Kuangyu Shi.

</span>
<span class="ltx_bibblock">Deep neural network for automatic characterization of lesions on 68ga-psma-11 pet/ct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">European Journal of Nuclear Medicine and Molecular Imaging</em>, 47(3):603‚Äì613, December 2019.

</span>
<span class="ltx_bibblock">ISSN 1619-7089.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s00259-019-04606-y</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s00259-019-04606-y" title="">http://dx.doi.org/10.1007/s00259-019-04606-y</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 15 15:22:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
