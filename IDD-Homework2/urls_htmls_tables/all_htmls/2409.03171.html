<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering</title>
<!--Generated on Thu Sep  5 01:56:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Natural Language Processing,  Large Language Models,  Information Retrieval" lang="en" name="keywords"/>
<base href="/html/2409.03171v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S1" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S2" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S3" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>CRAG Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S3.SS1" title="In 3. CRAG Dataset ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Task 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S3.SS2" title="In 3. CRAG Dataset ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Task 2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S3.SS3" title="In 3. CRAG Dataset ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Task 3</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>MARAGS Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.SS1" title="In 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Webpage Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.SS2" title="In 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>API Call Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.SS3" title="In 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Candidate Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.SS4" title="In 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S5" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S6" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S7" title="In MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mitchell DeHaven
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mdehaven@darkhive.com">mdehaven@darkhive.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Darkhive</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">San Antonio</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Texas</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id5.id1">In this paper we present a multi-adapter retrieval augmented generation system (MARAGS) for Meta’s Comprehensive RAG (CRAG) competition for KDD CUP 2024. CRAG is a question answering dataset contains 3 different subtasks aimed at realistic question and answering RAG related tasks, with a diverse set of question topics, question types, time dynamic answers, and questions featuring entities of varying popularity.</p>
<p class="ltx_p" id="id6.id2">Our system follows a standard setup for web based RAG, which uses processed web pages to provide context for an LLM to produce generations, while also querying API endpoints for additional information. MARAGS also utilizes multiple different adapters to solve the various requirements for these tasks with a standard cross-encoder model for ranking candidate passages relevant for answering the question. Our system achieved 2nd place for Task 1 as well as 3rd place on Task 2.</p>
</div>
<div class="ltx_keywords">Natural Language Processing, Large Language Models, Information Retrieval
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; August 25–29; Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Information extraction</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Multi-task learning</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language generation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval augmented generation (RAG) has been a popular approach for question answering systems for some time <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib7" title="">2020</a>)</cite>, although recently has become a very popular approach for a wide range of tasks due to the zero-shot capabilities of large language models (LLMs) with an appropriate prompt and access to the relevant context for the task. Despite the existence of numerous question answering benchmarks, many do accurately reflect the diverse usage of current RAG systems. Thus, tracking both the efficacy of certain RAG architectures as well as tracking process remains difficult. The CRAG <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib15" title="">2024</a>)</cite> benchmark aims to resolve this with 3 different subtasks representing realistic RAG usage scenarios.
The final key element of the CRAG benchmark is it’s scoring metric which explicitly punishes hallucinations. With the rising capabilities of LLMs, increasingly their outputs are taken at face value, despite the known issue of hallucinations. This has lead to high profile incidents causing concern with their use <cite class="ltx_cite ltx_citemacro_citep">(law, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib2" title="">[n. d.]</a>)</cite>. The CRAG score aims to punish hallucinated answers and encourages returning missing answers, equivalent to returning ”i don’t know” from the model, by giving scores of 1, 0, and -1 to correct, missing, and hallucinated answers respectively.
To address these various tasks, we train individual adapters for each of the various tasks, as well as API call generation required for accessing information in the mock API. This approach allows us to use a single LLama 3 <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib3" title="">2024</a>)</cite> in memory while swapping out adapters based on the current needs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The initial approach of Lewis et al. <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib7" title="">2020</a>)</cite> showed the benefits of providing additional text context for seq2seq models for NLP tasks that are knowledge entensive. Using BART, they were able to improve question answering tasks using dual biencoders for retrieval and training the model jointly, without the need for knowing which documents were relevant.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Adapters have become increasingly used since introduced by Houlsby et al. <cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib5" title="">2019</a>)</cite>. LoRa <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib6" title="">2022</a>)</cite> has become a popular adapter approach, particularly for LLMs as they have grown substantially larger in recent years. The use of adapters allows modifying a models output without training the entire network, which substantially saves on VRAM memory when training. Hu et al. <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib6" title="">2022</a>)</cite> discovered that when replacing a dot product between large vectors with an intermediate dot product of a much lower rank vector, the impacts on performance were minimal while further reducing the training parameters required.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Finally, Stickland and Murray <cite class="ltx_cite ltx_citemacro_citep">(Stickland and Murray, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib12" title="">2019</a>)</cite> produced a multi-adapter model based on BERT, an approach that our system follows. In particular for the GLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib13" title="">2018b</a>)</cite> benchmark, which is comprised of multiple datasets, they showed that simply training a task specific adapter per dataset, they could improve the average performance by 0.5 points for BERT, while only introducing 10% more parameters.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S2.F1.g1" src="extracted/5834477/marags_v4.drawio.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Pipeline for MARAGS. Each Llama in the figure represents a distinct LoRa model that can be rapidly swapped out during inference and is trained for its specific task. These tasks include the API call geneartion and the final question answering, for each task. Note, Task 1 does not include API documents in its final prompt.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>CRAG Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">CRAG is question answering dataset aimed at providing a realistic task to benchmark RAG systems as they are used in practice with a diverse set of questions, including 8 distinct question types and 5 distinct domains. Additionally, two sources of diversity which pose difficulty for LLMs are how dynamic a question’s answer is and the popularity of the topic of the question. As shown in the baseline systems, real-time answers pose a challenge for RAG systems and they similarly struggle when the topic of the question is less common (referred to as ”torso” and ”tail” questions).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Task 1</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For the first task the system must process 5 candidate HTML documents for generating answers, reflecting a standard web-based RAG application. A caveat is that the 5 candidates are sampled from the top-10 relevant documents retrieved from web search. Thus, there is no guarantee that the relevant information for answering the question is actually found within the top 5 documents. This creates an interesting challenge for hallucinations, as in some cases the answer should be easily generated by the model without the context from retrieved documents.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Task 2</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Task 2 reflects a more sophisticated RAG scenario, where the system is provided with the same 5 HTML documents from before, however it now has access to a knowledge graph, accessible via a REST API. The system must determine which API endpoint to call, with the correct arguments, to retrieve additional relevant context from the knowledge graph.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Task 3</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Finally, Task 3 represents an extension of Task 2, where the system has access to both HTML and the knowledge graph API. However, in this task, the number of HTML documents that are to be processed is 50. This task is meant to measure both the computational efficiency of the approach as well as its ability to filter large amounts of potentially irrelevant information.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>MARAGS Pipeline</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Webpage Processing</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our webpage processing pipeline utilizes BeautifulSoup4 <cite class="ltx_cite ltx_citemacro_citep">(Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib11" title="">2007</a>)</cite> for generating candidate segments from the HTML documents provided to the system. A common difficulty with RAG systems is determining a process for segmenting documents into smaller chunks to narrow the candidates to relevant sections and also reducing the length of the text sent to the model, given that a single document could exceed the context window of a model.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In our case, we utilize the structure of the HTML to provide the segmentation. We traverse the tree structure of the parse HTML with breath first search. Any time the length of the text contained within a node (which includes all of the text of its decedents) is less than 2000 characters, we treat that as a segment. If a leaf node is reached with greater than 2000 characters, the text is split on the white space and into as many segments are needed such that each segment is under the threshold. The segment length was determined via inspection of of HTML documents and their associated segmentation, thus future work could treat this as a hyperparameter and tune for performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>API Call Generation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For Task 2 and 3, the knowledge graph mock API is available to be used for gathering additional information. The difficulty, however, is not only determining which API endpoint is the most appropriate, but also the proper arguments and their formatting for getting valid results from the API.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Each API endpoint was transformed to a Python function with relevant documentation describing the purpose of the endpoint, the arguments, and what the endpoint returned. Each function also has an associated formatting function, which takes the returned JSON and converts it into segmented strings. The doc strings for each Python function are used to provide additional information to help guide the model on which one is the most appropriate to use.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">For training a model to generate API calls with associated arguments, we use LoRa <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib6" title="">2022</a>)</cite> to train one of the several adapters we use with Llama 3 8B. For generating the target string for training, we first use Llama 3 to generate an initial prediction for the API call. Any initial prediction that successfully calls a function is retained as a target, regardless of whether or not the relevant information for the question is contained in the returned JSON. Initial predictions that fail to make a successful call are inspected and manually corrected if the correct function call is clear from the initial prediction and the question. Again, the manually modified targets are evaluated only on successfully calling an endpoint, though not validating that the relevant information is returned by the API. Any question where the target cannot be quickly modified is changed to a target of ”None”.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">We acknowledge that this approach to annotation is not optimal, as it likely results in successful, but incorrectly selected API endpoint calls. However, manually annotating each question to determine the correct API call and validating the returned information were indeed relevant would have been too time consuming give the size of the dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Candidate Ranking</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For candidate ranking, we attempted 4 different candidate ranking approaches. We utilized TF-IDF, a biencoder, cross-encoder, and an ensemble of mentioned approaches (using mean rank as the ranking metric). Our TF-IDF implementation is based on Scikit-Learn <cite class="ltx_cite ltx_citemacro_citep">(Pedregosa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib9" title="">2011</a>)</cite>. The biencoder and cross-encoder are from the SentenceTransformer <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib10" title="">2019</a>)</cite> library, specifically the ”multi-qa-MiniLM-L6-cos-v1” <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1</span></span></span> and ”ms-marco-MiniLM-L-6-v2” <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2</span></span></span> respectively.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Evaluating candidate ranking in isolation is difficult, as relevant information is not labeled, so using the system accuracy and CRAG score is the most straight forward way to compare differences in each system. However, to test the various systems, we use the base Llama-3 8B model with not adapters for each retrieval approach and use the accuracy metric to determine the best performing approach. We use accuracy instead of CRAG at this stage for ranking, as we think this is a better representation of how often the relevant information retrieved. For a test set, we randomly select 500 samples from the Task 1 dataset.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>A comparison of the retrieval approaches on a 500 set sample of CRAG dev set against Accuracy and CRAG Score.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">Retrieval Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">Accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">CRAG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">TF-IDF</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">0.2740</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.3.1">-0.110</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1">Biencoder</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2.2">0.310</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.2.3">-0.132</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.3.1">Cross-encoder</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.2.1">0.328</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.3.3">-0.116</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.4.1">Ensemble (mean rank)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.4.2">0.308</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.4.3">-0.128</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The results of this experiment are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.T1" title="Table 1 ‣ 4.3. Candidate Ranking ‣ 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>. From the results, the cross-encoder is the best performing system, thus we used it for our retriever. We suspect that with proper tuning TF-IDF would and ensembling would be much more performant overall, but as mentioned running extensive experimentation is difficult as it required LLM generation to get an overall accuracy score. Using an LLM to label passages as relevant or not is a possible approach to allow for tuning of just the retriever, however we did not explore this.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Despite the cross-encoder being the most computationally expensive approach, we found it to be fast enough processing the candidates in the required 30 seconds per sample. In the case of Task 3, we found it necessary to use Python’s multiprocessing library to process multiple HTMLs simultaneously to meet the runtime requirement.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Retrieval Augmented Generation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Finally, with ranked candidates, we use Llama 3 8B to augment generation with the relevant context for the question. We ran experiments with 2 different prompt structures, the primary difference between them being the ordering of the question and context.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Our initial prompt structure started with the question first, then all of the retrieved candidates prior to the Llama model response, however we noticed that often due to how much context would be provided, the model would occasionally forget the question being asked. For example a question like ”What is the elevation of Honolulu?” would result in an answer of ”Honolulu population is 343,421”, indicating the model remember the subject of the question, but incorrectly answered with the population, rather than elevation. Our subsequent prompt structure placed the question after the context, which resolved the issue.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">For training Llama 3, we trained LoRa models for each task individually. Given the penalization of hallucinations in the scoring metric, we try to take steps to mitigate further hallucinations due to finetuning, as it has been observed that finetuning LLMs can be a source of further hallucinations <cite class="ltx_cite ltx_citemacro_citep">(Gekhman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib4" title="">2024</a>)</cite>. This likely applies to RAG systems in cases where the expected answer is not answerable given the context provided, i.e. no relevant information is given and the question is not answerable without further context, yet the model is trained to output the target answer regardless. Thus for our training setup, we first relabel the target for training samples in cases where our candidate retrieval system likely does not provide the correct information and Llama does not know the answer without that information. We use the provided dev set for training, with the 500 set sample used for retrieval comparison treated as our holdout set.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>A comparison across inference models test. Training on relabeled ”hittable” targets hurts accuracy, but provides the best CRAG Score overall.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">Accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">Hallucination</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">CRAG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1">Llama 3 8B</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2">0.328</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3">0.4440</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.4">-0.1160</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1">- LoRa</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.2.1">0.398</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.3">0.602</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.4">-0.204</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.3.1">- LoRa (relabeled)</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.4.3.2">0.242</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.3.1">0.056</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.4.1">0.186</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S4.F2.g1" src="extracted/5834477/results_v2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>CRAG Score results on the test dataset calculated via manual assessment.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Our initial approach for determining which training samples need relabeling was has been explored previously <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib14" title="">2018a</a>)</cite>. A common and simple approach to filter/relabel incorrectly<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>”Incorrectly” here simply means in the context of our retrieval system, not that the provided answer is not true.</span></span></span> labeled samples is to use a particular sample’s training loss after training to the point of overfitting. High loss examples after overfitting likely indicate examples that are incorrectly labeled and thus can be filtered out. Not all samples with high loss will be incorrect labels, instead simply being hard examples, yet typically the benefit of disposing of incorrectly labeled samples outweighs ignoring difficult ones.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Initial experiments, however, indicated that this method did not work well on finance based questions. Further analysis would be required for a more definitive answer, though we suspect that this is due to the fact that the loss in hallucinated answers when dealing with numeric outputs is likely less than typical string outputs. For example, with a question ”What was Apple’s closing price today?”, with a hypothetical correct answer ”$203.51”, a prediction of ”$204.52” would likely not result in filtering via this method. Compare that with a question such as ”Which movie won Best Picture in 1990?” with an answer of ”Driving Miss Daisy” and a prediction of ”Dancing with Wolves”, the loss will be comparatively much higher.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">We instead determine these samples by first running the system with the base Llama 3 model, with a prompt indicating to always produce a prediction for each of the 4 candidate retrieval approaches mentioned previously. We use GPT-4o to evaluate whether any of the generated answers are correct. If any are correct, the original label is retained for the training, otherwise ”i don’t know” is used as the target label. In the case of false premise questions, we always retain the original label as the target label. We repeat this process for each task, given that each has access to a different data sources, to generate a training dataset for each LoRa adapter.</p>
</div>
<div class="ltx_para" id="S4.SS4.p7">
<p class="ltx_p" id="S4.SS4.p7.1">We use the llama-recipes repository <cite class="ltx_cite ltx_citemacro_citep">(Meta, <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#bib.bib8" title="">2024</a>)</cite> for training the LoRa adapters, utilizing the default LoRa configuration. The only modifications were changing the LoRa rank from 8 to 256 and increasing the weight decay from 0.01 to 1.0.</p>
</div>
<div class="ltx_para" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1">We demonstrate the effectiveness of relabeling in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.T2" title="Table 2 ‣ 4.4. Retrieval Augmented Generation ‣ 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>. We ran 3 different answer generation setups for the 500 sample Task 1 hold out set we created. The first is an unmodified Llama 3 8B model, the second is a LoRa model using the original targets, and the final a LoRa model with relabeled targets. As shown, using the original targets provide the best accuracy, but also worsens hallucinations over the base model. While using the relabeled targets hurts accuracy, it also substantially reduces hallucinations, providing the best CRAG Score amongst the three.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">As part of the competition, a manual evaluation was conducted on user submissions. The automatic evaluation throughout was dependent on scoring via GPT-3.5 Turbo, given that correct user submissions may not have been exact matches to the expected answer. However, issues such as prompt injection still pose problems for automatic evaluation via LLMs. The results of our system across the various aspects of the dataset are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03171v1#S4.F2" title="Figure 2 ‣ 4.4. Retrieval Augmented Generation ‣ 4. MARAGS Pipeline ‣ MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>. As can be seen by the results, our system suffers many of the problems the dataset is meant to expose with most RAG systems.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Similar to the baseline systems for CRAG, finance was the most challenging domain. The exact reason warrants further analysis, though contributing factors likely include LLMs known issues with number processing and simple mathematics and the fact that much online finance data is often not stored in plain text, but rather visualizations such as graphs.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Dynamism proves to be the most challenging question categorization, with model performance steadily decreasing as a question becomes more dynamic. Real-time questions prove to be the most challenging question category of any of the breakouts. Our prompt structure did not include any meta-data provided by the HTML documents, such as publish data or access date, which likely would have improved performance on dynamic questions, although likely not significantly.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The performance difference between head, torso, and tail questions appeared less substantial than our original expectations, though clearly performance drops off as popularity falls. Interestingly, Task 3 here under performs the other tasks in head, torso, and tail. We suspect that including substantially more search results includes overlapping entities / subjects, at which point conflicting information would be difficult to resolve.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Finally, the most interesting results in the question type results are the false premise category. Our system was able to achieve scores similar to the SOTA systems featured in the CRAG paper, despite obviously being a much smaller system overall. Interestingly, the false premise questions were the only type where our training setup always kept the original target label, rather than mapping the target to ”i don’t know”.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Observations we had during the competition were instances of catastrophic forgetting due to our attempts to reduce hallucinations. For instance, the question ”Who has had a longer musical career, Sharika or Machine Gun Kelly?” is answerable by Llama without context, simply based on the knowledge it has of the two artists. However, after LoRa training, questions like this and others were often answered with ”i don’t know” in cases where the answer was not discoverable in the retrieved information. Methods to prevent this is something we are interested in pursuing in future work.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Additionally, we hope to explore larger Llama models, 70B+, in the future for this task. We were unable to get the 70B model running in the competition compute environments, so did not spend much timer looking at larger models. However, it is very likely moving to larger models would provide a substantial improvement over the 8B model.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work we presented MARAGS, a multi-adapter solution to the CRAG dataset. We demonstrated the effectiveness of training individual LoRa adapters for the 4 tasks in the pipeline, specifically API call generation and Task 1,2, and 3 answer generation. CRAG presents a variety of different tasks and questions to allow the tracking of progress of various methods used to build RAG systems. The penalization of hallucinations is a unique and important feature as future AI systems become increasingly common throughout society, as hallucinations hurt user trust in these systems. We discussed our methods for reducing these hallucinations, but they are not without cost, as in some cases the model fails to output previously known knowledge. Clearly the importance of balancing these two factors is a key to leveraging LLMs to their full potential, while also improving user trust.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We are grateful to the KDD Cup organizers, Meta, and AIcrowd for all the work that goes into hosting a successful competition.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">law ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock">Lawyer Used ChatGPT In Court—And Cited Fake Cases. A Judge Is Considering Sanctions.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/" title="">https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-08-08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,
Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan
Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua
Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham,
Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral,
Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney
Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei,
Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho,
Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim,
Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei
Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai
Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li,
Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov,
Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr
Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao
Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,
Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,
Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024.

</span>
<span class="ltx_bibblock">The Llama 3 Herd of Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.21783 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.21783" title="">https://arxiv.org/abs/2407.21783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gekhman et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024.

</span>
<span class="ltx_bibblock">Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.05904 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.05904" title="">https://arxiv.org/abs/2405.05904</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-Efficient Transfer Learning for NLP.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">CoRR</em> abs/1902.00751 (2019).

</span>
<span class="ltx_bibblock">arXiv:1902.00751

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1902.00751" title="">http://arxiv.org/abs/1902.00751</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">https://openreview.net/forum?id=nZeVKeeFYf9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em> (Vancouver, BC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(NIPS ’20)</em>. Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024)</span>
<span class="ltx_bibblock">
Meta. 2024.

</span>
<span class="ltx_bibblock">llama-recipes.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama-recipes" title="">https://github.com/meta-llama/llama-recipes</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedregosa et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine Learning in Python.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Journal of Machine Learning Research</em> 12 (2011), 2825–2830.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1908.10084" title="">http://arxiv.org/abs/1908.10084</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson (2007)</span>
<span class="ltx_bibblock">
Leonard Richardson. 2007.

</span>
<span class="ltx_bibblock">Beautiful soup documentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">April</em> (2007).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://beautiful-soup-4.readthedocs.io/en/latest/" title="">https://beautiful-soup-4.readthedocs.io/en/latest/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stickland and Murray (2019)</span>
<span class="ltx_bibblock">
Asa Cooper Stickland and Iain Murray. 2019.

</span>
<span class="ltx_bibblock">BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em> abs/1902.02671 (2019).

</span>
<span class="ltx_bibblock">arXiv:1902.02671

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1902.02671" title="">http://arxiv.org/abs/1902.02671</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2018b)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018b.

</span>
<span class="ltx_bibblock">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CoRR</em> abs/1804.07461 (2018).

</span>
<span class="ltx_bibblock">arXiv:1804.07461

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1804.07461" title="">http://arxiv.org/abs/1804.07461</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2018a)</span>
<span class="ltx_bibblock">
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia. 2018a.

</span>
<span class="ltx_bibblock">Iterative Learning with Open-set Noisy Labels. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024.

</span>
<span class="ltx_bibblock">CRAG – Comprehensive RAG Benchmark.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2406.04744 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.04744" title="">https://arxiv.org/abs/2406.04744</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep  5 01:56:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
