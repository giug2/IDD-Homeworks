<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</title>
<!--Generated on Tue Oct  8 15:08:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Autonomous Agents,  Autonomous Vehicle Navigation,  AI-Enabled Robotics
" lang="en" name="keywords"/>
<base href="/html/2409.18313v4/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S1" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S2" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Task: Embodied-RAG Benchmark</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3.SS0.SSS0.Px1" title="In III Related Works ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title">Nonparametric Methods Outside the Embodied Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3.SS0.SSS0.Px2" title="In III Related Works ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title">Parametric Use of Foundation Models in the Embodied Domain</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3.SS0.SSS0.Px3" title="In III Related Works ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title">Existing Methods of Semantic Memory and Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3.SS0.SSS0.Px4" title="In III Related Works ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title">Semantic Navigation and Question Answering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Method: Embodied Retrieval and Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.SS1" title="In IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Memory Construction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.SS2" title="In IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Retrieval</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.SS3" title="In IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Generation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S5" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S5.SS0.SSS0.Px1" title="In V Experiments ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.SS1" title="In VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Quantitative Result</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.SS2" title="In VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Qualitative Result</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.SS3" title="In VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Computational Efficiency</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.SS4" title="In VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Ablation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S7" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Limitations and Future work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S8" title="In Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Quanting Xie<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">1,∗</span></sup>, So Yeon Min<sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">1,∗</span></sup>, Tianyi Zhang<sup class="ltx_sup" id="id12.12.id3"><span class="ltx_text ltx_font_italic" id="id12.12.id3.1">1</span></sup>, Kedi Xu<sup class="ltx_sup" id="id13.13.id4"><span class="ltx_text ltx_font_italic" id="id13.13.id4.1">1</span></sup>, Aarav Bajaj<sup class="ltx_sup" id="id14.14.id5"><span class="ltx_text ltx_font_italic" id="id14.14.id5.1">1</span></sup>, Ruslan Salakhutdinov<sup class="ltx_sup" id="id15.15.id6"><span class="ltx_text ltx_font_italic" id="id15.15.id6.1">1</span></sup>, Matthew Johnson-Roberson<sup class="ltx_sup" id="id16.16.id7"><span class="ltx_text ltx_font_italic" id="id16.16.id7.1">1</span></sup>, and Yonatan Bisk<sup class="ltx_sup" id="id17.17.id8"><span class="ltx_text ltx_font_italic" id="id17.17.id8.1">1</span></sup>
</span><span class="ltx_author_notes">This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112490375 and partially supported by funding from Lockheed Martin Corporation.<sup class="ltx_sup" id="id18.18.id1"><span class="ltx_text ltx_font_italic" id="id18.18.id1.1">1</span></sup>Quanting Xie, So Yeon Min, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, and Yonatan Bisk are with Carnegie Mellon University, Pittsburgh, PA 15213, USA
<span class="ltx_text ltx_font_typewriter" id="id19.19.id2" style="font-size:80%;">quantinx@andrew.cmu.edu, soyeonm@andrew.cmu.edu</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id20.id1">There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction.</p>
<p class="ltx_p" id="id21.id2">To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance.
At its core, Embodied-RAG’s memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Autonomous Agents, Autonomous Vehicle Navigation, AI-Enabled Robotics

</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S0.F1.g1" src="x1.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S0.F1.2.1">Overview</span>: Our goal is for robots to navigate and communicate effectively in any environment where humans are present. We introduce Embodied-RAG, a framework for automatically building hierarchical spatial memory and providing both explanations and navigation across multiple levels of query abstraction. Embodied-RAG supports robotic operations regardless of the query’s abstraction level, the platform, or the environment.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark"></sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark"></sup><span class="ltx_tag ltx_tag_note"></span><sup class="ltx_sup" id="footnote1.1">*</sup> Equal contribution.</span></span></span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><a class="ltx_ref ltx_href" href="https://quanting-xie.github.io/Embodied-RAG-web/" title="">https://quanting-xie.github.io/Embodied-RAG-web/</a></span></span></span>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Humans excel as generalist embodied agents in part due to our ability to build, abstract, and reason over rich memories. We seamlessly log experiences at appropriate levels of detail and retrieve information ranging from specific facts to holistic impressions, allowing us to respond to diverse requests across different contexts. In contrast, current embodied agents<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib4" title="">4</a>]</cite> lack such versatile memory capabilities, limiting their ability to operate effectively in unbounded and complex real-world environments. While existing methods such as semantic mapping<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib2" title="">2</a>]</cite> and scene graphs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib6" title="">6</a>]</cite> attempt to capture spatial and contextual relationships, they largely fall short of the dynamic and flexible memory, retrieval, and generative abilities exhibited by humans.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the language domain, foundation models combined with non-parametric memory mechanisms have achieved near human-level performance across various tasks. Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib9" title="">9</a>]</cite> has been widely adopted in the field of Natural Language Processing (NLP) as a non-parametric memory mechanism over large document corpora, enhancing the accuracy and relevance of responses generated by Large Language Models (LLMs). Similarly, the continuous stream of experiences gathered by embodied agents forms vast databases that exceed the context window limitations of LLMs. To address this, approaches like RAG are essential for enabling human-like embodied agents to operate effectively in large, dynamic environments. By integrating non-parametric memory, foundation models within robots can store and retrieve a diverse range of experiences, enhancing their ability to navigate and respond in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, applying RAG to embodied scenarios presents unique challenges due to key differences between textual data and embodied experiences. First, while RAG relies on existing documents, building memory from embodied experiences is itself a core research challenge. Current methods, such as dense point clouds or scene graphs, fail to capture the full range of experiences beyond object-level attributes, without relying on human-engineered schemas or exceeding memory budgets. Second, unlike documents, embodied experiences have inherent correlated structure — semantically similar objects are often spatially correlated and hierarchically organized so embodied experiences should not be treated as independent samples. Finally, embodied observations vary in granularity and structure: outdoor scenes might be sparse, while indoor environments are cluttered, and repeated objects across frames can confuse LLMs, complicating retrieval.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To bridge this gap, we present Embodied-RAG. Embodied-RAG has two components, <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Memory Construction</span> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a>(a)) and <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">Retrieval and Generation</span> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a>(b c)). During <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">Memory Construction</span>, the system autonomously builds a topological map for low-level navigation and a hierarchical <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">semantic forest</span> without relying on hand-crafted constraints or features. This forest is organized based on spatial correlations between hierarchical nodes, each containing language descriptions of observations, and can be expanded to handle temporal or multi-modal inputs. Root nodes represent global explanations, leaf nodes capture specific object arrangements, and intermediate nodes reflect various mid-level scales. Embodied-RAG allows retrieval at various levels of <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">abstraction</span> in the language query (explicit, implicit, global), matching it with the <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">spatial/semantic</span> resolution (local, intermediate, global) of the memory (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a>(b)/(c), Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F3" title="Figure 3 ‣ IV-B Retrieval ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">3</span></a>). In the <span class="ltx_text ltx_font_italic" id="S1.p4.1.7">Retrieval and Generation</span> process, to mitigate perceptual hallucinations from semantic similarity searches, Embodied-RAG incorporates a robust reasoning component. This involves parallelized tree traversals scored by a language model, with retrieved results structured and used as context for generating explanations or navigational actions via an LLM.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To evaluate the performance of Embodied-RAG, we developed an Embodied-RAG Benchmark, which consists of queries that require multimodal outputs (navigational waypoints and text responses) and reasoning (implicit questions and global summaries). Across over 200 benchmark tasks, we compared Embodied-RAG with two other non-parametric memory baselines: Semantic Match and vanilla RAG. We found that our method serves as an initial step toward solving the problems mentioned above in applying non-parametric memory to embodied agents, showing superior performance against these baselines on the Embodied-RAG Benchmark in the following aspects: (1) More robust against object detection errors on explicit queries (direct object retrieval) since it leverages hierarchical spatial relevancy—for example, recognizing that a toothbrush is more likely found in a bathroom; (2) Improved reasoning on implicit queries (indirect object retrieval), achieving a 220% improvement over Semantic Match and a 30% relative improvement over RAG; (3) Generating more accurate global summarization and trend analysis within the environment, where Semantic Match is unsupported and RAG shows poor quality.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Furthermore, our experiments demonstrate that this pipeline is versatile and applicable across various practical forms of embodiment (drones, locobots, quadrupeds) and can be seamlessly integrate with existing low-level autonomous navigation pipelines. This highlights Embodied-RAG’s potential as a general system capable of task-, environment-, and platform-agnostic operation, enabling robots to effectively navigate and communicate in any environment where humans are present.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The key contributions and implications of this paper include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Method</span> We introduce the system of Embodied-RAG. This method addresses problems of naively apply non-parametric memories like RAG to embodied setting.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Task</span> We introduce the general task of <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.2">Embodied-RAG benchmark</span>, formulating semantic navigation and question answering under a single paradigm (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S3.T1" title="TABLE I ‣ III Related Works ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">I</span></a>, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S0.F1" title="Figure 1 ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Implications</span> Our results and discussion provide a basis for rethinking approaches to generalist robot agents based on non-parameteric memories.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Task: Embodied-RAG Benchmark</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The Embodied-RAG benchmark contains queries from the cross-product of {explicit, implicit, global} questions with potential {navigational action, language} generation outputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A task consists of:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Query</span>: The content can be explicit (e.g. a particular object instance), implicit (e.g. looking for adequacy, instruction with more pragmatic understanding required), or global. The request might pertain to a location or general vibe.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Experience</span>: The experience is a sequence of egocentric visual perception and odometry, occurring in indoor, outdoor, or mixed environments.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Output</span>: The expected output can be both navigation actions with language descriptions (Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a> top, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a> c-1), or language explanations (Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a> bottom, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a> c-2).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Example tasks are shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, with instances of explicit, implicit, and global queries in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S0.F1" title="Figure 1 ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">1</span></a>. Spatially, the queries range from specific regions small enough to contain certain objects to global regions encompassing the entire scene. Linguistically, global queries are closer to retrieval-augmented generation tasks, while explicit/implicit ones are more retrieval-focused. <span class="ltx_text ltx_font_italic" id="S2.p3.1.1">Explicit</span> and <span class="ltx_text ltx_font_italic" id="S2.p3.1.2">implicit</span> queries are <span class="ltx_text ltx_font_italic" id="S2.p3.1.3">navigational</span> tasks that expect navigation actions and text descriptions of the retrieved location. <span class="ltx_text ltx_font_italic" id="S2.p3.1.4">Global</span> queries are <span class="ltx_text ltx_font_italic" id="S2.p3.1.5">explanation</span> tasks requiring text generation at a more holistic level; there are no global navigation tasks since they pertain to large areas, sometimes the entire environment. who carefully went through the simulated or real environment.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Queries were collected by four human annotators with teleoperated robots through two real outdoor/mixed environments, three real indoor environments, and fourteen simulated environments. These diverse environments include a residential neighborhood, a deserted theme park, and a college campus. The environments convey different atmospheres through the activities of people or remnants of their activities—such as people lined up, chatting, or jackets left in office spaces. Navigation was performed by quadruped, drone, and locobot.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Related Works</span>
</h2>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span><span class="ltx_text" id="S3.T1.2.1" style="font-size:90%;">Comparison of related tasks and datasets.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.3">
<tr class="ltx_tr" id="S3.T1.3.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.3.1.1"></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.3.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.3.1">Scope of Query</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.3.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.4.1">Output Format</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.3.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.3.1.5.1">Experience</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T1.3.2.1">Task</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.2.2">Dataset</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.2.3">Explicit</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.4">Implicit</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.5">Global</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.6">Navigational</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.7">Free-form</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.8">Indoor</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.2.9">Outdoor</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.1" rowspan="3"><span class="ltx_text" id="S3.T1.3.3.1.1"><span class="ltx_text" id="S3.T1.3.3.1.1.1"></span> <span class="ltx_text" id="S3.T1.3.3.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.3.3.1.1.2.1">
<span class="ltx_tr" id="S3.T1.3.3.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.1.1.2.1.1.1.1">Semantic</span></span></span>
<span class="ltx_tr" id="S3.T1.3.3.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.3.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.1.1.2.1.2.1.1">Navigation</span></span></span>
</span></span> <span class="ltx_text" id="S3.T1.3.3.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.3.2">ObjectNav <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.3.3"><span class="ltx_text" id="S3.T1.3.3.3.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.5">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.6"><span class="ltx_text" id="S3.T1.3.3.6.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.7">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.8"><span class="ltx_text" id="S3.T1.3.3.8.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.9">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.4">
<td class="ltx_td ltx_align_left" id="S3.T1.3.4.1">ImageNav <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.4.2"><span class="ltx_text" id="S3.T1.3.4.2.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.3">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.4">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.5"><span class="ltx_text" id="S3.T1.3.4.5.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.6">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.7"><span class="ltx_text" id="S3.T1.3.4.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.4.8">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5">
<td class="ltx_td ltx_align_left" id="S3.T1.3.5.1">VLN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.5.2"><span class="ltx_text" id="S3.T1.3.5.2.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.3">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.4">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.5"><span class="ltx_text" id="S3.T1.3.5.5.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.6">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.7"><span class="ltx_text" id="S3.T1.3.5.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.5.8">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.1" rowspan="3"><span class="ltx_text" id="S3.T1.3.6.1.1"><span class="ltx_text" id="S3.T1.3.6.1.1.1"></span> <span class="ltx_text" id="S3.T1.3.6.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.3.6.1.1.2.1">
<span class="ltx_tr" id="S3.T1.3.6.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.6.1.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.1.1.2.1.1.1.1">Embodied</span></span></span>
<span class="ltx_tr" id="S3.T1.3.6.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.3.6.1.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.1.1.2.1.2.1.1">QA</span></span></span>
</span></span> <span class="ltx_text" id="S3.T1.3.6.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.6.2">OpenEQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.6.3"><span class="ltx_text" id="S3.T1.3.6.3.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.5">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.6">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.7"><span class="ltx_text" id="S3.T1.3.6.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.8"><span class="ltx_text" id="S3.T1.3.6.8.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.6.9">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.7">
<td class="ltx_td ltx_align_left" id="S3.T1.3.7.1">EQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.7.2"><span class="ltx_text" id="S3.T1.3.7.2.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.3">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.4">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.5">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.6"><span class="ltx_text" id="S3.T1.3.7.6.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.7"><span class="ltx_text" id="S3.T1.3.7.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.7.8">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.8">
<td class="ltx_td ltx_align_left" id="S3.T1.3.8.1">Excalibur <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.8.2"><span class="ltx_text" id="S3.T1.3.8.2.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.3">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.4">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.5">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.6"><span class="ltx_text" id="S3.T1.3.8.6.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.7"><span class="ltx_text" id="S3.T1.3.8.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.8.8">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T1.3.9.1.1">VideoQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.9.2">VideoQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.9.3"><span class="ltx_text" id="S3.T1.3.9.3.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.5">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.6">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.7">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.8"><span class="ltx_text" id="S3.T1.3.9.8.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.9.9"><span class="ltx_text" id="S3.T1.3.9.9.1" style="color:#519D45;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.10">
<td class="ltx_td ltx_align_left" id="S3.T1.3.10.1">OVQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.3.10.2"><span class="ltx_text" id="S3.T1.3.10.2.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.3">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.4">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.5">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.6">✗</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.7"><span class="ltx_text" id="S3.T1.3.10.7.1" style="color:#519D45;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.10.8"><span class="ltx_text" id="S3.T1.3.10.8.1" style="color:#519D45;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.11" style="background-color:#8FED8F;">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="2" id="S3.T1.3.11.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.11.1.1" style="background-color:#8FED8F;">Embodied-RAG</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T1.3.11.2"><span class="ltx_text" id="S3.T1.3.11.2.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.3"><span class="ltx_text" id="S3.T1.3.11.3.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.4"><span class="ltx_text" id="S3.T1.3.11.4.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.5"><span class="ltx_text" id="S3.T1.3.11.5.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.6"><span class="ltx_text" id="S3.T1.3.11.6.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.7"><span class="ltx_text" id="S3.T1.3.11.7.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.3.11.8"><span class="ltx_text" id="S3.T1.3.11.8.1" style="color:#519D45;background-color:#8FED8F;">✓</span></td>
</tr>
</table>
</figure>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Nonparametric Methods Outside the Embodied Domain</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">In the text and multimodal domain, RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib9" title="">9</a>]</cite> augments LLMs by incorporating a retrieval component from a vector database, enhancing the accuracy and relevance of generated content. However, these methods assume a pre-existing memory and focus on retrieval and generation rather than the construction of the memory. Active agents operating in space require a dynamic approach where memory construction and retrieval/generation are coupled and simultaneous. Furthermore, embodiment introduces the challenge of connecting spatial resolution with language abstraction. GraphRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib20" title="">20</a>]</cite> addresses the level of abstraction between language queries and document scope. In contrast, our method, Embodied-RAG, extends this resolution problem to the multimodal and spatial domains, simultaneously tackling all of memory construction, retrieval, and generation of language and navigational actions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Parametric Use of Foundation Models in the Embodied Domain</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">Current approaches in embodied AI often rely on the <span class="ltx_text ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.1">parametric </span>use of foundation models to perceive environments and plan <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib23" title="">23</a>]</cite>. Systems like PIVOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib24" title="">24</a>]</cite> and NavGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib4" title="">4</a>]</cite> employ large language models (LLMs) in a Markovian manner, where decisions are made based on the current state without incorporating external memory or past experiences. This reliance on pretrained LLMs or vision-language models (VLMs) can lead to hallucinations, as these models depend on internal knowledge for action generation, often misinterpreting the world state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib25" title="">25</a>]</cite>. Furthermore, agents may suffer from extrinsic visual hallucinations, which negatively affect decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib26" title="">26</a>]</cite>. In the textual and multimodal domain, it is known that RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib27" title="">27</a>]</cite> mitigates hallucinations. In our work, we show that Embodied-RAG reduces hallucinations with the use of nonparametric components. While embodied foundation models such as RT-x models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib28" title="">28</a>]</cite> and OpenVLA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib29" title="">29</a>]</cite> have been introduced, their nonparametric components have not been explored.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Existing Methods of Semantic Memory and Retrieval</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.3">Several methods have been proposed for storing and querying semantic memory in spatial environments, but they remain limited and task-specific compared to the potential of foundation models. Approaches like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib2" title="">2</a>]</cite> associate voxels with predefined object categories, enabling fixed vocabulary retrieval, while methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib33" title="">33</a>]</cite> map voxels to image embeddings, allowing for open vocabulary queries. Systems like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib34" title="">34</a>]</cite> store images per voxel, supporting queries about people, language/image inputs, and object categories. However, a common challenge across these approaches is aligning the semantic abstraction with the spatial resolution. Queries such as “cup,” “red cup,” or “I want to heat my lunch” are object-level, but methods like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib36" title="">36</a>]</cite> focus primarily on local retrieval during exploration, using structured frontiers based on object layouts. Scene graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib5" title="">5</a>]</cite>, while free from dense memory issues, rely on human-engineered schemas (e.g. floor <math alttext="\xrightarrow{}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px3.p1.1.m1.1a"><mover accent="true" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mo id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2" stretchy="false" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">→</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml"></mi></mover><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1">absent</csymbol><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2">→</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.1.m1.1c">\xrightarrow{}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p1.1.m1.1d">start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW</annotation></semantics></math> room <math alttext="\xrightarrow{}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS0.SSS0.Px3.p1.2.m2.1a"><mover accent="true" id="S3.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mo id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2" stretchy="false" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">→</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml"></mi></mover><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1">absent</csymbol><ci id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.2">→</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.2.m2.1c">\xrightarrow{}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p1.2.m2.1d">start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW</annotation></semantics></math> object <math alttext="\xrightarrow{}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS0.SSS0.Px3.p1.3.m3.1a"><mover accent="true" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mo id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2" stretchy="false" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">→</mo><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml"></mi></mover><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1">absent</csymbol><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2">→</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.3.m3.1c">\xrightarrow{}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p1.3.m3.1d">start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW</annotation></semantics></math> asset), making them unsuitable for novel or outdoor environments.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.1">Other approaches, such as OCTREE maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib37" title="">37</a>]</cite> and their semantic versions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib40" title="">40</a>]</cite>, organize occupancy data efficiently but still limit semantics to the object level. Methods like Semantic OCTREE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib40" title="">40</a>]</cite> and GENMos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib39" title="">39</a>]</cite> use fixed object categories, lacking support for free-form language queries or varying levels of spatial and semantic resolution needed for holistic understanding.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Semantic Navigation and Question Answering</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">Tasks like ObjectNav <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib10" title="">10</a>]</cite>, ImageNav <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib3" title="">3</a>]</cite>, and Visual Language Navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib13" title="">13</a>]</cite> assess a robot’s ability to navigate towards semantic targets based on object categories, images, or language descriptions. While recent efforts like GOATBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib1" title="">1</a>]</cite> combine multiple input types, these tasks still focus on object-level queries and lack the flexibility to handle broader, more abstract user requests. Embodied Question Answering (EQA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib44" title="">44</a>]</cite> and Video Question Answering (VideoQA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib48" title="">48</a>]</cite> extend navigation by requiring text-based answers within actionable or video environments, though EQA is limited to indoor settings and VideoQA lacks active navigation. Our approach expands these paradigms by integrating action-based and question-answering capabilities across a wider range of environments and user queries.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Method: Embodied Retrieval and Generation</span>
</h2>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="427" id="S4.F2.g1" src="extracted/5910947/images/Figure2_method-2.png" width="933"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S4.F2.2.1">Embodied-RAG method overview</span>. (a) Memory is constructed by hierarchically organizing the nodes of the topological map into a semantic forest. (b) The memory in (a) can be retrieved for a query, with parallelized tree traversals. (c) Navigation actions with text outputs, or global explanations can be generated for the query, with using the retrieval results as LLM contexts.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Memory Construction</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The memory construction process of Embodied-RAG consists of two parts: a topological map and a semantic forest. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Topological map</span>
We employ a topological graph composed of nodes with the following attributes:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.2">Position Information: The allocentric coordinates <math alttext="(x,y,z)" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.m1.3"><semantics id="S4.I1.i1.p1.1.m1.3a"><mrow id="S4.I1.i1.p1.1.m1.3.4.2" xref="S4.I1.i1.p1.1.m1.3.4.1.cmml"><mo id="S4.I1.i1.p1.1.m1.3.4.2.1" stretchy="false" xref="S4.I1.i1.p1.1.m1.3.4.1.cmml">(</mo><mi id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">x</mi><mo id="S4.I1.i1.p1.1.m1.3.4.2.2" xref="S4.I1.i1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.2.2" xref="S4.I1.i1.p1.1.m1.2.2.cmml">y</mi><mo id="S4.I1.i1.p1.1.m1.3.4.2.3" xref="S4.I1.i1.p1.1.m1.3.4.1.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.3.3" xref="S4.I1.i1.p1.1.m1.3.3.cmml">z</mi><mo id="S4.I1.i1.p1.1.m1.3.4.2.4" stretchy="false" xref="S4.I1.i1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.3b"><vector id="S4.I1.i1.p1.1.m1.3.4.1.cmml" xref="S4.I1.i1.p1.1.m1.3.4.2"><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝑥</ci><ci id="S4.I1.i1.p1.1.m1.2.2.cmml" xref="S4.I1.i1.p1.1.m1.2.2">𝑦</ci><ci id="S4.I1.i1.p1.1.m1.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3">𝑧</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.3c">(x,y,z)</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.m1.3d">( italic_x , italic_y , italic_z )</annotation></semantics></math> and the yaw angle <math alttext="\theta" class="ltx_Math" display="inline" id="S4.I1.i1.p1.2.m2.1"><semantics id="S4.I1.i1.p1.2.m2.1a"><mi id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.1b"><ci id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.2.m2.1d">italic_θ</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Image Path: Each node contains a path to an associated ego-centric image.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Captions: Generated by a vision-language model, these captions provide object-level natural language textual descriptions of the image.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The nodes form a topological map (blue nodes in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a>), eliminating the need for specific control parameters like velocity and yaw, which often vary across different drive systems. This abstraction enables compatibility with any local planner, regardless of the robot’s embodiment. Furthermore, the topological structure is far more memory-efficient than traditional metric maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib30" title="">30</a>]</cite>, allowing for efficient scaling in both large outdoor and complex indoor environments. Our experiments show that this approach successfully navigates kilometer-scale simulated environments.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Semantic Forest</span> We use a separate tree structure, referred to as a semantic forest, to capture meaning at various spatial resolutions. The nodes of this forest are those of the topological map, with the non-leaf nodes capturing larger spaces at a thinner density of semantic specificity. First, we create the forest through hierarchical clustering. Since spatially approximate leaf nodes (blue nodes in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F3" title="Figure 3 ‣ IV-B Retrieval ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">3</span></a>(a)) exhibit semantic correlations, we employ an agglomerative clustering mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib49" title="">49</a>]</cite> to group nodes based on their physical positions assigning the mean position of the leaves.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">This iterative process continues until a root node is formed, stopping when no further relevance is found based on a threshold set by the algorithm. Once we have a complete forest with one or more root nodes, each non-leaf node receives a language description. We achieve this by prompting a large language model (LLM, e.g., GPT-4) to generate a abstraction that encompasses the descriptions of its direct child nodes (see website for the prompting). This process is conducted bottom-up, starting from the leaf nodes and moving up to the parent nodes. We parallelized this process across all nodes at the same hierarchical level.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Retrieval</span>
</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F3.g1" src="extracted/5910947/images/Figure4_retrieval.png" width="974"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>We illustrate three retrieval methods: (a) Semantic Match, (b) Retrieval-Augmented Generation (RAG), and (c) our proposed method, Embodied-RAG. Semantic Match retrieves the node in the topological map with the highest cosine similarity with the query, while RAG outputs top <math alttext="k" class="ltx_Math" display="inline" id="S4.F3.3.m1.1"><semantics id="S4.F3.3.m1.1b"><mi id="S4.F3.3.m1.1.1" xref="S4.F3.3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F3.3.m1.1c"><ci id="S4.F3.3.m1.1.1.cmml" xref="S4.F3.3.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F3.3.m1.1e">italic_k</annotation></semantics></math> nodes. In contrast, Embodied-RAG retrieves the best <math alttext="k" class="ltx_Math" display="inline" id="S4.F3.4.m2.1"><semantics id="S4.F3.4.m2.1b"><mi id="S4.F3.4.m2.1.1" xref="S4.F3.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F3.4.m2.1c"><ci id="S4.F3.4.m2.1.1.cmml" xref="S4.F3.4.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F3.4.m2.1e">italic_k</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S4.F3.6.1">chains</span> of the semantic forest.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">To address perception hallucinations and improve reasoning capabilities over hierarchies of abstraction constructed for a given environment, we modified RAG’s relevancy scoring mechanism from semantic similarity to LLM selections at each level, following a strategy similar to Tree-of-Thoughts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib50" title="">50</a>]</cite>. The input to this retrieval process consists of <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_N</annotation></semantics></math> semantic trees, and the output is the top <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_k</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.2.1">chains</span>, which represent node paths from selected leaf nodes to the root (e.g., the concatenation of green, yellow, and red nodes in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a>(c)).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.7">We run the following <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.7.1">process</span>, which takes a single tree as input and outputs a single leaf node. Starting by visiting the root node, we run BFS with LLM selection; we ask <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.7.2">LLM_Selector</span> to choose the best child node of the currently visited node based on compatibility with the given query. For example, if the query is “find me a place that is bright and quiet but has some presence of people,” we prompt the LLM to select the best description among the children of the currently visited node. We then visit the selected best child node and iterate this process until we reach a leaf node. Once we obtain <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math> leaf nodes (<math alttext="\frac{k}{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mfrac id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">k</mi><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">N</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><divide id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"></divide><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑘</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\frac{k}{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">divide start_ARG italic_k end_ARG start_ARG italic_N end_ARG</annotation></semantics></math> nodes from each tree) by running this process <math alttext="\frac{k}{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mfrac id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">k</mi><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">N</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><divide id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"></divide><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑘</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\frac{k}{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">divide start_ARG italic_k end_ARG start_ARG italic_N end_ARG</annotation></semantics></math> times for each of the <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">italic_N</annotation></semantics></math> trees, we obtain the “chain” from the selected node to the root node. The <math alttext="\frac{k}{N}" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.1"><semantics id="S4.SS2.p2.5.m5.1a"><mfrac id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">k</mi><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">N</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><divide id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"></divide><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">𝑘</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\frac{k}{N}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">divide start_ARG italic_k end_ARG start_ARG italic_N end_ARG</annotation></semantics></math> processes are parallelized across the <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.1"><semantics id="S4.SS2.p2.6.m6.1a"><mi id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><ci id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.6.m6.1d">italic_N</annotation></semantics></math> trees. The set of these best <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.1"><semantics id="S4.SS2.p2.7.m7.1a"><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.7.m7.1d">italic_k</annotation></semantics></math> chains is the retrieval output, containing semantics at all scales for any specific location that corresponds to the leaf scale. Embodied-RAG unifies the retrieval process to handle explicit, implicit, and global queries, producing both explanations and navigational actions as outputs. Note, these hierarchies and corresponding trees allow for querying automatically created semantic regions, something particularly useful for outdoor navigation where walls and structures cannot be used to determine function.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Generation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">We pass the retrieved <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> best chains as part of a context, for the LLM to generate navigation and text description (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a> top) or global explanations (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a> bottom). Given the query and the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_k</annotation></semantics></math> chains, we prompt the LLM to “select” a waypoint with a reasoning, or to “explain” (prompt in our project website).</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Navigation </span> We select a waypoint (a leaf node of the semantic forest) and use a planner to generate navigational actions—sequences of (torque, velocity) pairs— to reach the waypoint. To select this waypoint, we ask the LLM to choose the best single leaf node, togehter with textual reasoning, using the query and the chain as input. Again, including the entire chain as input ensures that a waypoint can be generated for implicit navigation tasks as well.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.2.1">Text Answers</span> As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F2" title="Figure 2 ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">2</span></a> (c), we concatenate the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mi id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">italic_k</annotation></semantics></math> chains as part of the prompt to the LLM. We ask it to generate an answer to the query based on the <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">italic_k</annotation></semantics></math> retrieved chains. The spatial scale of attention in each node of the chain facilitate the LLM to generate responses at any semantic scale (explicit, implicit, general) based on the retrieved result.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Task</span> To assess the efficacy of our approach and ensure statistical robustness, we collected data across 19 diverse environments, including both indoor and outdoor settings. These environments span simulated settings (AirSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib51" title="">51</a>]</cite>, Habitat Matterport <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib52" title="">52</a>]</cite>) and real-world locations, comprising 7 small and 12 large environments. The dataset contains 250 distinct queries, categorized by their nature and complexity.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Embodiment</span> For the real-world robotic configuration, we utilized a Unitree Go2, equipped with three Realsense cameras to capture a 180-degree field of view. Positional data was acquired using the Go2’s integrated lidar and SLAM algorithms. For simulations, we use the default drone setup with a 210-degree panoramic view for and APIs for drone manipulation and positional data acquisition for AirSim. For Habitat, we use the default locobot setup. To construct the experience, human annotators teleoperated and mapped each environment. However, our methodology is adaptable to any frontier-based exploration with minimal modifications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib35" title="">35</a>]</cite>, since the problem distills into “retrieving” a frontier (potential leaf nodes of the topological map) under Embodied-RAG.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Evaluation</span> Before evaluation, users familiarized themselves with the collected dataset to understand the environment. The four human annotators who generated the queries cross-evaluated, with each query receiving three evaluations, excluding the one from its author. For navigation output, participants chose binary success or fail, and we calculated the Success Rate (SR) from the average across evaluators and tasks. For text output, participants rated the relevance and correctness of the response on a Likert scale of 1 to 5.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.2">To establish the comparative performance of our Embodied-RAG approach, we benchmarked it against two baseline methodologies. The first baseline, <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.2.1">Semantic Match</span>, follows existing methods by computing cosine similarities between the query and the semantic embeddings of captions from nodes in the topological map, which are also leaf nodes of the semantic forest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib56" title="">56</a>]</cite>. The second baseline, we employs an naive <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px1.p1.2.2">RAG</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib7" title="">7</a>]</cite> in embodied setting, identifying the <math alttext="k=10" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑘</ci><cn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">k=10</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">italic_k = 10</annotation></semantics></math> semantically closest nodes in the topological map and using their scene captions to augment a LLM’s prompt, enhancing retrieval accuracy. We use the same
<math alttext="k=10" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1"><eq id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1"></eq><ci id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2">𝑘</ci><cn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">k=10</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.2.m2.1d">italic_k = 10</annotation></semantics></math> for the retrieval of chains in Embodied-RAG.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Results</span>
</h2>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of Methods on different Embodied-RAG Benchmarks. Explicit and Implicit queries are evaluated using Success Rate (SR), while Global queries use a Likert Scale of 1 to 5. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T2.3">
<tr class="ltx_tr" id="S6.T2.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.1" style="font-size:80%;">Env.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.3.1.2"><span class="ltx_text" id="S6.T2.3.1.2.1" style="font-size:80%;">Explicit</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.3.1.3"><span class="ltx_text" id="S6.T2.3.1.3.1" style="font-size:80%;">Implicit</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.3.1.4"><span class="ltx_text" id="S6.T2.3.1.4.1" style="font-size:80%;">Global</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.1"><span class="ltx_text" id="S6.T2.3.2.1.1" style="font-size:80%;">Embodied-RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.2"><span class="ltx_text" id="S6.T2.3.2.2.1" style="font-size:80%;">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.3"><span class="ltx_text" id="S6.T2.3.2.3.1" style="font-size:80%;">Sem.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.4"><span class="ltx_text" id="S6.T2.3.2.4.1" style="font-size:80%;">Embodied-RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.5"><span class="ltx_text" id="S6.T2.3.2.5.1" style="font-size:80%;">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.6"><span class="ltx_text" id="S6.T2.3.2.6.1" style="font-size:80%;">Sem.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.7"><span class="ltx_text" id="S6.T2.3.2.7.1" style="font-size:80%;">Embodied-RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.8"><span class="ltx_text" id="S6.T2.3.2.8.1" style="font-size:80%;">RAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.2.9"><span class="ltx_text" id="S6.T2.3.2.9.1" style="font-size:80%;">Sem.</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.3.3.1"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.1.1" style="font-size:80%;">Small</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.2"><span class="ltx_text" id="S6.T2.3.3.2.1" style="font-size:80%;">0.955</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.3"><span class="ltx_text" id="S6.T2.3.3.3.1" style="font-size:80%;">0.955</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4"><span class="ltx_text" id="S6.T2.3.3.4.1" style="font-size:80%;">0.955</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.5"><span class="ltx_text" id="S6.T2.3.3.5.1" style="font-size:80%;">1.000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.6"><span class="ltx_text" id="S6.T2.3.3.6.1" style="font-size:80%;">0.818</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.7"><span class="ltx_text" id="S6.T2.3.3.7.1" style="font-size:80%;">0.364</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.8"><span class="ltx_text" id="S6.T2.3.3.8.1" style="font-size:80%;">4.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.9"><span class="ltx_text" id="S6.T2.3.3.9.1" style="font-size:80%;">3.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.10"><span class="ltx_text" id="S6.T2.3.3.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.4">
<td class="ltx_td ltx_align_left" id="S6.T2.3.4.1"><span class="ltx_text ltx_font_bold" id="S6.T2.3.4.1.1" style="font-size:80%;">Large</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.2"><span class="ltx_text" id="S6.T2.3.4.2.1" style="font-size:80%;">0.977</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3"><span class="ltx_text" id="S6.T2.3.4.3.1" style="font-size:80%;">0.947</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.4"><span class="ltx_text" id="S6.T2.3.4.4.1" style="font-size:80%;">0.895</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.5"><span class="ltx_text" id="S6.T2.3.4.5.1" style="font-size:80%;">0.914</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.6"><span class="ltx_text" id="S6.T2.3.4.6.1" style="font-size:80%;">0.695</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.7"><span class="ltx_text" id="S6.T2.3.4.7.1" style="font-size:80%;">0.426</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.8"><span class="ltx_text" id="S6.T2.3.4.8.1" style="font-size:80%;">4.86</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.9"><span class="ltx_text" id="S6.T2.3.4.9.1" style="font-size:80%;">2.43</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.10"><span class="ltx_text" id="S6.T2.3.4.10.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S6.T2.3.5.1"><span class="ltx_text ltx_font_bold" id="S6.T2.3.5.1.1" style="font-size:80%;">Total</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.2"><span class="ltx_text ltx_font_bold" id="S6.T2.3.5.2.1" style="font-size:80%;">0.969</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.3"><span class="ltx_text" id="S6.T2.3.5.3.1" style="font-size:80%;">0.949</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.4"><span class="ltx_text" id="S6.T2.3.5.4.1" style="font-size:80%;">0.877</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.5"><span class="ltx_text ltx_font_bold" id="S6.T2.3.5.5.1" style="font-size:80%;">0.926</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.6"><span class="ltx_text" id="S6.T2.3.5.6.1" style="font-size:80%;">0.706</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.7"><span class="ltx_text" id="S6.T2.3.5.7.1" style="font-size:80%;">0.410</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.8"><span class="ltx_text ltx_font_bold" id="S6.T2.3.5.8.1" style="font-size:80%;">4.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.9"><span class="ltx_text" id="S6.T2.3.5.9.1" style="font-size:80%;">2.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S6.T2.3.5.10"><span class="ltx_text" id="S6.T2.3.5.10.1" style="font-size:80%;">-</span></td>
</tr>
</table>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Quantitative Result</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We first present <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.1">quantitative</span> results that demonstrate the effectiveness of our approach in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.T2" title="TABLE II ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">II</span></a>. As outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S2" title="II Task: Embodied-RAG Benchmark ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">II</span></a>, we categorize the Embodied-RAG benchmark queries into three major types: explicit retrieval, implicit retrieval, and global retrieval. Additionally, we classify environments as either small or large based on the number of topological nodes mapped. Our results indicate that Embodied-RAG consistently outperforms RAG and Semantic Match across all tasks and environments. Crucially, all approaches yield expected strong results for explicit queries where a single node is being extracted. RAG’s multi-hypothesis approach outperforms Semantic Similarity, and the hierarchy of Embodied-RAG provides a small further boost. The story changes dramatically as we move to implicit queries where the lack of structure causes RAG and and Semantic search performance to drop dramatically, while Embodied-RAG maintains robustness even in large environments. A similar result is seen in the likert scale for Global questions. Note, Semantic Match cannot be applied for Global as it lacks summarizing and reasoning.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Qualitative Result</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We further conduct a <span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1">qualitative</span> comparison on the reasoning generated by Embodied-RAG and the baseline models before they select a retrieval goal. Embodied-RAG consistently demonstrated superior reasoning, especially in handling implicit requests and global queries. This is likely because relying solely on retrieving semantically similar objects, as the baselines do, is insufficient for addressing queries that require global context and understanding relationships between different parts of the environment (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="S6.F4.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example reasoning of Embodied-RAG and RAG for generation tasks are highlighted in blue and pink boxes, respectively.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">Implicit Query: Find where I can buy some drinks?</span> From the figure, we see that Embodied-RAG correctly identifies a food service area, while the baselines provide incorrect answers. For RAG and direct semantic match, the most relevant results retrieved are those with direct semantic associations, such as a refrigerator or water fountain. However, there is a clear mismatch between the user’s intention and the retrieved objects. The goal is to ‘buy’ water, which typically requires a counter or vending machine for the transaction, rather than simply grabbing it from a refrigerator or drinking from a water fountain. Embodied-RAG performs multi-step reasoning from the top of the tree to the bottom, and retrieves more diverse and plausible locations. It successfully identifies counters as the most appropriate locations for the user’s intention.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_italic" id="S6.SS2.p3.1.1">Global Query:</span>
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F4" title="Figure 4 ‣ VI-B Qualitative Result ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, Embodied-RAG demonstrates a comprehensive understanding of the environment by accurately describing it as a suburban neighborhood intertwined with a park. This holistic perception is attributed to Embodied-RAG’s hierarchical organization of information, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S4.F3" title="Figure 3 ‣ IV-B Retrieval ‣ IV Method: Embodied Retrieval and Generation ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, enables it to pseudo-attend to every node in the map. In contrast, RAG retrieves only the most similar nodes, resulting in a fragmented view characterized by redundant items and a failure to integrate observations into a cohesive environmental context. This limitation of RAG, where subareas are treated as independent rather than interconnected components of the whole, aligns with findings from previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#bib.bib20" title="">20</a>]</cite>. Specifically, Embodied-RAG recognizes the tree-dominated landscape as an integral part of the park, understanding it in relation to other elements such as bushes and shrubs, rather than as an isolated area. In contrast, RAG treats this landscape as a distinct entity, detached from its broader park context. As a result, RAG presents a fragmented collection of local observations, while Embodied-RAG generates outputs that are both spatially and semantically coherent, reflecting a human-like understanding of the environment.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">Computational Efficiency</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.3">Both memory construction and retrieval have a computational complexity of <math alttext="O(\log N)" class="ltx_Math" display="inline" id="S6.SS3.p1.1.m1.1"><semantics id="S6.SS3.p1.1.m1.1a"><mrow id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.3.cmml">O</mi><mo id="S6.SS3.p1.1.m1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S6.SS3.p1.1.m1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.cmml"><mo id="S6.SS3.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S6.SS3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.cmml">log</mi><mo id="S6.SS3.p1.1.m1.1.1.1.1.1a" lspace="0.167em" xref="S6.SS3.p1.1.m1.1.1.1.1.1.cmml">⁡</mo><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml">N</mi></mrow><mo id="S6.SS3.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S6.SS3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><apply id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"><times id="S6.SS3.p1.1.m1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.2"></times><ci id="S6.SS3.p1.1.m1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3">𝑂</ci><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1"><log id="S6.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1"></log><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.2">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">O(\log N)</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.1.m1.1d">italic_O ( roman_log italic_N )</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S6.SS3.p1.2.m2.1"><semantics id="S6.SS3.p1.2.m2.1a"><mi id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><ci id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.2.m2.1d">italic_N</annotation></semantics></math> represents the number of nodes in the environment. This choice allows us to efficiently scale to larger environments, as the time complexity only increases logarithmically with the number of nodes. Additionally, when performing the <math alttext="k" class="ltx_Math" display="inline" id="S6.SS3.p1.3.m3.1"><semantics id="S6.SS3.p1.3.m3.1a"><mi id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><ci id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.3.m3.1d">italic_k</annotation></semantics></math> retrievals, we execute them in parallel to minimize the overall time cost. In our real-life experiments, the time costs are demonstrated in the supplementary video, which is 8x fast-forwarded. On average, a single retrieval takes around 20 seconds in most of our environments, and the travel time depends on the speed of the specific embodiment in use.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">Ablation</span>
</h3>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S6.F5.g1" src="extracted/5910947/images/ablation_k.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Effect of total number of <math alttext="K" class="ltx_Math" display="inline" id="S6.F5.3.m1.1"><semantics id="S6.F5.3.m1.1b"><mi id="S6.F5.3.m1.1.1" xref="S6.F5.3.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.F5.3.m1.1c"><ci id="S6.F5.3.m1.1.1.cmml" xref="S6.F5.3.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.3.m1.1d">K</annotation><annotation encoding="application/x-llamapun" id="S6.F5.3.m1.1e">italic_K</annotation></semantics></math> searches or <math alttext="K" class="ltx_Math" display="inline" id="S6.F5.4.m2.1"><semantics id="S6.F5.4.m2.1b"><mi id="S6.F5.4.m2.1.1" xref="S6.F5.4.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.F5.4.m2.1c"><ci id="S6.F5.4.m2.1.1.cmml" xref="S6.F5.4.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F5.4.m2.1d">K</annotation><annotation encoding="application/x-llamapun" id="S6.F5.4.m2.1e">italic_K</annotation></semantics></math> retrievals</figcaption>
</figure>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.3">We investigate the impact of <math alttext="k\in\{1,\mathrm{GPT4\ Token\ Limit}\}" class="ltx_Math" display="inline" id="S6.SS4.p1.1.m1.2"><semantics id="S6.SS4.p1.1.m1.2a"><mrow id="S6.SS4.p1.1.m1.2.2" xref="S6.SS4.p1.1.m1.2.2.cmml"><mi id="S6.SS4.p1.1.m1.2.2.3" xref="S6.SS4.p1.1.m1.2.2.3.cmml">k</mi><mo id="S6.SS4.p1.1.m1.2.2.2" xref="S6.SS4.p1.1.m1.2.2.2.cmml">∈</mo><mrow id="S6.SS4.p1.1.m1.2.2.1.1" xref="S6.SS4.p1.1.m1.2.2.1.2.cmml"><mo id="S6.SS4.p1.1.m1.2.2.1.1.2" stretchy="false" xref="S6.SS4.p1.1.m1.2.2.1.2.cmml">{</mo><mn id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml">1</mn><mo id="S6.SS4.p1.1.m1.2.2.1.1.3" xref="S6.SS4.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S6.SS4.p1.1.m1.2.2.1.1.1" xref="S6.SS4.p1.1.m1.2.2.1.1.1.cmml"><mi id="S6.SS4.p1.1.m1.2.2.1.1.1.2" xref="S6.SS4.p1.1.m1.2.2.1.1.1.2.cmml">GPT4</mi><mo id="S6.SS4.p1.1.m1.2.2.1.1.1.1" lspace="0.500em" xref="S6.SS4.p1.1.m1.2.2.1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.2.2.1.1.1.3" xref="S6.SS4.p1.1.m1.2.2.1.1.1.3.cmml">Token</mi><mo id="S6.SS4.p1.1.m1.2.2.1.1.1.1a" lspace="0.500em" xref="S6.SS4.p1.1.m1.2.2.1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.2.2.1.1.1.4" xref="S6.SS4.p1.1.m1.2.2.1.1.1.4.cmml">Limit</mi></mrow><mo id="S6.SS4.p1.1.m1.2.2.1.1.4" stretchy="false" xref="S6.SS4.p1.1.m1.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.2b"><apply id="S6.SS4.p1.1.m1.2.2.cmml" xref="S6.SS4.p1.1.m1.2.2"><in id="S6.SS4.p1.1.m1.2.2.2.cmml" xref="S6.SS4.p1.1.m1.2.2.2"></in><ci id="S6.SS4.p1.1.m1.2.2.3.cmml" xref="S6.SS4.p1.1.m1.2.2.3">𝑘</ci><set id="S6.SS4.p1.1.m1.2.2.1.2.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1"><cn id="S6.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p1.1.m1.1.1">1</cn><apply id="S6.SS4.p1.1.m1.2.2.1.1.1.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1.1"><times id="S6.SS4.p1.1.m1.2.2.1.1.1.1.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1.1.1"></times><ci id="S6.SS4.p1.1.m1.2.2.1.1.1.2.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1.1.2">GPT4</ci><ci id="S6.SS4.p1.1.m1.2.2.1.1.1.3.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1.1.3">Token</ci><ci id="S6.SS4.p1.1.m1.2.2.1.1.1.4.cmml" xref="S6.SS4.p1.1.m1.2.2.1.1.1.4">Limit</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.2c">k\in\{1,\mathrm{GPT4\ Token\ Limit}\}</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.1.m1.2d">italic_k ∈ { 1 , GPT4 roman_Token roman_Limit }</annotation></semantics></math> on Embodied-RAG and RAG in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.18313v4#S6.F5" title="Figure 5 ‣ VI-D Ablation ‣ VI Results ‣ Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation"><span class="ltx_text ltx_ref_tag">5</span></a>. A total of 15 experiments were conducted for each <math alttext="k" class="ltx_Math" display="inline" id="S6.SS4.p1.2.m2.1"><semantics id="S6.SS4.p1.2.m2.1a"><mi id="S6.SS4.p1.2.m2.1.1" xref="S6.SS4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.m2.1b"><ci id="S6.SS4.p1.2.m2.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.2.m2.1d">italic_k</annotation></semantics></math> in each environment. We observe that with larger <math alttext="k" class="ltx_Math" display="inline" id="S6.SS4.p1.3.m3.1"><semantics id="S6.SS4.p1.3.m3.1a"><mi id="S6.SS4.p1.3.m3.1.1" xref="S6.SS4.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.3.m3.1b"><ci id="S6.SS4.p1.3.m3.1.1.cmml" xref="S6.SS4.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.3.m3.1d">italic_k</annotation></semantics></math>, both RAG and Embodied-RAG show improved performance, but this improvement plateaus at higher values. RAG still fails to capture the larger holistic resolution with just more object-level nodes and cannot adequately solve the implicit/general queries, further justifying our hierarchy and tree selection approach.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Limitations and Future work</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We primarily focused on semantic forests rather than a topological map. Therefore, we may not be robust in obstacle avoidance involving dynamic objects and people. Furthermore, Embodied-RAG currently struggles with requests that require precise counting of objects at a small scale (e.g., “How many chairs are there around the red table?”). This limitation arises because the agglomerative clustering of the semantic forest does not consider multi-view consistency. Future work could incorporate multi-view consistency in the hierarchies of the semantic forest with a learned or pre-trained mechanism to cluster with positional information (e.g. utilizing a LLM).</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We present Embodied-RAG, a system capable of capturing spatial memory at any spatial and semantic resolution in both indoor and outdoor environments, and retrieving and generating responses for navigation and explanation requests. Additionally, we introduce the task of Embodied-RAG benchmark, unifying semantic navigation and question answering. Our findings demonstrate that Embodied-RAG can robustly handle implicit and global queries, as well as ambiguously phrased requests from human annotators. Our results indicate that Embodied-RAG shows potential as the basis for incorporating large non-parameteric memories into robotics foundation models. We are excited for future extensions to manipulation and dynamic environments that enable robotics tasks out of reach for current memory/context constrained approaches.

</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Khanna, Roozbeh <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Goat-bench: A benchmark for multi-modal lifelong navigation,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">arXiv:2404.06609</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Chaplot, R. R <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Object goal navigation using goal-oriented semantic exploration,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">NeurIPS</em>, vol. 33, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Krantz, S. Lee, J. Malik, D. Batra, and D. S. Chaplot, “Instance-specific image goal navigation: Training embodied agents to find object instances,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CVPR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Zhou, Y. Hong, and Q. Wu, “Navgpt: Explicit reasoning in vision-and-language navigation with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv:2305.16986</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Rana <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “Sayplan: Grounding large language models using 3d scene graphs for scalable task planning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">CoRL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Li, Fuchun <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et al.</em>, “Embodied semantic scene graph generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">CoRL</em>, A. Faust, D. Hsu, and G. Neumann, Eds.   PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Lewis, D. Kiela <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Asai, S. Min, Z. Zhong, and D. Chen, “Acl 2023 tutorial: Retrieval-based language models and applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ACL 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language models in retrieval-augmented generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">et al.</em>, “On evaluation of embodied navigation agents,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2">arXiv:1807.06757</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, “Target-driven visual navigation in indoor scenes using deep reinforcement learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2017 ICRA</em>.   IEEE, 2017, pp. 3357–3364.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. v. d. Hengel, “Reverie: Remote embodied visual referring expression in real indoor environments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ICCV</em>, 2020, pp. 9982–9991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. E. Wang, “Vision-and-language navigation: A survey of tasks, methods, and future directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv:2203.12667</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ku, Anderson <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Anderson <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Vision-and-language navigation:interpreting visually-grounded navigation instructions in real environments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">CVPR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Embodied question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CVPR</em>, 2018, pp. 1–10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Zhu, R. Kapoor, S. Y. Min, W. Han, J. Li, K. Geng, G. Neubig, Y. Bisk, A. Kembhavi, and L. Weihs, “Excalibur: Encouraging and evaluating embodied exploration,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ICCV</em>, 2023, pp. 14 931–14 942.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting grounded instructions for everyday tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CVPR</em>, 2020, p. 10740.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Gao, G. S <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">et al.</em>, “Dialfred: Dialogue-enabled agents for embodied instruction following,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2">RA-L</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, “From local to global: A graph rag approach to query-focused summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2404.16130</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Z. Durante, J. Gao <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “Agent ai: Surveying the horizons of multimodal interaction,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Hu, Q. Xie, V. Jain, F. Xia, Y. Bisk <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">et al.</em>, “Toward general-purpose robots via foundation models: A survey and meta-analysis,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Wang, Yankai <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “A survey on large language model based autonomous agents,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">arXiv:2308.11432</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Nasiriany, Brian <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, “Pivot: Iterative visual prompting elicits actionable knowledge for vlms,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ji, Pascale <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et al.</em>, “Survey of hallucination in natural language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">ACM Computing Surveys</em>, vol. 55, no. 12, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhou, Huaxiu <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Analyzing and mitigating object hallucination in large vision-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">arXiv:2310.00754</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Shuster, Jason <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “Retrieval augmentation reduces hallucination in conversation,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">arXiv:2104.07567</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Open x-embodiment: Robotic learning datasets and rt-x models,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">arXiv:2310.08864</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et al.</em>, “Openvla: An open-source vision-language-action model,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">arXiv:2406.09246</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, “Film: Following instructions in language with modular methods,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ICLR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Y. Min, Yonatan <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et al.</em>, “Don’t copy the teacher: Data and model challenges in embodied dialogue,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam, “Clip-fields: Weakly supervised semantic fields for robotic memory,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv: Arxiv-2210.05663</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. Huang, O. Mees, A. Zeng, and W. Burgard, “Visual language maps for robot navigation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the ICRA</em>, London, UK, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Chang, T. Gervet, M. Khanna, S. Yenamandra, D. Shah, S. Y. Min, K. Shah, C. Paxton, S. Gupta, D. Batra <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">et al.</em>, “Goat: Go to any thing,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2">arXiv:2311.06430</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, “Poni: Potential functions for objectgoal navigation with interaction-free learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ICCV</em>, 2022, pp. 18 890–18 900.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. Y. Min, Y.-H. H. Tsai, W. Ding, A. Farhadi, R. Salakhutdinov, Y. Bisk, and J. Zhang, “Self-supervised object goal navigation with in-situ finetuning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2023 IROS</em>.   IEEE, 2023, pp. 7119–7126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, “Octomap: An efficient probabilistic 3d mapping framework based on octrees,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Autonomous robots</em>, vol. 34, pp. 189–206, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
L. Zhang, L. Wei, P. Shen, W. Wei, G. Zhu, and J. Song, “Semantic slam based on object detection and improved octomap,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">IEEE Access</em>, vol. 6, pp. 75 545–75 559, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
K. Zheng, A. Paul, and S. Tellex, “Asystem for generalized 3d multi-object search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2023 ICRA</em>.   IEEE, 2023, pp. 1638–1644.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
K. Liu, Z. Fan, M. Liu, and S. Zhang, “Object-aware semantic mapping of indoor scenes using octomap,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2019 Chinese Control Conference (CCC)</em>.   IEEE, 2019, pp. 8671–8676.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
L. Mezghan, S. Sukhbaatar, T. Lavril, O. Maksymets, D. Batra, P. Bojanowski, and K. Alahari, “Memory-augmented reinforcement learning for image-goal navigation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">2022 IROS</em>.   IEEE, 2022, pp. 3316–3323.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Padalkar <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">et al.</em>, “Open x-embodiment: Robotic learning datasets and rt-x models,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.2.2">arXiv:2310.08864</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra, “Multi-target embodied question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ICCV</em>, 2019, p. 6309.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
S. Tan, M. Ge, D. Guo, H. Liu, and F. Sun, “Knowledge-based embodied question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhong, Tat-Seng <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">et al.</em>, “Video question answering: Datasets, algorithms and challenges,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">arXiv:2203.01225</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
H. Yang, L. Chaisorn, Y. Zhao, S.-Y. Neo, and T.-S. Chua, “Videoqa: question answering on news video,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the eleventh ACM international conference on Multimedia</em>, 2003, pp. 632–641.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Castro, Rada <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">et al.</em>, “Lifeqa: A real-life dataset for video question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.2.2">LREC</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Xiao, X. Shang, A. Yao, and T.-S. Chua, “Next-qa: Next phase of question-answering to explaining temporal actions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ICCV</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
P. H. Sneath and R. R. Sokal, <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Numerical Taxonomy: The Principles and Practice of Numerical Classification</em>.   W.H. Freeman, 1973.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">NeurIPS</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and physical simulation for autonomous vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">FSR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">et al.</em>, “Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.2.2">arXiv:2109.08238</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
B. Yamauchi, “A frontier-based approach for autonomous exploration,” in <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA’97. ’Towards New Computational Principles for Robotics and Automation’</em>, 1997, pp. 146–151.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
H. Zhu <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">et al.</em>, “Dsvp: Dual-stage viewpoint planner for rapid exploration by dynamic expansion,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.2.2">IROS</em>, pp. 7623–7630, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
N. Hughes <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">et al.</em>, “Hydra: A real-time spatial perception system for 3d scene graph construction and optimization,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.2.2">RSS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
B. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">et al.</em>, “Open-vocabulary queryable scene representations for real world planning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib56.2.2">ICRA</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 15:08:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
