<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images</title>
<!--Generated on Sat Sep 28 13:36:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Deep Learning,  Medical Image Segmentation,  Transformer,  Skip Connection,  Cervical Spinal Cord,  Magnetic Resonance Imaging,  Diffusion Tensor Imaging,  Fractional Anisotropy,  Quantitative Analysis,  Cervical Stenosis
" lang="en" name="keywords"/>
<base href="/html/2409.19354v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S1" title="In Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction and Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S2" title="In Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S2.SS1" title="In II Related Work ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Cervical spinal cord disease analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S2.SS2" title="In II Related Work ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Deep learning-based medical image analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3" title="In Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed Solutions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3.SS1" title="In III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Insights into Cervical Spinal Cord and Imaging Modalities</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3.SS2" title="In III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Deep Learning-based Methodology for Medical Image Segmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S4" title="In Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Expected Contributions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S5" title="In Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maryam Tavakol Elahi


</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">School of Electrical Engineering and Computer Science</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">University of Ottawa
<br class="ltx_break"/></span>Ottawa, Canada 
<br class="ltx_break"/>mtava020@uottawa.ca
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">This research proposal discusses two challenges in the field of medical image analysis: the multi-parametric investigation on microstructural and macrostructural characteristics of the cervical spinal cord and deep learning-based medical image segmentation. First, we conduct a thorough analysis of the cervical spinal cord within a healthy population. Unlike most previous studies, which required medical professionals to perform functional examinations using metrics like the modified Japanese Orthopaedic Association (mJOA) score or the American Spinal Injury Association (ASIA) impairment scale, this research focuses solely on Magnetic Resonance (MR) images of the cervical spinal cord. Second, we employ cutting-edge deep learning-based segmentation methods to achieve highly accurate macrostructural measurements from MR images. To this end, we propose an enhanced UNet-like Transformer-based framework with attentive skip connections.
This paper reports on the problem domain, proposed solutions, current status of research, and expected contributions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deep Learning, Medical Image Segmentation, Transformer, Skip Connection, Cervical Spinal Cord, Magnetic Resonance Imaging, Diffusion Tensor Imaging, Fractional Anisotropy, Quantitative Analysis, Cervical Stenosis

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction and Motivation</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Medical images are highly expressive and useful, but the critical details and visual characteristics they encompass are rarely discernible to even the most trained eyes. Knowing that various types of medical images, especially MRI and CT scans, are among the essential and fundamental resources for acquiring relevant information about patients’ bodies and different diseases, interpreting them remains a top priority.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As a small and crucial part of the central nervous system, the spinal cord has greatly benefited from MRI’s non-invasive and superior soft tissue contrast capabilities. Over the past two decades, MRI has progressed from structural to quantitative imaging, allowing for both qualitative and quantitative assessments of spinal cord tissue structure.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Technological innovations, such as diffusion tensor imaging (DTI) and the development of high-field MRI scanners, have moved quantitative imaging forward, enabling more detailed examinations of tissue microstructure. Building on this progress, now is the time to delve deeper into the spinal cord’s microstructure using advanced quantitative MR imaging techniques.
This approach not only assists clinicians in tracking disease progression and predicting clinical outcomes but also facilitates earlier disease diagnosis by identifying quantitative neurological changes that precede structural alterations. Recent literature highlights the potential of DTI metrics as valuable biomarkers for early disease detection. In light of these advancements, our study focuses on utilizing quantitative measurements to evaluate the microstructural and macrostructural characteristics of the cervical spinal cord MR images.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Microstructural features refer to the fine details of spinal cord tissue, which can be analyzed at the microscopic level. They encompass elements such as nerve fibers, axons, and the myelin sheath surrounding the axons. Fractional anisotropy (FA), for instance, is a microstructural characteristic that can provide insights into fiber density, axonal diameter, and myelination in white matter. Variations in these microstructural elements can indicate damage or disease, often manifested in symptoms of pain, numbness, or motor function loss. On the other hand, macrostructural features focus on the more observable aspects of the spinal cord, including size, shape, volume, and other anatomical details. Macrostructural changes can be indicative of larger-scale problems such as spinal cord injury, deformity, or compression.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this study, we aim to contribute to the automatic analysis and interpretation of medical images with the spinal cord as an exemplar by developing applicable methodologies.
We seek to investigate the following research questions
</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">RQ1:</span>
How can the analysis and interpretation of medical images, specifically CT and MRI scans, be improved and automated using state-of-the-art deep learning-based computer vision methods?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">RQ2:</span>
How can we find relationships between microstructural and macrostructural features of the cervical spinal cord in a healthy population by eliminating the need for subjective measurements from clinicians, thereby objectively understanding these relationships using the acquired MR images purely?</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">RQ3:</span>
How can we achieve highly accurate macrostructural measurements from cervical spinal cord MR images by leveraging the capabilities of recent deep learning-based segmentation methods?</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">This research is conducted in three steps. First, addressing <span class="ltx_text ltx_font_bold" id="S1.p9.1.1">RQ1</span>, understanding the cervical spinal cord’s microstructural and macrostructural characteristics and their interrelations, we aim to improve and automate the extraction and analysis of these features from MR images, using the spinal cord as an exemplar.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Second, for <span class="ltx_text ltx_font_bold" id="S1.p10.1.1">RQ2</span>, we present a multiparametric approach for evaluating the microstructural and macrostructural features of the cervical spinal cord in healthy individuals. We delve into the investigation of the correlation between them while concurrently examining the influence of gender and different imaging machines on these correlations.
The proposed approach aims to establish relationships between these features, thereby fostering a deeper understanding of the microstructural and macrostructural characteristics and examining whether the microstructural changes might occur from various degrees of asymptomatic stenosis.
All these measurements are captured using quantitative MRI, eliminating the need for subjective assessments and relying primarily on objective MRI data.</p>
</div>
<div class="ltx_para" id="S1.p11">
<p class="ltx_p" id="S1.p11.1">In the third step, addressing <span class="ltx_text ltx_font_bold" id="S1.p11.1.1">RQ3</span>, we propose an enhanced UNet-like Transformer-based framework with attentive skip connections for high-performance image segmentation. Our approach features a novel Transformer-based skip connection module that integrates features extracted from both the encoder and decoder, enabling it to capture more complex dependencies between different levels of abstraction.
We further improve the framework’s efficiency and its ability to process high-resolution images by adopting a merging cross-covariance attention mechanism in place of the conventional self-attention operation.</p>
</div>
<div class="ltx_para" id="S1.p12">
<p class="ltx_p" id="S1.p12.1">The rest of this paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S2" title="II Related Work ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">II</span></a> discusses related work, and Section <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3" title="III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">III</span></a> presents the proposed multi-parametric method to evaluate the cervical spinal cord with a focus on quantitative measurements and the proposed deep learning-based segmentation framework. Expected contributions are outlined in section <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S4" title="IV Expected Contributions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">IV</span></a>, and section <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S5" title="V Conclusion ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">V</span></a> concludes and highlights future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Cervical spinal cord disease analysis</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In a study, Avinash et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib1" title="">1</a>]</cite> investigate the usefulness of FA as a biomarker for the severity of cervical spondylotic myelopathy (CSM) patient cases and as a prognostic biomarker for improvement after surgery. The regression analysis performed between FA and mJOA score indicates that FA at the level of maximal compression (LMC) correlates positively with pre-operative mJOA score, and pre-operative FA correlates inversely with recovery throughout the post-operative period.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In another study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib2" title="">2</a>]</cite>, the analysis of correlations between the pre- and post-operative FA and mean diffusivity (MD) values and the pre- and post-operative JOA scores reveals that although the JOA score improved significantly after surgery, no significant changes were observed in the pre- and post-operative FA and MD values.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Another work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib3" title="">3</a>]</cite> quantifies the reproducibility, temporal stability, and functional correlation of diffusion MR characteristics in the spinal cord of patients with cervical stenosis with or without myelopathy. The study explores the association between longitudinal DTI measurements and serial neurological function assessment.
Their research concerning the specific DTI measurements shows that FA within the spinal cord appears slightly more sensitive to neurological function and more stable than measures of MD.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Kara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib4" title="">4</a>]</cite> show that DTI may detect abnormalities in the spinal cord before the development of T2 hyper-intensity on conventional sequences in patients with CSM.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Other studies have examined the efficacy of diffusion tensor imaging in recognizing degenerative CSM. For instance, Orel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib5" title="">5</a>]</cite> investigates the role of diffusion tensor imaging in the diagnosis, prognosis, and assessment of recovery and treatment of spinal cord injury. It is demonstrated that DTI metrics and combinations thereof correlate significantly with clinical function in both model species and humans.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Deep learning-based medical image analysis</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib6" title="">6</a>]</cite> is a widely-used CNN-based medical image segmentation method that utilizes a symmetric, U-shaped structure and skip connections to efficiently capture both low and high-level features from input images. Following U-Net, numerous CNN-based segmentation approaches have been presented, e.g., SegNet  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib7" title="">7</a>]</cite>, DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib8" title="">8</a>]</cite>, PSPNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib9" title="">9</a>]</cite>, and Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib10" title="">10</a>]</cite>. Despite the variations in architectural design, the majority of approaches introduced after U-Net expand on its success by either modifying the architecture or offering new performance-enhancing strategies.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib11" title="">11</a>]</cite>, as the first successful attempt at employing Transformers in computer vision, has shown potential in several computer vision applications. Yet, there are hurdles with processing high-resolution images due to the high computational cost of processing all image patches. To address this issue, hierarchical structures such as Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib12" title="">12</a>]</cite> and Pyramid Vision Transformer (PVT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib13" title="">13</a>]</cite> were introduced for the downstream tasks. The shifted-window approach proposed in the Swin Transformer limits self-attention computation to non-overlapping local windows while permitting cross-window attention, thus reducing computational costs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Following the breakthroughs in Transformer-based architectures for both classification and dense prediction tasks in recent years, a number of studies investigating the capability of Transformers in medical image analysis applications, notably medical image segmentation, have been presented, with TransUnet  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib14" title="">14</a>]</cite>, U-Net Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib15" title="">15</a>]</cite>, CoTr <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib16" title="">16</a>]</cite>, TransFuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib17" title="">17</a>]</cite>, MIXED TRANSFORMER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib18" title="">18</a>]</cite>, and UNETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib19" title="">19</a>]</cite> being among some of the most successful designs introduced on this topic to date. These transformer-based models are built on the self-attention mechanism that allows them to attend to relevant image regions and capture long-range relationships.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Over the past few years, A number of studies have endeavoured to gain as much advantage as possible from skip connections by employing the attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib20" title="">20</a>]</cite>, redesigning skip pathways in UNet++, introducing Res paths in MultiResUnet networks  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib21" title="">21</a>]</cite>, or incorporating a skip connection module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib22" title="">22</a>]</cite> to narrow the semantic and resolution disparities between encoder and decoder representations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed Solutions</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Insights into Cervical Spinal Cord and Imaging Modalities</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We propose a multi-parametric method to evaluate the cervical spinal cord with a focus on quantitative measurements in a healthy population to investigate how its microstructural and macrostructural features are correlated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib23" title="">23</a>]</cite>. The MR images used for this analysis are acquired using a 3T scanner across three different MR machines. All centers have used a standardized imaging protocol, providing multimodal MR imaging, including T1-weighted imaging (T1WI), T2-weighted imaging (T2WI), T2*-weighted imaging (T2S), magnetization transfer imaging (MT), and diffusion-weighted imaging (DWI) of the cervical spinal cord. The primary goal is to determine whether there is any significant relationship between macrostructural and microstructural characteristics of the cervical spinal cord in a healthy population. The second objective is to evaluate the influence of gender and the type of MR machines used for acquisition on the correlation between the cervical spinal cord’s micro and macrostructural features.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The Cervical Spinal Cord Dataset, acquired through the spine-generic protocol, is utilized in this study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib24" title="">24</a>]</cite>. The dataset contains 125 healthy female and 142 healthy male participants from 42 different groups. The imaging data were then fed into the Spinal Cord Toolbox (SCT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib25" title="">25</a>]</cite> to estimate the spinal cord cross-section area (SC CSA), the space available for the cord (SAC) and the ratio of SAC/CSA, which is inversely proportional to stenosis at all vertebral levels.
Furthermore, DTI parameters, including the FA, MD, and radial diffusivity (RD), were calculated per-level using the weighted least squares fitting method presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib25" title="">25</a>]</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3.F1" title="Figure 1 ‣ III-A Insights into Cervical Spinal Cord and Imaging Modalities ‣ III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">1</span></a> represents the processing pipeline, including cervical spinal cord segmentation with a focus on SAC and CSA areas and per-vertebral level segmentation.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S3.F1.g1" src="extracted/5886461/pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Left: cervical spinal cord and cerebrospinal fluid (CSF) segmentation, Middle: cross-sectional view of cord (CSA) and SAC segmentation, Right: per-vertebral level segmentation.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We analyzed the per-level extracted metrics to identify any correlation between FA and SAC/CSA, taking into account factors such as the participant’s gender and MRI machine type. The results indicate that there is a potential positive Pearson correlation between FA and the degree of stenosis. Moreover, the correlation values vary depending on the type of MRI machine and the gender of participants.
The correlation coefficients are then compared using the z-test on Fisher z-transformed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib26" title="">26</a>]</cite> correlation coefficients, which allows for a more precise statistical comparison.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Deep Learning-based Methodology for Medical Image Segmentation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">U-Net is an extensively adopted and influential medical image segmentation method known for its unique symmetric contracting-expanding architecture, enabling the network to extract both low-level and high-level features from input images.
As an encoder-decoder architecture, UNet employs a series of convolutional, pooling, and upsampling layers with skip connections, which are designed to concatenate the low-level feature maps extracted from the encoder pathway with their corresponding feature maps in the decoder pathway. This allows the network to retrieve spatial information with a finer granularity that would otherwise be lost due to the downsampling process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">It has been demonstrated that skip connections are effective in bridging the semantic and resolution gaps between the encoder and decoder features. However, the incompatibility of corresponding representations limits their efficacy. U-Net and other UNet-like variants of encoder-decoder networks used for medical image segmentation suffer from two common challenges: the inadequacy of naive skip connections in modelling long-range correlations between distant pixels, and the structural computational complexity linked to fine-grained prediction tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We propose a deep learning-based medical image segmentation framework called SAttisUNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib27" title="">27</a>]</cite> to address the aforementioned challenges. As shown in Figure. <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3.F2" title="Figure 2 ‣ III-B Deep Learning-based Methodology for Medical Image Segmentation ‣ III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">2</span></a>, we have employed the Swin Transformer as a replacement for convolutional hierarchy in U-Net because of its remarkable aspect of hierarchical architecture and shifted-window technique, benefiting from the strengths of both CNN and Transformer. This design brings greater efficiency while offering the flexibility to model at various scales with linear computational complexity relative to image size. This encoder-decoder network particularly works well when working on dense-prediction tasks involving large variations in the scale of visual entities.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S3.F2.g1" src="extracted/5886461/framework.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The architecture of the proposed SAttisUNet for enhanced medical image segmentation.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">SAttisUNet enriches the encoder’s high-resolution representations prior to integrating them with the decoder’s corresponding feature maps and, therefore, captures fine-grained information more effectively. The features extracted from the Swin Transformer are then fed into the proposed attentive skip connection module, which leverages a merging cross-covariance attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#bib.bib28" title="">28</a>]</cite>. The purpose of this module is to fuse the encoder’s features effectively, bridging the semantic gap and ultimately enhancing the overall performance of the segmentation network. We have applied SAttisUNet to the cervical spinal cord dataset with the goal of performing per-level vertebral segmentation. So far, we have achieved an encouraging 89% segmentation accuracy, given the current scope of data and the stage of experimentation. This result showcases SAttisUNet’s adaptability and potential in medical image segmentation tasks. Figure. <a class="ltx_ref" href="https://arxiv.org/html/2409.19354v1#S3.F3" title="Figure 3 ‣ III-B Deep Learning-based Methodology for Medical Image Segmentation ‣ III Proposed Solutions ‣ Toward Deep Learning-based Segmentation and Quantitative Analysis of Cervical Spinal Cord Magnetic Resonance Images"><span class="ltx_text ltx_ref_tag">3</span></a> depicts a segmentation result from a sample cervical spinal cord MRI scan.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="265" id="S3.F3.g1" src="extracted/5886461/anat-spin-t2w.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The qualitative segmentation results of SAttisUNet on the cervical spinal cord dataset from the sagittal view.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Expected Contributions</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">we have surveyed fundamental medical concepts and state-of-the-art segmentation methods (<span class="ltx_text ltx_font_italic" id="S4.p1.1.1">RQ1</span>), and analyzed correlations between microstructural
and macrostructural features of the cervical spinal cord (<span class="ltx_text ltx_font_italic" id="S4.p1.1.2">RQ2</span>). Moreover, we implemented SAttisUNet, the proposed medical image segmentation method (<span class="ltx_text ltx_font_italic" id="S4.p1.1.3">RQ3</span>).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Future work may include refining and extending the proposed segmentation framework to other medical imaging modalities, such as investigating its efficacy when applied to diffusion-weighted images (DWI). We also plan to investigate the possibility of extending the framework to include classification capabilities. Additionally, refining and exploring modifications to the self-attention mechanism within the encoder-decoder network to better account for both local and global relationships among patches and applying the SAttisUNet model to datasets comprising CSM, thereby expanding its applicability in clinical settings are among the potential improvements and investigations we aim to pursue in our future work.
Furthermore, we are conducting an ablation study to examine the impact of varying factors, such as the number of skip connections, network depth, and the number of attention heads in multi-head self-attention blocks, on the model’s performance.
Exploring the factors influencing correlations between the microstructural and macrostructural features of the cervical spinal cord may also lead to identifying novel biomarkers for various neurological conditions.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We have addressed two challenges in the field of medical image analysis: first, objectively analyzing the microstructural and macrostructural characteristics of the cervical spinal cord in a healthy population, and second, enhancing medical image segmentation by overcoming the limitations of naive skip connection strategies, refining the modeling of long-range dependencies, and enriching feature representation.
Additionally, with segmentation tasks being computationally intensive by nature, we manage the added complexity in our Transformer-based model, thereby improving the macrostructural measurements through the proposed segmentation method.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In the first part of the research, we conducted an in-depth analysis of cervical spinal cord features in a healthy population. The investigation into the correlation between microstructural and macrostructural characteristics fills a knowledge gap, as limited research has been conducted in this area. The findings contribute to a deeper understanding of the cervical spinal cord’s features and highlight the influence of factors such as gender and acquisition machine types on observed correlations.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In the second part, the proposed UNet-like Transformer-based segmentation framework with attentive skip connections demonstrates improvements compared to conventional UNet-like architectures in medical image segmentation. The integration of the attentive skip connection module and the employment of a merging cross-covariance attention mechanism bridge the gaps between different abstraction levels and capture complex dependencies. Furthermore, incorporating Swin Transformer blocks into U-shaped architectures enhances performance in multi-organ medical image segmentation tasks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Rao, H. Soliman, M. Kaushal, O. Motovylyak, A. Vedantam, M. D. Budde,
B. Schmit, M. Wang, and S. N. Kurpad, “Diffusion tensor imaging in a large
longitudinal series of patients with cervical spondylotic myelopathy
correlated with long-term functional outcome,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Neurosurgery</em>, vol. 83,
no. 4, pp. 753–760, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Kitamura, S. Maki, M. Koda, T. Furuya, Y. Iijima, J. Saito, T. Miyamoto,
Y. Shiga, K. Inage, S. Orita <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Longitudinal diffusion tensor
imaging of patients with degenerative cervical myelopathy following
decompression surgery,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">Journal of Clinical Neuroscience</em>, vol. 74,
pp. 194–198, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. M. Ellingson, N. Salamon, D. C. Woodworth, H. Yokota, and L. T. Holly,
“Reproducibility, temporal stability, and functional correlation of
diffusion mr measurements within the spinal cord in patients with
asymptomatic cervical stenosis or cervical myelopathy,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Journal of
Neurosurgery: Spine</em>, vol. 28, no. 5, pp. 472–480, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Kara, A. Celik, S. Karadereler, L. Ulusoy, K. Ganiyusufoglu, L. Onat,
A. Mutlu, I. Ornek, M. Sirvanci, and A. Hamzaoglu, “The role of dti in early
detection of cervical spondylotic myelopathy: a preliminary study with 3-t
mri,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Neuroradiology</em>, vol. 53, pp. 609–616, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
O. A. Zaninovich, M. J. Avila, M. Kay, J. L. Becker, R. J. Hurlbert, and N. L.
Martirosyan, “The role of diffusion tensor imaging in the diagnosis,
prognosis, and assessment of recovery and treatment of spinal cord injury: a
systematic review,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Neurosurgical focus</em>, vol. 46, no. 3, p. E7, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
encoder-decoder architecture for image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE
transactions on pattern analysis and machine intelligence</em>, vol. 39, no. 12,
pp. 2481–2495, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE transactions on pattern analysis and
machine intelligence</em>, vol. 40, no. 4, pp. 834–848, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 2881–2890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2961–2969.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">et al.</em>,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE/CVF international conference on computer
vision</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE/CVF
international conference on computer vision</em>, 2021, pp. 568–578.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and
Y. Zhou, “Transunet: Transformers make strong encoders for medical image
segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2102.04306</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler, “U-net
transformer: Self and cross attention for medical image segmentation,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Machine Learning in Medical Imaging: 12th International Workshop, MLMI
2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27,
2021, Proceedings 12</em>.   Springer, 2021,
pp. 267–276.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Xie, J. Zhang, C. Shen, and Y. Xia, “Cotr: Efficiently bridging cnn and
transformer for 3d medical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Medical Image
Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
Part III 24</em>.   Springer, 2021, pp.
171–180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns for
medical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Medical Image Computing and Computer
Assisted Intervention–MICCAI 2021: 24th International Conference,
Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I
24</em>.   Springer, 2021, pp. 14–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H. Wang, S. Xie, L. Lin, Y. Iwamoto, X.-H. Han, Y.-W. Chen, and R. Tong,
“Mixed transformer u-net for medical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICASSP
2022-2022 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>.   IEEE, 2022, pp.
2390–2394.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R.
Roth, and D. Xu, “Unetr: Transformers for 3d medical image segmentation,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF winter conference on applications of
computer vision</em>, 2022, pp. 574–584.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Li, Y. Tan, W. Chen, X. Luo, Y. Gao, X. Jia, and Z. Wang, “Attention
unet++: A nested attention-aware u-net for liver ct image segmentation,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2020 IEEE international conference on image processing (ICIP)</em>.   IEEE, 2020, pp. 345–349.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
N. Ibtehaz and M. S. Rahman, “Multiresunet: Rethinking the u-net architecture
for multimodal biomedical image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Neural networks</em>, vol.
121, pp. 74–87, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “Uctransnet: rethinking the skip
connections in u-net from a channel-wise perspective with transformer,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the AAAI conference on artificial intelligence</em>,
vol. 36, no. 3, 2022, pp. 2441–2449.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. T. Elahi, W. S. Lee, and P. Phan, “Toward macrostructural and
microstructural investigation of the cervical spinal cord through
quantitative analysis of t2-weighted and diffusion-weighted imaging,”
<em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Canadian Journal of Surgery</em>, vol. 65, pp. S137–S138, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Cohen-Adad, E. Alonso-Ortiz, M. Abramovic, C. Arneitz, N. Atcheson,
L. Barlow, R. L. Barry, M. Barth, M. Battiston, C. Büchel <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>,
“Generic acquisition protocol for quantitative mri of the spinal cord,”
<em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Nature protocols</em>, vol. 16, no. 10, pp. 4611–4632, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B. De Leener, S. Lévy, S. M. Dupont, V. S. Fonov, N. Stikov, D. L. Collins,
V. Callot, and J. Cohen-Adad, “Sct: Spinal cord toolbox, an open-source
software for processing spinal cord mri data,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Neuroimage</em>, vol. 145,
pp. 24–43, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. E. Hinkle, W. Wiersma, and S. G. Jurs, <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Applied statistics for the
behavioral sciences</em>.   Houghton Mifflin
college division, 2003, vol. 663.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. T. Elahi, W. Lee, and P. Phan, “Sattisunet: Unet-like swin transformer with
attentive skip connections for enhanced medical image segmentation,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">2023 International Conference on Machine Learning and Applications
(ICMLA)</em>.   IEEE, 2023, pp. 1994–1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev,
N. Neverova, G. Synnaeve, J. Verbeek <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Xcit: Cross-covariance
image transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">Advances in neural information processing
systems</em>, vol. 34, pp. 20 014–20 027, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 28 13:36:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
