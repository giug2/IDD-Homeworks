<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.12520] A Dataset and Baselines for Visual Question Answering on Art</title><meta property="og:description" content="Answering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acqu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Dataset and Baselines for Visual Question Answering on Art">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Dataset and Baselines for Visual Question Answering on Art">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.12520">

<!--Generated on Tue Mar 19 01:28:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Visual question answering,  art dataset,  external knowledge">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Osaka University, Japan </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Carnegie Mellon University, USA </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>CyberAgent, Inc., Japan</span></span></span>
<h1 class="ltx_title ltx_title_document">A Dataset and Baselines for
<br class="ltx_break">Visual Question Answering on Art</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noa Garcia 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chentao Ye
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihua Liu
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingtao Hu
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mayu Otani
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Chenhui Chu
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuta Nakashima
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Teruko Mitamura
</span><span class="ltx_author_notes">22</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Answering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acquired through the study of the history of art. In this work, we introduce our first attempt towards building a new dataset, coined AQUA (Art QUestion Answering). The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers’ correctness. Our dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. We also present a two-branch model as baseline, where the visual and knowledge questions are handled independently. We extensively compare our baseline model against the state-of-the-art models for question answering, and we provide a comprehensive study about the challenges and potential future directions for visual question answering on art.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Visual question answering, art dataset, external knowledge
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Providing human-like semantic interpretation of visual information is one of the ultimate goals of technologies around artificial intelligence, computer vision, and natural language processing. Tremendous research efforts have been made towards this goal, including object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, phrase grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, image/video captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>. Visual question answering (VQA) is among these works and is now one of the main stream topics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. VQA may require high-level comprehension of the image content, as well as questions given as natural language. Recently, various extensions of the VQA task have been proposed, including ones requiring knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The main target of the VQA task has been natural images, which capture real-world objects, scenes, and events. Very few work addresses other types of visual information, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, the abstract image subset of the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and PororoQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. One of the primary reasons for using non-real-world images is to unburden visual recognition in the VQA pipeline to give more focus on answer prediction.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Meanwhile, artworks, or paintings, are another interesting domain for VQA. Besides their cultural and historical importance, paintings pose extra challenges in VQA: Firstly, paintings may express a subject in different abstraction levels, perhaps being associated to the continuum spanned by naturalism, realism, symbolism, impressionism, cubism, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">etc</em>. Pretrained models for, <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.3" class="ltx_text"></span>, object detection, may work well for realism but not necessarily for cubism. Secondly, the interpretation of paintings can be highly dependent on their background, such as the social and the author’s personal context, which may not be fully conveyed from the paintings themselves. This implies that external knowledge on the background of paintings may be needed for answering questions.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2008.12520/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples from the AQUA dataset. There are two different types of QA pairs: generated from paintings (left) and generated from paintings’ comments (right). </figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper offers our first attempt to build a benchmark for question answering on artworks by providing the AQUA (<span id="S1.p4.1.1" class="ltx_text ltx_font_bold">a</span>rt <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">qu</span>estion <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">a</span>nswering) dataset, built upon the SemArt dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, together with a baseline model. The QA pairs (see examples in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) are automatically generated using multiple state-of-the-art question generation methods, which have been studied in the communities of both natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. We use both the paintings themselves and the comments, provided in the SemArt dataset, as the input source for generating QA pairs. In this way, the paintings provide information to generate visual questions (<em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.5" class="ltx_text"></span> “What animal is this?” in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and the comments are used to generate knowledge-based questions about art (<em id="S1.p4.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.7" class="ltx_text"></span> “Who depicts Napoleon in 1814?” in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To address this new QA task, we propose a baseline model specially designed to leverage knowledge information about art, which comes with modality selection on top of recent VQA and text QA models, which is coined as VIKING (<span id="S1.p5.1.1" class="ltx_text ltx_font_bold">vi</span>sual- and <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">k</span>nowkedge-branch network for predict<span id="S1.p5.1.3" class="ltx_text ltx_font_bold">ing</span> answers). This network handles the dual-modality (visual and external knowledge-based) of the art questions with dedicated branches.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix1.1.1.m1.1b"><mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c"><ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p">Firstly, we propose a new task of art question answering, which inherently involves visual understanding of and knowledge on paintings. The latter may be deemed as textual understanding, as such knowledge can be found in books and online documents, <em id="S1.I1.ix1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.I1.ix1.p1.1.2" class="ltx_text"></span> <em id="S1.I1.ix1.p1.1.3" class="ltx_emph ltx_font_italic">Wikipedia</em>. Answering questions that require both visual and textual modalities has not been well explored so far.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix2.1.1.m1.1b"><mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c"><ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p">Secondly, we build a preliminary dataset, AQUA, and make it publicly available.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/noagarcia/ArtVQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/noagarcia/ArtVQA</a></span></span></span> The QA pairs are manually cleansed by crowdsourcing workers with respect to each question’s answerability and grammatical correctness, as well as the answer’s correctness.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix3.1.1.m1.1b"><mo id="S1.I1.ix3.1.1.m1.1.1" xref="S1.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix3.1.1.m1.1c"><ci id="S1.I1.ix3.1.1.m1.1.1.cmml" xref="S1.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p">Thirdly, we present a baseline model, named VIKING, for our art QA task. In addition to the question, the baseline model uses the painting and a paragraph retrieved from a knowledge base to predict the answer that is relevant to both the question and the painting.</p>
</div>
</li>
<li id="S1.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix4.1.1.m1.1b"><mo id="S1.I1.ix4.1.1.m1.1.1" xref="S1.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix4.1.1.m1.1c"><ci id="S1.I1.ix4.1.1.m1.1.1.cmml" xref="S1.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix4.p1" class="ltx_para">
<p id="S1.I1.ix4.p1.1" class="ltx_p">Finally, through the results of this study, we can envisage the challenges and possible solutions that future research aiming to address visual question answering on art must consider.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Computer Vision for Art</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Arts and computer vision have an inevitable link as many artworks have some visual components. One fundamental direction is the digitization of artworks for archiving and restoration (<em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>). Several studies have been done for artworks in the computer vision field, including author/style identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and image retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To the best of our knowledge, this is the first work for question answering on paintings.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Question Generation</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Question Generation</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Visual question generation (VQG) can be categorized into grounded and open-ended <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Grounded VQG generates questions whose answers can be found in the information relevant to the input image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. To this end, captions are first generated from the image, and either rule-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> or neural <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> models are used to further generate questions from the captions. Open-ended VQG are often about abstract concepts such as events and states, which can be inferred by the objects in an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Diversity is crucial for open-ended VQG, for which variational auto-encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and generative adversarial network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have been used.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Textual Question Generation</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">Either rule-based or neural model-based approach has been applied for textual question generation (TQG). Rule-based TQG first constructs question templates either manually <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
or via crowdsourcing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and then applies the templates to generate questions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> pioneers the first neural model for TQG, which apply the sequence-to-sequence model with attention. Neural TQG studies focus on how to encode answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, generate question words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and use paragraph-level context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Visual Question Answering</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Previous VQA studies are on either natural images or videos. Commonly used techniques for image-based VQA include joint visual and language embeddings, and attention mechanisms that model where to look in an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
One extension of VQA is to answer questions on video. Because of the temporal information in videos, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, story understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>,
and temporal coherence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> have been further incorporated. Another interesting extension is to use external knowledge beyond images and videos. The knowledge can be either general <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> or dataset specific <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Because the acquisition of data is not always a trivial task, synthetic datasets have been commonly used by the VQA community. For example, Malinowski and Fritz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> used automatically generated QA pairs based on some templates. Johnson <em id="S2.SS3.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> also employed generated QA pairs on synthetic images, mainly for excluding possible biases in standard datasets. Similarly, our AQUA dataset is also synthetic and, as with the datasets in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, we aim that it serves as a proof-of-concept for VQA on the domain of art.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>AQUA Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use the SemArt dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is originally designed for semantic art understanding, as our source for generating our QA pairs. The SemArt dataset contains paintings and associated comments, where comments are blocks of text, sometimes including the metadata about the painting, such as the author name and created year. They can also have several sentences about the story in the painting and the contextual background when it was created, such as the social and the author’s personal situations. These comments serve as knowledge. In order to show the potentials of AI technologies to comprehend paintings, it is important to explore techniques that work not only on the visual content in the painting themselves but also on their surrounding ideas. We therefore generate QA pairs from visual and knowledge modalities with respective question generation methods.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Question Generation</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Question Generation</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The inherent necessity of visual understanding makes question generation from the image content a tough problem. A number of methods have been proposed so far <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. We try two of them to generate a diverse set of visual questions. The first one is iQAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> trained on VQA v2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The code is reproduced by ourselves, and we confirmed a similar performance to that of the original paper.</span></span></span> which takes an image and an answer word as input and generates a question using a neural network model. We use the object detector provided in Amazon Rekognition<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://aws.amazon.com/rekognition/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/rekognition/</a></span></span></span> to obtain the answer words. The other one uses Pythia<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/facebookresearch/pythia" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/pythia</a></span></span></span> to generated a caption for each painting and transforms each generated caption into a QA pair by applying the rule-based TQG technique described below.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Knowledge-based Question Generation (KQG)</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">For generating questions that involves the knowledge about art, we apply TQG methods, which have been studied by the natural language processing community for the last decades, relying on the natural language knowledge comments available in the SemArt dataset. We tried several TQG approaches, <em id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px2.p1.1.2" class="ltx_text"></span>, rule-based and neural ones. The rule-based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> builds a parsing tree from an input sentence and transforms it to QA pairs based on a set of rules. The resulting QA pairs may be filtered by statistical ranking to drop less-likely samples. The neural approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is based on sequence-to-sequence modeling. We found that the rule-based technique yielded more satisfactory QA pairs.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>QA Pair Evaluation and Cleansing</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Question Generation Evaluation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We use Amazon Mechanical Turk<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="http://www.mturk.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.mturk.com</a></span></span></span> (AMT) to evaluate the quality of our QA pairs, given a painting as well as its associated comment and question, with the following criteria:</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix1.1.1.m1.1b"><mo id="S3.I1.ix1.1.1.m1.1.1" xref="S3.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix1.1.1.m1.1c"><ci id="S3.I1.ix1.1.1.m1.1.1.cmml" xref="S3.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Grammatical correctness</span> measures whether the QA pair is syntactically well-formed, specified by (i) <span id="S3.I1.ix1.p1.1.2" class="ltx_text ltx_font_italic">no grammatical error</span>, (ii) <span id="S3.I1.ix1.p1.1.3" class="ltx_text ltx_font_italic">minor errors</span> (there are some errors but the QA pair still makes sense), and (iii) <span id="S3.I1.ix1.p1.1.4" class="ltx_text ltx_font_italic">major errors</span> (the QA pair does not make any sense).</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix2.1.1.m1.1b"><mo id="S3.I1.ix2.1.1.m1.1.1" xref="S3.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix2.1.1.m1.1c"><ci id="S3.I1.ix2.1.1.m1.1.1.cmml" xref="S3.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Answer existence</span> identifies whether the question has a clear answer in the given painting and comment.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix3.1.1.m1.1b"><mo id="S3.I1.ix3.1.1.m1.1.1" xref="S3.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix3.1.1.m1.1c"><ci id="S3.I1.ix3.1.1.m1.1.1.cmml" xref="S3.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Answer correctness</span> measures given the QA pair whether the answer to the question is correct.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix4.1.1.m1.1b"><mo id="S3.I1.ix4.1.1.m1.1.1" xref="S3.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix4.1.1.m1.1c"><ci id="S3.I1.ix4.1.1.m1.1.1.cmml" xref="S3.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Necessity of visual information</span> evaluates whether the visual information in the painting is needed to answer the question.</p>
</div>
</li>
<li id="S3.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix5.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix5.1.1.m1.1b"><mo id="S3.I1.ix5.1.1.m1.1.1" xref="S3.I1.ix5.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix5.1.1.m1.1c"><ci id="S3.I1.ix5.1.1.m1.1.1.cmml" xref="S3.I1.ix5.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix5.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix5.p1" class="ltx_para">
<p id="S3.I1.ix5.p1.1" class="ltx_p"><span id="S3.I1.ix5.p1.1.1" class="ltx_text ltx_font_bold">Necessity of textual information</span> evaluates if the textual information in the comment is needed to answer the question.</p>
</div>
</li>
<li id="S3.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix6.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix6.1.1.m1.1b"><mo id="S3.I1.ix6.1.1.m1.1.1" xref="S3.I1.ix6.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix6.1.1.m1.1c"><ci id="S3.I1.ix6.1.1.m1.1.1.cmml" xref="S3.I1.ix6.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix6.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix6.p1" class="ltx_para">
<p id="S3.I1.ix6.p1.1" class="ltx_p"><span id="S3.I1.ix6.p1.1.1" class="ltx_text ltx_font_bold">Question reasonability</span> judges whether the QA pair looks like human-generated.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation for question generation by AMT. For grammatical correctness, the proportion of QA pairs with (i) no error and (ii) minor errors are shown, where 0.429 and 0.687 of QA pairs are with no error for VQG and KQG, respectively.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">Criterion</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">VQG</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">KQG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">Grammatical correctness</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">0.936</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">0.871</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Answer existence</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.504</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.842</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Answer correctness</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.337</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.735</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Necessity of visual information</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.977</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.514</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;">Necessity of knowledge</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.098</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">0.935</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">Question reasonability</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">0.691</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">0.690</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">We randomly selected 1,000 and 989 QA pairs from both VQG and KQG, respectively, and evaluated them (Table <a href="#S3.T1" title="Table 1 ‣ 3.2.1 Question Generation Evaluation ‣ 3.2 QA Pair Evaluation and Cleansing ‣ 3 AQUA Dataset ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Our VGQ samples have a high grammatical correctness. However, the answer existence and correctness are low. The errors mainly come from two factors: object detection and visual encoding. Our iQAN-based VQG uses an object as input. If the object detector fails, the answer will be incorrect, which will also affect the question generation. As Amazon Rekognition is trained on real-world photos, it sometimes predicts the objects incorrectly in paintings. For the same reason, the visual encoding in iQAN and the image captioning are not as accurate as that for real-world photo datasets. This explains why many questions do not have answers in the associated painting and comment. The necessity of knowledge is low because our models tend to ask relatively simple visual questions. Yet nearly 70% of QA pairs look like human generated.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">The result for KQG shows that our generated samples also have a high grammatical quality. A common source of negative responses is pronouns in generated answers (<em id="S3.SS2.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS1.p4.1.2" class="ltx_text"></span>, <span id="S3.SS2.SSS1.p4.1.3" class="ltx_text ltx_font_italic">it</span> and <span id="S3.SS2.SSS1.p4.1.4" class="ltx_text ltx_font_italic">they</span>) because our rule-based model does not exclude pronouns in grammar trees from the candidate answer list. For 84% of QA pairs, their answers are found in the context, and 74% of answers are correct; a possible reason for these superior results is that the question and answer are generated together from the same grammar tree. Knowledge is required in over 93% of QA pairs as expected because the questions are coming from the comments. Interestingly, crowd workers tend to find visual information is still necessary even for knowledge QA pairs. The question reasonability criterion shows that most QA pairs are likely to be generated by humans.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Dataset Cleansing and Statistics</h4>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistics on the AQUA dataset.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">Train</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">Val</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"># QA pairs</th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">69,812</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">5,124</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">4,912</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.3.2.1.1" class="ltx_verbatim ltx_font_typewriter"></code> # Visual QA pairs</th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">29,568</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">1,507</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">1,270</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.4.3.1.1" class="ltx_verbatim ltx_font_typewriter"></code> # Knowledge QA pairs</th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">40,244</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">3,617</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">3,642</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<th id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">Question length (in word)</th>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">8.82</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">9.21</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">9.41</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<th id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.6.5.1.1" class="ltx_verbatim ltx_font_typewriter"></code> for visual QA</th>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">6.53</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">6.50</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">6.51</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<th id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.7.6.1.1" class="ltx_verbatim ltx_font_typewriter"></code> for knowledge QA</th>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">10.50</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">10.33</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">10.43</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<th id="S3.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">Answer length</th>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">3.13</td>
<td id="S3.T2.1.8.7.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">3.68</td>
<td id="S3.T2.1.8.7.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">3.85</td>
</tr>
<tr id="S3.T2.1.9.8" class="ltx_tr">
<th id="S3.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.9.8.1.1" class="ltx_verbatim ltx_font_typewriter"></code> for visual QA</th>
<td id="S3.T2.1.9.8.2" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">1.00</td>
<td id="S3.T2.1.9.8.3" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">1.00</td>
<td id="S3.T2.1.9.8.4" class="ltx_td ltx_align_right" style="padding-left:10.0pt;padding-right:10.0pt;">1.00</td>
</tr>
<tr id="S3.T2.1.10.9" class="ltx_tr">
<th id="S3.T2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">
<code id="S3.T2.1.10.9.1.1" class="ltx_verbatim ltx_font_typewriter"></code> for knowledge QA</th>
<td id="S3.T2.1.10.9.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">4.69</td>
<td id="S3.T2.1.10.9.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">4.79</td>
<td id="S3.T2.1.10.9.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">4.85</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">To exclude QA pairs with major grammatical errors or without (correct) answers, we again use AMT. Unlike the evaluation, this time, we only evaluate grammatical correctness as well as answer existence/correctness but on the entire dataset. Table <a href="#S3.T2" title="Table 2 ‣ 3.2.2 Dataset Cleansing and Statistics ‣ 3.2 QA Pair Evaluation and Cleansing ‣ 3 AQUA Dataset ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the statistics of our AQUA dataset after cleansing. Due to low answer existence/correctness, the number of visual QA pairs is smaller than that of knowledge QA pairs. The question length comes with an obvious bias because of the difference in the question generation methods.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Task Definition</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.8" class="ltx_p">With our AQUA dataset, there can be several possible task definitions. In this paper, we focus on the one in which all the comments that are associated with paintings are available. More specifically, let <math id="S3.SS3.p1.1.m1.5" class="ltx_Math" alttext="C=\{c_{i}|i=1,\dots,N\}" display="inline"><semantics id="S3.SS3.p1.1.m1.5a"><mrow id="S3.SS3.p1.1.m1.5.5" xref="S3.SS3.p1.1.m1.5.5.cmml"><mi id="S3.SS3.p1.1.m1.5.5.4" xref="S3.SS3.p1.1.m1.5.5.4.cmml">C</mi><mo id="S3.SS3.p1.1.m1.5.5.3" xref="S3.SS3.p1.1.m1.5.5.3.cmml">=</mo><mrow id="S3.SS3.p1.1.m1.5.5.2.2" xref="S3.SS3.p1.1.m1.5.5.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.5.5.2.2.3" xref="S3.SS3.p1.1.m1.5.5.2.3.1.cmml">{</mo><msub id="S3.SS3.p1.1.m1.4.4.1.1.1" xref="S3.SS3.p1.1.m1.4.4.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.4.4.1.1.1.2" xref="S3.SS3.p1.1.m1.4.4.1.1.1.2.cmml">c</mi><mi id="S3.SS3.p1.1.m1.4.4.1.1.1.3" xref="S3.SS3.p1.1.m1.4.4.1.1.1.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.5.5.2.2.4" xref="S3.SS3.p1.1.m1.5.5.2.3.1.cmml">|</mo><mrow id="S3.SS3.p1.1.m1.5.5.2.2.2" xref="S3.SS3.p1.1.m1.5.5.2.2.2.cmml"><mi id="S3.SS3.p1.1.m1.5.5.2.2.2.2" xref="S3.SS3.p1.1.m1.5.5.2.2.2.2.cmml">i</mi><mo id="S3.SS3.p1.1.m1.5.5.2.2.2.1" xref="S3.SS3.p1.1.m1.5.5.2.2.2.1.cmml">=</mo><mrow id="S3.SS3.p1.1.m1.5.5.2.2.2.3.2" xref="S3.SS3.p1.1.m1.5.5.2.2.2.3.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">1</mn><mo id="S3.SS3.p1.1.m1.5.5.2.2.2.3.2.1" xref="S3.SS3.p1.1.m1.5.5.2.2.2.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml">…</mi><mo id="S3.SS3.p1.1.m1.5.5.2.2.2.3.2.2" xref="S3.SS3.p1.1.m1.5.5.2.2.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.1.m1.3.3" xref="S3.SS3.p1.1.m1.3.3.cmml">N</mi></mrow></mrow><mo stretchy="false" id="S3.SS3.p1.1.m1.5.5.2.2.5" xref="S3.SS3.p1.1.m1.5.5.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.5b"><apply id="S3.SS3.p1.1.m1.5.5.cmml" xref="S3.SS3.p1.1.m1.5.5"><eq id="S3.SS3.p1.1.m1.5.5.3.cmml" xref="S3.SS3.p1.1.m1.5.5.3"></eq><ci id="S3.SS3.p1.1.m1.5.5.4.cmml" xref="S3.SS3.p1.1.m1.5.5.4">𝐶</ci><apply id="S3.SS3.p1.1.m1.5.5.2.3.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.5.5.2.3.1.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2.3">conditional-set</csymbol><apply id="S3.SS3.p1.1.m1.4.4.1.1.1.cmml" xref="S3.SS3.p1.1.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.4.4.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.4.4.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.4.4.1.1.1.2">𝑐</ci><ci id="S3.SS3.p1.1.m1.4.4.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.4.4.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.p1.1.m1.5.5.2.2.2.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2.2"><eq id="S3.SS3.p1.1.m1.5.5.2.2.2.1.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2.2.1"></eq><ci id="S3.SS3.p1.1.m1.5.5.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2.2.2">𝑖</ci><list id="S3.SS3.p1.1.m1.5.5.2.2.2.3.1.cmml" xref="S3.SS3.p1.1.m1.5.5.2.2.2.3.2"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">1</cn><ci id="S3.SS3.p1.1.m1.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2">…</ci><ci id="S3.SS3.p1.1.m1.3.3.cmml" xref="S3.SS3.p1.1.m1.3.3">𝑁</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.5c">C=\{c_{i}|i=1,\dots,N\}</annotation></semantics></math> denote the set of all the comments. The aim of AQUA task is to answer question <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">q</annotation></semantics></math> given painting <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">v</annotation></semantics></math> using <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">C</annotation></semantics></math>, without an explicit association between <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">v</annotation></semantics></math> and a specific comment in <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">C</annotation></semantics></math>. In this task, <math id="S3.SS3.p1.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p1.7.m7.1a"><mi id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><ci id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">C</annotation></semantics></math> can be viewed as an external source of knowledge, containing the necessary information to answer the question when the comment associated with <math id="S3.SS3.p1.8.m8.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS3.p1.8.m8.1a"><mi id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><ci id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">q</annotation></semantics></math> is correctly retrieved.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">A more challenging extension of this task is to not use <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">C</annotation></semantics></math> but other sources of knowledge, <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS3.p2.1.2" class="ltx_text"></span> <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">Wikipedia</span>. With this extension, the performance also depends on the quality of the sources and their affinity to the original source. We leave the extension as future work.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>VIKING Model</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">By construction, the AQUA dataset contains two types of questions. We design our baseline model, coined VIKING, to handle them with dedicated branches. Figure <a href="#S4.F2" title="Figure 2 ‣ 4 VIKING Model ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the overall pipeline. Inspired by the intuition that humans first look into the given question and then try to locate the required information to answer it (in our case, either the associated painting or comment), VIKING consists of three main components: The question and painting are first fed into a <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">modality selector</span>, which classifies the question into visual or knowledge-based ones. Questions about the visual content go through the <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">visual QA branch</span>. Otherwise, questions are passed to the <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">knowledge QA branch</span>, in which an associated comment is retrieved from <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">C</annotation></semantics></math>. We detail these three components below.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2008.12520/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our VIKING model for the AQUA dataset.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Modality Selector</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.10" class="ltx_p">Our modality selector <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">S</annotation></semantics></math> classifies a question <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">q</annotation></semantics></math> into these two modalities given <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">q</annotation></semantics></math> and <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">v</annotation></semantics></math>, so that it can go through the corresponding branch. We use pretrained BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as the question encoder. More specifically, question <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">q</annotation></semantics></math> is encoded into a 1,024 dimensional vector <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\mathbf{q}</annotation></semantics></math>, which is BERT’s output corresponding to the special token <span id="S4.SS1.p1.10.1" class="ltx_text ltx_font_typewriter">[CLS]</span>. For our painting encoder, we use pretrained ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to encode the painting into a 2,048 dimensional vector <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">𝐯</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">𝐯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">\mathbf{v}</annotation></semantics></math>. We concatenate <math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathbf{q}" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">𝐪</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">𝐪</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">\mathbf{q}</annotation></semantics></math> and <math id="S4.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathbf{v}" display="inline"><semantics id="S4.SS1.p1.9.m9.1a"><mi id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">𝐯</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><ci id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1">𝐯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">\mathbf{v}</annotation></semantics></math> into a vector <math id="S4.SS1.p1.10.m10.1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><semantics id="S4.SS1.p1.10.m10.1a"><mi id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><ci id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">\mathbf{x}</annotation></semantics></math> and train a logistic regression model</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.3" class="ltx_Math" alttext="S(\mathbf{x})=\frac{1}{1+e^{-(\mathbf{w}_{\text{s}}^{\top}\mathbf{x}-b_{\text{s}})}}," display="block"><semantics id="S4.Ex1.m1.3a"><mrow id="S4.Ex1.m1.3.3.1" xref="S4.Ex1.m1.3.3.1.1.cmml"><mrow id="S4.Ex1.m1.3.3.1.1" xref="S4.Ex1.m1.3.3.1.1.cmml"><mrow id="S4.Ex1.m1.3.3.1.1.2" xref="S4.Ex1.m1.3.3.1.1.2.cmml"><mi id="S4.Ex1.m1.3.3.1.1.2.2" xref="S4.Ex1.m1.3.3.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.3.3.1.1.2.1" xref="S4.Ex1.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S4.Ex1.m1.3.3.1.1.2.3.2" xref="S4.Ex1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.Ex1.m1.3.3.1.1.2.3.2.1" xref="S4.Ex1.m1.3.3.1.1.2.cmml">(</mo><mi id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml">𝐱</mi><mo stretchy="false" id="S4.Ex1.m1.3.3.1.1.2.3.2.2" xref="S4.Ex1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.3.3.1.1.1" xref="S4.Ex1.m1.3.3.1.1.1.cmml">=</mo><mfrac id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml"><mn id="S4.Ex1.m1.1.1.3" xref="S4.Ex1.m1.1.1.3.cmml">1</mn><mrow id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.cmml"><mn id="S4.Ex1.m1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.3.cmml">1</mn><mo id="S4.Ex1.m1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.2.cmml">+</mo><msup id="S4.Ex1.m1.1.1.1.4" xref="S4.Ex1.m1.1.1.1.4.cmml"><mi id="S4.Ex1.m1.1.1.1.4.2" xref="S4.Ex1.m1.1.1.1.4.2.cmml">e</mi><mrow id="S4.Ex1.m1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.cmml"><mo id="S4.Ex1.m1.1.1.1.1.1a" xref="S4.Ex1.m1.1.1.1.1.1.cmml">−</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝐰</mi><mtext id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml">s</mtext><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">⊤</mo></msubsup><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml">𝐱</mi></mrow><mo id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mtext id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3a.cmml">s</mtext></msub></mrow><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow></mfrac></mrow><mo id="S4.Ex1.m1.3.3.1.2" xref="S4.Ex1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.3b"><apply id="S4.Ex1.m1.3.3.1.1.cmml" xref="S4.Ex1.m1.3.3.1"><eq id="S4.Ex1.m1.3.3.1.1.1.cmml" xref="S4.Ex1.m1.3.3.1.1.1"></eq><apply id="S4.Ex1.m1.3.3.1.1.2.cmml" xref="S4.Ex1.m1.3.3.1.1.2"><times id="S4.Ex1.m1.3.3.1.1.2.1.cmml" xref="S4.Ex1.m1.3.3.1.1.2.1"></times><ci id="S4.Ex1.m1.3.3.1.1.2.2.cmml" xref="S4.Ex1.m1.3.3.1.1.2.2">𝑆</ci><ci id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2">𝐱</ci></apply><apply id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"><divide id="S4.Ex1.m1.1.1.2.cmml" xref="S4.Ex1.m1.1.1"></divide><cn type="integer" id="S4.Ex1.m1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.3">1</cn><apply id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"><plus id="S4.Ex1.m1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.2"></plus><cn type="integer" id="S4.Ex1.m1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.3">1</cn><apply id="S4.Ex1.m1.1.1.1.4.cmml" xref="S4.Ex1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.4.1.cmml" xref="S4.Ex1.m1.1.1.1.4">superscript</csymbol><ci id="S4.Ex1.m1.1.1.1.4.2.cmml" xref="S4.Ex1.m1.1.1.1.4.2">𝑒</ci><apply id="S4.Ex1.m1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1"><minus id="S4.Ex1.m1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1"></minus><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1"><minus id="S4.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2"><times id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝐰</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3"><mtext mathsize="50%" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.2.3">top</csymbol></apply><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.2.3">𝐱</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="50%" id="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1.1.3.3">s</mtext></ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.3c">S(\mathbf{x})=\frac{1}{1+e^{-(\mathbf{w}_{\text{s}}^{\top}\mathbf{x}-b_{\text{s}})}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p1.14" class="ltx_p">where <math id="S4.SS1.p1.11.m1.1" class="ltx_Math" alttext="\mathbf{w}_{\text{s}}" display="inline"><semantics id="S4.SS1.p1.11.m1.1a"><msub id="S4.SS1.p1.11.m1.1.1" xref="S4.SS1.p1.11.m1.1.1.cmml"><mi id="S4.SS1.p1.11.m1.1.1.2" xref="S4.SS1.p1.11.m1.1.1.2.cmml">𝐰</mi><mtext id="S4.SS1.p1.11.m1.1.1.3" xref="S4.SS1.p1.11.m1.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.11.m1.1b"><apply id="S4.SS1.p1.11.m1.1.1.cmml" xref="S4.SS1.p1.11.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.11.m1.1.1.1.cmml" xref="S4.SS1.p1.11.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.11.m1.1.1.2.cmml" xref="S4.SS1.p1.11.m1.1.1.2">𝐰</ci><ci id="S4.SS1.p1.11.m1.1.1.3a.cmml" xref="S4.SS1.p1.11.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.p1.11.m1.1.1.3.cmml" xref="S4.SS1.p1.11.m1.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.11.m1.1c">\mathbf{w}_{\text{s}}</annotation></semantics></math> and <math id="S4.SS1.p1.12.m2.1" class="ltx_Math" alttext="b_{\text{s}}" display="inline"><semantics id="S4.SS1.p1.12.m2.1a"><msub id="S4.SS1.p1.12.m2.1.1" xref="S4.SS1.p1.12.m2.1.1.cmml"><mi id="S4.SS1.p1.12.m2.1.1.2" xref="S4.SS1.p1.12.m2.1.1.2.cmml">b</mi><mtext id="S4.SS1.p1.12.m2.1.1.3" xref="S4.SS1.p1.12.m2.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.12.m2.1b"><apply id="S4.SS1.p1.12.m2.1.1.cmml" xref="S4.SS1.p1.12.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m2.1.1.1.cmml" xref="S4.SS1.p1.12.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.12.m2.1.1.2.cmml" xref="S4.SS1.p1.12.m2.1.1.2">𝑏</ci><ci id="S4.SS1.p1.12.m2.1.1.3a.cmml" xref="S4.SS1.p1.12.m2.1.1.3"><mtext mathsize="70%" id="S4.SS1.p1.12.m2.1.1.3.cmml" xref="S4.SS1.p1.12.m2.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.12.m2.1c">b_{\text{s}}</annotation></semantics></math> are a trainable vector and scalar. Question <math id="S4.SS1.p1.13.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS1.p1.13.m3.1a"><mi id="S4.SS1.p1.13.m3.1.1" xref="S4.SS1.p1.13.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.13.m3.1b"><ci id="S4.SS1.p1.13.m3.1.1.cmml" xref="S4.SS1.p1.13.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.13.m3.1c">q</annotation></semantics></math> is passed to the visual QA branch when <math id="S4.SS1.p1.14.m4.1" class="ltx_Math" alttext="S(\mathbf{x})&gt;0.5" display="inline"><semantics id="S4.SS1.p1.14.m4.1a"><mrow id="S4.SS1.p1.14.m4.1.2" xref="S4.SS1.p1.14.m4.1.2.cmml"><mrow id="S4.SS1.p1.14.m4.1.2.2" xref="S4.SS1.p1.14.m4.1.2.2.cmml"><mi id="S4.SS1.p1.14.m4.1.2.2.2" xref="S4.SS1.p1.14.m4.1.2.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.14.m4.1.2.2.1" xref="S4.SS1.p1.14.m4.1.2.2.1.cmml">​</mo><mrow id="S4.SS1.p1.14.m4.1.2.2.3.2" xref="S4.SS1.p1.14.m4.1.2.2.cmml"><mo stretchy="false" id="S4.SS1.p1.14.m4.1.2.2.3.2.1" xref="S4.SS1.p1.14.m4.1.2.2.cmml">(</mo><mi id="S4.SS1.p1.14.m4.1.1" xref="S4.SS1.p1.14.m4.1.1.cmml">𝐱</mi><mo stretchy="false" id="S4.SS1.p1.14.m4.1.2.2.3.2.2" xref="S4.SS1.p1.14.m4.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.14.m4.1.2.1" xref="S4.SS1.p1.14.m4.1.2.1.cmml">&gt;</mo><mn id="S4.SS1.p1.14.m4.1.2.3" xref="S4.SS1.p1.14.m4.1.2.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.14.m4.1b"><apply id="S4.SS1.p1.14.m4.1.2.cmml" xref="S4.SS1.p1.14.m4.1.2"><gt id="S4.SS1.p1.14.m4.1.2.1.cmml" xref="S4.SS1.p1.14.m4.1.2.1"></gt><apply id="S4.SS1.p1.14.m4.1.2.2.cmml" xref="S4.SS1.p1.14.m4.1.2.2"><times id="S4.SS1.p1.14.m4.1.2.2.1.cmml" xref="S4.SS1.p1.14.m4.1.2.2.1"></times><ci id="S4.SS1.p1.14.m4.1.2.2.2.cmml" xref="S4.SS1.p1.14.m4.1.2.2.2">𝑆</ci><ci id="S4.SS1.p1.14.m4.1.1.cmml" xref="S4.SS1.p1.14.m4.1.1">𝐱</ci></apply><cn type="float" id="S4.SS1.p1.14.m4.1.2.3.cmml" xref="S4.SS1.p1.14.m4.1.2.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.14.m4.1c">S(\mathbf{x})&gt;0.5</annotation></semantics></math> and to the knowledge QA branch otherwise.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Visual QA Branch</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Visual questions can be answered solely based on the associated painting without any external knowledge. For this question type, the task is reduced to VQA over paintings.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">We again use iQAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> as our visual QA branch, which is a dual model and can take either a question or an answer as input, then output the counterpart. We separately train the iQAN model over the training split of our AQUA dataset (<em id="S4.SS2.p2.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.p2.2.2" class="ltx_text"></span>, we do not use the iQAN model trained for question generation). This branch produces a predicted answer <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="a_{v}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">a</mi><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑎</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">a_{v}</annotation></semantics></math>, which is from the answer vocabulary <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">A</annotation></semantics></math> consisting of the top-5,000 most common words in the training split.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Knowledge QA Branch</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.10" class="ltx_p">Questions classified as knowledge-based are fed to the knowledge QA branch. We first retrieve the comment in <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">C</annotation></semantics></math> that is the most relevant to <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">q</annotation></semantics></math> with a two-stage strategy. In the first stage, we apply TF-IDF to rank all the comments in <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">C</annotation></semantics></math> with respect to their relevance to <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mi id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><ci id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">q</annotation></semantics></math> and obtain the subset <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="C_{q}" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><msub id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">𝐶</ci><ci id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">C_{q}</annotation></semantics></math> consisting of the top-10 most relevant comments. In the second stage, comments in <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="C_{q}" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><msub id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mi id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.6.m6.1.1.3" xref="S4.SS3.p1.6.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">𝐶</ci><ci id="S4.SS3.p1.6.m6.1.1.3.cmml" xref="S4.SS3.p1.6.m6.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">C_{q}</annotation></semantics></math> are re-ranked using BERT to find the most relevant comment <math id="S4.SS3.p1.7.m7.1" class="ltx_Math" alttext="c_{q}" display="inline"><semantics id="S4.SS3.p1.7.m7.1a"><msub id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml"><mi id="S4.SS3.p1.7.m7.1.1.2" xref="S4.SS3.p1.7.m7.1.1.2.cmml">c</mi><mi id="S4.SS3.p1.7.m7.1.1.3" xref="S4.SS3.p1.7.m7.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><apply id="S4.SS3.p1.7.m7.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS3.p1.7.m7.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1.2">𝑐</ci><ci id="S4.SS3.p1.7.m7.1.1.3.cmml" xref="S4.SS3.p1.7.m7.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">c_{q}</annotation></semantics></math>. This two-stage strategy drastically reduces the computational cost compared to using BERT-based ranking directly over <math id="S4.SS3.p1.8.m8.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.p1.8.m8.1a"><mi id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.1b"><ci id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.1c">C</annotation></semantics></math>. Finally, the answer is predicted based on the question <math id="S4.SS3.p1.9.m9.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.p1.9.m9.1a"><mi id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.1b"><ci id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.1c">q</annotation></semantics></math> and the retrieved comment <math id="S4.SS3.p1.10.m10.1" class="ltx_Math" alttext="c_{q}" display="inline"><semantics id="S4.SS3.p1.10.m10.1a"><msub id="S4.SS3.p1.10.m10.1.1" xref="S4.SS3.p1.10.m10.1.1.cmml"><mi id="S4.SS3.p1.10.m10.1.1.2" xref="S4.SS3.p1.10.m10.1.1.2.cmml">c</mi><mi id="S4.SS3.p1.10.m10.1.1.3" xref="S4.SS3.p1.10.m10.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.10.m10.1b"><apply id="S4.SS3.p1.10.m10.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.10.m10.1.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS3.p1.10.m10.1.1.2.cmml" xref="S4.SS3.p1.10.m10.1.1.2">𝑐</ci><ci id="S4.SS3.p1.10.m10.1.1.3.cmml" xref="S4.SS3.p1.10.m10.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.10.m10.1c">c_{q}</annotation></semantics></math> with a XLNet-based model.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Two-Stage External Knowledge Retrieval</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.3" class="ltx_p">Finding the relevant comment <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="c_{q}" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><msub id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p1.1.m1.1.1.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">c_{q}</annotation></semantics></math> is critical for this task since it contains the answer. A naive approach is to train a ranking network and apply it to all comments in <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mi id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><ci id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">C</annotation></semantics></math>. This approach can be computationally expensive when <math id="S4.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><mi id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><ci id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">C</annotation></semantics></math> contains a large number of comments and an expensive model, such as Transformer-based ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, is used. We thus take a two-stage approach for retrieving knowledge.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.12" class="ltx_p">For the first stage, we adopt TF-IDF to encode both <math id="S4.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mi id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><ci id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">q</annotation></semantics></math> and all <math id="S4.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="c_{i}\in C" display="inline"><semantics id="S4.SS3.SSS1.p2.2.m2.1a"><mrow id="S4.SS3.SSS1.p2.2.m2.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.cmml"><msub id="S4.SS3.SSS1.p2.2.m2.1.1.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml"><mi id="S4.SS3.SSS1.p2.2.m2.1.1.2.2" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.2.cmml">c</mi><mi id="S4.SS3.SSS1.p2.2.m2.1.1.2.3" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS3.SSS1.p2.2.m2.1.1.1" xref="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml">∈</mo><mi id="S4.SS3.SSS1.p2.2.m2.1.1.3" xref="S4.SS3.SSS1.p2.2.m2.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.2.m2.1b"><apply id="S4.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1"><in id="S4.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.1"></in><apply id="S4.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p2.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.2">𝑐</ci><ci id="S4.SS3.SSS1.p2.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S4.SS3.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p2.2.m2.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.2.m2.1c">c_{i}\in C</annotation></semantics></math>. Letting <math id="S4.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\hat{\mathbf{q}}" display="inline"><semantics id="S4.SS3.SSS1.p2.3.m3.1a"><mover accent="true" id="S4.SS3.SSS1.p2.3.m3.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p2.3.m3.1.1.2" xref="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml">𝐪</mi><mo id="S4.SS3.SSS1.p2.3.m3.1.1.1" xref="S4.SS3.SSS1.p2.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.3.m3.1b"><apply id="S4.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1"><ci id="S4.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.1">^</ci><ci id="S4.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p2.3.m3.1.1.2">𝐪</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.3.m3.1c">\hat{\mathbf{q}}</annotation></semantics></math> and <math id="S4.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\hat{\mathbf{c}}_{i}" display="inline"><semantics id="S4.SS3.SSS1.p2.4.m4.1a"><msub id="S4.SS3.SSS1.p2.4.m4.1.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.cmml"><mover accent="true" id="S4.SS3.SSS1.p2.4.m4.1.1.2" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.cmml"><mi id="S4.SS3.SSS1.p2.4.m4.1.1.2.2" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.2.cmml">𝐜</mi><mo id="S4.SS3.SSS1.p2.4.m4.1.1.2.1" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS3.SSS1.p2.4.m4.1.1.3" xref="S4.SS3.SSS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.4.m4.1b"><apply id="S4.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.4.m4.1.1.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1">subscript</csymbol><apply id="S4.SS3.SSS1.p2.4.m4.1.1.2.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.2"><ci id="S4.SS3.SSS1.p2.4.m4.1.1.2.1.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.1">^</ci><ci id="S4.SS3.SSS1.p2.4.m4.1.1.2.2.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.2.2">𝐜</ci></apply><ci id="S4.SS3.SSS1.p2.4.m4.1.1.3.cmml" xref="S4.SS3.SSS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.4.m4.1c">\hat{\mathbf{c}}_{i}</annotation></semantics></math> be the respective TF-IDF vectors, we compute the score <math id="S4.SS3.SSS1.p2.5.m5.2" class="ltx_Math" alttext="s_{i}=\hat{\mathbf{q}}^{\top}\hat{\mathbf{c}}_{i}/(\|\hat{\mathbf{q}}\|\,\|\hat{\mathbf{c}}_{i}\|)" display="inline"><semantics id="S4.SS3.SSS1.p2.5.m5.2a"><mrow id="S4.SS3.SSS1.p2.5.m5.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.cmml"><msub id="S4.SS3.SSS1.p2.5.m5.2.2.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.3.cmml"><mi id="S4.SS3.SSS1.p2.5.m5.2.2.3.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.3.2.cmml">s</mi><mi id="S4.SS3.SSS1.p2.5.m5.2.2.3.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.3.3.cmml">i</mi></msub><mo id="S4.SS3.SSS1.p2.5.m5.2.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.2.cmml">=</mo><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.cmml"><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.cmml"><msup id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.cmml"><mover accent="true" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.cmml"><mi id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.2.cmml">𝐪</mi><mo id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.1.cmml">^</mo></mover><mo id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.3.cmml">⊤</mo></msup><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.1.cmml">​</mo><msub id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.cmml"><mover accent="true" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.cmml"><mi id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.2.cmml">𝐜</mi><mo id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.1.cmml">^</mo></mover><mi id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.3.cmml">i</mi></msub></mrow><mo id="S4.SS3.SSS1.p2.5.m5.2.2.1.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.2.cmml">/</mo><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.cmml"><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.2.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.1.1.cmml">‖</mo><mover accent="true" id="S4.SS3.SSS1.p2.5.m5.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.cmml"><mi id="S4.SS3.SSS1.p2.5.m5.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml">𝐪</mi><mo id="S4.SS3.SSS1.p2.5.m5.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.1.1.cmml">‖</mo></mrow><mo lspace="0.170em" rspace="0em" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.1.cmml">‖</mo><msub id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.2" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.2.cmml">𝐜</mi><mo id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.1" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow><mo stretchy="false" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.3" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.5.m5.2b"><apply id="S4.SS3.SSS1.p2.5.m5.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2"><eq id="S4.SS3.SSS1.p2.5.m5.2.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.2"></eq><apply id="S4.SS3.SSS1.p2.5.m5.2.2.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.5.m5.2.2.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.3">subscript</csymbol><ci id="S4.SS3.SSS1.p2.5.m5.2.2.3.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.3.2">𝑠</ci><ci id="S4.SS3.SSS1.p2.5.m5.2.2.3.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.3.3">𝑖</ci></apply><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1"><divide id="S4.SS3.SSS1.p2.5.m5.2.2.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.2"></divide><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3"><times id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.1"></times><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2">superscript</csymbol><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2"><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.1">^</ci><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.2.2">𝐪</ci></apply><csymbol cd="latexml" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.2.3">top</csymbol></apply><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3">subscript</csymbol><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2"><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.1">^</ci><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.2.2">𝐜</ci></apply><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.3.3.3">𝑖</ci></apply></apply><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1"><times id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.2"></times><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.2"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.3.2.1">norm</csymbol><apply id="S4.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1"><ci id="S4.SS3.SSS1.p2.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.1">^</ci><ci id="S4.SS3.SSS1.p2.5.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.1.1.2">𝐪</ci></apply></apply><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2"><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.2.2">𝐜</ci></apply><ci id="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.5.m5.2c">s_{i}=\hat{\mathbf{q}}^{\top}\hat{\mathbf{c}}_{i}/(\|\hat{\mathbf{q}}\|\,\|\hat{\mathbf{c}}_{i}\|)</annotation></semantics></math>. The set <math id="S4.SS3.SSS1.p2.6.m6.1" class="ltx_Math" alttext="C_{q}" display="inline"><semantics id="S4.SS3.SSS1.p2.6.m6.1a"><msub id="S4.SS3.SSS1.p2.6.m6.1.1" xref="S4.SS3.SSS1.p2.6.m6.1.1.cmml"><mi id="S4.SS3.SSS1.p2.6.m6.1.1.2" xref="S4.SS3.SSS1.p2.6.m6.1.1.2.cmml">C</mi><mi id="S4.SS3.SSS1.p2.6.m6.1.1.3" xref="S4.SS3.SSS1.p2.6.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.6.m6.1b"><apply id="S4.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.6.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p2.6.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1.2">𝐶</ci><ci id="S4.SS3.SSS1.p2.6.m6.1.1.3.cmml" xref="S4.SS3.SSS1.p2.6.m6.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.6.m6.1c">C_{q}</annotation></semantics></math> consists of <math id="S4.SS3.SSS1.p2.7.m7.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS1.p2.7.m7.1a"><msub id="S4.SS3.SSS1.p2.7.m7.1.1" xref="S4.SS3.SSS1.p2.7.m7.1.1.cmml"><mi id="S4.SS3.SSS1.p2.7.m7.1.1.2" xref="S4.SS3.SSS1.p2.7.m7.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p2.7.m7.1.1.3" xref="S4.SS3.SSS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.7.m7.1b"><apply id="S4.SS3.SSS1.p2.7.m7.1.1.cmml" xref="S4.SS3.SSS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.7.m7.1.1.1.cmml" xref="S4.SS3.SSS1.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p2.7.m7.1.1.2.cmml" xref="S4.SS3.SSS1.p2.7.m7.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p2.7.m7.1.1.3.cmml" xref="S4.SS3.SSS1.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.7.m7.1c">c_{i}</annotation></semantics></math>’s that have the 10 highest <math id="S4.SS3.SSS1.p2.8.m8.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S4.SS3.SSS1.p2.8.m8.1a"><msub id="S4.SS3.SSS1.p2.8.m8.1.1" xref="S4.SS3.SSS1.p2.8.m8.1.1.cmml"><mi id="S4.SS3.SSS1.p2.8.m8.1.1.2" xref="S4.SS3.SSS1.p2.8.m8.1.1.2.cmml">s</mi><mi id="S4.SS3.SSS1.p2.8.m8.1.1.3" xref="S4.SS3.SSS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.8.m8.1b"><apply id="S4.SS3.SSS1.p2.8.m8.1.1.cmml" xref="S4.SS3.SSS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.8.m8.1.1.1.cmml" xref="S4.SS3.SSS1.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p2.8.m8.1.1.2.cmml" xref="S4.SS3.SSS1.p2.8.m8.1.1.2">𝑠</ci><ci id="S4.SS3.SSS1.p2.8.m8.1.1.3.cmml" xref="S4.SS3.SSS1.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.8.m8.1c">s_{i}</annotation></semantics></math>’s. In order to improve the ranking accuracy, we apply to both <math id="S4.SS3.SSS1.p2.9.m9.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS1.p2.9.m9.1a"><mi id="S4.SS3.SSS1.p2.9.m9.1.1" xref="S4.SS3.SSS1.p2.9.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.9.m9.1b"><ci id="S4.SS3.SSS1.p2.9.m9.1.1.cmml" xref="S4.SS3.SSS1.p2.9.m9.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.9.m9.1c">q</annotation></semantics></math> and <math id="S4.SS3.SSS1.p2.10.m10.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS1.p2.10.m10.1a"><msub id="S4.SS3.SSS1.p2.10.m10.1.1" xref="S4.SS3.SSS1.p2.10.m10.1.1.cmml"><mi id="S4.SS3.SSS1.p2.10.m10.1.1.2" xref="S4.SS3.SSS1.p2.10.m10.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p2.10.m10.1.1.3" xref="S4.SS3.SSS1.p2.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.10.m10.1b"><apply id="S4.SS3.SSS1.p2.10.m10.1.1.cmml" xref="S4.SS3.SSS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.10.m10.1.1.1.cmml" xref="S4.SS3.SSS1.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p2.10.m10.1.1.2.cmml" xref="S4.SS3.SSS1.p2.10.m10.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p2.10.m10.1.1.3.cmml" xref="S4.SS3.SSS1.p2.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.10.m10.1c">c_{i}</annotation></semantics></math> (i) preprocessing with NLTK<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://www.nltk.org/</span></span></span> for stop word removal and word stemming and (ii) <math id="S4.SS3.SSS1.p2.11.m11.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS3.SSS1.p2.11.m11.1a"><mi id="S4.SS3.SSS1.p2.11.m11.1.1" xref="S4.SS3.SSS1.p2.11.m11.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.11.m11.1b"><ci id="S4.SS3.SSS1.p2.11.m11.1.1.cmml" xref="S4.SS3.SSS1.p2.11.m11.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.11.m11.1c">n</annotation></semantics></math>-gram TF-IDF where <math id="S4.SS3.SSS1.p2.12.m12.1" class="ltx_Math" alttext="n=3" display="inline"><semantics id="S4.SS3.SSS1.p2.12.m12.1a"><mrow id="S4.SS3.SSS1.p2.12.m12.1.1" xref="S4.SS3.SSS1.p2.12.m12.1.1.cmml"><mi id="S4.SS3.SSS1.p2.12.m12.1.1.2" xref="S4.SS3.SSS1.p2.12.m12.1.1.2.cmml">n</mi><mo id="S4.SS3.SSS1.p2.12.m12.1.1.1" xref="S4.SS3.SSS1.p2.12.m12.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p2.12.m12.1.1.3" xref="S4.SS3.SSS1.p2.12.m12.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.12.m12.1b"><apply id="S4.SS3.SSS1.p2.12.m12.1.1.cmml" xref="S4.SS3.SSS1.p2.12.m12.1.1"><eq id="S4.SS3.SSS1.p2.12.m12.1.1.1.cmml" xref="S4.SS3.SSS1.p2.12.m12.1.1.1"></eq><ci id="S4.SS3.SSS1.p2.12.m12.1.1.2.cmml" xref="S4.SS3.SSS1.p2.12.m12.1.1.2">𝑛</ci><cn type="integer" id="S4.SS3.SSS1.p2.12.m12.1.1.3.cmml" xref="S4.SS3.SSS1.p2.12.m12.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.12.m12.1c">n=3</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.6" class="ltx_p">The second stage further ranks <math id="S4.SS3.SSS1.p3.1.m1.1" class="ltx_Math" alttext="c_{i}\in C_{q}" display="inline"><semantics id="S4.SS3.SSS1.p3.1.m1.1a"><mrow id="S4.SS3.SSS1.p3.1.m1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml"><msub id="S4.SS3.SSS1.p3.1.m1.1.1.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml"><mi id="S4.SS3.SSS1.p3.1.m1.1.1.2.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.2.cmml">c</mi><mi id="S4.SS3.SSS1.p3.1.m1.1.1.2.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS3.SSS1.p3.1.m1.1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml">∈</mo><msub id="S4.SS3.SSS1.p3.1.m1.1.1.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS3.SSS1.p3.1.m1.1.1.3.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.2.cmml">C</mi><mi id="S4.SS3.SSS1.p3.1.m1.1.1.3.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3.cmml">q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.1.m1.1b"><apply id="S4.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"><in id="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.1"></in><apply id="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p3.1.m1.1.1.2.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.2">𝑐</ci><ci id="S4.SS3.SSS1.p3.1.m1.1.1.2.3.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.2">𝐶</ci><ci id="S4.SS3.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.1.m1.1c">c_{i}\in C_{q}</annotation></semantics></math> to find the comment associated with the question. We cast this into a sentence pair classification problem and use a BERT-based model to predict how likely a given <math id="S4.SS3.SSS1.p3.2.m2.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS1.p3.2.m2.1a"><msub id="S4.SS3.SSS1.p3.2.m2.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p3.2.m2.1.1.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p3.2.m2.1.1.3" xref="S4.SS3.SSS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.2.m2.1b"><apply id="S4.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.2.m2.1c">c_{i}</annotation></semantics></math> is relevant to <math id="S4.SS3.SSS1.p3.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS1.p3.3.m3.1a"><mi id="S4.SS3.SSS1.p3.3.m3.1.1" xref="S4.SS3.SSS1.p3.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.3.m3.1b"><ci id="S4.SS3.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p3.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.3.m3.1c">q</annotation></semantics></math>. We concatenate <math id="S4.SS3.SSS1.p3.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS1.p3.4.m4.1a"><mi id="S4.SS3.SSS1.p3.4.m4.1.1" xref="S4.SS3.SSS1.p3.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.4.m4.1b"><ci id="S4.SS3.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p3.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.4.m4.1c">q</annotation></semantics></math> and each <math id="S4.SS3.SSS1.p3.5.m5.1" class="ltx_Math" alttext="c_{i}\in C_{q}" display="inline"><semantics id="S4.SS3.SSS1.p3.5.m5.1a"><mrow id="S4.SS3.SSS1.p3.5.m5.1.1" xref="S4.SS3.SSS1.p3.5.m5.1.1.cmml"><msub id="S4.SS3.SSS1.p3.5.m5.1.1.2" xref="S4.SS3.SSS1.p3.5.m5.1.1.2.cmml"><mi id="S4.SS3.SSS1.p3.5.m5.1.1.2.2" xref="S4.SS3.SSS1.p3.5.m5.1.1.2.2.cmml">c</mi><mi id="S4.SS3.SSS1.p3.5.m5.1.1.2.3" xref="S4.SS3.SSS1.p3.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS3.SSS1.p3.5.m5.1.1.1" xref="S4.SS3.SSS1.p3.5.m5.1.1.1.cmml">∈</mo><msub id="S4.SS3.SSS1.p3.5.m5.1.1.3" xref="S4.SS3.SSS1.p3.5.m5.1.1.3.cmml"><mi id="S4.SS3.SSS1.p3.5.m5.1.1.3.2" xref="S4.SS3.SSS1.p3.5.m5.1.1.3.2.cmml">C</mi><mi id="S4.SS3.SSS1.p3.5.m5.1.1.3.3" xref="S4.SS3.SSS1.p3.5.m5.1.1.3.3.cmml">q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.5.m5.1b"><apply id="S4.SS3.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1"><in id="S4.SS3.SSS1.p3.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.1"></in><apply id="S4.SS3.SSS1.p3.5.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.5.m5.1.1.2.1.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS3.SSS1.p3.5.m5.1.1.2.2.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.2.2">𝑐</ci><ci id="S4.SS3.SSS1.p3.5.m5.1.1.2.3.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S4.SS3.SSS1.p3.5.m5.1.1.3.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.5.m5.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS1.p3.5.m5.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.3.2">𝐶</ci><ci id="S4.SS3.SSS1.p3.5.m5.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.5.m5.1.1.3.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.5.m5.1c">c_{i}\in C_{q}</annotation></semantics></math> with <span id="S4.SS3.SSS1.p3.6.1" class="ltx_text ltx_font_typewriter">[SEP]</span> and feed it to the pretrained BERT. The output <math id="S4.SS3.SSS1.p3.6.m6.1" class="ltx_Math" alttext="{o}_{i}" display="inline"><semantics id="S4.SS3.SSS1.p3.6.m6.1a"><msub id="S4.SS3.SSS1.p3.6.m6.1.1" xref="S4.SS3.SSS1.p3.6.m6.1.1.cmml"><mi id="S4.SS3.SSS1.p3.6.m6.1.1.2" xref="S4.SS3.SSS1.p3.6.m6.1.1.2.cmml">o</mi><mi id="S4.SS3.SSS1.p3.6.m6.1.1.3" xref="S4.SS3.SSS1.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.6.m6.1b"><apply id="S4.SS3.SSS1.p3.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.6.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p3.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.6.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p3.6.m6.1.1.2">𝑜</ci><ci id="S4.SS3.SSS1.p3.6.m6.1.1.3.cmml" xref="S4.SS3.SSS1.p3.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.6.m6.1c">{o}_{i}</annotation></semantics></math> associated with <span id="S4.SS3.SSS1.p3.6.2" class="ltx_text ltx_font_typewriter">[CLS]</span> is passed to a logistic regression model, given by</p>
<table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex2.m1.2" class="ltx_Math" alttext="R(\mathbf{o}_{i})=\frac{1}{1+e^{-(\mathbf{w}_{\text{r}}^{\top}\mathbf{o}_{i}-b_{\text{r}})}}," display="block"><semantics id="S4.Ex2.m1.2a"><mrow id="S4.Ex2.m1.2.2.1" xref="S4.Ex2.m1.2.2.1.1.cmml"><mrow id="S4.Ex2.m1.2.2.1.1" xref="S4.Ex2.m1.2.2.1.1.cmml"><mrow id="S4.Ex2.m1.2.2.1.1.1" xref="S4.Ex2.m1.2.2.1.1.1.cmml"><mi id="S4.Ex2.m1.2.2.1.1.1.3" xref="S4.Ex2.m1.2.2.1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.1.1.1.2" xref="S4.Ex2.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.Ex2.m1.2.2.1.1.1.1.1" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.Ex2.m1.2.2.1.1.1.1.1.2" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S4.Ex2.m1.2.2.1.1.1.1.1.1" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.Ex2.m1.2.2.1.1.1.1.1.1.2" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.2.cmml">𝐨</mi><mi id="S4.Ex2.m1.2.2.1.1.1.1.1.1.3" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.Ex2.m1.2.2.1.1.1.1.1.3" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.2.2.1.1.2" xref="S4.Ex2.m1.2.2.1.1.2.cmml">=</mo><mfrac id="S4.Ex2.m1.1.1" xref="S4.Ex2.m1.1.1.cmml"><mn id="S4.Ex2.m1.1.1.3" xref="S4.Ex2.m1.1.1.3.cmml">1</mn><mrow id="S4.Ex2.m1.1.1.1" xref="S4.Ex2.m1.1.1.1.cmml"><mn id="S4.Ex2.m1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.3.cmml">1</mn><mo id="S4.Ex2.m1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.2.cmml">+</mo><msup id="S4.Ex2.m1.1.1.1.4" xref="S4.Ex2.m1.1.1.1.4.cmml"><mi id="S4.Ex2.m1.1.1.1.4.2" xref="S4.Ex2.m1.1.1.1.4.2.cmml">e</mi><mrow id="S4.Ex2.m1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.cmml"><mo id="S4.Ex2.m1.1.1.1.1.1a" xref="S4.Ex2.m1.1.1.1.1.1.cmml">−</mo><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.Ex2.m1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝐰</mi><mtext id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml">r</mtext><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">⊤</mo></msubsup><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝐨</mi><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mtext id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3a.cmml">r</mtext></msub></mrow><mo stretchy="false" id="S4.Ex2.m1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow></mfrac></mrow><mo id="S4.Ex2.m1.2.2.1.2" xref="S4.Ex2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.2b"><apply id="S4.Ex2.m1.2.2.1.1.cmml" xref="S4.Ex2.m1.2.2.1"><eq id="S4.Ex2.m1.2.2.1.1.2.cmml" xref="S4.Ex2.m1.2.2.1.1.2"></eq><apply id="S4.Ex2.m1.2.2.1.1.1.cmml" xref="S4.Ex2.m1.2.2.1.1.1"><times id="S4.Ex2.m1.2.2.1.1.1.2.cmml" xref="S4.Ex2.m1.2.2.1.1.1.2"></times><ci id="S4.Ex2.m1.2.2.1.1.1.3.cmml" xref="S4.Ex2.m1.2.2.1.1.1.3">𝑅</ci><apply id="S4.Ex2.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.2">𝐨</ci><ci id="S4.Ex2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S4.Ex2.m1.1.1.cmml" xref="S4.Ex2.m1.1.1"><divide id="S4.Ex2.m1.1.1.2.cmml" xref="S4.Ex2.m1.1.1"></divide><cn type="integer" id="S4.Ex2.m1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.3">1</cn><apply id="S4.Ex2.m1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1"><plus id="S4.Ex2.m1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.2"></plus><cn type="integer" id="S4.Ex2.m1.1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.1.3">1</cn><apply id="S4.Ex2.m1.1.1.1.4.cmml" xref="S4.Ex2.m1.1.1.1.4"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.4.1.cmml" xref="S4.Ex2.m1.1.1.1.4">superscript</csymbol><ci id="S4.Ex2.m1.1.1.1.4.2.cmml" xref="S4.Ex2.m1.1.1.1.4.2">𝑒</ci><apply id="S4.Ex2.m1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1"><minus id="S4.Ex2.m1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1"></minus><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1"><minus id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2"><times id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.2">𝐰</ci><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3"><mtext mathsize="50%" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.2.3">r</mtext></ci></apply><csymbol cd="latexml" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.2.3">top</csymbol></apply><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.2">𝐨</ci><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="50%" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.3.3">r</mtext></ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.2c">R(\mathbf{o}_{i})=\frac{1}{1+e^{-(\mathbf{w}_{\text{r}}^{\top}\mathbf{o}_{i}-b_{\text{r}})}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS3.SSS1.p3.14" class="ltx_p">where <math id="S4.SS3.SSS1.p3.7.m1.1" class="ltx_Math" alttext="\mathbf{w}_{\text{r}}" display="inline"><semantics id="S4.SS3.SSS1.p3.7.m1.1a"><msub id="S4.SS3.SSS1.p3.7.m1.1.1" xref="S4.SS3.SSS1.p3.7.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p3.7.m1.1.1.2" xref="S4.SS3.SSS1.p3.7.m1.1.1.2.cmml">𝐰</mi><mtext id="S4.SS3.SSS1.p3.7.m1.1.1.3" xref="S4.SS3.SSS1.p3.7.m1.1.1.3a.cmml">r</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.7.m1.1b"><apply id="S4.SS3.SSS1.p3.7.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.7.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.7.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.7.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.7.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.7.m1.1.1.2">𝐰</ci><ci id="S4.SS3.SSS1.p3.7.m1.1.1.3a.cmml" xref="S4.SS3.SSS1.p3.7.m1.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS1.p3.7.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.7.m1.1.1.3">r</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.7.m1.1c">\mathbf{w}_{\text{r}}</annotation></semantics></math> and <math id="S4.SS3.SSS1.p3.8.m2.1" class="ltx_Math" alttext="b_{\text{r}}" display="inline"><semantics id="S4.SS3.SSS1.p3.8.m2.1a"><msub id="S4.SS3.SSS1.p3.8.m2.1.1" xref="S4.SS3.SSS1.p3.8.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p3.8.m2.1.1.2" xref="S4.SS3.SSS1.p3.8.m2.1.1.2.cmml">b</mi><mtext id="S4.SS3.SSS1.p3.8.m2.1.1.3" xref="S4.SS3.SSS1.p3.8.m2.1.1.3a.cmml">r</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.8.m2.1b"><apply id="S4.SS3.SSS1.p3.8.m2.1.1.cmml" xref="S4.SS3.SSS1.p3.8.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.8.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p3.8.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.8.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p3.8.m2.1.1.2">𝑏</ci><ci id="S4.SS3.SSS1.p3.8.m2.1.1.3a.cmml" xref="S4.SS3.SSS1.p3.8.m2.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS1.p3.8.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p3.8.m2.1.1.3">r</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.8.m2.1c">b_{\text{r}}</annotation></semantics></math> are trainable vector and scalar. The model is trained as a binary classifier that predicts whether <math id="S4.SS3.SSS1.p3.9.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS1.p3.9.m3.1a"><mi id="S4.SS3.SSS1.p3.9.m3.1.1" xref="S4.SS3.SSS1.p3.9.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.9.m3.1b"><ci id="S4.SS3.SSS1.p3.9.m3.1.1.cmml" xref="S4.SS3.SSS1.p3.9.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.9.m3.1c">q</annotation></semantics></math> and <math id="S4.SS3.SSS1.p3.10.m4.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS1.p3.10.m4.1a"><msub id="S4.SS3.SSS1.p3.10.m4.1.1" xref="S4.SS3.SSS1.p3.10.m4.1.1.cmml"><mi id="S4.SS3.SSS1.p3.10.m4.1.1.2" xref="S4.SS3.SSS1.p3.10.m4.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p3.10.m4.1.1.3" xref="S4.SS3.SSS1.p3.10.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.10.m4.1b"><apply id="S4.SS3.SSS1.p3.10.m4.1.1.cmml" xref="S4.SS3.SSS1.p3.10.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.10.m4.1.1.1.cmml" xref="S4.SS3.SSS1.p3.10.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.10.m4.1.1.2.cmml" xref="S4.SS3.SSS1.p3.10.m4.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p3.10.m4.1.1.3.cmml" xref="S4.SS3.SSS1.p3.10.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.10.m4.1c">c_{i}</annotation></semantics></math> are relevant or not, but its output <math id="S4.SS3.SSS1.p3.11.m5.1" class="ltx_Math" alttext="R(\mathbf{o}_{i})" display="inline"><semantics id="S4.SS3.SSS1.p3.11.m5.1a"><mrow id="S4.SS3.SSS1.p3.11.m5.1.1" xref="S4.SS3.SSS1.p3.11.m5.1.1.cmml"><mi id="S4.SS3.SSS1.p3.11.m5.1.1.3" xref="S4.SS3.SSS1.p3.11.m5.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p3.11.m5.1.1.2" xref="S4.SS3.SSS1.p3.11.m5.1.1.2.cmml">​</mo><mrow id="S4.SS3.SSS1.p3.11.m5.1.1.1.1" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.2" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.cmml">(</mo><msub id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.2" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.2.cmml">𝐨</mi><mi id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.3" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.3" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.11.m5.1b"><apply id="S4.SS3.SSS1.p3.11.m5.1.1.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1"><times id="S4.SS3.SSS1.p3.11.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.2"></times><ci id="S4.SS3.SSS1.p3.11.m5.1.1.3.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.3">𝑅</ci><apply id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.2">𝐨</ci><ci id="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.11.m5.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.11.m5.1c">R(\mathbf{o}_{i})</annotation></semantics></math> is treated as the score for <math id="S4.SS3.SSS1.p3.12.m6.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S4.SS3.SSS1.p3.12.m6.1a"><msub id="S4.SS3.SSS1.p3.12.m6.1.1" xref="S4.SS3.SSS1.p3.12.m6.1.1.cmml"><mi id="S4.SS3.SSS1.p3.12.m6.1.1.2" xref="S4.SS3.SSS1.p3.12.m6.1.1.2.cmml">c</mi><mi id="S4.SS3.SSS1.p3.12.m6.1.1.3" xref="S4.SS3.SSS1.p3.12.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.12.m6.1b"><apply id="S4.SS3.SSS1.p3.12.m6.1.1.cmml" xref="S4.SS3.SSS1.p3.12.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.12.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p3.12.m6.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.12.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p3.12.m6.1.1.2">𝑐</ci><ci id="S4.SS3.SSS1.p3.12.m6.1.1.3.cmml" xref="S4.SS3.SSS1.p3.12.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.12.m6.1c">c_{i}</annotation></semantics></math> when inference. We use <math id="S4.SS3.SSS1.p3.13.m7.1" class="ltx_Math" alttext="c_{i^{*}}" display="inline"><semantics id="S4.SS3.SSS1.p3.13.m7.1a"><msub id="S4.SS3.SSS1.p3.13.m7.1.1" xref="S4.SS3.SSS1.p3.13.m7.1.1.cmml"><mi id="S4.SS3.SSS1.p3.13.m7.1.1.2" xref="S4.SS3.SSS1.p3.13.m7.1.1.2.cmml">c</mi><msup id="S4.SS3.SSS1.p3.13.m7.1.1.3" xref="S4.SS3.SSS1.p3.13.m7.1.1.3.cmml"><mi id="S4.SS3.SSS1.p3.13.m7.1.1.3.2" xref="S4.SS3.SSS1.p3.13.m7.1.1.3.2.cmml">i</mi><mo id="S4.SS3.SSS1.p3.13.m7.1.1.3.3" xref="S4.SS3.SSS1.p3.13.m7.1.1.3.3.cmml">∗</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.13.m7.1b"><apply id="S4.SS3.SSS1.p3.13.m7.1.1.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.13.m7.1.1.1.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.13.m7.1.1.2.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1.2">𝑐</ci><apply id="S4.SS3.SSS1.p3.13.m7.1.1.3.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.13.m7.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1.3">superscript</csymbol><ci id="S4.SS3.SSS1.p3.13.m7.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1.3.2">𝑖</ci><times id="S4.SS3.SSS1.p3.13.m7.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.13.m7.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.13.m7.1c">c_{i^{*}}</annotation></semantics></math> where <math id="S4.SS3.SSS1.p3.14.m8.1" class="ltx_Math" alttext="i^{*}=\arg\max_{i}R(\mathbf{o}_{i})" display="inline"><semantics id="S4.SS3.SSS1.p3.14.m8.1a"><mrow id="S4.SS3.SSS1.p3.14.m8.1.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.cmml"><msup id="S4.SS3.SSS1.p3.14.m8.1.1.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.3.cmml"><mi id="S4.SS3.SSS1.p3.14.m8.1.1.3.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.3.2.cmml">i</mi><mo id="S4.SS3.SSS1.p3.14.m8.1.1.3.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.3.3.cmml">∗</mo></msup><mo id="S4.SS3.SSS1.p3.14.m8.1.1.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.2.cmml">=</mo><mrow id="S4.SS3.SSS1.p3.14.m8.1.1.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.cmml"><mrow id="S4.SS3.SSS1.p3.14.m8.1.1.1.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.cmml"><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S4.SS3.SSS1.p3.14.m8.1.1.1.3a" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.cmml">⁡</mo><mrow id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.cmml"><msub id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.cmml"><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.2.cmml">max</mi><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.3.cmml">i</mi></msub><mo lspace="0.167em" id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2a" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.cmml">⁡</mo><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.2.cmml">R</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p3.14.m8.1.1.1.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.2.cmml">​</mo><mrow id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.2" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.2.cmml">𝐨</mi><mi id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.3" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.14.m8.1b"><apply id="S4.SS3.SSS1.p3.14.m8.1.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1"><eq id="S4.SS3.SSS1.p3.14.m8.1.1.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.2"></eq><apply id="S4.SS3.SSS1.p3.14.m8.1.1.3.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.14.m8.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.3">superscript</csymbol><ci id="S4.SS3.SSS1.p3.14.m8.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.3.2">𝑖</ci><times id="S4.SS3.SSS1.p3.14.m8.1.1.3.3.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.3.3"></times></apply><apply id="S4.SS3.SSS1.p3.14.m8.1.1.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1"><times id="S4.SS3.SSS1.p3.14.m8.1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.2"></times><apply id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3"><arg id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.1"></arg><apply id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2"><apply id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1">subscript</csymbol><max id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.2"></max><ci id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.3.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.1.3">𝑖</ci></apply><ci id="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.3.2.2">𝑅</ci></apply></apply><apply id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.2">𝐨</ci><ci id="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS1.p3.14.m8.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.14.m8.1c">i^{*}=\arg\max_{i}R(\mathbf{o}_{i})</annotation></semantics></math> for answering the question.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Knowledge-Based Answer Prediction</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.4" class="ltx_p">We use XLNet<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We used XLNet instead of BERT as XLNet shows better performance on the popular Stanford question answering dataset (SQuAD2.0).</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> for predicting the answer in knowledge-based questions. We concatenate the question <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mi id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><ci id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">q</annotation></semantics></math> and the <math id="S4.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="c_{i^{*}}" display="inline"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><msub id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml">c</mi><msup id="S4.SS3.SSS2.p1.2.m2.1.1.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.2.cmml">i</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.3.cmml">∗</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><apply id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.2">𝑐</ci><apply id="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS3.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.2">𝑖</ci><times id="S4.SS3.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">c_{i^{*}}</annotation></semantics></math> with <span id="S4.SS3.SSS2.p1.4.1" class="ltx_text ltx_font_typewriter">[SEP]</span>, and feed it to XLNet, which predicts the positions of the answer starting and ending in <math id="S4.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="c_{i^{*}}" display="inline"><semantics id="S4.SS3.SSS2.p1.3.m3.1a"><msub id="S4.SS3.SSS2.p1.3.m3.1.1" xref="S4.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS2.p1.3.m3.1.1.2" xref="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml">c</mi><msup id="S4.SS3.SSS2.p1.3.m3.1.1.3" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml"><mi id="S4.SS3.SSS2.p1.3.m3.1.1.3.2" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.2.cmml">i</mi><mo id="S4.SS3.SSS2.p1.3.m3.1.1.3.3" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.3.cmml">∗</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.3.m3.1b"><apply id="S4.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.2">𝑐</ci><apply id="S4.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS3.SSS2.p1.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.2">𝑖</ci><times id="S4.SS3.SSS2.p1.3.m3.1.1.3.3.cmml" xref="S4.SS3.SSS2.p1.3.m3.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.3.m3.1c">c_{i^{*}}</annotation></semantics></math>. We extract the words between the predicted starting and ending position as answer <math id="S4.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="a_{k}" display="inline"><semantics id="S4.SS3.SSS2.p1.4.m4.1a"><msub id="S4.SS3.SSS2.p1.4.m4.1.1" xref="S4.SS3.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS3.SSS2.p1.4.m4.1.1.2" xref="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml">a</mi><mi id="S4.SS3.SSS2.p1.4.m4.1.1.3" xref="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.4.m4.1b"><apply id="S4.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.2">𝑎</ci><ci id="S4.SS3.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS3.SSS2.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.4.m4.1c">a_{k}</annotation></semantics></math>. We use a pre-trained XLNet and fine-tune it over the knowledge QA pairs in our AQUA dataset.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we show the performance of VIKING as well as several more basic baselines and state-of-the-art VQA methods on the AQUA dataset.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.3" class="ltx_p">The performance of our task is measured by exact match (EM), <em id="S5.SS1.p1.3.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS1.p1.3.2" class="ltx_text"></span> the percentage of predictions that match the ground truth answer exactly. This EM-based evaluation enables us to compare baselines, VIKING, and its variants in a single framework. It should be noted that, in the visual QA branch, the answer is the most probable word among the answer vocabulary <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">A</annotation></semantics></math> (the top 5,000 most common answers in the training split). The upper bound of the accuracy is 0.306 if all QA pairs in the test split would went through the visual QA branch because only 1,505 QA pairs out of 4,912 have the answer in <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">A</annotation></semantics></math>. In the models that exploit external knowledge, the answer is extracted from the text in <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">C</annotation></semantics></math>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Baselines and VIKING Variants</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Our first set of baselines are both blind and ignorant, answering questions without paintings and external knowledge.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix1.1.1.m1.1b"><mo id="S5.I1.ix1.1.1.m1.1.1" xref="S5.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.1.1.m1.1c"><ci id="S5.I1.ix1.1.1.m1.1.1.cmml" xref="S5.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p"><span id="S5.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">LSTM</span> Each word in a question is converted into word embeddings, which are trained from scratch. The word embeddings are input into a 2-layer LSTM. The hidden state of the last layer is fed into a fully connected layer classifier with the softmax activation over the answer vocabulary <math id="S5.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S5.I1.ix1.p1.1.m1.1a"><mi id="S5.I1.ix1.p1.1.m1.1.1" xref="S5.I1.ix1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.p1.1.m1.1b"><ci id="S5.I1.ix1.p1.1.m1.1.1.cmml" xref="S5.I1.ix1.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.p1.1.m1.1c">A</annotation></semantics></math>.</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix2.1.1.m1.1b"><mo id="S5.I1.ix2.1.1.m1.1.1" xref="S5.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix2.1.1.m1.1c"><ci id="S5.I1.ix2.1.1.m1.1.1.cmml" xref="S5.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p"><span id="S5.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">BERT</span> Each question is input into a fine-tuned base and uncased BERT model. The special tokens <span id="S5.I1.ix2.p1.1.2" class="ltx_text ltx_font_typewriter">[CLS]</span> and <span id="S5.I1.ix2.p1.1.3" class="ltx_text ltx_font_typewriter">[SEP]</span> are added at the beginning and at the end of each sentence, respectively. The output from the first token is fed into a fully connected layer classifier followed by softmax to predict the most probable answer in the same way as the LSTM baseline.</p>
</div>
</li>
<li id="S5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix3.1.1.m1.1b"><mo id="S5.I1.ix3.1.1.m1.1.1" xref="S5.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix3.1.1.m1.1c"><ci id="S5.I1.ix3.1.1.m1.1.1.cmml" xref="S5.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix3.p1" class="ltx_para">
<p id="S5.I1.ix3.p1.1" class="ltx_p"><span id="S5.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">XLNet</span> Instead of the BERT model, XLNet is used to encode questions. The classification is done in the same way as the BERT baseline.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The second set of baselines use paintings but not external knowledge to answer questions.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I2.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I2.ix1.1.1.m1.1b"><mo id="S5.I2.ix1.1.1.m1.1.1" xref="S5.I2.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I2.ix1.1.1.m1.1c"><ci id="S5.I2.ix1.1.1.m1.1.1.cmml" xref="S5.I2.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I2.ix1.p1" class="ltx_para">
<p id="S5.I2.ix1.p1.1" class="ltx_p"><span id="S5.I2.ix1.p1.1.1" class="ltx_text ltx_font_bold">BUTD</span> Bottom-up and top-down attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> consists of a bottom-up module that generates object proposals from the image, and a top-down module that predicts an attention distribution over those proposals based on the question, encoded with a GRU.
The answers are chosen from <math id="S5.I2.ix1.p1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S5.I2.ix1.p1.1.m1.1a"><mi id="S5.I2.ix1.p1.1.m1.1.1" xref="S5.I2.ix1.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S5.I2.ix1.p1.1.m1.1b"><ci id="S5.I2.ix1.p1.1.m1.1.1.cmml" xref="S5.I2.ix1.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.ix1.p1.1.m1.1c">A</annotation></semantics></math>.</p>
</div>
</li>
<li id="S5.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I2.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I2.ix2.1.1.m1.1b"><mo id="S5.I2.ix2.1.1.m1.1.1" xref="S5.I2.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I2.ix2.1.1.m1.1c"><ci id="S5.I2.ix2.1.1.m1.1.1.cmml" xref="S5.I2.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I2.ix2.p1" class="ltx_para">
<p id="S5.I2.ix2.p1.1" class="ltx_p"><span id="S5.I2.ix2.p1.1.1" class="ltx_text ltx_font_bold">BAN</span> Bilinear attention network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> also extracts a set of region proposals from the image and encodes questions with a GRU. Differently from BUTD, BAN computes attention between all the image proposals and all the words in the question.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">For our VIKING model, we have three variants, <em id="S5.SS2.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.p5.1.2" class="ltx_text"></span>, VIKING without the knowledge QA branch (<span id="S5.SS2.p5.1.3" class="ltx_text ltx_font_italic">w/o K</span>), without the visual QA branch (<span id="S5.SS2.p5.1.4" class="ltx_text ltx_font_italic">w/o P</span>), and the <span id="S5.SS2.p5.1.5" class="ltx_text ltx_font_italic">full</span> model. In addition to them, we also evaluate the upper bound performance when the ground truth modality labels are used instead of the modality selector (VIKING <span id="S5.SS2.p5.1.6" class="ltx_text ltx_font_italic">w/ L</span>).</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy for different methods on the AQUA test split. Q, P, K, and L stand for questions, paintings, external knowledge, and labels, which are the information used in the respective models.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Method</th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Q</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">P</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">K</td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">L</td>
<td id="S5.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">EM</td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">LSTM</th>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.198</td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<th id="S5.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">BERT</th>
<td id="S5.T3.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.3.3.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.194</td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<th id="S5.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">XLNet</th>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.4.4.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.193</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<th id="S5.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">BUTD</th>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.218</td>
</tr>
<tr id="S5.T3.1.6.6" class="ltx_tr">
<th id="S5.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">BAN</th>
<td id="S5.T3.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.224</td>
</tr>
<tr id="S5.T3.1.7.7" class="ltx_tr">
<th id="S5.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">VIKING <span id="S5.T3.1.7.7.1.1" class="ltx_text ltx_font_italic">w/o K</span>
</th>
<td id="S5.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.204</td>
</tr>
<tr id="S5.T3.1.8.8" class="ltx_tr">
<th id="S5.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">VIKING <span id="S5.T3.1.8.8.1.1" class="ltx_text ltx_font_italic">w/o P</span>
</th>
<td id="S5.T3.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.352</td>
</tr>
<tr id="S5.T3.1.9.9" class="ltx_tr">
<th id="S5.T3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">VIKING <span id="S5.T3.1.9.9.1.1" class="ltx_text ltx_font_italic">full</span>
</th>
<td id="S5.T3.1.9.9.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T3.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.555</td>
</tr>
<tr id="S5.T3.1.10.10" class="ltx_tr">
<th id="S5.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">VIKING <span id="S5.T3.1.10.10.1.1" class="ltx_text ltx_font_italic">w/ L</span>
</th>
<td id="S5.T3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.555</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results Analysis</h3>

<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Model Comparison</h5>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">Results are presented in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Baselines and VIKING Variants ‣ 5 Evaluation ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
As expected, methods relying only on questions to answer perform poorly, showing that the task requires the information from multiple inputs for answering correctly. When the paintings are added into the system, as in BUTD and BAN, performance improves up to 0.224. However, they lag well behind the accuracy obtained with our proposed VIKING that leverages the information from external sources of information. Overall, our proposed model outperforms other methods, including BUTD and BAN, by a huge margin.</p>
</div>
</section>
<section id="S5.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VIKING Variants</h5>

<div id="S5.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px2.p1.1" class="ltx_p">Our full model improves by more than 0.351 and 0.203 compared to the results of the visual and the knowledge QA branch only models, respectively, showing the benefits of using both the visual information obtained from the paintings and the information obtained from external knowledge. Also, we note that the use of ground truth labels instead of the modality selector hardly affects the overall performance. This implies the modality selector’s efficiency.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2008.12520/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="485" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>VIKING <span id="S5.F3.2.1" class="ltx_text ltx_font_italic">full</span> results for visual (up) and knowledge (down) questions. The right-most column shows incorrect predictions for both modalities.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2008.12520/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Failures in modality selector.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Qualitative Results</h5>

<div id="S5.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ VIKING Variants ‣ 5.3 Results Analysis ‣ 5 Evaluation ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows example predictions by VIKING <span id="S5.SS3.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">full</span>. The modalities of all six examples are correctly predicted. The dataset sometimes contains question and answer pairs that are not obvious. The top-right example illustrates this problem, in which it is not clear if there is a wall or any other things next to the fruit. For the bottom-right example, the ground-truth answer is “in the year before his death,” while VIKING predicted “before his death.” Semantically, the prediction is almost correct, but it is counted as incorrect due to the EM-based evaluation.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation of Knowledge-Related Components</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Next, we study the performance of the components involving external knowledge.</p>
</div>
<section id="S5.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Modality Selector</h5>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Confusion matrix of the modality selector.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;"></th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;" colspan="2">Prediction</td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">Label</th>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">Visual</td>
<td id="S5.T4.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">Knowledge</td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<th id="S5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">Visual</th>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">1,269</td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">1</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<th id="S5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">Knowledge</th>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">17</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">3,625</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px1.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ Modality Selector ‣ 5.4 Evaluation of Knowledge-Related Components ‣ 5 Evaluation ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the confusion matrix of the modality selector on the test split. Since the visual and the knowledge questions are generated using different methods, it is relatively easy for the classifier to distinguish them, getting an accuracy of 0.996. This result supports the fact that there is no gain between VIKING <span id="S5.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">full</span> and <span id="S5.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">w/ L</span>. Most failures in the modality selector (Figure <a href="#S5.F4" title="Figure 4 ‣ VIKING Variants ‣ 5.3 Results Analysis ‣ 5 Evaluation ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives some examples) are reasonable, asking questions that appear to require the other modality.</p>
</div>
</section>
<section id="S5.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">External Knowledge Retrieval</h5>

<div id="S5.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px2.p1.4" class="ltx_p">The performance of the external knowledge retrieval is reported in Table <a href="#S5.T5" title="Table 5 ‣ External Knowledge Retrieval ‣ 5.4 Evaluation of Knowledge-Related Components ‣ 5 Evaluation ‣ A Dataset and Baselines for Visual Question Answering on Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, together with its variants. The performance is measured as recall at <math id="S5.SS4.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS4.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS4.SSS0.Px2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS0.Px2.p1.1.m1.1c">k</annotation></semantics></math> (R@<math id="S5.SS4.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS0.Px2.p1.2.m2.1a"><mi id="S5.SS4.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS4.SSS0.Px2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS0.Px2.p1.2.m2.1b"><ci id="S5.SS4.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS4.SSS0.Px2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS0.Px2.p1.2.m2.1c">k</annotation></semantics></math>), <em id="S5.SS4.SSS0.Px2.p1.4.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS4.SSS0.Px2.p1.4.2" class="ltx_text"></span> the percentage of QA pairs whose original comment is ranked in the top <math id="S5.SS4.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.SSS0.Px2.p1.3.m3.1a"><mi id="S5.SS4.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS4.SSS0.Px2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS0.Px2.p1.3.m3.1b"><ci id="S5.SS4.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS4.SSS0.Px2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS0.Px2.p1.3.m3.1c">k</annotation></semantics></math> positions. Our two-stage external knowledge retrieval achieves the highest performance. Specifically, the full variant (<em id="S5.SS4.SSS0.Px2.p1.4.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS4.SSS0.Px2.p1.4.4" class="ltx_text"></span>, TF-IDF + PP + <math id="S5.SS4.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS4.SSS0.Px2.p1.4.m4.1a"><mi id="S5.SS4.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS4.SSS0.Px2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS0.Px2.p1.4.m4.1b"><ci id="S5.SS4.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S5.SS4.SSS0.Px2.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS0.Px2.p1.4.m4.1c">n</annotation></semantics></math>-gram, where PP stands for preprocessing) of the first stage ranked the original comments within top-10 for over 90% of QA pairs, whereas the second stage gains over 5% by the BERT-based ranking.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>External knowledge retrieval performance. PP stands for preprocessing.</figcaption>
<table id="S5.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.2.3.1" class="ltx_tr">
<th id="S5.T5.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">First stage</th>
<th id="S5.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">Second stage</th>
<th id="S5.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">R@1</th>
<th id="S5.T5.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">R@5</th>
<th id="S5.T5.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:7.0pt;padding-right:7.0pt;">R@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.2.4.1" class="ltx_tr">
<th id="S5.T5.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">TF-IDF</th>
<td id="S5.T5.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T5.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.588</td>
<td id="S5.T5.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.775</td>
<td id="S5.T5.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:7.0pt;padding-right:7.0pt;">0.822</td>
</tr>
<tr id="S5.T5.2.5.2" class="ltx_tr">
<th id="S5.T5.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">TF-IDF + PP</th>
<td id="S5.T5.2.5.2.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T5.2.5.2.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.600</td>
<td id="S5.T5.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.803</td>
<td id="S5.T5.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.844</td>
</tr>
<tr id="S5.T5.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:7.0pt;padding-right:7.0pt;">TF-IDF + PP + <math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mi id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">n</annotation></semantics></math>-gram</th>
<td id="S5.T5.1.1.2" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">-</td>
<td id="S5.T5.1.1.3" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.712</td>
<td id="S5.T5.1.1.4" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.878</td>
<td id="S5.T5.1.1.5" class="ltx_td ltx_align_center" style="padding-left:7.0pt;padding-right:7.0pt;">0.907</td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<th id="S5.T5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">TF-IDF + PP + <math id="S5.T5.2.2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.T5.2.2.1.m1.1a"><mi id="S5.T5.2.2.1.m1.1.1" xref="S5.T5.2.2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.m1.1b"><ci id="S5.T5.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.m1.1c">n</annotation></semantics></math>-gram</th>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">✓</td>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">0.769</td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">0.879</td>
<td id="S5.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:7.0pt;padding-right:7.0pt;">0.907</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work is presented as a concise first approximation to the task of art-based visual question answering and it aims to set the foundations for incorporating art knowledge in a computer vision system. However, despite the encouraging results obtained in our experimental evaluation, it presents some limitations.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dataset Limitations:</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">The questions and answers in our proposed AQUA dataset are automatically generated from paintings and their associated comments. This process presents some limitations: 1) questions and their answers are either relate to the visual content or to the background knowledge, but usually not both. It would be interesting to incorporate samples where both are needed, increasing the complexity of the problem; 2) the visual questions are based on the labels extracted by an object detector trained on real-world photos, which introduces noise specially on paintings with non-realistic styles; and 3) because of the automatic generation of answers, the variety of the questions and of the capabilities required to answer them is rather limited, <em id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.SS0.SSS0.Px1.p1.1.2" class="ltx_text"></span>, the answers of visual questions can only be detected objects in the images.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Baseline Limitation:</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">We introduced VIKING as a baseline model for art-based VQA. VIKING is built on top of state-of-the-art VQA and TQA models. Apart from the specific limitations of those systems, VIKING presents two domain specific limitations: 1) on the visual part, the model is applied equally to all the painting images, without considering the differences on artistic styles. Incorporating style correction techniques would benefit the visual recognition part, specially for object detection; 2) on the knowledge part, VIKING uses the same source of information as in the question generation process (<em id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.SS0.SSS0.Px2.p1.1.2" class="ltx_text"></span>, comments from the SemArt dataset). A more realistic setting would require to query independent sources of external knowledge, such as Wikipedia.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p2.1" class="ltx_p">Considering these limitations, we envisage some promising future directions on art-based VQA. On the dataset construction part, it would be interesting to incorporate human expert question-answer pairs that require both visual and knowledge understanding. This would increase the complexity and relevance of the dataset. On the model design part, the addition of specific features related to paintings, differing from the real-world images, would probably improve the model performance. For example, adaptation between the real-world and the painting domain in the object detector, or disentanglement of content and style.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper proposes a new task of visual question answering on art pieces, and presents our preliminary dataset on this task, coined AQUA, which consists on automatically generated visual and knowledge-based QA pairs. We also present a model called VIKING that serves as a baseline for future exploration on this task. VIKING leverages complementary information in paintings and external knowledge with a two branch model. Our experimental results demonstrated that VIKING outperformed existing models with a large margin, which means that the model is a strong baseline to compare. The task definition in this paper assumes that the external knowledge is strongly tied with the questions (<em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p1.1.2" class="ltx_text"></span>, the comments used for generating the QA pairs are available for the QA module). This may be in a sense viewed as the upper bound of the performance. Using other sources of external knowledge will be our future direction.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgment</span> This work was partly supported by JSPS KAKENHI Nos. 18H03264 and 20K19822, and JST ACT-I.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: CVPR (2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: VQA: Visual Question Answering. In: ICCV (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Carneiro, G., da Silva, N.P., Del Bue, A., Costeira, J.P.: Artistic image
classification: An analysis on the printart database. In: ECCV (2012)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Crowley, E., Zisserman, A.: The state of the art: Object retrieval in paintings
using discriminative regions. In: BMVC (2014)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Crowley, E.J., Parkhi, O.M., Zisserman, A.: Face painting: querying art with
photos. In: BMVC (2015)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: NAACL-HLT (2019)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Du, X., Cardie, C.: Harvesting paragraph-level question-answer pairs from
Wikipedia. In: ACL (2018)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Du, X., Shao, J., Cardie, C.: Learning to ask: Neural question generation for
reading comprehension. In: ACL (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Duan, N., Tang, D., Chen, P., Zhou, M.: Question generation for question
answering. In: EMNLP (2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
Pascal Visual Object Classes (VOC) challenge. IJCV <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">88</span>(2),
303–338 (2010)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Fan, Z., Wei, Z., Wang, S., Liu, Y., Huang, X.: A reinforcement learning
framework for natural question generation using bi-discriminators. In: COLING
(2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Garcia, N., Otani, M., Chu, C., Nakashima, Y.: KnowIT VQA: Answering
knowledge-based questions about videos. In: AAAI (2020)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Garcia, N., Renoust, B., Nakashima, Y.: Context-aware embeddings for automatic
art analysis. In: ICMR (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Garcia, N., Vogiatzis, G.: How to read paintings: Semantic art understanding
with multi-modal retrieval. In: ECCV Workshops (2018)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in
VQA matter: Elevating the role of image understanding in visual question
answering. In: Proc. CVPR (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: CVPR (2016)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Heilman, M., Smith, N.A.: Good question! Statistical ranking for question
generation. In: NAACL (2010)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Huckle, N., Garcia, N., Vogiatzis, G.: Demographic influences on contemporary
art with unsupervised style embeddings. In: ECCV workshops (2020)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ikeuchi, K., Oishi, T., Takamatsu, J., Sagawa, R., Nakazawa, A., Kurazume, R.,
Nishino, K., Kamakura, M., Okamoto, Y.: The great Buddha project:
Dijitally archiving restoring, and analyzing cultural heritage objects.
IJCV pp. 189–208 (2007)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jain, U., Zhang, Z., Schwing, A.G.: Creativity: Generating diverse questions
using variational autoencoders. In: CVPR (2017)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: TGIF-QA: Toward
spatio-temporal reasoning in visual question answering. In: Proc. CVPR (2017)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Johnson, C.R., Hendriks, E., Berezhnoy, I.J., Brevdo, E., Hughes, S.M.,
Daubechies, I., Li, J., Postma, E., Wang, J.Z.: Image processing for artist
identification. IEEE Signal Processing Magazine <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">25</span>(4) (2008)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L.,
Girshick, R.: CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning. In: Proc. CVPR (2017)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kim, J.H., Jun, J., Zhang, B.T.: Bilinear attention networks. In: NeurIPS
(2018)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kim, K.M., Heo, M.O., Choi, S.H., Zhang, B.T.: DeepStory: Video story QA
by deep embedded memory networks. In: Proc. IJCAI (2017)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kim, Y., Lee, H., Shin, J., Jung, K.: Improving neural question generation
using answer separation. In: AAAI (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Krishna, R., Bernstein, M., Fei-Fei, L.: Information maximizing visual question
generation. In: CVPR (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Labutov, I., Basu, S., Vanderwende, L.: Deep questions without deep
understanding. In: ACL-IJCNLP (2015)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Lewis, M., Fan, A.: Generative question answering: Learning to answer the whole
question. In: ICLR (2019)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Li, Y., Duan, N., Zhou, B., Chu, X., Ouyang, W., Wang, X., Zhou, M.: Visual
question generation as dual task of visual question answering. In: CVPR
(2018)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ma, D., Gao, F., Bai, Y., Lou, Y., Wang, S., Huang, T., Duan, L.Y.: From part
to whole: Who is behind the painting? In: ACMMM (2017)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Maharaj, T., Ballas, N., Rohrbach, A., Courville, A., Pal, C.: A dataset and
exploration of models for understanding video data through fill-in-the-blank
question-answering. In: CVPR (2017)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Malinowski, M., Fritz, M.: A multi-world approach to question answering about
real-world scenes based on uncertain input. In: Proc. NIPS (2014)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question
answering benchmark requiring external knowledge. In: CVPR (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mazidi, K., Nielsen, R.D.: Linguistic considerations in automatic question
generation. In: ACL (2014)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Misra, I., Girshick, R., Fergus, R., Hebert, M., Gupta, A., van der Maaten, L.:
Learning by asking questions. In: CVPR (2018)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Mostafazadeh, N., Misra, I., Devlin, J., Mitchell, M., He, X., Vanderwende, L.:
Generating natural questions about an image. In: ACL (2016)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Mun, J., Hongsuck Seo, P., Jung, I., Han, B.: MarioQA: Answering questions
by watching gameplay videos. In: Proc. ICCV (2017)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Pan, L., Lei, W., Chua, T., Kan, M.: Recent advances in neural question
generation. CoRR <span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">abs/1905.08949</span> (2019)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J.,
Lazebnik, S.: Flickr30k Entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models. In: ICCV (2015)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., Zemel, R.S.: Exploring models and data for image question
answering. In: NeurIPS (2015)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Shamir, L., Macura, T., Orlov, N., Eckley, D.M., Goldberg, I.G.: Impressionism,
expressionism, surrealism: Automated recognition of painters and schools of
art. ACM Transactions on Applied Perception (2010)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Sun, X., Liu, J., Lyu, Y., He, W., Ma, Y., Wang, S.: Answer-focused and
position-aware neural question generation. In: EMNLP (2018)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Tan, W.R., Chan, C.S., Aguirre, H.E., Tanaka, K.: Ceci n’est pas une pipe: A
deep convolutional network for fine-art paintings classification. In: ICIP
(2016)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.:
MovieQA: Understanding stories in movies through question-answering. In:
Proc. CVPR (2016)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image
caption generator. In: CVPR (2015)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wang, P., Wu, Q., Shen, C., Dick, A., van den Hengel, A.: FVQA: Fact-based
visual question answering. TPAMI <span id="bib.bib47.1.1" class="ltx_text ltx_font_bold">40</span>(10), 2413–2427 (2018)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Wang, P., Wu, Q., Shen, C., Dick, A., Van Den Henge, A.: Explicit
knowledge-based reasoning for visual question answering. In: IJCAI. pp.
1290–1296 (2017)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wu, Q., Teney, D., Wang, P., Shen, C., Dick, A., Hengel, A.v.d.: Visual
question answering: A survey of methods and datasets. CVIU pp. 1–20 (2017)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wu, Q., Wang, P., Shen, C., Dick, A., van den Hengel, A.: Ask me anything:
Free-form visual question answering based on knowledge from external
sources. In: CVPR (2016)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yang, J., Lu, J., Lee, S., Dhruv Batra, D.P.: Visual curiosity: Learning to
ask questions to learn visual recognition. In: CoRL (2018)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.:
XLNet: Generalized autoregressive pretraining for language understanding.
In: NeurIPS (2019)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition:
Visual commonsense reasoning. In: Proc. CVPR (2019)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Zhang, S., Qu, L., You, S., Yang, Z., Zhang, J.: Automatic generation of
grounded visual questions. In: IJCAI. pp. 4235–4243 (2017)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Zhao, Y., Ni, X., Ding, Y., Ke, Q.: Paragraph-level neural question generation
with maxout pointer and gated self-attention networks. In: EMNLP (2018)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhu, L., Xu, Z., Yang, Y., Hauptmann, A.G.: Uncovering the temporal context for
video question answering. IJCV <span id="bib.bib56.1.1" class="ltx_text ltx_font_bold">124</span>(3), 409–421 (2017)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhu, Y., Groth, O., Bernstein, M.S., Fei-Fei, L.: Visual7W: Grounded question
answering in images. CVPR (2016)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.12518" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.12520" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.12520">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.12520" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.12521" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 01:28:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
