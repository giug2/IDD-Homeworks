<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.05626] Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach</title><meta property="og:description" content="Recent semantic segmentation models perform well under standard weather conditions and sufficient illumination but struggle with adverse weather conditions and nighttime. Collecting and annotating training data under tâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.05626">

<!--Generated on Thu Mar 14 03:17:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Abdulrahman Kerima.kerim@lancaster.ac.uk1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Felipe Chamonecadar@dcc.ufmg.br2
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Washington Ramoswashington.ramos@dcc.ufmg.br2
<span id="p1.2.3" class="ltx_ERROR undefined">\addauthor</span>Leandro Soriano Marcolinol.marcolino@lancaster.ac.uk1
<span id="p1.2.4" class="ltx_ERROR undefined">\addauthor</span>Erickson R. Nascimentoerickson@dcc.ufmg.br2
<span id="p1.2.5" class="ltx_ERROR undefined">\addauthor</span>Richard Jiangr.jiang2@lancaster.ac.uk1
<span id="p1.2.6" class="ltx_ERROR undefined">\addinstitution</span>
School of Computing and Communications
<br class="ltx_break">Lancaster University
<br class="ltx_break">Lancaster, UK

<span id="p1.2.7" class="ltx_ERROR undefined">\addinstitution</span>
Computer Science Department
<br class="ltx_break">Universidade Federal de Minas Gerais,
<br class="ltx_break">Minas Gerais, Brazil

Semantic Segmentation under Adverse Conditions



<span id="p1.2.8" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p"><span id="id2.2.2" class="ltx_text" lang="en">Recent semantic segmentation models perform well under standard weather conditions and sufficient illumination but struggle with adverse weather conditions and nighttime. Collecting and annotating training data under these conditions is expensive, time-consuming, error-prone, and not always practical. Usually, synthetic data is used as a feasible data source to increase the amount of training data. However, just directly using synthetic data may actually harm the modelâ€™s performance under normal weather conditions while getting only small gains in adverse situations. Therefore, we present a novel architecture specifically designed for using synthetic training data for domain adaptation. We propose a simple yet powerful addition to DeepLabV3+
by using weather and time-of-the-day supervisors trained with multi-task learning, making it both weather and nighttime aware, which improves its mIoU accuracy by <math id="id1.1.1.m1.1" class="ltx_Math" alttext="14" display="inline"><semantics id="id1.1.1.m1.1a"><mn id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><cn type="integer" id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">14</annotation></semantics></math> percentage points on the ACDC dataset while maintaining a score of <math id="id2.2.2.m2.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="id2.2.2.m2.1a"><mrow id="id2.2.2.m2.1.1" xref="id2.2.2.m2.1.1.cmml"><mn id="id2.2.2.m2.1.1.2" xref="id2.2.2.m2.1.1.2.cmml">75</mn><mo id="id2.2.2.m2.1.1.1" xref="id2.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.2.m2.1b"><apply id="id2.2.2.m2.1.1.cmml" xref="id2.2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.2.m2.1.1.1.cmml" xref="id2.2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="id2.2.2.m2.1.1.2.cmml" xref="id2.2.2.m2.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m2.1c">75\%</annotation></semantics></math> mIoU on the Cityscapes dataset. Our code is available at <a target="_blank" href="https://github.com/lsmcolab/Semantic-Segmentation-under-Adverse-Conditions" title="" class="ltx_ref ltx_href">https://github.com/lsmcolab/Semantic-Segmentation-under-Adverse-Conditions</a>.</span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2210.05626/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text ltx_font_bold">Existing domain adaptation vs. our proposed pipeline.</span> Unlike other approaches, our pipeline utilizes synthetic data, Weather-Aware-Supervisor (WAS), and Time-Aware-Supervisor (TAS) to handle standard-to-adverse domain adaptation. Leveraging our synthetic-aware training procedure, we train our weather and daytime-nighttime aware architecture, simultaneously, on synthetic adverse weather and real normal weather data.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Understanding the environment using visual data has been an active research problem since the early beginning of computer vision. It started to attract even more researchers with the great advancement in autonomous carsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Teichmann etÂ al.(2018)Teichmann, Weber, Zoellner, Cipolla, and
Urtasun</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Kumar etÂ al.(2021)Kumar, Klingner, Yogamani, Milz, Fingscheidt, and
Mader</a>, <a href="#bib.bibx42" title="" class="ltx_ref">Wiseman(2022)</a>]</cite>, human-computer-interactionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Ren and Bao(2020)</a>, <a href="#bib.bibx27" title="" class="ltx_ref">Nazar etÂ al.(2021)Nazar, Alam, Yafi, and
Mazliham</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Liu etÂ al.(2022)Liu, Sivaparthipan, and Shankar</a>]</cite>, and augmented realityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Baroroh etÂ al.(2021)Baroroh, Chu, and Wang</a>, <a href="#bib.bibx10" title="" class="ltx_ref">Costa etÂ al.(2022)Costa, Petry, and Moreira</a>, <a href="#bib.bibx7" title="" class="ltx_ref">Chiang etÂ al.(2022)Chiang, Shang, and Qiao</a>]</cite>. Semantic segmentation is at the core of these applications, with the data-driven supervised learning methods dominating this field, achieving state-of-the-art resultsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Ronneberger etÂ al.(2015)Ronneberger, Fischer, and
Brox</a>, <a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>, <a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Fu etÂ al.(2019)Fu, Liu, Tian, Li, Bao, Fang, and Lu</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia</a>]</cite>. Training these models on real data requires large-scale human annotated images, which is expensive and time-consuming, especially for images taken under challenging weather and illumination conditions such as fog and nighttime. For instance, a person takes about <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="integer" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">90</annotation></semantics></math> minutes to annotate an image from the Cityscapes datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite>, which contains only daylight and clear weather conditions, while it exceeds three hours for the Adverse Conditions Dataset with Correspondences (ACDC)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> dataset.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the success of recent semantic segmentation models in clear weather and standard illumination conditions, these methods struggle with adverse conditions (<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, rainy, foggy, snowy, and nighttime), which degrade the feature extraction process. Falling rain and snow particles change the visual appearance of objects, partially occlude them, and cause distortion on the camera sensor, while fog works as a low-pass filter, removing high-frequency components. Nighttime is even more problematic because of the dramatic change in the light distribution and other severe artifacts, such as lens flare, bright spots, and chromatic aberration. Yet, few works have tried to investigate the effect of weather conditions and nighttime in semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Guo etÂ al.(2020)Guo, Chen, Ma, Li, Li, and Xie</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Lei etÂ al.(2020)Lei, Emaru, Ravankar, Kobayashi, and
Wang</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Alshammari etÂ al.(2020)Alshammari, Akcay, and
Breckon</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Xu etÂ al.(2021)Xu, Ma, Wu, Long, and Huang</a>, <a href="#bib.bibx24" title="" class="ltx_ref">Ma etÂ al.(2022)Ma, Wang, Zhan, Zheng, Wang, Dai, and Lin</a>]</cite>. Although they achieve remarkable results, they are limited to one weather condition only and are too narrow in their scope.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a novel training procedure to address the issues in the semantic segmentation under adverse conditions and in the annotation efforts, simultaneously. We leverage synthetic data to produce ground-truth images at no human annotation effort and create a new dataset, the AWSS, which is composed of images specially generated by a modified version of the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Silver</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang</a>]</cite> simulator. To reduce the gap between synthetic and real, our approach combines synthetic and real images by alternating their batches at training time as illustrated in Fig.Â <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We also propose the Weather-Aware Supervisor (WAS) and the Time-Aware Supervisor (TAS), which are trained jointly with the main module to improve the feature extraction. Our main module derives from the DeepLabV3+ which contains the powerful atrous convolutions that increase the receptive field while not increasing the dimensions of feature maps and computation cost. Thus, better performance at low computation. Unlike the current methods that work only with a single weather condition, our approach can handle the three main ones, <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">i.e.</span>, rainy, foggy, and snowy, as well as nighttime images. The results show that our novel model achieves state-of-the-art results under adverse weather conditions (0.49 mIoU on ACDC) while it maintains adequate performance under standard conditions (0.75 mIoU on Cityscapes).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In summary, our contributions are three-fold:
<span id="S1.p4.1.1" class="ltx_text ltx_font_italic">i)</span> a novel synthetic-aware training procedure that can be used to train on both synthetic and real data simultaneously. In particular, we significantly improve DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite> robustness on adverse conditions by making its encoder both weather and nighttime aware;<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The synthetic data, code, and our modified version of the <span id="footnote1.1" class="ltx_text ltx_font_italic">Silver</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang</a>]</cite> simulator are all publicly available under the paperâ€™s GitHub repository.</span></span></span>
<span id="S1.p4.1.2" class="ltx_text ltx_font_italic">ii)</span> We extend the <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">Silver</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang</a>]</cite> simulator to generate more photo-realistic and diverse adverse weather conditions and increase the supported semantic segmentation classes;
<span id="S1.p4.1.4" class="ltx_text ltx_font_italic">iii)</span> leveraging our modified version of <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">Silver</span>, we generate a new synthetic semantic segmentation dataset, the AWSS, composed of photo-realistic annotated images spanning foggy, rainy, and snowy weather conditions and nighttime attributes.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S1.T1.2.1" class="ltx_text ltx_font_bold">Comparison among synthetic semantic segmentation datasets.</span> Our dataset, named AWSS, is composed of photo-realistic pixel-wise annotated images under standard and adverse conditions.</figcaption>
<div id="S1.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:104.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-129.9pt,31.4pt) scale(0.625312048485508,0.625312048485508) ;">
<table id="S1.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.3.1.1.1" class="ltx_tr">
<th id="S1.T1.3.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S1.T1.3.1.1.1.2" class="ltx_td ltx_align_center" colspan="4">Weather Conditions</td>
<td id="S1.T1.3.1.1.1.3" class="ltx_td ltx_align_center" colspan="2">Times-of-Day</td>
<td id="S1.T1.3.1.1.1.4" class="ltx_td ltx_align_center">Photo-realism</td>
<td id="S1.T1.3.1.1.1.5" class="ltx_td ltx_align_center">
<table id="S1.T1.3.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1.1.5.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Public</td>
</tr>
<tr id="S1.T1.3.1.1.1.5.1.2" class="ltx_tr">
<td id="S1.T1.3.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Availability</td>
</tr>
</table>
</td>
</tr>
<tr id="S1.T1.3.1.2.2" class="ltx_tr">
<th id="S1.T1.3.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S1.T1.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Normal</td>
<td id="S1.T1.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Rain</td>
<td id="S1.T1.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Fog</td>
<td id="S1.T1.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Snow</td>
<td id="S1.T1.3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Daytime</td>
<td id="S1.T1.3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Nighttime</td>
<td id="S1.T1.3.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">/</td>
<td id="S1.T1.3.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">/</td>
</tr>
<tr id="S1.T1.3.1.3.3" class="ltx_tr">
<th id="S1.T1.3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GTA-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Richter etÂ al.(2016)Richter, Vineet, Roth, and
Koltun</a>]</cite>
</th>
<td id="S1.T1.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
<td id="S1.T1.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
<td id="S1.T1.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S1.T1.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S1.T1.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
<td id="S1.T1.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S1.T1.3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
<td id="S1.T1.3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">âœ”</td>
</tr>
<tr id="S1.T1.3.1.4.4" class="ltx_tr">
<th id="S1.T1.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SynscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Tsirikoglou etÂ al.(2017)Tsirikoglou, Kronander, Wrenninge, and
Unger</a>, <a href="#bib.bibx44" title="" class="ltx_ref">Wrenninge and Unger(2018)</a>]</cite>
</th>
<td id="S1.T1.3.1.4.4.2" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.4.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.4.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.4.4.6" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.4.4.7" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.4.4.8" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.4.4.9" class="ltx_td ltx_align_center">âœ”</td>
</tr>
<tr id="S1.T1.3.1.5.5" class="ltx_tr">
<th id="S1.T1.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Virtual KITTIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Gaidon etÂ al.(2016)Gaidon, Wang, Cabon, and Vig</a>]</cite>
</th>
<td id="S1.T1.3.1.5.5.2" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.5.5.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.5.5.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.5.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.5.5.6" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.5.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.5.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.5.5.9" class="ltx_td ltx_align_center">âœ”</td>
</tr>
<tr id="S1.T1.3.1.6.6" class="ltx_tr">
<th id="S1.T1.3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SynthiaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Ros etÂ al.(2016)Ros, Sellart, Materzynska, Vazquez, and
Lopez</a>]</cite>
</th>
<td id="S1.T1.3.1.6.6.2" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.6.6.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.6.6.5" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.6.6.6" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.6.6.7" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.6.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.6.6.9" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S1.T1.3.1.7.7" class="ltx_tr">
<th id="S1.T1.3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SHIFTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Sun etÂ al.(2022)Sun, Segu, Postels, Wang, VanÂ Gool, Schiele, Tombari,
and Yu</a>]</cite>
</th>
<td id="S1.T1.3.1.7.7.2" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S1.T1.3.1.7.7.6" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.7" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.8" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.7.7.9" class="ltx_td ltx_align_center">âœ”</td>
</tr>
<tr id="S1.T1.3.1.8.8" class="ltx_tr">
<th id="S1.T1.3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AWSS (Ours)</th>
<td id="S1.T1.3.1.8.8.2" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.3" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.4" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.5" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.6" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.7" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.8" class="ltx_td ltx_align_center">âœ”</td>
<td id="S1.T1.3.1.8.8.9" class="ltx_td ltx_align_center">âœ”</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic data for semantic segmentation.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The high performance of recent semantic segmentation models is associated with the ability to train deep models on large-scale training data. The early real semantic segmentation datasets like CamVidÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Brostow etÂ al.(2009)Brostow, Fauqueur, and
Cipolla</a>]</cite>, Stanford BackgroundÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Liu etÂ al.(2010)Liu, Gould, and Koller</a>, <a href="#bib.bibx35" title="" class="ltx_ref">Saxena etÂ al.(2005)Saxena, Chung, and Ng</a>, <a href="#bib.bibx36" title="" class="ltx_ref">Saxena etÂ al.(2008)Saxena, Sun, and Ng</a>]</cite>, and KITTI-LayoutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Alvarez etÂ al.(2012)Alvarez, Gevers, LeCun, and
Lopez</a>]</cite> are limited in terms of the number of training samples, classes, resolution, and diversity. The problem is partially alleviated with the recent availability of datasets like CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite>, ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite>, ADE20KÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Zhou etÂ al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
Torralba</a>]</cite>, and Mapillary VistasÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Neuhold etÂ al.(2017)Neuhold, Ollmann, RotaÂ Bulo, and
Kontschieder</a>]</cite>. Nevertheless, annotating large-scale datasets of high-resolution images is still the bottleneck. At the same time, ensuring diverse training data under challenging attributes like adverse weather conditions is not only dangerous, time-consuming, and hard to collect but also cumbersome and subjective to human errors in the annotation process.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Synthetic data comes as a resort to handle all the above issues. Their success in computer vision is specifically seen in semantic segmentation. GoyalÂ <em id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p2.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Goyal etÂ al.(2017)Goyal, Rajpura, Bojinov, and
Hegde</a>]</cite> demonstrate that augmenting synthetic data with weakly annotated data can improve the performance on the PASCAL VOC datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Everingham etÂ al.(2015)Everingham, Eslami, VanÂ Gool, Williams, Winn,
and Zisserman</a>]</cite>. Similarly, RichterÂ <em id="S2.SS0.SSS0.Px1.p2.1.3" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p2.1.4" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Richter etÂ al.(2016)Richter, Vineet, Roth, and
Koltun</a>]</cite> generate synthetic training data by utilizing the Grand Theft Auto V game. They show that training semantic segmentation models on one third of the training split of CamVidÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Brostow etÂ al.(2009)Brostow, Fauqueur, and
Cipolla</a>]</cite> dataset along with their generated synthetic data achieves superior results compared to training on the full CamVidÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Brostow etÂ al.(2009)Brostow, Fauqueur, and
Cipolla</a>]</cite>. In parallel, IvanovsÂ <em id="S2.SS0.SSS0.Px1.p2.1.5" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p2.1.6" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Ivanovs etÂ al.(2022)Ivanovs, Ozols, Dobrajs, and
Kadikis</a>]</cite> augment the CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> dataset with synthetic images generated using the CARLAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Dosovitskiy etÂ al.(2017)Dosovitskiy, Ros, Codevilla, Lopez, and
Koltun</a>]</cite> simulator. They show that the performance improves when compared to training only on CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite>. Similar to these works, we use synthetic data to boost the performance of semantic segmentation models. However, we tackle the domain shift problem using synthetic data and a synthetic-aware training procedure.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p"><span id="S2.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">Domain adaptation in semantic segmentation.</span>
A major limitation of synthetic data is the domain shift: models trained on synthetic data do not perform well on real-world dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Sankaranarayanan etÂ al.(2018)Sankaranarayanan, Balaji, Jain, Lim, and
Chellappa</a>, <a href="#bib.bibx47" title="" class="ltx_ref">Xu etÂ al.(2019)Xu, Du, Zhang, Zhang, Wang, and Zhang</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Dundar etÂ al.(2020)Dundar, Liu, Yu, Wang, Zedlewski, and
Kautz</a>]</cite>. SankaranarayananÂ <em id="S2.SS0.SSS0.Px1.p3.1.2" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p3.1.3" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Sankaranarayanan etÂ al.(2018)Sankaranarayanan, Balaji, Jain, Lim, and
Chellappa</a>]</cite> propose a Generative Adversarial Network (GAN) based approach that minimizes the distance between the encodings of both domains. They show that their approach can boost the performance of synthetic-to-real domain adaptation tasks. Our work is similar to theirs as we use synthetic data for domain adaptation and propose a synthetic-aware training procedure. However, our work tackles this problem under harder set-ups utilizing synthetic data to mitigate standard-to-adverse domain shifts. In the same context, AlshammariÂ <em id="S2.SS0.SSS0.Px1.p3.1.4" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p3.1.5" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Alshammari etÂ al.(2020)Alshammari, Akcay, and
Breckon</a>]</cite> address standard to foggy weather domain shift by using an adversarial training strategy that guides the model to produce outputs close to the target domain. Similarly, MaÂ <em id="S2.SS0.SSS0.Px1.p3.1.6" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p3.1.7" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Ma etÂ al.(2022)Ma, Wang, Zhan, Zheng, Wang, Dai, and Lin</a>]</cite> tackle standard weather to foggy weather domain adaptation using both fog and style variations by adopting a Cumulative style-fog-dual disentanglement Domain Adaptation method (CuDA-Net). Alternatively, XuÂ <em id="S2.SS0.SSS0.Px1.p3.1.8" class="ltx_emph ltx_font_italic">et al</em><span id="S2.SS0.SSS0.Px1.p3.1.9" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">Xu etÂ al.(2021)Xu, Ma, Wu, Long, and Huang</a>]</cite> address the daytime to nighttime domain shift. They utilize a novel Curriculum Domain Adaptation method (CDAda) that uses labeled synthetic nighttime images. Our method is closely related to these works. However, we tackle domain adaptation from a standard domain (<span id="S2.SS0.SSS0.Px1.p3.1.10" class="ltx_text ltx_font_italic">i.e.,</span> daytime and normal weather condition) to an adverse domain (<span id="S2.SS0.SSS0.Px1.p3.1.11" class="ltx_text ltx_font_italic">i.e.</span>, nighttime and adverse weather conditions such as rain, fog, and snow).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The AWSS Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">There have been many synthetic datasets proposed for the semantic segmentation problem. However, they are usually non-photo-realistic such as SynthiaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Ros etÂ al.(2016)Ros, Sellart, Materzynska, Vazquez, and
Lopez</a>]</cite> and Virtual KITTIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Gaidon etÂ al.(2016)Gaidon, Wang, Cabon, and Vig</a>]</cite>, limited in diversity such as GTA-VÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Richter etÂ al.(2016)Richter, Vineet, Roth, and
Koltun</a>]</cite> and SynscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Tsirikoglou etÂ al.(2017)Tsirikoglou, Kronander, Wrenninge, and
Unger</a>, <a href="#bib.bibx44" title="" class="ltx_ref">Wrenninge and Unger(2018)</a>]</cite> as clearly demonstrated in TableÂ <a href="#S1.T1" title="Table 1 â€£ 1 Introduction â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Recently, SHIFTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Sun etÂ al.(2022)Sun, Segu, Postels, Wang, VanÂ Gool, Schiele, Tombari,
and Yu</a>]</cite> dataset was introduced, which is photo-realistic and diverse similar to our generated synthetic dataset but does not cover the snowy weather.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">We extend <span id="S3.p2.3.1" class="ltx_text ltx_font_italic">Silver</span>, proposed by KerimÂ <em id="S3.p2.3.2" class="ltx_emph ltx_font_italic">et al</em><span id="S3.p2.3.3" class="ltx_ERROR undefined">\bmvaOneDot</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang</a>]</cite>, to generate adverse weather photo-realistic images along with their corresponding ground-truth for the semantic segmentation task. We generate the Adverse Weather Synthetic Segmentation (AWSS) dataset, which comprises <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="1{,}250" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">1</mn><mo id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">1</cn><cn type="integer" id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">250</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">1{,}250</annotation></semantics></math> images with a resolution of <math id="S3.p2.2.m2.2" class="ltx_Math" alttext="1{,}200\times 780" display="inline"><semantics id="S3.p2.2.m2.2a"><mrow id="S3.p2.2.m2.2.2.1" xref="S3.p2.2.m2.2.2.2.cmml"><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">1</mn><mo id="S3.p2.2.m2.2.2.1.2" xref="S3.p2.2.m2.2.2.2.cmml">,</mo><mrow id="S3.p2.2.m2.2.2.1.1" xref="S3.p2.2.m2.2.2.1.1.cmml"><mn id="S3.p2.2.m2.2.2.1.1.2" xref="S3.p2.2.m2.2.2.1.1.2.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.2.m2.2.2.1.1.1" xref="S3.p2.2.m2.2.2.1.1.1.cmml">Ã—</mo><mn id="S3.p2.2.m2.2.2.1.1.3" xref="S3.p2.2.m2.2.2.1.1.3.cmml">780</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2b"><list id="S3.p2.2.m2.2.2.2.cmml" xref="S3.p2.2.m2.2.2.1"><cn type="integer" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">1</cn><apply id="S3.p2.2.m2.2.2.1.1.cmml" xref="S3.p2.2.m2.2.2.1.1"><times id="S3.p2.2.m2.2.2.1.1.1.cmml" xref="S3.p2.2.m2.2.2.1.1.1"></times><cn type="integer" id="S3.p2.2.m2.2.2.1.1.2.cmml" xref="S3.p2.2.m2.2.2.1.1.2">200</cn><cn type="integer" id="S3.p2.2.m2.2.2.1.1.3.cmml" xref="S3.p2.2.m2.2.2.1.1.3">780</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2c">1{,}200\times 780</annotation></semantics></math> pixels and spans normal, rainy, foggy, and snowy weather conditions at daytime and nighttime. It follows the same conventions, <span id="S3.p2.3.4" class="ltx_text ltx_font_italic">i.e.</span>, classes definitions and color encoding,
as CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> and ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> datasets. However, we limit the number of classes to <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.p2.3.m3.1a"><mn id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><cn type="integer" id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">10</annotation></semantics></math>, namely <span id="S3.p2.3.5" class="ltx_text ltx_font_italic">Road</span>, <span id="S3.p2.3.6" class="ltx_text ltx_font_italic">Sidewalk</span>, <span id="S3.p2.3.7" class="ltx_text ltx_font_italic">Building</span>, <span id="S3.p2.3.8" class="ltx_text ltx_font_italic">Pole</span>, <span id="S3.p2.3.9" class="ltx_text ltx_font_italic">Traffic Light</span>, <span id="S3.p2.3.10" class="ltx_text ltx_font_italic">Traffic Sign</span>, <span id="S3.p2.3.11" class="ltx_text ltx_font_italic">Vegetation</span>, <span id="S3.p2.3.12" class="ltx_text ltx_font_italic">Sky</span>, <span id="S3.p2.3.13" class="ltx_text ltx_font_italic">Person</span>, and <span id="S3.p2.3.14" class="ltx_text ltx_font_italic">Car</span>. FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 The AWSS Dataset â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows sample images from the AWSS dataset spanning various standard and challenging attributes.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2210.05626/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold">Samples from AWSS dataset.</span> Our generated AWSS synthetic dataset spans normal, rainy, foggy, snowy, and nighttime attributes. </figcaption>
</figure>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Extensions to the Silver framework.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Silver</span> is based on the Unity game engineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Unity(2022)</a>]</cite>. It allows users to create 3D virtual worlds by only specifying a set of scene descriptive parameters like the weather condition, time-of-the-day, number of cars and humans, camera type, and lens artifacts. The simulator achieves photo-realism by using the recent High Definition Rendering Pipeline (HDRP). In addition, the simulator applies a set of Procedural Content Generation (PCG) concepts to generate, populate, and control the scenesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang</a>]</cite>.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p"><span id="S3.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">i) Adverse conditions.</span>
The original simulator can simulate standard and adverse weather conditions
at daytime and nighttime but with a limited photo-realism and diversity. For each weather condition, we diversify weather severeness, time-of-the-day, and other scene elements if not specified. Based on the environment being simulated, scene elements materials, shaders and textures are selected from a predefined large set. We customize and integrate Procedural TerrainÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">with Rocks and (HDRP)(2022)</a>]</cite> with Adobe Substance materialsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Material(2022)</a>]</cite> to simulate photo-realistic snow accumulation on ground, mud, mold, wet surfaces, and water puddles. Water drops splashes on the ground are simulated by customizing the Unity particle system. Rain splash intensity is controlled by the rain weather severeness which is sampled from a uniform distribution. Additionally, we simulate slightly foggy weather condition once heavy rain is simulated. For nighttime simulation, street lights are turned on and their intensity is randomized. Some of these lights are flickered or turned off to increase diversity.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p3.1" class="ltx_p"><span id="S3.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">ii) Dash camera mode.</span>
Initially <span id="S3.SS0.SSS0.Px1.p3.1.2" class="ltx_text ltx_font_italic">Silver</span> simulates Unmanned Aerial Vehicle (UAV) and first-person view cameras. However, most existing semantic segmentation datasets like CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> and ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> datasets are recorded using a dash camera mounted on a car. To generate our AWSS dataset, we develop the dash camera mode to facilitate this task. Furthermore, to increase view angle diversity, we simulate vertical and horizontal lens shifts.</p>
</div>
<div id="S3.SS0.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p4.1" class="ltx_p"><span id="S3.SS0.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_bold">iii) Semantic segmentation automatic ground-truth.</span>
The simulator supports semantic segmentation automatic ground-truth generation. However, the number of semantic classes was limited to 4: humans, ground, buildings, and trees. We extend the number of supported classes by adding new elements to the scene like traffic signs and modify the road mesh into road and sidewalk. At the same time, we customize the ground-truth generation pipeline to match CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> color codes and conventions. With our extension, <span id="S3.SS0.SSS0.Px1.p4.1.2" class="ltx_text ltx_font_italic">Silver</span> now can provide semantic segmentation ground-truth for 10 classes, as specified earlier in this section. 
</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2210.05626/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">An overview of our proposed architecture.</span> DCNN of DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite> is forced to learn weather and daytime-nighttime specific and roboust features by the means of multi-task learning. WAS and TAS branches learn to predict weather and daytime-nighttime, respectively. At the same time, they guide the encoder and specifically DCNN to learn extracting robust features under adverse and standard conditions. </figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.6" class="ltx_p">We aim to reduce the domain shift in adverse weather conditions while not acquiring additional real data. Hence, we propose a novel training approach that leverages synthetic data, while making the architecture aware of the weather condition and nighttime. Our architecture is trained on both synthetic and real data simultaneously (see Figure <a href="#S3.F3" title="Figure 3 â€£ Extensions to the Silver framework. â€£ 3 The AWSS Dataset â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Our methodology is based on three components: i) adding two simple networks WAS and TAS that work as supervisors to teach the model to learn weather and nighttime specific features; ii) the full-model is trained using multi-task learning where the baseline learn semantic segmentation and WAS and TAS learns to predict weather condition and day-night, respectively; iii) the model is trained on images from synthetic domainÂ <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{adv-synth}" display="inline"><semantics id="S4.p1.1.m1.1a"><msub id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mrow id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml"><mi id="S4.p1.1.m1.1.1.3.2.2" xref="S4.p1.1.m1.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.2.1" xref="S4.p1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.2.3" xref="S4.p1.1.m1.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.2.1a" xref="S4.p1.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.2.4" xref="S4.p1.1.m1.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p1.1.m1.1.1.3.1" xref="S4.p1.1.m1.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.1.m1.1.1.3.3" xref="S4.p1.1.m1.1.1.3.3.cmml"><mi id="S4.p1.1.m1.1.1.3.3.2" xref="S4.p1.1.m1.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.3.1" xref="S4.p1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.3.3" xref="S4.p1.1.m1.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.3.1a" xref="S4.p1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.3.4" xref="S4.p1.1.m1.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.3.1b" xref="S4.p1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.3.5" xref="S4.p1.1.m1.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.3.1c" xref="S4.p1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.1.m1.1.1.3.3.6" xref="S4.p1.1.m1.1.1.3.3.6.cmml">h</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><minus id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3.1"></minus><apply id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2"><times id="S4.p1.1.m1.1.1.3.2.1.cmml" xref="S4.p1.1.m1.1.1.3.2.1"></times><ci id="S4.p1.1.m1.1.1.3.2.2.cmml" xref="S4.p1.1.m1.1.1.3.2.2">ğ‘</ci><ci id="S4.p1.1.m1.1.1.3.2.3.cmml" xref="S4.p1.1.m1.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p1.1.m1.1.1.3.2.4.cmml" xref="S4.p1.1.m1.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p1.1.m1.1.1.3.3.cmml" xref="S4.p1.1.m1.1.1.3.3"><times id="S4.p1.1.m1.1.1.3.3.1.cmml" xref="S4.p1.1.m1.1.1.3.3.1"></times><ci id="S4.p1.1.m1.1.1.3.3.2.cmml" xref="S4.p1.1.m1.1.1.3.3.2">ğ‘ </ci><ci id="S4.p1.1.m1.1.1.3.3.3.cmml" xref="S4.p1.1.m1.1.1.3.3.3">ğ‘¦</ci><ci id="S4.p1.1.m1.1.1.3.3.4.cmml" xref="S4.p1.1.m1.1.1.3.3.4">ğ‘›</ci><ci id="S4.p1.1.m1.1.1.3.3.5.cmml" xref="S4.p1.1.m1.1.1.3.3.5">ğ‘¡</ci><ci id="S4.p1.1.m1.1.1.3.3.6.cmml" xref="S4.p1.1.m1.1.1.3.3.6">â„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\mathcal{D}_{adv-synth}</annotation></semantics></math> and real domainÂ <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{stand-real}" display="inline"><semantics id="S4.p1.2.m2.1a"><msub id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml"><mrow id="S4.p1.2.m2.1.1.3.2" xref="S4.p1.2.m2.1.1.3.2.cmml"><mi id="S4.p1.2.m2.1.1.3.2.2" xref="S4.p1.2.m2.1.1.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.2.1" xref="S4.p1.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.2.3" xref="S4.p1.2.m2.1.1.3.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.2.1a" xref="S4.p1.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.2.4" xref="S4.p1.2.m2.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.2.1b" xref="S4.p1.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.2.5" xref="S4.p1.2.m2.1.1.3.2.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.2.1c" xref="S4.p1.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.2.6" xref="S4.p1.2.m2.1.1.3.2.6.cmml">d</mi></mrow><mo id="S4.p1.2.m2.1.1.3.1" xref="S4.p1.2.m2.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.2.m2.1.1.3.3" xref="S4.p1.2.m2.1.1.3.3.cmml"><mi id="S4.p1.2.m2.1.1.3.3.2" xref="S4.p1.2.m2.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.3.1" xref="S4.p1.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.3.3" xref="S4.p1.2.m2.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.3.1a" xref="S4.p1.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.3.4" xref="S4.p1.2.m2.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.3.3.1b" xref="S4.p1.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.2.m2.1.1.3.3.5" xref="S4.p1.2.m2.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3"><minus id="S4.p1.2.m2.1.1.3.1.cmml" xref="S4.p1.2.m2.1.1.3.1"></minus><apply id="S4.p1.2.m2.1.1.3.2.cmml" xref="S4.p1.2.m2.1.1.3.2"><times id="S4.p1.2.m2.1.1.3.2.1.cmml" xref="S4.p1.2.m2.1.1.3.2.1"></times><ci id="S4.p1.2.m2.1.1.3.2.2.cmml" xref="S4.p1.2.m2.1.1.3.2.2">ğ‘ </ci><ci id="S4.p1.2.m2.1.1.3.2.3.cmml" xref="S4.p1.2.m2.1.1.3.2.3">ğ‘¡</ci><ci id="S4.p1.2.m2.1.1.3.2.4.cmml" xref="S4.p1.2.m2.1.1.3.2.4">ğ‘</ci><ci id="S4.p1.2.m2.1.1.3.2.5.cmml" xref="S4.p1.2.m2.1.1.3.2.5">ğ‘›</ci><ci id="S4.p1.2.m2.1.1.3.2.6.cmml" xref="S4.p1.2.m2.1.1.3.2.6">ğ‘‘</ci></apply><apply id="S4.p1.2.m2.1.1.3.3.cmml" xref="S4.p1.2.m2.1.1.3.3"><times id="S4.p1.2.m2.1.1.3.3.1.cmml" xref="S4.p1.2.m2.1.1.3.3.1"></times><ci id="S4.p1.2.m2.1.1.3.3.2.cmml" xref="S4.p1.2.m2.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p1.2.m2.1.1.3.3.3.cmml" xref="S4.p1.2.m2.1.1.3.3.3">ğ‘’</ci><ci id="S4.p1.2.m2.1.1.3.3.4.cmml" xref="S4.p1.2.m2.1.1.3.3.4">ğ‘</ci><ci id="S4.p1.2.m2.1.1.3.3.5.cmml" xref="S4.p1.2.m2.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathcal{D}_{stand-real}</annotation></semantics></math> in alternating fashion to ensure that the model learn to extract adverse weather features only from synthetic data which presents a proxy of the adverse real domainÂ <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{adv-real}" display="inline"><semantics id="S4.p1.3.m3.1a"><msub id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml"><mrow id="S4.p1.3.m3.1.1.3.2" xref="S4.p1.3.m3.1.1.3.2.cmml"><mi id="S4.p1.3.m3.1.1.3.2.2" xref="S4.p1.3.m3.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.1.3.2.1" xref="S4.p1.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.3.m3.1.1.3.2.3" xref="S4.p1.3.m3.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.1.3.2.1a" xref="S4.p1.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.3.m3.1.1.3.2.4" xref="S4.p1.3.m3.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p1.3.m3.1.1.3.1" xref="S4.p1.3.m3.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.3.m3.1.1.3.3" xref="S4.p1.3.m3.1.1.3.3.cmml"><mi id="S4.p1.3.m3.1.1.3.3.2" xref="S4.p1.3.m3.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.1.3.3.1" xref="S4.p1.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.3.m3.1.1.3.3.3" xref="S4.p1.3.m3.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.1.3.3.1a" xref="S4.p1.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.3.m3.1.1.3.3.4" xref="S4.p1.3.m3.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.3.m3.1.1.3.3.1b" xref="S4.p1.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.3.m3.1.1.3.3.5" xref="S4.p1.3.m3.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3"><minus id="S4.p1.3.m3.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.3.1"></minus><apply id="S4.p1.3.m3.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.3.2"><times id="S4.p1.3.m3.1.1.3.2.1.cmml" xref="S4.p1.3.m3.1.1.3.2.1"></times><ci id="S4.p1.3.m3.1.1.3.2.2.cmml" xref="S4.p1.3.m3.1.1.3.2.2">ğ‘</ci><ci id="S4.p1.3.m3.1.1.3.2.3.cmml" xref="S4.p1.3.m3.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p1.3.m3.1.1.3.2.4.cmml" xref="S4.p1.3.m3.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p1.3.m3.1.1.3.3.cmml" xref="S4.p1.3.m3.1.1.3.3"><times id="S4.p1.3.m3.1.1.3.3.1.cmml" xref="S4.p1.3.m3.1.1.3.3.1"></times><ci id="S4.p1.3.m3.1.1.3.3.2.cmml" xref="S4.p1.3.m3.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p1.3.m3.1.1.3.3.3.cmml" xref="S4.p1.3.m3.1.1.3.3.3">ğ‘’</ci><ci id="S4.p1.3.m3.1.1.3.3.4.cmml" xref="S4.p1.3.m3.1.1.3.3.4">ğ‘</ci><ci id="S4.p1.3.m3.1.1.3.3.5.cmml" xref="S4.p1.3.m3.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\mathcal{D}_{adv-real}</annotation></semantics></math>. At the same time, it does not overfit to synthetic data and still ensure that the architecture other components leverage real data. Throughout the paper, <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{stand-real}" display="inline"><semantics id="S4.p1.4.m4.1a"><msub id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml"><mrow id="S4.p1.4.m4.1.1.3.2" xref="S4.p1.4.m4.1.1.3.2.cmml"><mi id="S4.p1.4.m4.1.1.3.2.2" xref="S4.p1.4.m4.1.1.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.2.1" xref="S4.p1.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.2.3" xref="S4.p1.4.m4.1.1.3.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.2.1a" xref="S4.p1.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.2.4" xref="S4.p1.4.m4.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.2.1b" xref="S4.p1.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.2.5" xref="S4.p1.4.m4.1.1.3.2.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.2.1c" xref="S4.p1.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.2.6" xref="S4.p1.4.m4.1.1.3.2.6.cmml">d</mi></mrow><mo id="S4.p1.4.m4.1.1.3.1" xref="S4.p1.4.m4.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.4.m4.1.1.3.3" xref="S4.p1.4.m4.1.1.3.3.cmml"><mi id="S4.p1.4.m4.1.1.3.3.2" xref="S4.p1.4.m4.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.3.1" xref="S4.p1.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.3.3" xref="S4.p1.4.m4.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.3.1a" xref="S4.p1.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.3.4" xref="S4.p1.4.m4.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.3.1b" xref="S4.p1.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.3.5" xref="S4.p1.4.m4.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3"><minus id="S4.p1.4.m4.1.1.3.1.cmml" xref="S4.p1.4.m4.1.1.3.1"></minus><apply id="S4.p1.4.m4.1.1.3.2.cmml" xref="S4.p1.4.m4.1.1.3.2"><times id="S4.p1.4.m4.1.1.3.2.1.cmml" xref="S4.p1.4.m4.1.1.3.2.1"></times><ci id="S4.p1.4.m4.1.1.3.2.2.cmml" xref="S4.p1.4.m4.1.1.3.2.2">ğ‘ </ci><ci id="S4.p1.4.m4.1.1.3.2.3.cmml" xref="S4.p1.4.m4.1.1.3.2.3">ğ‘¡</ci><ci id="S4.p1.4.m4.1.1.3.2.4.cmml" xref="S4.p1.4.m4.1.1.3.2.4">ğ‘</ci><ci id="S4.p1.4.m4.1.1.3.2.5.cmml" xref="S4.p1.4.m4.1.1.3.2.5">ğ‘›</ci><ci id="S4.p1.4.m4.1.1.3.2.6.cmml" xref="S4.p1.4.m4.1.1.3.2.6">ğ‘‘</ci></apply><apply id="S4.p1.4.m4.1.1.3.3.cmml" xref="S4.p1.4.m4.1.1.3.3"><times id="S4.p1.4.m4.1.1.3.3.1.cmml" xref="S4.p1.4.m4.1.1.3.3.1"></times><ci id="S4.p1.4.m4.1.1.3.3.2.cmml" xref="S4.p1.4.m4.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p1.4.m4.1.1.3.3.3.cmml" xref="S4.p1.4.m4.1.1.3.3.3">ğ‘’</ci><ci id="S4.p1.4.m4.1.1.3.3.4.cmml" xref="S4.p1.4.m4.1.1.3.3.4">ğ‘</ci><ci id="S4.p1.4.m4.1.1.3.3.5.cmml" xref="S4.p1.4.m4.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">\mathcal{D}_{stand-real}</annotation></semantics></math>, <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{adv-real}" display="inline"><semantics id="S4.p1.5.m5.1a"><msub id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.5.m5.1.1.2" xref="S4.p1.5.m5.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.5.m5.1.1.3" xref="S4.p1.5.m5.1.1.3.cmml"><mrow id="S4.p1.5.m5.1.1.3.2" xref="S4.p1.5.m5.1.1.3.2.cmml"><mi id="S4.p1.5.m5.1.1.3.2.2" xref="S4.p1.5.m5.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.2.1" xref="S4.p1.5.m5.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.2.3" xref="S4.p1.5.m5.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.2.1a" xref="S4.p1.5.m5.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.2.4" xref="S4.p1.5.m5.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p1.5.m5.1.1.3.1" xref="S4.p1.5.m5.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.5.m5.1.1.3.3" xref="S4.p1.5.m5.1.1.3.3.cmml"><mi id="S4.p1.5.m5.1.1.3.3.2" xref="S4.p1.5.m5.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.3.1" xref="S4.p1.5.m5.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.3.3" xref="S4.p1.5.m5.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.3.1a" xref="S4.p1.5.m5.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.3.4" xref="S4.p1.5.m5.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.3.1b" xref="S4.p1.5.m5.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.3.5" xref="S4.p1.5.m5.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><apply id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.1.cmml" xref="S4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.p1.5.m5.1.1.2.cmml" xref="S4.p1.5.m5.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.5.m5.1.1.3.cmml" xref="S4.p1.5.m5.1.1.3"><minus id="S4.p1.5.m5.1.1.3.1.cmml" xref="S4.p1.5.m5.1.1.3.1"></minus><apply id="S4.p1.5.m5.1.1.3.2.cmml" xref="S4.p1.5.m5.1.1.3.2"><times id="S4.p1.5.m5.1.1.3.2.1.cmml" xref="S4.p1.5.m5.1.1.3.2.1"></times><ci id="S4.p1.5.m5.1.1.3.2.2.cmml" xref="S4.p1.5.m5.1.1.3.2.2">ğ‘</ci><ci id="S4.p1.5.m5.1.1.3.2.3.cmml" xref="S4.p1.5.m5.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p1.5.m5.1.1.3.2.4.cmml" xref="S4.p1.5.m5.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p1.5.m5.1.1.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3"><times id="S4.p1.5.m5.1.1.3.3.1.cmml" xref="S4.p1.5.m5.1.1.3.3.1"></times><ci id="S4.p1.5.m5.1.1.3.3.2.cmml" xref="S4.p1.5.m5.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p1.5.m5.1.1.3.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3.3">ğ‘’</ci><ci id="S4.p1.5.m5.1.1.3.3.4.cmml" xref="S4.p1.5.m5.1.1.3.3.4">ğ‘</ci><ci id="S4.p1.5.m5.1.1.3.3.5.cmml" xref="S4.p1.5.m5.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">\mathcal{D}_{adv-real}</annotation></semantics></math>, and <math id="S4.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{D}_{adv-synth}" display="inline"><semantics id="S4.p1.6.m6.1a"><msub id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.6.m6.1.1.2" xref="S4.p1.6.m6.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p1.6.m6.1.1.3" xref="S4.p1.6.m6.1.1.3.cmml"><mrow id="S4.p1.6.m6.1.1.3.2" xref="S4.p1.6.m6.1.1.3.2.cmml"><mi id="S4.p1.6.m6.1.1.3.2.2" xref="S4.p1.6.m6.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.2.1" xref="S4.p1.6.m6.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.2.3" xref="S4.p1.6.m6.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.2.1a" xref="S4.p1.6.m6.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.2.4" xref="S4.p1.6.m6.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p1.6.m6.1.1.3.1" xref="S4.p1.6.m6.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p1.6.m6.1.1.3.3" xref="S4.p1.6.m6.1.1.3.3.cmml"><mi id="S4.p1.6.m6.1.1.3.3.2" xref="S4.p1.6.m6.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.3.1" xref="S4.p1.6.m6.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.3.3" xref="S4.p1.6.m6.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.3.1a" xref="S4.p1.6.m6.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.3.4" xref="S4.p1.6.m6.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.3.1b" xref="S4.p1.6.m6.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.3.5" xref="S4.p1.6.m6.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.6.m6.1.1.3.3.1c" xref="S4.p1.6.m6.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p1.6.m6.1.1.3.3.6" xref="S4.p1.6.m6.1.1.3.3.6.cmml">h</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><apply id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p1.6.m6.1.1.1.cmml" xref="S4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.p1.6.m6.1.1.2.cmml" xref="S4.p1.6.m6.1.1.2">ğ’Ÿ</ci><apply id="S4.p1.6.m6.1.1.3.cmml" xref="S4.p1.6.m6.1.1.3"><minus id="S4.p1.6.m6.1.1.3.1.cmml" xref="S4.p1.6.m6.1.1.3.1"></minus><apply id="S4.p1.6.m6.1.1.3.2.cmml" xref="S4.p1.6.m6.1.1.3.2"><times id="S4.p1.6.m6.1.1.3.2.1.cmml" xref="S4.p1.6.m6.1.1.3.2.1"></times><ci id="S4.p1.6.m6.1.1.3.2.2.cmml" xref="S4.p1.6.m6.1.1.3.2.2">ğ‘</ci><ci id="S4.p1.6.m6.1.1.3.2.3.cmml" xref="S4.p1.6.m6.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p1.6.m6.1.1.3.2.4.cmml" xref="S4.p1.6.m6.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p1.6.m6.1.1.3.3.cmml" xref="S4.p1.6.m6.1.1.3.3"><times id="S4.p1.6.m6.1.1.3.3.1.cmml" xref="S4.p1.6.m6.1.1.3.3.1"></times><ci id="S4.p1.6.m6.1.1.3.3.2.cmml" xref="S4.p1.6.m6.1.1.3.3.2">ğ‘ </ci><ci id="S4.p1.6.m6.1.1.3.3.3.cmml" xref="S4.p1.6.m6.1.1.3.3.3">ğ‘¦</ci><ci id="S4.p1.6.m6.1.1.3.3.4.cmml" xref="S4.p1.6.m6.1.1.3.3.4">ğ‘›</ci><ci id="S4.p1.6.m6.1.1.3.3.5.cmml" xref="S4.p1.6.m6.1.1.3.3.5">ğ‘¡</ci><ci id="S4.p1.6.m6.1.1.3.3.6.cmml" xref="S4.p1.6.m6.1.1.3.3.6">â„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">\mathcal{D}_{adv-synth}</annotation></semantics></math> are represented by CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite>, ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite>, and AWSS datasets, respectively.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Weather and nighttime aware encoder.</span>
We use the DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite> architecture because of its powerful encoder-decoder architecture. Originally, it is assumed that the encoder will learn how to extract low-level and high-level features independent of weather and illumination conditions. This prevents the model from learning how to extract weather-specific features, resulting in low-quality features being fed to the decoder. The problem becomes even harder without training samples under these conditions.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p">To alleviate this problem, we focus on the Deep Convolutional Neural Network (DCNN) which is a modified version of XceptionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Chollet(2017)</a>]</cite>. We leverage multi-task learning to enforce the encoder to learn weather and time specific features. We add two simple identical models Weather-Aware-Supervisor (WAS) and Time-Aware-Supervisor (TAS). Each model is composed of two <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">3</cn><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">3\times 3</annotation></semantics></math> atrous 2D convolutions with a rate of <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="integer" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">2</annotation></semantics></math> and padding of <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">6</annotation></semantics></math>. Each convolution is followed by a batch normalization and a rectified linear unit (ReLU). After this, the feature map is flattened and fed to 3 fully connected layers. The last layer predicts the weather for WAS and the daytime-nighttime for TAS. It is worth noting that WAS and TAS are only activated in the training process to guide the feature extraction learning process.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Multi-task learning to improve semantic segmentation.</span>
In the original implementation of DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite>, the output of DCNN is passed to the remaining part of the encoder and to the decoder. In our implementation, we also feed the output of DCNN to WAS and TAS. The total objective to train the new architecture is defined as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\min_{\theta}\mathcal{L}=\mathcal{L}_{Segment}+\alpha\times\mathcal{L}_{WAS}+\beta\times\mathcal{L}_{TAS}," display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml"><munder id="S4.E1.m1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.2.1.cmml"><mi id="S4.E1.m1.1.1.1.1.2.1.2" xref="S4.E1.m1.1.1.1.1.2.1.2.cmml">min</mi><mi id="S4.E1.m1.1.1.1.1.2.1.3" xref="S4.E1.m1.1.1.1.1.2.1.3.cmml">Î¸</mi></munder><mo lspace="0.167em" id="S4.E1.m1.1.1.1.1.2a" xref="S4.E1.m1.1.1.1.1.2.cmml">â¡</mo><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.2.2.cmml">â„’</mi></mrow><mo id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.1.3.2.2.cmml">â„’</mi><mrow id="S4.E1.m1.1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.1.1.3.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1a" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.4" xref="S4.E1.m1.1.1.1.1.3.2.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1b" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.5" xref="S4.E1.m1.1.1.1.1.3.2.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1c" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.6" xref="S4.E1.m1.1.1.1.1.3.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1d" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.7" xref="S4.E1.m1.1.1.1.1.3.2.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.2.3.1e" xref="S4.E1.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.2.3.8" xref="S4.E1.m1.1.1.1.1.3.2.3.8.cmml">t</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.2.cmml">Î±</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.3.3.1.cmml">Ã—</mo><msub id="S4.E1.m1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.3.2.cmml">â„’</mi><mrow id="S4.E1.m1.1.1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.3.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.3.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.3.3.3.1" xref="S4.E1.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.3.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.3.3.3.1a" xref="S4.E1.m1.1.1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.3.3.3.4" xref="S4.E1.m1.1.1.1.1.3.3.3.3.4.cmml">S</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.1.1.3.1a" xref="S4.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1.3.4" xref="S4.E1.m1.1.1.1.1.3.4.cmml"><mi id="S4.E1.m1.1.1.1.1.3.4.2" xref="S4.E1.m1.1.1.1.1.3.4.2.cmml">Î²</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.1.3.4.1" xref="S4.E1.m1.1.1.1.1.3.4.1.cmml">Ã—</mo><msub id="S4.E1.m1.1.1.1.1.3.4.3" xref="S4.E1.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.3.4.3.2" xref="S4.E1.m1.1.1.1.1.3.4.3.2.cmml">â„’</mi><mrow id="S4.E1.m1.1.1.1.1.3.4.3.3" xref="S4.E1.m1.1.1.1.1.3.4.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.4.3.3.2" xref="S4.E1.m1.1.1.1.1.3.4.3.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.4.3.3.1" xref="S4.E1.m1.1.1.1.1.3.4.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.4.3.3.3" xref="S4.E1.m1.1.1.1.1.3.4.3.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.4.3.3.1a" xref="S4.E1.m1.1.1.1.1.3.4.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.1.1.3.4.3.3.4" xref="S4.E1.m1.1.1.1.1.3.4.3.3.4.cmml">S</mi></mrow></msub></mrow></mrow></mrow><mo id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"></eq><apply id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><apply id="S4.E1.m1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.1.1.cmml" xref="S4.E1.m1.1.1.1.1.2.1">subscript</csymbol><min id="S4.E1.m1.1.1.1.1.2.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2.1.2"></min><ci id="S4.E1.m1.1.1.1.1.2.1.3.cmml" xref="S4.E1.m1.1.1.1.1.2.1.3">ğœƒ</ci></apply><ci id="S4.E1.m1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2">â„’</ci></apply><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><plus id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.1"></plus><apply id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2.2">â„’</ci><apply id="S4.E1.m1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3"><times id="S4.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.2">ğ‘†</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.3">ğ‘’</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.4.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.4">ğ‘”</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.5.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.5">ğ‘š</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.6.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.6">ğ‘’</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.7.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.7">ğ‘›</ci><ci id="S4.E1.m1.1.1.1.1.3.2.3.8.cmml" xref="S4.E1.m1.1.1.1.1.3.2.3.8">ğ‘¡</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2">ğ›¼</ci><apply id="S4.E1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.2">â„’</ci><apply id="S4.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3"><times id="S4.E1.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3.2">ğ‘Š</ci><ci id="S4.E1.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3.3">ğ´</ci><ci id="S4.E1.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3.3.4">ğ‘†</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.1.1.3.4"><times id="S4.E1.m1.1.1.1.1.3.4.1.cmml" xref="S4.E1.m1.1.1.1.1.3.4.1"></times><ci id="S4.E1.m1.1.1.1.1.3.4.2.cmml" xref="S4.E1.m1.1.1.1.1.3.4.2">ğ›½</ci><apply id="S4.E1.m1.1.1.1.1.3.4.3.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.4.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.4.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.2">â„’</ci><apply id="S4.E1.m1.1.1.1.1.3.4.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.3"><times id="S4.E1.m1.1.1.1.1.3.4.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.4.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.3.2">ğ‘‡</ci><ci id="S4.E1.m1.1.1.1.1.3.4.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.3.3">ğ´</ci><ci id="S4.E1.m1.1.1.1.1.3.4.3.3.4.cmml" xref="S4.E1.m1.1.1.1.1.3.4.3.3.4">ğ‘†</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\min_{\theta}\mathcal{L}=\mathcal{L}_{Segment}+\alpha\times\mathcal{L}_{WAS}+\beta\times\mathcal{L}_{TAS},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.9" class="ltx_p">where <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{Segment}" display="inline"><semantics id="S4.p5.1.m1.1a"><msub id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">â„’</mi><mrow id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml"><mi id="S4.p5.1.m1.1.1.3.2" xref="S4.p5.1.m1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.3" xref="S4.p5.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1a" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.4" xref="S4.p5.1.m1.1.1.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1b" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.5" xref="S4.p5.1.m1.1.1.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1c" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.6" xref="S4.p5.1.m1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1d" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.7" xref="S4.p5.1.m1.1.1.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.3.1e" xref="S4.p5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3.8" xref="S4.p5.1.m1.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1">subscript</csymbol><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">â„’</ci><apply id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3"><times id="S4.p5.1.m1.1.1.3.1.cmml" xref="S4.p5.1.m1.1.1.3.1"></times><ci id="S4.p5.1.m1.1.1.3.2.cmml" xref="S4.p5.1.m1.1.1.3.2">ğ‘†</ci><ci id="S4.p5.1.m1.1.1.3.3.cmml" xref="S4.p5.1.m1.1.1.3.3">ğ‘’</ci><ci id="S4.p5.1.m1.1.1.3.4.cmml" xref="S4.p5.1.m1.1.1.3.4">ğ‘”</ci><ci id="S4.p5.1.m1.1.1.3.5.cmml" xref="S4.p5.1.m1.1.1.3.5">ğ‘š</ci><ci id="S4.p5.1.m1.1.1.3.6.cmml" xref="S4.p5.1.m1.1.1.3.6">ğ‘’</ci><ci id="S4.p5.1.m1.1.1.3.7.cmml" xref="S4.p5.1.m1.1.1.3.7">ğ‘›</ci><ci id="S4.p5.1.m1.1.1.3.8.cmml" xref="S4.p5.1.m1.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\mathcal{L}_{Segment}</annotation></semantics></math> is the original loss used to train DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite>, <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{WAS}" display="inline"><semantics id="S4.p5.2.m2.1a"><msub id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">â„’</mi><mrow id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml"><mi id="S4.p5.2.m2.1.1.3.2" xref="S4.p5.2.m2.1.1.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.3.1" xref="S4.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.2.m2.1.1.3.3" xref="S4.p5.2.m2.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p5.2.m2.1.1.3.1a" xref="S4.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.2.m2.1.1.3.4" xref="S4.p5.2.m2.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1">subscript</csymbol><ci id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">â„’</ci><apply id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3"><times id="S4.p5.2.m2.1.1.3.1.cmml" xref="S4.p5.2.m2.1.1.3.1"></times><ci id="S4.p5.2.m2.1.1.3.2.cmml" xref="S4.p5.2.m2.1.1.3.2">ğ‘Š</ci><ci id="S4.p5.2.m2.1.1.3.3.cmml" xref="S4.p5.2.m2.1.1.3.3">ğ´</ci><ci id="S4.p5.2.m2.1.1.3.4.cmml" xref="S4.p5.2.m2.1.1.3.4">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">\mathcal{L}_{WAS}</annotation></semantics></math> and <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{TAS}" display="inline"><semantics id="S4.p5.3.m3.1a"><msub id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.3.m3.1.1.2" xref="S4.p5.3.m3.1.1.2.cmml">â„’</mi><mrow id="S4.p5.3.m3.1.1.3" xref="S4.p5.3.m3.1.1.3.cmml"><mi id="S4.p5.3.m3.1.1.3.2" xref="S4.p5.3.m3.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.p5.3.m3.1.1.3.1" xref="S4.p5.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.3.m3.1.1.3.3" xref="S4.p5.3.m3.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p5.3.m3.1.1.3.1a" xref="S4.p5.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.3.m3.1.1.3.4" xref="S4.p5.3.m3.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><apply id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p5.3.m3.1.1.1.cmml" xref="S4.p5.3.m3.1.1">subscript</csymbol><ci id="S4.p5.3.m3.1.1.2.cmml" xref="S4.p5.3.m3.1.1.2">â„’</ci><apply id="S4.p5.3.m3.1.1.3.cmml" xref="S4.p5.3.m3.1.1.3"><times id="S4.p5.3.m3.1.1.3.1.cmml" xref="S4.p5.3.m3.1.1.3.1"></times><ci id="S4.p5.3.m3.1.1.3.2.cmml" xref="S4.p5.3.m3.1.1.3.2">ğ‘‡</ci><ci id="S4.p5.3.m3.1.1.3.3.cmml" xref="S4.p5.3.m3.1.1.3.3">ğ´</ci><ci id="S4.p5.3.m3.1.1.3.4.cmml" xref="S4.p5.3.m3.1.1.3.4">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">\mathcal{L}_{TAS}</annotation></semantics></math> are the cross-entropy losses utilized to train WAS and TAS, respectively. <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p5.4.m4.1a"><mi id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><ci id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">\alpha</annotation></semantics></math> and <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.p5.5.m5.1a"><mi id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><ci id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">\beta</annotation></semantics></math> are scalars to ensure numerical stability during the training and to give more emphasis to the main loss, i.e., <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{Segment}" display="inline"><semantics id="S4.p5.6.m6.1a"><msub id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.6.m6.1.1.2" xref="S4.p5.6.m6.1.1.2.cmml">â„’</mi><mrow id="S4.p5.6.m6.1.1.3" xref="S4.p5.6.m6.1.1.3.cmml"><mi id="S4.p5.6.m6.1.1.3.2" xref="S4.p5.6.m6.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.3" xref="S4.p5.6.m6.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1a" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.4" xref="S4.p5.6.m6.1.1.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1b" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.5" xref="S4.p5.6.m6.1.1.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1c" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.6" xref="S4.p5.6.m6.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1d" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.7" xref="S4.p5.6.m6.1.1.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1e" xref="S4.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.6.m6.1.1.3.8" xref="S4.p5.6.m6.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><apply id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p5.6.m6.1.1.1.cmml" xref="S4.p5.6.m6.1.1">subscript</csymbol><ci id="S4.p5.6.m6.1.1.2.cmml" xref="S4.p5.6.m6.1.1.2">â„’</ci><apply id="S4.p5.6.m6.1.1.3.cmml" xref="S4.p5.6.m6.1.1.3"><times id="S4.p5.6.m6.1.1.3.1.cmml" xref="S4.p5.6.m6.1.1.3.1"></times><ci id="S4.p5.6.m6.1.1.3.2.cmml" xref="S4.p5.6.m6.1.1.3.2">ğ‘†</ci><ci id="S4.p5.6.m6.1.1.3.3.cmml" xref="S4.p5.6.m6.1.1.3.3">ğ‘’</ci><ci id="S4.p5.6.m6.1.1.3.4.cmml" xref="S4.p5.6.m6.1.1.3.4">ğ‘”</ci><ci id="S4.p5.6.m6.1.1.3.5.cmml" xref="S4.p5.6.m6.1.1.3.5">ğ‘š</ci><ci id="S4.p5.6.m6.1.1.3.6.cmml" xref="S4.p5.6.m6.1.1.3.6">ğ‘’</ci><ci id="S4.p5.6.m6.1.1.3.7.cmml" xref="S4.p5.6.m6.1.1.3.7">ğ‘›</ci><ci id="S4.p5.6.m6.1.1.3.8.cmml" xref="S4.p5.6.m6.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">\mathcal{L}_{Segment}</annotation></semantics></math>. It should be noted that each loss is back-propagated separately. <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="\mathcal{L}_{Segment}" display="inline"><semantics id="S4.p5.7.m7.1a"><msub id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.7.m7.1.1.2" xref="S4.p5.7.m7.1.1.2.cmml">â„’</mi><mrow id="S4.p5.7.m7.1.1.3" xref="S4.p5.7.m7.1.1.3.cmml"><mi id="S4.p5.7.m7.1.1.3.2" xref="S4.p5.7.m7.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.3" xref="S4.p5.7.m7.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1a" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.4" xref="S4.p5.7.m7.1.1.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1b" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.5" xref="S4.p5.7.m7.1.1.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1c" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.6" xref="S4.p5.7.m7.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1d" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.7" xref="S4.p5.7.m7.1.1.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p5.7.m7.1.1.3.1e" xref="S4.p5.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.7.m7.1.1.3.8" xref="S4.p5.7.m7.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><apply id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S4.p5.7.m7.1.1.1.cmml" xref="S4.p5.7.m7.1.1">subscript</csymbol><ci id="S4.p5.7.m7.1.1.2.cmml" xref="S4.p5.7.m7.1.1.2">â„’</ci><apply id="S4.p5.7.m7.1.1.3.cmml" xref="S4.p5.7.m7.1.1.3"><times id="S4.p5.7.m7.1.1.3.1.cmml" xref="S4.p5.7.m7.1.1.3.1"></times><ci id="S4.p5.7.m7.1.1.3.2.cmml" xref="S4.p5.7.m7.1.1.3.2">ğ‘†</ci><ci id="S4.p5.7.m7.1.1.3.3.cmml" xref="S4.p5.7.m7.1.1.3.3">ğ‘’</ci><ci id="S4.p5.7.m7.1.1.3.4.cmml" xref="S4.p5.7.m7.1.1.3.4">ğ‘”</ci><ci id="S4.p5.7.m7.1.1.3.5.cmml" xref="S4.p5.7.m7.1.1.3.5">ğ‘š</ci><ci id="S4.p5.7.m7.1.1.3.6.cmml" xref="S4.p5.7.m7.1.1.3.6">ğ‘’</ci><ci id="S4.p5.7.m7.1.1.3.7.cmml" xref="S4.p5.7.m7.1.1.3.7">ğ‘›</ci><ci id="S4.p5.7.m7.1.1.3.8.cmml" xref="S4.p5.7.m7.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">\mathcal{L}_{Segment}</annotation></semantics></math> is back-propagated over all the architecture except WAS and TAS. On the other hand, <math id="S4.p5.8.m8.1" class="ltx_Math" alttext="\mathcal{L}_{WAS}" display="inline"><semantics id="S4.p5.8.m8.1a"><msub id="S4.p5.8.m8.1.1" xref="S4.p5.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.8.m8.1.1.2" xref="S4.p5.8.m8.1.1.2.cmml">â„’</mi><mrow id="S4.p5.8.m8.1.1.3" xref="S4.p5.8.m8.1.1.3.cmml"><mi id="S4.p5.8.m8.1.1.3.2" xref="S4.p5.8.m8.1.1.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.p5.8.m8.1.1.3.1" xref="S4.p5.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.8.m8.1.1.3.3" xref="S4.p5.8.m8.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p5.8.m8.1.1.3.1a" xref="S4.p5.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.8.m8.1.1.3.4" xref="S4.p5.8.m8.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.8.m8.1b"><apply id="S4.p5.8.m8.1.1.cmml" xref="S4.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p5.8.m8.1.1.1.cmml" xref="S4.p5.8.m8.1.1">subscript</csymbol><ci id="S4.p5.8.m8.1.1.2.cmml" xref="S4.p5.8.m8.1.1.2">â„’</ci><apply id="S4.p5.8.m8.1.1.3.cmml" xref="S4.p5.8.m8.1.1.3"><times id="S4.p5.8.m8.1.1.3.1.cmml" xref="S4.p5.8.m8.1.1.3.1"></times><ci id="S4.p5.8.m8.1.1.3.2.cmml" xref="S4.p5.8.m8.1.1.3.2">ğ‘Š</ci><ci id="S4.p5.8.m8.1.1.3.3.cmml" xref="S4.p5.8.m8.1.1.3.3">ğ´</ci><ci id="S4.p5.8.m8.1.1.3.4.cmml" xref="S4.p5.8.m8.1.1.3.4">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.8.m8.1c">\mathcal{L}_{WAS}</annotation></semantics></math> and <math id="S4.p5.9.m9.1" class="ltx_Math" alttext="\mathcal{L}_{TAS}" display="inline"><semantics id="S4.p5.9.m9.1a"><msub id="S4.p5.9.m9.1.1" xref="S4.p5.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.9.m9.1.1.2" xref="S4.p5.9.m9.1.1.2.cmml">â„’</mi><mrow id="S4.p5.9.m9.1.1.3" xref="S4.p5.9.m9.1.1.3.cmml"><mi id="S4.p5.9.m9.1.1.3.2" xref="S4.p5.9.m9.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.p5.9.m9.1.1.3.1" xref="S4.p5.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.9.m9.1.1.3.3" xref="S4.p5.9.m9.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.p5.9.m9.1.1.3.1a" xref="S4.p5.9.m9.1.1.3.1.cmml">â€‹</mo><mi id="S4.p5.9.m9.1.1.3.4" xref="S4.p5.9.m9.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.9.m9.1b"><apply id="S4.p5.9.m9.1.1.cmml" xref="S4.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p5.9.m9.1.1.1.cmml" xref="S4.p5.9.m9.1.1">subscript</csymbol><ci id="S4.p5.9.m9.1.1.2.cmml" xref="S4.p5.9.m9.1.1.2">â„’</ci><apply id="S4.p5.9.m9.1.1.3.cmml" xref="S4.p5.9.m9.1.1.3"><times id="S4.p5.9.m9.1.1.3.1.cmml" xref="S4.p5.9.m9.1.1.3.1"></times><ci id="S4.p5.9.m9.1.1.3.2.cmml" xref="S4.p5.9.m9.1.1.3.2">ğ‘‡</ci><ci id="S4.p5.9.m9.1.1.3.3.cmml" xref="S4.p5.9.m9.1.1.3.3">ğ´</ci><ci id="S4.p5.9.m9.1.1.3.4.cmml" xref="S4.p5.9.m9.1.1.3.4">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.9.m9.1c">\mathcal{L}_{TAS}</annotation></semantics></math> are back-propagated only to DCNN.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Synthetic-aware training procedure.</span>
Training on source domain and fine-tuning on the target domain is a well-known approach to mitigate the domain gapÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx40" title="" class="ltx_ref">Tzeng etÂ al.(2017)Tzeng, Hoffman, Saenko, and
Darrell</a>]</cite>. However, it is not practical as it requires annotated real data from the target domain which may not be always affordable. Furthermore, training the model on data from one distribution and then forcing the model to learn a new distribution limits the ability of the network to learn and may not converge to a global minima.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.4" class="ltx_p">Thus, we propose training our modified DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite> on data from both synthetic and real distributions simultaneously and from scratch. For that aim, we train in alternating fashion: one batch fromÂ <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{stand-real}" display="inline"><semantics id="S4.p7.1.m1.1a"><msub id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mrow id="S4.p7.1.m1.1.1.3.2" xref="S4.p7.1.m1.1.1.3.2.cmml"><mi id="S4.p7.1.m1.1.1.3.2.2" xref="S4.p7.1.m1.1.1.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.2.1" xref="S4.p7.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.2.3" xref="S4.p7.1.m1.1.1.3.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.2.1a" xref="S4.p7.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.2.4" xref="S4.p7.1.m1.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.2.1b" xref="S4.p7.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.2.5" xref="S4.p7.1.m1.1.1.3.2.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.2.1c" xref="S4.p7.1.m1.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.2.6" xref="S4.p7.1.m1.1.1.3.2.6.cmml">d</mi></mrow><mo id="S4.p7.1.m1.1.1.3.1" xref="S4.p7.1.m1.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p7.1.m1.1.1.3.3" xref="S4.p7.1.m1.1.1.3.3.cmml"><mi id="S4.p7.1.m1.1.1.3.3.2" xref="S4.p7.1.m1.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.3.1" xref="S4.p7.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.3.3" xref="S4.p7.1.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.3.1a" xref="S4.p7.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.3.4" xref="S4.p7.1.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.1.m1.1.1.3.3.1b" xref="S4.p7.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.1.m1.1.1.3.3.5" xref="S4.p7.1.m1.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1">subscript</csymbol><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">ğ’Ÿ</ci><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><minus id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3.1"></minus><apply id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3.2"><times id="S4.p7.1.m1.1.1.3.2.1.cmml" xref="S4.p7.1.m1.1.1.3.2.1"></times><ci id="S4.p7.1.m1.1.1.3.2.2.cmml" xref="S4.p7.1.m1.1.1.3.2.2">ğ‘ </ci><ci id="S4.p7.1.m1.1.1.3.2.3.cmml" xref="S4.p7.1.m1.1.1.3.2.3">ğ‘¡</ci><ci id="S4.p7.1.m1.1.1.3.2.4.cmml" xref="S4.p7.1.m1.1.1.3.2.4">ğ‘</ci><ci id="S4.p7.1.m1.1.1.3.2.5.cmml" xref="S4.p7.1.m1.1.1.3.2.5">ğ‘›</ci><ci id="S4.p7.1.m1.1.1.3.2.6.cmml" xref="S4.p7.1.m1.1.1.3.2.6">ğ‘‘</ci></apply><apply id="S4.p7.1.m1.1.1.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3"><times id="S4.p7.1.m1.1.1.3.3.1.cmml" xref="S4.p7.1.m1.1.1.3.3.1"></times><ci id="S4.p7.1.m1.1.1.3.3.2.cmml" xref="S4.p7.1.m1.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p7.1.m1.1.1.3.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3.3">ğ‘’</ci><ci id="S4.p7.1.m1.1.1.3.3.4.cmml" xref="S4.p7.1.m1.1.1.3.3.4">ğ‘</ci><ci id="S4.p7.1.m1.1.1.3.3.5.cmml" xref="S4.p7.1.m1.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\mathcal{D}_{stand-real}</annotation></semantics></math> and next batch fromÂ <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{adv-synth}" display="inline"><semantics id="S4.p7.2.m2.1a"><msub id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml"><mrow id="S4.p7.2.m2.1.1.3.2" xref="S4.p7.2.m2.1.1.3.2.cmml"><mi id="S4.p7.2.m2.1.1.3.2.2" xref="S4.p7.2.m2.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.2.1" xref="S4.p7.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.2.3" xref="S4.p7.2.m2.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.2.1a" xref="S4.p7.2.m2.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.2.4" xref="S4.p7.2.m2.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p7.2.m2.1.1.3.1" xref="S4.p7.2.m2.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p7.2.m2.1.1.3.3" xref="S4.p7.2.m2.1.1.3.3.cmml"><mi id="S4.p7.2.m2.1.1.3.3.2" xref="S4.p7.2.m2.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.3.1" xref="S4.p7.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.3.3" xref="S4.p7.2.m2.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.3.1a" xref="S4.p7.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.3.4" xref="S4.p7.2.m2.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.3.1b" xref="S4.p7.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.3.5" xref="S4.p7.2.m2.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.3.3.1c" xref="S4.p7.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.2.m2.1.1.3.3.6" xref="S4.p7.2.m2.1.1.3.3.6.cmml">h</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1">subscript</csymbol><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">ğ’Ÿ</ci><apply id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3"><minus id="S4.p7.2.m2.1.1.3.1.cmml" xref="S4.p7.2.m2.1.1.3.1"></minus><apply id="S4.p7.2.m2.1.1.3.2.cmml" xref="S4.p7.2.m2.1.1.3.2"><times id="S4.p7.2.m2.1.1.3.2.1.cmml" xref="S4.p7.2.m2.1.1.3.2.1"></times><ci id="S4.p7.2.m2.1.1.3.2.2.cmml" xref="S4.p7.2.m2.1.1.3.2.2">ğ‘</ci><ci id="S4.p7.2.m2.1.1.3.2.3.cmml" xref="S4.p7.2.m2.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p7.2.m2.1.1.3.2.4.cmml" xref="S4.p7.2.m2.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p7.2.m2.1.1.3.3.cmml" xref="S4.p7.2.m2.1.1.3.3"><times id="S4.p7.2.m2.1.1.3.3.1.cmml" xref="S4.p7.2.m2.1.1.3.3.1"></times><ci id="S4.p7.2.m2.1.1.3.3.2.cmml" xref="S4.p7.2.m2.1.1.3.3.2">ğ‘ </ci><ci id="S4.p7.2.m2.1.1.3.3.3.cmml" xref="S4.p7.2.m2.1.1.3.3.3">ğ‘¦</ci><ci id="S4.p7.2.m2.1.1.3.3.4.cmml" xref="S4.p7.2.m2.1.1.3.3.4">ğ‘›</ci><ci id="S4.p7.2.m2.1.1.3.3.5.cmml" xref="S4.p7.2.m2.1.1.3.3.5">ğ‘¡</ci><ci id="S4.p7.2.m2.1.1.3.3.6.cmml" xref="S4.p7.2.m2.1.1.3.3.6">â„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\mathcal{D}_{adv-synth}</annotation></semantics></math>. At the same time, since the aim is to learn how to extract useful features under adverse conditions, we freeze DCNN weights when training on a batch fromÂ <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{stand-real}" display="inline"><semantics id="S4.p7.3.m3.1a"><msub id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p7.3.m3.1.1.2" xref="S4.p7.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p7.3.m3.1.1.3" xref="S4.p7.3.m3.1.1.3.cmml"><mrow id="S4.p7.3.m3.1.1.3.2" xref="S4.p7.3.m3.1.1.3.2.cmml"><mi id="S4.p7.3.m3.1.1.3.2.2" xref="S4.p7.3.m3.1.1.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.2.1" xref="S4.p7.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.2.3" xref="S4.p7.3.m3.1.1.3.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.2.1a" xref="S4.p7.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.2.4" xref="S4.p7.3.m3.1.1.3.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.2.1b" xref="S4.p7.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.2.5" xref="S4.p7.3.m3.1.1.3.2.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.2.1c" xref="S4.p7.3.m3.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.2.6" xref="S4.p7.3.m3.1.1.3.2.6.cmml">d</mi></mrow><mo id="S4.p7.3.m3.1.1.3.1" xref="S4.p7.3.m3.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p7.3.m3.1.1.3.3" xref="S4.p7.3.m3.1.1.3.3.cmml"><mi id="S4.p7.3.m3.1.1.3.3.2" xref="S4.p7.3.m3.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.3.1" xref="S4.p7.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.3.3" xref="S4.p7.3.m3.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.3.1a" xref="S4.p7.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.3.4" xref="S4.p7.3.m3.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.3.m3.1.1.3.3.1b" xref="S4.p7.3.m3.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.3.m3.1.1.3.3.5" xref="S4.p7.3.m3.1.1.3.3.5.cmml">l</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><apply id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p7.3.m3.1.1.1.cmml" xref="S4.p7.3.m3.1.1">subscript</csymbol><ci id="S4.p7.3.m3.1.1.2.cmml" xref="S4.p7.3.m3.1.1.2">ğ’Ÿ</ci><apply id="S4.p7.3.m3.1.1.3.cmml" xref="S4.p7.3.m3.1.1.3"><minus id="S4.p7.3.m3.1.1.3.1.cmml" xref="S4.p7.3.m3.1.1.3.1"></minus><apply id="S4.p7.3.m3.1.1.3.2.cmml" xref="S4.p7.3.m3.1.1.3.2"><times id="S4.p7.3.m3.1.1.3.2.1.cmml" xref="S4.p7.3.m3.1.1.3.2.1"></times><ci id="S4.p7.3.m3.1.1.3.2.2.cmml" xref="S4.p7.3.m3.1.1.3.2.2">ğ‘ </ci><ci id="S4.p7.3.m3.1.1.3.2.3.cmml" xref="S4.p7.3.m3.1.1.3.2.3">ğ‘¡</ci><ci id="S4.p7.3.m3.1.1.3.2.4.cmml" xref="S4.p7.3.m3.1.1.3.2.4">ğ‘</ci><ci id="S4.p7.3.m3.1.1.3.2.5.cmml" xref="S4.p7.3.m3.1.1.3.2.5">ğ‘›</ci><ci id="S4.p7.3.m3.1.1.3.2.6.cmml" xref="S4.p7.3.m3.1.1.3.2.6">ğ‘‘</ci></apply><apply id="S4.p7.3.m3.1.1.3.3.cmml" xref="S4.p7.3.m3.1.1.3.3"><times id="S4.p7.3.m3.1.1.3.3.1.cmml" xref="S4.p7.3.m3.1.1.3.3.1"></times><ci id="S4.p7.3.m3.1.1.3.3.2.cmml" xref="S4.p7.3.m3.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S4.p7.3.m3.1.1.3.3.3.cmml" xref="S4.p7.3.m3.1.1.3.3.3">ğ‘’</ci><ci id="S4.p7.3.m3.1.1.3.3.4.cmml" xref="S4.p7.3.m3.1.1.3.3.4">ğ‘</ci><ci id="S4.p7.3.m3.1.1.3.3.5.cmml" xref="S4.p7.3.m3.1.1.3.3.5">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">\mathcal{D}_{stand-real}</annotation></semantics></math> and update them for a batch fromÂ <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{adv-synth}" display="inline"><semantics id="S4.p7.4.m4.1a"><msub id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p7.4.m4.1.1.2" xref="S4.p7.4.m4.1.1.2.cmml">ğ’Ÿ</mi><mrow id="S4.p7.4.m4.1.1.3" xref="S4.p7.4.m4.1.1.3.cmml"><mrow id="S4.p7.4.m4.1.1.3.2" xref="S4.p7.4.m4.1.1.3.2.cmml"><mi id="S4.p7.4.m4.1.1.3.2.2" xref="S4.p7.4.m4.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.2.1" xref="S4.p7.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.2.3" xref="S4.p7.4.m4.1.1.3.2.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.2.1a" xref="S4.p7.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.2.4" xref="S4.p7.4.m4.1.1.3.2.4.cmml">v</mi></mrow><mo id="S4.p7.4.m4.1.1.3.1" xref="S4.p7.4.m4.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.p7.4.m4.1.1.3.3" xref="S4.p7.4.m4.1.1.3.3.cmml"><mi id="S4.p7.4.m4.1.1.3.3.2" xref="S4.p7.4.m4.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.3.1" xref="S4.p7.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.3.3" xref="S4.p7.4.m4.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.3.1a" xref="S4.p7.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.3.4" xref="S4.p7.4.m4.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.3.1b" xref="S4.p7.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.3.5" xref="S4.p7.4.m4.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p7.4.m4.1.1.3.3.1c" xref="S4.p7.4.m4.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.p7.4.m4.1.1.3.3.6" xref="S4.p7.4.m4.1.1.3.3.6.cmml">h</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><apply id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p7.4.m4.1.1.1.cmml" xref="S4.p7.4.m4.1.1">subscript</csymbol><ci id="S4.p7.4.m4.1.1.2.cmml" xref="S4.p7.4.m4.1.1.2">ğ’Ÿ</ci><apply id="S4.p7.4.m4.1.1.3.cmml" xref="S4.p7.4.m4.1.1.3"><minus id="S4.p7.4.m4.1.1.3.1.cmml" xref="S4.p7.4.m4.1.1.3.1"></minus><apply id="S4.p7.4.m4.1.1.3.2.cmml" xref="S4.p7.4.m4.1.1.3.2"><times id="S4.p7.4.m4.1.1.3.2.1.cmml" xref="S4.p7.4.m4.1.1.3.2.1"></times><ci id="S4.p7.4.m4.1.1.3.2.2.cmml" xref="S4.p7.4.m4.1.1.3.2.2">ğ‘</ci><ci id="S4.p7.4.m4.1.1.3.2.3.cmml" xref="S4.p7.4.m4.1.1.3.2.3">ğ‘‘</ci><ci id="S4.p7.4.m4.1.1.3.2.4.cmml" xref="S4.p7.4.m4.1.1.3.2.4">ğ‘£</ci></apply><apply id="S4.p7.4.m4.1.1.3.3.cmml" xref="S4.p7.4.m4.1.1.3.3"><times id="S4.p7.4.m4.1.1.3.3.1.cmml" xref="S4.p7.4.m4.1.1.3.3.1"></times><ci id="S4.p7.4.m4.1.1.3.3.2.cmml" xref="S4.p7.4.m4.1.1.3.3.2">ğ‘ </ci><ci id="S4.p7.4.m4.1.1.3.3.3.cmml" xref="S4.p7.4.m4.1.1.3.3.3">ğ‘¦</ci><ci id="S4.p7.4.m4.1.1.3.3.4.cmml" xref="S4.p7.4.m4.1.1.3.3.4">ğ‘›</ci><ci id="S4.p7.4.m4.1.1.3.3.5.cmml" xref="S4.p7.4.m4.1.1.3.3.5">ğ‘¡</ci><ci id="S4.p7.4.m4.1.1.3.3.6.cmml" xref="S4.p7.4.m4.1.1.3.3.6">â„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">\mathcal{D}_{adv-synth}</annotation></semantics></math>. It is worth noting that all other weights are updated for data from both domains. This strategy encourages the encoder to leverage synthetic data to better learn feature extraction for the target domain while it ensures that the decoder is learning how to interpret both features to perform segmentation task under standard and adverse conditions.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.3.1" class="ltx_text ltx_font_bold">mIoU results for our approach Vs. standard domain adaptation methods.</span> Training our weather and nighttime-aware architecture on both CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> and AWSS, improves the performance on ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> dataset and achieves adequate peformance on CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite>. Best results are <span id="S4.T2.4.2" class="ltx_text ltx_font_bold">bolded</span>. Fnt stands for Fine-Tuned.</figcaption>
<div id="S4.T2.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:176.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.4pt,10.8pt) scale(0.891405574241268,0.891405574241268) ;">
<table id="S4.T2.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.5.1.1.1" class="ltx_tr">
<th id="S4.T2.5.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.5.1.1.1.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.5.1.1.1.3" class="ltx_td ltx_align_center" colspan="5">ACDC</td>
<td id="S4.T2.5.1.1.1.4" class="ltx_td ltx_align_center">Cityscapes</td>
</tr>
<tr id="S4.T2.5.1.2.2" class="ltx_tr">
<th id="S4.T2.5.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.5.1.2.2.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Rain</td>
<td id="S4.T2.5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Fog</td>
<td id="S4.T2.5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Snow</td>
<td id="S4.T2.5.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Night</td>
<td id="S4.T2.5.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Overall</td>
<td id="S4.T2.5.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">Overall</td>
</tr>
<tr id="S4.T2.5.1.3.3" class="ltx_tr">
<th id="S4.T2.5.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T2.5.1.3.3.1.1" class="ltx_text">DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite></span></th>
<th id="S4.T2.5.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T2.5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.41</td>
<td id="S4.T2.5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S4.T2.5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td id="S4.T2.5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.17</td>
<td id="S4.T2.5.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S4.T2.5.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.78</td>
</tr>
<tr id="S4.T2.5.1.4.4" class="ltx_tr">
<th id="S4.T2.5.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FnT on AWSS</th>
<td id="S4.T2.5.1.4.4.2" class="ltx_td ltx_align_center">0.44</td>
<td id="S4.T2.5.1.4.4.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S4.T2.5.1.4.4.4" class="ltx_td ltx_align_center">0.47</td>
<td id="S4.T2.5.1.4.4.5" class="ltx_td ltx_align_center">0.19</td>
<td id="S4.T2.5.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">0.39</td>
<td id="S4.T2.5.1.4.4.7" class="ltx_td ltx_align_center">0.59</td>
</tr>
<tr id="S4.T2.5.1.5.5" class="ltx_tr">
<th id="S4.T2.5.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.5.1.5.5.1.1" class="ltx_text">HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>]</cite></span></th>
<th id="S4.T2.5.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T2.5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S4.T2.5.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.42</td>
<td id="S4.T2.5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.41</td>
<td id="S4.T2.5.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">0.09</td>
<td id="S4.T2.5.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S4.T2.5.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
</tr>
<tr id="S4.T2.5.1.6.6" class="ltx_tr">
<th id="S4.T2.5.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FnT on AWSS</th>
<td id="S4.T2.5.1.6.6.2" class="ltx_td ltx_align_center">0.47</td>
<td id="S4.T2.5.1.6.6.3" class="ltx_td ltx_align_center">0.49</td>
<td id="S4.T2.5.1.6.6.4" class="ltx_td ltx_align_center">0.35</td>
<td id="S4.T2.5.1.6.6.5" class="ltx_td ltx_align_center">0.14</td>
<td id="S4.T2.5.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">0.36</td>
<td id="S4.T2.5.1.6.6.7" class="ltx_td ltx_align_center">0.51</td>
</tr>
<tr id="S4.T2.5.1.7.7" class="ltx_tr">
<th id="S4.T2.5.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.5.1.7.7.1.1" class="ltx_text">DANetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Fu etÂ al.(2019)Fu, Liu, Tian, Li, Bao, Fang, and Lu</a>]</cite></span></th>
<th id="S4.T2.5.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T2.5.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="S4.T2.5.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">0.57</td>
<td id="S4.T2.5.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">0.44</td>
<td id="S4.T2.5.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
<td id="S4.T2.5.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.42</td>
<td id="S4.T2.5.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t">0.82</td>
</tr>
<tr id="S4.T2.5.1.8.8" class="ltx_tr">
<th id="S4.T2.5.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FnT on AWSS</th>
<td id="S4.T2.5.1.8.8.2" class="ltx_td ltx_align_center">0.48</td>
<td id="S4.T2.5.1.8.8.3" class="ltx_td ltx_align_center">0.58</td>
<td id="S4.T2.5.1.8.8.4" class="ltx_td ltx_align_center">0.48</td>
<td id="S4.T2.5.1.8.8.5" class="ltx_td ltx_align_center">0.26</td>
<td id="S4.T2.5.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">0.45</td>
<td id="S4.T2.5.1.8.8.7" class="ltx_td ltx_align_center">0.74</td>
</tr>
<tr id="S4.T2.5.1.9.9" class="ltx_tr">
<th id="S4.T2.5.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.5.1.9.9.1.1" class="ltx_text">PSPNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia</a>]</cite></span></th>
<th id="S4.T2.5.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S4.T2.5.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.49</td>
<td id="S4.T2.5.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.54</td>
<td id="S4.T2.5.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.43</td>
<td id="S4.T2.5.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">0.20</td>
<td id="S4.T2.5.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.41</td>
<td id="S4.T2.5.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.9.9.8.1" class="ltx_text ltx_font_bold">0.86</span></td>
</tr>
<tr id="S4.T2.5.1.10.10" class="ltx_tr">
<th id="S4.T2.5.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FnT on AWSS</th>
<td id="S4.T2.5.1.10.10.2" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T2.5.1.10.10.3" class="ltx_td ltx_align_center">0.56</td>
<td id="S4.T2.5.1.10.10.4" class="ltx_td ltx_align_center">0.46</td>
<td id="S4.T2.5.1.10.10.5" class="ltx_td ltx_align_center">0.18</td>
<td id="S4.T2.5.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r">0.43</td>
<td id="S4.T2.5.1.10.10.7" class="ltx_td ltx_align_center">0.86</td>
</tr>
<tr id="S4.T2.5.1.11.11" class="ltx_tr">
<th id="S4.T2.5.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Ours</th>
<th id="S4.T2.5.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Full-Model</th>
<td id="S4.T2.5.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.11.11.3.1" class="ltx_text ltx_font_bold">0.57</span></td>
<td id="S4.T2.5.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.11.11.4.1" class="ltx_text ltx_font_bold">0.60</span></td>
<td id="S4.T2.5.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.11.11.5.1" class="ltx_text ltx_font_bold">0.50</span></td>
<td id="S4.T2.5.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.1.11.11.6.1" class="ltx_text ltx_font_bold">0.27</span></td>
<td id="S4.T2.5.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.5.1.11.11.7.1" class="ltx_text ltx_font_bold">0.49</span></td>
<td id="S4.T2.5.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">For training experiments, we use two datasets: AWSS dataset and the training split of CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> dataset. For evaluation, we use validation splits of Cityscapes and ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> datasets. The three datasets follow the same conventions and color codes.
Cityscapes comprises 2975 training images and 500 validation images. It is captured in urban scenes under normal weather conditions in the daytime. ACDC validation split comprises 506 images spanning rainy, foggy, snowy weather conditions and nighttime attributes. 
<br class="ltx_break"><span id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">Implementation details.</span> Experiments are conducted on a Tesla V100 GPU. For all experiments, we keep the default parameters of the authors. For our adopted DeepLabV3+ architectures, we use a batch size of 4 while we keep all other parameters same as DeepLabV3+. For DeepLabV3+ baseline, our architecture, and all ablation study experiments, we train for 30K iterations. We set <math id="S5.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\alpha=\beta=10^{-5}" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">Î±</mi><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">=</mo><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.4" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.4.cmml">Î²</mi><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.5" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.5.cmml">=</mo><msup id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.cmml"><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.2.cmml">10</mn><mrow id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.cmml"><mo id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3a" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.cmml">âˆ’</mo><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><and id="S5.SS0.SSS0.Px1.p1.1.m1.1.1a.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"></and><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1b.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3"></eq><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">ğ›¼</ci><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.4.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.4">ğ›½</ci></apply><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1c.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.5.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.5"></eq><share href="#S5.SS0.SSS0.Px1.p1.1.m1.1.1.4.cmml" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1d.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"></share><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6">superscript</csymbol><cn type="integer" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.2">10</cn><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3"><minus id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3"></minus><cn type="integer" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.6.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">\alpha=\beta=10^{-5}</annotation></semantics></math>, as these values achieved the best results. 
<br class="ltx_break"><span id="S5.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold">Baselines.</span>
To analyse the robustness of recent semantic segmentation methods under adverse conditions, we use DeepLabV3+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>]</cite>, HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>]</cite>, DANetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Fu etÂ al.(2019)Fu, Liu, Tian, Li, Bao, Fang, and Lu</a>]</cite>, and PSPNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia</a>]</cite>. 
<br class="ltx_break"><span id="S5.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_bold">Evaluation metric.</span> We use the common Mean Intersection over Union (mIoU)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam</a>, <a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Fu etÂ al.(2019)Fu, Liu, Tian, Li, Bao, Fang, and Lu</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia</a>]</cite> on the validation sets of Cityscapes and ACDC similar toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Chen etÂ al.(2020)Chen, Hang, Chan, and Lin</a>, <a href="#bib.bibx45" title="" class="ltx_ref">Xie etÂ al.(2022)Xie, Yuan, Li, Liu, and Cheng</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Musat etÂ al.(2021)Musat, Fursa, Newman, Cuzzolin, and
Bradley</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Before discussing our architecture results, we will discuss how the domain shift degrades the state-of-the-art, and the improvements achieved by fine-tuning on synthetic data.
<br class="ltx_break"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Standard-Adverse domain shift.</span> As shown by our results in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Methodology â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the performance of recent methods clearly degrade under adverse weather conditions and at nighttime (see rows <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">Baseline</span>). Additionally, it seems that snow and nighttime represent a clear challenge for recent models. Snow causes a drastic change in scene appearance: falling snow particles, snow on pavements and other scene elements makes these objects considerably different compared to what the model learned in the training phase. Thus, the model struggles to segment these elements. Similarly, nighttime scenes with the radical decrease in illumination presents a major challenge for segmentation methods.
<br class="ltx_break"><span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_bold">Domain adaptation using synthetic data.</span>
Transfer learning is usually applied to handle a domain shift. However, although it improves the performance on the target domain, it generally degrades the performance on the source domain. As shown in Table Â <a href="#S4.T2" title="Table 2 â€£ 4 Methodology â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (FnT on AWSS), we can improve the performance of each semantic segmentation model.
For some attributes like night and snow, the improvement was more than 50% (e.g. HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>]</cite> under night). Generally, each semantic segmentation model was able to leverage AWSS to improve its performance for each adverse attribute. However, when evaluating these fine-tuned models on the original domain (Cityscapes), we see a clear degradation in performance. This degradation was more severe for some models like HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang</a>]</cite> while it was slight for PSPNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia</a>]</cite></p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.8.1" class="ltx_text ltx_font_bold">Per-class mIoU results on ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> dataset.</span> Our model achieves the best overall results on ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite>. It maintains the best results on <span id="S5.T3.9.2" class="ltx_text ltx_font_italic">Road</span>, <span id="S5.T3.10.3" class="ltx_text ltx_font_italic">Sidewalk</span>, <span id="S5.T3.11.4" class="ltx_text ltx_font_italic">Building</span>, and <span id="S5.T3.12.5" class="ltx_text ltx_font_italic">Person</span> classes. Best and second best results are <span id="S5.T3.13.6" class="ltx_text ltx_font_bold">bolded</span> and <span id="S5.T3.14.7" class="ltx_text ltx_framed ltx_framed_underline">underlined</span>, respectively.</figcaption>
<div id="S5.T3.15" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:87pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,10.5pt) scale(0.805277010880216,0.805277010880216) ;">
<table id="S5.T3.15.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.15.1.1.1" class="ltx_tr">
<th id="S5.T3.15.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S5.T3.15.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Road</th>
<th id="S5.T3.15.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Sidewalk</th>
<th id="S5.T3.15.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Building</th>
<th id="S5.T3.15.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Pole</th>
<th id="S5.T3.15.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Tr. Light</th>
<th id="S5.T3.15.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Tr. Sign</th>
<th id="S5.T3.15.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">Vegetation</th>
<th id="S5.T3.15.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Sky</th>
<th id="S5.T3.15.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">Person</th>
<th id="S5.T3.15.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Car</th>
<th id="S5.T3.15.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.15.1.2.1" class="ltx_tr">
<th id="S5.T3.15.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DeepLabV3+</th>
<td id="S5.T3.15.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.15.1.2.1.2.1" class="ltx_text ltx_framed ltx_framed_underline">0.71</span></td>
<td id="S5.T3.15.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.15.1.2.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">0.22</span></td>
<td id="S5.T3.15.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td id="S5.T3.15.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td id="S5.T3.15.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.22</td>
<td id="S5.T3.15.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">0.29</td>
<td id="S5.T3.15.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.15.1.2.1.8.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S5.T3.15.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td id="S5.T3.15.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">0.24</td>
<td id="S5.T3.15.1.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.23</td>
<td id="S5.T3.15.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
</tr>
<tr id="S5.T3.15.1.3.2" class="ltx_tr">
<th id="S5.T3.15.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HRNet</th>
<td id="S5.T3.15.1.3.2.2" class="ltx_td ltx_align_center">0.55</td>
<td id="S5.T3.15.1.3.2.3" class="ltx_td ltx_align_center">0.16</td>
<td id="S5.T3.15.1.3.2.4" class="ltx_td ltx_align_center">0.44</td>
<td id="S5.T3.15.1.3.2.5" class="ltx_td ltx_align_center">0.14</td>
<td id="S5.T3.15.1.3.2.6" class="ltx_td ltx_align_center">0.28</td>
<td id="S5.T3.15.1.3.2.7" class="ltx_td ltx_align_center">0.24</td>
<td id="S5.T3.15.1.3.2.8" class="ltx_td ltx_align_center">0.66</td>
<td id="S5.T3.15.1.3.2.9" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.3.2.9.1" class="ltx_text ltx_font_bold">0.72</span></td>
<td id="S5.T3.15.1.3.2.10" class="ltx_td ltx_align_center">0.07</td>
<td id="S5.T3.15.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r">0.19</td>
<td id="S5.T3.15.1.3.2.12" class="ltx_td ltx_align_center">0.35</td>
</tr>
<tr id="S5.T3.15.1.4.3" class="ltx_tr">
<th id="S5.T3.15.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DANet</th>
<td id="S5.T3.15.1.4.3.2" class="ltx_td ltx_align_center">0.68</td>
<td id="S5.T3.15.1.4.3.3" class="ltx_td ltx_align_center">0.11</td>
<td id="S5.T3.15.1.4.3.4" class="ltx_td ltx_align_center">0.19</td>
<td id="S5.T3.15.1.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.4.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">0.28</span></td>
<td id="S5.T3.15.1.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.4.3.6.1" class="ltx_text ltx_font_bold">0.54</span></td>
<td id="S5.T3.15.1.4.3.7" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.4.3.7.1" class="ltx_text ltx_font_bold">0.67</span></td>
<td id="S5.T3.15.1.4.3.8" class="ltx_td ltx_align_center">0.26</td>
<td id="S5.T3.15.1.4.3.9" class="ltx_td ltx_align_center">0.65</td>
<td id="S5.T3.15.1.4.3.10" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.4.3.10.1" class="ltx_text ltx_framed ltx_framed_underline">0.29</span></td>
<td id="S5.T3.15.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.15.1.4.3.11.1" class="ltx_text ltx_font_bold">0.53</span></td>
<td id="S5.T3.15.1.4.3.12" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.4.3.12.1" class="ltx_text ltx_framed ltx_framed_underline">0.42</span></td>
</tr>
<tr id="S5.T3.15.1.5.4" class="ltx_tr">
<th id="S5.T3.15.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PSPNet</th>
<td id="S5.T3.15.1.5.4.2" class="ltx_td ltx_align_center">0.63</td>
<td id="S5.T3.15.1.5.4.3" class="ltx_td ltx_align_center">0.12</td>
<td id="S5.T3.15.1.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.5.4.4.1" class="ltx_text ltx_framed ltx_framed_underline">0.60</span></td>
<td id="S5.T3.15.1.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.5.4.5.1" class="ltx_text ltx_font_bold">0.30</span></td>
<td id="S5.T3.15.1.5.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.5.4.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.48</span></td>
<td id="S5.T3.15.1.5.4.7" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.5.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">0.41</span></td>
<td id="S5.T3.15.1.5.4.8" class="ltx_td ltx_align_center">0.62</td>
<td id="S5.T3.15.1.5.4.9" class="ltx_td ltx_align_center">0.61</td>
<td id="S5.T3.15.1.5.4.10" class="ltx_td ltx_align_center">0.21</td>
<td id="S5.T3.15.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r">0.17</td>
<td id="S5.T3.15.1.5.4.12" class="ltx_td ltx_align_center">0.41</td>
</tr>
<tr id="S5.T3.15.1.6.5" class="ltx_tr">
<th id="S5.T3.15.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S5.T3.15.1.6.5.2" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.2.1" class="ltx_text ltx_font_bold">0.79</span></td>
<td id="S5.T3.15.1.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.3.1" class="ltx_text ltx_font_bold">0.40</span></td>
<td id="S5.T3.15.1.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.4.1" class="ltx_text ltx_font_bold">0.63</span></td>
<td id="S5.T3.15.1.6.5.5" class="ltx_td ltx_align_center">0.25</td>
<td id="S5.T3.15.1.6.5.6" class="ltx_td ltx_align_center">0.26</td>
<td id="S5.T3.15.1.6.5.7" class="ltx_td ltx_align_center">0.33</td>
<td id="S5.T3.15.1.6.5.8" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.8.1" class="ltx_text ltx_framed ltx_framed_underline">0.69</span></td>
<td id="S5.T3.15.1.6.5.9" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.9.1" class="ltx_text ltx_framed ltx_framed_underline">0.66</span></td>
<td id="S5.T3.15.1.6.5.10" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.10.1" class="ltx_text ltx_font_bold">0.32</span></td>
<td id="S5.T3.15.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.15.1.6.5.11.1" class="ltx_text ltx_framed ltx_framed_underline">0.52</span></td>
<td id="S5.T3.15.1.6.5.12" class="ltx_td ltx_align_center"><span id="S5.T3.15.1.6.5.12.1" class="ltx_text ltx_font_bold">0.49</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Weather and night aware architecture.</span>
While the previous solution is simple, the improvement on the target domain was limited, and the performance on the source domain was sharply degraded. As a remedy, our architecture based solution achieves the best results on the target domain and it maintains an adequate performance on the source domain. As reported in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Methodology â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, making the model aware of the weather condition and daytime-nighttime attributes of the images in the training phase helps the model to learn how to extract more efficient features under both standard and challenging scenarios. Qualitative results are shown in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.1 Results â€£ 5 Experiments â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Furthermore, per-class results are demonstrated in TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.1 Results â€£ 5 Experiments â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our model achieves the best results on <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">Road</span>, <span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">Sidewalk</span>, <span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">Building</span>, and <span id="S5.SS1.p2.1.5" class="ltx_text ltx_font_italic">Person</span> semantic classes. The largest improvement was on the <span id="S5.SS1.p2.1.6" class="ltx_text ltx_font_italic">Sidewalk</span> which is around 82% improvement compared to DeepLabV3+, the best performing baseline on this class. As expected, snow and rain changes the visual appearance of this class significantly. This is because of snow accumulation, footsteps on snow, rain splash and mud, in addition to light reflection due to wet surface when raining.
<br class="ltx_break"></p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2210.05626/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.2.1" class="ltx_text ltx_font_bold">Visual comparison between baselines and our approach.</span> Segmentation results are shown on ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite> and CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> dataset, respectively.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Study</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To understand the effect of each design decision, we perform several experiments. 
<br class="ltx_break"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Training data type.</span> We train the baseline model on AWSS from scratch (TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Ablation Study â€£ 5 Experiments â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> first row). As expected, training on synthetic data alone does not achieve satisfactory results due to domain gap between synthetic and real data. Thus, this suggests that AWSS can be used as complementary to the real data and not as an alternative. On the other hand, training the model from scratch on standard weather will perform well on these conditions but will fail under challenging conditions (TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Ablation Study â€£ 5 Experiments â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> second row).
<br class="ltx_break"><span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">Training strategy.</span> As shown in TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Ablation Study â€£ 5 Experiments â€£ Semantic Segmentation under Adverse Conditions: A Weather and Nighttime- aware Synthetic Data-based Approach" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> third row, the standard method of transfer learning (fine-tuning the last layers on the target domain) improves the performance on the target domain but degrades the performance on the source domain.
<br class="ltx_break"><span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">Weather-Time awareness.</span>
Our approach achieves the best results under adverse conditions while still maintaining a satisfactory performance under standard conditions. Making the model synthetic aware and training the model without weather and nighttime-awareness achieve better results on the source domain but low performance on the target domain, compared to fine-tuning experiment. Adding the weather awareness to the model, i.e WAS, improves the performance at standard and adverse conditions. All adverse weather attributes were improved clearly as expected but the night attribute maintained a slight improvement. Finally, making the model aware of nighttime too, boosts significantly the performance under nighttime. Interestingly, it improves also the performance of the other weather conditions too. This is expected as TAS and WAS teachers allow the model to learn weather specific and nighttime-specific robust features which enables the model to achieve better results under these challenging conditions.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S5.T4.4.1" class="ltx_text ltx_font_bold">Ablation analysis of weather and time awareness on performance.</span> Making the DeepLabV3+ weather and time aware improved the performance significantly at both normal weather, i.e. CityscapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele</a>]</cite> (CS), and adverse weather, i.e. ACDCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool</a>]</cite>, scenarios. Best and second best results are <span id="S5.T4.5.2" class="ltx_text ltx_font_bold">bolded</span> and <span id="S5.T4.6.3" class="ltx_text ltx_framed ltx_framed_underline">underlined</span>, respectively.</figcaption>
<div id="S5.T4.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:112.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.5pt,15.9pt) scale(0.779034622451233,0.779034622451233) ;">
<table id="S5.T4.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.7.1.1.1" class="ltx_tr">
<th id="S5.T4.7.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T4.7.1.1.1.2" class="ltx_td ltx_align_center" rowspan="2"><span id="S5.T4.7.1.1.1.2.1" class="ltx_text">Training Mode</span></td>
<td id="S5.T4.7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" colspan="5">ACDC</td>
<td id="S5.T4.7.1.1.1.4" class="ltx_td ltx_align_center">Cityscapes</td>
</tr>
<tr id="S5.T4.7.1.2.2" class="ltx_tr">
<th id="S5.T4.7.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T4.7.1.2.2.2" class="ltx_td ltx_align_center">Rain</td>
<td id="S5.T4.7.1.2.2.3" class="ltx_td ltx_align_center">Fog</td>
<td id="S5.T4.7.1.2.2.4" class="ltx_td ltx_align_center">Snow</td>
<td id="S5.T4.7.1.2.2.5" class="ltx_td ltx_align_center">Night</td>
<td id="S5.T4.7.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Overall</td>
<td id="S5.T4.7.1.2.2.7" class="ltx_td ltx_align_center">Overall</td>
</tr>
<tr id="S5.T4.7.1.3.3" class="ltx_tr">
<th id="S5.T4.7.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T4.7.1.3.3.1.1" class="ltx_text">Baseline</span></th>
<td id="S5.T4.7.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">scratch on AWSS</td>
<td id="S5.T4.7.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.24</td>
<td id="S5.T4.7.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S5.T4.7.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td id="S5.T4.7.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
<td id="S5.T4.7.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.22</td>
<td id="S5.T4.7.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.27</td>
</tr>
<tr id="S5.T4.7.1.4.4" class="ltx_tr">
<td id="S5.T4.7.1.4.4.1" class="ltx_td ltx_align_center">scratch on CS</td>
<td id="S5.T4.7.1.4.4.2" class="ltx_td ltx_align_center">0.41</td>
<td id="S5.T4.7.1.4.4.3" class="ltx_td ltx_align_center">0.46</td>
<td id="S5.T4.7.1.4.4.4" class="ltx_td ltx_align_center">0.36</td>
<td id="S5.T4.7.1.4.4.5" class="ltx_td ltx_align_center">0.17</td>
<td id="S5.T4.7.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">0.35</td>
<td id="S5.T4.7.1.4.4.7" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.4.4.7.1" class="ltx_text ltx_font_bold">0.78</span></td>
</tr>
<tr id="S5.T4.7.1.5.5" class="ltx_tr">
<td id="S5.T4.7.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">scratch on CS and fine-tuned on AWSS</td>
<td id="S5.T4.7.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">0.44</td>
<td id="S5.T4.7.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S5.T4.7.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="S5.T4.7.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.19</td>
<td id="S5.T4.7.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.39</td>
<td id="S5.T4.7.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.59</td>
</tr>
<tr id="S5.T4.7.1.6.6" class="ltx_tr">
<th id="S5.T4.7.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T4.7.1.6.6.1.1" class="ltx_text">Ours</span></th>
<td id="S5.T4.7.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">scratch on CS and AWSS</td>
<td id="S5.T4.7.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">0.41</td>
<td id="S5.T4.7.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">0.43</td>
<td id="S5.T4.7.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td id="S5.T4.7.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">0.19</td>
<td id="S5.T4.7.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.35</td>
<td id="S5.T4.7.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
</tr>
<tr id="S5.T4.7.1.7.7" class="ltx_tr">
<td id="S5.T4.7.1.7.7.1" class="ltx_td ltx_align_center">scratch on CS and AWSS + Weather Aware</td>
<td id="S5.T4.7.1.7.7.2" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.7.7.2.1" class="ltx_text ltx_framed ltx_framed_underline">0.49</span></td>
<td id="S5.T4.7.1.7.7.3" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.7.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">0.55</span></td>
<td id="S5.T4.7.1.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.7.7.4.1" class="ltx_text ltx_framed ltx_framed_underline">0.47</span></td>
<td id="S5.T4.7.1.7.7.5" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.7.7.5.1" class="ltx_text ltx_framed ltx_framed_underline">0.20</span></td>
<td id="S5.T4.7.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.1.7.7.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.43</span></td>
<td id="S5.T4.7.1.7.7.7" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.7.7.7.1" class="ltx_text ltx_font_italic">0.73</span></td>
</tr>
<tr id="S5.T4.7.1.8.8" class="ltx_tr">
<td id="S5.T4.7.1.8.8.1" class="ltx_td ltx_align_center">scratch on CS and AWSS + Weather and Nighttime Aware</td>
<td id="S5.T4.7.1.8.8.2" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.8.8.2.1" class="ltx_text ltx_font_bold">0.57</span></td>
<td id="S5.T4.7.1.8.8.3" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.8.8.3.1" class="ltx_text ltx_font_bold">0.60</span></td>
<td id="S5.T4.7.1.8.8.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.8.8.4.1" class="ltx_text ltx_font_bold">0.50</span></td>
<td id="S5.T4.7.1.8.8.5" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.8.8.5.1" class="ltx_text ltx_font_bold">0.27</span></td>
<td id="S5.T4.7.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.1.8.8.6.1" class="ltx_text ltx_font_bold">0.49</span></td>
<td id="S5.T4.7.1.8.8.7" class="ltx_td ltx_align_center"><span id="S5.T4.7.1.8.8.7.1" class="ltx_text ltx_framed ltx_framed_underline">0.75</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduce a novel synthetic dataset, the AWSS, that covers various adverse conditions. We show that fine-tuning four state-of-the-art semantic segmentation models improve performance under adverse conditions but degrades the performance under standard conditions. Our proposed solution shows that making the model aware of the synthetic data and utilizing weather-aware-supervisor and time-aware-supervisor achieves the best results under adverse weather conditions while maintaining an adequate performance under standard conditions.</p>
</div>
</section>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was funded by the Faculty of Science and Technology of Lancaster University. We thank the High End Computing facility of Lancaster University for the computing resources. The authors would also like to thank CAPES, CNPq, and FAPEMIG for funding different parts of this work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Alshammari etÂ al.(2020)Alshammari, Akcay, and
Breckon]</span>
<span class="ltx_bibblock">
Naif Alshammari, Samet Akcay, and TobyÂ P Breckon.

</span>
<span class="ltx_bibblock">Competitive simplicity for multi-task learning for real-time foggy
scene understanding via domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.05304</em>, 2020.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Alvarez etÂ al.(2012)Alvarez, Gevers, LeCun, and
Lopez]</span>
<span class="ltx_bibblock">
JoseÂ M Alvarez, Theo Gevers, Yann LeCun, and AntonioÂ M Lopez.

</span>
<span class="ltx_bibblock">Road scene segmentation from a single image.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 376â€“389.
Springer, 2012.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Baroroh etÂ al.(2021)Baroroh, Chu, and Wang]</span>
<span class="ltx_bibblock">
DawiÂ Karomati Baroroh, Chih-Hsing Chu, and Lihui Wang.

</span>
<span class="ltx_bibblock">Systematic literature review on augmented reality in smart
manufacturing: Collaboration between human and computational intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Journal of Manufacturing Systems</em>, 61:696â€“711, 2021.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Brostow etÂ al.(2009)Brostow, Fauqueur, and
Cipolla]</span>
<span class="ltx_bibblock">
GabrielÂ J Brostow, Julien Fauqueur, and Roberto Cipolla.

</span>
<span class="ltx_bibblock">Semantic object classes in video: A high-definition ground truth
database.

</span>
<span class="ltx_bibblock"><em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters</em>, 30(2):88â€“97,
2009.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chen etÂ al.(2018)Chen, Zhu, Papandreou, Schroff, and
Adam]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
Adam.

</span>
<span class="ltx_bibblock">Encoder-decoder with atrous separable convolution for semantic image
segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</em>, pages 801â€“818, 2018.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chen etÂ al.(2020)Chen, Hang, Chan, and Lin]</span>
<span class="ltx_bibblock">
Ping-Rong Chen, Hsueh-Ming Hang, Sheng-Wei Chan, and Jing-Jhih Lin.

</span>
<span class="ltx_bibblock">Dsnet: An efficient cnn for road scene segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">APSIPA Transactions on Signal and Information Processing</em>, 9,
2020.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chiang etÂ al.(2022)Chiang, Shang, and Qiao]</span>
<span class="ltx_bibblock">
Feng-Kuang Chiang, Xiaojing Shang, and LuÂ Qiao.

</span>
<span class="ltx_bibblock">Augmented reality in vocational training: A systematic review of
research and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Computers in Human Behavior</em>, 129:107125, 2022.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chollet(2017)]</span>
<span class="ltx_bibblock">
FranÃ§ois Chollet.

</span>
<span class="ltx_bibblock">Xception: Deep learning with depthwise separable convolutions.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 1251â€“1258, 2017.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Cordts etÂ al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
Franke, Roth, and Schiele]</span>
<span class="ltx_bibblock">
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.

</span>
<span class="ltx_bibblock">The cityscapes dataset for semantic urban scene understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 3213â€“3223, 2016.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Costa etÂ al.(2022)Costa, Petry, and Moreira]</span>
<span class="ltx_bibblock">
Gabriel deÂ Moura Costa, MarceloÂ Roberto Petry, and AntÃ³nioÂ Paulo Moreira.

</span>
<span class="ltx_bibblock">Augmented reality for humanâ€“robot collaboration and cooperation in
industrial applications: A systematic literature review.

</span>
<span class="ltx_bibblock"><em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 22(7):2725, 2022.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Dosovitskiy etÂ al.(2017)Dosovitskiy, Ros, Codevilla, Lopez, and
Koltun]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.

</span>
<span class="ltx_bibblock">Carla: An open urban driving simulator.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">Conference on robot learning</em>, pages 1â€“16. PMLR, 2017.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Dundar etÂ al.(2020)Dundar, Liu, Yu, Wang, Zedlewski, and
Kautz]</span>
<span class="ltx_bibblock">
Aysegul Dundar, Ming-Yu Liu, Zhiding Yu, Ting-Chun Wang, John Zedlewski, and
Jan Kautz.

</span>
<span class="ltx_bibblock">Domain stylization: A fast covariance matching framework towards
domain adaptation.

</span>
<span class="ltx_bibblock"><em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 43(7):2360â€“2372, 2020.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Everingham etÂ al.(2015)Everingham, Eslami, VanÂ Gool, Williams, Winn,
and Zisserman]</span>
<span class="ltx_bibblock">
Mark Everingham, SMÂ Eslami, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn,
and Andrew Zisserman.

</span>
<span class="ltx_bibblock">The pascal visual object classes challenge: A retrospective.

</span>
<span class="ltx_bibblock"><em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 111(1):98â€“136, 2015.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Fu etÂ al.(2019)Fu, Liu, Tian, Li, Bao, Fang, and Lu]</span>
<span class="ltx_bibblock">
Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing
Lu.

</span>
<span class="ltx_bibblock">Dual attention network for scene segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 3146â€“3154, 2019.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gaidon etÂ al.(2016)Gaidon, Wang, Cabon, and Vig]</span>
<span class="ltx_bibblock">
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.

</span>
<span class="ltx_bibblock">Virtual Worlds as Proxy for Multi-Object Tracking Analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 2016.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Goyal etÂ al.(2017)Goyal, Rajpura, Bojinov, and
Hegde]</span>
<span class="ltx_bibblock">
Manik Goyal, Param Rajpura, Hristo Bojinov, and Ravi Hegde.

</span>
<span class="ltx_bibblock">Dataset augmentation with synthetic images improves semantic
segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">National Conference on Computer Vision, Pattern Recognition,
Image Processing, and Graphics</em>, pages 348â€“359. Springer, 2017.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Guo etÂ al.(2020)Guo, Chen, Ma, Li, Li, and Xie]</span>
<span class="ltx_bibblock">
Mengxi Guo, Mingtao Chen, Cong Ma, Yuan Li, Xianfeng Li, and Xiaodong Xie.

</span>
<span class="ltx_bibblock">High-level task-driven single image deraining: Segmentation in rainy
days.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">International Conference on Neural Information Processing</em>,
pages 350â€“362. Springer, 2020.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ivanovs etÂ al.(2022)Ivanovs, Ozols, Dobrajs, and
Kadikis]</span>
<span class="ltx_bibblock">
Maksims Ivanovs, Kaspars Ozols, Artis Dobrajs, and Roberts Kadikis.

</span>
<span class="ltx_bibblock">Improving semantic segmentation of urban scenes for self-driving cars
with synthetic images.

</span>
<span class="ltx_bibblock"><em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 22(6):2252, 2022.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kerim etÂ al.(2021)Kerim, SorianoÂ Marcolino, and
Jiang]</span>
<span class="ltx_bibblock">
Abdulrahman Kerim, Leandro SorianoÂ Marcolino, and Richard Jiang.

</span>
<span class="ltx_bibblock">Silver: Novel rendering engine for data hungry computer vision
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">2nd International Workshop on Data Quality Assessment for
Machine Learning</em>, 2021.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kumar etÂ al.(2021)Kumar, Klingner, Yogamani, Milz, Fingscheidt, and
Mader]</span>
<span class="ltx_bibblock">
VarunÂ Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim
Fingscheidt, and Patrick Mader.

</span>
<span class="ltx_bibblock">Syndistnet: Self-supervised monocular fisheye camera distance
estimation synergized with semantic segmentation for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</em>, pages 61â€“71, 2021.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lei etÂ al.(2020)Lei, Emaru, Ravankar, Kobayashi, and
Wang]</span>
<span class="ltx_bibblock">
Yayun Lei, Takanori Emaru, AnkitÂ A Ravankar, Yukinori Kobayashi, and SuÂ Wang.

</span>
<span class="ltx_bibblock">Semantic image segmentation on snow driving scenarios.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Mechatronics and
Automation (ICMA)</em>, pages 1094â€“1100. IEEE, 2020.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Liu etÂ al.(2010)Liu, Gould, and Koller]</span>
<span class="ltx_bibblock">
Beyang Liu, Stephen Gould, and Daphne Koller.

</span>
<span class="ltx_bibblock">Single image depth estimation from predicted semantic labels.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">2010 IEEE computer society conference on computer vision and
pattern recognition</em>, pages 1253â€“1260. IEEE, 2010.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Liu etÂ al.(2022)Liu, Sivaparthipan, and Shankar]</span>
<span class="ltx_bibblock">
Yubin Liu, CBÂ Sivaparthipan, and Achyut Shankar.

</span>
<span class="ltx_bibblock">Humanâ€“computer interaction based visual feedback system for
augmentative and alternative communication.

</span>
<span class="ltx_bibblock"><em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">International Journal of Speech Technology</em>, 25(2):305â€“314, 2022.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ma etÂ al.(2022)Ma, Wang, Zhan, Zheng, Wang, Dai, and Lin]</span>
<span class="ltx_bibblock">
Xianzheng Ma, Zhixiang Wang, Yacheng Zhan, Yinqiang Zheng, Zheng Wang, Dengxin
Dai, and Chia-Wen Lin.

</span>
<span class="ltx_bibblock">Both style and fog matter: Cumulative domain adaptation for semantic
foggy scene understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 18922â€“18931, 2022.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Material(2022)]</span>
<span class="ltx_bibblock">
AdobeÂ Substance Material.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://substance3d.adobe.com/assets" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://substance3d.adobe.com/assets</a>, 2022.

</span>
<span class="ltx_bibblock">Online; accessed: 2022-07-26.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Musat etÂ al.(2021)Musat, Fursa, Newman, Cuzzolin, and
Bradley]</span>
<span class="ltx_bibblock">
Valentina Musat, Ivan Fursa, Paul Newman, Fabio Cuzzolin, and Andrew Bradley.

</span>
<span class="ltx_bibblock">Multi-weather city: Adverse weather stacking for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 2906â€“2915, 2021.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Nazar etÂ al.(2021)Nazar, Alam, Yafi, and
Mazliham]</span>
<span class="ltx_bibblock">
Mobeen Nazar, MuhammadÂ Mansoor Alam, Eiad Yafi, and MSÂ Mazliham.

</span>
<span class="ltx_bibblock">A systematic review of human-computer interaction and explainable
artificial intelligence in healthcare with artificial intelligence
techniques.

</span>
<span class="ltx_bibblock"><em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 2021.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Neuhold etÂ al.(2017)Neuhold, Ollmann, RotaÂ Bulo, and
Kontschieder]</span>
<span class="ltx_bibblock">
Gerhard Neuhold, Tobias Ollmann, Samuel RotaÂ Bulo, and Peter Kontschieder.

</span>
<span class="ltx_bibblock">The mapillary vistas dataset for semantic understanding of street
scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 4990â€“4999, 2017.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ren and Bao(2020)]</span>
<span class="ltx_bibblock">
Fuji Ren and Yanwei Bao.

</span>
<span class="ltx_bibblock">A review on human-computer interaction and intelligent robots.

</span>
<span class="ltx_bibblock"><em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">International Journal of Information Technology &amp; Decision
Making</em>, 19(01):5â€“47, 2020.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Richter etÂ al.(2016)Richter, Vineet, Roth, and
Koltun]</span>
<span class="ltx_bibblock">
StephanÂ R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Playing for Data: Ground Truth from Computer Games.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, 2016.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ronneberger etÂ al.(2015)Ronneberger, Fischer, and
Brox]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical image computing and
computer-assisted intervention</em>, pages 234â€“241. Springer, 2015.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ros etÂ al.(2016)Ros, Sellart, Materzynska, Vazquez, and
Lopez]</span>
<span class="ltx_bibblock">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and AntonioÂ M
Lopez.

</span>
<span class="ltx_bibblock">The SYNTHIA Dataset: A Large Collection of Synthetic Images for
Semantic Segmentation of Urban Scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, 2016.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sakaridis etÂ al.(2021)Sakaridis, Dai, and VanÂ Gool]</span>
<span class="ltx_bibblock">
Christos Sakaridis, Dengxin Dai, and Luc VanÂ Gool.

</span>
<span class="ltx_bibblock">ACDC: The adverse conditions dataset with correspondences for
semantic driving scene understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 10765â€“10775, 2021.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sankaranarayanan etÂ al.(2018)Sankaranarayanan, Balaji, Jain, Lim, and
Chellappa]</span>
<span class="ltx_bibblock">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, SerÂ Nam Lim, and Rama
Chellappa.

</span>
<span class="ltx_bibblock">Learning from synthetic data: Addressing domain shift for semantic
segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 3752â€“3761, 2018.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Saxena etÂ al.(2005)Saxena, Chung, and Ng]</span>
<span class="ltx_bibblock">
Ashutosh Saxena, Sung Chung, and Andrew Ng.

</span>
<span class="ltx_bibblock">Learning depth from single monocular images.

</span>
<span class="ltx_bibblock"><em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 18, 2005.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Saxena etÂ al.(2008)Saxena, Sun, and Ng]</span>
<span class="ltx_bibblock">
Ashutosh Saxena, Min Sun, and AndrewÂ Y Ng.

</span>
<span class="ltx_bibblock">Make3d: Learning 3d scene structure from a single still image.

</span>
<span class="ltx_bibblock"><em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 31(5):824â€“840, 2008.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sun etÂ al.(2022)Sun, Segu, Postels, Wang, VanÂ Gool, Schiele, Tombari,
and Yu]</span>
<span class="ltx_bibblock">
Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc VanÂ Gool, Bernt Schiele,
Federico Tombari, and Fisher Yu.

</span>
<span class="ltx_bibblock">Shift: A synthetic driving dataset for continuous multi-task domain
adaptation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 21371â€“21382, 2022.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Teichmann etÂ al.(2018)Teichmann, Weber, Zoellner, Cipolla, and
Urtasun]</span>
<span class="ltx_bibblock">
Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel
Urtasun.

</span>
<span class="ltx_bibblock">Multinet: Real-time joint semantic reasoning for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">2018 IEEE intelligent vehicles symposium (IV)</em>, pages
1013â€“1020. IEEE, 2018.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tsirikoglou etÂ al.(2017)Tsirikoglou, Kronander, Wrenninge, and
Unger]</span>
<span class="ltx_bibblock">
Apostolia Tsirikoglou, Joel Kronander, Magnus Wrenninge, and Jonas Unger.

</span>
<span class="ltx_bibblock">Procedural Modeling and Physically Based Rendering for Synthetic
Data Generation in Automotive Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">arXiv:1710.06270</em>, 2017.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tzeng etÂ al.(2017)Tzeng, Hoffman, Saenko, and
Darrell]</span>
<span class="ltx_bibblock">
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Adversarial discriminative domain adaptation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 7167â€“7176, 2017.

</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Unity(2022)]</span>
<span class="ltx_bibblock">
Unity.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://unity.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://unity.com/</a>, 2022.

</span>
<span class="ltx_bibblock">Online; accessed: 2022-07-26.

</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wiseman(2022)]</span>
<span class="ltx_bibblock">
Yair Wiseman.

</span>
<span class="ltx_bibblock">Autonomous vehicles.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx42.1.1" class="ltx_emph ltx_font_italic">Research Anthology on Cross-Disciplinary Designs and
Applications of Automation</em>, pages 878â€“889. IGI Global, 2022.

</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[with Rocks and (HDRP)(2022)]</span>
<span class="ltx_bibblock">
4KÂ ProceduralÂ Terrain with Rocks and Mold SubstanceÂ Material (HDRP).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://assetstore.unity.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://assetstore.unity.com/</a>, 2022.

</span>
<span class="ltx_bibblock">Online; accessed: 2022-07-26.

</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wrenninge and Unger(2018)]</span>
<span class="ltx_bibblock">
Magnus Wrenninge and Jonas Unger.

</span>
<span class="ltx_bibblock">Synscapes: A Photorealistic Synthetic Dataset for Street Scene
Parsing.

</span>
<span class="ltx_bibblock"><em id="bib.bibx44.1.1" class="ltx_emph ltx_font_italic">arXiv:1810.08705</em>, 2018.

</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Xie etÂ al.(2022)Xie, Yuan, Li, Liu, and Cheng]</span>
<span class="ltx_bibblock">
Binhui Xie, Longhui Yuan, Shuang Li, ChiÂ Harold Liu, and Xinjing Cheng.

</span>
<span class="ltx_bibblock">Towards fewer annotations: Active learning via region impurity and
prediction uncertainty for domain adaptive semantic segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 8068â€“8078, 2022.

</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Xu etÂ al.(2021)Xu, Ma, Wu, Long, and Huang]</span>
<span class="ltx_bibblock">
QiÂ Xu, Yinan Ma, Jing Wu, Chengnian Long, and Xiaolin Huang.

</span>
<span class="ltx_bibblock">Cdada: A curriculum domain adaptation for nighttime semantic
segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 2962â€“2971, 2021.

</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Xu etÂ al.(2019)Xu, Du, Zhang, Zhang, Wang, and Zhang]</span>
<span class="ltx_bibblock">
Yonghao Xu, BoÂ Du, Lefei Zhang, Qian Zhang, Guoli Wang, and Liangpei Zhang.

</span>
<span class="ltx_bibblock">Self-ensembling attention networks: Addressing domain shift for
semantic segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 33, pages 5581â€“5588, 2019.

</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yuan etÂ al.(2019)Yuan, Chen, Chen, and Wang]</span>
<span class="ltx_bibblock">
Yuhui Yuan, Xiaokang Chen, Xilin Chen, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Segmentation transformer: Object-contextual representations for
semantic segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bibx48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.11065</em>, 2019.

</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhao etÂ al.(2017)Zhao, Shi, Qi, Wang, and Jia]</span>
<span class="ltx_bibblock">
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Pyramid scene parsing network.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2881â€“2890, 2017.

</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhou etÂ al.(2017)Zhou, Zhao, Puig, Fidler, Barriuso, and
Torralba]</span>
<span class="ltx_bibblock">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
Torralba.

</span>
<span class="ltx_bibblock">Scene parsing through ade20k dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 633â€“641, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.05625" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.05626" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.05626">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.05626" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.05627" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 03:17:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
