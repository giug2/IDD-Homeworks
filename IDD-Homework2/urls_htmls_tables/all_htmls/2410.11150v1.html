<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems</title>
<!--Generated on Tue Oct 15 00:13:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender Systems,  Session-Based Recommendation,  transformer Models,  GNN,  Masking Techniques" lang="en" name="keywords"/>
<base href="/html/2410.11150v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S1" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S2" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S3" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S3.SS1" title="In 3. Preliminaries â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S3.SS2" title="In 3. Preliminaries â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S3.SS2.SSS1" title="In 3.2. Language Models â€£ 3. Preliminaries â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Masked Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S3.SS2.SSS2" title="In 3.2. Language Models â€£ 3. Preliminaries â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Causal Language Modeling</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>The proposed method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS1" title="In 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data Augmentation with Window Sliding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS2" title="In 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Masking the Penultimate Token</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3" title="In 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Hypotheses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3.SSS1" title="In 4.3. Hypotheses â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Advantages of <span class="ltx_glossaryref" title="">SMM</span> Over <span class="ltx_glossaryref" title="">CLM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3.SSS2" title="In 4.3. Hypotheses â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Advantages of <span class="ltx_glossaryref" title="">SMM</span> Over <span class="ltx_glossaryref" title="">MLM</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3.SSS3" title="In 4.3. Hypotheses â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Masking the penultimate instead of the last</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S5" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Optimizing techniques</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S5.SS1" title="In 5. Optimizing techniques â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Weight Tying</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S5.SS2" title="In 5. Optimizing techniques â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Pre-layer Normalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S5.SS3" title="In 5. Optimizing techniques â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Contextual Positional Encoding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluation and Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS1" title="In 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Experimental Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS1.SSS1" title="In 6.1. Experimental Datasets â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Data Preprocessing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS2" title="In 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS3" title="In 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS3.SSS1" title="In 6.3. Metrics â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Precision@20</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS3.SSS2" title="In 6.3. Metrics â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Mean Reciprocal Rank</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS4" title="In 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Masking Techniques Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS5" title="In 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Sequentially Masking the Last K Items</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Comparison with the State of the Art</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7.SS1" title="In 7. Comparison with the State of the Art â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Types of Benchmark Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7.SS2" title="In 7. Comparison with the State of the Art â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Benchmark Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7.SS3" title="In 7. Comparison with the State of the Art â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Global Results Table</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S8" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1" title="In Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.SS1" title="In Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Ablation study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.SS2" title="In Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Optimized BERT Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.SS3" title="In Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Hyperparameter Setup</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anis Redjdal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:anis.redjdal.poly@gmail.com">anis.redjdal.poly@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/1234-5678-9012" title="ORCID identifier">1234-5678-9012</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Polytechnique Montreal</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Montreal</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Quebec</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">Canada</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luis Pinto
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:luis.pinto1@airliquide.com">luis.pinto1@airliquide.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/1234-5678-9012" title="ORCID identifier">1234-5678-9012</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Air Liquide</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Montreal</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Quebec</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">Canada</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michel Desmarais
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:michel.desmarais@polymtl.ca">michel.desmarais@polymtl.ca</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/1234-5678-9012" title="ORCID identifier">1234-5678-9012</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Polytechnique Montreal</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Montreal</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">Quebec</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">Canada</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Session-based recommendation is the task of predicting the next item a user will interact with, often without access to historical user data. In this work, we introduce Sequential Masked Modeling, a novel approach for encoder-only transformer architectures to tackle the challenges of single-session recommendation. Our method combines data augmentation through window sliding with a unique penultimate token masking strategy to capture sequential dependencies more effectively. By enhancing how transformers handle session data, Sequential Masked Modeling significantly improves next-item prediction performance.</p>
<p class="ltx_p" id="id14.id2">We evaluate our approach on three widely-used datasets, Yoochoose 1/64, Diginetica, and Tmall, comparing it to state-of-the-art single-session, cross-session, and multi-relation approaches. The results demonstrate that our Transformer-SMM models consistently outperform all models that rely on the same amount of information, while even rivaling methods that have access to more extensive user history. This study highlights the potential of encoder-only transformers in session-based recommendation and opens the door for further improvements.</p>
</div>
<div class="ltx_keywords">Recommender Systems, Session-Based Recommendation, transformer Models, GNN, Masking Techniques
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Traditional recommendation systems predominantly rely on a userâ€™s historical interactions and preferences <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib22" title="">2015</a>)</cite>. However, when user identities are partially known or entirely anonymous, recommendations must be generated based solely on the actions taken within a single session. These session-based scenarios challenge traditional models, which depend heavily on extensive user-item interaction history to provide accurate recommendations.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_glossaryref" title="">Session-Based Recommendation (SBR)</span> models tackle this challenge by analyzing user behavior within a single session to predict future actions and make relevant recommendations. Each session is treated as an independent, ordered sequence of consecutive interactions with items, regardless of whether the same user appears in multiple sessions, focusing solely on the interactions within a specific context. Accurately modeling these sessions is crucial for improving recommendation relevance and providing a personalized, engaging user experience. For example, on an e-commerce website, a session might begin when the user logs in and conclude when they leave the site, capturing the sequence of items they viewed in chronological order.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Considering that the task of <span class="ltx_glossaryref" title="">SBR</span> is to predict the next item that might interest a user, the task closely resembles the challenges associated with predicting the next word in <span class="ltx_glossaryref" title="">Natural Language Processing (NLP)</span>. This parallel is a natural motivation to explore the application of transformer models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib33" title="">2017</a>)</cite>. Just like in <span class="ltx_glossaryref" title="">NLP</span>, items are represented as tokens and processed as a sequence.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we introduce <span class="ltx_glossaryref" title="">Sequential Masked Modeling (SMM)</span>, a novel approach for improving session-based recommendation using encoder-only transformer architectures. Inspired by the success of transformers in NLP, our method adapts these models to <span class="ltx_glossaryref" title="">SBR</span> by employing data augmentation through window sliding and a new masking strategy. Additionally, we optimize the transformer architectures to further enhance performance. We evaluate our proposed <span class="ltx_glossaryref" title="">SMM</span> method on three benchmark datasets: Yoochoose 1/64, Diginetica, and Tmall. Our Transformer-SMM models were evaluated against state-of-the-art models, demonstrating clear improvements when working with the same level of information. Even in comparison to models utilizing more user data, our approach remained competitive in terms of precision and ranking metrics. In the following sections, we describe our method, the optimization techniques applied, and evaluate our approach against existing benchmarks.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S1.F1.g1" src="extracted/5923458/images/next_click_pred.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Next-click prediction task <cite class="ltx_cite ltx_citemacro_citep">(Cloudera Fast Forward Labs, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib6" title="">2021</a>)</cite>.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Early research in session-based recommendations primarily explored sequential models, with the Markov Chain (MC) model being one of the first to address this domain <cite class="ltx_cite ltx_citemacro_citep">(Dias etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib9" title="">2005</a>)</cite>. However, MC often struggles with capturing long-range dependencies in the userâ€™s behavior, as it tends to compress information from earlier interactions. As a result, its effectiveness diminishes as the sequence length increases.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Hidasi et al. <cite class="ltx_cite ltx_citemacro_citep">(Hidasi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib14" title="">2015</a>)</cite> introduced a <span class="ltx_glossaryref" title="">Recurrent Neural Network (RNN)</span>-based method, which was later enhanced by incorporating data augmentation and accounting for temporal shifts in user behavior, as explored by <cite class="ltx_cite ltx_citemacro_citep">(VeliÄkoviÄ‡ etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib34" title="">2017</a>)</cite>. Further developments in 2016 introduced the concept of integrating dwell time, the duration of item views, into session-based recommendations <cite class="ltx_cite ltx_citemacro_citep">(Hidasi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib15" title="">2016</a>)</cite>. This enhancement allowed <span class="ltx_glossaryref" title="">RNN</span> models to more accurately capture user interest and engagement, thereby boosting their predictive performance. Building on this, NARM <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib20" title="">2017</a>)</cite> introduced a dual-approach <span class="ltx_glossaryref" title="">RNN</span> recommendation system that captures both the sequential patterns of user behavior and their primary intentions. Despite the inherent challenges associated with <span class="ltx_glossaryref" title="">RNN</span> s, such as the difficulty in representing users with limited data or the complexity of modeling distant item transitions within a sequence, researchers have gradually improved this methodology over time, though the associated challenges remain significant.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">To address these limitations, the introduction of <span class="ltx_glossaryref" title="">Graph Neural Networks (GNN)</span> in 2019 marked a major advancement in the field of <span class="ltx_glossaryref" title="">SBR</span>. Session-based <span class="ltx_glossaryref" title="">GNN</span> s <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib39" title="">2019</a>; Huang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib16" title="">2021</a>; Zhang and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib42" title="">2023</a>)</cite> are specifically designed to generate representations within graphs of item interactions, making them particularly effective at capturing and representing the intricate patterns in these sequences. Since the introduction of SR-GNN <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib39" title="">2019</a>)</cite>, graph-based models have gained significant recognition in <span class="ltx_glossaryref" title="">SBR</span>, consistently delivering superior performance. Over time, models like CARES <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib43" title="">2023</a>)</cite>, STAR <cite class="ltx_cite ltx_citemacro_citep">(Ahn etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib2" title="">2023</a>)</cite>, CoTREC <cite class="ltx_cite ltx_citemacro_citep">(Xia etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib37" title="">2021a</a>)</cite>, and GCE-GNN <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib36" title="">2020</a>)</cite> have further refined this approach, positioning <span class="ltx_glossaryref" title="">GNN</span> s at the current state-of-the-art in <span class="ltx_glossaryref" title="">SBR</span>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Transformers have also achieved significant success in the SBR domain. In the RecSys Challenge 2022 <cite class="ltx_cite ltx_citemacro_citep">(Landia etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib18" title="">2022</a>)</cite>, a team employing transformer architectures secured 2nd place among 300 participants <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib23" title="">2022</a>)</cite>. Furthermore, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib42" title="">2023</a>)</cite> introduced a novel approach that integrated a <span class="ltx_glossaryref" title="">GNN</span> with a transformer, using an attention mechanism to process sequences in graph form. This hybrid model, based on the BERT architecture <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib8" title="">2018</a>)</cite>, primarily utilizes an encoder to enhance session data representations. The fusion of graph-based models and transformer architectures marks a significant step forward in <span class="ltx_glossaryref" title="">SBR</span>, combining the strengths of both techniques.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Building on the success of transformers in <span class="ltx_glossaryref" title="">SBR</span>, <span class="ltx_text ltx_font_italic" id="S2.p5.1.1">Meta</span>, in collaboration with <span class="ltx_text ltx_font_italic" id="S2.p5.1.2">Nvidia</span>, developed a dedicated library that provides the most popular transformer architecture for session-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(deÂ Souza PereiraÂ Moreira etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib7" title="">2021</a>)</cite>. While their reported performance on benchmarks falls short of <span class="ltx_glossaryref" title="">GNN</span>-based models, this library offers a useful solution by streamlining session data preprocessing and transformer language model training into a single pipeline.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Preliminaries</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Problem Statement</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.12">First, we formally define the task of next-click prediction in session-based recommendations. Let <math alttext="V=\{v_{1},v_{2},\dots,v_{m}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.4"><semantics id="S3.SS1.p1.1.m1.4a"><mrow id="S3.SS1.p1.1.m1.4.4" xref="S3.SS1.p1.1.m1.4.4.cmml"><mi id="S3.SS1.p1.1.m1.4.4.5" xref="S3.SS1.p1.1.m1.4.4.5.cmml">V</mi><mo id="S3.SS1.p1.1.m1.4.4.4" xref="S3.SS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.4.4.3.3" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml"><mo id="S3.SS1.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml">v</mi><mn id="S3.SS1.p1.1.m1.2.2.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.4.4.3.3.5" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.3.3.2.2.2" xref="S3.SS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.3.3.2.2.2.2" xref="S3.SS1.p1.1.m1.3.3.2.2.2.2.cmml">v</mi><mn id="S3.SS1.p1.1.m1.3.3.2.2.2.3" xref="S3.SS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.1.m1.4.4.3.3.6" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S3.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.1.m1.4.4.3.3.7" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.1.m1.4.4.3.3.3" xref="S3.SS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.4.4.3.3.3.2" xref="S3.SS1.p1.1.m1.4.4.3.3.3.2.cmml">v</mi><mi id="S3.SS1.p1.1.m1.4.4.3.3.3.3" xref="S3.SS1.p1.1.m1.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S3.SS1.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S3.SS1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.4b"><apply id="S3.SS1.p1.1.m1.4.4.cmml" xref="S3.SS1.p1.1.m1.4.4"><eq id="S3.SS1.p1.1.m1.4.4.4.cmml" xref="S3.SS1.p1.1.m1.4.4.4"></eq><ci id="S3.SS1.p1.1.m1.4.4.5.cmml" xref="S3.SS1.p1.1.m1.4.4.5">ğ‘‰</ci><set id="S3.SS1.p1.1.m1.4.4.3.4.cmml" xref="S3.SS1.p1.1.m1.4.4.3.3"><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2">ğ‘£</ci><cn id="S3.SS1.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2.2.2">ğ‘£</ci><cn id="S3.SS1.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">â€¦</ci><apply id="S3.SS1.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.4.4.3.3.3.2">ğ‘£</ci><ci id="S3.SS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.4.4.3.3.3.3">ğ‘š</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.4c">V=\{v_{1},v_{2},\dots,v_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.4d">italic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math> represent the set of all items, where <math alttext="m" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_m</annotation></semantics></math> denotes the number of items in <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_V</annotation></semantics></math>. Assume that all sessions are represented as <math alttext="U=\{S_{1},S_{2},\dots,S_{n}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.4"><semantics id="S3.SS1.p1.4.m4.4a"><mrow id="S3.SS1.p1.4.m4.4.4" xref="S3.SS1.p1.4.m4.4.4.cmml"><mi id="S3.SS1.p1.4.m4.4.4.5" xref="S3.SS1.p1.4.m4.4.4.5.cmml">U</mi><mo id="S3.SS1.p1.4.m4.4.4.4" xref="S3.SS1.p1.4.m4.4.4.4.cmml">=</mo><mrow id="S3.SS1.p1.4.m4.4.4.3.3" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml"><mo id="S3.SS1.p1.4.m4.4.4.3.3.4" stretchy="false" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml">{</mo><msub id="S3.SS1.p1.4.m4.2.2.1.1.1" xref="S3.SS1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS1.p1.4.m4.2.2.1.1.1.2.cmml">S</mi><mn id="S3.SS1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS1.p1.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.4.m4.4.4.3.3.5" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.4.m4.3.3.2.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.4.m4.3.3.2.2.2.2" xref="S3.SS1.p1.4.m4.3.3.2.2.2.2.cmml">S</mi><mn id="S3.SS1.p1.4.m4.3.3.2.2.2.3" xref="S3.SS1.p1.4.m4.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.4.m4.4.4.3.3.6" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><mi id="S3.SS1.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p1.4.m4.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.4.m4.4.4.3.3.7" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.4.m4.4.4.3.3.3" xref="S3.SS1.p1.4.m4.4.4.3.3.3.cmml"><mi id="S3.SS1.p1.4.m4.4.4.3.3.3.2" xref="S3.SS1.p1.4.m4.4.4.3.3.3.2.cmml">S</mi><mi id="S3.SS1.p1.4.m4.4.4.3.3.3.3" xref="S3.SS1.p1.4.m4.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S3.SS1.p1.4.m4.4.4.3.3.8" stretchy="false" xref="S3.SS1.p1.4.m4.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.4b"><apply id="S3.SS1.p1.4.m4.4.4.cmml" xref="S3.SS1.p1.4.m4.4.4"><eq id="S3.SS1.p1.4.m4.4.4.4.cmml" xref="S3.SS1.p1.4.m4.4.4.4"></eq><ci id="S3.SS1.p1.4.m4.4.4.5.cmml" xref="S3.SS1.p1.4.m4.4.4.5">ğ‘ˆ</ci><set id="S3.SS1.p1.4.m4.4.4.3.4.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3"><apply id="S3.SS1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.2.2.1.1.1.2">ğ‘†</ci><cn id="S3.SS1.p1.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p1.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2.2.2.2">ğ‘†</ci><cn id="S3.SS1.p1.4.m4.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.4.m4.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">â€¦</ci><apply id="S3.SS1.p1.4.m4.4.4.3.3.3.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.4.4.3.3.3.1.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.4.4.3.3.3.2.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.3.2">ğ‘†</ci><ci id="S3.SS1.p1.4.m4.4.4.3.3.3.3.cmml" xref="S3.SS1.p1.4.m4.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.4c">U=\{S_{1},S_{2},\dots,S_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.4d">italic_U = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_n</annotation></semantics></math> is the total number of sessions. Each session <math alttext="S_{\tau}\in U" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><msub id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2.2" xref="S3.SS1.p1.6.m6.1.1.2.2.cmml">S</mi><mi id="S3.SS1.p1.6.m6.1.1.2.3" xref="S3.SS1.p1.6.m6.1.1.2.3.cmml">Ï„</mi></msub><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">âˆˆ</mo><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><in id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></in><apply id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS1.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2.2">ğ‘†</ci><ci id="S3.SS1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS1.p1.6.m6.1.1.2.3">ğœ</ci></apply><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">S_{\tau}\in U</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT âˆˆ italic_U</annotation></semantics></math>, denoted as <math alttext="S_{\tau}=\{v_{\tau 1},v_{\tau 2},\dots,v_{\tau t}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.4"><semantics id="S3.SS1.p1.7.m7.4a"><mrow id="S3.SS1.p1.7.m7.4.4" xref="S3.SS1.p1.7.m7.4.4.cmml"><msub id="S3.SS1.p1.7.m7.4.4.5" xref="S3.SS1.p1.7.m7.4.4.5.cmml"><mi id="S3.SS1.p1.7.m7.4.4.5.2" xref="S3.SS1.p1.7.m7.4.4.5.2.cmml">S</mi><mi id="S3.SS1.p1.7.m7.4.4.5.3" xref="S3.SS1.p1.7.m7.4.4.5.3.cmml">Ï„</mi></msub><mo id="S3.SS1.p1.7.m7.4.4.4" xref="S3.SS1.p1.7.m7.4.4.4.cmml">=</mo><mrow id="S3.SS1.p1.7.m7.4.4.3.3" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml"><mo id="S3.SS1.p1.7.m7.4.4.3.3.4" stretchy="false" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml">{</mo><msub id="S3.SS1.p1.7.m7.2.2.1.1.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.2.cmml">v</mi><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.3.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.2.cmml">Ï„</mi><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.3.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.1.cmml">â¢</mo><mn id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.p1.7.m7.4.4.3.3.5" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.7.m7.3.3.2.2.2" xref="S3.SS1.p1.7.m7.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.7.m7.3.3.2.2.2.2" xref="S3.SS1.p1.7.m7.3.3.2.2.2.2.cmml">v</mi><mrow id="S3.SS1.p1.7.m7.3.3.2.2.2.3" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.cmml"><mi id="S3.SS1.p1.7.m7.3.3.2.2.2.3.2" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.2.cmml">Ï„</mi><mo id="S3.SS1.p1.7.m7.3.3.2.2.2.3.1" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.1.cmml">â¢</mo><mn id="S3.SS1.p1.7.m7.3.3.2.2.2.3.3" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S3.SS1.p1.7.m7.4.4.3.3.6" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml">,</mo><mi id="S3.SS1.p1.7.m7.1.1" mathvariant="normal" xref="S3.SS1.p1.7.m7.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.7.m7.4.4.3.3.7" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml">,</mo><msub id="S3.SS1.p1.7.m7.4.4.3.3.3" xref="S3.SS1.p1.7.m7.4.4.3.3.3.cmml"><mi id="S3.SS1.p1.7.m7.4.4.3.3.3.2" xref="S3.SS1.p1.7.m7.4.4.3.3.3.2.cmml">v</mi><mrow id="S3.SS1.p1.7.m7.4.4.3.3.3.3" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.cmml"><mi id="S3.SS1.p1.7.m7.4.4.3.3.3.3.2" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.2.cmml">Ï„</mi><mo id="S3.SS1.p1.7.m7.4.4.3.3.3.3.1" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.7.m7.4.4.3.3.3.3.3" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.3.cmml">t</mi></mrow></msub><mo id="S3.SS1.p1.7.m7.4.4.3.3.8" stretchy="false" xref="S3.SS1.p1.7.m7.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.4b"><apply id="S3.SS1.p1.7.m7.4.4.cmml" xref="S3.SS1.p1.7.m7.4.4"><eq id="S3.SS1.p1.7.m7.4.4.4.cmml" xref="S3.SS1.p1.7.m7.4.4.4"></eq><apply id="S3.SS1.p1.7.m7.4.4.5.cmml" xref="S3.SS1.p1.7.m7.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.4.4.5.1.cmml" xref="S3.SS1.p1.7.m7.4.4.5">subscript</csymbol><ci id="S3.SS1.p1.7.m7.4.4.5.2.cmml" xref="S3.SS1.p1.7.m7.4.4.5.2">ğ‘†</ci><ci id="S3.SS1.p1.7.m7.4.4.5.3.cmml" xref="S3.SS1.p1.7.m7.4.4.5.3">ğœ</ci></apply><set id="S3.SS1.p1.7.m7.4.4.3.4.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3"><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.2">ğ‘£</ci><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3"><times id="S3.SS1.p1.7.m7.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.1"></times><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.2">ğœ</ci><cn id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.p1.7.m7.3.3.2.2.2.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.7.m7.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2.2">ğ‘£</ci><apply id="S3.SS1.p1.7.m7.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3"><times id="S3.SS1.p1.7.m7.3.3.2.2.2.3.1.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.1"></times><ci id="S3.SS1.p1.7.m7.3.3.2.2.2.3.2.cmml" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.2">ğœ</ci><cn id="S3.SS1.p1.7.m7.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS1.p1.7.m7.3.3.2.2.2.3.3">2</cn></apply></apply><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">â€¦</ci><apply id="S3.SS1.p1.7.m7.4.4.3.3.3.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.4.4.3.3.3.1.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.7.m7.4.4.3.3.3.2.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3.2">ğ‘£</ci><apply id="S3.SS1.p1.7.m7.4.4.3.3.3.3.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3"><times id="S3.SS1.p1.7.m7.4.4.3.3.3.3.1.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.1"></times><ci id="S3.SS1.p1.7.m7.4.4.3.3.3.3.2.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.2">ğœ</ci><ci id="S3.SS1.p1.7.m7.4.4.3.3.3.3.3.cmml" xref="S3.SS1.p1.7.m7.4.4.3.3.3.3.3">ğ‘¡</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.4c">S_{\tau}=\{v_{\tau 1},v_{\tau 2},\dots,v_{\tau t}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.4d">italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT = { italic_v start_POSTSUBSCRIPT italic_Ï„ 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_Ï„ 2 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_Ï„ italic_t end_POSTSUBSCRIPT }</annotation></semantics></math>, consists of a sequence of interactions in chronological order, where <math alttext="v_{\tau t}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">v</mi><mrow id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml"><mi id="S3.SS1.p1.8.m8.1.1.3.2" xref="S3.SS1.p1.8.m8.1.1.3.2.cmml">Ï„</mi><mo id="S3.SS1.p1.8.m8.1.1.3.1" xref="S3.SS1.p1.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS1.p1.8.m8.1.1.3.3" xref="S3.SS1.p1.8.m8.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ‘£</ci><apply id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><times id="S3.SS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.p1.8.m8.1.1.3.1"></times><ci id="S3.SS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.p1.8.m8.1.1.3.2">ğœ</ci><ci id="S3.SS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">v_{\tau t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_v start_POSTSUBSCRIPT italic_Ï„ italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents the item interacted with by the user at the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_t</annotation></semantics></math>-th timestamp in session <math alttext="S_{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">Ï„</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">ğ‘†</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">S_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT</annotation></semantics></math>, and the length of <math alttext="S_{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">Ï„</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">ğ‘†</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">S_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT</annotation></semantics></math> is <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">italic_t</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">The goal of <span class="ltx_glossaryref" title="">SBR</span> is to recommend the next item from <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_V</annotation></semantics></math> that the user is most likely to interact with, given the current session <math alttext="S_{\tau}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">Ï„</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ğ‘†</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">S_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT</annotation></semantics></math>. Specifically, the item interacted with at the <math alttext="(t+1)" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml"><mo id="S3.SS1.p2.3.m3.1.1.1.2" stretchy="false" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p2.3.m3.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.3.m3.1.1.1.3" stretchy="false" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"><plus id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1"></plus><ci id="S3.SS1.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.2">ğ‘¡</ci><cn id="S3.SS1.p2.3.m3.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">(t+1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">( italic_t + 1 )</annotation></semantics></math>-th timestamp is referred to as the target item or ground truth item of the session. Thus, a session and its target item pair can be expressed as <math alttext="([v_{1},v_{2},\dots,v_{t}],v_{t+1})" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.3"><semantics id="S3.SS1.p2.4.m4.3a"><mrow id="S3.SS1.p2.4.m4.3.3.2" xref="S3.SS1.p2.4.m4.3.3.3.cmml"><mo id="S3.SS1.p2.4.m4.3.3.2.3" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.3.cmml">(</mo><mrow id="S3.SS1.p2.4.m4.2.2.1.1.3" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml"><mo id="S3.SS1.p2.4.m4.2.2.1.1.3.4" stretchy="false" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml">[</mo><msub id="S3.SS1.p2.4.m4.2.2.1.1.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.2.cmml">v</mi><mn id="S3.SS1.p2.4.m4.2.2.1.1.1.1.3" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.4.m4.2.2.1.1.3.5" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.2.2.1.1.2.2" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.1.2.2.2" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2.2.cmml">v</mi><mn id="S3.SS1.p2.4.m4.2.2.1.1.2.2.3" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p2.4.m4.2.2.1.1.3.6" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml">,</mo><mi id="S3.SS1.p2.4.m4.1.1" mathvariant="normal" xref="S3.SS1.p2.4.m4.1.1.cmml">â€¦</mi><mo id="S3.SS1.p2.4.m4.2.2.1.1.3.7" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml">,</mo><msub id="S3.SS1.p2.4.m4.2.2.1.1.3.3" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.1.3.3.2" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3.2.cmml">v</mi><mi id="S3.SS1.p2.4.m4.2.2.1.1.3.3.3" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3.3.cmml">t</mi></msub><mo id="S3.SS1.p2.4.m4.2.2.1.1.3.8" stretchy="false" xref="S3.SS1.p2.4.m4.2.2.1.1.4.cmml">]</mo></mrow><mo id="S3.SS1.p2.4.m4.3.3.2.4" xref="S3.SS1.p2.4.m4.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.4.m4.3.3.2.2" xref="S3.SS1.p2.4.m4.3.3.2.2.cmml"><mi id="S3.SS1.p2.4.m4.3.3.2.2.2" xref="S3.SS1.p2.4.m4.3.3.2.2.2.cmml">v</mi><mrow id="S3.SS1.p2.4.m4.3.3.2.2.3" xref="S3.SS1.p2.4.m4.3.3.2.2.3.cmml"><mi id="S3.SS1.p2.4.m4.3.3.2.2.3.2" xref="S3.SS1.p2.4.m4.3.3.2.2.3.2.cmml">t</mi><mo id="S3.SS1.p2.4.m4.3.3.2.2.3.1" xref="S3.SS1.p2.4.m4.3.3.2.2.3.1.cmml">+</mo><mn id="S3.SS1.p2.4.m4.3.3.2.2.3.3" xref="S3.SS1.p2.4.m4.3.3.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.p2.4.m4.3.3.2.5" stretchy="false" xref="S3.SS1.p2.4.m4.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.3b"><interval closure="open" id="S3.SS1.p2.4.m4.3.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3.2"><list id="S3.SS1.p2.4.m4.2.2.1.1.4.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3"><apply id="S3.SS1.p2.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.2">ğ‘£</ci><cn id="S3.SS1.p2.4.m4.2.2.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.p2.4.m4.2.2.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.1.1.2.2.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.1.1.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2.2">ğ‘£</ci><cn id="S3.SS1.p2.4.m4.2.2.1.1.2.2.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.2.2.1.1.2.2.3">2</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">â€¦</ci><apply id="S3.SS1.p2.4.m4.2.2.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.1.1.3.3.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.1.1.3.3.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3.2">ğ‘£</ci><ci id="S3.SS1.p2.4.m4.2.2.1.1.3.3.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.3.3.3">ğ‘¡</ci></apply></list><apply id="S3.SS1.p2.4.m4.3.3.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.2.2.1.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.2.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2.2">ğ‘£</ci><apply id="S3.SS1.p2.4.m4.3.3.2.2.3.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2.3"><plus id="S3.SS1.p2.4.m4.3.3.2.2.3.1.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2.3.1"></plus><ci id="S3.SS1.p2.4.m4.3.3.2.2.3.2.cmml" xref="S3.SS1.p2.4.m4.3.3.2.2.3.2">ğ‘¡</ci><cn id="S3.SS1.p2.4.m4.3.3.2.2.3.3.cmml" type="integer" xref="S3.SS1.p2.4.m4.3.3.2.2.3.3">1</cn></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.3c">([v_{1},v_{2},\dots,v_{t}],v_{t+1})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.3d">( [ italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] , italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Language Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Motivated by the success of transformers in <span class="ltx_glossaryref" title="">NLP</span>, where they perform exceptionally at capturing sequential dependencies and contextual information, we apply these models to session-based recommendation. With their ability to model long-range dependencies through the attention mechanism, transformers have proven highly effective in tasks like next-word prediction, and this same architecture can be adapted to predict the next item in a userâ€™s browsing session. While the data and context differ from NLP, the underlying principle of predicting the next interaction based on past behavior remains similar, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">1</span></a>. For the experiments and evaluations conducted in this work, we focused on three popular transformer models: two encoder-only models, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib8" title="">2018</a>)</cite> and DeBERTa <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib13" title="">2020</a>)</cite>, and one decoder-only model, GPT <cite class="ltx_cite ltx_citemacro_citep">(Lee and Hsiang, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib19" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">These language models are trained with a technique called masking. This approach helps the model learn contextual relationships and dependencies between tokens by hiding parts of the input sequence and requiring the model to predict the missing elements. This method is particularly effective for capturing patterns and structure within sequences, making it essential for a variety of tasks, including session-based recommendations. Two masking techniques are commonly used: <span class="ltx_glossaryref" title="">Masked Language Modeling (MLM)</span> and <span class="ltx_glossaryref" title="">Causal Language Modeling (CLM)</span>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Masked Language Modeling</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">This is a training technique where a random selection of tokens within a sequence is masked, and the model is trained to predict the masked tokens based on the surrounding unmasked context. By leveraging both preceding and following tokens, this approach enables the model to learn bidirectional contextual representations, enhancing its ability to capture the full context of the input data. The models that employ this masking approach are BERT and DeBERTa. The corresponding loss function is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{MLM}}=-\sum_{i\in\mathcal{M}}\log P(x_{i}|x_{\setminus i})" class="ltx_Math" display="block" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.3.2" xref="S3.Ex1.m1.1.1.3.2.cmml">â„’</mi><mtext id="S3.Ex1.m1.1.1.3.3" xref="S3.Ex1.m1.1.1.3.3a.cmml">MLM</mtext></msub><mo id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1a" xref="S3.Ex1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><munder id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2" movablelimits="false" xref="S3.Ex1.m1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.3.2" xref="S3.Ex1.m1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.Ex1.m1.1.1.1.1.2.3.1" xref="S3.Ex1.m1.1.1.1.1.2.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.1.1.2.3.3" xref="S3.Ex1.m1.1.1.1.1.2.3.3.cmml">â„³</mi></mrow></munder><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S3.Ex1.m1.1.1.1.1.1.3a" lspace="0.167em" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml">â¡</mo><mi id="S3.Ex1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3a" rspace="0em" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml">âˆ–</mo><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">i</mi></mrow></msub></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><eq id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2"></eq><apply id="S3.Ex1.m1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.3.2">â„’</ci><ci id="S3.Ex1.m1.1.1.3.3a.cmml" xref="S3.Ex1.m1.1.1.3.3"><mtext id="S3.Ex1.m1.1.1.3.3.cmml" mathsize="70%" xref="S3.Ex1.m1.1.1.3.3">MLM</mtext></ci></apply><apply id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><minus id="S3.Ex1.m1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1"></minus><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1"><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex1.m1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2"></sum><apply id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3"><in id="S3.Ex1.m1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.1"></in><ci id="S3.Ex1.m1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.2">ğ‘–</ci><ci id="S3.Ex1.m1.1.1.1.1.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3.3">â„³</ci></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3"><log id="S3.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.1"></log><ci id="S3.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.2">ğ‘¥</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.2">ğ‘¥</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3"><setdiff id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3"></setdiff><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.3.3.2">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathcal{L}_{\text{MLM}}=-\sum_{i\in\mathcal{M}}\log P(x_{i}|x_{\setminus i})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">caligraphic_L start_POSTSUBSCRIPT MLM end_POSTSUBSCRIPT = - âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ caligraphic_M end_POSTSUBSCRIPT roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT âˆ– italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.5">where <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.1.m1.1"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><ci id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.1.m1.1d">caligraphic_M</annotation></semantics></math> is the set of masked positions, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.2.m2.1"><semantics id="S3.SS2.SSS1.p3.2.m2.1a"><msub id="S3.SS2.SSS1.p3.2.m2.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS1.p3.2.m2.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2">ğ‘¥</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the token at position <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.3.m3.1"><semantics id="S3.SS2.SSS1.p3.3.m3.1a"><mi id="S3.SS2.SSS1.p3.3.m3.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.3.m3.1b"><ci id="S3.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.3.m3.1d">italic_i</annotation></semantics></math>, and <math alttext="x_{\setminus i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.4.m4.1"><semantics id="S3.SS2.SSS1.p3.4.m4.1a"><msub id="S3.SS2.SSS1.p3.4.m4.1.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p3.4.m4.1.1.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml">x</mi><mrow id="S3.SS2.SSS1.p3.4.m4.1.1.3" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml"><mo id="S3.SS2.SSS1.p3.4.m4.1.1.3a" rspace="0em" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml">âˆ–</mo><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.4.m4.1b"><apply id="S3.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.2">ğ‘¥</ci><apply id="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3"><setdiff id="S3.SS2.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3"></setdiff><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.2">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.4.m4.1c">x_{\setminus i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT âˆ– italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents all tokens except <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.5.m5.1"><semantics id="S3.SS2.SSS1.p3.5.m5.1a"><msub id="S3.SS2.SSS1.p3.5.m5.1.1" xref="S3.SS2.SSS1.p3.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p3.5.m5.1.1.2" xref="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS1.p3.5.m5.1.1.3" xref="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.5.m5.1b"><apply id="S3.SS2.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.2">ğ‘¥</ci><ci id="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.5.m5.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p3.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Causal Language Modeling</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Also known as autoregressive modeling, trains the model to predict each token in a sequence using only the preceding context. Since each prediction is based on the tokens preceding the target token, the model is limited to unidirectional attention. The model that we used for this masking technique is GPT, trained with the following loss function:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{\lx@glossaries@gls@link{acronym}{clm}{\leavevmode CLM}}}=-%
\sum_{i=1}^{n}\log P(x_{i}|x_{1},x_{2},\ldots,x_{i-1})" class="ltx_Math" display="block" id="S3.Ex2.m1.2"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><msub id="S3.Ex2.m1.2.2.3" xref="S3.Ex2.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.2.2.3.2" xref="S3.Ex2.m1.2.2.3.2.cmml">â„’</mi><mtext id="S3.Ex2.m1.2.2.3.3" xref="S3.Ex2.m1.2.2.3.3b.cmml"><span class="ltx_glossaryref" style="font-size:70%;" title="">CLM</span></mtext></msub><mo id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.cmml"><mo id="S3.Ex2.m1.2.2.1a" xref="S3.Ex2.m1.2.2.1.cmml">âˆ’</mo><mrow id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><munderover id="S3.Ex2.m1.2.2.1.1.2" xref="S3.Ex2.m1.2.2.1.1.2.cmml"><mo id="S3.Ex2.m1.2.2.1.1.2.2.2" movablelimits="false" xref="S3.Ex2.m1.2.2.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.2.2.1.1.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2.3.2" xref="S3.Ex2.m1.2.2.1.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex2.m1.2.2.1.1.2.2.3.1" xref="S3.Ex2.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex2.m1.2.2.1.1.2.2.3.3" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex2.m1.2.2.1.1.2.3" xref="S3.Ex2.m1.2.2.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.Ex2.m1.2.2.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.cmml"><mrow id="S3.Ex2.m1.2.2.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.3.1" xref="S3.Ex2.m1.2.2.1.1.1.3.1.cmml">log</mi><mo id="S3.Ex2.m1.2.2.1.1.1.3a" lspace="0.167em" xref="S3.Ex2.m1.2.2.1.1.1.3.cmml">â¡</mo><mi id="S3.Ex2.m1.2.2.1.1.1.3.2" xref="S3.Ex2.m1.2.2.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.Ex2.m1.2.2.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex2.m1.2.2.1.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.2.2.1.1.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.4" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.4.cmml"><msub id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.4" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.5" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S3.Ex2.m1.1.1" mathvariant="normal" xref="S3.Ex2.m1.1.1.cmml">â€¦</mi><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.6" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.Ex2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><eq id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2"></eq><apply id="S3.Ex2.m1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.3.1.cmml" xref="S3.Ex2.m1.2.2.3">subscript</csymbol><ci id="S3.Ex2.m1.2.2.3.2.cmml" xref="S3.Ex2.m1.2.2.3.2">â„’</ci><ci id="S3.Ex2.m1.2.2.3.3b.cmml" xref="S3.Ex2.m1.2.2.3.3"><mtext id="S3.Ex2.m1.2.2.3.3.cmml" mathsize="70%" xref="S3.Ex2.m1.2.2.3.3"><span class="ltx_glossaryref" style="font-size:70%;" title="">CLM</span></mtext></ci></apply><apply id="S3.Ex2.m1.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1"><minus id="S3.Ex2.m1.2.2.1.2.cmml" xref="S3.Ex2.m1.2.2.1"></minus><apply id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1"><apply id="S3.Ex2.m1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.Ex2.m1.2.2.1.1.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.Ex2.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2"></sum><apply id="S3.Ex2.m1.2.2.1.1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3"><eq id="S3.Ex2.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.1"></eq><ci id="S3.Ex2.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.3.2">ğ‘–</ci><cn id="S3.Ex2.m1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex2.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex2.m1.2.2.1.1.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.3">ğ‘›</ci></apply><apply id="S3.Ex2.m1.2.2.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1"><times id="S3.Ex2.m1.2.2.1.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.2"></times><apply id="S3.Ex2.m1.2.2.1.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.3"><log id="S3.Ex2.m1.2.2.1.1.1.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.3.1"></log><ci id="S3.Ex2.m1.2.2.1.1.1.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.2">ğ‘¥</ci><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.5.3">ğ‘–</ci></apply><list id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.4.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3"><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><cn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.2.2.2.3">2</cn></apply><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">â€¦</ci><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.2">ğ‘¥</ci><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3"><minus id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn id="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S3.Ex2.m1.2.2.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\mathcal{L}_{\text{\lx@glossaries@gls@link{acronym}{clm}{\leavevmode CLM}}}=-%
\sum_{i=1}^{n}\log P(x_{i}|x_{1},x_{2},\ldots,x_{i-1})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.2d">caligraphic_L start_POSTSUBSCRIPT end_POSTSUBSCRIPT = - âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.5">where <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1.1"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><msub id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">ğ‘¥</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.2.m2.1"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><mi id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><ci id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.2.m2.1d">italic_i</annotation></semantics></math>-th token in the sequence, and <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.3.m3.1"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><mi id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><ci id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.3.m3.1d">italic_n</annotation></semantics></math> is the total number of tokens. The model estimates the likelihood of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.4.m4.1"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><mi id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><ci id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.4.m4.1d">italic_i</annotation></semantics></math>-th token given all previous tokens <math alttext="x_{1},x_{2},\ldots,x_{i-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.5.m5.4"><semantics id="S3.SS2.SSS2.p3.5.m5.4a"><mrow id="S3.SS2.SSS2.p3.5.m5.4.4.3" xref="S3.SS2.SSS2.p3.5.m5.4.4.4.cmml"><msub id="S3.SS2.SSS2.p3.5.m5.2.2.1.1" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.2" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1.2.cmml">x</mi><mn id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.3" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p3.5.m5.4.4.3.4" xref="S3.SS2.SSS2.p3.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS2.SSS2.p3.5.m5.3.3.2.2" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.2" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2.2.cmml">x</mi><mn id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.3" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p3.5.m5.4.4.3.5" xref="S3.SS2.SSS2.p3.5.m5.4.4.4.cmml">,</mo><mi id="S3.SS2.SSS2.p3.5.m5.1.1" mathvariant="normal" xref="S3.SS2.SSS2.p3.5.m5.1.1.cmml">â€¦</mi><mo id="S3.SS2.SSS2.p3.5.m5.4.4.3.6" xref="S3.SS2.SSS2.p3.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS2.SSS2.p3.5.m5.4.4.3.3" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.2" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.2.cmml">x</mi><mrow id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.2" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.2.cmml">i</mi><mo id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.1" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.3" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m5.4b"><list id="S3.SS2.SSS2.p3.5.m5.4.4.4.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3"><apply id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1.2">ğ‘¥</ci><cn id="S3.SS2.SSS2.p3.5.m5.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2.2">ğ‘¥</ci><cn id="S3.SS2.SSS2.p3.5.m5.3.3.2.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1">â€¦</ci><apply id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.2">ğ‘¥</ci><apply id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3"><minus id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.1"></minus><ci id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.2">ğ‘–</ci><cn id="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.5.m5.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m5.4c">x_{1},x_{2},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.5.m5.4d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. This approach enables the model to generate coherent and contextually relevant sequences, making it particularly useful for tasks like text generation. However, the unidirectional nature of this model means it cannot consider future tokens during training (when computing scores for each token), which may limit its performance in tasks that require understanding both preceding and following contexts.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>The proposed method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we introduce Sequential Masked Modelling (<span class="ltx_glossaryref" title="">SMM</span>), a new approach designed to improve next-click prediction in session-based recommender systems. This method employs a novel masking technique during training to enhance performance.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The <span class="ltx_glossaryref" title="">SMM</span> approach consists of two key principles: data augmentation with window sliding, and masking the penultimate token of the augmented sequences. This masking approach is specifically applied to encoder-only transformer models, as it leverages their bidirectional attention mechanism.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Data Augmentation with Window Sliding</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In transformer models, the input sequence length is constrained by a parameter known as <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">max_len</span>, which controls the maximum number of tokens the model can process in a single pass. Due to computational limits and the statistical properties of the input data, sequences that exceed this length must be truncated or split into smaller chunks. In many cases, shrinking the window to fit within this constraint is acceptable without compromising the modelâ€™s ability to learn effectively.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To address this, we implement a sliding window technique to divide sequences into fixed-size chunks based on the <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">max_len</span> parameter. This ensures that the model can handle variable-length sequences while adhering to computational limits. Each session is represented as a sequence of items with a minimum length of two<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A minimum length of 2 is necessary because, during training, at least one item must remain unmasked for prediction.</span></span></span>. For sequences longer than two items, subsequences of length <math alttext="n-1" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">n</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><minus id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></minus><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ‘›</ci><cn id="S4.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">n-1</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_n - 1</annotation></semantics></math> are generated by progressively removing the last item at each step, where <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">n</span> is the total number of items in the sequence.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The sliding window always starts at the end of the sequence and moves leftward, generating subsequences of length <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">max_len</span>. This allows all relevant parts of the sequence to be included in training and evaluation, regardless of the original sequence length. If the sequence length is shorter than <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">max_len</span>, the entire sequence is used.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Given an initial sequence <math alttext="s=(x_{1},x_{2},\ldots,x_{n})" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.4"><semantics id="S4.SS1.p4.1.m1.4a"><mrow id="S4.SS1.p4.1.m1.4.4" xref="S4.SS1.p4.1.m1.4.4.cmml"><mi id="S4.SS1.p4.1.m1.4.4.5" xref="S4.SS1.p4.1.m1.4.4.5.cmml">s</mi><mo id="S4.SS1.p4.1.m1.4.4.4" xref="S4.SS1.p4.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS1.p4.1.m1.4.4.3.3" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml"><mo id="S4.SS1.p4.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml">(</mo><msub id="S4.SS1.p4.1.m1.2.2.1.1.1" xref="S4.SS1.p4.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.2.2.1.1.1.2" xref="S4.SS1.p4.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS1.p4.1.m1.2.2.1.1.1.3" xref="S4.SS1.p4.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p4.1.m1.4.4.3.3.5" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p4.1.m1.3.3.2.2.2" xref="S4.SS1.p4.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS1.p4.1.m1.3.3.2.2.2.2" xref="S4.SS1.p4.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S4.SS1.p4.1.m1.3.3.2.2.2.3" xref="S4.SS1.p4.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p4.1.m1.4.4.3.3.6" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS1.p4.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p4.1.m1.1.1.cmml">â€¦</mi><mo id="S4.SS1.p4.1.m1.4.4.3.3.7" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p4.1.m1.4.4.3.3.3" xref="S4.SS1.p4.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS1.p4.1.m1.4.4.3.3.3.2" xref="S4.SS1.p4.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S4.SS1.p4.1.m1.4.4.3.3.3.3" xref="S4.SS1.p4.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS1.p4.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS1.p4.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.4b"><apply id="S4.SS1.p4.1.m1.4.4.cmml" xref="S4.SS1.p4.1.m1.4.4"><eq id="S4.SS1.p4.1.m1.4.4.4.cmml" xref="S4.SS1.p4.1.m1.4.4.4"></eq><ci id="S4.SS1.p4.1.m1.4.4.5.cmml" xref="S4.SS1.p4.1.m1.4.4.5">ğ‘ </ci><vector id="S4.SS1.p4.1.m1.4.4.3.4.cmml" xref="S4.SS1.p4.1.m1.4.4.3.3"><apply id="S4.SS1.p4.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p4.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn id="S4.SS1.p4.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.p4.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p4.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.p4.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS1.p4.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p4.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS1.p4.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn id="S4.SS1.p4.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS1.p4.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">â€¦</ci><apply id="S4.SS1.p4.1.m1.4.4.3.3.3.cmml" xref="S4.SS1.p4.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS1.p4.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p4.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS1.p4.1.m1.4.4.3.3.3.2">ğ‘¥</ci><ci id="S4.SS1.p4.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS1.p4.1.m1.4.4.3.3.3.3">ğ‘›</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.4c">s=(x_{1},x_{2},\ldots,x_{n})</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.4d">italic_s = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, the sliding window moves along the sequence to generate subsequences as follows:</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Step 1</span>: The first subsequence is extracted from the end of the initial sequence. If the sequence length is shorter than <span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.2">max_len</span>, the subsequence contains all elements. For example, <math alttext="s_{1}=(x_{n-\textit{max\_len}+1},\ldots,x_{n})" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.m1.3"><semantics id="S4.I1.i1.p1.1.m1.3a"><mrow id="S4.I1.i1.p1.1.m1.3.3" xref="S4.I1.i1.p1.1.m1.3.3.cmml"><msub id="S4.I1.i1.p1.1.m1.3.3.4" xref="S4.I1.i1.p1.1.m1.3.3.4.cmml"><mi id="S4.I1.i1.p1.1.m1.3.3.4.2" xref="S4.I1.i1.p1.1.m1.3.3.4.2.cmml">s</mi><mn id="S4.I1.i1.p1.1.m1.3.3.4.3" xref="S4.I1.i1.p1.1.m1.3.3.4.3.cmml">1</mn></msub><mo id="S4.I1.i1.p1.1.m1.3.3.3" xref="S4.I1.i1.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S4.I1.i1.p1.1.m1.3.3.2.2" xref="S4.I1.i1.p1.1.m1.3.3.2.3.cmml"><mo id="S4.I1.i1.p1.1.m1.3.3.2.2.3" stretchy="false" xref="S4.I1.i1.p1.1.m1.3.3.2.3.cmml">(</mo><msub id="S4.I1.i1.p1.1.m1.2.2.1.1.1" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.2.2.1.1.1.2" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mrow id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml"><mrow id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.cmml"><mi id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.2" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.2.cmml">n</mi><mo id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.1" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3a.cmml">max_len</mtext></mrow><mo id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.1" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.1.cmml">+</mo><mn id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.3" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S4.I1.i1.p1.1.m1.3.3.2.2.4" xref="S4.I1.i1.p1.1.m1.3.3.2.3.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.i1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S4.I1.i1.p1.1.m1.3.3.2.2.5" xref="S4.I1.i1.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S4.I1.i1.p1.1.m1.3.3.2.2.2" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.I1.i1.p1.1.m1.3.3.2.2.2.2" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mi id="S4.I1.i1.p1.1.m1.3.3.2.2.2.3" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml">n</mi></msub><mo id="S4.I1.i1.p1.1.m1.3.3.2.2.6" stretchy="false" xref="S4.I1.i1.p1.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.3b"><apply id="S4.I1.i1.p1.1.m1.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3"><eq id="S4.I1.i1.p1.1.m1.3.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3.3"></eq><apply id="S4.I1.i1.p1.1.m1.3.3.4.cmml" xref="S4.I1.i1.p1.1.m1.3.3.4"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.3.3.4.1.cmml" xref="S4.I1.i1.p1.1.m1.3.3.4">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.3.3.4.2.cmml" xref="S4.I1.i1.p1.1.m1.3.3.4.2">ğ‘ </ci><cn id="S4.I1.i1.p1.1.m1.3.3.4.3.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.3.3.4.3">1</cn></apply><vector id="S4.I1.i1.p1.1.m1.3.3.2.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2"><apply id="S4.I1.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><apply id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3"><plus id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.1"></plus><apply id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2"><minus id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.1.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.1"></minus><ci id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.2.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.2">ğ‘›</ci><ci id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3a.cmml" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3"><mtext class="ltx_mathvariant_italic" id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3.cmml" mathsize="70%" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.2.3">max_len</mtext></ci></apply><cn id="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.3.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.2.2.1.1.1.3.3">1</cn></apply></apply><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">â€¦</ci><apply id="S4.I1.i1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.I1.i1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.I1.i1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><ci id="S4.I1.i1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3.2.2.2.3">ğ‘›</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.3c">s_{1}=(x_{n-\textit{max\_len}+1},\ldots,x_{n})</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.m1.3d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - max_len + 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Step 2</span>: The window then shifts one step to the left to create the next subsequence. For example, <math alttext="s_{2}=(x_{n-\textit{max\_len}},\ldots,x_{n-1})" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.3"><semantics id="S4.I1.i2.p1.1.m1.3a"><mrow id="S4.I1.i2.p1.1.m1.3.3" xref="S4.I1.i2.p1.1.m1.3.3.cmml"><msub id="S4.I1.i2.p1.1.m1.3.3.4" xref="S4.I1.i2.p1.1.m1.3.3.4.cmml"><mi id="S4.I1.i2.p1.1.m1.3.3.4.2" xref="S4.I1.i2.p1.1.m1.3.3.4.2.cmml">s</mi><mn id="S4.I1.i2.p1.1.m1.3.3.4.3" xref="S4.I1.i2.p1.1.m1.3.3.4.3.cmml">2</mn></msub><mo id="S4.I1.i2.p1.1.m1.3.3.3" xref="S4.I1.i2.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S4.I1.i2.p1.1.m1.3.3.2.2" xref="S4.I1.i2.p1.1.m1.3.3.2.3.cmml"><mo id="S4.I1.i2.p1.1.m1.3.3.2.2.3" stretchy="false" xref="S4.I1.i2.p1.1.m1.3.3.2.3.cmml">(</mo><msub id="S4.I1.i2.p1.1.m1.2.2.1.1.1" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.I1.i2.p1.1.m1.2.2.1.1.1.2" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mrow id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.cmml"><mi id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.2" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.2.cmml">n</mi><mo id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.1" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3a.cmml">max_len</mtext></mrow></msub><mo id="S4.I1.i2.p1.1.m1.3.3.2.2.4" xref="S4.I1.i2.p1.1.m1.3.3.2.3.cmml">,</mo><mi id="S4.I1.i2.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.i2.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S4.I1.i2.p1.1.m1.3.3.2.2.5" xref="S4.I1.i2.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S4.I1.i2.p1.1.m1.3.3.2.2.2" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.I1.i2.p1.1.m1.3.3.2.2.2.2" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mrow id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.cmml"><mi id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.2" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.2.cmml">n</mi><mo id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.1" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.1.cmml">âˆ’</mo><mn id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.3" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S4.I1.i2.p1.1.m1.3.3.2.2.6" stretchy="false" xref="S4.I1.i2.p1.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.3b"><apply id="S4.I1.i2.p1.1.m1.3.3.cmml" xref="S4.I1.i2.p1.1.m1.3.3"><eq id="S4.I1.i2.p1.1.m1.3.3.3.cmml" xref="S4.I1.i2.p1.1.m1.3.3.3"></eq><apply id="S4.I1.i2.p1.1.m1.3.3.4.cmml" xref="S4.I1.i2.p1.1.m1.3.3.4"><csymbol cd="ambiguous" id="S4.I1.i2.p1.1.m1.3.3.4.1.cmml" xref="S4.I1.i2.p1.1.m1.3.3.4">subscript</csymbol><ci id="S4.I1.i2.p1.1.m1.3.3.4.2.cmml" xref="S4.I1.i2.p1.1.m1.3.3.4.2">ğ‘ </ci><cn id="S4.I1.i2.p1.1.m1.3.3.4.3.cmml" type="integer" xref="S4.I1.i2.p1.1.m1.3.3.4.3">2</cn></apply><vector id="S4.I1.i2.p1.1.m1.3.3.2.3.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2"><apply id="S4.I1.i2.p1.1.m1.2.2.1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.I1.i2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><apply id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3"><minus id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.1"></minus><ci id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.2">ğ‘›</ci><ci id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3a.cmml" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3.cmml" mathsize="70%" xref="S4.I1.i2.p1.1.m1.2.2.1.1.1.3.3">max_len</mtext></ci></apply></apply><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">â€¦</ci><apply id="S4.I1.i2.p1.1.m1.3.3.2.2.2.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.I1.i2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.I1.i2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><apply id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3"><minus id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.1.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.1"></minus><ci id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.2.cmml" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.2">ğ‘›</ci><cn id="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.3.cmml" type="integer" xref="S4.I1.i2.p1.1.m1.3.3.2.2.2.3.3">1</cn></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.3c">s_{2}=(x_{n-\textit{max\_len}},\ldots,x_{n-1})</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.3d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - max_len end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Step 3</span>: This process continues, with the window sliding leftward until the window reaches the beginning of the initial sequence. For example, <math alttext="s_{3}=(x_{n-\textit{max\_len}-1},\ldots,x_{n-2})" class="ltx_Math" display="inline" id="S4.I1.i3.p1.1.m1.3"><semantics id="S4.I1.i3.p1.1.m1.3a"><mrow id="S4.I1.i3.p1.1.m1.3.3" xref="S4.I1.i3.p1.1.m1.3.3.cmml"><msub id="S4.I1.i3.p1.1.m1.3.3.4" xref="S4.I1.i3.p1.1.m1.3.3.4.cmml"><mi id="S4.I1.i3.p1.1.m1.3.3.4.2" xref="S4.I1.i3.p1.1.m1.3.3.4.2.cmml">s</mi><mn id="S4.I1.i3.p1.1.m1.3.3.4.3" xref="S4.I1.i3.p1.1.m1.3.3.4.3.cmml">3</mn></msub><mo id="S4.I1.i3.p1.1.m1.3.3.3" xref="S4.I1.i3.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S4.I1.i3.p1.1.m1.3.3.2.2" xref="S4.I1.i3.p1.1.m1.3.3.2.3.cmml"><mo id="S4.I1.i3.p1.1.m1.3.3.2.2.3" stretchy="false" xref="S4.I1.i3.p1.1.m1.3.3.2.3.cmml">(</mo><msub id="S4.I1.i3.p1.1.m1.2.2.1.1.1" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.I1.i3.p1.1.m1.2.2.1.1.1.2" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mrow id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml"><mi id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.2" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.2.cmml">n</mi><mo id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3a.cmml">max_len</mtext><mo id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1a" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1.cmml">âˆ’</mo><mn id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.4" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.4.cmml">1</mn></mrow></msub><mo id="S4.I1.i3.p1.1.m1.3.3.2.2.4" xref="S4.I1.i3.p1.1.m1.3.3.2.3.cmml">,</mo><mi id="S4.I1.i3.p1.1.m1.1.1" mathvariant="normal" xref="S4.I1.i3.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S4.I1.i3.p1.1.m1.3.3.2.2.5" xref="S4.I1.i3.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S4.I1.i3.p1.1.m1.3.3.2.2.2" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.I1.i3.p1.1.m1.3.3.2.2.2.2" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mrow id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.cmml"><mi id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.2" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.2.cmml">n</mi><mo id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.1" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.1.cmml">âˆ’</mo><mn id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.3" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S4.I1.i3.p1.1.m1.3.3.2.2.6" stretchy="false" xref="S4.I1.i3.p1.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.3b"><apply id="S4.I1.i3.p1.1.m1.3.3.cmml" xref="S4.I1.i3.p1.1.m1.3.3"><eq id="S4.I1.i3.p1.1.m1.3.3.3.cmml" xref="S4.I1.i3.p1.1.m1.3.3.3"></eq><apply id="S4.I1.i3.p1.1.m1.3.3.4.cmml" xref="S4.I1.i3.p1.1.m1.3.3.4"><csymbol cd="ambiguous" id="S4.I1.i3.p1.1.m1.3.3.4.1.cmml" xref="S4.I1.i3.p1.1.m1.3.3.4">subscript</csymbol><ci id="S4.I1.i3.p1.1.m1.3.3.4.2.cmml" xref="S4.I1.i3.p1.1.m1.3.3.4.2">ğ‘ </ci><cn id="S4.I1.i3.p1.1.m1.3.3.4.3.cmml" type="integer" xref="S4.I1.i3.p1.1.m1.3.3.4.3">3</cn></apply><vector id="S4.I1.i3.p1.1.m1.3.3.2.3.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2"><apply id="S4.I1.i3.p1.1.m1.2.2.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.I1.i3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><apply id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3"><minus id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.1"></minus><ci id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.2">ğ‘›</ci><ci id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3a.cmml" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3"><mtext class="ltx_mathvariant_italic" id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3.cmml" mathsize="70%" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.3">max_len</mtext></ci><cn id="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.4.cmml" type="integer" xref="S4.I1.i3.p1.1.m1.2.2.1.1.1.3.4">1</cn></apply></apply><ci id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">â€¦</ci><apply id="S4.I1.i3.p1.1.m1.3.3.2.2.2.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.I1.i3.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.I1.i3.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><apply id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3"><minus id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.1.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.1"></minus><ci id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.2.cmml" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.2">ğ‘›</ci><cn id="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.3.cmml" type="integer" xref="S4.I1.i3.p1.1.m1.3.3.2.2.2.3.3">2</cn></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.3c">s_{3}=(x_{n-\textit{max\_len}-1},\ldots,x_{n-2})</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.1.m1.3d">italic_s start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - max_len - 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n - 2 end_POSTSUBSCRIPT )</annotation></semantics></math>, and so on.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.2">Through this method, each token in the initial sequence <math alttext="s" class="ltx_Math" display="inline" id="S4.SS1.p6.1.m1.1"><semantics id="S4.SS1.p6.1.m1.1a"><mi id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><ci id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.1.m1.1d">italic_s</annotation></semantics></math> appears in multiple subsequences, allowing the model to encounter each token in varied training contexts. The general form of each subsequence <math alttext="s_{i}" class="ltx_Math" display="inline" id="S4.SS1.p6.2.m2.1"><semantics id="S4.SS1.p6.2.m2.1a"><msub id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml"><mi id="S4.SS1.p6.2.m2.1.1.2" xref="S4.SS1.p6.2.m2.1.1.2.cmml">s</mi><mi id="S4.SS1.p6.2.m2.1.1.3" xref="S4.SS1.p6.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><apply id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p6.2.m2.1.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p6.2.m2.1.1.2.cmml" xref="S4.SS1.p6.2.m2.1.1.2">ğ‘ </ci><ci id="S4.SS1.p6.2.m2.1.1.3.cmml" xref="S4.SS1.p6.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p6.2.m2.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can be defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s_{i}=(x_{n-\textit{max\_len}-i+2},\ldots,x_{n-i+1})\quad\text{for}\quad i=0,1%
,2,\ldots,n-\textit{max\_len}" class="ltx_Math" display="block" id="S4.Ex3.m1.8"><semantics id="S4.Ex3.m1.8a"><mrow id="S4.Ex3.m1.8.8.2" xref="S4.Ex3.m1.8.8.3.cmml"><mrow id="S4.Ex3.m1.7.7.1.1" xref="S4.Ex3.m1.7.7.1.1.cmml"><msub id="S4.Ex3.m1.7.7.1.1.3" xref="S4.Ex3.m1.7.7.1.1.3.cmml"><mi id="S4.Ex3.m1.7.7.1.1.3.2" xref="S4.Ex3.m1.7.7.1.1.3.2.cmml">s</mi><mi id="S4.Ex3.m1.7.7.1.1.3.3" xref="S4.Ex3.m1.7.7.1.1.3.3.cmml">i</mi></msub><mo id="S4.Ex3.m1.7.7.1.1.2" xref="S4.Ex3.m1.7.7.1.1.2.cmml">=</mo><mrow id="S4.Ex3.m1.7.7.1.1.1.1" xref="S4.Ex3.m1.7.7.1.1.1.2.cmml"><mrow id="S4.Ex3.m1.7.7.1.1.1.1.1.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml"><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.3" stretchy="false" xref="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml">(</mo><msub id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.cmml"><mrow id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.cmml"><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.2.cmml">n</mi><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3a.cmml">max_len</mtext><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1a" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1.cmml">âˆ’</mo><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.4" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.4.cmml">i</mi></mrow><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.1" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.3.cmml">2</mn></mrow></msub><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.4" xref="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml">,</mo><mi id="S4.Ex3.m1.1.1" mathvariant="normal" xref="S4.Ex3.m1.1.1.cmml">â€¦</mi><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.5" xref="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml">,</mo><msub id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.cmml"><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.2.cmml">x</mi><mrow id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.cmml"><mrow id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.cmml"><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.2" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.2.cmml">n</mi><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.1" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.1.cmml">âˆ’</mo><mi id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.3.cmml">i</mi></mrow><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.1" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.3" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S4.Ex3.m1.7.7.1.1.1.1.1.2.6" stretchy="false" xref="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml">)</mo></mrow><mspace id="S4.Ex3.m1.7.7.1.1.1.1.2" width="1em" xref="S4.Ex3.m1.7.7.1.1.1.2.cmml"></mspace><mtext id="S4.Ex3.m1.6.6" xref="S4.Ex3.m1.6.6a.cmml">for</mtext></mrow></mrow><mspace id="S4.Ex3.m1.8.8.2.3" width="1em" xref="S4.Ex3.m1.8.8.3a.cmml"></mspace><mrow id="S4.Ex3.m1.8.8.2.2" xref="S4.Ex3.m1.8.8.2.2.cmml"><mi id="S4.Ex3.m1.8.8.2.2.3" xref="S4.Ex3.m1.8.8.2.2.3.cmml">i</mi><mo id="S4.Ex3.m1.8.8.2.2.2" xref="S4.Ex3.m1.8.8.2.2.2.cmml">=</mo><mrow id="S4.Ex3.m1.8.8.2.2.1.1" xref="S4.Ex3.m1.8.8.2.2.1.2.cmml"><mn id="S4.Ex3.m1.2.2" xref="S4.Ex3.m1.2.2.cmml">0</mn><mo id="S4.Ex3.m1.8.8.2.2.1.1.2" xref="S4.Ex3.m1.8.8.2.2.1.2.cmml">,</mo><mn id="S4.Ex3.m1.3.3" xref="S4.Ex3.m1.3.3.cmml">1</mn><mo id="S4.Ex3.m1.8.8.2.2.1.1.3" xref="S4.Ex3.m1.8.8.2.2.1.2.cmml">,</mo><mn id="S4.Ex3.m1.4.4" xref="S4.Ex3.m1.4.4.cmml">2</mn><mo id="S4.Ex3.m1.8.8.2.2.1.1.4" xref="S4.Ex3.m1.8.8.2.2.1.2.cmml">,</mo><mi id="S4.Ex3.m1.5.5" mathvariant="normal" xref="S4.Ex3.m1.5.5.cmml">â€¦</mi><mo id="S4.Ex3.m1.8.8.2.2.1.1.5" xref="S4.Ex3.m1.8.8.2.2.1.2.cmml">,</mo><mrow id="S4.Ex3.m1.8.8.2.2.1.1.1" xref="S4.Ex3.m1.8.8.2.2.1.1.1.cmml"><mi id="S4.Ex3.m1.8.8.2.2.1.1.1.2" xref="S4.Ex3.m1.8.8.2.2.1.1.1.2.cmml">n</mi><mo id="S4.Ex3.m1.8.8.2.2.1.1.1.1" xref="S4.Ex3.m1.8.8.2.2.1.1.1.1.cmml">âˆ’</mo><mtext class="ltx_mathvariant_italic" id="S4.Ex3.m1.8.8.2.2.1.1.1.3" xref="S4.Ex3.m1.8.8.2.2.1.1.1.3a.cmml">max_len</mtext></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m1.8b"><apply id="S4.Ex3.m1.8.8.3.cmml" xref="S4.Ex3.m1.8.8.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.8.8.3a.cmml" xref="S4.Ex3.m1.8.8.2.3">formulae-sequence</csymbol><apply id="S4.Ex3.m1.7.7.1.1.cmml" xref="S4.Ex3.m1.7.7.1.1"><eq id="S4.Ex3.m1.7.7.1.1.2.cmml" xref="S4.Ex3.m1.7.7.1.1.2"></eq><apply id="S4.Ex3.m1.7.7.1.1.3.cmml" xref="S4.Ex3.m1.7.7.1.1.3"><csymbol cd="ambiguous" id="S4.Ex3.m1.7.7.1.1.3.1.cmml" xref="S4.Ex3.m1.7.7.1.1.3">subscript</csymbol><ci id="S4.Ex3.m1.7.7.1.1.3.2.cmml" xref="S4.Ex3.m1.7.7.1.1.3.2">ğ‘ </ci><ci id="S4.Ex3.m1.7.7.1.1.3.3.cmml" xref="S4.Ex3.m1.7.7.1.1.3.3">ğ‘–</ci></apply><list id="S4.Ex3.m1.7.7.1.1.1.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1"><vector id="S4.Ex3.m1.7.7.1.1.1.1.1.3.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2"><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.2">ğ‘¥</ci><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3"><plus id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.1"></plus><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2"><minus id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.1"></minus><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.2">ğ‘›</ci><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3"><mtext class="ltx_mathvariant_italic" id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.3">max_len</mtext></ci><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.4.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.2.4">ğ‘–</ci></apply><cn id="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.Ex3.m1.7.7.1.1.1.1.1.1.1.3.3">2</cn></apply></apply><ci id="S4.Ex3.m1.1.1.cmml" xref="S4.Ex3.m1.1.1">â€¦</ci><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.2">ğ‘¥</ci><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3"><plus id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.1"></plus><apply id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2"><minus id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.1.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.1"></minus><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.2.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.2">ğ‘›</ci><ci id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.3.cmml" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.2.3">ğ‘–</ci></apply><cn id="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.Ex3.m1.7.7.1.1.1.1.1.2.2.3.3">1</cn></apply></apply></vector><ci id="S4.Ex3.m1.6.6a.cmml" xref="S4.Ex3.m1.6.6"><mtext id="S4.Ex3.m1.6.6.cmml" xref="S4.Ex3.m1.6.6">for</mtext></ci></list></apply><apply id="S4.Ex3.m1.8.8.2.2.cmml" xref="S4.Ex3.m1.8.8.2.2"><eq id="S4.Ex3.m1.8.8.2.2.2.cmml" xref="S4.Ex3.m1.8.8.2.2.2"></eq><ci id="S4.Ex3.m1.8.8.2.2.3.cmml" xref="S4.Ex3.m1.8.8.2.2.3">ğ‘–</ci><list id="S4.Ex3.m1.8.8.2.2.1.2.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1"><cn id="S4.Ex3.m1.2.2.cmml" type="integer" xref="S4.Ex3.m1.2.2">0</cn><cn id="S4.Ex3.m1.3.3.cmml" type="integer" xref="S4.Ex3.m1.3.3">1</cn><cn id="S4.Ex3.m1.4.4.cmml" type="integer" xref="S4.Ex3.m1.4.4">2</cn><ci id="S4.Ex3.m1.5.5.cmml" xref="S4.Ex3.m1.5.5">â€¦</ci><apply id="S4.Ex3.m1.8.8.2.2.1.1.1.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1.1"><minus id="S4.Ex3.m1.8.8.2.2.1.1.1.1.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1.1.1"></minus><ci id="S4.Ex3.m1.8.8.2.2.1.1.1.2.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1.1.2">ğ‘›</ci><ci id="S4.Ex3.m1.8.8.2.2.1.1.1.3a.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.Ex3.m1.8.8.2.2.1.1.1.3.cmml" xref="S4.Ex3.m1.8.8.2.2.1.1.1.3">max_len</mtext></ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m1.8c">s_{i}=(x_{n-\textit{max\_len}-i+2},\ldots,x_{n-i+1})\quad\text{for}\quad i=0,1%
,2,\ldots,n-\textit{max\_len}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex3.m1.8d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - max_len - italic_i + 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n - italic_i + 1 end_POSTSUBSCRIPT ) for italic_i = 0 , 1 , 2 , â€¦ , italic_n - max_len</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Masking the Penultimate Token</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In traditional sequence-based tasks, the last item in a sequence is masked, and the sequence, truncated to the modelâ€™s maximum input length (<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">max_len</span>), is used for prediction. In our approach, we retain this strategy for the base sequence, where we input the original sequence truncated to <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">max_len</span> and mask the last item as usual.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">However, with the introduction of data augmentation through sliding windows, we adopt a different masking strategy for the augmented sequences. Instead of masking the last item, as done in the base sequence, we mask the penultimate token in each augmented subsequence. This method enables the model to learn from both the preceding context and minimal future information, resulting in more varied and informative training examples. Specifically, for each input sequence <math alttext="s=(x_{1},x_{2},\ldots,x_{n})" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.4"><semantics id="S4.SS2.p2.1.m1.4a"><mrow id="S4.SS2.p2.1.m1.4.4" xref="S4.SS2.p2.1.m1.4.4.cmml"><mi id="S4.SS2.p2.1.m1.4.4.5" xref="S4.SS2.p2.1.m1.4.4.5.cmml">s</mi><mo id="S4.SS2.p2.1.m1.4.4.4" xref="S4.SS2.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.4.4.3.3" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml"><mo id="S4.SS2.p2.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">(</mo><msub id="S4.SS2.p2.1.m1.2.2.1.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.2" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S4.SS2.p2.1.m1.2.2.1.1.1.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.5" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.3.3.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S4.SS2.p2.1.m1.3.3.2.2.2.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.6" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS2.p2.1.m1.1.1.cmml">â€¦</mi><mo id="S4.SS2.p2.1.m1.4.4.3.3.7" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.4.4.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.2" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.4b"><apply id="S4.SS2.p2.1.m1.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4"><eq id="S4.SS2.p2.1.m1.4.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4.4"></eq><ci id="S4.SS2.p2.1.m1.4.4.5.cmml" xref="S4.SS2.p2.1.m1.4.4.5">ğ‘ </ci><vector id="S4.SS2.p2.1.m1.4.4.3.4.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3"><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn id="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn id="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">â€¦</ci><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2">ğ‘¥</ci><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3">ğ‘›</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.4c">s=(x_{1},x_{2},\ldots,x_{n})</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.4d">italic_s = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, the sliding window generates subsequences of length <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.2.1">max_len</span>, where the penultimate token <math alttext="x_{i-1}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><msub id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">x</mi><mrow id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p2.2.m2.1.1.3.1" xref="S4.SS2.p2.2.m2.1.1.3.1.cmml">âˆ’</mo><mn id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">ğ‘¥</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><minus id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3.1"></minus><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS2.p2.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> is masked. The loss is then computed based on the modelâ€™s ability to predict the masked penultimate token using the surrounding context.</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{\lx@glossaries@gls@link{acronym}{smm}{\leavevmode SMM}}}=-%
\log P(x_{i-1}|x_{1},x_{2},\ldots,x_{i-2},x_{i})" class="ltx_Math" display="block" id="S4.Ex4.m1.2"><semantics id="S4.Ex4.m1.2a"><mrow id="S4.Ex4.m1.2.2" xref="S4.Ex4.m1.2.2.cmml"><msub id="S4.Ex4.m1.2.2.3" xref="S4.Ex4.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex4.m1.2.2.3.2" xref="S4.Ex4.m1.2.2.3.2.cmml">â„’</mi><mtext id="S4.Ex4.m1.2.2.3.3" xref="S4.Ex4.m1.2.2.3.3b.cmml"><span class="ltx_glossaryref" style="font-size:70%;" title="">SMM</span></mtext></msub><mo id="S4.Ex4.m1.2.2.2" xref="S4.Ex4.m1.2.2.2.cmml">=</mo><mrow id="S4.Ex4.m1.2.2.1" xref="S4.Ex4.m1.2.2.1.cmml"><mo id="S4.Ex4.m1.2.2.1a" rspace="0.167em" xref="S4.Ex4.m1.2.2.1.cmml">âˆ’</mo><mrow id="S4.Ex4.m1.2.2.1.1" xref="S4.Ex4.m1.2.2.1.1.cmml"><mrow id="S4.Ex4.m1.2.2.1.1.3" xref="S4.Ex4.m1.2.2.1.1.3.cmml"><mi id="S4.Ex4.m1.2.2.1.1.3.1" xref="S4.Ex4.m1.2.2.1.1.3.1.cmml">log</mi><mo id="S4.Ex4.m1.2.2.1.1.3a" lspace="0.167em" xref="S4.Ex4.m1.2.2.1.1.3.cmml">â¡</mo><mi id="S4.Ex4.m1.2.2.1.1.3.2" xref="S4.Ex4.m1.2.2.1.1.3.2.cmml">P</mi></mrow><mo id="S4.Ex4.m1.2.2.1.1.2" xref="S4.Ex4.m1.2.2.1.1.2.cmml">â¢</mo><mrow id="S4.Ex4.m1.2.2.1.1.1.1" xref="S4.Ex4.m1.2.2.1.1.1.1.1.cmml"><mo id="S4.Ex4.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.Ex4.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S4.Ex4.m1.2.2.1.1.1.1.1" xref="S4.Ex4.m1.2.2.1.1.1.1.1.cmml"><msub id="S4.Ex4.m1.2.2.1.1.1.1.1.6" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.6.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.2.cmml">x</mi><mrow id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.2.cmml">i</mi><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.1" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.1.cmml">âˆ’</mo><mn id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S4.Ex4.m1.2.2.1.1.1.1.1.5" xref="S4.Ex4.m1.2.2.1.1.1.1.1.5.cmml">|</mo><mrow id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml"><msub id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.5" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.6" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml">,</mo><mi id="S4.Ex4.m1.1.1" mathvariant="normal" xref="S4.Ex4.m1.1.1.cmml">â€¦</mi><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.7" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.1" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.3.cmml">2</mn></mrow></msub><mo id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.8" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.cmml"><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.2" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.2.cmml">x</mi><mi id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.3" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.3.cmml">i</mi></msub></mrow></mrow><mo id="S4.Ex4.m1.2.2.1.1.1.1.3" stretchy="false" xref="S4.Ex4.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex4.m1.2b"><apply id="S4.Ex4.m1.2.2.cmml" xref="S4.Ex4.m1.2.2"><eq id="S4.Ex4.m1.2.2.2.cmml" xref="S4.Ex4.m1.2.2.2"></eq><apply id="S4.Ex4.m1.2.2.3.cmml" xref="S4.Ex4.m1.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.3.1.cmml" xref="S4.Ex4.m1.2.2.3">subscript</csymbol><ci id="S4.Ex4.m1.2.2.3.2.cmml" xref="S4.Ex4.m1.2.2.3.2">â„’</ci><ci id="S4.Ex4.m1.2.2.3.3b.cmml" xref="S4.Ex4.m1.2.2.3.3"><mtext id="S4.Ex4.m1.2.2.3.3.cmml" mathsize="70%" xref="S4.Ex4.m1.2.2.3.3"><span class="ltx_glossaryref" style="font-size:70%;" title="">SMM</span></mtext></ci></apply><apply id="S4.Ex4.m1.2.2.1.cmml" xref="S4.Ex4.m1.2.2.1"><minus id="S4.Ex4.m1.2.2.1.2.cmml" xref="S4.Ex4.m1.2.2.1"></minus><apply id="S4.Ex4.m1.2.2.1.1.cmml" xref="S4.Ex4.m1.2.2.1.1"><times id="S4.Ex4.m1.2.2.1.1.2.cmml" xref="S4.Ex4.m1.2.2.1.1.2"></times><apply id="S4.Ex4.m1.2.2.1.1.3.cmml" xref="S4.Ex4.m1.2.2.1.1.3"><log id="S4.Ex4.m1.2.2.1.1.3.1.cmml" xref="S4.Ex4.m1.2.2.1.1.3.1"></log><ci id="S4.Ex4.m1.2.2.1.1.3.2.cmml" xref="S4.Ex4.m1.2.2.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S4.Ex4.m1.2.2.1.1.1.1.1.5.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.5">conditional</csymbol><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.6.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.1.1.1.1.1.6.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6">subscript</csymbol><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.6.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.2">ğ‘¥</ci><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3"><minus id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.1"></minus><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.2">ğ‘–</ci><cn id="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.3.cmml" type="integer" xref="S4.Ex4.m1.2.2.1.1.1.1.1.6.3.3">1</cn></apply></apply><list id="S4.Ex4.m1.2.2.1.1.1.1.1.4.5.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4"><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn id="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.Ex4.m1.2.2.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><cn id="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S4.Ex4.m1.2.2.1.1.1.1.1.2.2.2.3">2</cn></apply><ci id="S4.Ex4.m1.1.1.cmml" xref="S4.Ex4.m1.1.1">â€¦</ci><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.2">ğ‘¥</ci><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3"><minus id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn id="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S4.Ex4.m1.2.2.1.1.1.1.1.3.3.3.3.3">2</cn></apply></apply><apply id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4"><csymbol cd="ambiguous" id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.1.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4">subscript</csymbol><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.2.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.2">ğ‘¥</ci><ci id="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.3.cmml" xref="S4.Ex4.m1.2.2.1.1.1.1.1.4.4.4.3">ğ‘–</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex4.m1.2c">\mathcal{L}_{\text{\lx@glossaries@gls@link{acronym}{smm}{\leavevmode SMM}}}=-%
\log P(x_{i-1}|x_{1},x_{2},\ldots,x_{i-2},x_{i})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex4.m1.2d">caligraphic_L start_POSTSUBSCRIPT end_POSTSUBSCRIPT = - roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">This masking strategy takes advantage of the bidirectional attention in encoder-only transformer models, such as BERT, allowing the model to leverage both the preceding context and the limited right-hand context (provided by the next element in the sequence). Although real-world next-click prediction tasks lack future context, masking the penultimate token proves useful in scenarios where item order is flexible, such as in e-commerce, where item proximity is often more important than strict order.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">By combining data augmentation with this penultimate token masking strategy, we ensure that each token in the sequence is masked at least once during training. This complements the base sequence where the last item is masked, allowing the model to adapt to both the original and augmented contexts. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.F2" title="Figure 2 â€£ 4.2. Masking the Penultimate Token â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the <span class="ltx_glossaryref" title="">SMM</span> training approach using a single base sequence. In this example, the base sequence is augmented into multiple subsequences, and the goal is to predict the masked token <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p4.1.1">[M]</span> in each subsequence. For illustration purposes, the maximum sequence length (<math alttext="max\_len" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">m</mi><mo id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">a</mi><mo id="S4.SS2.p4.1.m1.1.1.1a" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.4" xref="S4.SS2.p4.1.m1.1.1.4.cmml">x</mi><mo id="S4.SS2.p4.1.m1.1.1.1b" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.5" mathvariant="normal" xref="S4.SS2.p4.1.m1.1.1.5.cmml">_</mi><mo id="S4.SS2.p4.1.m1.1.1.1c" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.6" xref="S4.SS2.p4.1.m1.1.1.6.cmml">l</mi><mo id="S4.SS2.p4.1.m1.1.1.1d" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.7" xref="S4.SS2.p4.1.m1.1.1.7.cmml">e</mi><mo id="S4.SS2.p4.1.m1.1.1.1e" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS2.p4.1.m1.1.1.8" xref="S4.SS2.p4.1.m1.1.1.8.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><times id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></times><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">ğ‘š</ci><ci id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">ğ‘</ci><ci id="S4.SS2.p4.1.m1.1.1.4.cmml" xref="S4.SS2.p4.1.m1.1.1.4">ğ‘¥</ci><ci id="S4.SS2.p4.1.m1.1.1.5.cmml" xref="S4.SS2.p4.1.m1.1.1.5">_</ci><ci id="S4.SS2.p4.1.m1.1.1.6.cmml" xref="S4.SS2.p4.1.m1.1.1.6">ğ‘™</ci><ci id="S4.SS2.p4.1.m1.1.1.7.cmml" xref="S4.SS2.p4.1.m1.1.1.7">ğ‘’</ci><ci id="S4.SS2.p4.1.m1.1.1.8.cmml" xref="S4.SS2.p4.1.m1.1.1.8">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">max\_len</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_m italic_a italic_x _ italic_l italic_e italic_n</annotation></semantics></math>) is set to 5.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix1.p1">
<p class="ltx_p" id="S4.I2.ix1.p1.1"><span class="ltx_text" id="S4.I2.ix1.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix1.p1.1.2" style="color:#FF0000;">5, 6, 8, 4, 98, 56</span><span class="ltx_text" id="S4.I2.ix1.p1.1.3" style="color:#0000FF;">, 54, 74, 23, 56, [M]]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix2.p1">
<p class="ltx_p" id="S4.I2.ix2.p1.1"><span class="ltx_text" id="S4.I2.ix2.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix2.p1.1.2" style="color:#FF0000;">5, 6, 8, 4, 98, 56</span><span class="ltx_text" id="S4.I2.ix2.p1.1.3" style="color:#0000FF;">, 54, 74, 23, [M], 57]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix3.p1">
<p class="ltx_p" id="S4.I2.ix3.p1.1"><span class="ltx_text" id="S4.I2.ix3.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix3.p1.1.2" style="color:#FF0000;">5, 6, 8, 4, 98</span><span class="ltx_text" id="S4.I2.ix3.p1.1.3" style="color:#0000FF;">, 56, 54, 74, [M], 56]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix4.p1">
<p class="ltx_p" id="S4.I2.ix4.p1.1"><span class="ltx_text" id="S4.I2.ix4.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix4.p1.1.2" style="color:#FF0000;">5, 6, 8, 4</span><span class="ltx_text" id="S4.I2.ix4.p1.1.3" style="color:#0000FF;">, 98, 56, 54, [M], 23]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix5.p1">
<p class="ltx_p" id="S4.I2.ix5.p1.1"><span class="ltx_text" id="S4.I2.ix5.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix5.p1.1.2" style="color:#FF0000;">5, 6, 8</span><span class="ltx_text" id="S4.I2.ix5.p1.1.3" style="color:#0000FF;">, 4, 98, 56, [M], 74]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix6.p1">
<p class="ltx_p" id="S4.I2.ix6.p1.1"><span class="ltx_text" id="S4.I2.ix6.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix6.p1.1.2" style="color:#FF0000;">5, 6</span><span class="ltx_text" id="S4.I2.ix6.p1.1.3" style="color:#0000FF;">, 8, 4, 98, [M], 54]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix7.p1">
<p class="ltx_p" id="S4.I2.ix7.p1.1"><span class="ltx_text" id="S4.I2.ix7.p1.1.1" style="color:#0000FF;">[</span><span class="ltx_text" id="S4.I2.ix7.p1.1.2" style="color:#FF0000;">5</span><span class="ltx_text" id="S4.I2.ix7.p1.1.3" style="color:#0000FF;">, 6, 8, 4, [M], 56]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix8.p1">
<p class="ltx_p" id="S4.I2.ix8.p1.1"><span class="ltx_text" id="S4.I2.ix8.p1.1.1" style="color:#0000FF;">[5, 6, 8, [M], 98]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix9.p1">
<p class="ltx_p" id="S4.I2.ix9.p1.1"><span class="ltx_text" id="S4.I2.ix9.p1.1.1" style="color:#0000FF;">[5, 6, [M], 4]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix10.p1">
<p class="ltx_p" id="S4.I2.ix10.p1.1"><span class="ltx_text" id="S4.I2.ix10.p1.1.1" style="color:#0000FF;">[5, [M], 8]</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S4.I2.ix11.p1">
<p class="ltx_p" id="S4.I2.ix11.p1.1"><span class="ltx_text" id="S4.I2.ix11.p1.1.1" style="color:#0000FF;">[[M], 6]</span></p>
</div>
</li>
</ul>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>SMM training representation. The <span class="ltx_text ltx_font_typewriter" id="S4.F2.6.1">[M]</span> token is the masked item the model is trained to predict. <span class="ltx_text" id="S4.F2.7.2" style="color:#FF0000;">Red</span> tokens are cut from the input due to the maximum sequence length constraint (<math alttext="max\_len=5" class="ltx_Math" display="inline" id="S4.F2.2.m1.1"><semantics id="S4.F2.2.m1.1b"><mrow id="S4.F2.2.m1.1.1" xref="S4.F2.2.m1.1.1.cmml"><mrow id="S4.F2.2.m1.1.1.2" xref="S4.F2.2.m1.1.1.2.cmml"><mi id="S4.F2.2.m1.1.1.2.2" xref="S4.F2.2.m1.1.1.2.2.cmml">m</mi><mo id="S4.F2.2.m1.1.1.2.1" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.3" xref="S4.F2.2.m1.1.1.2.3.cmml">a</mi><mo id="S4.F2.2.m1.1.1.2.1b" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.4" xref="S4.F2.2.m1.1.1.2.4.cmml">x</mi><mo id="S4.F2.2.m1.1.1.2.1c" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.5" mathvariant="normal" xref="S4.F2.2.m1.1.1.2.5.cmml">_</mi><mo id="S4.F2.2.m1.1.1.2.1d" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.6" xref="S4.F2.2.m1.1.1.2.6.cmml">l</mi><mo id="S4.F2.2.m1.1.1.2.1e" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.7" xref="S4.F2.2.m1.1.1.2.7.cmml">e</mi><mo id="S4.F2.2.m1.1.1.2.1f" xref="S4.F2.2.m1.1.1.2.1.cmml">â¢</mo><mi id="S4.F2.2.m1.1.1.2.8" xref="S4.F2.2.m1.1.1.2.8.cmml">n</mi></mrow><mo id="S4.F2.2.m1.1.1.1" xref="S4.F2.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.2.m1.1.1.3" xref="S4.F2.2.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><apply id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1"><eq id="S4.F2.2.m1.1.1.1.cmml" xref="S4.F2.2.m1.1.1.1"></eq><apply id="S4.F2.2.m1.1.1.2.cmml" xref="S4.F2.2.m1.1.1.2"><times id="S4.F2.2.m1.1.1.2.1.cmml" xref="S4.F2.2.m1.1.1.2.1"></times><ci id="S4.F2.2.m1.1.1.2.2.cmml" xref="S4.F2.2.m1.1.1.2.2">ğ‘š</ci><ci id="S4.F2.2.m1.1.1.2.3.cmml" xref="S4.F2.2.m1.1.1.2.3">ğ‘</ci><ci id="S4.F2.2.m1.1.1.2.4.cmml" xref="S4.F2.2.m1.1.1.2.4">ğ‘¥</ci><ci id="S4.F2.2.m1.1.1.2.5.cmml" xref="S4.F2.2.m1.1.1.2.5">_</ci><ci id="S4.F2.2.m1.1.1.2.6.cmml" xref="S4.F2.2.m1.1.1.2.6">ğ‘™</ci><ci id="S4.F2.2.m1.1.1.2.7.cmml" xref="S4.F2.2.m1.1.1.2.7">ğ‘’</ci><ci id="S4.F2.2.m1.1.1.2.8.cmml" xref="S4.F2.2.m1.1.1.2.8">ğ‘›</ci></apply><cn id="S4.F2.2.m1.1.1.3.cmml" type="integer" xref="S4.F2.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">max\_len=5</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.m1.1e">italic_m italic_a italic_x _ italic_l italic_e italic_n = 5</annotation></semantics></math>), and <span class="ltx_text" id="S4.F2.8.3" style="color:#0000FF;">blue</span> tokens are the ones visible in the modelâ€™s input sequence.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Hypotheses</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Advantages of <span class="ltx_glossaryref" title="">SMM</span> Over <span class="ltx_glossaryref" title="">CLM</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We hypothesize that one of the main advantages of our <span class="ltx_glossaryref" title="">SMM</span> approach compared to the Causal Language Modeling used in the GPT architecture is that the attention mechanism changes significantly. In a <span class="ltx_glossaryref" title="">CLM</span> model, attention is unidirectional, meaning each token can only <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.1">look at</span> the tokens that precede it. This limits the modelâ€™s ability to capture complex relationships between tokens, as the full context of the sequence is not considered.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">In contrast, with <span class="ltx_glossaryref" title="">SMM</span>, each token in the sequence can look anywhere, thanks to the data augmentation that generates subsequences. This allows the model to learn richer and more contextual representations, as each token has access to the entire sequential context and therefore will have a more accurate attention score.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Advantages of <span class="ltx_glossaryref" title="">SMM</span> Over <span class="ltx_glossaryref" title="">MLM</span>
</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.2">One of the key advantages of our <span class="ltx_glossaryref" title="">SMM</span> approach over the Masked Language Modeling method lies in its handling of sequences longer than the modelâ€™s <math alttext="max\_len" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.1.m1.1"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mrow id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p1.1.m1.1.1.2" xref="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml">m</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.3" xref="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml">a</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1a" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.4" xref="S4.SS3.SSS2.p1.1.m1.1.1.4.cmml">x</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1b" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.5" mathvariant="normal" xref="S4.SS3.SSS2.p1.1.m1.1.1.5.cmml">_</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1c" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.6" xref="S4.SS3.SSS2.p1.1.m1.1.1.6.cmml">l</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1d" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.7" xref="S4.SS3.SSS2.p1.1.m1.1.1.7.cmml">e</mi><mo id="S4.SS3.SSS2.p1.1.m1.1.1.1e" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.8" xref="S4.SS3.SSS2.p1.1.m1.1.1.8.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><apply id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1"><times id="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.1"></times><ci id="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.2">ğ‘š</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.3">ğ‘</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.4.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.4">ğ‘¥</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.5.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.5">_</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.6.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.6">ğ‘™</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.7.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.7">ğ‘’</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.8.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.8">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">max\_len</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.1.m1.1d">italic_m italic_a italic_x _ italic_l italic_e italic_n</annotation></semantics></math> parameter. In models trained with <span class="ltx_glossaryref" title="">MLM</span>, when input sequences exceed the <math alttext="max\_len" class="ltx_Math" display="inline" id="S4.SS3.SSS2.p1.2.m2.1"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><mrow id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml">m</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml">a</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1a" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.4" xref="S4.SS3.SSS2.p1.2.m2.1.1.4.cmml">x</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1b" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.5" mathvariant="normal" xref="S4.SS3.SSS2.p1.2.m2.1.1.5.cmml">_</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1c" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.6" xref="S4.SS3.SSS2.p1.2.m2.1.1.6.cmml">l</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1d" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.7" xref="S4.SS3.SSS2.p1.2.m2.1.1.7.cmml">e</mi><mo id="S4.SS3.SSS2.p1.2.m2.1.1.1e" xref="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.SSS2.p1.2.m2.1.1.8" xref="S4.SS3.SSS2.p1.2.m2.1.1.8.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><apply id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1"><times id="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.1"></times><ci id="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.2">ğ‘š</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3">ğ‘</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.4.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.4">ğ‘¥</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.5.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.5">_</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.6.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.6">ğ‘™</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.7.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.7">ğ‘’</ci><ci id="S4.SS3.SSS2.p1.2.m2.1.1.8.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.8">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">max\_len</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS2.p1.2.m2.1d">italic_m italic_a italic_x _ italic_l italic_e italic_n</annotation></semantics></math>, they must be split into smaller segments. This splitting breaks the continuity between tokens that span across sequence boundaries, resulting in the loss of important contextual information and potentially diminishing model performance.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">While overlapping subsequences could mitigate this issue, random masking in <span class="ltx_glossaryref" title="">MLM</span> introduces another challenge: a bias toward the tokens in the middle of the sequence. These tokens are more likely to be masked in multiple overlapping segments, whereas tokens near the sequence boundaries are underrepresented. This imbalance can negatively impact the modelâ€™s ability to learn representations for tokens at the start or end of a sequence.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">Rather than breaking the sequence, <span class="ltx_glossaryref" title="">SMM</span> generates subsequences that maintain continuity across the full sequence during training, avoiding these limitations and allowing for a more balanced representation of tokens throughout.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Masking the penultimate instead of the last</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">The <span class="ltx_glossaryref" title="">SMM</span> training approach allows the model to take advantage of a small amount of right-side context thanks to the bidirectional attention used in encoder-only transformer architectures, which helps during training. Even though in practice, predicting the next item doesnâ€™t have right-side context (since we donâ€™t have access to what comes next), this method works well in situations where the order of items is not strictly fixed. Itâ€™s more about the proximity of item pairs in e-commerce data, unlike linguistic data where word order is more rigid. This hypothesis will be experimentally explored and further confirmed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.SS5" title="6.5. Sequentially Masking the Last K Items â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">6.5</span></a>, where we compare the effects of masking the penultimate token versus masking other tokens, including the third-to-last, in the sequence.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Optimizing techniques</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present several techniques that we introduce to the transformer architectures, which contributed to further improving prediction performance. All the optimizations are implemented across all three transformer models, except for Contextual Positional Encoding, which is only applied to BERT due to compatibility constraints with the attention block. Ablation studies for these optimization techniques, detailing their individual impact, can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.SS1" title="A.1. Ablation study â€£ Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">A.1</span></a>. Furthermore, the optimized BERT architecture can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.SS2" title="A.2. Optimized BERT Architecture â€£ Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Weight Tying</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Weight tying <cite class="ltx_cite ltx_citemacro_citep">(Press and Wolf, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib27" title="">2016</a>)</cite> technique is used in transformer models to reduce the number of parameters and enhance performance. It involves using the same weights for both the input embeddings and the modelâ€™s output layer. In other words, the weight matrices used to encode tokens into embeddings are the same as those used to predict tokens from these representation vectors at the output of our encoder. By sharing the same weight matrix for the embeddings and the output layer, not only is the parameter count reduced, but the modelâ€™s generalization is also improved by linking the input and output embeddings.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Pre-layer Normalization</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Pre-layer normalization is the second technique implemented across all compatible transformer architectures (GPT, BERT, and DeBERTa). Large Language Models (LLMs), such as Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib17" title="">2023</a>)</cite> or Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib32" title="">2023</a>)</cite>, use pre-layer normalization, which applies normalization before the multi-head attention layer and the feed-forward layer. By normalizing before the attention operation, the model can better manage gradients during training, leading to more stable convergence. In our implementation, we use RMSNorm <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib41" title="">2019</a>)</cite> instead of layer normalization <cite class="ltx_cite ltx_citemacro_citep">(Ba etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib3" title="">2016</a>)</cite>, just like in previously discussed LLM architectures.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Contextual Positional Encoding</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We implemented an enhancement to the positional embeddings known as Contextual Positional Encoding (CoPE) <cite class="ltx_cite ltx_citemacro_citep">(Golovneva etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib11" title="">2024</a>)</cite>, which replaces the <span class="ltx_glossaryref" title="">Rotary Position Embedding (RoPE)</span> method <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib30" title="">2021</a>)</cite> commonly used in popular LLMs. Unlike traditional methods that rely on simple token counting, CoPE determines token positions based on contextual information. For compatibility purposes, we applied this technique exclusively to our BERT architecture.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Evaluation and Experiments</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The various techniques we developed were tested through a series of experiments to evaluate their effectiveness. We describe here the methodological details of the experiments.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Experimental Datasets</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">To evaluate and compare the performance of our transformer models enhanced with our new masking approach and optimization techniques, we selected three publicly available datasets: Yoochoose 1/64 <cite class="ltx_cite ltx_citemacro_citep">(Yoochoose, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib40" title="">2015</a>)</cite>, Tmall <cite class="ltx_cite ltx_citemacro_citep">(Tmall, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib31" title="">2015</a>)</cite>, and Diginetica <cite class="ltx_cite ltx_citemacro_citep">(Diginetica, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib10" title="">2016</a>)</cite>, which are popular datasets for the session-based recommendation task.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Data Preprocessing</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">The datasets were preprocessed to ensure the results are comparable with various studies in session-based recommendation, particularly those that are considered state-of-the-art and use GNNs <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib39" title="">2019</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib43" title="">2023</a>)</cite>. The preprocessing involves three main steps to match the statistics shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.T1" title="Table 1 â€£ 6.1.1. Data Preprocessing â€£ 6.1. Experimental Datasets â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">All sessions are ordered chronologically, and the data is split into training and test sets based on session timestamps.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">We filter out items that appear less than five times or only appear in the test set, as well as sessions of length one.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">Data augmentation is performed using a sliding window of size <span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">30</span> to generate more data samples within a session. For a session [v1, v2, â€¦, vn], we generate samples ([v1, v2, â€¦, vn-1], vn), â€¦, ([v1, v2], v3), ([v1], v2).</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Dataset Statistics</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T1.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T1.1.1.1.2">Diginetica</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T1.1.1.1.3">Tmall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T1.1.1.1.4">Yoochoose 1/64</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T1.1.2.1.1">#Train sessions</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T1.1.2.1.2">719,470</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T1.1.2.1.3">351,268</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T1.1.2.1.4">369,859</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.1.3.2.1">#Test sessions</th>
<td class="ltx_td ltx_align_right" id="S6.T1.1.3.2.2">60,858</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.3.2.3">25,898</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.3.2.4">55,898</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T1.1.4.3.1">#Items</th>
<td class="ltx_td ltx_align_right" id="S6.T1.1.4.3.2">43,097</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.4.3.3">40,728</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.4.3.4">16,766</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S6.T1.1.5.4.1">Avg. length</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="S6.T1.1.5.4.2">5.12</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S6.T1.1.5.4.3">6.69</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S6.T1.1.5.4.4">6.16</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Implementation</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We implemented all three transformer models (BERT, GPT, and DeBERTa) using the PyTorch library <cite class="ltx_cite ltx_citemacro_citep">(Paszke etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib25" title="">2019</a>)</cite>. It was not possible to directly import pre-existing models because those models are pre-trained on large text corpora and not on e-commerce items specific to each of our three datasets. Additionally, having access to the internal code of these models was essential to implement recent techniques used by large language models. The code will be available after reviewing.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">The three transformer models use the BERT Medium configuration. This configuration consists of 8 encoder layers (decoder for GPT), a hidden size of 512, 8 attention heads, and a total of 41 million parameters <cite class="ltx_cite ltx_citemacro_citep">(Prajjwal1, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib26" title="">2023</a>)</cite>. We chose this configuration for its balance between computational efficiency and performance. We found that larger configurations often result in overfitting due to the smaller size of e-commerce datasets compared to text corpora, while smaller configurations tend to underfit. The BERT Medium configuration offers a suitable compromise.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Metrics</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">The evaluations are based on two main metrics:</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Precision@20</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">P@20 is defined as the number of relevant recommendations within the top 20 results, divided by 20. It is a commonly used metric to evaluate the performance of recommendation systems. The formula is given by:</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P@20=\frac{1}{N}\sum_{i=1}^{N}\frac{\text{relevant predictions at position }%
\leq 20}{20}" class="ltx_Math" display="block" id="S6.E1.m1.1"><semantics id="S6.E1.m1.1a"><mrow id="S6.E1.m1.1.1" xref="S6.E1.m1.1.1.cmml"><mrow id="S6.E1.m1.1.1.2" xref="S6.E1.m1.1.1.2.cmml"><mi id="S6.E1.m1.1.1.2.2" xref="S6.E1.m1.1.1.2.2.cmml">P</mi><mo id="S6.E1.m1.1.1.2.1" xref="S6.E1.m1.1.1.2.1.cmml">â¢</mo><mi id="S6.E1.m1.1.1.2.3" mathvariant="normal" xref="S6.E1.m1.1.1.2.3.cmml">@</mi><mo id="S6.E1.m1.1.1.2.1a" xref="S6.E1.m1.1.1.2.1.cmml">â¢</mo><mn id="S6.E1.m1.1.1.2.4" xref="S6.E1.m1.1.1.2.4.cmml">20</mn></mrow><mo id="S6.E1.m1.1.1.1" xref="S6.E1.m1.1.1.1.cmml">=</mo><mrow id="S6.E1.m1.1.1.3" xref="S6.E1.m1.1.1.3.cmml"><mfrac id="S6.E1.m1.1.1.3.2" xref="S6.E1.m1.1.1.3.2.cmml"><mn id="S6.E1.m1.1.1.3.2.2" xref="S6.E1.m1.1.1.3.2.2.cmml">1</mn><mi id="S6.E1.m1.1.1.3.2.3" xref="S6.E1.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo id="S6.E1.m1.1.1.3.1" xref="S6.E1.m1.1.1.3.1.cmml">â¢</mo><mrow id="S6.E1.m1.1.1.3.3" xref="S6.E1.m1.1.1.3.3.cmml"><munderover id="S6.E1.m1.1.1.3.3.1" xref="S6.E1.m1.1.1.3.3.1.cmml"><mo id="S6.E1.m1.1.1.3.3.1.2.2" movablelimits="false" xref="S6.E1.m1.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S6.E1.m1.1.1.3.3.1.2.3" xref="S6.E1.m1.1.1.3.3.1.2.3.cmml"><mi id="S6.E1.m1.1.1.3.3.1.2.3.2" xref="S6.E1.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S6.E1.m1.1.1.3.3.1.2.3.1" xref="S6.E1.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S6.E1.m1.1.1.3.3.1.2.3.3" xref="S6.E1.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S6.E1.m1.1.1.3.3.1.3" xref="S6.E1.m1.1.1.3.3.1.3.cmml">N</mi></munderover><mfrac id="S6.E1.m1.1.1.3.3.2" xref="S6.E1.m1.1.1.3.3.2.cmml"><mrow id="S6.E1.m1.1.1.3.3.2.2" xref="S6.E1.m1.1.1.3.3.2.2.cmml"><mtext id="S6.E1.m1.1.1.3.3.2.2.2" xref="S6.E1.m1.1.1.3.3.2.2.2a.cmml">relevant predictions at positionÂ </mtext><mo id="S6.E1.m1.1.1.3.3.2.2.1" xref="S6.E1.m1.1.1.3.3.2.2.1.cmml">â‰¤</mo><mn id="S6.E1.m1.1.1.3.3.2.2.3" xref="S6.E1.m1.1.1.3.3.2.2.3.cmml">20</mn></mrow><mn id="S6.E1.m1.1.1.3.3.2.3" xref="S6.E1.m1.1.1.3.3.2.3.cmml">20</mn></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.1b"><apply id="S6.E1.m1.1.1.cmml" xref="S6.E1.m1.1.1"><eq id="S6.E1.m1.1.1.1.cmml" xref="S6.E1.m1.1.1.1"></eq><apply id="S6.E1.m1.1.1.2.cmml" xref="S6.E1.m1.1.1.2"><times id="S6.E1.m1.1.1.2.1.cmml" xref="S6.E1.m1.1.1.2.1"></times><ci id="S6.E1.m1.1.1.2.2.cmml" xref="S6.E1.m1.1.1.2.2">ğ‘ƒ</ci><ci id="S6.E1.m1.1.1.2.3.cmml" xref="S6.E1.m1.1.1.2.3">@</ci><cn id="S6.E1.m1.1.1.2.4.cmml" type="integer" xref="S6.E1.m1.1.1.2.4">20</cn></apply><apply id="S6.E1.m1.1.1.3.cmml" xref="S6.E1.m1.1.1.3"><times id="S6.E1.m1.1.1.3.1.cmml" xref="S6.E1.m1.1.1.3.1"></times><apply id="S6.E1.m1.1.1.3.2.cmml" xref="S6.E1.m1.1.1.3.2"><divide id="S6.E1.m1.1.1.3.2.1.cmml" xref="S6.E1.m1.1.1.3.2"></divide><cn id="S6.E1.m1.1.1.3.2.2.cmml" type="integer" xref="S6.E1.m1.1.1.3.2.2">1</cn><ci id="S6.E1.m1.1.1.3.2.3.cmml" xref="S6.E1.m1.1.1.3.2.3">ğ‘</ci></apply><apply id="S6.E1.m1.1.1.3.3.cmml" xref="S6.E1.m1.1.1.3.3"><apply id="S6.E1.m1.1.1.3.3.1.cmml" xref="S6.E1.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.3.3.1.1.cmml" xref="S6.E1.m1.1.1.3.3.1">superscript</csymbol><apply id="S6.E1.m1.1.1.3.3.1.2.cmml" xref="S6.E1.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E1.m1.1.1.3.3.1.2.1.cmml" xref="S6.E1.m1.1.1.3.3.1">subscript</csymbol><sum id="S6.E1.m1.1.1.3.3.1.2.2.cmml" xref="S6.E1.m1.1.1.3.3.1.2.2"></sum><apply id="S6.E1.m1.1.1.3.3.1.2.3.cmml" xref="S6.E1.m1.1.1.3.3.1.2.3"><eq id="S6.E1.m1.1.1.3.3.1.2.3.1.cmml" xref="S6.E1.m1.1.1.3.3.1.2.3.1"></eq><ci id="S6.E1.m1.1.1.3.3.1.2.3.2.cmml" xref="S6.E1.m1.1.1.3.3.1.2.3.2">ğ‘–</ci><cn id="S6.E1.m1.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S6.E1.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S6.E1.m1.1.1.3.3.1.3.cmml" xref="S6.E1.m1.1.1.3.3.1.3">ğ‘</ci></apply><apply id="S6.E1.m1.1.1.3.3.2.cmml" xref="S6.E1.m1.1.1.3.3.2"><divide id="S6.E1.m1.1.1.3.3.2.1.cmml" xref="S6.E1.m1.1.1.3.3.2"></divide><apply id="S6.E1.m1.1.1.3.3.2.2.cmml" xref="S6.E1.m1.1.1.3.3.2.2"><leq id="S6.E1.m1.1.1.3.3.2.2.1.cmml" xref="S6.E1.m1.1.1.3.3.2.2.1"></leq><ci id="S6.E1.m1.1.1.3.3.2.2.2a.cmml" xref="S6.E1.m1.1.1.3.3.2.2.2"><mtext id="S6.E1.m1.1.1.3.3.2.2.2.cmml" xref="S6.E1.m1.1.1.3.3.2.2.2">relevant predictions at positionÂ </mtext></ci><cn id="S6.E1.m1.1.1.3.3.2.2.3.cmml" type="integer" xref="S6.E1.m1.1.1.3.3.2.2.3">20</cn></apply><cn id="S6.E1.m1.1.1.3.3.2.3.cmml" type="integer" xref="S6.E1.m1.1.1.3.3.2.3">20</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.1c">P@20=\frac{1}{N}\sum_{i=1}^{N}\frac{\text{relevant predictions at position }%
\leq 20}{20}</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.1d">italic_P @ 20 = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG relevant predictions at position â‰¤ 20 end_ARG start_ARG 20 end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p3">
<p class="ltx_p" id="S6.SS3.SSS1.p3.1">where <math alttext="N" class="ltx_Math" display="inline" id="S6.SS3.SSS1.p3.1.m1.1"><semantics id="S6.SS3.SSS1.p3.1.m1.1a"><mi id="S6.SS3.SSS1.p3.1.m1.1.1" xref="S6.SS3.SSS1.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS1.p3.1.m1.1b"><ci id="S6.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS3.SSS1.p3.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS1.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS1.p3.1.m1.1d">italic_N</annotation></semantics></math> is the total number of test sessions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Mean Reciprocal Rank</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">MRR is a metric that evaluates the effectiveness of recommendation systems by considering the position of the first relevant item in the result list. The formula is given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="MRR=\frac{1}{N}\sum_{i=1}^{N}\frac{1}{\text{rank}_{i}}" class="ltx_Math" display="block" id="S6.E2.m1.1"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1" xref="S6.E2.m1.1.1.cmml"><mrow id="S6.E2.m1.1.1.2" xref="S6.E2.m1.1.1.2.cmml"><mi id="S6.E2.m1.1.1.2.2" xref="S6.E2.m1.1.1.2.2.cmml">M</mi><mo id="S6.E2.m1.1.1.2.1" xref="S6.E2.m1.1.1.2.1.cmml">â¢</mo><mi id="S6.E2.m1.1.1.2.3" xref="S6.E2.m1.1.1.2.3.cmml">R</mi><mo id="S6.E2.m1.1.1.2.1a" xref="S6.E2.m1.1.1.2.1.cmml">â¢</mo><mi id="S6.E2.m1.1.1.2.4" xref="S6.E2.m1.1.1.2.4.cmml">R</mi></mrow><mo id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.cmml">=</mo><mrow id="S6.E2.m1.1.1.3" xref="S6.E2.m1.1.1.3.cmml"><mfrac id="S6.E2.m1.1.1.3.2" xref="S6.E2.m1.1.1.3.2.cmml"><mn id="S6.E2.m1.1.1.3.2.2" xref="S6.E2.m1.1.1.3.2.2.cmml">1</mn><mi id="S6.E2.m1.1.1.3.2.3" xref="S6.E2.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo id="S6.E2.m1.1.1.3.1" xref="S6.E2.m1.1.1.3.1.cmml">â¢</mo><mrow id="S6.E2.m1.1.1.3.3" xref="S6.E2.m1.1.1.3.3.cmml"><munderover id="S6.E2.m1.1.1.3.3.1" xref="S6.E2.m1.1.1.3.3.1.cmml"><mo id="S6.E2.m1.1.1.3.3.1.2.2" movablelimits="false" xref="S6.E2.m1.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S6.E2.m1.1.1.3.3.1.2.3" xref="S6.E2.m1.1.1.3.3.1.2.3.cmml"><mi id="S6.E2.m1.1.1.3.3.1.2.3.2" xref="S6.E2.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S6.E2.m1.1.1.3.3.1.2.3.1" xref="S6.E2.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S6.E2.m1.1.1.3.3.1.2.3.3" xref="S6.E2.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S6.E2.m1.1.1.3.3.1.3" xref="S6.E2.m1.1.1.3.3.1.3.cmml">N</mi></munderover><mfrac id="S6.E2.m1.1.1.3.3.2" xref="S6.E2.m1.1.1.3.3.2.cmml"><mn id="S6.E2.m1.1.1.3.3.2.2" xref="S6.E2.m1.1.1.3.3.2.2.cmml">1</mn><msub id="S6.E2.m1.1.1.3.3.2.3" xref="S6.E2.m1.1.1.3.3.2.3.cmml"><mtext id="S6.E2.m1.1.1.3.3.2.3.2" xref="S6.E2.m1.1.1.3.3.2.3.2a.cmml">rank</mtext><mi id="S6.E2.m1.1.1.3.3.2.3.3" xref="S6.E2.m1.1.1.3.3.2.3.3.cmml">i</mi></msub></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.cmml" xref="S6.E2.m1.1.1"><eq id="S6.E2.m1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"></eq><apply id="S6.E2.m1.1.1.2.cmml" xref="S6.E2.m1.1.1.2"><times id="S6.E2.m1.1.1.2.1.cmml" xref="S6.E2.m1.1.1.2.1"></times><ci id="S6.E2.m1.1.1.2.2.cmml" xref="S6.E2.m1.1.1.2.2">ğ‘€</ci><ci id="S6.E2.m1.1.1.2.3.cmml" xref="S6.E2.m1.1.1.2.3">ğ‘…</ci><ci id="S6.E2.m1.1.1.2.4.cmml" xref="S6.E2.m1.1.1.2.4">ğ‘…</ci></apply><apply id="S6.E2.m1.1.1.3.cmml" xref="S6.E2.m1.1.1.3"><times id="S6.E2.m1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.3.1"></times><apply id="S6.E2.m1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.3.2"><divide id="S6.E2.m1.1.1.3.2.1.cmml" xref="S6.E2.m1.1.1.3.2"></divide><cn id="S6.E2.m1.1.1.3.2.2.cmml" type="integer" xref="S6.E2.m1.1.1.3.2.2">1</cn><ci id="S6.E2.m1.1.1.3.2.3.cmml" xref="S6.E2.m1.1.1.3.2.3">ğ‘</ci></apply><apply id="S6.E2.m1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.3.3"><apply id="S6.E2.m1.1.1.3.3.1.cmml" xref="S6.E2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.1.1.cmml" xref="S6.E2.m1.1.1.3.3.1">superscript</csymbol><apply id="S6.E2.m1.1.1.3.3.1.2.cmml" xref="S6.E2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.1.2.1.cmml" xref="S6.E2.m1.1.1.3.3.1">subscript</csymbol><sum id="S6.E2.m1.1.1.3.3.1.2.2.cmml" xref="S6.E2.m1.1.1.3.3.1.2.2"></sum><apply id="S6.E2.m1.1.1.3.3.1.2.3.cmml" xref="S6.E2.m1.1.1.3.3.1.2.3"><eq id="S6.E2.m1.1.1.3.3.1.2.3.1.cmml" xref="S6.E2.m1.1.1.3.3.1.2.3.1"></eq><ci id="S6.E2.m1.1.1.3.3.1.2.3.2.cmml" xref="S6.E2.m1.1.1.3.3.1.2.3.2">ğ‘–</ci><cn id="S6.E2.m1.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S6.E2.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S6.E2.m1.1.1.3.3.1.3.cmml" xref="S6.E2.m1.1.1.3.3.1.3">ğ‘</ci></apply><apply id="S6.E2.m1.1.1.3.3.2.cmml" xref="S6.E2.m1.1.1.3.3.2"><divide id="S6.E2.m1.1.1.3.3.2.1.cmml" xref="S6.E2.m1.1.1.3.3.2"></divide><cn id="S6.E2.m1.1.1.3.3.2.2.cmml" type="integer" xref="S6.E2.m1.1.1.3.3.2.2">1</cn><apply id="S6.E2.m1.1.1.3.3.2.3.cmml" xref="S6.E2.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S6.E2.m1.1.1.3.3.2.3.1.cmml" xref="S6.E2.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S6.E2.m1.1.1.3.3.2.3.2a.cmml" xref="S6.E2.m1.1.1.3.3.2.3.2"><mtext id="S6.E2.m1.1.1.3.3.2.3.2.cmml" xref="S6.E2.m1.1.1.3.3.2.3.2">rank</mtext></ci><ci id="S6.E2.m1.1.1.3.3.2.3.3.cmml" xref="S6.E2.m1.1.1.3.3.2.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">MRR=\frac{1}{N}\sum_{i=1}^{N}\frac{1}{\text{rank}_{i}}</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.1d">italic_M italic_R italic_R = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG rank start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p2">
<p class="ltx_p" id="S6.SS3.SSS2.p2.3">where <math alttext="\text{rank}_{i}" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.1.m1.1"><semantics id="S6.SS3.SSS2.p2.1.m1.1a"><msub id="S6.SS3.SSS2.p2.1.m1.1.1" xref="S6.SS3.SSS2.p2.1.m1.1.1.cmml"><mtext id="S6.SS3.SSS2.p2.1.m1.1.1.2" xref="S6.SS3.SSS2.p2.1.m1.1.1.2a.cmml">rank</mtext><mi id="S6.SS3.SSS2.p2.1.m1.1.1.3" xref="S6.SS3.SSS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p2.1.m1.1b"><apply id="S6.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS3.SSS2.p2.1.m1.1.1.1.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS3.SSS2.p2.1.m1.1.1.2a.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1.2"><mtext id="S6.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1.2">rank</mtext></ci><ci id="S6.SS3.SSS2.p2.1.m1.1.1.3.cmml" xref="S6.SS3.SSS2.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p2.1.m1.1c">\text{rank}_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p2.1.m1.1d">rank start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the position of the first relevant item in the <math alttext="i" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.2.m2.1"><semantics id="S6.SS3.SSS2.p2.2.m2.1a"><mi id="S6.SS3.SSS2.p2.2.m2.1.1" xref="S6.SS3.SSS2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p2.2.m2.1b"><ci id="S6.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S6.SS3.SSS2.p2.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p2.2.m2.1d">italic_i</annotation></semantics></math>-th session, and <math alttext="N" class="ltx_Math" display="inline" id="S6.SS3.SSS2.p2.3.m3.1"><semantics id="S6.SS3.SSS2.p2.3.m3.1a"><mi id="S6.SS3.SSS2.p2.3.m3.1.1" xref="S6.SS3.SSS2.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS2.p2.3.m3.1b"><ci id="S6.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S6.SS3.SSS2.p2.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS2.p2.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.SSS2.p2.3.m3.1d">italic_N</annotation></semantics></math> is the total number of test sessions.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Masking Techniques Comparison</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">We evaluate and compare the performance of optimized transformer model architectures to determine which masking technique yields the best overall Precision@20 results. The models compared include GPT-CLM, BERT-MLM, DeBERTa-MLM, BERT-SMM, and DeBERTa-SMM, each using the respective masking strategies discussed earlier. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S6.T2" title="Table 2 â€£ 6.4. Masking Techniques Comparison â€£ 6. Evaluation and Experiments â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the results across various combinations of models and masking methods.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Precision@20 Results for Different Masking Techniques Across Various Datasets</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.2.1">Diginetica</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.3.1">Yoochoose 1/64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.4.1">Tmall</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.1.2.2.1">DeBERTa (MLM)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2.2">52.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2.3">70.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.2.2.4">33.86</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.1.3.3.1">BERT (MLM)</th>
<td class="ltx_td ltx_align_center" id="S6.T2.1.3.3.2">52.98</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.3.3.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.1.3.3.3.1">70.93</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.3.3.4">34.21</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.1.4.4.1">GPT (CLM)</th>
<td class="ltx_td ltx_align_center" id="S6.T2.1.4.4.2">52.45</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.4.4.3">70.42</td>
<td class="ltx_td ltx_align_center" id="S6.T2.1.4.4.4">33.40</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.1.5.5.1">DeBERTa (SMM)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.5.5.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.1.5.5.2.1">53.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.5.5.3">70.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.5.5.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T2.1.5.5.4.1">36.08</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S6.T2.1.6.6.1">BERT (SMM)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.6.6.2.1">53.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.6.6.3.1">71.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S6.T2.1.6.6.4.1">38.34</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">As shown above, the <span class="ltx_glossaryref" title="">SMM</span> masking approach outperforms both CLM and <span class="ltx_glossaryref" title="">MLM</span> techniques across all three datasets and the transformer architectures tested. While CLM, commonly used in auto-regressive models like GPT, predicts tokens sequentially based on prior context, and <span class="ltx_glossaryref" title="">MLM</span>, employed in models like BERT, masks and predicts certain tokens based on surrounding context to capture bidirectional relationships, <span class="ltx_glossaryref" title="">SMM</span> is specifically designed for recommendation systems. It focuses on predicting the next item a user is likely to interact with, directly aligning with the goal of next-click prediction. This confirms the hypothesis <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3.SSS1" title="4.3.1. Advantages of SMM Over CLM â€£ 4.3. Hypotheses â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>Sequentially Masking the Last K Items</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">In this section, we explain our decision to mask the penultimate token of each sub-sequence rather than the last one. Additionally, we explore whether masking the penultimate token yields better results compared to masking the third-to-last token. Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table:last_k_mask</span> presents the performance of the optimized BERT architecture under different masking strategies. The results indicate that, in some cases, masking the third-to-last token can produce better outcomes. However, this is highly dependent on the level of disorder in the dataset. In an ideal scenario with perfectly ordered data, masking only the last token would likely be the most effective. Unfortunately, such ideal conditions are rare in <span class="ltx_glossaryref" title="">SBR</span> tasks, unlike in <span class="ltx_glossaryref" title="">NLP</span>, where data typically follows a more fixed sequence.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.2">Furthermore, the results confirm that masking the last token (<math alttext="K=1" class="ltx_Math" display="inline" id="S6.SS5.p2.1.m1.1"><semantics id="S6.SS5.p2.1.m1.1a"><mrow id="S6.SS5.p2.1.m1.1.1" xref="S6.SS5.p2.1.m1.1.1.cmml"><mi id="S6.SS5.p2.1.m1.1.1.2" xref="S6.SS5.p2.1.m1.1.1.2.cmml">K</mi><mo id="S6.SS5.p2.1.m1.1.1.1" xref="S6.SS5.p2.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS5.p2.1.m1.1.1.3" xref="S6.SS5.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.1.m1.1b"><apply id="S6.SS5.p2.1.m1.1.1.cmml" xref="S6.SS5.p2.1.m1.1.1"><eq id="S6.SS5.p2.1.m1.1.1.1.cmml" xref="S6.SS5.p2.1.m1.1.1.1"></eq><ci id="S6.SS5.p2.1.m1.1.1.2.cmml" xref="S6.SS5.p2.1.m1.1.1.2">ğ¾</ci><cn id="S6.SS5.p2.1.m1.1.1.3.cmml" type="integer" xref="S6.SS5.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.1.m1.1c">K=1</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.1.m1.1d">italic_K = 1</annotation></semantics></math>) consistently yielded the poorest performance, indicating that this approach is rarely optimal. Therefore, we use <math alttext="K=2" class="ltx_Math" display="inline" id="S6.SS5.p2.2.m2.1"><semantics id="S6.SS5.p2.2.m2.1a"><mrow id="S6.SS5.p2.2.m2.1.1" xref="S6.SS5.p2.2.m2.1.1.cmml"><mi id="S6.SS5.p2.2.m2.1.1.2" xref="S6.SS5.p2.2.m2.1.1.2.cmml">K</mi><mo id="S6.SS5.p2.2.m2.1.1.1" xref="S6.SS5.p2.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS5.p2.2.m2.1.1.3" xref="S6.SS5.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS5.p2.2.m2.1b"><apply id="S6.SS5.p2.2.m2.1.1.cmml" xref="S6.SS5.p2.2.m2.1.1"><eq id="S6.SS5.p2.2.m2.1.1.1.cmml" xref="S6.SS5.p2.2.m2.1.1.1"></eq><ci id="S6.SS5.p2.2.m2.1.1.2.cmml" xref="S6.SS5.p2.2.m2.1.1.2">ğ¾</ci><cn id="S6.SS5.p2.2.m2.1.1.3.cmml" type="integer" xref="S6.SS5.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p2.2.m2.1c">K=2</annotation><annotation encoding="application/x-llamapun" id="S6.SS5.p2.2.m2.1d">italic_K = 2</annotation></semantics></math> for all <span class="ltx_glossaryref" title="">SMM</span> training throughout the rest of this paper, as it provides a better balance between context utilization and accurate next-item prediction, confirming hypothesis <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S4.SS3.SSS3" title="4.3.3. Masking the penultimate instead of the last â€£ 4.3. Hypotheses â€£ 4. The proposed method â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">4.3.3</span></a>.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison of Different Last K Item Masked.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.1">Diginetica</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.1">Yoochoose 1/64</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.4.1">Tmall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.2.1.1">K = 1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.2.1.2">52.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.2.1.3">70.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.2.1.4">35.16</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.3.2.1">K = 2</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.3.2.2.1">53.49</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.3.2.3">71.23</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S6.T3.1.3.2.4.1">38.42</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.1.4.3.1">K = 3</th>
<td class="ltx_td ltx_align_center" id="S6.T3.1.4.3.2">53.47</td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.4.3.3.1">71.24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.1.4.3.4">38.40</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Comparison with the State of the Art</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, we compare our optimized encoder-only transformer architectures trained with the <span class="ltx_glossaryref" title="">SMM</span> methodâ€”against various state-of-the-art models. The results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7.T4" title="Table 4 â€£ 7.1. Types of Benchmark Approaches â€£ 7. Comparison with the State of the Art â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a>, which compares the <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">P@20</span> and <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">MRR@20</span> metrics across the Yoochoose 1/64, Diginetica, and Tmall datasets.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Types of Benchmark Approaches</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">In this section, we outline the different types of benchmark approaches used to compare against our models. These include single-session, cross-session, and multi-relation approaches, each varying in the amount of user session history and relational data they utilize for making recommendations <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib43" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Single-Session Approach</span>: This approach considers each session independently, without taking into account information from other sessions. It primarily relies on the content of the current session.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.1.1">Cross-Session Approach</span>: Cross-session approach incorporates information from multiple past sessions of a user to enhance the performance of recommendations. It leverages the history of sessions to understand long-term user preferences and behavior trends. This approach may use session merging techniques, where past sessions are aggregated or encoded to provide additional context during recommendations.</p>
</div>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p4.1.1">Multi-Relation Approach</span>: This method considers various relationships between sessions and users to make more personalized and accurate recommendations. In other words, it uses information that goes beyond individual sessions, drawing from broader user-related data to generate recommendations.</p>
</div>
<div class="ltx_para" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1">Cross-session and multi-relation approaches benefit from access to more information during prediction, often leading to improved prediction performance. Since our prediction approach using the optimized transformer-SMM relies solely on single-session data, we focus on comparing its performance against state-of-the-art single-session approaches. Therefore, our performance analysis primarily benchmarks our method against other single-session models.</p>
</div>
<figure class="ltx_table" id="S7.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparison of our models with state-of-the-art models. The best-performing model per approach is highlighted in <span class="ltx_text ltx_font_bold" id="S7.T4.3.1">bold</span>, while the second-best model is underlined.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S7.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T4.1.2.1">
<td class="ltx_td ltx_border_t" id="S7.T4.1.2.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S7.T4.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.2.1.2.1">Diginetica</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S7.T4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.2.1.3.1">Tmall</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S7.T4.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.2.1.4.1">Yoochoose 1/64</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.1.1">Single-Session Approach</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.2.1">P@20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.3.1">MRR@20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.4.1">P@20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.5.1">MRR@20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.6.1">P@20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.7.1">MRR@20</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.1">POP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.2">1.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.3">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.4">2.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.5">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.6">6.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.4.3.7">0.58</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.1">Item-KNN</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.2">35.75</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.3">11.57</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.4">9.15</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.5">3.31</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.6">51.60</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.5.4.7">21.81</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.6.5">
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.1">FPMC</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.2">22.14</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.3">6.66</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.4">7.32</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.5">2.01</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.6">45.62</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.6.5.7">15.01</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.7.6">
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.1">GRU4Rec</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.2">30.79</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.3">8.60</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.4">10.93</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.5">5.89</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.6">60.64</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.7.6.7">22.89</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.8.7">
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.1">NARM</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.2">48.32</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.3">16.00</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.4">23.00</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.5">10.64</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.6">68.32</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.8.7.7">28.63</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.9.8">
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.1">STAMP</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.2">46.62</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.3">15.13</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.4">20.47</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.5">9.36</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.6">68.74</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.9.8.7">29.67</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.10.9">
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.1">SR-GNN</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.2">50.73</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.3">17.59</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.4">27.57</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.5">13.57</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.6">70.57</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.10.9.7">30.94</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.11.10">
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.1">LESSR</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.2">51.71</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.11.10.3.1">18.15</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.4">23.53</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.5">9.56</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.6">70.65</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.11.10.7">30.59</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.12.11">
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.1"><span class="ltx_text ltx_font_italic" id="S7.T4.1.12.11.1.1">GPT-CLM</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.2">52.45</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.3">17.16</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.4">33.40</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.5">16.91</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.6">70.42</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.12.11.7">30.98</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.13.12">
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.1"><span class="ltx_text ltx_font_italic" id="S7.T4.1.13.12.1.1">DeBERTa-MLM</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.2">52.75</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.3">17.79</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.4">33.86</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.5">17.23</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.6">70.66</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.13.12.7">31.61</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.14.13">
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.1"><span class="ltx_text ltx_font_italic" id="S7.T4.1.14.13.1.1">BERT-MLM</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.14.13.2.1">52.98</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.3">17.81</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.4">34.21</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.5">17.96</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.14.13.6.1">70.93</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.14.13.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.14.13.7.1">31.86</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.15.14">
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.1"><span class="ltx_text ltx_font_italic" id="S7.T4.1.15.14.1.1">DeBERTa-SMM</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.2">52.85</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.3">18.02</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.15.14.4.1">36.08</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.15.14.5.1">19.59</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.6">70.86</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.15.14.7">31.84</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.16.15">
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.1"><span class="ltx_text ltx_font_italic" id="S7.T4.1.16.15.1.1">BERT-SMM</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.2.1">53.49</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.3.1">18.63</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.4.1">38.34</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.5"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.5.1">21.19</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.6"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.6.1">71.23</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.16.15.7"><span class="ltx_text ltx_font_bold" id="S7.T4.1.16.15.7.1">32.05</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.17.16.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.17.16.1.1">Cross-Session Approach</span></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.2"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.3"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.4"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.5"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.6"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.17.16.7"></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.1">CSRM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.2">48.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.3">17.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.4">29.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.5">13.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.18.17.7">-</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.19.18">
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.1">CoSAN</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.2">51.97</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.3">17.92</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.4">32.68</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.5">14.09</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.6">-</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.19.18.7">-</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.20.19">
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.1">GCE-GNN</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.20.19.2.1">54.82</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.3">19.04</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.4">31.42</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.5">14.05</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.6">70.91</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.20.19.7">30.63</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1">
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.1">S<sup class="ltx_sup" id="S7.T4.1.1.1.1">2</sup>-DHCM</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.2">51.38</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.3">18.44</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.4">31.26</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.5">13.73</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.1.6.1">71.88</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.1.7.1">31.32</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.21.20">
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.1">MTD</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.2">51.82</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.3">17.26</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.4">30.41</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.5">13.15</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.6">-</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.21.20.7">-</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.22.21">
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.1">COTREC</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.2">54.18</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.22.21.3.1">19.07</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.22.21.4.1">36.35</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.22.21.5.1">18.04</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.6">-</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.22.21.7">-</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.23.22">
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.1">CARES</td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.2.1">56.49</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.3.1">23.22</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.4.1">38.77</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.5"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.5.1">18.37</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.6"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.6.1">72.21</span></td>
<td class="ltx_td ltx_align_center" id="S7.T4.1.23.22.7"><span class="ltx_text ltx_font_bold" id="S7.T4.1.23.22.7.1">34.40</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.24.23.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.24.23.1.1">Multi-Relation Approach</span></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.2"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.3"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.4"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.5"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.6"></td>
<td class="ltx_td ltx_border_t" id="S7.T4.1.24.23.7"></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.1">AutoGSR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.25.24.2.1">54.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.25.24.3.1">19.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.25.24.4.1">33.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S7.T4.1.25.24.5.1">15.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.6"><span class="ltx_text ltx_font_bold" id="S7.T4.1.25.24.6.1">71.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.1.25.24.7"><span class="ltx_text ltx_font_bold" id="S7.T4.1.25.24.7.1">31.02</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.1">MGIR</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.4"><span class="ltx_text ltx_font_bold" id="S7.T4.1.26.25.4.1">36.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.5"><span class="ltx_text ltx_font_bold" id="S7.T4.1.26.25.5.1">17.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S7.T4.1.26.25.7">-</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Benchmark Models</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">To evaluate the performance of our models, we compared them with 17 other models, spanning single-session, cross-session, and multi-relation approaches.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">Single-Session Approach</span>:</p>
<ul class="ltx_itemize" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i1.p1.1.1">POP</span> recommends the most popular items.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i2.p1.1.1">Item-KNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Sarwar etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib29" title="">2001</a>)</cite> recommends items based on the cosine similarity between the items in the current session and candidate items.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i3.p1">
<p class="ltx_p" id="S7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i3.p1.1.1">FPMC</span> <cite class="ltx_cite ltx_citemacro_citep">(Rendle etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib28" title="">2010</a>)</cite> uses both Markov chains and matrix factorization to incorporate personalized and general user information.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i4.p1">
<p class="ltx_p" id="S7.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i4.p1.1.1">GRU4REC</span> <cite class="ltx_cite ltx_citemacro_citep">(Hidasi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib14" title="">2015</a>)</cite> leverages the memory of GRU by modeling the entire sequence.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i5.p1">
<p class="ltx_p" id="S7.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i5.p1.1.1">NARM</span> <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib20" title="">2017</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S7.I1.i5.p1.1.2">STAMP</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib21" title="">2018</a>)</cite> use the attention mechanism to capture the userâ€™s current and general interest.</p>
</div>
</li>
<li class="ltx_item" id="S7.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I1.i6.p1">
<p class="ltx_p" id="S7.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I1.i6.p1.1.1">SRGNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib39" title="">2019</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S7.I1.i6.p1.1.2">LESSER</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen and Wong, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib5" title="">2020</a>)</cite> convert each session into a graph without using inter-session information.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.1">Cross-Session Approach</span>:</p>
<ul class="ltx_itemize" id="S7.I2">
<li class="ltx_item" id="S7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I2.i1.p1">
<p class="ltx_p" id="S7.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I2.i1.p1.1.1">CSRM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib35" title="">2019</a>)</cite> incorporates relevant information from neighboring sessions via a memory network.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I2.i2.p1">
<p class="ltx_p" id="S7.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I2.i2.p1.1.1">CoSAN</span> <cite class="ltx_cite ltx_citemacro_citep">(Luo etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib24" title="">2020</a>)</cite> uses a multi-head attention mechanism to build dynamic representations of items by merging item representations from collaborative sessions.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I2.i3.p1">
<p class="ltx_p" id="S7.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I2.i3.p1.1.1">GCE-GNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib36" title="">2020</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S7.I2.i3.p1.1.2">MTD</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib16" title="">2021</a>)</cite> simultaneously focus on both inter-session and intra-session dependencies.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I2.i4.p1">
<p class="ltx_p" id="S7.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I2.i4.p1.1.1">COTREC</span> <cite class="ltx_cite ltx_citemacro_citep">(Xia etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib37" title="">2021a</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S7.I2.i4.p1.1.2">S2-DHCN</span> <cite class="ltx_cite ltx_citemacro_citep">(Xia etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib38" title="">2021b</a>)</cite> employ a global argumentative view of items to extract informative self-supervision signals.</p>
</div>
</li>
<li class="ltx_item" id="S7.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I2.i5.p1">
<p class="ltx_p" id="S7.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I2.i5.p1.1.1">CARES</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib43" title="">2023</a>)</cite> uses contextual information from multiple sessions to enhance recommendations, even during evaluation on the test set.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.1">Multi-Relation Approach</span>:</p>
<ul class="ltx_itemize" id="S7.I3">
<li class="ltx_item" id="S7.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S7.I3.i1.p1">
<p class="ltx_p" id="S7.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.I3.i1.p1.1.1">AutoGSR</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib4" title="">2022</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S7.I3.i1.p1.1.2">MGIR</span> <cite class="ltx_cite ltx_citemacro_citep">(Han etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib12" title="">2022</a>)</cite> both learn multi-faceted item relations to improve session representation. Note that MGIR uses inter-session information, while AutoGSR does not.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Global Results Table</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#S7.T4" title="Table 4 â€£ 7.1. Types of Benchmark Approaches â€£ 7. Comparison with the State of the Art â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a> compares the performance of our transformer models against several state-of-the-art models across the Diginetica, Tmall, and Yoochoose 1/64 datasets. The results highlight the effectiveness of our models, particularly those trained using the <span class="ltx_glossaryref" title="">SMM</span> method. Notably, BERT-SMM consistently outperforms all other models in the single-session approach category across the three datasets. When comparing our models to state-of-the-art cross-session and multi-relation methods, BERT-SMM remains highly competitive.</p>
</div>
<div class="ltx_para" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1">DeBERTa-SMM also demonstrates competitive results, coming in just behind BERT-SMM in most cases. For example, on the Tmall dataset, DeBERTa-SMM achieves a Precision@20 of 36.08, even outperforming several cross-session and multi-relation models.</p>
</div>
<div class="ltx_para" id="S7.SS3.p3">
<p class="ltx_p" id="S7.SS3.p3.1">These results underscore the strong performance of our optimized transformer models, particularly BERT-SMM, even when compared to models that have access to more comprehensive session or user-level data. The ability of our models to outperform in the single-session category demonstrates the effectiveness of the <span class="ltx_glossaryref" title="">SMM</span> method. With further enhancements, such as incorporating user-level or multi-session data, we believe our models have the potential to rival and even surpass state-of-the-art cross-session and multi-relation approaches.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we introduced Sequential Masked Modeling (<span class="ltx_glossaryref" title="">SMM</span>), a novel masking technique specifically designed for encoder-only transformer models, along with broader architectural improvements that were applied to both encoder- and decoder-based models. By using data augmentation through sliding windows and masking the penultimate token in augmented sequences, <span class="ltx_glossaryref" title="">SMM</span> significantly improved next-click prediction performance. In combination with the architectural enhancements, we demonstrated strong performance gains in several key metrics across three widely-used session-based recommendation datasets: Yoochoose 1/64, Diginetica, and Tmall.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Our experimental results showed that the proposed BERT-SMM and DeBERTa-SMM models consistently outperformed traditional single-session approaches and remained competitive with state-of-the-art cross-session and multi-relation methods, despite being limited to single-session data. These findings validate the effectiveness of the <span class="ltx_glossaryref" title="">SMM</span> technique in capturing sequential dependencies and improving recommendation performance in session-based environments.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Furthermore, the success of BERT-SMM and DeBERTa-SMM in the single-session category suggests that there is significant potential for enhancing performance even further by incorporating additional context, such as cross-session data or multi-relational information. Future work could explore how these models perform when extended to utilize user-level histories, or how <span class="ltx_glossaryref" title="">SMM</span> can be adapted to improve performance in other recommendation domains. Overall, this study provides strong evidence for the efficacy of encoder-only transformer architectures in session-based recommendation tasks when enhanced by <span class="ltx_glossaryref" title="">SMM</span>, offering a promising direction for future research in this area.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn etÂ al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dasom Ahn, Sangwon Kim, Hyunsu Hong, and ByoungÂ Chul Ko. 2023.

</span>
<span class="ltx_bibblock">Star-transformer: A spatio-temporal cross attention transformer for human action recognition. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>. 3330â€“3339.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
JimmyÂ Lei Ba, JamieÂ Ryan Kiros, and GeoffreyÂ E. Hinton. 2016.

</span>
<span class="ltx_bibblock">Layer Normalization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1607.06450Â [stat.ML]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1607.06450" title="">https://arxiv.org/abs/1607.06450</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jingfan Chen, Guanghui Zhu, Haojun Hou, Chunfeng Yuan, and Yihua Huang. 2022.

</span>
<span class="ltx_bibblock">AutoGSR: Neural architecture search for graph-based session recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval</em>. 1694â€“1704.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Wong (2020)</span>
<span class="ltx_bibblock">
Tianwen Chen and Raymond Chi-Wing Wong. 2020.

</span>
<span class="ltx_bibblock">Handling information loss of graph neural networks for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 1172â€“1180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cloudera Fast Forward Labs (2021)</span>
<span class="ltx_bibblock">
Cloudera Fast Forward Labs. 2021.

</span>
<span class="ltx_bibblock">Session-based Recommender Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://session-based-recommenders.fastforwardlabs.com/" title="">https://session-based-recommenders.fastforwardlabs.com/</a>
</span>
<span class="ltx_bibblock">FF19 Â· Â©2021 Cloudera, Inc. All rights reserved.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">deÂ Souza PereiraÂ Moreira etÂ al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Gabriel de Souza PereiraÂ Moreira, Sara Rabhi, JeongÂ Min Lee, Ronay Ak, and Even Oldridge. 2021.

</span>
<span class="ltx_bibblock">Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 15th ACM conference on recommender systems</em>. 143â€“153.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dias etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
M. Dias, J. Lozano, and A. Panigrahi. 2005.

</span>
<span class="ltx_bibblock">WEB RECOMMENDATION SYSTEM BASED ON A MARKOV-CHAIN MODEL. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 7th International Conference on Enterprise Information Systems</em>. SCITEPRESS - Science and Technology Publications, Miami, USA, 59â€“66.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5220/0002550700590066" title="">https://doi.org/10.5220/0002550700590066</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diginetica (2016)</span>
<span class="ltx_bibblock">
Diginetica. 2016.

</span>
<span class="ltx_bibblock">Diginetica Dataset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://darel13712.github.io/rs_datasets/Datasets/diginetica/" title="">https://darel13712.github.io/rs_datasets/Datasets/diginetica/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golovneva etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar. 2024.

</span>
<span class="ltx_bibblock">Contextual Position Encoding: Learning to Count Whatâ€™s Important.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.18719Â [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.18719" title="">https://arxiv.org/abs/2405.18719</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han etÂ al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qilong Han, Chi Zhang, Rui Chen, Riwei Lai, Hongtao Song, and Li Li. 2022.

</span>
<span class="ltx_bibblock">Multi-faceted global item relation learning for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 1705â€“1715.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020.

</span>
<span class="ltx_bibblock">Deberta: Decoding-enhanced bert with disentangled attention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv preprint arXiv:2006.03654</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidasi etÂ al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015.

</span>
<span class="ltx_bibblock">Session-based recommendations with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">arXiv preprint arXiv:1511.06939</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hidasi etÂ al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016.

</span>
<span class="ltx_bibblock">Incorporating Dwell Time in Session-Based Recommendations with Recurrent Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 10th ACM Conference on Recommender Systems</em>. ACM, 113â€“120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen, Liefeng Bo, Jiashu Zhao, and JimmyÂ Xiangji Huang. 2021.

</span>
<span class="ltx_bibblock">Graph-enhanced multi-task learning of multi-level transition dynamics for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol.Â 35. 4123â€“4130.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
AlbertÂ Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, etÂ al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">arXiv preprint arXiv:2310.06825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landia etÂ al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nick Landia, Bruce Ferwerda, Saikishore Kalloori, Abhishek Srivastava, Frederick Cheung, and Donna North. 2022.

</span>
<span class="ltx_bibblock">RecSys Challenge 2022: Fashion Recommendations.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://recsys.acm.org/recsys22/challenge/" title="">https://recsys.acm.org/recsys22/challenge/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Hsiang (2020)</span>
<span class="ltx_bibblock">
Jieh-Sheng Lee and Jieh Hsiang. 2020.

</span>
<span class="ltx_bibblock">Patent claim generation by fine-tuning OpenAI GPT-2.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">World Patent Information</em> 62 (2020), 101983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.

</span>
<span class="ltx_bibblock">Neural attentive session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</em>. 1419â€“1428.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018.

</span>
<span class="ltx_bibblock">STAMP: short-term attention/memory priority model for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1831â€“1839.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015.

</span>
<span class="ltx_bibblock">Recommender system application developments: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Decision Support Systems</em> 74 (2015), 12â€“32.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.dss.2015.03.008" title="">https://doi.org/10.1016/j.dss.2015.03.008</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yichao Lu, Zhaolin Gao, Zhaoyue Cheng, Jianing Sun, Bradley Brown, Guangwei Yu, Anson Wong, Felipe PÃ©rez, and Maksims Volkovs. 2022.

</span>
<span class="ltx_bibblock">Session-based recommendation with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the Recommender Systems Challenge 2022</em>. 29â€“33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo etÂ al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Anjing Luo, Pengpeng Zhao, Yanchi Liu, Fuzhen Zhuang, Deqing Wang, Jiajie Xu, Junhua Fang, and VictorÂ S Sheng. 2020.

</span>
<span class="ltx_bibblock">Collaborative Self-Attention Network for Session-based Recommendation.. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">IJCAI</em>. 2591â€“2597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke etÂ al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zachary DeVito, MartÃ­n Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock">PyTorch: An Imperative Style, High-Performance Deep Learning Library.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1912.01703Â [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajjwal1 (2023)</span>
<span class="ltx_bibblock">
Prajjwal1. 2023.

</span>
<span class="ltx_bibblock">bert-medium.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/prajjwal1/bert-medium" title="">https://huggingface.co/prajjwal1/bert-medium</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press and Wolf (2016)</span>
<span class="ltx_bibblock">
Ofir Press and Lior Wolf. 2016.

</span>
<span class="ltx_bibblock">Using the Output Embedding to Improve Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1608.05859</em> (2016).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.1608.05859" title="">https://doi.org/10.48550/arXiv.1608.05859</a>
</span>
<span class="ltx_bibblock">To appear in EACL 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle etÂ al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010.

</span>
<span class="ltx_bibblock">Factorizing personalized markov chains for next-basket recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 19th international conference on World wide web</em>. 811â€“820.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarwar etÂ al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001.

</span>
<span class="ltx_bibblock">Item-based collaborative filtering recommendation algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 10th international conference on World Wide Web</em>. 285â€“295.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021.

</span>
<span class="ltx_bibblock">RoFormer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2104.09864</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2104.09864" title="">https://arxiv.org/pdf/2104.09864</a>
</span>
<span class="ltx_bibblock">Accessed: 2024-07-08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tmall (2015)</span>
<span class="ltx_bibblock">
Tmall. 2015.

</span>
<span class="ltx_bibblock">Tmall Dataset for Recommendation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://competitions.codalab.org/competitions/11161" title="">https://competitions.codalab.org/competitions/11161</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-06-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al<span class="ltx_text" id="bib.bib32.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.1">arXiv preprint arXiv:2302.13971</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VeliÄkoviÄ‡ etÂ al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017.

</span>
<span class="ltx_bibblock">Graph attention networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:1710.10903</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Meirui Wang, Pengjie Ren, Lei Mei, Zhumin Chen, Jun Ma, and Maarten DeÂ Rijke. 2019.

</span>
<span class="ltx_bibblock">A collaborative session-based recommendation approach with parallel memory modules. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</em>. 345â€“354.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui Qiu. 2020.

</span>
<span class="ltx_bibblock">Global context enhanced graph neural networks for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>. 169â€“178.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia etÂ al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Xin Xia, Hongzhi Yin, Junliang Yu, Yingxia Shao, and Lizhen Cui. 2021a.

</span>
<span class="ltx_bibblock">Self-supervised graph co-training for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the 30th ACM international conference on information &amp; knowledge management</em>. 2180â€“2190.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia etÂ al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. 2021b.

</span>
<span class="ltx_bibblock">Self-supervised hypergraph convolutional networks for session-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol.Â 35. 4503â€“4511.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, VictorÂ S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019.

</span>
<span class="ltx_bibblock">Graph contextualized self-attention network for session-based recommendation.. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">IJCAI</em>, Vol.Â 19. 3940â€“3946.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoochoose (2015)</span>
<span class="ltx_bibblock">
Yoochoose. 2015.

</span>
<span class="ltx_bibblock">Yoochoose Dataset.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://darel13712.github.io/rs_datasets/Datasets/yoochoose/" title="">https://darel13712.github.io/rs_datasets/Datasets/yoochoose/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich. 2019.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information Processing Systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Wang (2023)</span>
<span class="ltx_bibblock">
Xiaoyan Zhang and Teng Wang. 2023.

</span>
<span class="ltx_bibblock">A Graph Convolutional Network for Session Recommendation Model Based on Improved Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">IEEE Access</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhihui Zhang, Jianxiang Yu, and Xiang Li. 2023.

</span>
<span class="ltx_bibblock">Context-Aware Session-Based Recommendation with Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">2023 IEEE International Conference on Knowledge Graph (ICKG)</em>. IEEE, 35â€“44.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Ablation study</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Our experiments demonstrate that the improved BERT architecture, combined with the <span class="ltx_glossaryref" title="">SMM</span> technique, delivers the best overall performance across the three datasets. In this section, we present an ablation study to assess the individual contribution of each optimization technique integrated into BERT-SMM. The results of this analysis, visualized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.F3" title="Figure 3 â€£ A.1. Ablation study â€£ Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, show how progressively adding these optimizations boosts performance on Diginetica.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.1">Different categories of ablation:</span></p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">Base</span>: This represents the standard BERT architecture as described in the original paper <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib8" title="">2018</a>)</cite>, trained with the SMM objective.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">Weight Tying</span>: Reduces the number of parameters by sharing weights between input embeddings and the output layer.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">Pre-layer Normalization</span>: Stabilizes training and improves convergence by applying normalization before attention layers.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">Contextual Positional Encoding</span>: Enhances positional embeddings by incorporating contextual information instead of fixed token positions.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="A1.F3.g1" src="extracted/5923458/images/ablation_plot.png" width="349"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Performance impact of various enhancement techniques applied to the BERT architecture trained with the Sequential Masked Modeling objective.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Optimized BERT Architecture</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.F4" title="Figure 4 â€£ A.2. Optimized BERT Architecture â€£ Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a> shows the optimized BERT architecture that incorporates several key enhancements. These include pre-normalization with RMSNorm, Weight Tying, and Contextual Positional Encoding within the attention module. This design draws inspiration from modern transformer architectures, such as Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib32" title="">2023</a>)</cite>, to boost performance and stability.</p>
</div>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="844" id="A1.F4.g1" src="extracted/5923458/images/bert_opti_arch.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Optimized BERT model architecture.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Hyperparameter Setup</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">Our model configuration is based on the BERT-Medium architecture from <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.1">HuggingFace</span> <cite class="ltx_cite ltx_citemacro_citep">(Prajjwal1, <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#bib.bib26" title="">2023</a>)</cite>. The training setup was consistent across all experiments to ensure fair comparison. All hyperparameter values can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11150v1#A1.T5" title="Table 5 â€£ A.3. Hyperparameter Setup â€£ Appendix A Appendix â€£ Optimizing Encoder-Only Transformers for Session-Based Recommendation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>, where we only conducted a search over the batch size. The model includes a maximum sequence length of 30, GeLU as the activation function, and a hidden size of 512. The optimizer used is Adam with an initial learning rate of 5e-5. Additionally, we applied a dynamic learning rate schedule that adjusts depending on the epoch, as detailed below.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.2">The following learning rate scheduler was used: A custom learning rate schedule was applied using the function <math alttext="\lambda_{\text{lr}}=0.1" class="ltx_Math" display="inline" id="A1.SS3.p2.1.m1.1"><semantics id="A1.SS3.p2.1.m1.1a"><mrow id="A1.SS3.p2.1.m1.1.1" xref="A1.SS3.p2.1.m1.1.1.cmml"><msub id="A1.SS3.p2.1.m1.1.1.2" xref="A1.SS3.p2.1.m1.1.1.2.cmml"><mi id="A1.SS3.p2.1.m1.1.1.2.2" xref="A1.SS3.p2.1.m1.1.1.2.2.cmml">Î»</mi><mtext id="A1.SS3.p2.1.m1.1.1.2.3" xref="A1.SS3.p2.1.m1.1.1.2.3a.cmml">lr</mtext></msub><mo id="A1.SS3.p2.1.m1.1.1.1" xref="A1.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS3.p2.1.m1.1.1.3" xref="A1.SS3.p2.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.1.m1.1b"><apply id="A1.SS3.p2.1.m1.1.1.cmml" xref="A1.SS3.p2.1.m1.1.1"><eq id="A1.SS3.p2.1.m1.1.1.1.cmml" xref="A1.SS3.p2.1.m1.1.1.1"></eq><apply id="A1.SS3.p2.1.m1.1.1.2.cmml" xref="A1.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS3.p2.1.m1.1.1.2.1.cmml" xref="A1.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="A1.SS3.p2.1.m1.1.1.2.2.cmml" xref="A1.SS3.p2.1.m1.1.1.2.2">ğœ†</ci><ci id="A1.SS3.p2.1.m1.1.1.2.3a.cmml" xref="A1.SS3.p2.1.m1.1.1.2.3"><mtext id="A1.SS3.p2.1.m1.1.1.2.3.cmml" mathsize="70%" xref="A1.SS3.p2.1.m1.1.1.2.3">lr</mtext></ci></apply><cn id="A1.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="A1.SS3.p2.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.1.m1.1c">\lambda_{\text{lr}}=0.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.p2.1.m1.1d">italic_Î» start_POSTSUBSCRIPT lr end_POSTSUBSCRIPT = 0.1</annotation></semantics></math> if the epoch is 3 or higher, otherwise <math alttext="\lambda_{\text{lr}}=1.0" class="ltx_Math" display="inline" id="A1.SS3.p2.2.m2.1"><semantics id="A1.SS3.p2.2.m2.1a"><mrow id="A1.SS3.p2.2.m2.1.1" xref="A1.SS3.p2.2.m2.1.1.cmml"><msub id="A1.SS3.p2.2.m2.1.1.2" xref="A1.SS3.p2.2.m2.1.1.2.cmml"><mi id="A1.SS3.p2.2.m2.1.1.2.2" xref="A1.SS3.p2.2.m2.1.1.2.2.cmml">Î»</mi><mtext id="A1.SS3.p2.2.m2.1.1.2.3" xref="A1.SS3.p2.2.m2.1.1.2.3a.cmml">lr</mtext></msub><mo id="A1.SS3.p2.2.m2.1.1.1" xref="A1.SS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS3.p2.2.m2.1.1.3" xref="A1.SS3.p2.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.2.m2.1b"><apply id="A1.SS3.p2.2.m2.1.1.cmml" xref="A1.SS3.p2.2.m2.1.1"><eq id="A1.SS3.p2.2.m2.1.1.1.cmml" xref="A1.SS3.p2.2.m2.1.1.1"></eq><apply id="A1.SS3.p2.2.m2.1.1.2.cmml" xref="A1.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.SS3.p2.2.m2.1.1.2.1.cmml" xref="A1.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="A1.SS3.p2.2.m2.1.1.2.2.cmml" xref="A1.SS3.p2.2.m2.1.1.2.2">ğœ†</ci><ci id="A1.SS3.p2.2.m2.1.1.2.3a.cmml" xref="A1.SS3.p2.2.m2.1.1.2.3"><mtext id="A1.SS3.p2.2.m2.1.1.2.3.cmml" mathsize="70%" xref="A1.SS3.p2.2.m2.1.1.2.3">lr</mtext></ci></apply><cn id="A1.SS3.p2.2.m2.1.1.3.cmml" type="float" xref="A1.SS3.p2.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.2.m2.1c">\lambda_{\text{lr}}=1.0</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.p2.2.m2.1d">italic_Î» start_POSTSUBSCRIPT lr end_POSTSUBSCRIPT = 1.0</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Hyperparameter configuration</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.2.2.1">Batch Sizes</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.2.2.2">32, 64, 128, 252</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.3.3.1">Optimizer</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.3.3.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.4.4.1">Learning Rate</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.4.4.2">5e-5</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.5.5.1">Model Max Length</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.5.5.2">30</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.6.6.1">Activation Function</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.6.6.2">GeLU</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.7.7.1">Hidden Size</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.7.7.2">512</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.8.8.1">Number of Attention Heads</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T5.1.8.8.2">8</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T5.1.9.9.1">Depth (Layers)</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.9.9.2">8</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 15 00:13:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
