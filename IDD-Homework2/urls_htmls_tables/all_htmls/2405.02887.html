<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English</title>
<!--Generated on Mon Sep  2 15:39:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.02887v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S1" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S2" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S2.SS1" title="In 2 Related Work ‣ Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Sentiment Analysis and Emotion Detection from Text</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S2.SS2" title="In 2 Related Work ‣ Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Challenges in Multilingual Sentiment Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S2.SS3" title="In 2 Related Work ‣ Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>BERT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S2.SS4" title="In 2 Related Work ‣ Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>XLM RoBERTa</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S3" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S4" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S5" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S6" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S7" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Observations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S8" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#S9" title="In Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Code Availability</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aekansh Kathunia
<br class="ltx_break"/>IIIT Delhi
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">aekansh21127@iiitd.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Kaif
<br class="ltx_break"/>IIIT Delhi
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">kaif21067@iiitd.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nalin Arora
<br class="ltx_break"/>IIIT Delhi
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1" style="font-size:90%;">nalin21478@iiitd.ac.in</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">N Narotam
<br class="ltx_break"/>IIIT Delhi
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1" style="font-size:90%;">narotam21477@iiitd.ac.in</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">People communicate in more than 7,000 languages around the world, with around 780 languages spoken in India alone. Despite this linguistic diversity, research on Sentiment Analysis has predominantly focused on English text data, resulting in a disproportionate availability of sentiment resources for English. This paper examines the performance of transformer models in Sentiment Analysis tasks across multilingual datasets and text that has undergone machine translation. By comparing the effectiveness of these models in different linguistic contexts, we gain insights into their performance variations and potential implications for sentiment analysis across diverse languages. We also discuss the shortcomings and potential for future work towards the end.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The term <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Sentiment Analysis</span> refers to the process of analyzing text to determine the emotional tone of the message. More generally, it can be understood as assessing an individual towards a particular target.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Machine translation</span> refers to the conversion of text from a source language to a target language via the use of computer algorithms.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Bert and XLM Roberta are Large language models based on the transformer architecture introduced by Google, trained on a huge corpus of data, ideal for fine-tuning downstream tasks such as Sentiment Analysis and Machine Translation.
Given the accessibility and usefulness of these models, they find their application in various languages other than English and various multilingual settings, where models are tuned to understand context in multiple languages.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Due to easy accessibility and the plethora of literature available in English, tasks like <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Sentiment Analysis</span> have extensively been researched on English texts, which naturally leads to many sentiment resources for English texts but less so for texts in other languages.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In our project, we aim to compare <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Sentiment Analysis</span> performance on the original language texts for the languages French, German, Spanish, Japanese and Chinese while also comparing machine translation performance of models across different languages.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Other work done with similar objectives, we believe, often falls short of creating a robust pipeline to process multilingual datasets and often implements simple rudimentary pipelines that don’t use underlying datasets to the fullest. We identify certain gaps and interesting areas in which we could expand existing research done on this topic and thus present the major contributions of our project:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present robust pipelines that incorporate and compare various state-of-the-art sentiment analysis and machine translation models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We provide domain-tuned versions of large language models on a subset of the <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Multilingual Amazon Reviews Corpus<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S1.I1.i2.p1.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib10" title="">10</a><span class="ltx_text ltx_font_medium" id="S1.I1.i2.p1.1.1.2.2">]</span></cite></span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We analyse the translation models in different languages by their ability to recreate the baseline for sentiment analysis of English models. This allows us to understand the progress of NLP in different languages compared to English.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We explore if it is viable to use cross-lingual over uni-lingual models and if significant performance can be achieved by machine translation to transform the dataset into English, in which models are, in general, better.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p6.2">For all tasks, we use transformer-based models that have been pre-trained on an enormous corpus of text prior to our deployment and pertaining.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S2.F1.g1" src="extracted/5828748/images/training_pipeline-nobg.png" width="685"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Training Pipeline</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sentiment Analysis and Emotion Detection from Text</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">There have been several studies exploring sentiment analysis and emotion detection techniques being applied to textual data. The paper from Nandwani and Verma provides insights into different methodologies used for analyzing sentiments and detecting emotions. They delve into traditional machine learning algorithms and deep learning models, highlighting their strengths and limitations on the above-mentioned task.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Mohammad et al. examine how translation alters sentiment, focusing on the impact of machine translation on sentiment analysis across languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib13" title="">13</a>]</cite>. They observe that machine translation can significantly alter the sentiment expressed in a sentence and highlight the challenges of accurately capturing sentiment in a multilingual setting.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Araujo et al. evaluate machine translation for multilingual sentence-level sentiment analysis and assess the effectiveness of different machine translation models in preserving sentiment across languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib2" title="">2</a>]</cite>. However, their findings underscore the importance of considering the quality of machine translation while conducting sentiment analysis in multilingual contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Challenges in Multilingual Sentiment Analysis</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">One of the key challenges identified by both Mohammad et al. and Araujo et al. in accurately interpreting emotions from text is the inherent complexity of language, which includes nuances, context, and cultural disparities. This challenge is more inherent in a multilingual setting, where the translation of text to English may not fully capture the original sentiment or emotion.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Moreover, recent advancements in sentiment analysis and emotion detection have focused on integrating multimodal data and leveraging pre-trained language models to enhance accuracy. These developments are particularly relevant in the context of multilingual sentiment analysis, where incorporating additional modalities such as images or audio can provide valuable context for understanding emotions expressed in text.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Overall, the insights provided by Nandwani and Verma, Mohammad et al., and Araujo et al. offer valuable considerations for evaluating sentiment analysis across languages, both before and after machine translation to English.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>BERT</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib8" title="">8</a>]</cite>. It is based on a multi-layer bidirectional transformer encoder, which generates contextualized representations of input text. BERT is pre-trained on a large corpus of text and is fine-tuned on specific tasks, achieving state-of-the-art results in various natural language processing tasks.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For languages other than English, we used the following instances of Bert from hugging face:</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">1. bert-base-german-cased<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib5" title="">5</a>]</cite> - This is a German language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of German text.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">2. dccuchile/bert-base-spanish-wwm-cased<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib4" title="">4</a>]</cite> - This is a Spanish language model based on the BERT architecture. It was developed by the University of Chile and has been fine-tuned on a large corpus of Spanish text.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">3. dbmdz/bert-base-french-europeana-cased<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib16" title="">16</a>]</cite> - is a French language model based on the BERT architecture. It was developed by the researchers at the Center for Information and Language Processing (CIS), LMU Munich.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">4. cl-tohoku/bert-base-japanese<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib18" title="">18</a>]</cite> - This is a Japanese language model based on the BERT architecture. It was developed by Tohoku University and has been fine-tuned on a large corpus of Japanese text.</p>
</div>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1">5. bert-base-chinese(addition of original bert paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib9" title="">9</a>]</cite> - This is a Chinese language model based on the BERT architecture. It was developed by Google and has been fine-tuned on a large corpus of Chinese text.</p>
</div>
<div class="ltx_para" id="S2.SS3.p8">
<p class="ltx_p" id="S2.SS3.p8.1">BERT has been shown to be effective in sentiment classification tasks, achieving high accuracy by leveraging its contextualized representations and fine-tuning on specific datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib15" title="">15</a>]</cite>. BERT fine-tuning has led to remarkable state-of-the-art results on various downstream tasks, including sentiment analysis.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>XLM RoBERTa</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">XLM-RoBERTa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib6" title="">6</a>]</cite> is a multi-lingual language model that combines the strengths of XLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib7" title="">7</a>]</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib12" title="">12</a>]</cite>. The architecture is based on the RoBERTa model, with a modified XLM encoder that enables cross-lingual transfer learning. This allows XLM-RoBERTa to leverage pre-training in multiple languages and fine-tune on specific tasks, achieving state-of-the-art results in various natural language processing tasks.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">XLM-RoBERTa has been shown to be effective in sentiment classification tasks, achieving high accuracy in multiple languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib3" title="">3</a>]</cite>. By leveraging its multi-lingual capabilities and fine-tuning on our dataset, we achieve high performance in sentiment classification.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We meticulously constructed a robust pipeline to fulfill the project’s objectives of comparing transformer performance in sentiment analysis across multilingual data and machine-translated text. This pipeline used advanced transformer architectures such as BERT and XLM-RoBERTa.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">BERT, short for Bidirectional Encoder Representations from Transformers, is an encoder-only model that utilises masked language modeling and next-sentence prediction techniques. BERT models are primarily trained in a single language context.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">XLM-RoBERTa is an advancement over XLM, leveraging the robust RoBERTa architecture. Through extensive pre-training on a larger dataset and prolonged training sessions, XLM-RoBERTa excels in various NLP tasks. It inherits XLM’s cross-lingual capabilities and benefits from RoBERTa’s enhanced representation learning, making it highly proficient in handling multilingual data and achieving superior performance across diverse NLP tasks.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">To achieve our machine translation objective, we employed the OPUS-MT machine translation model, which utilizes state-of-the-art transformer-based neural machine translation techniques. By leveraging the usage of transformers, OPUS-MT achieves good-quality translations and fluency across multiple languages. This choice aligns perfectly with our project’s focus on evaluating transformer performance on machine-translated text, ensuring robustness and reliability in our analyses.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">To compare the model performance, we utilized F1-scores as the metric since they give a holistic review of the model’s performance across classes.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">The pipeline we used during the project is as follows, we first used 50000 entries from each language due to limited computational capability to fine-tune BERT and XLM-RoBERTa models and evaluate their performance using F1-score. In the second part we translated 20000 entries from each language using OPUS-MT to English and then combined them together to form a dataset of 100000 entries. We fine-tuned our BERT and XLM-RoBERTa models on this combined dataset and then evaluated the model’s performance using the F1-score as the evaluation metric.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">By employing these state-of-the-art transformer models within our pipeline, we aimed to comprehensively evaluate their efficacy in sentiment analysis tasks across multilingual datasets and machine-translated texts.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">Training corpus statistics. 200,000 reviews per language. Taken from the corpus review paper by Keung et al.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T1.4.1.1.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.2">En</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.3">De</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.4">Es</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.5">Fr</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.6">Ja</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.7">Zh</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.4.2.1.1">Number of products</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.2">196,745</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.3">189,148</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.4">179,076</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.5">183,345</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.6">185,436</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.4.2.1.7">164,540</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.3.2.1">Number of reviewers</th>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.2">185,541</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.3">171,620</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.4">150,938</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.5">157,922</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.6">164,776</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.3.2.7">132,246</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.4.3.1">Average characters/review</th>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.2">178.8</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.3">207.9</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.4">151.3</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.5">159.4</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.6">101.4</td>
<td class="ltx_td ltx_align_right" id="S3.T1.4.4.3.7">51.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.4.5.4.1">Average characters/review title</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.2">24.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.3">21.8</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.4">19.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.5">19.1</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.6">9.5</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.4.5.4.7">7.6</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The dataset used in this study is the <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Multilingual Amazon Reviews Corpus</span>, as described by Keung et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib10" title="">10</a>]</cite>. This corpus represents a rich collection of product reviews gathered from the Amazon platform, spanning multiple languages, including English, Spanish, French, German, Japanese, Chinese, and Italian.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The Multilingual Amazon Reviews Corpus contains reviews across a wide range of product categories, such as electronics, books, movies, home appliances, and more. Each review entry within the dataset is accompanied by comprehensive metadata, including the product ID, reviewer ID, review text, star rating, and review date. Additionally, the dataset contains information about the geographic location of reviewers, providing insights into regional variations in sentiment expression.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">One of the notable features of the Multilingual Amazon Reviews Corpus is its extensive coverage of languages and product categories, making it a valuable resource for studying sentiment analysis and multilingual natural language processing tasks. Researchers can leverage this dataset to explore the complexities and nuances of sentiment analysis across diverse linguistic and cultural contexts.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">In this study, we selected the Multilingual Amazon Reviews Corpus due to its multilingual nature and diverse product categories, which provided us with an opportunity to investigate the challenges and opportunities of sentiment analysis across different languages and domains.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Due to limited computational resources, we utilize a randomly sampled subset of the Multilingual Amazon Reviews Corpus, consisting of 50,000 samples of the languages French, German, Spanish, Japanese and Chinese while also ensuring that the data is balanced.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:285.4pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T2.2.1"><span class="ltx_text" id="S5.T2.2.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.2.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T2.2.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.2.1.1.1.1.1.1">Hugging Face Model ID</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.1.1.1.1.1.2">Language Specific</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T2.2.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.1.1.1.2.1.1">bert-base-german-cased</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.1.1.2.1.2">german_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.1.1.1.3.2.1">dccuchile/bert-base-spanish-wwm-cased</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.1.1.1.3.2.2">spanish_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.1.1.1.4.3.1">dbmdz/bert-base-french-europeana-cased</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.1.1.1.4.3.2">french_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.5.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.1.1.1.5.4.1">cl-tohoku/bert-base-japanese</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.1.1.1.5.4.2">japanese_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.6.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.1.1.1.6.5.1">bert-base-chinese</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.1.1.1.6.5.2">chinese_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.7.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.1.1.1.7.6.1">bert-base-uncased</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.1.1.1.7.6.2">english_BERT</span></span>
<span class="ltx_tr" id="S5.T2.2.1.1.1.8.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T2.2.1.1.1.8.7.1">xlm-roberta-base</span>
<span class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.2.1.1.1.8.7.2">Multi-Lingual</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Mapping Hugging Face Model IDs to Language Specificity</span></figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We tune uni-lingual BERT models for each language and also a multi-lingual XLM-RoBERTa model on our dataset to further assess the performance of these different languages across different languages. The performance of each model in each language is ultimately compared to the performance of BERT and XLM-RoBERTa on English models tuned on the dataset.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">For our evaluation, we observe the model performance on two tasks:</p>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Sentiment Classification:</span> Classify the review text into two classes, positive review and negative review.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Star Rating Prediction:</span> Predict the star rating of the review text.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_square" height="157" id="S5.F2.g1" src="extracted/5828748/images/pn_class_distribution.jpeg" width="138"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_square" height="157" id="S5.F2.g2" src="extracted/5828748/images/stars_class_distribution.jpeg" width="138"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S5.F2.3.2" style="font-size:90%;">Distribution of Labels</span></figcaption>
</figure>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The gold labels for Star Rating Prediction were already present in the dataset. However, the gold labels for Sentiment Classification were not present in the dataset and were obtained from the annotated ”hungnm/multilingual-amazon-review-sentiment-processed” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib1" title="">1</a>]</cite> dataset from huggingface.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">The model evaluation tasks were first performed on all of the sampled data. Then, 20,000 samples from each of the source languages were translated to the target language, English. The evaluation tasks were then repeated on the translated samples to assess post <span class="ltx_text ltx_font_italic" id="S5.p5.1.1">Machine Translation</span> performance.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.2" style="width:306.4pt;height:225pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T3.2.1"><span class="ltx_text" id="S6.T3.2.1.1" style="font-size:80%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.2.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T3.2.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.1.1.1">Language</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.1.1.2">Before Machine Translation</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.1.1.3">After Machine Translation</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T3.2.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.2.1.1.1">Spanish(ES)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.2.1.2.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.2.1.2.1.1.1.1">Spanish_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.2.1.1.2">0.90461</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.2.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.2.1.2.1.2.1.1">Spanish_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.2.1.2.2">0.88878</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.2.1.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.2.1.3.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.2.1.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.2.1.3.1.1.1.1">Spanish_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.3.1.1.2">0.89965</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.2.1.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.2.1.3.1.2.1.1">Spanish_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.2.1.3.1.2.2">0.89875</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.3.2.1.1">German (DE)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.3.2.2.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.3.2.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.3.2.2.1.1.1.1">German_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.2.1.1.2">0.90585</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.3.2.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.3.2.2.1.2.1.1">German_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.2.1.2.2">0.90730</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.3.2.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.3.2.3.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.3.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.3.2.3.1.1.1.1">German_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.3.1.1.2">0.89420</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.3.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.3.2.3.1.2.1.1">German_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.3.2.3.1.2.2">0.89672</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.4.3.1.1">French (FR)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.4.3.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.4.3.2.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.4.3.2.1.1.1.1">French_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.2.1.1.2">0.90294</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.4.3.2.1.2.1.1">French_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.2.1.2.2">0.87775</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.4.3.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.4.3.3.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.4.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.4.3.3.1.1.1.1">French_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.3.1.1.2">0.88740</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.4.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.4.3.3.1.2.1.1">French_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.4.3.3.1.2.2">0.89131</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.5.4.1.1">Chinese (ZH)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.5.4.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.5.4.2.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.5.4.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.5.4.2.1.1.1.1">Chinese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.2.1.1.2">0.86715</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.5.4.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.5.4.2.1.2.1.1">Chinese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.2.1.2.2">0.86844</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.5.4.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.5.4.3.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.5.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.5.4.3.1.1.1.1">Chinese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.3.1.1.2">0.83660</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.5.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.5.4.3.1.2.1.1">Chinese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.5.4.3.1.2.2">0.82391</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.6.5.1.1">Japanese(JA)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.6.5.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.6.5.2.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.6.5.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.6.5.2.1.1.1.1">Japanese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.2.1.1.2">0.90161</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.6.5.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.6.5.2.1.2.1.1">Japanese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.2.1.2.2">0.89231</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T3.2.1.1.1.6.5.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.2.1.1.1.6.5.3.1">
<span class="ltx_tr" id="S6.T3.2.1.1.1.6.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.6.5.3.1.1.1.1">Japanese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.3.1.1.2">0.83049</span></span>
<span class="ltx_tr" id="S6.T3.2.1.1.1.6.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T3.2.1.1.1.6.5.3.1.2.1.1">Japanese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.2.1.1.1.6.5.3.1.2.2">0.82479</span></span>
</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T3.5.1.1" style="font-size:113%;">Table 3</span>: </span><span class="ltx_text" id="S6.T3.6.2" style="font-size:113%;">Sentiment Classification</span></figcaption>
</figure>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.2" style="width:306.4pt;height:225pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T4.2.1"><span class="ltx_text" id="S6.T4.2.1.1" style="font-size:80%;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.2.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T4.2.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.1.1.1">Language</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.1.1.2">Before Machine Translation</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.1.1.3">After Machine Translation</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T4.2.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.2.1.1.1">Spanish(ES)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.2.1.2.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.2.1.2.1.1.1.1">Spanish_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.2.1.1.2">0.60697</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.2.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.2.1.2.1.2.1.1">Spanish_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.2.1.2.2">0.58337</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.2.1.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.2.1.3.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.2.1.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.2.1.3.1.1.1.1">Spanish_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.3.1.1.2">0.58155</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.2.1.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.2.1.3.1.2.1.1">Spanish_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.2.1.3.1.2.2">0.62267</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.3.2.1.1">German (DE)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.3.2.2.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.3.2.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.3.2.2.1.1.1.1">German_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.2.1.1.2">0.64942</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.3.2.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.3.2.2.1.2.1.1">German_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.2.1.2.2">0.62421</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.3.2.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.3.2.3.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.3.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.3.2.3.1.1.1.1">German_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.3.1.1.2">0.61658</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.3.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.3.2.3.1.2.1.1">German_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.3.2.3.1.2.2">0.63514</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.4.3.1.1">French (FR)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.4.3.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.4.3.2.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.4.3.2.1.1.1.1">French_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.2.1.1.2">0.61209</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.4.3.2.1.2.1.1">French_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.2.1.2.2">0.58065</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.4.3.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.4.3.3.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.4.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.4.3.3.1.1.1.1">French_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.3.1.1.2">0.56994</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.4.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.4.3.3.1.2.1.1">French_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.4.3.3.1.2.2">0.60932</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.5.4.1.1">Chinese (ZH)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.5.4.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.5.4.2.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.5.4.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.5.4.2.1.1.1.1">Chinese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.2.1.1.2">0.62267</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.5.4.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.5.4.2.1.2.1.1">Chinese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.2.1.2.2">0.54087</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.5.4.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.5.4.3.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.5.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.5.4.3.1.1.1.1">Chinese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.3.1.1.2">0.54025</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.5.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.5.4.3.1.2.1.1">Chinese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.5.4.3.1.2.2">0.61938</span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.6.5.1.1">Japanese(JA)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.6.5.2">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.6.5.2.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.6.5.2.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.6.5.2.1.1.1.1">Japanese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.2.1.1.2">0.60377</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.6.5.2.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.6.5.2.1.2.1.1">Japanese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.2.1.2.2">0.51262</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S6.T4.2.1.1.1.6.5.3">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.2.1.1.1.6.5.3.1">
<span class="ltx_tr" id="S6.T4.2.1.1.1.6.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.6.5.3.1.1.1.1">Japanese_XLM</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.3.1.1.2">0.52043</span></span>
<span class="ltx_tr" id="S6.T4.2.1.1.1.6.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.2.1.1.1.6.5.3.1.2.1.1">Japanese_BERT</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.2.1.1.1.6.5.3.1.2.2">0.59127</span></span>
</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T4.5.1.1" style="font-size:113%;">Table 4</span>: </span><span class="ltx_text" id="S6.T4.6.2" style="font-size:113%;">Star Rating Prediction</span></figcaption>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">It can be observed that machine translation didn’t affect if not significantly improve or derail performance of models in their downstream applications for languages like Spanish, German and French, which as clearly European languages and share a lot of semantic similarities with english. On the other hand Machine Translation significantly, affects the performance of languages Chinese and Japanese in a negative way, as they are significantly unique to english. However no model could reach the fine tuned performance of the english baseline of about 0.91 for english reviews, which could be due to gaps in Machine Translation models, as theoretically they translated sentences in english should be able to convey full meaning of original sentence and reach english benchmarks.However failure to do so even after intensive fine-tuning, leads us to conclude that either more robust fine-tuning on a much larger dataset rather than a subset or a more robust pipeline should help, which would have been outside the scope of our project given the resource, and time.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Observations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Table 1 and Table 2 show the F1 scores for our tasks for each of the language specific model before and after machine Translation.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We can infer that XLM-RoBERTA performs slightly better than BERT for each language dataset as expected due to XLM’s cross-lingual capabilities. For the task of Sentiment Analysis, the average F1 score over all the models was 0.89. Before Machine Translation, the German model got the best F1 score across all the models, while the Chinese performed the worst. After Translation, the performance of all the models degraded with the Japanese Model being the most affected. The Spanish model got the best F1 score after machine Translation.
For the task of Star Rating Prediction, the average F1 score over all the models was 0.61. Before Machine Translation, the German model outperformed all the models, while the Japanese performed the worst. After Translation, the performance of all the models degraded with the Japanese and Chinese Models being the most affected. The German model still got the best F1 score after machine Translation across all the models.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">During the course of the project, we developed language-specific models and models utilizing translated texts. There was no significant difference between the models developed during the project(language-specific and models using translated tests) as they achieved similar performance. However, the slight difference that occurred can be due to the following shortcomings.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Languages like Spanish, German, and French share many similarities with English regarding sentence structure sharing the same SVO word order with English. As a result, machine translation from these languages to English may preserve the original meaning well, leading to consistent sentiment analysis results. However, Asian languages like Japanese and Chinese have different linguistic structures. Japanese syntax follows SOV word order while Chinese sentence structure is characterized by its lack of inflectional morphology and grammatical markers, relying heavily on word order and context for conveying meaning. Machine translation may struggle to accurately capture such semantic meaning, leading to loss of information and thus Higher <span class="ltx_text ltx_font_bold" id="S8.p2.1.1">Semantic difference</span>.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">Asian cultures, for example, may have unique ways of conveying sentiment that differ from Western cultures.So <span class="ltx_text ltx_font_bold" id="S8.p3.1.1">Cultural difference</span> also plays a role in Machine Translation.
Lastly, The <span class="ltx_text ltx_font_bold" id="S8.p3.1.2">availability and quality of training data</span> may vary across languages. English sentiment analysis models may have been trained on larger and more diverse datasets compared to models for other languages. This discrepancy in training data quality can impact the effectiveness of sentiment analysis after machine translation, especially for languages with less available data.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1">In summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, and data availability. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1">To improve the work done, there can be further experimentation, which involves, firstly, fine-tuning machine translation models specifically for sentiment-related tasks. This could involve adding sentiment-specific data or annotations into the fine-tuning process to improve the efficacy of translations, especially for languages with high linguistic differences from English. Extensive literature is available to improve sentiment analysis of models by training them on general or domain-specific Knowledge graphs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib11" title="">11</a>]</cite>, such as ConceptNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib17" title="">17</a>]</cite>. Secondly, utilizing multimodal approaches incorporating visual and textual information for sentiment analysis across languages. Exploring how images or videos can complement machine-translated text to improve the performance of sentiment analysis, especially in languages where textual data may be limited or unreliable. Some preliminary work on this could be explored in work done by Yoon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02887v2#bib.bib19" title="">19</a>]</cite>. Finally, focusing on improving sentiment analysis performance in low-resource languages with a lack of training data. Experimentation can be done by exploring transfer, semi-supervised, or unsupervised learning to adapt to sentiment analysis tasks for languages with limited labelled data.</p>
</div>
<div class="ltx_para" id="S8.p6">
<p class="ltx_p" id="S8.p6.1">In summary, the effectiveness of machine translation in preserving sentiment and maintaining performance in downstream applications such as sentiment analysis depends on factors such as linguistic similarity, syntactic complexity, cultural differences, data availability, and translation quality. While machine translation may perform well for languages closely related to English, it may encounter challenges in accurately capturing sentiment for languages with greater linguistic and cultural differences.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Code Availability</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">We have published all our code files and processed the dataset to
<a class="ltx_ref ltx_href" href="https://github.com/Nalin21478/NLP-Project" title="">GitHub</a>. Fine-tuned model checkpoints of all the models can be found at <a class="ltx_ref ltx_href" href="https://drive.google.com/drive/folders/1R00drvoxtaUIxxDaJxbdy2yfBKd2mkQ5?usp=sharing" title="">Drive</a>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
hungnm/multilingual-amazon-review-sentiment-processed.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/hungnm/multilingual-amazon-review-sentiment-processed" style="font-size:90%;" title="">https://huggingface.co/datasets/hungnm/multilingual-amazon-review-sentiment-processed</a><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Matheus Araujo, Julio Reis, Adriano Pereira, and Fabricio Benevenuto.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">An evaluation of machine translation for multilingual sentence-level sentiment analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Proceedings of the 31st Annual ACM Symposium on Applied Computing</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, SAC ’16, page 1140–1145, New York, NY, USA, 2016. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">Proceedings of the Thirteenth Language Resources and Evaluation Conference</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, pages 258–266, Marseille, France, June 2022. European Language Resources Association.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge Pérez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Spanish pre-trained bert model and evaluation data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">PML4DC at ICLR 2020</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Branden Chan, Stefan Schweter, and Timo Möller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">German’s next language model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In Donia Scott, Nuria Bel, and Chengqing Zong, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">Proceedings of the 28th International Conference on Computational Linguistics</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pages 6788–6796, Barcelona, Spain (Online), Dec. 2020. International Committee on Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Unsupervised cross-lingual representation learning at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Alexis CONNEAU and Guillaume Lample.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Cross-lingual language model pretraining.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, volume 32. Curran Associates, Inc., 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">BERT: Pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Google.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">google-bert/bert-base-chinese · Hugging Face — huggingface.co.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/google-bert/bert-base-chinese" style="font-size:90%;" title="">https://huggingface.co/google-bert/bert-base-chinese</a><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.4.1" style="font-size:90%;">[Accessed 25-04-2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Phillip Keung, Yichao Lu, György Szarvas, and Noah A. Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">The multilingual Amazon reviews corpus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, pages 4563–4568, Online, Nov. 2020. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Jie Li, Xuan Li, Linmei Hu, Yirui Zhang, and Jinrui Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Knowledge graph enhanced language models for sentiment analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.2" style="font-size:90%;">The Semantic Web – ISWC 2023</span><span class="ltx_text" id="bib.bib11.5.3" style="font-size:90%;">, Lecture notes in computer science, pages 447–464. Springer Nature Switzerland, Cham, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">RoBERTa: A robustly optimized BERT pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Saif M. Mohammad, Mohammad Salameh, and Svetlana Kiritchenko.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">How translation alters sentiment.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">Journal of Artificial Intelligence Research</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 55:95–130, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Pansy Nandwani and Rupali Verma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">A review on sentiment analysis and emotion detection from text.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">Social Network Analysis and Mining</span><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">, 11(1):81, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Md Shohel Sayeed, Varsha Mohan, and Kalaiarasi Sonai Muthu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">BERT: A review of applications in sentiment analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">HighTech. Innov. J.</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 4(2):453–462, June 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Stefan Schweter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Europeana bert and electra models, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Robyn Speer, Joshua Chin, and Catherine Havasi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">ConceptNet 5.5: An open multilingual graph of general knowledge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
TOHOKU UNIVERSITY.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Bert (base japanese).
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tohoku-nlp/bert-base-japanese" style="font-size:90%;" title="">https://huggingface.co/tohoku-nlp/bert-base-japanese</a><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Multimodal speech emotion recognition using audio and text.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">2018.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  2 15:39:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
