<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation</title>
<!--Generated on Tue Jul  2 13:41:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.01280v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S1" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.SS0.SSS0.Px1" title="In 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Reinforcement Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.SS0.SSS0.Px2" title="In 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Softmax with Temperature</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.SS0.SSS0.Px3" title="In 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">RL for NAR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.SS0.SSS0.Px4" title="In 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Levenshtein Transformer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3.SS0.SSS0.Px1" title="In 3 Approaches ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Stepwise Reward Maximization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3.SS0.SSS0.Px2" title="In 3 Approaches ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Episodic Reward Maximization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3.SS0.SSS0.Px3" title="In 3 Approaches ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Temperature Control</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS1" title="In 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Data &amp; Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Baseline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">RL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS1.SSS0.Px4" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title">Computational Cost</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.SS2" title="In 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S5" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A1" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Pseudo code of stepwise reward maximization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A2" title="In Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Pseudo code of episodic reward maximization</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Reinforcement Learning for Edit-Based Non-Autoregressive
<br class="ltx_break"/>Neural Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Wang<sup class="ltx_sup" id="id9.9.id1"><span class="ltx_text ltx_font_italic" id="id9.9.id1.1">1</span></sup>  Tetsuro Morimura<sup class="ltx_sup" id="id10.10.id2">2</sup>  Ukyo Honda<sup class="ltx_sup" id="id11.11.id3">2</sup>  Daisuke Kawahara<sup class="ltx_sup" id="id12.12.id4">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id13.13.id5">1</sup> Waseda University  <sup class="ltx_sup" id="id14.14.id6">2</sup> CyberAgent
<br class="ltx_break"/><sup class="ltx_sup" id="id15.15.id7">1</sup> <span class="ltx_text ltx_font_typewriter" id="id8.8.1">{conan1024hao@akane., dkw@}waseda.jp
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.1.1"><span class="ltx_text ltx_font_serif" id="id8.8.1.1.1">2</span></sup></span> <span class="ltx_text ltx_font_typewriter" id="id16.16.id8">{morimura_tetsuro, honda_ukyo}@cyberagent.co.jp
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">Work done during internship at CyberAgent AI Lab.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">Non-autoregressive (NAR) language models are known for their low latency in neural machine translation (NMT).
However, a performance gap exists between NAR and autoregressive models due to the large decoding space and difficulty in capturing dependency between target words accurately.
Compounding this, preparing appropriate training data for NAR models is a non-trivial task, often exacerbating exposure bias.
To address these challenges, we apply reinforcement learning (RL) to Levenshtein Transformer, a representative edit-based NAR model, demonstrating that RL with self-generated data can enhance the performance of edit-based NAR models.
We explore two RL approaches: stepwise reward maximization and episodic reward maximization.
We discuss the respective pros and cons of these two approaches and empirically verify them.
Moreover, we experimentally investigate the impact of temperature setting on performance, confirming the importance of proper temperature setting for NAR models’ training.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.8">
<p class="ltx_p" id="p1.8.9"><span class="ltx_text ltx_font_bold" id="p1.8.9.1">Reinforcement Learning for Edit-Based Non-Autoregressive
<br class="ltx_break"/>Neural Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.8.8" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.8.8.8" style="width:0.0pt;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p1.8.8.8.8">
<span class="ltx_thead">
<span class="ltx_tr" id="p1.4.4.4.4.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4">Hao Wang<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.1.1">1</span></sup><span class="ltx_note ltx_role_thanks" id="p1.4.4.4.4.4.4.4.2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Work done during internship at CyberAgent AI Lab.</span></span></span>  Tetsuro Morimura<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.3.1">2</span></sup>  Ukyo Honda<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.4.1">2</span></sup>  Daisuke Kawahara<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.5"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.5.1">1</span></sup></span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.2"><sup class="ltx_sup" id="p1.6.6.6.6.6.2.1">1</sup> Waseda University  <sup class="ltx_sup" id="p1.6.6.6.6.6.2.2">2</sup> CyberAgent</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.1"><sup class="ltx_sup" id="p1.7.7.7.7.7.1.1">1</sup> <span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.7.1.2">{conan1024hao@akane., dkw@}waseda.jp</span></span></span>
<span class="ltx_tr" id="p1.8.8.8.8.8">
<span class="ltx_td ltx_align_center" id="p1.8.8.8.8.8.1"><sup class="ltx_sup" id="p1.8.8.8.8.8.1.1">2</sup> <span class="ltx_text ltx_font_typewriter" id="p1.8.8.8.8.8.1.2">{morimura_tetsuro, honda_ukyo}@cyberagent.co.jp</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Non-autoregressive (NAR) language models <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib6" title="">2018</a>)</cite> generate translations in parallel, enabling faster inference and having the potential for real-time translation applications.
However, despite their computational efficiency, NAR models have been observed to underperform autoregressive (AR) models due to the challenges posed by the large decoding space and difficulty in capturing dependency between target words accurately <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib6" title="">2018</a>)</cite>.
To bridge the performance gap, many NAR architectures and training methods have been proposed, including edit-based models like Insertion Transformer <cite class="ltx_cite ltx_citemacro_citep">(Stern et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib19" title="">2019</a>)</cite> and Levenshtein Transformer <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>.
Prior research has also explored knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Ghazvininejad et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib5" title="">2019</a>)</cite>, which is effective but introduces additional complexity.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Unlike AR models, preparing teacher data and designing appropriate training objectives have always been challenging for NAR models <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib11" title="">2023</a>)</cite>.
Teacher forcing with inappropriate teacher data may exacerbate the exposure bias problem <cite class="ltx_cite ltx_citemacro_citep">(Ranzato et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib15" title="">2016</a>)</cite>, affecting model performance.
Reinforcement learning (RL) is known for its ability to tackle the exposure bias <cite class="ltx_cite ltx_citemacro_citep">(Ranzato et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib15" title="">2016</a>)</cite> and alleviate the object mismatch issue <cite class="ltx_cite ltx_citemacro_citep">(Ding and Soricut, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib4" title="">2017</a>)</cite>.
Despite its importance, explorations of RL for NAR are still scarce.
<cite class="ltx_cite ltx_citemacro_citet">Shao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib18" title="">2021</a>)</cite> proposed a method for reducing the estimation variance.
However, this method is only applicable to NAR models with a fixed output length, which is unsuitable for edit-based models.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we empirically analyze conditions for performance improvement in applying RL to edit-based NAR models in neural machine translation (NMT).
Specifically, we focus on Levenshtein Transformer (LevT) <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>, a prominent edit-based NAR architecture that has shown promise in reducing decoding latency and flexible length adjustment.
We demonstrate that RL with self-generated data significantly improves LevT’s performance.
Importantly, our methods are orthogonal to existing research on NAR architectures, indicating potential for widespread applicability.
We explore two RL approaches: stepwise reward maximization, which computes rewards after each edit operation, and episodic reward maximization, which only computes rewards after all generations are completed.
We analyze these two approaches’ respective advantages and disadvantages and empirically verify them.
Furthermore, through a series of experiments, we investigate the impact of temperature settings on softmax sampling, aiming to identify the optimal temperature that strikes a balance between exploration and exploitation during the RL training process.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reinforcement Learning</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.6">Reinforcement learning has been widely applied to improve the performance of AR NMT models <cite class="ltx_cite ltx_citemacro_citep">(Ranzato et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib15" title="">2016</a>; Bahdanau et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib1" title="">2016</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib22" title="">2016</a>)</cite> because its ability to train models to optimize non-differentiable score functions and tackle the exposure bias problem <cite class="ltx_cite ltx_citemacro_citep">(Ranzato et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib15" title="">2016</a>)</cite>.
In practice, REINFORCE <cite class="ltx_cite ltx_citemacro_citep">(Williams, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib21" title="">1992</a>)</cite> with a baseline is commonly used for estimating the policy gradient, which can be computed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bigtriangledown_{\theta}L(\theta)\approx-(r(y)-b(s))\bigtriangledown_{\theta}%
\mathrm{log}\pi_{\theta}(y|s)," class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.4" xref="S2.E1.m1.4.4.1.1.4.cmml"><msub id="S2.E1.m1.4.4.1.1.4.1" xref="S2.E1.m1.4.4.1.1.4.1.cmml"><mo id="S2.E1.m1.4.4.1.1.4.1.2" xref="S2.E1.m1.4.4.1.1.4.1.2.cmml">▽</mo><mi id="S2.E1.m1.4.4.1.1.4.1.3" xref="S2.E1.m1.4.4.1.1.4.1.3.cmml">θ</mi></msub><mrow id="S2.E1.m1.4.4.1.1.4.2" xref="S2.E1.m1.4.4.1.1.4.2.cmml"><mi id="S2.E1.m1.4.4.1.1.4.2.2" xref="S2.E1.m1.4.4.1.1.4.2.2.cmml">L</mi><mo id="S2.E1.m1.4.4.1.1.4.2.1" xref="S2.E1.m1.4.4.1.1.4.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.4.2.3.2" xref="S2.E1.m1.4.4.1.1.4.2.cmml"><mo id="S2.E1.m1.4.4.1.1.4.2.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.4.2.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">θ</mi><mo id="S2.E1.m1.4.4.1.1.4.2.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.4.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml">≈</mo><mrow id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mrow id="S2.E1.m1.4.4.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1a" xref="S2.E1.m1.4.4.1.1.1.1.cmml">−</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.2.cmml">r</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">y</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.2.cmml">b</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">s</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><msub id="S2.E1.m1.4.4.1.1.2.3" xref="S2.E1.m1.4.4.1.1.2.3.cmml"><mo id="S2.E1.m1.4.4.1.1.2.3.2" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.4.4.1.1.2.3.2.cmml">▽</mo><mi id="S2.E1.m1.4.4.1.1.2.3.3" xref="S2.E1.m1.4.4.1.1.2.3.3.cmml">θ</mi></msub><mrow id="S2.E1.m1.4.4.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.3" xref="S2.E1.m1.4.4.1.1.2.2.3.cmml">log</mi><mo id="S2.E1.m1.4.4.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.cmml">⁢</mo><msub id="S2.E1.m1.4.4.1.1.2.2.4" xref="S2.E1.m1.4.4.1.1.2.2.4.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.4.2" xref="S2.E1.m1.4.4.1.1.2.2.4.2.cmml">π</mi><mi id="S2.E1.m1.4.4.1.1.2.2.4.3" xref="S2.E1.m1.4.4.1.1.2.2.4.3.cmml">θ</mi></msub><mo id="S2.E1.m1.4.4.1.1.2.2.2a" xref="S2.E1.m1.4.4.1.1.2.2.2.cmml">⁢</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.1.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.2.2.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.1.1.2.2.1.1.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.1.1.1.2" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.E1.m1.4.4.1.1.2.2.1.1.1.1" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.1.cmml">|</mo><mi id="S2.E1.m1.4.4.1.1.2.2.1.1.1.3" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.3.cmml">s</mi></mrow><mo id="S2.E1.m1.4.4.1.1.2.2.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><approx id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"></approx><apply id="S2.E1.m1.4.4.1.1.4.cmml" xref="S2.E1.m1.4.4.1.1.4"><apply id="S2.E1.m1.4.4.1.1.4.1.cmml" xref="S2.E1.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.4.1.1.cmml" xref="S2.E1.m1.4.4.1.1.4.1">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.4.1.2.cmml" xref="S2.E1.m1.4.4.1.1.4.1.2">▽</ci><ci id="S2.E1.m1.4.4.1.1.4.1.3.cmml" xref="S2.E1.m1.4.4.1.1.4.1.3">𝜃</ci></apply><apply id="S2.E1.m1.4.4.1.1.4.2.cmml" xref="S2.E1.m1.4.4.1.1.4.2"><times id="S2.E1.m1.4.4.1.1.4.2.1.cmml" xref="S2.E1.m1.4.4.1.1.4.2.1"></times><ci id="S2.E1.m1.4.4.1.1.4.2.2.cmml" xref="S2.E1.m1.4.4.1.1.4.2.2">𝐿</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝜃</ci></apply></apply><apply id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"><apply id="S2.E1.m1.4.4.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.3.2.cmml" xref="S2.E1.m1.4.4.1.1.2.3.2">▽</ci><ci id="S2.E1.m1.4.4.1.1.2.3.3.cmml" xref="S2.E1.m1.4.4.1.1.2.3.3">𝜃</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1"><minus id="S2.E1.m1.4.4.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1"></minus><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1"><minus id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1"></minus><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2"><times id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.1"></times><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.2.2">𝑟</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑦</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3"><times id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑠</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2"><times id="S2.E1.m1.4.4.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2"></times><ci id="S2.E1.m1.4.4.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.3">log</ci><apply id="S2.E1.m1.4.4.1.1.2.2.4.cmml" xref="S2.E1.m1.4.4.1.1.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.4.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.4">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.4.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.4.2">𝜋</ci><ci id="S2.E1.m1.4.4.1.1.2.2.4.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.4.3">𝜃</ci></apply><apply id="S2.E1.m1.4.4.1.1.2.2.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.1">conditional</csymbol><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.2">𝑦</ci><ci id="S2.E1.m1.4.4.1.1.2.2.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.1.1.3">𝑠</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\bigtriangledown_{\theta}L(\theta)\approx-(r(y)-b(s))\bigtriangledown_{\theta}%
\mathrm{log}\pi_{\theta}(y|s),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">▽ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_L ( italic_θ ) ≈ - ( italic_r ( italic_y ) - italic_b ( italic_s ) ) ▽ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y | italic_s ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.5">where <math alttext="r" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.1d">italic_r</annotation></semantics></math> is the reward function, <math alttext="b" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.2.m2.1d">italic_b</annotation></semantics></math> is the baseline, <math alttext="y" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.3.m3.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.3.m3.1d">italic_y</annotation></semantics></math> is a sample from policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px1.p1.4.m4.1a"><msub id="S2.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">π</mi><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2">𝜋</ci><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.4.m4.1c">\pi_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.4.m4.1d">italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and state <math alttext="s" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px1.p1.5.m5.1a"><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.5.m5.1b"><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.5.m5.1d">italic_s</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Softmax with Temperature</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.2">In the domain of RL, we need to consider the exploration-exploitation trade-off <cite class="ltx_cite ltx_citemacro_citep">(Sutton and Barto, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib20" title="">2018</a>)</cite>, where temperature <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.1.m1.1d">italic_τ</annotation></semantics></math> is an important parameter.
<math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.2.m2.1d">italic_τ</annotation></semantics></math> is used to control the softness of the softmax distribution,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{i}=\frac{\mathrm{exp}(y_{i}/\tau)}{\sum_{i}\mathrm{exp}(y_{i}/\tau)}." class="ltx_Math" display="block" id="S2.E2.m1.3"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2" xref="S2.E2.m1.3.3.1.1.2.2.cmml">p</mi><mi id="S2.E2.m1.3.3.1.1.2.3" xref="S2.E2.m1.3.3.1.1.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml">=</mo><mfrac id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml">exp</mi><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">τ</mi></mrow><mo id="S2.E2.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml"><msub id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml"><mo id="S2.E2.m1.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.cmml">∑</mo><mi id="S2.E2.m1.2.2.2.2.3" xref="S2.E2.m1.2.2.2.2.3.cmml">i</mi></msub><mrow id="S2.E2.m1.2.2.2.1" xref="S2.E2.m1.2.2.2.1.cmml"><mi id="S2.E2.m1.2.2.2.1.3" xref="S2.E2.m1.2.2.2.1.3.cmml">exp</mi><mo id="S2.E2.m1.2.2.2.1.2" xref="S2.E2.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.2.2.2.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E2.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.2.1.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml"><msub id="S2.E2.m1.2.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.2.1.1.1.1.2.cmml"><mi id="S2.E2.m1.2.2.2.1.1.1.1.2.2" xref="S2.E2.m1.2.2.2.1.1.1.1.2.2.cmml">y</mi><mi id="S2.E2.m1.2.2.2.1.1.1.1.2.3" xref="S2.E2.m1.2.2.2.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.1.cmml">/</mo><mi id="S2.E2.m1.2.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.2.1.1.1.1.3.cmml">τ</mi></mrow><mo id="S2.E2.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S2.E2.m1.3.3.1.2" lspace="0em" xref="S2.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1"><eq id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"></eq><apply id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2">𝑝</ci><ci id="S2.E2.m1.3.3.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><divide id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2"></divide><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3">exp</ci><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><divide id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"></divide><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2">𝑦</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">𝜏</ci></apply></apply><apply id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"><apply id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2">subscript</csymbol><sum id="S2.E2.m1.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2"></sum><ci id="S2.E2.m1.2.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.1"><times id="S2.E2.m1.2.2.2.1.2.cmml" xref="S2.E2.m1.2.2.2.1.2"></times><ci id="S2.E2.m1.2.2.2.1.3.cmml" xref="S2.E2.m1.2.2.2.1.3">exp</ci><apply id="S2.E2.m1.2.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1"><divide id="S2.E2.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.1"></divide><apply id="S2.E2.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2.2">𝑦</ci><ci id="S2.E2.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E2.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.3">𝜏</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">p_{i}=\frac{\mathrm{exp}(y_{i}/\tau)}{\sum_{i}\mathrm{exp}(y_{i}/\tau)}.</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.3d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG roman_exp ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_τ ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_exp ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_τ ) end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.4">A larger <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.3.m1.1a"><mi id="S2.SS0.SSS0.Px2.p1.3.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m1.1b"><ci id="S2.SS0.SSS0.Px2.p1.3.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.3.m1.1d">italic_τ</annotation></semantics></math> leads to a more uniform distribution, promoting exploration, while a smaller <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.4.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.4.m2.1a"><mi id="S2.SS0.SSS0.Px2.p1.4.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.4.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.4.m2.1b"><ci id="S2.SS0.SSS0.Px2.p1.4.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.4.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.4.m2.1d">italic_τ</annotation></semantics></math> creates a more peaky distribution, emphasizing exploitation.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Kiegeland and Kreutzer (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib9" title="">2021</a>)</cite> shows that training with an increased temperature can mitigate the peakiness effect due to RL <cite class="ltx_cite ltx_citemacro_citep">(Choshen et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib3" title="">2020</a>)</cite>, indicating that a suitable temperature is significant for RL training in NMT.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">RL for NAR</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.2">Compared to AR methods, studies of reinforcement learning for NAR remain unexplored.
<cite class="ltx_cite ltx_citemacro_citet">Shao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib18" title="">2021</a>)</cite> proposed a method to reduce the estimation variance of REINFORCE by fixing the predicted word at position <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.1.m1.1d">italic_t</annotation></semantics></math> and sampling words of other positions for <math alttext="n" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px3.p1.2.m2.1a"><mi id="S2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.2.m2.1b"><ci id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.2.m2.1d">italic_n</annotation></semantics></math> times.
However, this method is only applicable to models with a fixed length, which is unsuitable for edit-based models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Levenshtein Transformer</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">Levenshtein Transformer <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite> is an NAR model based on three edit operations: delete tokens, insert placeholders, and replace placeholders with new tokens.
It uses a supervised dual-policy learning algorithm to minimize the Levenshtein distance <cite class="ltx_cite ltx_citemacro_citep">(Levenshtein, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib10" title="">1965</a>)</cite> for training and greedy sampling for decoding.
The decoding stops when two consecutive refinement iterations return the same output or a maximum number of iterations (set to 10) is reached.
We illustrate the decoding process in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.F1" title="Figure 1 ‣ Levenshtein Transformer ‣ 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p2.1">LevT’s dual-policy learning generates teacher data by corrupting the ground truth and reconstructing it with its adversary policy.
This mechanism not only offers a unique approach to data generation but also underscores the inherent difficulty in preparing teacher data.
This introduces concerns regarding the exposure bias, particularly whether the training process can maintain consistency with the text during decoding.
To address this issue, we employ RL approaches that use self-generated data for training.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="453" id="S2.F1.g1" src="extracted/5705646/imgs/levt_en.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The illustration of Levenshtein Transformer’s decoding process <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>. In each decoding iteration, three edit operations are performed sequentially: delete tokens, insert placeholders, and replace placeholders with new tokens.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approaches</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present our reinforcement learning approaches in detail.
We train a Levenshtein Transformer model as our baseline using the dual-policy learning algorithm.
Based on it, we introduce two distinct RL approaches within the REINFORCE framework: stepwise reward maximization and episodic reward maximization.
Moreover, we present our methods for temperature control.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S3.F2.g1" src="extracted/5705646/imgs/method-v2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The illustration of the two RL approaches. (A) is the stepwise reward maximization, which randomly samples from a previous node for each edit operation and calculates BLEU and RL gradient after each edit operation (except for the insert operation, since it is not easy to calculate BLEU after inserting placeholders). (B) is the episodic reward maximization, where each sample is edited multiple times in a linear fashion, without branching into different paths, and BLEU and RL gradient are calculated only after the completion of all edit operations. At every orange node, we sample <math alttext="k" class="ltx_Math" display="inline" id="S3.F2.3.m1.1"><semantics id="S3.F2.3.m1.1b"><mi id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><ci id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.m1.1e">italic_k</annotation></semantics></math> times from this node (in this example, the sample size <math alttext="k" class="ltx_Math" display="inline" id="S3.F2.4.m2.1"><semantics id="S3.F2.4.m2.1b"><mi id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><ci id="S3.F2.4.m2.1.1.cmml" xref="S3.F2.4.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m2.1e">italic_k</annotation></semantics></math> is 2).</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Stepwise Reward Maximization</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">General RL training methods for AR NMT models are all episodic<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In this context, “episodic” denotes training based on entirely generated sequences</span></span></span>, as it is difficult to calculate BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib14" title="">2002</a>)</cite> when the sentence is not fully generated.
In contrast, NAR models can calculate BLEU on outputs at each decoding step.
From the perspective of estimating a more accurate gradient, we propose stepwise reward maximization, which calculates reward for each edit operation<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>In practice, since it is not easy to calculate BLEU after inserting placeholders, we consider placeholder insertion and token replacement as one edit operation.</span></span></span> using score differences from one previous edit.
Since every step’s reward is calculated separately, this approach should be easier to learn than episodic approaches <cite class="ltx_cite ltx_citemacro_citep">(Sutton and Barto, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib20" title="">2018</a>)</cite>.
However, it is also more prone to learning bias since the editing process is inherently multi-step.
This drawback should not be emphasized since maximizing the reward for each step will likely maximize the episodic reward in NAR models’ training.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.2">We use a leave-one-out baseline <cite class="ltx_cite ltx_citemacro_citep">(Luo, <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib12" title="">2020</a>)</cite> for <math alttext="b(s)" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p2.1.m1.1a"><mrow id="S3.SS0.SSS0.Px1.p2.1.m1.1.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.cmml"><mi id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.2.cmml">b</mi><mo id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.1" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.3.2" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.cmml"><mo id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.cmml">(</mo><mi id="S3.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">s</mi><mo id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2"><times id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.1.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.1"></times><ci id="S3.SS0.SSS0.Px1.p2.1.m1.1.2.2.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.2.2">𝑏</ci><ci id="S3.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.1.m1.1.1">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.1.m1.1c">b(s)</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p2.1.m1.1d">italic_b ( italic_s )</annotation></semantics></math> in Equation <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.E1" title="In Reinforcement Learning ‣ 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> instead of the greedy baseline proposed in SCST <cite class="ltx_cite ltx_citemacro_citep">(Rennie et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib17" title="">2017</a>)</cite> because the greedy decoding is too strong in LevT, which makes gaining positive rewards in SCST difficult and may reduce learning efficiency.
For each edit, we sample <math alttext="k" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p2.2.m2.1"><semantics id="S3.SS0.SSS0.Px1.p2.2.m2.1a"><mi id="S3.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.2.m2.1b"><ci id="S3.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p2.2.m2.1d">italic_k</annotation></semantics></math> actions from the policy at this point.
Then, we calculate the baseline as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="b_{i}(s)=\frac{1}{k-1}\sum_{j\neq i}r(y_{j})," class="ltx_Math" display="block" id="S3.E3.m1.2"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><msub id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.3.2.2.cmml">b</mi><mi id="S3.E3.m1.2.2.1.1.3.2.3" xref="S3.E3.m1.2.2.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.3.cmml"><mo id="S3.E3.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S3.E3.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">s</mi><mo id="S3.E3.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mfrac id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml"><mn id="S3.E3.m1.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.3.2.cmml">1</mn><mrow id="S3.E3.m1.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.1.3.3.2.cmml">k</mi><mo id="S3.E3.m1.2.2.1.1.1.3.3.1" xref="S3.E3.m1.2.2.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E3.m1.2.2.1.1.1.3.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.3.cmml">1</mn></mrow></mfrac><mo id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml"><munder id="S3.E3.m1.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.2.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.2.2" movablelimits="false" xref="S3.E3.m1.2.2.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.2.3" xref="S3.E3.m1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.1.1.2.3.2.cmml">j</mi><mo id="S3.E3.m1.2.2.1.1.1.1.2.3.1" xref="S3.E3.m1.2.2.1.1.1.1.2.3.1.cmml">≠</mo><mi id="S3.E3.m1.2.2.1.1.1.1.2.3.3" xref="S3.E3.m1.2.2.1.1.1.1.2.3.3.cmml">i</mi></mrow></munder><mrow id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.3.cmml">r</mi><mo id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E3.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></eq><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><times id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1"></times><apply id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2.2">𝑏</ci><ci id="S3.E3.m1.2.2.1.1.3.2.3.cmml" xref="S3.E3.m1.2.2.1.1.3.2.3">𝑖</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑠</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></times><apply id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3"><divide id="S3.E3.m1.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3"></divide><cn id="S3.E3.m1.2.2.1.1.1.3.2.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.3.2">1</cn><apply id="S3.E3.m1.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3"><minus id="S3.E3.m1.2.2.1.1.1.3.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.1"></minus><ci id="S3.E3.m1.2.2.1.1.1.3.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3.2">𝑘</ci><cn id="S3.E3.m1.2.2.1.1.1.3.3.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1"><apply id="S3.E3.m1.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.2"></sum><apply id="S3.E3.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.3"><neq id="S3.E3.m1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.3.1"></neq><ci id="S3.E3.m1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.3.2">𝑗</ci><ci id="S3.E3.m1.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.2.3.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.3">𝑟</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.1.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">b_{i}(s)=\frac{1}{k-1}\sum_{j\neq i}r(y_{j}),</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.2d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_s ) = divide start_ARG 1 end_ARG start_ARG italic_k - 1 end_ARG ∑ start_POSTSUBSCRIPT italic_j ≠ italic_i end_POSTSUBSCRIPT italic_r ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.4">where <math alttext="y_{j}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p2.3.m1.1"><semantics id="S3.SS0.SSS0.Px1.p2.3.m1.1a"><msub id="S3.SS0.SSS0.Px1.p2.3.m1.1.1" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1.2.cmml">y</mi><mi id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.3.m1.1b"><apply id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1.2">𝑦</ci><ci id="S3.SS0.SSS0.Px1.p2.3.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px1.p2.3.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.3.m1.1c">y_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p2.3.m1.1d">italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p2.4.m2.1"><semantics id="S3.SS0.SSS0.Px1.p2.4.m2.1a"><mi id="S3.SS0.SSS0.Px1.p2.4.m2.1.1" xref="S3.SS0.SSS0.Px1.p2.4.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p2.4.m2.1b"><ci id="S3.SS0.SSS0.Px1.p2.4.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p2.4.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p2.4.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p2.4.m2.1d">italic_j</annotation></semantics></math>th sample from the current policy.
The final RL gradient estimation becomes</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bigtriangledown_{\theta}L(\theta)\approx-(r(y_{i})-b_{i}(s))\bigtriangledown_%
{\theta}\mathrm{log}\pi_{\theta}(y_{i}|s)." class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.4" xref="S3.E4.m1.3.3.1.1.4.cmml"><msub id="S3.E4.m1.3.3.1.1.4.1" xref="S3.E4.m1.3.3.1.1.4.1.cmml"><mo id="S3.E4.m1.3.3.1.1.4.1.2" xref="S3.E4.m1.3.3.1.1.4.1.2.cmml">▽</mo><mi id="S3.E4.m1.3.3.1.1.4.1.3" xref="S3.E4.m1.3.3.1.1.4.1.3.cmml">θ</mi></msub><mrow id="S3.E4.m1.3.3.1.1.4.2" xref="S3.E4.m1.3.3.1.1.4.2.cmml"><mi id="S3.E4.m1.3.3.1.1.4.2.2" xref="S3.E4.m1.3.3.1.1.4.2.2.cmml">L</mi><mo id="S3.E4.m1.3.3.1.1.4.2.1" xref="S3.E4.m1.3.3.1.1.4.2.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.4.2.3.2" xref="S3.E4.m1.3.3.1.1.4.2.cmml"><mo id="S3.E4.m1.3.3.1.1.4.2.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.4.2.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">θ</mi><mo id="S3.E4.m1.3.3.1.1.4.2.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.4.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml">≈</mo><mrow id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1a" xref="S3.E4.m1.3.3.1.1.1.1.cmml">−</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">r</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">s</mi><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><msub id="S3.E4.m1.3.3.1.1.2.3" xref="S3.E4.m1.3.3.1.1.2.3.cmml"><mo id="S3.E4.m1.3.3.1.1.2.3.2" lspace="0.222em" rspace="0.222em" xref="S3.E4.m1.3.3.1.1.2.3.2.cmml">▽</mo><mi id="S3.E4.m1.3.3.1.1.2.3.3" xref="S3.E4.m1.3.3.1.1.2.3.3.cmml">θ</mi></msub><mrow id="S3.E4.m1.3.3.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2.3" xref="S3.E4.m1.3.3.1.1.2.2.3.cmml">log</mi><mo id="S3.E4.m1.3.3.1.1.2.2.2" xref="S3.E4.m1.3.3.1.1.2.2.2.cmml">⁢</mo><msub id="S3.E4.m1.3.3.1.1.2.2.4" xref="S3.E4.m1.3.3.1.1.2.2.4.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2.4.2" xref="S3.E4.m1.3.3.1.1.2.2.4.2.cmml">π</mi><mi id="S3.E4.m1.3.3.1.1.2.2.4.3" xref="S3.E4.m1.3.3.1.1.2.2.4.3.cmml">θ</mi></msub><mo id="S3.E4.m1.3.3.1.1.2.2.2a" xref="S3.E4.m1.3.3.1.1.2.2.2.cmml">⁢</mo><mrow id="S3.E4.m1.3.3.1.1.2.2.1.1" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.cmml"><mo id="S3.E4.m1.3.3.1.1.2.2.1.1.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.2.2.1.1.1" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.2.cmml">y</mi><mi id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.3" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.E4.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.1.cmml">|</mo><mi id="S3.E4.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.3.cmml">s</mi></mrow><mo id="S3.E4.m1.3.3.1.1.2.2.1.1.3" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" lspace="0em" xref="S3.E4.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><approx id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"></approx><apply id="S3.E4.m1.3.3.1.1.4.cmml" xref="S3.E4.m1.3.3.1.1.4"><apply id="S3.E4.m1.3.3.1.1.4.1.cmml" xref="S3.E4.m1.3.3.1.1.4.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.4.1.1.cmml" xref="S3.E4.m1.3.3.1.1.4.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.4.1.2.cmml" xref="S3.E4.m1.3.3.1.1.4.1.2">▽</ci><ci id="S3.E4.m1.3.3.1.1.4.1.3.cmml" xref="S3.E4.m1.3.3.1.1.4.1.3">𝜃</ci></apply><apply id="S3.E4.m1.3.3.1.1.4.2.cmml" xref="S3.E4.m1.3.3.1.1.4.2"><times id="S3.E4.m1.3.3.1.1.4.2.1.cmml" xref="S3.E4.m1.3.3.1.1.4.2.1"></times><ci id="S3.E4.m1.3.3.1.1.4.2.2.cmml" xref="S3.E4.m1.3.3.1.1.4.2.2">𝐿</ci><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝜃</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"><apply id="S3.E4.m1.3.3.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.3.1.cmml" xref="S3.E4.m1.3.3.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.3.2.cmml" xref="S3.E4.m1.3.3.1.1.2.3.2">▽</ci><ci id="S3.E4.m1.3.3.1.1.2.3.3.cmml" xref="S3.E4.m1.3.3.1.1.2.3.3">𝜃</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1"><minus id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1"></minus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><minus id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2"></minus><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.3">𝑟</ci><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3"><times id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1"></times><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.2">𝑏</ci><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑠</ci></apply></apply></apply><apply id="S3.E4.m1.3.3.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2"><times id="S3.E4.m1.3.3.1.1.2.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2.2"></times><ci id="S3.E4.m1.3.3.1.1.2.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.2.3">log</ci><apply id="S3.E4.m1.3.3.1.1.2.2.4.cmml" xref="S3.E4.m1.3.3.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.2.4.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.4">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.2.4.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2.4.2">𝜋</ci><ci id="S3.E4.m1.3.3.1.1.2.2.4.3.cmml" xref="S3.E4.m1.3.3.1.1.2.2.4.3">𝜃</ci></apply><apply id="S3.E4.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.1">conditional</csymbol><apply id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.2">𝑦</ci><ci id="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.3.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E4.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.2.2.1.1.1.3">𝑠</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">\bigtriangledown_{\theta}L(\theta)\approx-(r(y_{i})-b_{i}(s))\bigtriangledown_%
{\theta}\mathrm{log}\pi_{\theta}(y_{i}|s).</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">▽ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_L ( italic_θ ) ≈ - ( italic_r ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_s ) ) ▽ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_s ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p3.1">In a straightforward implementation, one might consider applying sampling again to all <math alttext="k" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p3.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p3.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p3.1.m1.1d">italic_k</annotation></semantics></math> samples from the last edit.
However, this will cause a combination explosion when the number of edit operations increases.
Practically, we randomly choose a sample from the previous edit to perform the subsequent operations.
We show an illustration of the sampling process in (A) of Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3.F2" title="Figure 2 ‣ 3 Approaches ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> and pseudo code of our algorithm in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A1" title="Appendix A Pseudo code of stepwise reward maximization ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Episodic Reward Maximization</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">We also introduce episodic reward maximization, which calculates rewards only once for each sample and gives all actions the same weight.
It is a more traditional way to train NMT models in RL.
It allows unbiased learning but may not be efficient.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p2.1">We use the leave-one-out baseline for the episodic reward as well as the stepwise reward.
We sample <math alttext="k" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS0.SSS0.Px2.p2.1.m1.1a"><mi id="S3.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p2.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p2.1.m1.1d">italic_k</annotation></semantics></math> samples from the initial input.
Each sample will be edited multiple times without a branch.
After the final edit, we calculate the rewards and baselines.
We show an illustration of the sampling process in (B) of Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S3.F2" title="Figure 2 ‣ 3 Approaches ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> and pseudo code of our algorithm in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A2" title="Appendix B Pseudo code of episodic reward maximization ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Temperature Control</h4>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">Applying RL to NAR differs significantly from AR because there could be various types of actions rather than just predicting the next token, like deletion and insertion.
Due to this difficulty, NAR may need more fine-grained temperature control during training.
To investigate the impact of exploration and exploitation in the training process, we explore five different settings of the temperature.
Due to the large decoding space of Levenshtein Transformer, default temperature 1 may result in poor rewards, and too small temperature may result in peaky distribution, which are both harmful to learning.
We use three constant temperature settings set to 0.1, 0.5, and 1 to verify the effect of temperature magnitude.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.4">An annealing schedule is known for balancing the trade-off between model accuracy and variance during training <cite class="ltx_cite ltx_citemacro_citep">(Jang et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib8" title="">2016</a>)</cite>.
There are two ways of thinking here.
First, to reduce the exposure bias, we want to get close to the decoding scenario, which is greedy decoding in our experiments.
Thus, we can apply a regular annealing schedule to gradually reduce the temperature from 1 to 0.1 during training.
The temperature function can be written as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau_{i+1}=\max(\tau_{i}*\mathrm{exp}(-\frac{\mathrm{log}(\tau_{0}/\tau_{T})}{%
T}),\tau_{T})," class="ltx_Math" display="block" id="S3.E5.m1.3"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><msub id="S3.E5.m1.3.3.1.1.4" xref="S3.E5.m1.3.3.1.1.4.cmml"><mi id="S3.E5.m1.3.3.1.1.4.2" xref="S3.E5.m1.3.3.1.1.4.2.cmml">τ</mi><mrow id="S3.E5.m1.3.3.1.1.4.3" xref="S3.E5.m1.3.3.1.1.4.3.cmml"><mi id="S3.E5.m1.3.3.1.1.4.3.2" xref="S3.E5.m1.3.3.1.1.4.3.2.cmml">i</mi><mo id="S3.E5.m1.3.3.1.1.4.3.1" xref="S3.E5.m1.3.3.1.1.4.3.1.cmml">+</mo><mn id="S3.E5.m1.3.3.1.1.4.3.3" xref="S3.E5.m1.3.3.1.1.4.3.3.cmml">1</mn></mrow></msub><mo id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.3.cmml"><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">max</mi><mo id="S3.E5.m1.3.3.1.1.2.2a" xref="S3.E5.m1.3.3.1.1.2.3.cmml">⁡</mo><mrow id="S3.E5.m1.3.3.1.1.2.2.2" xref="S3.E5.m1.3.3.1.1.2.3.cmml"><mo id="S3.E5.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.3.cmml">(</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.cmml"><msub id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.2.cmml">τ</mi><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.1.cmml">∗</mo><mi id="S3.E5.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.3.cmml">exp</mi></mrow><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1a" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">−</mo><mfrac id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml">log</mi><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.cmml">τ</mi><mn id="S3.E5.m1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml">/</mo><msub id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.3.2.cmml">τ</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.3.3.cmml">T</mi></msub></mrow><mo id="S3.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mi id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">T</mi></mfrac></mrow><mo id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.3.3.1.1.2.2.2.4" xref="S3.E5.m1.3.3.1.1.2.3.cmml">,</mo><msub id="S3.E5.m1.3.3.1.1.2.2.2.2" xref="S3.E5.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2.2.2.2" xref="S3.E5.m1.3.3.1.1.2.2.2.2.2.cmml">τ</mi><mi id="S3.E5.m1.3.3.1.1.2.2.2.2.3" xref="S3.E5.m1.3.3.1.1.2.2.2.2.3.cmml">T</mi></msub><mo id="S3.E5.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"></eq><apply id="S3.E5.m1.3.3.1.1.4.cmml" xref="S3.E5.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.4.1.cmml" xref="S3.E5.m1.3.3.1.1.4">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.4.2.cmml" xref="S3.E5.m1.3.3.1.1.4.2">𝜏</ci><apply id="S3.E5.m1.3.3.1.1.4.3.cmml" xref="S3.E5.m1.3.3.1.1.4.3"><plus id="S3.E5.m1.3.3.1.1.4.3.1.cmml" xref="S3.E5.m1.3.3.1.1.4.3.1"></plus><ci id="S3.E5.m1.3.3.1.1.4.3.2.cmml" xref="S3.E5.m1.3.3.1.1.4.3.2">𝑖</ci><cn id="S3.E5.m1.3.3.1.1.4.3.3.cmml" type="integer" xref="S3.E5.m1.3.3.1.1.4.3.3">1</cn></apply></apply><apply id="S3.E5.m1.3.3.1.1.2.3.cmml" xref="S3.E5.m1.3.3.1.1.2.2"><max id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"></max><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1"><times id="S3.E5.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3"><times id="S3.E5.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.1"></times><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.2">𝜏</ci><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.E5.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.3.3">exp</ci></apply><apply id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1"><minus id="S3.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><divide id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1"></divide><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">log</ci><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><divide id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1"></divide><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2">𝜏</ci><cn id="S3.E5.m1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.E5.m1.1.1.1.1.1.1.2.3">0</cn></apply><apply id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2">𝜏</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply></apply><ci id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">𝑇</ci></apply></apply></apply><apply id="S3.E5.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2.2.2.2">𝜏</ci><ci id="S3.E5.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E5.m1.3.3.1.1.2.2.2.2.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\tau_{i+1}=\max(\tau_{i}*\mathrm{exp}(-\frac{\mathrm{log}(\tau_{0}/\tau_{T})}{%
T}),\tau_{T}),</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.3d">italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = roman_max ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∗ roman_exp ( - divide start_ARG roman_log ( italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / italic_τ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) end_ARG start_ARG italic_T end_ARG ) , italic_τ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.3">where <math alttext="T" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS0.SSS0.Px3.p2.1.m1.1a"><mi id="S3.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.1.m1.1b"><ci id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p2.1.m1.1d">italic_T</annotation></semantics></math> is the number of total training steps, and <math alttext="\tau_{0}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p2.2.m2.1"><semantics id="S3.SS0.SSS0.Px3.p2.2.m2.1a"><msub id="S3.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml">τ</mi><mn id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.2">𝜏</ci><cn id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.2.m2.1c">\tau_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p2.2.m2.1d">italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\tau_{T}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px3.p2.3.m3.1"><semantics id="S3.SS0.SSS0.Px3.p2.3.m3.1a"><msub id="S3.SS0.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml">τ</mi><mi id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.3.m3.1b"><apply id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.2">𝜏</ci><ci id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.3.m3.1c">\tau_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px3.p2.3.m3.1d">italic_τ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> are the initial and the target temperatures.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p3.1">Second, using high temperatures in the early stages of training may lead to poor rewards and result in low learning efficiency.
We can apply an inverted annealing schedule to gradually increase the temperature from 0.1 to 1, guaranteeing stable training in the early stages and gradually increasing the exploration space for efficient training.
The temperature function can be written as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau_{i+1}=\min(\tau_{i}/\mathrm{exp}(-\frac{\mathrm{log}(\tau_{T}/\tau_{0})}{%
T}),\tau_{T})." class="ltx_Math" display="block" id="S3.E6.m1.3"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3.1" xref="S3.E6.m1.3.3.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1" xref="S3.E6.m1.3.3.1.1.cmml"><msub id="S3.E6.m1.3.3.1.1.4" xref="S3.E6.m1.3.3.1.1.4.cmml"><mi id="S3.E6.m1.3.3.1.1.4.2" xref="S3.E6.m1.3.3.1.1.4.2.cmml">τ</mi><mrow id="S3.E6.m1.3.3.1.1.4.3" xref="S3.E6.m1.3.3.1.1.4.3.cmml"><mi id="S3.E6.m1.3.3.1.1.4.3.2" xref="S3.E6.m1.3.3.1.1.4.3.2.cmml">i</mi><mo id="S3.E6.m1.3.3.1.1.4.3.1" xref="S3.E6.m1.3.3.1.1.4.3.1.cmml">+</mo><mn id="S3.E6.m1.3.3.1.1.4.3.3" xref="S3.E6.m1.3.3.1.1.4.3.3.cmml">1</mn></mrow></msub><mo id="S3.E6.m1.3.3.1.1.3" xref="S3.E6.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E6.m1.3.3.1.1.2.2" xref="S3.E6.m1.3.3.1.1.2.3.cmml"><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">min</mi><mo id="S3.E6.m1.3.3.1.1.2.2a" xref="S3.E6.m1.3.3.1.1.2.3.cmml">⁡</mo><mrow id="S3.E6.m1.3.3.1.1.2.2.2" xref="S3.E6.m1.3.3.1.1.2.3.cmml"><mo id="S3.E6.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="S3.E6.m1.3.3.1.1.2.3.cmml">(</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.cmml"><msub id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.2.cmml">τ</mi><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.3.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.1.cmml">/</mo><mi id="S3.E6.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.3.cmml">exp</mi></mrow><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1a" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">−</mo><mfrac id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml">log</mi><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.1.1.2.2.cmml">τ</mi><mi id="S3.E6.m1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.1.1.2.3.cmml">T</mi></msub><mo id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">/</mo><msub id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.1.1.3.2.cmml">τ</mi><mn id="S3.E6.m1.1.1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.E6.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mi id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml">T</mi></mfrac></mrow><mo id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.2.2.2.4" xref="S3.E6.m1.3.3.1.1.2.3.cmml">,</mo><msub id="S3.E6.m1.3.3.1.1.2.2.2.2" xref="S3.E6.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E6.m1.3.3.1.1.2.2.2.2.2" xref="S3.E6.m1.3.3.1.1.2.2.2.2.2.cmml">τ</mi><mi id="S3.E6.m1.3.3.1.1.2.2.2.2.3" xref="S3.E6.m1.3.3.1.1.2.2.2.2.3.cmml">T</mi></msub><mo id="S3.E6.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="S3.E6.m1.3.3.1.1.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.3.3.1.2" lspace="0em" xref="S3.E6.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.1.1.cmml" xref="S3.E6.m1.3.3.1"><eq id="S3.E6.m1.3.3.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.3"></eq><apply id="S3.E6.m1.3.3.1.1.4.cmml" xref="S3.E6.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.4.1.cmml" xref="S3.E6.m1.3.3.1.1.4">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.4.2.cmml" xref="S3.E6.m1.3.3.1.1.4.2">𝜏</ci><apply id="S3.E6.m1.3.3.1.1.4.3.cmml" xref="S3.E6.m1.3.3.1.1.4.3"><plus id="S3.E6.m1.3.3.1.1.4.3.1.cmml" xref="S3.E6.m1.3.3.1.1.4.3.1"></plus><ci id="S3.E6.m1.3.3.1.1.4.3.2.cmml" xref="S3.E6.m1.3.3.1.1.4.3.2">𝑖</ci><cn id="S3.E6.m1.3.3.1.1.4.3.3.cmml" type="integer" xref="S3.E6.m1.3.3.1.1.4.3.3">1</cn></apply></apply><apply id="S3.E6.m1.3.3.1.1.2.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2"><min id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2"></min><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1"><times id="S3.E6.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3"><divide id="S3.E6.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.1"></divide><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.2">𝜏</ci><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S3.E6.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.3.3">exp</ci></apply><apply id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1"><minus id="S3.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><divide id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1"></divide><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">log</ci><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><divide id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1"></divide><apply id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.2">𝜏</ci><ci id="S3.E6.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3.2">𝜏</ci><cn id="S3.E6.m1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E6.m1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><ci id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3">𝑇</ci></apply></apply></apply><apply id="S3.E6.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2.2">𝜏</ci><ci id="S3.E6.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2.2.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">\tau_{i+1}=\min(\tau_{i}/\mathrm{exp}(-\frac{\mathrm{log}(\tau_{T}/\tau_{0})}{%
T}),\tau_{T}).</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.3d">italic_τ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = roman_min ( italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / roman_exp ( - divide start_ARG roman_log ( italic_τ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT / italic_τ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG italic_T end_ARG ) , italic_τ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p4">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p4.1">In each decoding iteration, multiple edit operations occur, and each operation has a different decoding space size.
It may be beneficial to optimize this by using varying temperatures for each operation in every iteration.
This is a complicated research question and we leave this exploration to future work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data &amp; Evaluation</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We use WMT’14 English-German (EN-DE) <cite class="ltx_cite ltx_citemacro_citep">(Bojar et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib2" title="">2014</a>)</cite> and WAT’17 English-Japanese (EN-JA) Small-NMT datasets <cite class="ltx_cite ltx_citemacro_citep">(Nakazawa et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib13" title="">2017</a>)</cite> for experiments.
We use BPE token-based BLEU scores for evaluations.
Data preprocessing follows <cite class="ltx_cite ltx_citemacro_citet">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Baseline</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We use Levenshtein Transformer as our baseline.
Following <cite class="ltx_cite ltx_citemacro_citet">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>, we trained a LevT with 300K steps and a max batch size of 65,536 tokens per step.
However, like <cite class="ltx_cite ltx_citemacro_citet">Reid et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib16" title="">2023</a>)</cite>, we cannot reproduce the results of <cite class="ltx_cite ltx_citemacro_citet">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>.
We use our results in this paper.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">RL</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.2">According to <cite class="ltx_cite ltx_citemacro_citet">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#bib.bib7" title="">2019</a>)</cite>, most decodings are gotten in 1-4 iterations, and the average number of decoding iterations is 2.43.
To minimize the gap between the training and decoding states, we start with a null string and conduct 3 iterations (8 edits) for each sample during RL training.
We set the total training steps <math alttext="T" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px3.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px3.p1.1.m1.1d">italic_T</annotation></semantics></math> to 50,000, with a max batch size of 4,096 tokens per step.
To prevent the out-of-memory issue, we limit the decoding space of placeholder insertion from 256 to 64.
The sample size <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px3.p1.2.m2.1d">italic_k</annotation></semantics></math> of the baseline is set to 5.
Our implementation is based on Fairseq<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fairseq" title="">https://github.com/facebookresearch/fairseq</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Computational Cost</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">The pre-training phase of LevT on a GCP VM instance with A100x4 GPUs requires roughly 3 days, while the subsequent RL fine-tuning process takes approximately 1 day to complete.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We show the BLEU scores of our approaches in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.T1" title="Table 1 ‣ 4.2 Results ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.
The episodic reward model<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The term “episode/stepwise reward model” specifically refers to the model trained using the “episode/stepwise reward maximization” approach.</span></span></span> showed notable improvement over the baseline.
The score is even close to the distillation model, which requires a heavy pre-training<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>To produce a distillation model, we need to train an autoregressive Transformer first, which needs additional 3 days of training on our machine.</span></span></span> of AR models.
However, the stepwise reward model showed only limited improvement.
To explain this, we focus on the advantage, <math alttext="r(y)-b(s)" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.2"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3" xref="S4.SS2.p1.1.m1.2.3.cmml"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.2.cmml"><mi id="S4.SS2.p1.1.m1.2.3.2.2" xref="S4.SS2.p1.1.m1.2.3.2.2.cmml">r</mi><mo id="S4.SS2.p1.1.m1.2.3.2.1" xref="S4.SS2.p1.1.m1.2.3.2.1.cmml">⁢</mo><mrow id="S4.SS2.p1.1.m1.2.3.2.3.2" xref="S4.SS2.p1.1.m1.2.3.2.cmml"><mo id="S4.SS2.p1.1.m1.2.3.2.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.2.cmml">(</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">y</mi><mo id="S4.SS2.p1.1.m1.2.3.2.3.2.2" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p1.1.m1.2.3.1" xref="S4.SS2.p1.1.m1.2.3.1.cmml">−</mo><mrow id="S4.SS2.p1.1.m1.2.3.3" xref="S4.SS2.p1.1.m1.2.3.3.cmml"><mi id="S4.SS2.p1.1.m1.2.3.3.2" xref="S4.SS2.p1.1.m1.2.3.3.2.cmml">b</mi><mo id="S4.SS2.p1.1.m1.2.3.3.1" xref="S4.SS2.p1.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S4.SS2.p1.1.m1.2.3.3.3.2" xref="S4.SS2.p1.1.m1.2.3.3.cmml"><mo id="S4.SS2.p1.1.m1.2.3.3.3.2.1" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.3.cmml">(</mo><mi id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">s</mi><mo id="S4.SS2.p1.1.m1.2.3.3.3.2.2" stretchy="false" xref="S4.SS2.p1.1.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><apply id="S4.SS2.p1.1.m1.2.3.cmml" xref="S4.SS2.p1.1.m1.2.3"><minus id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.1"></minus><apply id="S4.SS2.p1.1.m1.2.3.2.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><times id="S4.SS2.p1.1.m1.2.3.2.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2.1"></times><ci id="S4.SS2.p1.1.m1.2.3.2.2.cmml" xref="S4.SS2.p1.1.m1.2.3.2.2">𝑟</ci><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑦</ci></apply><apply id="S4.SS2.p1.1.m1.2.3.3.cmml" xref="S4.SS2.p1.1.m1.2.3.3"><times id="S4.SS2.p1.1.m1.2.3.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.3.1"></times><ci id="S4.SS2.p1.1.m1.2.3.3.2.cmml" xref="S4.SS2.p1.1.m1.2.3.3.2">𝑏</ci><ci id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">r(y)-b(s)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.2d">italic_r ( italic_y ) - italic_b ( italic_s )</annotation></semantics></math>, included in the policy gradient (Equation <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S2.E1" title="In Reinforcement Learning ‣ 2 Background ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>), as a larger value of the advantage can increase the policy gradient’s magnitude.
A higher standard deviation (SD) of the advantages indicates larger fluctuations in policy gradients.
Table <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.T2" title="Table 2 ‣ 4.2 Results ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the SDs of the advantages of the stepwise reward model, with notably higher values in the early stages of edit operations compared to later stages.
This suggests that the stepwise reward model disproportionately focuses on early operations, potentially leading to uneven learning and reduced performance.
In contrast, the episodic reward model applies the same rewards and advantages across all operations, facilitating more uniform learning and improved performance.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We only report scores of applying RL to the model without distillation since we found that RL significantly improved the model without distillation (max 1.69 points) compared to when distillation was applied (max 0.5 point).
Moreover, when confronted with distillation models, it raises questions such as which data we should use for RL training, the original or the distillation one.
We leave these research questions to future work.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:346.9pt;height:158.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.1pt,-34.4pt) scale(1.76388252029171,1.76388252029171) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.2">EN-DE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1.3">EN-JA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.2.1.1">LevT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.2">24.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.2.1.3">31.76</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.3.2.1">LevT + distillation</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.2">26.49</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3.2.3">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.1.4.3.1">LevT + RL (stepwise)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.3.2">24.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.3.3">31.73</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.1.5.4.1">LevT + RL (episodic)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.4.2.1">25.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.4.3.1">32.75</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The BLEU scores of our approaches and the baseline. Temperatures are set to 1. Due to the limited computational resources, we only trained the distillation model for the EN-DE dataset using the ready-made distillation dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:390.3pt;height:189.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.0pt,-40.8pt) scale(1.75570786866261,1.75570786866261) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.1">Iteration</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1.2">Edit Operation</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3">EN-DE</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.4">EN-JA</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.2.2.1">1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.2.2.2">Insert + Replace</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.3">9.99</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.4">8.59</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.3.1.1">2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.3.1.2">Delete</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.3.1.3">2.05</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.3.1.4">1.35</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.4.2.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.4.2.2">Insert + Replace</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4.2.3">3.28</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4.2.4">2.48</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.5.3.1">3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.5.3.2">Delete</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.5.3.3">1.67</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.5.3.4">1.29</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.1.6.4.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.1.1.6.4.2">Insert + Replace</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.1.6.4.3">3.04</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.1.6.4.4">1.60</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Stepwise reward model’s standard deviation (SD) of the advantage in each edit operation. Insertion and replacement share the same reward.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.5" style="width:372.9pt;height:200pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(85.8pt,-46.0pt) scale(1.85212302654604,1.85212302654604) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.5.5.6.1.1">Temperature</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.5.6.1.2">EN-DE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.5.5.6.1.3">EN-JA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1">Constant (<math alttext="\tau=1" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">τ</mi><mo id="S4.T3.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><eq id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.1"></eq><ci id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">𝜏</ci><cn id="S4.T3.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\tau=1</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">italic_τ = 1</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.2">25.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.3">32.75</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.2.1">Constant (<math alttext="\tau=0.5" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.2.cmml">τ</mi><mo id="S4.T3.2.2.2.1.m1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><eq id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"></eq><ci id="S4.T3.2.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2">𝜏</ci><cn id="S4.T3.2.2.2.1.m1.1.1.3.cmml" type="float" xref="S4.T3.2.2.2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\tau=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">italic_τ = 0.5</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.2.1">25.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3">33.45</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.3.3.1">Constant (<math alttext="\tau=0.1" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.3.1.m1.1.1.2.cmml">τ</mi><mo id="S4.T3.3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.1.cmml">=</mo><mn id="S4.T3.3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.3.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><eq id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.1"></eq><ci id="S4.T3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.1.m1.1.1.2">𝜏</ci><cn id="S4.T3.3.3.3.1.m1.1.1.3.cmml" type="float" xref="S4.T3.3.3.3.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">\tau=0.1</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">italic_τ = 0.1</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.2">25.76</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3">33.60</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.4.4.4.1">Annealing (<math alttext="\tau=1\rightarrow 0.1" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><mrow id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml"><mi id="S4.T3.4.4.4.1.m1.1.1.2" xref="S4.T3.4.4.4.1.m1.1.1.2.cmml">τ</mi><mo id="S4.T3.4.4.4.1.m1.1.1.3" xref="S4.T3.4.4.4.1.m1.1.1.3.cmml">=</mo><mn id="S4.T3.4.4.4.1.m1.1.1.4" xref="S4.T3.4.4.4.1.m1.1.1.4.cmml">1</mn><mo id="S4.T3.4.4.4.1.m1.1.1.5" stretchy="false" xref="S4.T3.4.4.4.1.m1.1.1.5.cmml">→</mo><mn id="S4.T3.4.4.4.1.m1.1.1.6" xref="S4.T3.4.4.4.1.m1.1.1.6.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><apply id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1"><and id="S4.T3.4.4.4.1.m1.1.1a.cmml" xref="S4.T3.4.4.4.1.m1.1.1"></and><apply id="S4.T3.4.4.4.1.m1.1.1b.cmml" xref="S4.T3.4.4.4.1.m1.1.1"><eq id="S4.T3.4.4.4.1.m1.1.1.3.cmml" xref="S4.T3.4.4.4.1.m1.1.1.3"></eq><ci id="S4.T3.4.4.4.1.m1.1.1.2.cmml" xref="S4.T3.4.4.4.1.m1.1.1.2">𝜏</ci><cn id="S4.T3.4.4.4.1.m1.1.1.4.cmml" type="integer" xref="S4.T3.4.4.4.1.m1.1.1.4">1</cn></apply><apply id="S4.T3.4.4.4.1.m1.1.1c.cmml" xref="S4.T3.4.4.4.1.m1.1.1"><ci id="S4.T3.4.4.4.1.m1.1.1.5.cmml" xref="S4.T3.4.4.4.1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2405.01280v2#S4.T3.4.4.4.1.m1.1.1.4.cmml" id="S4.T3.4.4.4.1.m1.1.1d.cmml" xref="S4.T3.4.4.4.1.m1.1.1"></share><cn id="S4.T3.4.4.4.1.m1.1.1.6.cmml" type="float" xref="S4.T3.4.4.4.1.m1.1.1.6">0.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">\tau=1\rightarrow 0.1</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">italic_τ = 1 → 0.1</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.4.2">25.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.3.1">33.76</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.5.5.1">Annealing (<math alttext="\tau=0.1\rightarrow 1" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.1.m1.1a"><mrow id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml"><mi id="S4.T3.5.5.5.1.m1.1.1.2" xref="S4.T3.5.5.5.1.m1.1.1.2.cmml">τ</mi><mo id="S4.T3.5.5.5.1.m1.1.1.3" xref="S4.T3.5.5.5.1.m1.1.1.3.cmml">=</mo><mn id="S4.T3.5.5.5.1.m1.1.1.4" xref="S4.T3.5.5.5.1.m1.1.1.4.cmml">0.1</mn><mo id="S4.T3.5.5.5.1.m1.1.1.5" stretchy="false" xref="S4.T3.5.5.5.1.m1.1.1.5.cmml">→</mo><mn id="S4.T3.5.5.5.1.m1.1.1.6" xref="S4.T3.5.5.5.1.m1.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><apply id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1"><and id="S4.T3.5.5.5.1.m1.1.1a.cmml" xref="S4.T3.5.5.5.1.m1.1.1"></and><apply id="S4.T3.5.5.5.1.m1.1.1b.cmml" xref="S4.T3.5.5.5.1.m1.1.1"><eq id="S4.T3.5.5.5.1.m1.1.1.3.cmml" xref="S4.T3.5.5.5.1.m1.1.1.3"></eq><ci id="S4.T3.5.5.5.1.m1.1.1.2.cmml" xref="S4.T3.5.5.5.1.m1.1.1.2">𝜏</ci><cn id="S4.T3.5.5.5.1.m1.1.1.4.cmml" type="float" xref="S4.T3.5.5.5.1.m1.1.1.4">0.1</cn></apply><apply id="S4.T3.5.5.5.1.m1.1.1c.cmml" xref="S4.T3.5.5.5.1.m1.1.1"><ci id="S4.T3.5.5.5.1.m1.1.1.5.cmml" xref="S4.T3.5.5.5.1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2405.01280v2#S4.T3.5.5.5.1.m1.1.1.4.cmml" id="S4.T3.5.5.5.1.m1.1.1d.cmml" xref="S4.T3.5.5.5.1.m1.1.1"></share><cn id="S4.T3.5.5.5.1.m1.1.1.6.cmml" type="integer" xref="S4.T3.5.5.5.1.m1.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">\tau=0.1\rightarrow 1</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">italic_τ = 0.1 → 1</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5.2">25.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.5.5.5.3">33.43</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The BLEU scores of episodic reward models using different temperature settings.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We show the BLEU scores of different temperature settings in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Experiments ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>.
Model performance varies significantly with temperature settings (max 1.01 points in EN-JA).
Among constant setting models, the model with a temperature of 0.5 performed best in EN-DE, and the model with a temperature of 0.1 performed best in EN-JA, indicating that too large temperature harms RL training.
The two models using annealing schedules performed great in both tasks, showing the effectiveness of the annealing algorithms for improving learning efficiency.
However, the annealing models did not always outperform the constant models, which suggests the difficulty of seeking the optimal temperature setting for NAR models’ RL training.
Also, we found the inverted annealing model (<math alttext="\tau{=}0.1{\rightarrow}1" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">τ</mi><mo id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">=</mo><mn id="S4.SS2.p3.1.m1.1.1.4" xref="S4.SS2.p3.1.m1.1.1.4.cmml">0.1</mn><mo id="S4.SS2.p3.1.m1.1.1.5" stretchy="false" xref="S4.SS2.p3.1.m1.1.1.5.cmml">→</mo><mn id="S4.SS2.p3.1.m1.1.1.6" xref="S4.SS2.p3.1.m1.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><and id="S4.SS2.p3.1.m1.1.1a.cmml" xref="S4.SS2.p3.1.m1.1.1"></and><apply id="S4.SS2.p3.1.m1.1.1b.cmml" xref="S4.SS2.p3.1.m1.1.1"><eq id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"></eq><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝜏</ci><cn id="S4.SS2.p3.1.m1.1.1.4.cmml" type="float" xref="S4.SS2.p3.1.m1.1.1.4">0.1</cn></apply><apply id="S4.SS2.p3.1.m1.1.1c.cmml" xref="S4.SS2.p3.1.m1.1.1"><ci id="S4.SS2.p3.1.m1.1.1.5.cmml" xref="S4.SS2.p3.1.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2405.01280v2#S4.SS2.p3.1.m1.1.1.4.cmml" id="S4.SS2.p3.1.m1.1.1d.cmml" xref="S4.SS2.p3.1.m1.1.1"></share><cn id="S4.SS2.p3.1.m1.1.1.6.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\tau{=}0.1{\rightarrow}1</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_τ = 0.1 → 1</annotation></semantics></math>) begins dropping performance after 10,000 steps training in EN-JA, indicating that the speed of annealing will significantly affect the model training quality.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">We also quickly surveyed the relationship between performance and the number of decoding iterations in RL.
The model performance dropped when we reduced the number of iterations to 2 during training and remained flat when we increased it to 4, indicating that our setting is reasonable.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper explored the application of reinforcement learning to edit-based non-autoregressive neural machine translation.
By incorporating RL into the training process, we achieved a significant performance improvement.
By empirically comparing stepwise and episodic reward maximization, we analyzed the advantages and disadvantages of these RL approaches.
We plan to have a deeper exploration of stepwise reward maximization and find a way to alleviate training inequality for multiple edit operations in the future.
</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our investigation of temperature settings in NAR softmax sampling provided insights into striking a balance between exploration and exploitation during training.
Although our annealing methods perform well, they are not optimal and still depend on manually adjusting the parameters such as total training steps.
In the future, we plan to develop a self-adaption temperature control method using various indicators like entropy and advantage SD.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The experiments in this paper focused on the basics, and we plan to do more study for practical applications in future work.
As our methods are orthogonal to existing research on NAR architectures, our next step involves exploring the methods’ applicability across a broader spectrum, including state-of-the-art models.
Additionally, we plan to investigate how to effectively apply RL to the distillation model, the impact of different baseline designs on performance, and the impact of RL on output diversity.
Applying RL to NAR is a massive and complex research question.
We look forward to more researchers joining this topic.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2016)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016.

</span>
<span class="ltx_bibblock">An actor-critic algorithm for sequence prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:1607.07086</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2014)</span>
<span class="ltx_bibblock">
Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/W14-3302" title="">Findings of the 2014 workshop on statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Ninth Workshop on Statistical Machine Translation</em>, pages 12–58, Baltimore, Maryland, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choshen et al. (2020)</span>
<span class="ltx_bibblock">
Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=H1eCw3EKvH" title="">On the weaknesses of reinforcement learning for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding and Soricut (2017)</span>
<span class="ltx_bibblock">
Nan Ding and Radu Soricut. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1709.09346" title="">Cold-start reinforcement learning with softmax policy gradient</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazvininejad et al. (2019)</span>
<span class="ltx_bibblock">
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1633" title="">Mask-predict: Parallel decoding of conditional masked language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 6112–6121, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2018)</span>
<span class="ltx_bibblock">
Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1711.02281" title="">Non-autoregressive neural machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2019)</span>
<span class="ltx_bibblock">
Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019.

</span>
<span class="ltx_bibblock">Levenshtein transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. (2016)</span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole. 2016.

</span>
<span class="ltx_bibblock">Categorical reparameterization with gumbel-softmax.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1611.01144</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiegeland and Kreutzer (2021)</span>
<span class="ltx_bibblock">
Samuel Kiegeland and Julia Kreutzer. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.133" title="">Revisiting the weaknesses of reinforcement learning for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1673–1681, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein (1965)</span>
<span class="ltx_bibblock">
Vladimir I. Levenshtein. 1965.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:60827152" title="">Binary codes capable of correcting deletions, insertions, and reversals</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Soviet physics. Doklady</em>, 10:707–710.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yifan Li, Kun Zhou, Wayne Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.06574" title="">Diffusion models for non-autoregressive text generation: A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo (2020)</span>
<span class="ltx_bibblock">
Ruotian Luo. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2003.09971" title="">A better variant of self-critical sequence training</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakazawa et al. (2017)</span>
<span class="ltx_bibblock">
Toshiaki Nakazawa, Shohei Higashiyama, Chenchen Ding, Hideya Mino, Isao Goto, Hideto Kazawa, Yusuke Oda, Graham Neubig, and Sadao Kurohashi. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W17-5701" title="">Overview of the 4th workshop on Asian translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</em>, pages 1–54, Taipei, Taiwan. Asian Federation of Natural Language Processing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranzato et al. (2016)</span>
<span class="ltx_bibblock">
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1511.06732" title="">Sequence level training with recurrent neural networks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2023)</span>
<span class="ltx_bibblock">
Machel Reid, Vincent Josua Hellendoorn, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nG9RF9z1yy3" title="">DiffusER: Diffusion via edit-based reconstruction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rennie et al. (2017)</span>
<span class="ltx_bibblock">
Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017.

</span>
<span class="ltx_bibblock">Self-critical sequence training for image captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2021)</span>
<span class="ltx_bibblock">
Chenze Shao, Yang Feng, Jinchao Zhang, Fandong Meng, and Jie Zhou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli_a_00421" title="">Sequence-level training for non-autoregressive neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Computational Linguistics</em>, 47(4):891–925.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stern et al. (2019)</span>
<span class="ltx_bibblock">
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019.

</span>
<span class="ltx_bibblock">Insertion transformer: Flexible sequence generation via insertion operations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Machine Learning</em>, pages 5976–5985. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutton and Barto (2018)</span>
<span class="ltx_bibblock">
Richard S. Sutton and Andrew G. Barto. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://incompleteideas.net/book/the-book-2nd.html" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1.1">Reinforcement Learning: An Introduction</em></a>, second edition.

</span>
<span class="ltx_bibblock">The MIT Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams (1992)</span>
<span class="ltx_bibblock">
Ronald J Williams. 1992.

</span>
<span class="ltx_bibblock">Simple statistical gradient-following algorithms for connectionist reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Reinforcement learning</em>, pages 5–32.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016)</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1609.08144" title="">Google’s neural machine translation system: Bridging the gap between human and machine translation</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Pseudo code of stepwise reward maximization</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We show pseudo code of stepwise reward maximization in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A1.F3" title="Figure 3 ‣ Appendix A Pseudo code of stepwise reward maximization ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="414" id="A1.F3.g1" src="extracted/5705646/imgs/stepwise.png" width="544"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The pseudo code of stepwise reward maximization.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Pseudo code of episodic reward maximization</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We show pseudo code of episodic reward maximization in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.01280v2#A2.F4" title="Figure 4 ‣ Appendix B Pseudo code of episodic reward maximization ‣ Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="A2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="235" id="A2.F4.g1" src="extracted/5705646/imgs/episodic.png" width="544"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The pseudo code of episodic reward maximization.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul  2 13:41:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
