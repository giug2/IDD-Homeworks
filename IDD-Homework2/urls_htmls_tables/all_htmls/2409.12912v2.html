<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Relevance of Item-Co-Exposure For Exposure Bias Mitigation</title>
<!--Generated on Fri Sep 20 07:35:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12912v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S1" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S2" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S3" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S3.SS1" title="In 3. Results and Discussion ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Exposure frequency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S3.SS2" title="In 3. Results and Discussion ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Competition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S3.SS3" title="In 3. Results and Discussion ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Differences between discrete choice models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S4" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#A1" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Measured bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#A2" title="In The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Measured performance</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">The Relevance of Item-Co-Exposure For Exposure Bias Mitigation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thorsten Krause
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:thorsten.krause@dfki.de">thorsten.krause@dfki.de</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">German Research Center for AI</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Osnabrück</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alina Deriyeva
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:alina.deriyeva@dfki.de">alina.deriyeva@dfki.de</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">German Research Center for AI</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Osnabrück</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan H. Beinke
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jan.beinke@dfki.de">jan.beinke@dfki.de</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">German Research Center for AI</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Osnabrück</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gerrit Bartels
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gerrit.bartels@uni-osnabrueck.de">gerrit.bartels@uni-osnabrueck.de</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">University of Osnabrück</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Osnabrück</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">Germany</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Oliver Thomas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:oliver.thomas@dfki.de">oliver.thomas@dfki.de</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">German Research Center for AI</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Osnabrück</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">Germany</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id16.id1">Through exposing items to users, implicit feedback recommender systems influence the logged interactions, and, ultimately, their own recommendations.
This effect is called exposure bias and it can lead to issues such as filter bubbles and echo chambers.
Previous research employed the multinomial logit model (MNL) with exposure information to reduce exposure bias on synthetic data.</p>
<p class="ltx_p" id="id17.id2">This extended abstract summarizes our previous study in which we investigated whether (i) these findings hold for human-generated choices, (ii) other discrete choice models mitigate bias better, and (iii) an item’s estimated relevance can depend on the relevances of the other items that were presented with it.
We collected a data set of biased and unbiased choices in a controlled online user study and measured the effects of overexposure and competition.</p>
<p class="ltx_p" id="id18.id3">We found that (i) the discrete choice models effectively mitigated exposure bias on human-generated choice data, (ii) there were no significant differences in robustness among the different discrete choice models, and (iii) only multivariate discrete choice models were robust to competition between items.
We conclude that discrete choice models mitigate exposure bias effectively because they consider item-co-exposure.
Moreover, exposing items alongside more or less popular items can bias future recommendations significantly and item exposure must be tracked for overcoming exposure bias.
We consider our work vital for understanding what exposure bias it, how it forms, and how it can be mitigated.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>In CONSEQUENCES Workshop at RecSys ’24; October 14th, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Exposure bias arises when a previous recommendation policy influenced the logged users’ choices <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib12" title="">2020</a>)</cite>.
It is a property of implicit feedback data sets that affects future models during training <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib7" title="">2023</a>)</cite>.
Existing approaches debias learning from binary datasets that only contain positive user-item interactions but do not differentiate between rejects or non-observations <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib18" title="">2021</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib11" title="">2022</a>; Saito et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib14" title="">2020</a>)</cite>.
An alternative approach is changing how data are collected.
If the core problem is the unavailability of exposure information, then we could simply collect this information alongside the user-item interactions.
We would then only need to find a way for processing it.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This extended abstract summarizes our study on using discrete choice models for considering choice alternatives and mitigating exposure bias <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
Previous research proposed replacing the loss-functions of existing models with discrete choice models <cite class="ltx_cite ltx_citemacro_citep">(Train, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib15" title="">2009</a>)</cite> that can consider observed alternatives as well as the (sometimes fuzzy) transitivity through rationality that holds over multiple choices <cite class="ltx_cite ltx_citemacro_citep">(Çapan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib6" title="">2022</a>; Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib10" title="">2022</a>)</cite>.
Assuming that random utility maximization <cite class="ltx_cite ltx_citemacro_citep">(Train, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib15" title="">2009</a>)</cite> accurately captures users’ behavior, we can condition their choices on the choice sets, dropping the effect of exposure.
However, the only studies on this approach relied on synthetic data that fulfilled the standard MNL’s assumptions <cite class="ltx_cite ltx_citemacro_citep">(Çapan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib6" title="">2022</a>; Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib10" title="">2022</a>)</cite>.
Empirical evidence suggests that the MNL can be inaccurate for modeling human choices <cite class="ltx_cite ltx_citemacro_citep">(Blattberg and Wisniewski, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib5" title="">1989</a>; Benson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib4" title="">2016</a>)</cite>.
Hence, we defined our first research question:
<span class="ltx_text ltx_font_bold" id="S1.p2.1.1">RQ1.:</span> <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">Does the MNL reduce exposure bias on human choice data compared with traditional recommendation approaches?</span>
Further, we asked:
<span class="ltx_text ltx_font_bold" id="S1.p2.1.3">RQ2.:</span> <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">Can other discrete choice models reduce exposure bias more than the MNL?</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Moreover, existing approaches only consider items’ <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">exposure frequencies</em> as a source of exposure bias.
When recommendations include multiple items, an item’s choice probability can depend on the alternatives due to reference-point effects <cite class="ltx_cite ltx_citemacro_citep">(Abeler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib3" title="">2011</a>)</cite> or mututally exclusive choices.
Hence, which items are exposed together, i.e., <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">competition</em>, should also be considered a form of exposure bias.
Because binary data lack this information, no model that learns from binary data, including existing debiasing models, could counter this effect. We asked:
<span class="ltx_text ltx_font_bold" id="S1.p3.1.3">RQ3.:</span> <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">Can the composition of exposed items induce bias, and which models can mitigate it compared with exposure bias through non-uniform exposure frequencies?</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To measure the effects of exposure on discrete choice-based models and baselines, we collected a partially biased dataset in a controlled online user study, and analyzed how different exposure policies affect estimated item ranks.
The discrete choice-based models mitigated nearly all exposure bias while out-performing the baselines in accuracy.
Moreover, we found that only the discrete choice-based models mitigated exposure bias from competition.
Our implementation is publicly available on GitHub<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/krauthorDFKI/DiscreteChoiceForBiasMitigation" title="">https://github.com/krauthorDFKI/DiscreteChoiceForBiasMitigation</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Experimental Setup</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We aimed to measure how shifting the exposure distribution w.r.t.
(i) <em class="ltx_emph ltx_font_italic" id="S2.p1.1.1">exposure frequencies</em> and (ii) <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">competition</em> affects recommendations.
We elaborate on our procedure here as we deem it potentially useful to others and are unaware of any similar existing methods.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.8">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#S2.F1" title="Figure 1 ‣ 2. Experimental Setup ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the dataset structure. First, we collected partially biased choices in an online survey.
The item set consisted of 100 educational courses split into two 50-course subsets <math alttext="J_{A}" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">J</mi><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">𝐽</ci><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">J_{A}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_J start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><msub id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">J</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">𝐽</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
We drew a five-item subset <math alttext="J_{\mathrm{Bias}}" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><msub id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">J</mi><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">Bias</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝐽</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">Bias</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">J_{\mathrm{Bias}}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_J start_POSTSUBSCRIPT roman_Bias end_POSTSUBSCRIPT</annotation></semantics></math> from <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><msub id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">J</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">𝐽</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
Each participant made 40 choices from four-item choice sets of which
50% were uniformly sampled from <math alttext="J_{A}" class="ltx_Math" display="inline" id="S2.p2.5.m5.1"><semantics id="S2.p2.5.m5.1a"><msub id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mi id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">J</mi><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">𝐽</ci><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">J_{A}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">italic_J start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math>, 25% were uniformly sampled from <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p2.6.m6.1"><semantics id="S2.p2.6.m6.1a"><msub id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mi id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">J</mi><mi id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">𝐽</ci><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>, and 25% were sampled from <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p2.7.m7.1"><semantics id="S2.p2.7.m7.1a"><msub id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mi id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">J</mi><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1">subscript</csymbol><ci id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">𝐽</ci><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> but more likely to contain an item from <math alttext="J_{\mathrm{Bias}}" class="ltx_Math" display="inline" id="S2.p2.8.m8.1"><semantics id="S2.p2.8.m8.1a"><msub id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><mi id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml">J</mi><mi id="S2.p2.8.m8.1.1.3" xref="S2.p2.8.m8.1.1.3.cmml">Bias</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1">subscript</csymbol><ci id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2">𝐽</ci><ci id="S2.p2.8.m8.1.1.3.cmml" xref="S2.p2.8.m8.1.1.3">Bias</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">J_{\mathrm{Bias}}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m8.1d">italic_J start_POSTSUBSCRIPT roman_Bias end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.7">We then compiled dataset pairs from the choices so that each pair only differed in the exposure distribution on <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><msub id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">J</mi><mi id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">𝐽</ci><ci id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
The distributions were either (i) overexposing <math alttext="J_{\mathrm{Bias}}" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><msub id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">J</mi><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">Bias</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">𝐽</ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">Bias</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">J_{\mathrm{Bias}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_J start_POSTSUBSCRIPT roman_Bias end_POSTSUBSCRIPT</annotation></semantics></math> versus uniformly exposing all items or (ii) exposing the items from <math alttext="J_{\mathrm{Bias}}" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><msub id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml"><mi id="S2.p3.3.m3.1.1.2" xref="S2.p3.3.m3.1.1.2.cmml">J</mi><mi id="S2.p3.3.m3.1.1.3" xref="S2.p3.3.m3.1.1.3.cmml">Bias</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p3.3.m3.1.1.1.cmml" xref="S2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.2">𝐽</ci><ci id="S2.p3.3.m3.1.1.3.cmml" xref="S2.p3.3.m3.1.1.3">Bias</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">J_{\mathrm{Bias}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">italic_J start_POSTSUBSCRIPT roman_Bias end_POSTSUBSCRIPT</annotation></semantics></math> with popular versus unpopular competitors.
Crucially, we split the user set into two subsets <math alttext="U^{\mathrm{Train}}" class="ltx_Math" display="inline" id="S2.p3.4.m4.1"><semantics id="S2.p3.4.m4.1a"><msup id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">U</mi><mi id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml">Train</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">superscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">𝑈</ci><ci id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3">Train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">U^{\mathrm{Train}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.1d">italic_U start_POSTSUPERSCRIPT roman_Train end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="U^{\mathrm{Eval}}" class="ltx_Math" display="inline" id="S2.p3.5.m5.1"><semantics id="S2.p3.5.m5.1a"><msup id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mi id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml">U</mi><mi id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml">Eval</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1">superscript</csymbol><ci id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2">𝑈</ci><ci id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3">Eval</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">U^{\mathrm{Eval}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m5.1d">italic_U start_POSTSUPERSCRIPT roman_Eval end_POSTSUPERSCRIPT</annotation></semantics></math> and did not train on any choices of the users from <math alttext="U^{\mathrm{Eval}}" class="ltx_Math" display="inline" id="S2.p3.6.m6.1"><semantics id="S2.p3.6.m6.1a"><msup id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">U</mi><mi id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">Eval</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1">superscript</csymbol><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">𝑈</ci><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">Eval</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">U^{\mathrm{Eval}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.6.m6.1d">italic_U start_POSTSUPERSCRIPT roman_Eval end_POSTSUPERSCRIPT</annotation></semantics></math> on <math alttext="J_{B}" class="ltx_Math" display="inline" id="S2.p3.7.m7.1"><semantics id="S2.p3.7.m7.1a"><msub id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">J</mi><mi id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">𝐽</ci><ci id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.7.m7.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
This enabled distilling the effect of some users’ exposure on other users’ recommendations while avoiding information leakages.
For measuring (i) overexposure, we corrected for some items appearing more popular on some datasets by chance.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S2.F1.g1" src="x1.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
Dataset structure.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Exposure frequency</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#A1.F2" title="Figure 2 ‣ Appendix A Measured bias ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_tag">2</span></a> compares the bias from (i) overexposure between the models, i.e., how much higher the models ranked items due to overexposure, on average over 500 simulation repetitions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Clearly, all negative sampling-based approaches suffered strongly from exposure bias except for CPR <cite class="ltx_cite ltx_citemacro_citep">(Wan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib16" title="">2022</a>)</cite>.
Overexposing an item caused them to overestimate its rank by up to 17, which equals 34% of <math alttext="J_{B}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">J</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝐽</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
Note that we arbitrarily selected the rate of overexposure and cannot conclude on the real-world effect size.
CPR was the least biased model.
However, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#A2.F4" title="Figure 4 ‣ Appendix B Measured performance ‣ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation"><span class="ltx_text ltx_ref_tag">4</span></a> shows that it also performed close to the Random model in terms of nDCG.
In the main contribution, we discussed that some changes to negative sampling, that our dataset’s special structure required, could have corrupted CPR’s performance, which relies on a complex and unique sampling strategy.
Among the debiasing baselines, only MACR <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib17" title="">2021</a>)</cite> and BISER <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib11" title="">2022</a>)</cite> noticeably reduced bias, by about 33% of the other models.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Importantly, the discrete choice-based models exhibited almost no bias.
The MNL, GEV and BL underestimated the overexposed items’ popularity slightly.
Hence, including information about observed alternatives can remove exposure bias from overexposure almost entirely.
Even the univariate BL model was able to deal with this source of exposure bias.
From our observations, we answered <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">RQ1.</span> as follows:
<span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">Yes. The MNL reduced exposure bias on human choice data more than all baselines […].</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Competition</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">For exposure bias from (ii) competition, the picture was similar but not identical.
Negative sampling-based approaches were again more biased than the discrete choice-based approaches.
This time, the bias attributed for 8 to 10 item ranks, which equals up to 20% of set <math alttext="J_{B}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">J</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝐽</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">J_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_J start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>.
Again, this number does not necessarily apply for real-world use cases.
However, no debiasing baselines noticebly reduced bias, except for CPR, which we excluded from the analysis as discussed above.
Previous research had only focused on exposure bias from (i) exposure frequencies, apparently neglecting (ii) competition.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Strikingly, this time, only the multivariate models exhibited almost no bias.
BL was significantly biased, though less than the other univariate models.
Hence, we concluded, the multivariate property matters for adressing exposure bias from competition and confirmed <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">RQ3.</span>: <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Yes, we were able to induce bias by varying the competitiveness of choice sets. […] Only models that consider the choice set mitigated it without sacrificing predictive accuracy. </span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Differences between discrete choice models</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Within the selection of discrete choice models, we could not spot any significant differences in bias robustness or performance.
This suggests that either the precise specification of the discrete choice model does not matter or that the dataset was too small for measuring significant effects.
Because we lack a theoretical foundation for why one discrete choice model could be more or less biased than another, we leave this matter for future research.
This led us to answering <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">RQ2.</span> with:
<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">We did not find conclusive evidence that other discrete-choice models could reduce bias further. Future studies
should compare the models in more detail for a definite conclusion.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We summarized our previous study on how incorporating discrete choice models into implicit feedback recommender systems can mitigate exposure bias <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
We found that we can effectively reduce exposure bias by logging observed but not chosen items and considering them during training with discrete choice models. Moreover, we showed that the composition of choice sets is also a source of exposure bias that previous approaches neglected.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Our findings imply that logging policies should also track choice alternatives. Moreover, the real-world effect of exposure bias could be enormous. In our experiment, we overexposed items by a factor of <math alttext="3.2" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">3.2</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn id="S4.p2.1.m1.1.1.cmml" type="float" xref="S4.p2.1.m1.1.1">3.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">3.2</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">3.2</annotation></semantics></math>. On real-world datasets, exposure discrepancies can be much higher, for example on the <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">last.fm</em> dataset <cite class="ltx_cite ltx_citemacro_citep">(Abdollahpouri and Mansoury, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib2" title="">2020</a>)</cite>.
Hence, adjusting logging policies and implementing discrete choice models could fundamentally change recommendations.
However, how logging and processing choice alternatives affects computational complexity compared to negative sampling remains unclear.
Also, our findings have yet to be reproduced on real-world data. Some datasets that include item-co-exposure are the <em class="ltx_emph ltx_font_italic" id="S4.p2.1.2">finn.no</em> dataset <cite class="ltx_cite ltx_citemacro_citep">(Eide et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib8" title="">2021</a>)</cite> and this year’s RecSys challenge dataset from <em class="ltx_emph ltx_font_italic" id="S4.p2.1.3">Ekstra Bladet</em><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_ref_self">https://recsys.eb.dk/dataset/</span></span></span></span>.
Shortly after ours, another study considered inter-item effects on choice from a causal perspective <cite class="ltx_cite ltx_citemacro_citep">(Ruiz De villa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib13" title="">2024</a>)</cite>.
Given the increasing attention on this matter, we believe that our study shines important light on why choice alternatives must be considered, and on how framing the interaction with recommendations as a choice setting achieves just that.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdollahpouri and Mansoury (2020)</span>
<span class="ltx_bibblock">
Himan Abdollahpouri and Masoud Mansoury. 2020.

</span>
<span class="ltx_bibblock">Multi-sided exposure bias in recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2006.15772</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abeler et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Johannes Abeler, Armin Falk, Lorenz Goette, and David Huffman. 2011.

</span>
<span class="ltx_bibblock">Reference points and effort provision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">American Economic Review</em> 101, 2 (2011), 470–492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benson et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Austin R Benson, Ravi Kumar, and Andrew Tomkins. 2016.

</span>
<span class="ltx_bibblock">On the relevance of irrelevant alternatives. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 25th international conference on world wide web</em>. 963–973.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattberg and Wisniewski (1989)</span>
<span class="ltx_bibblock">
Robert C Blattberg and Kenneth J Wisniewski. 1989.

</span>
<span class="ltx_bibblock">Price-induced patterns of competition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Marketing science</em> 8, 4 (1989), 291–309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Çapan et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gökhan Çapan, İlker Gündoğdu, Ali Caner Türkmen, and Ali Taylan Cemgil. 2022.

</span>
<span class="ltx_bibblock">Dirichlet–Luce choice model for learning from interactions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">User Modeling and User-Adapted Interaction</em> 32, 4 (2022), 611–648.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">Bias and debias in recommender system: A survey and future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">ACM Transactions on Information Systems</em> 41, 3 (2023), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eide et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Simen Eide, David S Leslie, Arnoldo Frigessi, Joakim Rishaug, Helge Jenssen, and Sofie Verrewaere. 2021.

</span>
<span class="ltx_bibblock">Finn. no slates dataset: A new sequential dataset logging interactions, all viewed items and click responses/no-click for recommender systems research. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 15th ACM Conference on Recommender Systems</em>. 556–558.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krause et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Thorsten Krause, Alina Deriyeva, Jan H Beinke, Gerrit Y Bartels, and Oliver Thomas. 2024.

</span>
<span class="ltx_bibblock">Mitigating Exposure Bias in Recommender Systems–A Comparative Analysis of Discrete Choice Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">ACM Transactions on Recommender Systems</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krause et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Thorsten Krause, Daniel Stattkus, Alina Deriyeva, Jan Heinrich Beinke, and Oliver Thomas. 2022.

</span>
<span class="ltx_bibblock">Beyond the rating matrix: debiasing implicit feedback loops in collaborative filtering.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jae-woong Lee, Seongmin Park, Joonseok Lee, and Jongwuk Lee. 2022.

</span>
<span class="ltx_bibblock">Bilateral self-unbiased learning from biased implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 29–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Dugang Liu, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Weike Pan, and Zhong Ming. 2020.

</span>
<span class="ltx_bibblock">A general knowledge distillation framework for counterfactual recommendation via uniform data. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>. 831–840.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz De villa et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aleix Ruiz De villa, Gabriele Sottocornola, Ludovik Coba, Federico Lucchesi, and Bartłomiej Skorulski. 2024.

</span>
<span class="ltx_bibblock">Ranking the causal impact of recommendations under collider bias in k-spots recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">ACM Transactions on Recommender Systems</em> 2, 2 (2024), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saito et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata. 2020.

</span>
<span class="ltx_bibblock">Unbiased recommender learning from missing-not-at-random implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 501–509.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Train (2009)</span>
<span class="ltx_bibblock">
Kenneth E Train. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Discrete choice methods with simulation</em>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qi Wan, Xiangnan He, Xiang Wang, Jiancan Wu, Wei Guo, and Ruiming Tang. 2022.

</span>
<span class="ltx_bibblock">Cross pairwise ranking for unbiased item recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the ACM Web Conference 2022</em>. 2370–2378.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He. 2021.

</span>
<span class="ltx_bibblock">Model-agnostic counterfactual reasoning for eliminating popularity bias in recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining</em>. 1791–1800.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021.

</span>
<span class="ltx_bibblock">Causal intervention for leveraging popularity bias in recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval</em>. 11–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Measured bias</h2>
<figure class="ltx_figure" id="A1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="647" id="A1.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>
Bias from overexposure from <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="644" id="A1.F3.g1" src="x3.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>
Bias from competition from <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Measured performance</h2>
<figure class="ltx_figure" id="A2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="454" id="A2.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>
Measured nDCG when training on data with uniform exposure frequencies and overexposure from <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="454" id="A2.F5.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>
Measured nDCG when training on data with popular and unpopular competition from <cite class="ltx_cite ltx_citemacro_citep">(Krause et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12912v2#bib.bib9" title="">2024</a>)</cite>.
</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 07:35:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
