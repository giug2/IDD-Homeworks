<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.08458] Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare</title><meta property="og:description" content="The advent of Federated Learning has enabled the creation of a high-performing model as if it had been trained on a considerable amount of data. A multitude of participants and a server cooperatively train a model with…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.08458">

<!--Generated on Tue Feb 27 09:45:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Security Privacy Vulnerabilities Attacks Threats Defenses.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>College of Computing, Sungkyunkwan University, 
<br class="ltx_break">Suwon, Korea (Republic of)
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{june.jeong}@g.skku.edu</span></span></span> and <span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{tmchung}@skku.edu</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hyejun Jeong [✉]
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tai-Myoung Chung* [✉]
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The advent of Federated Learning has enabled the creation of a high-performing model as if it had been trained on a considerable amount of data. A multitude of participants and a server cooperatively train a model without the need for data disclosure or collection. The healthcare industry, where security and privacy are paramount, can substantially benefit from this new learning paradigm, as data collection is no longer feasible due to stringent data policies. Nonetheless, unaddressed challenges and insufficient attack mitigation are hampering its adoption. Attack surfaces differ from traditional centralized learning in that the server and clients communicate between each round of training. In this paper, we thus present vulnerabilities, attacks, and defenses based on the widened attack surfaces, as well as suggest promising new research directions toward a more robust FL.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Security Privacy Vulnerabilities Attacks Threats Defenses.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Digital health has rapidly grown, and the COVID-19 outbreak accelerated its evolution. However, HIPAA reported that there were 712 healthcare data breaches in 2021, exceeding 2020 by 11% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and Verizon confirmed that data breaches in the healthcare industry increased by 58% during the pandemic. An Electronic Health Record (EHR) contains a wealth of sensitive private information about each patient, such as name, social security number, financial information, current and previous addresses, and medical history. However, traditional digital healthcare relies on centralized AI techniques that operate on a single location such as a server or data center for analytics; thus, it requires data collection. It is often not only time and resource-consuming but likely to violate stringent privacy protection policies, such as GDPR, CCPA, and HIPAA, that mandate securing patient health-related data management.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) decouples the use of AI techniques from gathering data by training a global model in a distributed manner under the orchestration of a central server (often referred to as an aggregator) and multiple local clients. The server updates the global model by aggregating the local models’ parameters, trained using each end-device data. In other words, data does not leave the data-owning devices, so it reduces the risk of raw training data being exposed in the middle of communication. FL thus came to light for its data privacy improvement, allowing learning without data leakage in situations where personal information must be protected. The advantages hold great promise to leverage AI techniques in the healthcare sector while complying with privacy policies.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the benefits, FL opens new attack surfaces and vulnerabilities that adversaries can exploit to harm the global model or leak the data. The expanded attack surfaces necessitate an updated vulnerability analysis to minimize the threat probabilities considering the rapidly growing needs of digital healthcare. This paper aims to introduce various vulnerabilities, attacks, and defenses, as well as open challenges and future directions toward more robust federated learning in the healthcare industry.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The rest of the paper is structured as follows: Section II identifies vulnerabilities, Section III categorizes attack methods by exploiting them, and Section IV introduces defense methods. Section V suggests future research directions, and finally, we conclude the paper in Section VI.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Sources of Vulnerabilities</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2401.08458/assets/images/taxonomy.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A Taxonomy of Federated Learning: Vulnerabilities, Attacks, and Defenses.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Identifying vulnerabilities in a system helps to mitigate and prevent potential attacks. FL creates a shared model by aggregating locally computed updates using client-specific data. FL involves three entities in <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">a distributed nature</span>: <span id="S2.p1.1.2" class="ltx_text ltx_font_bold">clients</span> in which each of them computes a local model based on its own dataset, <span id="S2.p1.1.3" class="ltx_text ltx_font_bold">a server</span> that aggregates the local model updates to recompute the latest global model, and <span id="S2.p1.1.4" class="ltx_text ltx_font_bold">communication channels</span> that the clients and the server communicate. Fig. <a href="#S2.F1" title="Figure 1 ‣ 2 Sources of Vulnerabilities ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> visualizes the relationship between the vulnerabilities, attacks, and defense strategies in a vanilla FL framework. 
<br class="ltx_break"></p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Compromisable Clients</span>
The FL workflow involves multiple clients computing local models based on each site-specific dataset inaccessible to other clients or the server. Thus, each client is a source of vulnerability. <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">Compromised clients</span> entail the following:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">clients unable to keep a stable connection to the server</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">clients equipped with insufficient computing resources for a model training</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">clients having insufficient quality or quantity of data for a fair contribution</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">clients with malicious intents to disturb the process or to plant backdoors</p>
</div>
</li>
</ul>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">A Compromisable Server</span> or an honest-but-curious server may attempt to infer the training data from the updates, alter the model parameters, and manipulate the aggregation algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The server is also vulnerable to flooding endangering its availability. 
<br class="ltx_break"></p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Outsiders or Eavesdroppers</span> intercept communications between the participating parties and steal the model parameters. They also can launch consecutive attacks, such as inference or MitM attacks, using the stolen information. The outsiders also might theft the final model parameters at the deployment phase to launch inference-time attacks. 
<br class="ltx_break"></p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">A Distributed Nature</span> opens attack surfaces related to communication, such as free-riding, DoS, MitM attacks, or communication bottlenecks. The participating clients’ not uniform data distribution and resources have additionally introduced non-malicious failure, resulting in detrimentally impacted model performance and extended training time.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Attacks and Threats</h2>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1" class="ltx_p" style="width:91.1pt;"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Threats</span></span>
</span>
</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p" style="width:52.0pt;"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Severity</span></span>
</span>
</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.1.1" class="ltx_p" style="width:277.5pt;"><span id="S3.T1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1" class="ltx_p" style="width:91.1pt;">Poisoning (§<a href="#S3.SS1" title="3.1 Poisoning Attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1" class="ltx_p" style="width:52.0pt;">High to Medium</span>
</span>
</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.3.1.1" class="ltx_p" style="width:277.5pt;">alter the training data or model parameters to modify the model’s behavior in a malicious direction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.1.1.1" class="ltx_p" style="width:91.1pt;">Inference (§<a href="#S3.SS2" title="3.2 Inference Attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_p" style="width:52.0pt;">High to Medium</span>
</span>
</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.1.1" class="ltx_p" style="width:277.5pt;">analyze the global or local model parameters to infer the information in the training dataset</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.1.1.1" class="ltx_p" style="width:91.1pt;">Backdoor (§<a href="#S3.SS3" title="3.3 Backdoor Attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.2.1.1" class="ltx_p" style="width:52.0pt;">High</span>
</span>
</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1" class="ltx_p" style="width:277.5pt;">insert hidden backdoor to train the global model on malicious tasks while the main tasks are not affected</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.1.1.1" class="ltx_p" style="width:91.1pt;">Comm Bottleneck (§<a href="#S3.SS4" title="3.4 Communication Bottleneck ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.2.1.1" class="ltx_p" style="width:52.0pt;">High</span>
</span>
</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.3.1.1" class="ltx_p" style="width:277.5pt;">congested communication due to the large size of the payload of the trained model parameters</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.1.1.1" class="ltx_p" style="width:91.1pt;">Free-riding (§<a href="#S3.SS5" title="3.5 Free-Riding Attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.2.1.1" class="ltx_p" style="width:52.0pt;">Medium</span>
</span>
</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.3.1.1" class="ltx_p" style="width:277.5pt;">fake contribution to effortlessly gain the global model</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.1.1.1" class="ltx_p" style="width:91.1pt;">MitM (§<a href="#S3.SS6" title="3.6 Man-in-the-Middle attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.2.1.1" class="ltx_p" style="width:52.0pt;">Medium to Low</span>
</span>
</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.3.1.1" class="ltx_p" style="width:277.5pt;">steal the model parameters in-between the endpoints to breach data or launch consequent attacks</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.1.1.1" class="ltx_p" style="width:91.1pt;">DoS (§<a href="#S3.SS7" title="3.7 DoS attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.7</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.2.1.1" class="ltx_p" style="width:52.0pt;">Low</span>
</span>
</td>
<td id="S3.T1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.3.1.1" class="ltx_p" style="width:277.5pt;">flood the server to harm its availability</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9" class="ltx_tr">
<td id="S3.T1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.1.1.1" class="ltx_p" style="width:91.1pt;">Client Dropout (§<a href="#S3.SS8" title="3.8 Client Dropout ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.8</span></a>)</span>
</span>
</td>
<td id="S3.T1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.2.1.1" class="ltx_p" style="width:52.0pt;">Low</span>
</span>
</td>
<td id="S3.T1.1.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S3.T1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.3.1.1" class="ltx_p" style="width:277.5pt;">forceful or accidental dropping out of participating clients due to resource instability</span>
</span>
</td>
</tr>
</table>
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The Severity and Short Description of Threats</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">By exploiting the vulnerabilities mentioned in Section <a href="#S2" title="2 Sources of Vulnerabilities ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, attackers aim to achieve two objectives: performance degradation and data leakage at training or testing time. Various actors attempt to harm a global model performance by disrupting its convergence, modifying the training data or the model updates, contribution-less participation, and incurring latency. The severity and short descriptions of each threat are summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Poisoning Attacks</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2401.08458/assets/images/poisoning_taxonomy.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="332" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A Taxonomy of Poisoning Attacks.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">FL framework is known as especially vulnerable to poisoning attacks due to its distributed and data-isolated nature. In poisoning attacks, the adversaries impact either the training dataset or local models in order to modify the behavior of the target model in some undesirable ways <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Poisoning attacks can be categorized based on the adversarial goal: untargeted and targeted attacks. Untargeted attacks, also known as random attacks, aim to reduce the overall accuracy of a global model. On the other hand, targeted attacks aim to induce the model to output the target class specified by the adversaries, while the other classes are predicted ordinarily. Targeted attacks are generally more complicated than random ones because it has a specific goal to achieve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> while minimizing their influences on non-targeted classes. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Poisoning Attacks ‣ 3 Attacks and Threats ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the categories of poisoning attacks. Based on what the adversaries attempt to manipulate, poisoning attacks are categorized into <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">data poisoning</span> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">model poisoning</span>. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Data Poisoning</span> attacks allow adversaries on the client side to alter the training dataset to compromise data integrity and modify the model’s behavior in an attacker-chosen direction. These attacks begin in the training phase, during local data collection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Data poisoning attacks fall into clean-label and dirty-label poisoning attacks. For the clean-label attacks, the adversary injects a few correctly labeled and minimally perturbed instances into the training data as if the adversarial or evasion attacks. In the FL environment, however, because the raw data instances are only observable to the data owner, dirty-label attacks prevail in which adversaries can freely change labels and data samples as they wish to misclassify <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. A representative dirty-label poisoning attack is a label-flipping attack, flipping the labels of two different classes to induce the global model to misclassify one class to another.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Model Poisoning</span> attacks allow client-side adversaries to directly modify the local model parameters at the training phase before sending them to the server. The adversarial clients also manipulate the model hyperparameter, such as the learning rate, number of the local epoch, the batch size, and the optimization objective, to manipulate the training rules before the local training. The attacks improved stealthiness by optimizing the local model for both training loss and an adversarial objective to avoid deviation from the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Accordingly, robust aggregation rules, such as Krum<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Bulyan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, trimmed-mean, coomed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> were proposed; however, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> have broken them by tailoring model updates in a malicious direction. It has also been shown that only one non-colluding malicious client achieves targeted misclassification with 100% confidence while ensuring the convergence of the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Model poisoning attacks are far more effective than data poisoning attacks because the malicious client’s updates are tuned to maximize the damage to the overall model performance while remaining stealthy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Inference Attacks</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Inference attacks target participant privacy during both the training and testing phases. A compromised client, an honest-but-curious server, or outsiders may want to infer participating clients’ training data. This type of attack is not explicit to FL but has been rampant in ML or DL. A DL model contains data properties that appear unrelated to the main tasks, mainly because the gradients of a given layer are computed using this layer’s features and errors backpropagated from the layer above <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Hence, trained model gradients contain extra information about the unintentional features of participants’ training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The attackers thus infer a substantial amount of private information such as <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">membership</span>, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">properties</span>, and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">class representatives</span>. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Membership Inference</span> attacks are arguably the most basic privacy attack that infers the presence of a particular sample in the sensitive training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The attackers, for example, may learn whether a particular patient profile was used to train the model linked with a disease, exposing that the particular patient went to the hospital for the associated disease <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Shokri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> constructed shadow models that imitate the target model’s behavior to distinguish the target model’s output on membership versus non-members of its training dataset. Nasr et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> exploit the vulnerability of the SGD algorithm, gradually influencing some parameters to adapt themselves towards reducing the loss. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Property Inference</span> attacks allow an adversarial party to infer whether the training dataset has specific general properties seemingly unrelated to the model’s primary task. For instance, a model is originally trained on facial images to predict if someone’s mouth is open (primary task); the attacker’s goal is to infer whether the training dataset is gender-balanced (inferred property) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Adversarial clients can identify when a property appears and disappears in the data during the training phase by inferring from the history of global models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. 
<br class="ltx_break"></p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Class Representatives Inference</span> attacks allow adversarial parties to reconstruct sensitive training data features by taking advantage of their correlation with the model output <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. It had been demonstrated that attackers could correctly characterize features of the class from simple models, such as logistic regression and decision trees, with no false positives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Recent efforts utilize Generative Adversarial Networks (GANs) to produce synthetic class representations from training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> or even recover the exact training images or texts from the gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Nevertheless, it is less feasible in FL scenarios since GAN-generated representatives are only similar to the training data when the training datasets of participants are similar (i.e., IID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Backdoor Attacks</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Backdoor attacks include adversarial clients inserting triggers into the training data or model updates in order to train a global model on both backdoor and main tasks. On test data with the same trigger embedded, the model produces false-positive predictions with high confidence. Adversaries have also reinforced the detection evasion strategy by tuning the backdoored models to not diverge from other models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. For example, Bagdasaryan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> rewards the model for backdoor task accuracy and penalizes it for deviating from what the aggregator considers benign. Xie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> proposed a Distributed Backdoor Attack that decomposes a global trigger pattern into separate small local patterns and embeds them into the training sets of multiple adversarial parties. Their efforts reached a higher attack success rate than inserting one global pattern. Because the malevolent behavior only appears when the triggers are present at test time, it is difficult and time-consuming to identify the existence of backdoor attacks.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Communication Bottleneck</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Although FL has reduced communication costs by transmitting training models rather than much larger quantities of data, the communication overhead is still of the utmost importance, especially with larger deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Participating clients are typically large in number and have slow or unstable internet connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. System heterogeneity—an inequality of computation and communication capabilities across the clients—causes asymmetric arrival timing at the server. Considering that 1 to 1000 clients participate in typical federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, accumulated delayed uplink time will ultimately result in a substantial delay in training time.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Free-Riding Attacks</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Free-riding attacks refer to an effortless extraction of a trained model; attackers benefit from the global model while not contributing to the training process. Exploiting the distributed nature and the server’s blindness, they obtain the trained global model without affording their computing resources and data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Free-riders generate fake updates without training with their local datasets because they have to send something to the server on each round, even if they do not update the local model parameter during the iterative federated optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. As such, the attackers steal intellectual property and breach privacy.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Man-in-the-Middle attacks</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">In a Man-in-the-Middle (MitM) attack, the attackers position themselves in a conversation between the endpoints either to eavesdrop or to impersonate one of the participants, making it appear as if a normal exchange is taking place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. They aim to jeopardize confidentiality by eavesdropping, integrity by manipulation, and availability by interrupting the communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The attackers intercept the client-server connection and replace the model parameters with malicious updates or make a shadow model to enable consecutive attacks such as model poisoning, backdoor, or inference attacks.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>DoS attacks</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Denial-of-Service (DoS) attacks include an insider (compromised client) or outsider flooding a server with traffic to compromise resource availability. DoS attacks thus disrupt the server with receiving, computing, and sending model parameters. It has been a known threat in the computer network domain for a long time. For instance, Fung et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> simulated three types of DoS attacks by increasing training time, bandwidth, and CPU usage at the server and clients. As the model could not be adequately trained, making a final model will be much more expensive in computation and communication.</p>
</div>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>Client Dropout</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">Clients could be accidentally or forcefully dropped out of the procedure, which may yield ineffective results and raise concerns about fairness. Mobile devices on the client-side might frequently be offline or on slow and costly connections at any time owing to user behaviors or the unpredictable network environment in which they are located <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. However, because a server cannot tell whether the clients are malicious or heavily non-IID, it may exclude the clients from future rounds, thus producing a less generalized global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Mitigation Techniques</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2401.08458/assets/images/mitigations.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A Category of Mitigation Techniques</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">A server has to secure the system by only examining the local model parameters. As illustrated in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2 Sources of Vulnerabilities ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a single technique can mitigate multiple issues or attacks simultaneously. A Fig. <a href="#S4.F3" title="Figure 3 ‣ 4 Mitigation Techniques ‣ Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> categorize the mitigation techniques. In this section, we introduce various mitigation strategies.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Anomaly Detection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Anomaly or outlier detection is a proactive strategy that employs analytical and statistical methods to filter out malicious occurrences that do not conform to an expected pattern or activity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. This technique primarily addresses poisoning, backdoor, free-riding, and DoS attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Fang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed LFR (Loss Function-based Rejection) and ERR (Error Rate-based Rejection) that reject client participation which negatively impacts the global model. Baruch et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> employed norm thresholding of client model update to remove models with boosted model parameters exceeding a specified threshold.
ZeKoC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and FLAME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> leverage various clustering algorithms, DBSCAN and HDBSCAN, to detect deviating clients. FoolsGold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> prevents Sybil-based attacks, inspired by the fact that Sybil clients’ gradients have unexpectedly high cosine similarities because they are trained for the same malicious objectives.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">However, anomaly detection techniques could easily fail when clients with highly non-IID datasets join. The detection algorithm may misclassify the benign clients as abnormal because of the unique distribution of model parameters owing to learning from the non-IID dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Robust Aggregation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Robust aggregation is a widely studied proactive strategy to reduce the impact of malicious model updates, used as a defense technique against poisoning and backdoor attacks. According to Shejwalkar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, robust aggregation algorithms (AGRs) remove the suspicious client based on the following criteria:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Distances from the benign gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Distributional differences with benign gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Differences in <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="L_{p}" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><msub id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml"><mi id="S4.I1.i3.p1.1.m1.1.1.2" xref="S4.I1.i3.p1.1.m1.1.1.2.cmml">L</mi><mi id="S4.I1.i3.p1.1.m1.1.1.3" xref="S4.I1.i3.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><apply id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I1.i3.p1.1.m1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.I1.i3.p1.1.m1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.1.1.2">𝐿</ci><ci id="S4.I1.i3.p1.1.m1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">L_{p}</annotation></semantics></math>-norms of benign and malicious gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite></p>
</div>
</li>
</ol>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The first criteria-based methods measure the pair-wise cosine distance between all clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or Euclidean distances between all clients and the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to identify malicious clients. Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> followed the second criteria to address free-riding attacks; they measured the clients’ potential contributions using Gaussian Distribution. Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> followed the second and third criteria to address label-flipping attacks. Inspired by the distinct weight distribution of malicious and benign clients, they calculated the L2 norm of local weights and compared it with a specified threshold. The server omits suspicious clients whose value is below the threshold from further aggregation. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> did not directly analyze the model parameter in the parameter space. Found that Penultimate Layer Representations (PLRs) in latent space are highly differentiating features for the poisonous models, they measured the Euclidean distance of the PLRs to estimate the trust score of local models’ updates to determine the amount of weight on each local model when it comes to aggregation.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The degree of non-IID is directly proportional to the impact of attacks because when the data distributions are highly non-IID, it is difficult for an aggregation algorithm to detect and remove the malicious clients reliably <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Regarding that IID data assumption often does not hold in practice, implementing a robust AGR without any assumptions comes as a challenge.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pruning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Pruning refers to reducing the model size by dropping neurons, thus relaxing computational complexity and communication bottleneck <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, as well as addressing backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Inspired by that backdoors exploit the spare capacity in the neural network, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> dropped the backdoor attack success rate by removing spare neurons that might have been trained for backdoor tasks and fine-tuning the parameters after the training phase.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Differential Privacy</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Differential Privacy (DP) introduces additional noise to the client’s sensitive data so that the attacker cannot meaningfully distinguish a single data record from the rest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. DP is initially believed to withstand privacy attacks, such as inference attacks, but it can also inherently defend against poisoning and backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Miao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> bounded the norm of malicious updates by adaptively setting a proper clipping threshold throughout the training process to eliminate backdoors and enhance the main task accuracy.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Although DP is one of the preferred techniques due to its low computational overhead and privacy quantification properties, inserted perturbation often suggests a trade-off between performance and privacy. To this extent, Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> estimate a minimal amount of noise to ensure the elimination of backdoors while maintaining the benign performance of the global model.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Moving Target Defense</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Moving Target Defense (MTD) is a proactive defense strategy against MitM attacks. Continually randomizing FL system modules obscures the vulnerability source from attackers, thereby increasing the cost and complexity of locating the exact target <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. As such, the added dynamics consequently reduce the likelihood of successful attacks and increase the system resiliency while limiting the disclosure of system vulnerabilities and opportunities for attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> introduced ADS-MTD, a double-shuffle system comprising model and client-shuffling components. Specifically, MTD takes place in the second phase as a hierarchical multi-shuffler structure to dynamically and efficiently assess and eliminate malicious FL participants to enhance the aggregated model’s integrity and availability.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Trusted Execution Environment</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">A Trusted Execution Environment (TEE) protects the FL system during a training phase by allocating a separate region for code execution and data handling. TEE guarantees integrity and confidentiality of computations while incurring lower overhead but higher privacy compared to encryption-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
TEEs, however, have limited memory for computation in order to keep the Trusting Computing Base as small as possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> to minimize the attack surface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Thus, Mo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposed PPFL that enlarges its memory capability by greedy layer-wise training and aggregation. They demonstrated that PPFL achieved comparable accuracy while dealing with data heterogeneity and accelerating local training processes.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Encryption-based Methods</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">Encryption-based methods apply <span id="S4.SS7.p1.1.1" class="ltx_text ltx_font_italic">Homomorphic Encryption (HE)</span> and/or <span id="S4.SS7.p1.1.2" class="ltx_text ltx_font_italic">Secure Multi-party Computation (SMC)</span> to the model updates to combat privacy-related attacks, inference, and MitM attacks. Further, they can be combined with a perturbation-based method (i.e., DP) for more robustness. 
<br class="ltx_break"></p>
</div>
<div id="S4.SS7.p2" class="ltx_para ltx_noindent">
<p id="S4.SS7.p2.1" class="ltx_p"><span id="S4.SS7.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Homomorphic Encryption</span> enables data to be processed without decryption, and the outcome of a homomorphic operation after decryption is equivalent to the operation on plain data. Since only encrypted parameters are communicated, and the server only views and computes over the encrypted parameters, it thus can protect the data from inference attacks by a compromised server or eavesdroppers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p">However, its effectiveness comes at a substantial amount of computational, communicational, and memory overhead hampering its applicability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> thus proposed BatchCrypt that sped up training time while reducing communication overhead by encoding the gradients into long integers in a batch followed by gradient-wise aggregation. 
<br class="ltx_break"></p>
</div>
<div id="S4.SS7.p4" class="ltx_para ltx_noindent">
<p id="S4.SS7.p4.1" class="ltx_p"><span id="S4.SS7.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Secure Multi-party Computation</span> is a cryptographic protocol that distributes a computation process across multiple parties, with no single party having access to the data of the others. For instance, an encryption key may be divided into shares so that no individual possesses all the components needed to reassemble the key completely. SMC is a preferred approach because it is 1000 times faster than HE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Hao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Truex et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> integrate SMC and Gaussian-based DP to mitigate privacy threats launched by multiple colluding clients, balancing performance and privacy guarantee trade-offs by injecting a reduced amount of perturbation with the aid of SMC.</p>
</div>
<div id="S4.SS7.p5" class="ltx_para">
<p id="S4.SS7.p5.1" class="ltx_p">Applying SMC to FL, on the other hand, generally demands all parties to produce and exchange secret shares with all other parties. This procedure inevitably introduces a substantial communication overhead, exponentially growing with the number of participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Federated Distillation </h3>

<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">Federated Distillation (FD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is a variant of the model compression techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> to effectively handle communication bottlenecks and heterogeneity and to combat inference attacks. FD refers to transferring knowledge from a large and fully trained teacher model to another small student model without losing validity. Sharing knowledge instead of model weights saves communication and computational costs in resource-restricted local devices as well as protects the model information from being interpreted for inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> built an auxiliary generator in a server to fine-tune the model aggregation procedure. Exploiting its powerful processing capability, the server safely explores knowledge in local models and adapts them to global models.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Research Direction</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Despite the intensive studies on FL, there still exists room for improvement. In this section, we suggest future research direction that needs more attention. 
<br class="ltx_break"></p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Standardization</span>
Multiple institutions across the globe use different languages, and each institution has its own convention to format data instances. It means that local models are trained on differently formatted data instances, thus might incurring unnecessary deviation. Hence, the local models should go through a proper preprocessing step to standardize different types and contents of data. 
<br class="ltx_break"></p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Various Type of Data</span>
Data are often not limited to a single type; for example, EHR contains texts, images, or tables. Numerous papers are, however, limited in that they have tested their methods on simple image datasets, such as the variety of MNISTs or CIFAR-10. Thus, their superiority might not stand out on text or a combination of multi-types of datasets. Moreover, most papers on backdoor attacks had been only simulated on image classification tasks. More experiments with various types of datasets are needed for more practicality. 
<br class="ltx_break"></p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Incentive Mechanism</span>
A sufficient incentive mechanism is needed as well. Large institutions are highly likely to have enough data (in terms of quality and quantity) to build a model that makes predictions with reasonable confidence. If the coordination deteriorates performance, security, or privacy, they have no reason to participate and contribute, consuming their resources or risking privacy. A robust data quality verification could be a possible solution for encouragement. By ensuring that variable data produces a more generalizable and performance-enhanced global model, they may be willing to join. 
<br class="ltx_break"></p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Balancing Trade-off</span>
Although multiple efforts have tried to prevent sudden client dropout, privacy is not well-preserved at the moment. For example, a server inquires about clients’ geolocation to form a homogeneous group for client selection to deal with system heterogeneity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Nishio et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> limit their global model to a simple DNN structure at the cost of accuracy. More research should be conducted to find a harmonious balance between privacy and performance in applying FL to real-world practice. 
<br class="ltx_break"></p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">United and Non-orthogonal Defense techniques</span>
All defensive methods are not mutually exclusive; a single defense technique can thwart multiple attacks. Nevertheless, existing countermeasures are mostly studied separately and orthogonal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. For example, some encryption-based methods come at a high communication cost, causing communication bottlenecks, and perturbation-based methods sacrifice performance. Defense objectives should be congratulated for more practicality.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Federated Learning has been suggested as a new paradigm for utilizing AI techniques in data-sensitive industries. To train a comparable AI model thus far, data have been collected centrally, putting privacy at risk and incurring substantial communication costs. Such data collection, however, is no longer possible due to stringent regulations. FL comes with the benefit of creating well-performing ML/DL models without data disclosure or collection as if being trained on extensive data. It did, however, enlarge attack surfaces and introduce new vulnerabilities. Immense research works have been conducted to mitigate the vulnerabilities, yet open challenges still exist prohibiting its practical application. We expect that with this survey on vulnerabilities, attacks, and defense, researchers would pay greater attention to the unmet needs.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was supported by Healthcare AI Convergence Research &amp; Development Program through the National IT Industry Promotion Agency of Korea (NIPA) funded by Ministry of Science and ICT (No. S1601-20-1041).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
AbdulRahman, S., Tout, H., Mourad, A., Talhi, C.: Fedmccs: Multicriteria client
selection model for optimal iot federated learning. IEEE Internet of Things
Journal <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">8</span>(6), 4723–4735 (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Awan, S., Luo, B., Li, F.: Contra: Defending against poisoning attacks in
federated learning. In: European Symposium on Research in Computer Security.
pp. 455–475. Springer (2021)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., Shmatikov, V.: How to backdoor
federated learning. In: International Conference on Artificial Intelligence
and Statistics. pp. 2938–2948. PMLR (2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Baruch, G., Baruch, M., Goldberg, Y.: A little is enough: Circumventing
defenses for distributed learning. Advances in Neural Information Processing
Systems <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">32</span> (2019)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Benmalek, M., Benrekia, M.A., Challal, Y.: Security of federated learning:
Attacks, defensive mechanisms, and challenges. Revue des Sciences et
Technologies de l’Information-Série RIA: Revue d’Intelligence
Artificielle <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">36</span>(1), 49–59 (2022)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bhagoji, A.N., Chakraborty, S., Mittal, P., Calo, S.: Analyzing federated
learning through an adversarial lens. In: International Conference on Machine
Learning. pp. 634–643. PMLR (2019)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Bhushan, B., Sahoo, G., Rai, A.K.: Man-in-the-middle attack in wireless and
computer networking—a review. In: 2017 3rd International Conference on
Advances in Computing, Communication &amp; Automation (ICACCA)(Fall). pp. 1–6.
IEEE (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Blanchard, P., El Mhamdi, E.M., Guerraoui, R., Stainer, J.: Machine learning
with adversaries: Byzantine tolerant gradient descent. Advances in Neural
Information Processing Systems <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Bouacida, N., Mohapatra, P.: Vulnerabilities in federated learning. IEEE Access
<span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">9</span>, 63229–63249 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cao, X., Fang, M., Liu, J., Gong, N.Z.: Fltrust: Byzantine-robust federated
learning via trust bootstrapping. arXiv preprint arXiv:2012.13995 (2020)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen, Z., Tian, P., Liao, W., Yu, W.: Zero knowledge clustering based
adversarial mitigation in heterogeneous federated learning. IEEE Transactions
on Network Science and Engineering <span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">8</span>(2), 1070–1083 (2020)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fang, H., Qian, Q.: Privacy preserving machine learning with homomorphic
encryption and federated learning. Future Internet <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">13</span>(4),  94
(2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Fang, M., Cao, X., Jia, J., Gong, N.: Local model poisoning attacks to
<math id="bib.bib13.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib13.1.m1.1a"><mo stretchy="false" id="bib.bib13.1.m1.1.1" xref="bib.bib13.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib13.1.m1.1b"><ci id="bib.bib13.1.m1.1.1.cmml" xref="bib.bib13.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.1.m1.1c">\{</annotation></semantics></math>Byzantine-Robust<math id="bib.bib13.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib13.2.m2.1a"><mo stretchy="false" id="bib.bib13.2.m2.1.1" xref="bib.bib13.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib13.2.m2.1b"><ci id="bib.bib13.2.m2.1.1.cmml" xref="bib.bib13.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.2.m2.1c">\}</annotation></semantics></math> federated learning. In: 29th USENIX Security
Symposium (USENIX Security 20). pp. 1605–1622 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Fraboni, Y., Vidal, R., Lorenzi, M.: Free-rider attacks on model aggregation in
federated learning. In: International Conference on Artificial Intelligence
and Statistics. pp. 1846–1854. PMLR (2021)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Fredrikson, M., Jha, S., Ristenpart, T.: Model inversion attacks that exploit
confidence information and basic countermeasures. In: Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security. pp. 1322–1333
(2015)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Fung, C., Yoon, C.J., Beschastnikh, I.: The limitations of federated learning
in sybil settings. In: 23rd International Symposium on Research in Attacks,
Intrusions and Defenses (RAID 2020). pp. 301–316 (2020)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Geiping, J., Bauermeister, H., Dröge, H., Moeller, M.: Inverting
gradients-how easy is it to break privacy in federated learning? Advances in
Neural Information Processing Systems <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">33</span>, 16937–16947 (2020)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Guerraoui, R., Rouault, S., et al.: The hidden vulnerability of distributed
learning in byzantium. In: International Conference on Machine Learning. pp.
3521–3530. PMLR (2018)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Haddadpour, F., Kamani, M.M., Mokhtari, A., Mahdavi, M.: Federated learning
with compression: Unified analysis and sharp guarantees. In: International
Conference on Artificial Intelligence and Statistics. pp. 2350–2358. PMLR
(2021)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Hao, M., Li, H., Luo, X., Xu, G., Yang, H., Liu, S.: Efficient and
privacy-enhanced federated learning for industrial artificial intelligence.
IEEE Transactions on Industrial Informatics <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">16</span>(10), 6532–6542
(2019)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Ho, S., Qu, Y., Gu, B., Gao, L., Li, J., Xiang, Y.: Dp-gan: Differentially
private consecutive data publishing using generative adversarial nets.
Journal of Network and Computer Applications <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">185</span>, 103066 (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Huang, W., Li, T., Wang, D., Du, S., Zhang, J.: Fairness and accuracy in
federated learning. arXiv preprint arXiv:2012.10069 (2020)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., Kim, S.L.:
Communication-efficient on-device machine learning: Federated distillation
and augmentation under non-iid private data. arXiv preprint arXiv:1811.11479
(2018)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jiang, Y., Wang, S., Valls, V., Ko, B.J., Lee, W.H., Leung, K.K., Tassiulas,
L.: Model pruning enables efficient federated learning on edge devices. IEEE
Transactions on Neural Networks and Learning Systems (2022)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Journal, H.: December 2021 healthcare data breach report (Jun 2022),
<a target="_blank" href="https://www.hipaajournal.com/december-2021-healthcare-data-breach-report/" title="" class="ltx_ref">https://www.hipaajournal.com/december-2021-healthcare-data-breach-report/</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kanagavelu, R., Li, Z., Samsudin, J., Yang, Y., Yang, F., Goh, R.S.M., Cheah,
M., Wiwatphonthana, P., Akkarajitsakul, K., Wang, S.: Two-phase multi-party
computation enabled privacy-preserving federated learning. In: 2020 20th
IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing
(CCGRID). pp. 410–419. IEEE (2020)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Khosravy, M., Nakamura, K., Hirose, Y., Nitta, N., Babaguchi, N.: Model
inversion attack by integration of deep generative models: Privacy-sensitive
face generation from a face recognition system. IEEE Transactions on
Information Forensics and Security <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">17</span>, 357–372 (2022).
https://doi.org/10.1109/TIFS.2022.3140687

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Konečnỳ, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh,
A.T., Bacon, D.: Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492 (2016)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Li, W., Xia, Y., Lu, L., Chen, H., Zang, B.: Teev: Virtualizing trusted
execution environments on mobile platforms. In: Proceedings of the 15th ACM
SIGPLAN/SIGOPS International Conference on Virtual Execution Environments.
pp. 2–16 (2019)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Lin, J., Du, M., Liu, J.: Free-riders in federated learning: Attacks and
defenses. arXiv preprint arXiv:1911.12560 (2019)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lu, Y., Fan, L.: An efficient and robust aggregation algorithm for learning
federated cnn. In: Proceedings of the 2020 3rd International Conference on
Signal Processing and Machine Learning. pp. 1–7 (2020)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Ma, X., Sun, L., Zhao, J., Yang, Q., Yu, P.S.: Privacy and
robustness in federated learning: Attacks and defenses. arXiv preprint
arXiv:2012.06337 (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Lyu, L., Yu, H., Yang, Q.: Threats to federated learning: A survey. arXiv
preprint arXiv:2003.02133 (2020)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ma, Y., Zhu, X., Hsu, J.: Data poisoning against differentially-private
learners: Attacks and defenses. arXiv preprint arXiv:1903.09860 (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mallik, A.: Man-in-the-middle-attack: Understanding in simple words.
Cyberspace: Jurnal Pendidikan Teknologi Informasi <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">2</span>(2), 109–134
(2019)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Melis, L., Song, C., De Cristofaro, E., Shmatikov, V.: Exploiting unintended
feature leakage in collaborative learning. In: 2019 IEEE symposium on
security and privacy (SP). pp. 691–706. IEEE (2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Miao, L., Yang, W., Hu, R., Li, L., Huang, L.: Against backdoor attacks in
federated learning with differential privacy. In: ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP).
pp. 2999–3003. IEEE (2022)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Mo, F., Haddadi, H.: Efficient and private federated learning using tee. In:
Proc. EuroSys Conf., Dresden, Germany (2019)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Mo, F., Haddadi, H., Katevas, K., Marin, E., Perino, D., Kourtellis, N.: Ppfl:
privacy-preserving federated learning with trusted execution environments.
In: Proceedings of the 19th Annual International Conference on Mobile
Systems, Applications, and Services. pp. 94–108 (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Mothukuri, V., Parizi, R.M., Pouriyeh, S., Huang, Y., Dehghantanha, A.,
Srivastava, G.: A survey on security and privacy of federated learning.
Future Generation Computer Systems <span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">115</span>, 619–640 (2021)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Nasr, M., Shokri, R., Houmansadr, A.: Comprehensive privacy analysis of deep
learning: Passive and active white-box inference attacks against centralized
and federated learning. In: 2019 IEEE symposium on security and privacy (SP).
pp. 739–753. IEEE (2019)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Nguyen, T.D., Rieger, P., Chen, H., Yalame, H., Möllering, H., Fereidooni,
H., Marchal, S., Miettinen, M., Mirhoseini, A., Zeitouni, S., et al.: Flame:
Taming backdoors in federated learning. Cryptology ePrint Archive (2021)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Nishio, T., Yonetani, R.: Client selection for federated learning with
heterogeneous resources in mobile edge. In: ICC 2019-2019 IEEE international
conference on communications (ICC). pp. 1–7. IEEE (2019)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Pan, X., Zhang, M., Ji, S., Yang, M.: Privacy risks of general-purpose language
models. In: 2020 IEEE Symposium on Security and Privacy (SP). pp. 1314–1331.
IEEE (2020)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Parisot, M.P.M., Pejo, B., Spagnuelo, D.: Property inference attacks on
convolutional neural networks: Influence and implications of target model’s
complexity. CoRR <span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">abs/2104.13061</span> (2021),
<a target="_blank" href="https://arxiv.org/abs/2104.13061" title="" class="ltx_ref">https://arxiv.org/abs/2104.13061</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Shejwalkar, V., Houmansadr, A.: Manipulating the byzantine: Optimizing model
poisoning attacks and defenses for federated learning. In: NDSS (2021)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks
against machine learning models. In: 2017 IEEE Symposium on Security and
Privacy (SP). pp. 3–18 (2017). https://doi.org/10.1109/SP.2017.41

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Sun, Z., Kairouz, P., Suresh, A.T., McMahan, H.B.: Can you really backdoor
federated learning? arXiv preprint arXiv:1911.07963 (2019)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Truex, S., Baracaldo, N., Anwar, A., Steinke, T., Ludwig, H., Zhang, R., Zhou,
Y.: A hybrid approach to privacy-preserving federated learning. In:
Proceedings of the 12th ACM workshop on artificial intelligence and security.
pp. 1–11 (2019)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn,
J.y., Lee, K., Papailiopoulos, D.: Attack of the tails: Yes, you really can
backdoor federated learning. Advances in Neural Information Processing
Systems <span id="bib.bib50.1.1" class="ltx_text ltx_font_bold">33</span>, 16070–16084 (2020)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wang, N., Xiao, Y., Chen, Y., Hu, Y., Lou, W., Hou, Y.T.: Flare: Defending
federated learning against model poisoning attacks via latent space
representations. In: Proceedings of the 2022 ACM on Asia Conference on
Computer and Communications Security. pp. 946–958 (2022)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Wu, C., Yang, X., Zhu, S., Mitra, P.: Mitigating backdoor attacks in federated
learning. arXiv preprint arXiv:2011.01767 (2020)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Wu, C., Wu, F., Lyu, L., Huang, Y., Xie, X.: Communication-efficient federated
learning via knowledge distillation. Nature communications <span id="bib.bib53.1.1" class="ltx_text ltx_font_bold">13</span>(1),
 1–8 (2022)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Xie, C., Huang, K., Chen, P.Y., Li, B.: Dba: Distributed backdoor attacks
against federated learning. In: International Conference on Learning
Representations (2019)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Yin, D., Chen, Y., Kannan, R., Bartlett, P.: Byzantine-robust distributed
learning: Towards optimal statistical rates. In: International Conference on
Machine Learning. pp. 5650–5659. PMLR (2018)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhang, C., Li, S., Xia, J., Wang, W., Yan, F., Liu, Y.: <math id="bib.bib56.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib56.1.m1.1a"><mo stretchy="false" id="bib.bib56.1.m1.1.1" xref="bib.bib56.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib56.1.m1.1b"><ci id="bib.bib56.1.m1.1.1.cmml" xref="bib.bib56.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib56.1.m1.1c">\{</annotation></semantics></math>BatchCrypt<math id="bib.bib56.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib56.2.m2.1a"><mo stretchy="false" id="bib.bib56.2.m2.1.1" xref="bib.bib56.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib56.2.m2.1b"><ci id="bib.bib56.2.m2.1.1.cmml" xref="bib.bib56.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib56.2.m2.1c">\}</annotation></semantics></math>:
Efficient homomorphic encryption for <math id="bib.bib56.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib56.3.m3.1a"><mo stretchy="false" id="bib.bib56.3.m3.1.1" xref="bib.bib56.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib56.3.m3.1b"><ci id="bib.bib56.3.m3.1.1.cmml" xref="bib.bib56.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib56.3.m3.1c">\{</annotation></semantics></math>Cross-Silo<math id="bib.bib56.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib56.4.m4.1a"><mo stretchy="false" id="bib.bib56.4.m4.1.1" xref="bib.bib56.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib56.4.m4.1b"><ci id="bib.bib56.4.m4.1.1.cmml" xref="bib.bib56.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib56.4.m4.1c">\}</annotation></semantics></math> federated learning.
In: 2020 USENIX annual technical conference (USENIX ATC 20). pp. 493–506
(2020)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhang, L., Shen, L., Ding, L., Tao, D., Duan, L.Y.: Fine-tuning global model
via data-free knowledge distillation for non-iid federated learning. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 10174–10183 (2022)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Zhang, Y., Jia, R., Pei, H., Wang, W., Li, B., Song, D.: The secret revealer:
Generative model-inversion attacks against deep neural networks. In:
Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. pp. 253–261 (2020)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhou, Z., Xu, C., Wang, M., Ma, T., Yu, S.: Augmented dual-shuffle-based moving
target defense to ensure cia-triad in federated learning. In: 2021 IEEE
Global Communications Conference (GLOBECOM). pp. 01–06. IEEE (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.08457" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.08458" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.08458">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.08458" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.08460" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 09:45:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
