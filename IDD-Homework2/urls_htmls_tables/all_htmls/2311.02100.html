<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.02100] A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning</title><meta property="og:description" content="Advancement in the field of machine learning is unavoidable, but something of major concern is preserving the privacy of the users whose data is being used for training these machine learning algorithms. Federated lear…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.02100">

<!--Generated on Mon Feb 26 21:19:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
federated learning,  initialization,  model-training,  survey
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adwaita Janardhan Jadhav
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">*Corresponding Author: ishmeet3kk@gmail.com</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">adwaitas28@gmail.com</span>
 and Ishmeet Kaur


</span>Both authors, Ishmeet Kaur and Adwaita Janardhan Jadhav, are currently at Apple Inc. Both authors contributed equally to this work.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Advancement in the field of machine learning is unavoidable, but something of major concern is preserving the privacy of the users whose data is being used for training these machine learning algorithms. Federated learning(FL) has emerged as a promising paradigm for training machine learning models in a distributed and privacy-preserving manner which enables one to collaborate and train a global model without sharing local data. But starting this learning process on each device in the right way, called “model initialization” is critical. The choice of initialization methods used for models plays a crucial role in the performance, convergence speed, communication efficiency, privacy guarantees of federated learning systems, etc.
In this survey, we dive deeper into a comprehensive study of various ways of model initialization techniques in FL.Unlike other studies, our research meticulously compares, categorizes, and delineates the merits and demerits of each technique, examining their applicability across diverse FL scenarios.
We highlight how factors like client variability, data non-IIDness, model caliber, security considerations, and network restrictions influence FL model outcomes and propose how strategic initialization can address and potentially rectify many such challenges.
The motivation behind this survey is to highlight that the right start can help overcome challenges like varying data quality, security issues, and network problems. Our insights provide a foundational base for experts looking to fully utilize FL, also while understanding the complexities of model initialization.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
federated learning, initialization, model-training, survey

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the era of big data and machine learning, the conventional centralized model of data processing and training is increasingly being challenged by emerging paradigms that emphasize user privacy and data security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. One such paradigm is Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>.
Federated Learning is a decentralized learning approach that enables model training across multiple devices or servers while keeping data localized. Instead of transmitting raw data to a central server, devices compute model updates locally and then send these updates, ensuring user data privacy and reduced data transmission overheads. Sometimes this is even an iterative process till we receive the expected performance for the global model.
FL finds application in industries like healthcare, finance, and telecommunications, where data privacy is paramount and data-sharing can be a regulatory or logistical challenge, and in IoT devices or edge devices where resource constraint is an issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>.
However, despite its revolutionary approach, Federated Learning comes with its own set of challenges.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.02100/assets/myfile.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="204" height="155" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.02100/assets/myfile2.png" id="S1.F1.2.2.g1" class="ltx_graphics ltx_img_landscape" width="204" height="152" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of data distributions on two different clients. (a) Client 1 has a similar number of samples for all the input labels. This represents IID data. (b) Client 2 has varied numbers of samples for the input labels. This represents non-IID data.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Data Non-IIDness: In FL, data is often non-independent and identically distributed (non-IID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>. As you can see in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, This means that data across different nodes can be highly unbalanced or skewed, which can lead to models that do not generalize well across the network.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Diverse Node Characteristics: Nodes in FL can range from high-performance computing servers to everyday smartphones hence diverse node characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Communication constraints: Given the distributed nature of FL, network constraints such as latency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>, bandwidth limitations, and intermittent connectivity can hamper the efficient exchange of model updates.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Multiple efforts are being made in the direction of developing various algorithms and architectures to make Federated Learning more efficient.
Amidst these challenges, model initialization in FL emerges as a key factor determining the system’s overall success<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>. The inspiration to dive deeper into this topic arises from the advantages of effective model initialization. In decentralized datasets, optimizing training initiation can result in quicker convergence, improved computational efficiency, better model accuracy against data non-IIDness challenges, and a balanced learning experience across varied nodes without undue data skewness from any single node  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>.
To the best of our knowledge, this is the first study where methodically model initialization in FL is categorized and compared against FL dystem challenges and how it can benefit.
To further make our study clear, we have categorized initialization techniques in the FL system into 3 subcategories as below:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I2" class="ltx_enumerate">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">Localized Initialization: This focuses on leveraging device-specific data characteristics to initiate model training. These methods prioritize understanding and adapting to the unique data distribution of each participating node, promoting a more tailored and efficient learning process.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">Centralized Initialization: It utilizes a central server to initiate model training. This approach often involves pre-training a global model using vast datasets leveraging pre-existing models, which is then fine-tuned on edge devices through Federated Learning.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">Privacy-preserving Initialization: As the name suggests the emphasis is on preserving the privacy of users and avoiding data security issues while initializing the model.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the following sections, we dive deeper into the above categories and compare them against the FL systems challenges , then present metrics and evaluation recommendations and in the final section, we present our research recommendations and conclude this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Localized Initialization</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we have identified two subcategories: starting with random settings, known as Random Initialization, and tailoring the start based on specific client data, called Client-Specific Initialization.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Random Initialization</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the area of Federated Learning (FL), Random Initialization serves as a fundamental technique where model parameters, including weights and biases, are populated with values drawn from certain probability distributions  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>. The choice of distribution is paramount as it greatly influences the model’s convergence speed and performance. To list some common techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Uniform Distribution: Parameters are sampled uniformly with random values from a given range, such as [-0.5, 0.5].</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Truncated Normal Distribution: This approach mirrors the normal distribution, but values beyond a determined range are excluded and replaced.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Normal Distribution: Parameters are randomly drawn from a Gaussian distribution, typically with a mean of 0 and a standard deviation of 1.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">One of the intrinsic benefits of Random Initialization is that it assignes each client with distinct weights, leading to diversity during the learning process.
It also addresses pivotal challenges in FL as we discussed before, particularly Client Heterogeneity and model convergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>. By beginning with random weights, the model explores diverse regions of the parameter space, nicely tuning to each client’s unique data attributes. This promotes a more adaptive learning process in a federated landscape. As random initialization provides a neutral starting point for model parameters, the model training does not make any assumptions about the data distribution leading to improvement in model convergence and avoiding poor local optima.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Random initialization, while useful, is not without its limitations. As indicated by  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, certain initialization techniques can surpass the performance of random initialization, with evaluations drawing from diverse datasets like CIFAR-10, FEMNIST, Stack Overflow, and Reddit. Moreover, random initialization’s impact on communication efficiency is noteworthy. It may also result in initial models with large parameter differences across clients. During the training process, when model updates are aggregated at the central server, these pronounced differences could translate to heftier model update transmissions, thereby amplifying the communication overhead during aggregation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Client-Specific Initialization</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Client-specific initialization in federated learning tailors the initialization of local models based on the unique data attributes of individual clients. There are scenarios where recognizing the distinctiveness of each edge device becomes important. For instance, personal autocomplete tasks like <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">”I love dancing”</span> can be particular to an individual. While models drawing from expansive global data shows enhanced performance, those focused on localized, client-specific data offer the benefit of capturing individual data characteristics.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The Client-Specific initialization techniques can be further divided into the following categories.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Data-Driven Initialization: Prior to model initialization, every client conducts an analysis of their local data, identifying statistical properties like data distribution, imbalances, and distinct patterns. This process helps to highlight properties unique to that client, which are then used to set the model’s weights and biases.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Domain-Specific Initialization: A model trained for a particular domain may not be directly useful for another domain’s dataset, even though there might be some common knowledge between these domains.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite> refers to the practice of initializing the model parameters of a federated learning model using domain-specific knowledge or pre-trained models that are relevant to the target task in each participating device (client) before starting the federated learning process. This helps to improve convergence and overall performance by providing a good starting point for the optimization process. For example, sentiment analysis for tweets, newspapers, and hospital reviews is very different, but they might have some commonality.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">Clustering-Based Initialization: Cluster-based initialization groups client devices by their similarities, then tunes local models based on client-specific attributes before the federated learning commences.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>.For instance, in diverse Natural Language Processing tasks, data might range from news articles to social media posts and song lyrics. By categorizing clients with like data and adjusting the local model through averaging or other aggregation techniques, one can optimize for that cluster’s specificities. While this method enhances privacy, it’s vulnerable to byzantine clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>, where a malicious initialization could skew the global model’s training and yield inaccurate results.</p>
</div>
</li>
</ol>
<p id="S2.SS2.p3.1" class="ltx_p">In FL systems, client-specific initialization offers notable advantages. Firstly, it addresses model fairness and bias, promoting better equity by tailoring models to each client’s personalized data. Secondly, it mitigates the challenges posed by non-IID data. By initializing models that resonate with their local data distributions, clients can begin with models predisposed towards their specific data, enhancing convergence for their local tasks. This method also fosters greater personalization, marking it as a compelling area for ongoing research. However, challenges persist, notably the risk of byzantine clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> who may malevolently initialize models, potentially undermining the global training process.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2311.02100/assets/Picture1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="377" height="122" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of pre-training initialization on the federated learning cloud server. Once the model is pre-trained, the model is used to initialize the local models on each client. Each client updates its local model’s weights to fine-tune the model.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Centralized Initialization</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Centralized Pre-training Initializationis the technique in which the global model is first initialized on the global server and then distributed to the local servers. This type of technique is depicted in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Client-Specific Initialization ‣ II Localized Initialization ‣ A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The global model initialization uses two techniques as Pre-training on the Central Server and Transfer Learning.
This method aims to leverage the benefits of pre-training, which helps the model learn general features from a large dataset, while also enabling personalized fine-tuning on decentralized data sources.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Pre-training on Central Server</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Given the vast data available online, models can be pre-trained on these large datasets prior to distribution to edge devices. Research<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> indicates that such pre-training enhances task accuracy and addresses federated learning challenges, notably non-IID client data.
Pre-training requires extensive and varied datasets. If unavailable, synthetic data can be used. The model is initially trained on a central server with substantial computational power, learning general features, semantic relationships, and linguistic structures present in the data.This process typically involves unsupervised learning methods like language modeling, autoencoders, or other self-supervised learning techniques. Tasks like image classification have notably benefited from this approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>.
Pre-training a model on a centralized dataset offers several benefits for FL. Firstly, it improves generalization as the model learns general features and patterns. This ensures the model’s adaptability to varying data distributions across individual devices. Additionally, initializing a model through pre-training on a resource-rich central server can accelerate convergence during device-specific fine-tuning. However, this approach has its challenges. For instance, the complexity of fine-tuning rises post pre-training. Depending on the model’s architecture, careful parameter adjustments and regularization might be necessary to prevent overfitting during the local device training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Transfer Learning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Transfer-learning-based initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite> is a technique where a pre-trained model’s knowledge is used, and the model’s parameters such as weights and bias are used as an initial starting point for training a new model on a target task. Transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite> is used when the knowledge might be sharable between domains, or there might not be enough data available for the target task.
Implementing transfer learning in federated learning offers great benefits. Firstly, by using pre-trained weights, models achieve quicker convergence, by utilizing the prior knowledge from the parameters. This rapid training ensures efficient data usage. Secondly, the generic features achieved by pre-trained models enhance their generalization capabilities, enabling them to perform well even with limited data specific to the target task. However, challenges still exist. The efficiency of transfer learning depends on the likeness between source and target domains; vast dissimilarities might render it ineffective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>. Moreover, the relevancy of tasks is crucial: non-aligned tasks can make the transferred knowledge redundant. Careful fine-tuning is essential to avoid overfitting and potential biases or noise from the source data, which might adversely impact the target task’s performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Privacy-preserving Initialization</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">With the rise of various applications of Machine Learning, privacy is one of the major concerns of most of the governments of various countries, The European Union even passed various laws such as GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>. Although one of the major advantages of Federated Learning is said to be Privacy preservation, few studies have found that hackers and attackers have the potential to deduce the specifics from local models exchanged between clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. There are three major types of privacy-preserving initialization which are Homomorphic Encryption-based initialization, Secure Multi-Party Computation (SMPC) Initialization, and Differential Privacy-based Initialization. SMPC and Homomorphic Encryption are Encryption-based techniques whereas in Differential privacy initialization noise is added at the time of model initialization.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Homomorphic Encryption-based Initialization</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Homomorphic Encryption-based Initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite> safeguards client data privacy during the global model’s initiation. It uses homomorphic encryption techniques which allows the computations to be performed on encrypted data without the need to decrypt it.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Homomorphic encryption is a lattice-based public key system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite> that permits operations on encrypted text without decryption. Using a public key for encryption and a secret key for decryption, the sent message represents the input’s sum and product, which is deciphered using the secret key.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The technique is further categorized as :</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Partially Homomorphic Encryption: This method uses only a single operation either addition or subtraction. Some examples of Partially Homomorphic Encryption are
ElGamal encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite> which utilizes the multiplication operation and Paillier encryption<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite> which utilizes the addition operation.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Somewhat Homomorphic Encryption: This type supports both addition and multiplication operations on encrypted data, but if overdone then the text might be susceptible to security issues. The RSA encryption scheme <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite> is an example of somewhat homomorphic encryption.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Fully Homomorphic Encryption (FHE): Despite repeated operations, encryption remains intact without needing decryption. FHE schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite> are resource-intensive and less practical, they are potent for privacy applications, such as the BGV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite> and TFHE methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Homomorphic encryption is protected from quantum threats due to its lattice-based foundation. Furthermore, it offers adjustable security parameters, guaranteeing its safety under standard assumptions, and bolstering its defense against adversaries with limited computational capabilities.
Homomorphic encryption-based initialization in federated learning setup involves initializing and updating the parameters of the local model with the decrypted global model data. This ensures privacy, security, and client autonomy for local updates with encrypted data. However, there are computational and communication challenges due to encryption operations. We suggest combining this technique with client-specific initialization to boost security and privacy.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Secure Multi-Party Computation (SMPC) Initialization</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Secure Multi-Party Computation Initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite> initializes the global model without revealing client details, allowing computations on encrypted data where only the final results are disclosed.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Steps involved in SMPC for FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Local Data Encryption: Every client can encrypt their weights and send a part of it to other clients.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Collaborative Computation: The client performs computations on the encrypted data, and these computed values are communicated to the global server. They evaluate mathematical functions over their encrypted inputs without exposing the individual data.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">The global server can then perform aggregation on these values to find the final answer.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">SMPC ensures privacy and client data confidentiality during initialization but faces challenges like computational overhead from encryption and algorithm complexity, which may slow convergence. We recommend its use, especially prioritizing security, combined with client-specific initialization for personalized and private results.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Model Initialization Techniques in Federated Learning</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Feature/Technique</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.2.1.1" class="ltx_p"><span id="S4.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Localized Initialization</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.3.1.1" class="ltx_p"><span id="S4.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Centralized Initialization</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.4.1.1" class="ltx_p"><span id="S4.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Privacy-preserving Initialization</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Main Concept</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.2.1.1" class="ltx_p">Device-specific data characteristics.</span>
</span>
</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.3.1.1" class="ltx_p">Global model on central server.</span>
</span>
</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.4.1.1" class="ltx_p">User data privacy during initialization.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Key Methods</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.2.1.1" class="ltx_p">Device-based tailoring. 
<br class="ltx_break">Data-specific adaptation.</span>
</span>
</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.3.1.1" class="ltx_p">Cloud-based pre-training. 
<br class="ltx_break">Global-to-local fine-tuning.</span>
</span>
</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.4.1.1" class="ltx_p">Differential privacy. 
<br class="ltx_break">Encrypted computation.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Benefits</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.2.1.1" class="ltx_p">Tailored learning experience. 
<br class="ltx_break">Better local accuracy.</span>
</span>
</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.3.1.1" class="ltx_p">Rapid start with pre-trained model. 
<br class="ltx_break">Consistency in initial model across devices.</span>
</span>
</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.4.1.1" class="ltx_p">Enhanced user trust. 
<br class="ltx_break">Regulatory compliance.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Challenges</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.2.1.1" class="ltx_p">Varied starting points can complicate global model aggregation.</span>
</span>
</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.3.1.1" class="ltx_p">Requires significant server resources. 
<br class="ltx_break">May not reflect local data nuances.</span>
</span>
</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.4.1.1" class="ltx_p">Computational overhead. 
<br class="ltx_break">Possible accuracy trade-off.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Convergence Speed (relative).</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.2.1.1" class="ltx_p">Moderate.</span>
</span>
</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.3.1.1" class="ltx_p">Fast.</span>
</span>
</td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.4.1.1" class="ltx_p">Slow.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Computational Efficiency(relative).</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.2.1.1" class="ltx_p">High.</span>
</span>
</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.3.1.1" class="ltx_p">Medium.</span>
</span>
</td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.4.1.1" class="ltx_p">Low due to encryption processes.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Suitability for Data Non-IIDness</th>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.2.1.1" class="ltx_p">Very Suitable (as it tailors to local data).</span>
</span>
</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.3.1.1" class="ltx_p">Moderate.</span>
</span>
</td>
<td id="S4.T1.1.8.7.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.4.1.1" class="ltx_p">Moderate.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Privacy &amp; Security Measures.</th>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.2.1.1" class="ltx_p">Local data stays local.</span>
</span>
</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.3.1.1" class="ltx_p">Centralized data aggregation.</span>
</span>
</td>
<td id="S4.T1.1.9.8.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.4.1.1" class="ltx_p">Data encryption. 
<br class="ltx_break">Noise addition for privacy.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<th id="S4.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Applicability (Use Cases)</th>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.2.1.1" class="ltx_p">Edge devices with diverse data. 
<br class="ltx_break">IoT devices.</span>
</span>
</td>
<td id="S4.T1.1.10.9.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.3.1.1" class="ltx_p">Situations with robust central servers. 
<br class="ltx_break">Scenarios where pre-trained models are available.</span>
</span>
</td>
<td id="S4.T1.1.10.9.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T1.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.4.1.1" class="ltx_p">Healthcare and finance where data privacy is paramount. 
<br class="ltx_break">User-centric applications.</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.11.10" class="ltx_tr">
<th id="S4.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Drawbacks</th>
<td id="S4.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S4.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.2.1.1" class="ltx_p">Might introduce inconsistency in global model.</span>
</span>
</td>
<td id="S4.T1.1.11.10.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S4.T1.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.3.1.1" class="ltx_p">Might overlook unique local data patterns.</span>
</span>
</td>
<td id="S4.T1.1.11.10.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S4.T1.1.11.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.4.1.1" class="ltx_p">Slower due to privacy processes. 
<br class="ltx_break">Might introduce noise to data.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Differential Privacy-based Initialization</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Differential Privacy-based initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite> protects individual data privacy without impacting the model’s results. It conceals individual specifics by assessing each data point’s influence on the model and adding noise during initialization. This ensures data identities remain hidden and prevents encoding of sensitive information in initial parameters.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Four key steps in DP-based initialization are:</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<ol id="S4.I3" class="ltx_enumerate">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Sensitivity Computation: Here, the model’s sensitivity to changes in local node initialization parameters is evaluated. It measures the shift in model predictions when a single data point enters or leaves the training dataset.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Noise Addition: After calculating the sensitivity, some noise is added to the model’s parameters during initialization. The addition of this noise makes it more secure since there are very less chances for it to carry any specific information from the training data.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Privacy Budget:Represented by epsilon, this metric quantifies the privacy protection level. A lower epsilon means added noise for enhanced privacy assurances.</p>
</div>
</li>
<li id="S4.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I3.i4.p1" class="ltx_para">
<p id="S4.I3.i4.p1.1" class="ltx_p">Trade-off: There’s a trade-off between privacy and utility. Stronger privacy protection usually implies more noise, which can degrade the model’s performance. Balancing this trade-off is a crucial consideration in Differential Privacy-based Initialization. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>]</cite></p>
</div>
</li>
</ol>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">In federated learning emphasizing DP, clients add noise to model parameters before sending them. This communication’s cost aligns with standard federated learning. Unlike the two aforementioned encryption-based initialization methods, DP offers reduced computation and communication burdens. Future enhancements should retain client result personalization while preserving privacy.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Proposed Evaluation Metrics</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Findings and Discussion</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In our exploration of model initialization techniques in Federated Learning (FL), we discerned three primary approaches: Localized, Centralized, and Privacy-preserving Initialization. We summarize our findings in TABLE <a href="#S4.T1" title="TABLE I ‣ IV-B Secure Multi-Party Computation (SMPC) Initialization ‣ IV Privacy-preserving Initialization ‣ A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. Localized Initialization, tailored to individual devices, excels in environments with diverse data distributions but may challenge global model consistency. Centralized Initialization offers speed and uniformity but could gloss over local data nuances. Meanwhile, Privacy-preserving Initialization prioritizes user data security, vital in sensitive sectors like healthcare and finance, but introduces computational challenges and potential trade-offs in model accuracy. While some research has delved into FL’s intricacies, our systematic categorization and comparison focused on model initialization provide an essential roadmap for researchers and practitioners, underscoring the nuances and critical considerations previously not noted in the realm of Federated Learning.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Proposed Evaluation Metrics</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In evaluating the efficacy of model initialization techniques in Federated Learning (FL), the metrics play a pivotal role in understanding their performance under varying conditions and system settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">32</a>]</cite>. Below are some metrics criteria we introduce and recommend to choose the best initialization strategy based on the discussed challenges of FL system.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.5.1.1" class="ltx_text">V-B</span>1 </span>Quantitative Metrics</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Convergence Speed: It’s about the number of times data needs to be exchanged between the server and nodes before the model starts giving good results and meeting certain accuracy benchmarks.Faster convergence means less time and potentially less resource consumption.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Communication Overhead: Quantifying the amount of data exchanged between the nodes and the central server and the time taken during initialization. In scenarios where bandwidth is limited or costly, minimizing communication is vital <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Computational Load: This assesses how much computer power is needed in the early stages of model training. If the computational load is too high, it might exclude devices with less processing power or lead to longer training times.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.5.1.1" class="ltx_text">V-B</span>2 </span>Qualitative Metrics </h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Scalability: Evaluate the ability of the initialization technique to cater to an increasing number of nodes or devices. One can run multiple experiments with varying numbers of nodes (e.g., 10, 100, 1000) and observe if the initialization technique maintains efficiency and effectiveness.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Robustness: Assesses the initialization technique’s performance under real-world challenges like device dropouts, asynchronous updates, etc.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">Interoperability: Measures the compatibility of the initialization method with various FL architectures and algorithms.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Federated learning systems are filled with challenges that are further made complex when deciding on the most optimal model initialization technique. Through our comprehensive survey, it becomes evident that the right initialization can significantly boost the performance, speed, and robustness of FL models. We have compared and categorized some major challenges of FL systems and propose potential solutions for them using various initialization techniques. We also introduced a new categorization and metrics for researchers to navigate Federated Learning Systems. As the usage of decentralized learning continues to expand, so will the techniques to initialize the process, ensuring that Federated Learning maintains its status of privacy-preserving and resource-efficient machine learning technique.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Joost Verbraeken et al.
</span>
<span class="ltx_bibblock">“A survey on distributed machine learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Acm computing surveys (csur)</em> <span id="bib.bibx1.2.2" class="ltx_text ltx_font_bold">53.2</span>
</span>
<span class="ltx_bibblock">ACM New York, NY, USA, 2020, pp. 1–33
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Brendan McMahan et al.
</span>
<span class="ltx_bibblock">“Communication-efficient learning of deep networks from
decentralized data”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, 2017, pp. 1273–1282
</span>
<span class="ltx_bibblock">PMLR
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Li Li, Yuxi Fan, Mike Tse and Kuo-Yi Lin
</span>
<span class="ltx_bibblock">“A review of applications in federated learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Industrial Engineering</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">149</span>
</span>
<span class="ltx_bibblock">Elsevier, 2020, pp. 106854
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Yue Zhao et al.
</span>
<span class="ltx_bibblock">“Federated learning with non-iid data”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em>, 2018
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Tuo Zhang et al.
</span>
<span class="ltx_bibblock">“Federated Learning for the Internet of Things: Applications,
Challenges, and Opportunities”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">5.1</span>, 2022, pp. 24–29
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">John Nguyen et al.
</span>
<span class="ltx_bibblock">“Where to Begin? On the Impact of Pre-Training and
Initialization in Federated Learning”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2206.15387" title="" class="ltx_ref ltx_href">2206.15387 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Benyuan Sun, Hongxing Huo, Yi Yang and Bo Bai
</span>
<span class="ltx_bibblock">“Partialfed: Cross-domain personalized federated learning via
partial initialization”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="bib.bibx7.2.2" class="ltx_text ltx_font_bold">34</span>, 2021, pp. 23309–23320
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">G. Thimm and E. Fiesler
</span>
<span class="ltx_bibblock">“Neural network initialization”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">From Natural to Artificial Neural Computation</em>, 1995
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Ryan P. Browne, Paul D. McNicholas and Matthew D. Sparling
</span>
<span class="ltx_bibblock">“Model-Based Learning Using a Mixture of Mixtures of Gaussian
and Uniform Distributions”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em> <span id="bib.bibx9.2.2" class="ltx_text ltx_font_bold">34.4</span>, 2012, pp. 814–817
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Benyuan Sun, Hongxing Huo, YI YANG and Bo Bai
</span>
<span class="ltx_bibblock">“PartialFed: Cross-Domain Personalized Federated Learning via
Partial Initialization”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="bib.bibx10.2.2" class="ltx_text ltx_font_bold">34</span>
</span>
<span class="ltx_bibblock">Curran Associates, Inc., 2021, pp. 23309–23320
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Lei Yang, Jiaming Huang, Wanyu Lin and Jiannong Cao
</span>
<span class="ltx_bibblock">“Personalized Federated Learning on Non-IID Data via
Group-Based Meta-Learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Knowl. Discov. Data</em> <span id="bib.bibx11.2.2" class="ltx_text ltx_font_bold">17.4</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2023
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Virat Shejwalkar and Amir Houmansadr
</span>
<span class="ltx_bibblock">“Manipulating the byzantine: Optimizing model poisoning
attacks and defenses for federated learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">NDSS</em>, 2021
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Amit Portnoy, Yoav Tirosh and Danny Hendler
</span>
<span class="ltx_bibblock">“Towards Federated Learning With Byzantine-Robust Client
Weighting”, 2021
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2004.04986" title="" class="ltx_ref ltx_href">2004.04986 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Hong-You Chen et al.
</span>
<span class="ltx_bibblock">“On the Importance and Applicability of Pre-Training for
Federated Learning”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2206.11488" title="" class="ltx_ref ltx_href">2206.11488 [cs.LG]</a>
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Hong-You Chen et al.
</span>
<span class="ltx_bibblock">“On pre-training for federated learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.11488</em>, 2022
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Maithra Raghu, Chiyuan Zhang, Jon Kleinberg and Samy Bengio
</span>
<span class="ltx_bibblock">“Transfusion: Understanding transfer learning for medical
imaging”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> <span id="bib.bibx16.2.2" class="ltx_text ltx_font_bold">32</span>, 2019
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Lisa Torrey and Jude Shavlik
</span>
<span class="ltx_bibblock">“Transfer learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Handbook of research on machine learning applications and
trends: algorithms, methods, and techniques</em>
</span>
<span class="ltx_bibblock">IGI global, 2010, pp. 242–264
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Yiqiang Chen et al.
</span>
<span class="ltx_bibblock">“Fedhealth: A federated transfer learning framework for
wearable healthcare”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> <span id="bib.bibx18.2.2" class="ltx_text ltx_font_bold">35.4</span>
</span>
<span class="ltx_bibblock">IEEE, 2020, pp. 83–93
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">David Basin, Søren Debois and Thomas Hildebrandt
</span>
<span class="ltx_bibblock">“On purpose and by necessity: compliance under the GDPR”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Financial Cryptography and Data Security: 22nd
International Conference, FC 2018, Nieuwpoort, Curaçao, February
26–March 2, 2018, Revised Selected Papers 22</em>, 2018, pp. 20–37
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">H. Brendan McMahan, Eider Moore, Daniel Ramage and Blaise Agüera Arcas
</span>
<span class="ltx_bibblock">“Federated Learning of Deep Networks using Model Averaging”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx20.2.2" class="ltx_text ltx_font_bold">abs/1602.05629</span>, 2016
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Li Zhang et al.
</span>
<span class="ltx_bibblock">“Homomorphic Encryption-based Privacy-preserving Federated
Learning in IoT-enabled Healthcare System”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and Engineering</em>, 2022, pp. 1–17
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/TNSE.2022.3185327" title="" class="ltx_ref ltx_href">10.1109/TNSE.2022.3185327</a>
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Carlos Aguilar Melchor, Guilhem Castagnos and Philippe Gaborit
</span>
<span class="ltx_bibblock">“Lattice-based homomorphic encryption of vector spaces”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">2008 IEEE international symposium on information theory</em>, 2008, pp. 1858–1862
</span>
<span class="ltx_bibblock">IEEE
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Zengqiang Wu, Di Su and Gang Ding
</span>
<span class="ltx_bibblock">“ElGamal algorithm for encryption of data transmission”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">2014 International Conference on Mechatronics and Control
(ICMC)</em>, 2014, pp. 1464–1467
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICMC.2014.7231798" title="" class="ltx_ref ltx_href">10.1109/ICMC.2014.7231798</a>
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Sattar J Aboud, Mohammad A AL-Fayoumi, Mustafa Al-Fayoumi and Haidar S Jabbar
</span>
<span class="ltx_bibblock">“An efficient RSA public key encryption scheme”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">Fifth International Conference on Information Technology:
New Generations (itng 2008)</em>, 2008, pp. 127–130
</span>
<span class="ltx_bibblock">IEEE
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Craig Gentry
</span>
<span class="ltx_bibblock">“A fully homomorphic encryption scheme”
</span>
<span class="ltx_bibblock">Stanford university, 2009
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Craig Gentry, Shai Halevi, Chris Peikert and Nigel P Smart
</span>
<span class="ltx_bibblock">“Ring switching in BGV-style homomorphic encryption”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">International Conference on Security and Cryptography for
Networks</em>, 2012, pp. 19–37
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Ilaria Chillotti, Nicolas Gama, Mariya Georgieva and Malika Izabachène
</span>
<span class="ltx_bibblock">“TFHE: fast fully homomorphic encryption over the torus”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">Journal of Cryptology</em> <span id="bib.bibx27.2.2" class="ltx_text ltx_font_bold">33.1</span>
</span>
<span class="ltx_bibblock">Springer, 2020, pp. 34–91
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Oded Goldreich
</span>
<span class="ltx_bibblock">“Secure multi-party computation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">Manuscript. Preliminary version</em> <span id="bib.bibx28.2.2" class="ltx_text ltx_font_bold">78.110</span>
</span>
<span class="ltx_bibblock">Citeseer, 1998
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">David Byrd and Antigoni Polychroniadou
</span>
<span class="ltx_bibblock">“Differentially private secure multi-party computation for
federated learning in financial applications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First ACM International Conference on
AI in Finance</em>, 2020, pp. 1–9
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Kang Wei et al.
</span>
<span class="ltx_bibblock">“Federated learning with differential privacy: Algorithms and
performance analysis”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and
Security</em> <span id="bib.bibx30.2.2" class="ltx_text ltx_font_bold">15</span>
</span>
<span class="ltx_bibblock">IEEE, 2020, pp. 3454–3469
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Xia Wu, Lei Xu and Liehuang Zhu
</span>
<span class="ltx_bibblock">“Local Differential Privacy-Based Federated Learning under
Personalized Settings”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> <span id="bib.bibx31.2.2" class="ltx_text ltx_font_bold">13.7</span>, 2023
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://www.mdpi.com/2076-3417/13/7/4168" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2076-3417/13/7/4168</a>
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Fan Lai et al.
</span>
<span class="ltx_bibblock">“Fedscale: Benchmarking model and system performance of
federated learning at scale”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2022, pp. 11814–11827
</span>
<span class="ltx_bibblock">PMLR
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Leylane Graziele Ferreira Silva, Djamel FH Sadok and Patricia Takako Endo
</span>
<span class="ltx_bibblock">“Resource optimizing federated learning for use with IoT: A
systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Journal of Parallel and Distributed Computing</em>
</span>
<span class="ltx_bibblock">Elsevier, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.02099" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.02100" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.02100">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.02100" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.02101" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 21:19:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
