<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.06737] Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data</title><meta property="og:description" content="Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios. We address the limitation by proposing a novel approach that tackles the challenges posed by …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.06737">

<!--Generated on Wed Feb 28 17:21:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Miroslav Purkrábek and Jiří Matas
<br class="ltx_break">Visual Recognition Group
<br class="ltx_break">Department of Cybernetics
<br class="ltx_break">Faculty of Electrical Engineering
<br class="ltx_break">
Czech Technical University in Prague
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{purkrmir, matas}@fel.cvut.cz</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Human Pose Estimation is a thoroughly researched problem; however, most datasets focus on the side and front-view scenarios. We address the limitation by proposing a novel approach that tackles the challenges posed by extreme viewpoints and poses.
We introduce a new method
for synthetic data generation
– RePoGen, RarE POses GENerator –
with comprehensive control over pose and view to augment the COCO dataset. Experiments on a new dataset of real images show that adding RePoGen data to the COCO surpasses previous attempts to top-view pose estimation and significantly improves performance on the bottom-view dataset. Through an extensive ablation study on both the top and bottom view data, we elucidate the contributions of methodological choices and demonstrate improved performance. The code and the datasets are available on the project website<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://mirapurkrabek.github.io/RePoGen-paper/" title="" class="ltx_ref ltx_href">https://MiraPurkrabek.github.io/RePoGen-paper/</a></span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.06737/assets/imgs/comparison/image_000006-61_paper.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="431" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text" style="font-size:90%;">
Pose estimation trained on COCO (left) and
by our method (right).
The COCO model mistakes the left and right sides and interprets the <span id="S1.F1.7.2.1" class="ltx_text" style="background-color:#FFFF00;">right hand</span> as the <span id="S1.F1.7.2.2" class="ltx_text" style="color:#FFFFFF;background-color:#0000FF;">left leg</span> and the <span id="S1.F1.7.2.3" class="ltx_text" style="background-color:#FF8000;">right leg</span> as the <span id="S1.F1.7.2.4" class="ltx_text" style="background-color:#61FFE0;">left hand</span> (color indicates the corresponding label).</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The availability of large-scale, manually annotated datasets has greatly advanced research in human pose estimation from 2D monocular images.
Current datasets primarily focus on camera viewpoints from what we call <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">an orbital view</span>, i.e. side, front, and back views, where challenges such as occlusion by objects or individuals are prevalent. Similarly, they focus on common poses like standing, sitting, or walking by sampling everyday activities. As a result, much of the research has been dedicated to tackling occlusion. Specialized datasets have been curated to evaluate the effectiveness of pose estimation models in scenarios involving occluded individuals.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the other hand, the issue of unusual viewpoints has received less attention. In what we refer to as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">extreme viewpoints</span> (top and bottom view; the complement of orbital view), the appearance of humans significantly differs from that of the orbital view. Although such views are less common in everyday activities and videos, they frequently appear in sports or surveillance footage. Annotating persons in extreme views poses considerable challenges as human annotators struggle to comprehend scenes unfamiliar to the human eye.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We employ an SMPL-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> synthetic data approach similar to previous methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to address the scarcity of training data.
However, we distinguish ourselves by generating novel poses, even if they occasionally deviate from anatomical accuracy. We allow for the possibility of body parts, like limbs, intersecting with each other, as long as the overall pose maintains physical plausibility. Minor mesh intersections can simulate body deformations without impeding training.
This novel approach allows us to generate new poses from a wider distribution than previous methods. We demonstrate that pose variability, combined with novel views, is crucial for accurate pose estimation in sports, where extreme poses and extreme views are prevalent.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We introduce a novel method for generating likely realistic poses and utilize them to augment existing datasets, thereby incorporating novel views and poses. Furthermore, we demonstrate the applicability of our approach to the top view, which is on par with or potentially superior to previous methods.
The main contributions of the paper are:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">RePoGen - a new method for generating synthetic real-looking images with humans.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The RePoGen dataset - a new dataset of synthetic images prioritizing rare poses and viewpoints.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">RePo - a new manually annotated dataset of real images of rare poses from the top and bottom views enabling comprehensive evaluation of pose estimation from unusual views.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We demonstrate a significant increase in the pose estimation accuracy on extreme views without harming COCO performance by augmenting the existing COCO dataset with RePo synthetic data.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We will release the RePoGen code and the synthetic RePoGen and real-world, annotated, RePo datasets.
Additionally, we provide enhanced annotations for the previously published PoseFES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Numerous datasets have been developed to support advancements in human pose estimation. Real-world datasets like COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and MPII <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> offer diverse images that capture human poses in everyday scenes, while the LSP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> focuses on sports-related poses. To address the challenge of occlusion, specialized datasets such as OCHuman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and CrowdPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have been curated, enabling the evaluation of pose estimation algorithms in occluded scenarios.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Several models have emerged, demonstrating significant advancements in accuracy and performance. These models primarily fall into top-down approaches, which rely on bounding boxes as input for pose estimation. Among these models, ViTPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> stands out as the current SOTA on the COCO dataset leveraging the transformer architecture. Similarly, models such as SWIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and PSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> also employ transformer-based architectures, although they perform slightly below ViTPose in terms of accuracy.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">An alternative approach that garnered attention is the HRNet model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which combines convolutional neural networks with an integral part, Unbiased Data Processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This combination yields excellent results and has become a common baseline for evaluating the performance of new pose estimation methods.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Addressing the challenges posed by occlusion and crowded scenes, specialized models have been developed to focus on these specific scenarios. For example, the I2RNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a transformer-based network designed to tackle the challenges of occlusion and crowd-related issues.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Furthermore, proper data processing techniques have been proposed to enhance the performance of pose estimation models. The DARK algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and the UDP (Ungrouped Distance Parameterization) method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> are two notable papers that highlight the importance of data processing in achieving superior results.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">To facilitate pose estimation research and development, the MMPose framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> has emerged as a comprehensive resource. It offers an extensive model zoo and many pre-trained models, including the widely used HRNet.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Synthetic datasets have also played a significant role in augmenting the available data and expanding the range of pose variations. The THEODORE+ dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> provides a synthetic collection of top-view videos generated using a game engine. These videos depict individuals walking in a room, although they only pro 13 keypoints instead of the more commonly used 17. Synthetic datasets like SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and PanopTOP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> utilize the SMPL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, fitting it to measured 3D point clouds of real poses from datasets such as Human36M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and Panoptic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, PanopTOP has limitations regarding low resolution and issues with ghost hands, which should be considered.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">The estimation of poses from extreme viewpoints is another research area of interest. The WEPDTOF-Pose dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> represents the largest dataset of top-view images for pose estimation. Although specialized for top-view poses, it is noteworthy that most people captured in the dataset are from the orbital view due to fisheye lens distortion. Similarly, the PoseFES dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, designed for evaluating top-view human pose estimation, also suffers from a prevalence of orbital views caused by fisheye lens distortion. Another dataset, ITop <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, focuses on pose estimation from top-view depthmaps with no RGB images available.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">Data augmentation is critical in addressing the scarcity of annotated real-world data for human pose estimation. Various methods have been introduced to tackle this challenge, often involving human parsing techniques for body part segmentation. HumanPaste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and AdversarialAugmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> employ strategies to simulate occlusion by pasting additional people or selective body parts. Similarly, JointlyOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and NearbyPersonOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> augment data by introducing body parts or whole bodies to mimic occlusion and crowded scenarios.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">While these augmentation methods prove effective for specific challenges, they do not directly address the problem of unseen viewpoints. In contrast, generating synthetic data using game engines have been explored to introduce variability. However, datasets created with game engines, such as PoseFES <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and LetsPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, often suffer from limited pose variability, typically showcasing walking or a narrow range of everyday activities.</p>
</div>
<div id="S2.p11" class="ltx_para">
<p id="S2.p11.1" class="ltx_p">Another avenue for synthetic data generation involves fitting the SMPL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to 3D point clouds obtained from motion capture systems. For example, SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> fits the SMPL model to the Human36M dataset, providing a pool of textures applicable to SMPL models. Similarly, PanopTOP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> employs the SMPL model fitted to the Panoptic dataset. However, these methods face challenges in fitting the model to point clouds, resulting in issues such as ghost hands. Furthermore, the limitations of motion capture systems make capturing extreme dynamic poses or new poses challenging. SyntheticHF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> estimates the SMPL pose and shape from a monocular image and modifies the shape while preserving the pose, creating data resembling SURREAL and Panoptic. However, this approach has limitations due to the initial SMPL estimate, resulting in difficulties handling poses beyond its accurate capture.</p>
</div>
<div id="S2.p12" class="ltx_para">
<p id="S2.p12.1" class="ltx_p">Efforts have also been made to enhance the realism of the SMPL model. SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> enhances the previous model with hand poses and facial expressions. PoseNDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> learns a manifold of known poses, enabling the generation of random realistic poses within the manifold. Similarly, CAPE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> introduces a clothing layer on top of existing SMPL models, aiming to narrow the domain gap between generated and real data.</p>
</div>
<div id="S2.p13" class="ltx_para">
<p id="S2.p13.1" class="ltx_p">GAN-based methods like SynthetizeAnyone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, UnpairedPG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and SynthetizingIO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> generate synthetic data by preserving the given pose or style. On the other hand, diffusion-based methods such as StableDiffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> offer promising approaches for synthetic data generation, allowing control over the rendered images. However, both approaches have limitations regarding extreme views and rare poses due to the need for more training data.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.06737/assets/imgs/comparison/image_000002-27_paper_crop.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="313" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.06737/assets/imgs/comparison/image_000008-8_paper_crop.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="307" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.06737/assets/imgs/comparison/image_000025-58_paper_crop.png" id="S2.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="211" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.06737/assets/imgs/comparison/image_000053-23_paper_crop.png" id="S2.F2.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="173" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2307.06737/assets/imgs/comparison/image_000064-32_paper_crop.png" id="S2.F2.5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="134" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.11.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.12.2" class="ltx_text" style="font-size:90%;">
Examples from the RePo test set. ViTPose-s estimates when trained on COCO (left) and on RePoGen data (right).
Colors as in <a href="#S1.F1" title="In 1 Introduction ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> –
<span id="S2.F2.12.2.1" class="ltx_text" style="background-color:#FFFF00;">right hand</span>, <span id="S2.F2.12.2.2" class="ltx_text" style="background-color:#FF8000;">right leg</span>, <span id="S2.F2.12.2.3" class="ltx_text" style="background-color:#61FFE0;">left hand</span> and <span id="S2.F2.12.2.4" class="ltx_text" style="color:#FFFFFF;background-color:#0000FF;">left leg</span>
</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F3" class="ltx_figure ltx_align_center"><img src="/html/2307.06737/assets/x1.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text" style="font-size:90%;">RePoGen synthetic data generation pipeline. All steps are detailed in <a href="#S3" title="3 Method ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. The ground truth outputs of the method are (A) 2D and 3D keypoints, (B) the depth map, (C) the mask, and (D) an RGB image.</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides a detailed description of our approach to enhancing an existing dataset using synthetic data generation. We developed a novel method inspired by prior works that offer enhanced control over pose parameters. Unlike previous approaches that relied on re-using point clouds from motion capture, RePoGen allows us to define a pose simplicity and generate individuals in rare poses. Although the realisticity of the generated poses is not guaranteed, we demonstrate that it is not a prerequisite for effective performance.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The proposed RePoGen pipeline is outlined in the <a href="#S3.F3" title="In 3 Method ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. Following paragraphs present a step-by-step walkthrough of te RePoGen data generation process, highlighting the main techniques employed to achieve pose control and generate diverse synthetic data.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pose Generation</h3>

<figure id="S3.F4" class="ltx_figure"><img src="/html/2307.06737/assets/imgs/distributions.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Examples of joint angle distributions used in data generation.
Baseline - a hand-crafted joint angle distribution approximating statistics of common poses. Uniform sampling of joint angles generates many extreme poses.
</span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">RePoGen leverages the SMPL-X model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which defines 21 body joints with free rotation around three axes each. In addition to the basic SMPL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, SMPL-X also includes joints for hands and face. The rotation angles for the face and hand joints are randomly determined, as they do not influence the 17 COCO keypoints.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We sample each body angle from an asymmetrical normal distribution, composed of two normal distributions with different variances, visualized in <a href="#S3.F4" title="In 3.1 Pose Generation ‣ 3 Method ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, to generate diverse poses. Each angle has its unique constraints and mean. This distribution allows us to generate pose angles centered around a standard pose, with unique and asymmetric ranges for each joint. It is a hand-crafted approximation of angle distribution in common poses.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">By applying constraints on joint rotation, a substantial portion of the pose space, primarily composed of unrealistic poses, is effectively eliminated. The remaining poses are highly likely to exhibit realistic characteristics, although some instances of mesh intersection may occur. However, these small-scale mesh intersections do not pose significant issues during training, as they effectively simulate minor body deformations within the rendered images.
The major advantage of our approach is the ability to generate rare poses that are not present in previous datasets.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">On the other hand, it is important to acknowledge the inherent limitation of the SMPL-X model, which represents the human body with only 21 joints. In comparison, the actual human body consists of over 300 joints. This discrepancy poses challenges, particularly in accurately modeling complex spine rotations.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Another advantage of the methos is the ability to control the complexity of the generated poses using a single parameter referred to as <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">pose simplicity</span> <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\alpha</annotation></semantics></math>. By scaling the distribution by a constant, we restrict the pose space, and generated poses are closer to the standard pose. Changing the standard pose mathematically means changing the mean of the composed distribution. We experimented with two standard poses - standing straight and the default SMPL pose. Additionally, we introduce the option to sample joints from a uniform distribution instead of the composed normal distribution, which produces more frequent extreme poses. The ablation study in <a href="#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.5</span></a> refers to this option as <span id="S3.SS1.p5.1.2" class="ltx_text ltx_font_italic">uniform distribution</span>.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Last, we changed the default pose to standing straight with hands along the body instead of the default SMPL pose with hands horizontally. Both poses are visualized in the <a href="#S3.F3" title="In 3 Method ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The output of this stage is a triangular mesh representing a human body in a randomly generated pose. The generated mesh is smooth and without noise, ensuring a consistent and visually coherent pose representation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Texture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Once the random pose is generated, we apply a randomized texture to the mesh. For this purpose, we utilize textures provided by the SURREAL project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and do not differentiate between male and female textures. If no texture is applied (as examined in the ablation study in <a href="#S4.SS5" title="4.5 Ablation Study ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>), we color the mesh to resemble natural skin tones. This approach ensures that the generated synthetic data exhibits variation in texture, contributing to a more realistic appearance.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Lights and Camera Positions</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In our pose generation technique, we randomly sample both light and camera positions from a surface of a unit sphere. Initially, we distribute five light sources randomly on the unit sphere, creating shadows on the texture to enhance the realism of the generated data.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">All distances utilized in our pose generation process are measured in the coordinates of the SMPL-X model. The SMPL unit corresponds to a length of approximately less than 1 meter. The coordinate system is visually represented in the <a href="#S4.F6" title="In 4.3 Viewpoint Dependency Analysis ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, aiding in understanding the coordinate transformations involved in RePoGen.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Random Background</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The final component for generating visually appealing images is the background. We incorporate a random image as the background and crop the rendered scene to a 1.25 multiple of the bounding box size. When selecting background images, we ensure that they depict environments where people are commonly observed. However, we refrain from including discernible individuals in the background, which could confuse the network since we do not focus on crowded scenes.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Ground Truth Extraction</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The output of the pipeline includes not only the rendered RGB image but also the corresponding ground truth information. We first extract the depth map from the triangular mesh representation to obtain the ground truth. This depthmap is then used to generate a segmentation mask through thresholding. The segmentation mask defines the bounding box.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">However, determining the visibility of joints is a complex process, as the joints of the SMPL-X model are positioned within the triangular mesh and are, therefore, always hidden from view in the rendered image. To address this, we define a neighborhood around each joint and consider the joint visible if at least one vertex from its respective neighborhood is visible in the image. The size of the neighborhood is proportional to the joint size and is determined based on the human annotation error defined in the OKS metric from the COCO dataset. This approach allows us to estimate the visibility of the joints and accurately generate the corresponding ground truth annotations for evaluation and training purposes.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To optimize computation power and time efficiency, we primarily conduct experiments using the ViTPose-s model unless otherwise specified. The training parameters align with the ViTpose model, with a batch size of 128 and a base learning rate 5e-5. We follow the training paradigm from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and fine-tune the model pretrained on the COCO dataset.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To focus on analyzing and improving the pose estimation model, we utilize ground truth bounding boxes to crop individuals from the images. This approach is chosen to mitigate errors from detectors, particularly in extreme views.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">All synthetic images used in experiments are generated exclusively through RePoGen, with a preference for the top or bottom views. Synthetic data from orbital views are not generated as they provide no notable improvement.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">During training, the model is not exposed to any real extreme view images that are not present in the original COCO dataset. Instead, all additional data used for training purposes are synthetically generated. The model used for comparison with other approaches used 3 000 images. The ablation study was done using 1 000 images.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.2" class="ltx_p"><span id="S4.SS1.p5.2.1" class="ltx_text ltx_font_bold">Rotation</span>. During training, we incorporate extensive rotation data augmentation of COCO and synthetic images. In experiments labeled as <span id="S4.SS1.p5.2.2" class="ltx_text ltx_font_italic">w/o rotation</span>, we follow the standard rotation augmentation up to <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="40^{\circ}" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><msup id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mn id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">40</mn><mo id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">40</cn><compose id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">40^{\circ}</annotation></semantics></math>, while in other cases, we apply a rotation up to <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="180^{\circ}" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><msup id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mn id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">180</mn><mo id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">180</cn><compose id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">180^{\circ}</annotation></semantics></math>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset name</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"># of poses</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<td id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">PoseFES Top</td>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">431</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<td id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left">RePo <span id="S4.T1.2.3.2.1.1" class="ltx_text" style="font-size:90%;">(Bottom Val)</span>
</td>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_nopad_r ltx_align_right">31</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<td id="S4.T1.2.4.3.1" class="ltx_td ltx_align_left">RePo <span id="S4.T1.2.4.3.1.1" class="ltx_text" style="font-size:90%;">(Bottom Test)</span>
</td>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_nopad_r ltx_align_right">94</td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<td id="S4.T1.2.5.4.1" class="ltx_td ltx_align_left">RePo <span id="S4.T1.2.5.4.1.1" class="ltx_text" style="font-size:90%;">(Bottom Seq)</span>
</td>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_nopad_r ltx_align_right">62</td>
</tr>
<tr id="S4.T1.2.6.5" class="ltx_tr">
<td id="S4.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">RePo <span id="S4.T1.2.6.5.1.1" class="ltx_text" style="font-size:90%;">(Top Val)</span>
</td>
<td id="S4.T1.2.6.5.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">91</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">The number of annotated poses for the new datasets.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We created a new dataset to evaluate pose estimation from extreme views in real-world data. We conduct experiments on the following datasets:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">COCO.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p2.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S4.SS2.p2.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span> This standard dataset is commonly used for human pose estimation. It contains approximately 250,000 annotated poses from various everyday activities. However, the COCO dataset includes very few images captured from extreme views.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">PoseFES.<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p3.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.SS2.p3.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span> PoseFES is a manually annotated dataset captured by a ceiling-mounted fisheye camera, serving as the solely available top-view dataset for human pose estimation. Although we know another dataset (WEPDTOF-Pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>), our attempts to obtain it from the authors were unsuccessful. PoseFES consists of two sequences: one focusing on two well-separated individuals, while the second involves multiple people interacting and creating challenging scenarios with occlusions. We primarily utilize the first sequence for testing to align with our research focus on single-person human pose estimation. However, since this sequence predominantly contains orbital view images due to the fisheye transformation, we extracted a subset of images and annotations from both sequences to create PoseFES Top, which consists of images of individuals directly beneath the camera, representing the extreme top view.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Bottom Val, Test, and Seq.</span> Since no existing datasets specifically cater to bottom-view data, we created a new dataset called RePo (RarE POses) to evaluate our approach. The dataset consists of images extracted from various sports videos obtained from YouTube. The most common sports featured are swimming, climbing, and skydiving. The Val and Test sets possess similar structures derived from comparable videos, while the Seq set comprises consecutive frames from one specific video of the pole vault. We employ the Seq set to demonstrate that substantial rotations of the person often accompany extreme views. Examples of real images from the new dataset are in the <a href="#S2.F2.5" title="In 2 Related Work ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Top Val.</span> Similar to the Bottom datasets, this dataset is collected from sports videos focusing on the top-view perspective. It serves as a validation set during the top-view training phase. The Top Val is also part of the new RePo dataset.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">For further reference, a summary of the new datasets introduced in this work is presented in the <a href="#S4.T1" title="In 4.1 Implementation Details ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Metrics.</span> All experiments were conducted following the COCO-style settings. The evaluation metric used was OKS-based AP (average precision), as specified in the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Viewpoint Dependency Analysis</h3>

<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:119.2pt;"><img src="/html/2307.06737/assets/imgs/view_analysis.png" id="S4.F6.1.g1" class="ltx_graphics ltx_img_portrait" width="598" height="875" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F6.1.2.2" class="ltx_text" style="font-size:90%;">Pose estimation quality as a function of viewpoint, in spherical coordinates. Darker colors mark higher OKS (smaller error).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:86.7pt;"><img src="/html/2307.06737/assets/imgs/SMPL_coordinates.png" id="S4.F6.2.g1" class="ltx_graphics ltx_img_portrait" width="598" height="1303" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.2.2.2" class="ltx_text" style="font-size:90%;">SMPL coordinates:
x (red), y (green), and z (blue)</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">RePoGen enables us to analyze the performance of state-of-the-art methods from different viewpoints. We analyze the performance in controlled settings, where individuals are well-separated and have clearly defined bounding boxes. Given the vast and complex pose space, we do not sample poses systematically. Instead, we sample 4 000 random poses with uniform pose simplicity between 1.0 and 3.0 and render each one from 5 views uniformly distributed along a sphere surface resulting in 20 000 images.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The analysis is based on the ViTPose model, which demonstrated the best performance on the COCO dataset at the time of writing. However, the results were also verified on other models, namely SWIN and HRNet, as implemented in the MMPose framework.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.3" class="ltx_p">The <a href="#S4.F6" title="In 4.3 Viewpoint Dependency Analysis ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> visualizes the errors of each sample in a spherical coordinate system with a fixed radius, where the horizontal and vertical axis represents latitude and longitude, respectively. The top view is indicated by a red circle at coordinates <math id="S4.SS3.p3.1.m1.2" class="ltx_Math" alttext="[\frac{\pi}{2},\frac{\pi}{2}]" display="inline"><semantics id="S4.SS3.p3.1.m1.2a"><mrow id="S4.SS3.p3.1.m1.2.3.2" xref="S4.SS3.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS3.p3.1.m1.2.3.2.1" xref="S4.SS3.p3.1.m1.2.3.1.cmml">[</mo><mfrac id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">π</mi><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">2</mn></mfrac><mo id="S4.SS3.p3.1.m1.2.3.2.2" xref="S4.SS3.p3.1.m1.2.3.1.cmml">,</mo><mfrac id="S4.SS3.p3.1.m1.2.2" xref="S4.SS3.p3.1.m1.2.2.cmml"><mi id="S4.SS3.p3.1.m1.2.2.2" xref="S4.SS3.p3.1.m1.2.2.2.cmml">π</mi><mn id="S4.SS3.p3.1.m1.2.2.3" xref="S4.SS3.p3.1.m1.2.2.3.cmml">2</mn></mfrac><mo stretchy="false" id="S4.SS3.p3.1.m1.2.3.2.3" xref="S4.SS3.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.2b"><interval closure="closed" id="S4.SS3.p3.1.m1.2.3.1.cmml" xref="S4.SS3.p3.1.m1.2.3.2"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><divide id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></divide><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝜋</ci><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">2</cn></apply><apply id="S4.SS3.p3.1.m1.2.2.cmml" xref="S4.SS3.p3.1.m1.2.2"><divide id="S4.SS3.p3.1.m1.2.2.1.cmml" xref="S4.SS3.p3.1.m1.2.2"></divide><ci id="S4.SS3.p3.1.m1.2.2.2.cmml" xref="S4.SS3.p3.1.m1.2.2.2">𝜋</ci><cn type="integer" id="S4.SS3.p3.1.m1.2.2.3.cmml" xref="S4.SS3.p3.1.m1.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.2c">[\frac{\pi}{2},\frac{\pi}{2}]</annotation></semantics></math>, and the bottom view is denoted by a red cross at coordinates <math id="S4.SS3.p3.2.m2.2" class="ltx_Math" alttext="[\frac{\pi}{2},-\frac{\pi}{2}]" display="inline"><semantics id="S4.SS3.p3.2.m2.2a"><mrow id="S4.SS3.p3.2.m2.2.2.1" xref="S4.SS3.p3.2.m2.2.2.2.cmml"><mo stretchy="false" id="S4.SS3.p3.2.m2.2.2.1.2" xref="S4.SS3.p3.2.m2.2.2.2.cmml">[</mo><mfrac id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">π</mi><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">2</mn></mfrac><mo id="S4.SS3.p3.2.m2.2.2.1.3" xref="S4.SS3.p3.2.m2.2.2.2.cmml">,</mo><mrow id="S4.SS3.p3.2.m2.2.2.1.1" xref="S4.SS3.p3.2.m2.2.2.1.1.cmml"><mo id="S4.SS3.p3.2.m2.2.2.1.1a" xref="S4.SS3.p3.2.m2.2.2.1.1.cmml">−</mo><mfrac id="S4.SS3.p3.2.m2.2.2.1.1.2" xref="S4.SS3.p3.2.m2.2.2.1.1.2.cmml"><mi id="S4.SS3.p3.2.m2.2.2.1.1.2.2" xref="S4.SS3.p3.2.m2.2.2.1.1.2.2.cmml">π</mi><mn id="S4.SS3.p3.2.m2.2.2.1.1.2.3" xref="S4.SS3.p3.2.m2.2.2.1.1.2.3.cmml">2</mn></mfrac></mrow><mo stretchy="false" id="S4.SS3.p3.2.m2.2.2.1.4" xref="S4.SS3.p3.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.2b"><interval closure="closed" id="S4.SS3.p3.2.m2.2.2.2.cmml" xref="S4.SS3.p3.2.m2.2.2.1"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><divide id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"></divide><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝜋</ci><cn type="integer" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">2</cn></apply><apply id="S4.SS3.p3.2.m2.2.2.1.1.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1"><minus id="S4.SS3.p3.2.m2.2.2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1"></minus><apply id="S4.SS3.p3.2.m2.2.2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1.2"><divide id="S4.SS3.p3.2.m2.2.2.1.1.2.1.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1.2"></divide><ci id="S4.SS3.p3.2.m2.2.2.1.1.2.2.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1.2.2">𝜋</ci><cn type="integer" id="S4.SS3.p3.2.m2.2.2.1.1.2.3.cmml" xref="S4.SS3.p3.2.m2.2.2.1.1.2.3">2</cn></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.2c">[\frac{\pi}{2},-\frac{\pi}{2}]</annotation></semantics></math>. The front view corresponds to coordinates <math id="S4.SS3.p3.3.m3.2" class="ltx_Math" alttext="[0,0]" display="inline"><semantics id="S4.SS3.p3.3.m3.2a"><mrow id="S4.SS3.p3.3.m3.2.3.2" xref="S4.SS3.p3.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS3.p3.3.m3.2.3.2.1" xref="S4.SS3.p3.3.m3.2.3.1.cmml">[</mo><mn id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">0</mn><mo id="S4.SS3.p3.3.m3.2.3.2.2" xref="S4.SS3.p3.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS3.p3.3.m3.2.2" xref="S4.SS3.p3.3.m3.2.2.cmml">0</mn><mo stretchy="false" id="S4.SS3.p3.3.m3.2.3.2.3" xref="S4.SS3.p3.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.2b"><interval closure="closed" id="S4.SS3.p3.3.m3.2.3.1.cmml" xref="S4.SS3.p3.3.m3.2.3.2"><cn type="integer" id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">0</cn><cn type="integer" id="S4.SS3.p3.3.m3.2.2.cmml" xref="S4.SS3.p3.3.m3.2.2">0</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.2c">[0,0]</annotation></semantics></math>, located at the left edge of the image. The OKS score of each sample is indicated by the color of the point, with darker blue indicating a higher score and yellow representing a lower score.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">As expected, the findings showed that state-of-the-art methods performed poorly on extreme views. Notably, the top-back view performed worse than the top-front view, while the error distribution around the bottom view appeared symmetric. The spread of the error around the bottom view is wider. The image is not smooth because some poses with lower pose simplicity proved challenging even in orbital views.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with baseline</h3>

<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Dataset</th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bottom Test</th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">PoseFES Top</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<th id="S4.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">COCO</th>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">35.1</td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">42.0</td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<th id="S4.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RePoGen <span id="S4.T2.2.3.2.1.1" class="ltx_text" style="font-size:90%;">(bottom)</span>
</th>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.3.2.2.1" class="ltx_text ltx_font_bold">61.8</span></td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">52.9</td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<th id="S4.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RePoGen <span id="S4.T2.2.4.3.1.1" class="ltx_text" style="font-size:90%;">(top)</span>
</th>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_center">46.3</td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center">53.9</td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<th id="S4.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">RePoGen <span id="S4.T2.2.5.4.1.1" class="ltx_text" style="font-size:90%;">(top+bottom)</span>
</th>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">53.9</td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T2.2.5.4.3.1" class="ltx_text ltx_font_bold">54.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;"> AP on the RePo Bottom Test set and PoseFES Top; training on COCO and sets of 3 000 images from the RePoGen.
</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Dataset</th>
<th id="S4.T3.1.2.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">PoseFES 1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">COCO</th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">75.7</td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<th id="S4.T3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">THEODORE+</th>
<td id="S4.T3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">76.1<span id="S4.T3.1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;"></span><sup id="S4.T3.1.1.1.2" class="ltx_sup">†</sup>
</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RePoGen <span id="S4.T3.1.4.2.1.1" class="ltx_text" style="font-size:90%;">(30 epochs)</span>
</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_nopad_r ltx_align_center">77.9</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">RePoGen</th>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T3.1.5.3.2.1" class="ltx_text ltx_font_bold">79.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.6.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.1" class="ltx_text" style="font-size:90%;">
AP on the PoseFES1 set; training on COCO, THEODORE+ by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and RePoGen dataset. The result marked (<sup id="S4.T3.3.1.1" class="ltx_sup">†</sup>) taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The comparison table <a href="#S4.T2" title="In 4.4 Comparison with baseline ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates the performance comparison between the baseline model (off-the-shelf ViTPose-s trained on the COCO dataset) and the proposed approach. We show variants with bottom-view, top-view, and mixed bottom and top-view RePoGen synthetic images. The results highlight a notable improvement achieved through the utilization of synthetic data and training with rotation augmentation. Interestingly, incorporating synthetic data from the bottom view enhances the model’s performance on the bottom and top view, suggesting a similarity between the two extreme view domains. Similarly, training with synthetic data from the top-view demonstrates improvements across top-view and bottom-view scenarios.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">To facilitate a comprehensive comparison of RePoGen with prior research, we conducted fine-tuning of the HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> model from the MMPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> model zoo following the same procedure as described by Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The performance evaluation, as presented in <a href="#S4.T3" title="In 4.4 Comparison with baseline ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, showcases the effectiveness of RePoGen in comparison to the THEODORE+ dataset and a model trained solely on the COCO dataset. We observed that surpassing the prescribed 30-epoch fine-tuning, as mentioned in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, led to further improvements in performance. Consequently, we report results for the 30-epoch mark and the best-achieved performance. RePoGen achieves superior results despite utilizing significantly fewer data, incorporating 3000 synthetic images compared to 160,000 THEODORE+ images.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We analyze and evaluate the influence of each component individually, as described in the following paragraphs. Throughout the ablation study, the strong rotation augmentation is consistently applied, and unless otherwise specified, 1000 RePoGen are used for experimentation.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"># of images</th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bottom Test</th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bottom Seq</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.1" class="ltx_tr">
<th id="S4.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">500</th>
<td id="S4.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">54.1</td>
<td id="S4.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">86.1</td>
</tr>
<tr id="S4.T4.2.3.2" class="ltx_tr">
<th id="S4.T4.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1000</th>
<td id="S4.T4.2.3.2.2" class="ltx_td ltx_align_center">59.1</td>
<td id="S4.T4.2.3.2.3" class="ltx_td ltx_align_center">89.0</td>
</tr>
<tr id="S4.T4.2.4.3" class="ltx_tr">
<th id="S4.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3000</th>
<td id="S4.T4.2.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.3.2.1" class="ltx_text ltx_font_bold">61.8</span></td>
<td id="S4.T4.2.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.3.3.1" class="ltx_text ltx_font_bold">90.5</span></td>
</tr>
<tr id="S4.T4.2.5.4" class="ltx_tr">
<th id="S4.T4.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">5000</th>
<td id="S4.T4.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">58.8</td>
<td id="S4.T4.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">86.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">
AP on the Bottom dataset of RePo; training with different number of RePoGen images.
</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">RePoGen data</th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bottom Test</th>
<th id="S4.T5.2.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bottom Seq</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.2.1" class="ltx_tr">
<th id="S4.T5.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">baseline</th>
<td id="S4.T5.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">59.1</td>
<td id="S4.T5.2.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">89.0</td>
</tr>
<tr id="S4.T5.2.3.2" class="ltx_tr">
<th id="S4.T5.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o rotation</th>
<td id="S4.T5.2.3.2.2" class="ltx_td ltx_align_center">45.9</td>
<td id="S4.T5.2.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">72.3</td>
</tr>
<tr id="S4.T5.2.4.3" class="ltx_tr">
<th id="S4.T5.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o background</th>
<td id="S4.T5.2.4.3.2" class="ltx_td ltx_align_center">56.2</td>
<td id="S4.T5.2.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center">85.2</td>
</tr>
<tr id="S4.T5.2.5.4" class="ltx_tr">
<th id="S4.T5.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">w/o texture</th>
<td id="S4.T5.2.5.4.2" class="ltx_td ltx_align_center">59.5</td>
<td id="S4.T5.2.5.4.3" class="ltx_td ltx_nopad_r ltx_align_center">88.2</td>
</tr>
<tr id="S4.T5.2.6.5" class="ltx_tr">
<th id="S4.T5.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">default SMPL pose</th>
<td id="S4.T5.2.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.6.5.2.1" class="ltx_text ltx_font_bold">60.4</span></td>
<td id="S4.T5.2.6.5.3" class="ltx_td ltx_nopad_r ltx_align_center">88.4</td>
</tr>
<tr id="S4.T5.2.7.6" class="ltx_tr">
<th id="S4.T5.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">uniform distribution</th>
<td id="S4.T5.2.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">59.2</td>
<td id="S4.T5.2.7.6.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.2.7.6.3.1" class="ltx_text ltx_font_bold">89.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">
Ablation study.
Training without various components - AP comparison on the Bottom dataset of RePo.
</span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Number of images.</span> The <a href="#S4.T4" title="In 4.5 Ablation Study ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a> provides insights into the impact of adding additional images to the COCO dataset. With the COCO train set already containing over 200 000 poses, adding 5 000 images represents approximately 2% of the dataset, resulting in minimal impact on training time. Remarkably, even including as few as 500 images yields noticeable improvements. However, saturation is observed at around 3 000 images, beyond which further additions may have a marginal negative effect on performance probably due to the overfit to the synthetic data.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">Texture and background.</span> The <a href="#S4.T5" title="In 4.5 Ablation Study ‣ 4 Experiments ‣ Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> validates other design choices made in the pose generation technique. It demonstrates the improvement in performance on the Test set, which assesses the model’s ability to handle extreme views. Additionally, the results on the Seq set, which includes extreme and adjacent views, further support the effectiveness of these design choices. Notably, including background images contributes to a modest enhancement in performance. On the other hand, adding random texture does not yield significant improvements, suggesting that the realism of the data may not be a crucial factor in this context.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p"><span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Rotation.</span> Incorporating stronger rotation yields significant performance improvements. The effect is particularly pronounced in the Seq set, where the presence of views adjacent to the extreme ones amplifies the difference even further. Even without rotation, our approach outperforms the off-the-shelf model, highlighting the importance of including extreme view data in the training. Consequently, it is advisable always to employ rotation data augmentation up to 180<sup id="S4.SS5.p4.1.2" class="ltx_sup"><span id="S4.SS5.p4.1.2.1" class="ltx_text ltx_font_italic">∘</span></sup> for applications involving pose estimation in videos with extreme views.</p>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p id="S4.SS5.p5.1" class="ltx_p"><span id="S4.SS5.p5.1.1" class="ltx_text ltx_font_bold">Default SMPL pose and uniform distribution.</span> The impact of <span id="S4.SS5.p5.1.2" class="ltx_text ltx_font_italic">uniform joint angle distribution</span> and the <span id="S4.SS5.p5.1.3" class="ltx_text ltx_font_italic">default SMPL pose</span> remains inconclusive. In the experimentation with the Bottom datasets, training models with uniform distribution proved advantageous compared to the baseline. However, contrasting results were observed when training on the top view and evaluating on the PoseFES dataset. This discrepancy may be attributed to the nature of the Bottom datasets, which encompass sports activities characterized by extreme poses.
In contrast, the PoseFES dataset primarily features individuals engaged in walking and standing. Similar results can be observed with the default SMPL pose. Both approaches generate poses from less usual distribution than the baseline. The observed difference in performance compared to the baseline is approximately 0.5 percentage points, indicating a relatively minor effect. Nonetheless, employing poses aligned with the target domain appears preferable for optimal results.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In conclusion, this paper presented a novel method for generating synthetic images (RePoGen) with accurate human pose ground truth by incorporating constraints on joint rotation. The view dependency of performance in SOTA methods was thoroughly analyzed, revealing substantial performance degradation in extreme views. We then trained a state-of-the-art model on the COCO dataset enhanced by RePoGen data to improve performance in extreme views.
The key findings can be summarized as follows:</p>
</div>
<div id="S5.p2" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">The SOTA methods perform worse in top and bottom views. The top-back view exhibited poorer results than the top-front view, likely attributed to challenges associated with face visibility.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Including a small number of synthetic training samples with extreme views significantly improved extreme view pose estimation.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Stronger rotation data augmentation proved crucial, particularly for views adjacent to extreme viewpoints. This augmentation technique is recommended especially for fisheye ceiling-mounted cameras.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">The pose estimation performance increased when synthetic data closely resembled the poses observed in the target domain.</p>
</div>
</li>
</ol>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The next step would be utilizing the proposed model to pre-annotate a larger dataset of extreme views from sports using a human-in-the-loop approach. This process will enable further investigation into the challenges arising from extreme poses. By delving deeper into these complexities, future research endeavors can enhance the understanding and performance of pose estimation in extreme-view scenarios. Furthermore, the annotated dataset comprising almost 200 images of the bottom view and nearly 100 images of the front view, primarily sourced from sports activities, will be made publicly available, contributing to the advancement of the field.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Potential misuse.</span> Among other things, our method improves the pose estimation models in ceiling-mounted and surveillance cameras, and it is important to consider potential privacy implications when coupled with face recognition or action recognition systems. This paper focuses on enhancing pose estimation rather than utilizing privacy-sensitive identification models. Nevertheless, we will restrict the usage of our code in a legal way as other fields could benefit from improved extreme view pose estimation.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Acknowledgements.</span> This work was supported by the Technology Agency of the Czech Republic project No. SS05010008 and Ministry of the Interior of the Czech Republic project No. VJ02010041.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">2d human pose estimation: New benchmark and state of the art
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, June 2014.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Frédo Durand, and John V.
Guttag.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Synthesizing images of humans in unseen poses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, pages 8340–8348, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Yanrui Bin, Xuan Cao, Xinya Chen, Yanhao Ge, Ying Tai, Chengjie Wang, Jilin Li,
Feiyue Huang, Changxin Gao, and Nong Sang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Adversarial semantic data augmentation for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xu Chen, Jie Song, and Otmar Hilliges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Unpaired pose guided human image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, abs/1901.02284, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yucheng Chen, Mingyi He, and Yuchao Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Nearby-person occlusion data augmentation for human pose estimation
with non-extra annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 Asia-Pacific Signal and Information Processing Association
Annual Summit and Conference (APSIPA ASC)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, pages 282–287, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
MMPose Contributors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Openmmlab pose estimation toolbox and benchmark.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmpose" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/mmpose</a><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">
Yiwei Ding, Wenjin Deng, Yinglin Zheng, Pengfei Liu, Meihong Wang, Xuan Cheng,
Jianmin Bao, Dong Chen, and Ming Zeng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">I</span><sup id="bib.bib7.4.2" class="ltx_sup"><span id="bib.bib7.4.2.1" class="ltx_text" style="font-size:90%;">2</span></sup><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">r-net: Intra- and inter-human relation network for multi-person
pose estimation, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Nicola Garau, Giulia Martinelli, Piotr Bródka, Niccolò Bisagno, and Nicola
Conci.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Panoptop: a framework for generating viewpoint-invariant human pose
estimation datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision
Workshops (ICCVW)</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 234–242, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, and Li
Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Towards viewpoint invariant 3d human pose estimation, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">The devil is in the details: Delving into unbiased data processing
for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Linzhi Huang, Yulong Li, Hongbo Tian, Yue Yang, Xiangang Li, Weihong Deng, and
Jieping Ye.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Semi-supervised 2d human pose estimation driven by position
inconsistency pseudo label correction module.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 693–703, June 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Håkon Hukkelås and Frank Lindseth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Synthesizing anyone, anywhere, in any pose, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Human3.6m: Large scale datasets and predictive methods for 3d human
sensing in natural environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">,
36(7):1325–1339, jul 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Sam Johnson and Mark Everingham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Clustered pose and nonlinear appearance models for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages
12.1–12.11. BMVA Press, 2010.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">doi:10.5244/C.24.12.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart C. Nabbe, I. Matthews, Takeo
Kanade, Shohei Nobuhara, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Panoptic studio: A massively multiview system for social motion
capture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">,
pages 3334–3342, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Crowdpose: Efficient crowded scenes pose estimation and a new
benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00324</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Evan Ling, De-Kai Huang, and Minhoe Hur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Humans need not label more humans: Occlusion copy &amp; paste for
occluded human instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">British Machine Vision Conference</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Huajun Liu, Fuqiang Liu, Xinyi Fan, and Dong Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Polarized self-attention: Towards high-quality pixel-wise regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Arxiv Pre-Print arXiv:2107.00782</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.
Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">SMPL: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 34(6):248:1–248:16,
Oct. 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu
Tang, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Learning to dress 3D people in generative clothing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A.
Osman, Dimitrios Tzionas, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Expressive body capture: 3d hands, face, and body from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Xi Peng, Zhiqiang Tang, Fei Yang, Rogério Schmidt Feris, and Dimitris N.
Metaxas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Jointly optimize data augmentation and network training: Adversarial
data augmentation in human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, pages 2226–2234, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon
Reiß, and Rainer Stiefelhagen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Let’s play for action: Recognizing activities of daily living by
learning from life simulation video games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, pages 8563–8569, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 5693–5703, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Garvita Tiwari, Dimitrije Antic, Jan Eric Lenssen, Nikolaos Sarafianos, Tony
Tung, and Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Pose-ndf: Modeling human pose manifolds with neural distance fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, October
2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Synthetic humans for action recognition from unseen viewpoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 129:2264 – 2287,
2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black,
Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">ViTPose: Simple vision transformer baselines for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jingrui Yu, Tobias Scheck, Roman Seidel, Yukti Adya, Dipankar Nandi, and
Gangolf Hirtz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Human pose estimation in monocular omnidirectional top-view images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 6410–6419, June 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Distribution-aware coordinate representation for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Adding conditional control to text-to-image diffusion models, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Song-Hai Zhang, Ruilong Li, Xin Dong, Paul L. Rosin, Zixi Cai, Han Xi,
Dingcheng Yang, Hao-Zhi Huang, and Shi-Min Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Pose2seg: Detection free human instance segmentation, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.06736" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.06737" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.06737">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.06737" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.06738" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:21:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
