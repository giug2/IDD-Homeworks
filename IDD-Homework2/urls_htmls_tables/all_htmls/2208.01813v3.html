<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.01813] TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation</title><meta property="og:description" content="Text-VQA aims at answering questions that require understanding the textual cues in an image. Despite the great progress of existing Text-VQA methods, their performance suffers from insufficient human-labeled question-…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.01813">

<!--Generated on Wed Mar 13 15:44:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Jun Wang<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Part of this work was done when JW was an intern at Salesforce Research.</span></span></span>junwang@umiacs.umd.edu1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Mingfei Gaomingfei.gao@salesforce.com2
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Yuqian Huyhu1109@umd.edu1
<span id="p1.2.3" class="ltx_ERROR undefined">\addauthor</span>Ramprasaath R. Selvarajurselvaraju@salesforce.com2
<span id="p1.2.4" class="ltx_ERROR undefined">\addauthor</span>Chetan Ramaiahcramaiah@salesforce.com2
<span id="p1.2.5" class="ltx_ERROR undefined">\addauthor</span>Ran Xuran.xu@salesforce.com2
<span id="p1.2.6" class="ltx_ERROR undefined">\addauthor</span>Joseph F. JaJajosephj@umd.edu1
<span id="p1.2.7" class="ltx_ERROR undefined">\addauthor</span>Larry S. Davislsd@umiacs.umd.edu1
<span id="p1.2.8" class="ltx_ERROR undefined">\addinstitution</span>
University of Maryland, 
<br class="ltx_break">College Park, MD. USA.

<span id="p1.2.9" class="ltx_ERROR undefined">\addinstitution</span>
Salesforce Research, 
<br class="ltx_break">Palo Alto, CA. USA.


  BMVC Author Guidelines</p>
</div>
<h1 class="ltx_title ltx_title_document">TAG: Boosting Text-VQA via Text-aware
<br class="ltx_break">Visual Question-answer Generation</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Text-VQA aims at answering questions that require understanding the textual cues in an image. Despite the great progress of existing Text-VQA methods, their performance suffers from insufficient human-labeled question-answer (QA) pairs. However, we observe that, in general, the scene text is not fully exploited in the existing datasets– only a small portion of the text in each image participates in the annotated QA activities. This results in a huge waste of useful information. To address this deficiency, we develop a new method to generate high-quality and diverse QA pairs by explicitly utilizing the existing rich text available in the scene context of each image. Specifically, we propose, TAG, a text-aware visual question-answer generation architecture that learns to produce meaningful, and accurate QA samples using a multimodal transformer. The architecture exploits underexplored scene text information and enhances scene understanding of Text-VQA models by combining the generated QA pairs with the initial training data. Extensive experimental results on two well-known Text-VQA benchmarks (TextVQA and ST-VQA) demonstrate that our proposed TAG effectively enlarges the training data that helps improve the Text-VQA performance without extra labeling effort. Moreover, our model outperforms state-of-the-art approaches that are pre-trained with extra large-scale data. <a target="_blank" href="https://github.com/HenryJunW/TAG" title="" class="ltx_ref ltx_href">Code is available here</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA) task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Antol et al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
Parikh</a>]</cite> aims at inferring the answer to a question based on a holistic understanding of an image. It facilitates many AI applications such as robot interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Anderson et al.(2018b)Anderson, Wu, Teney, Bruce,
Johnson, Sünderhauf, Reid, Gould, and Van Den Hengel</a>]</cite>, document analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Mishra et al.(2019)Mishra, Shekhar, Singh, and
Chakraborty</a>]</cite> and assistance for visually impaired people <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Bigham et al.(2010)Bigham, Jayant, Ji, Little, Miller, Miller, Miller,
Tatarowicz, White, White, et al.</a>]</cite>.
Text-VQA specifically addresses question answering requests where reasoning text in an image is essential to answer a question. It is a more challenging task in a sense that it requires not only understanding the question and the visual context, but also the embedded text in an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite>.
To achieve this goal, Text-VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> aim at studying the interactions among question words, visual objects, and scene text in an image. Recent approaches have focused on either improving transformer-based architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
Kaiser, and Polosukhin</a>]</cite> in a multi-modal manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>, <a href="#bib.bibx32" title="" class="ltx_ref">Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>, <a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>, <a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite>, or adopting pre-training using additional large-scale data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> to further boost their model performance. All of these methods heavily rely on annotations of question-answer (QA) pairs for model training. Intuitively, the more annotated pairs are leveraged, the better performance a model can achieve. Thanks to the development of text-related VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Gurari et al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
Bigham</a>, <a href="#bib.bibx48" title="" class="ltx_ref">Wang et al.(2021a)Wang, Xiao, Lu, Jin, and
He</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>, <a href="#bib.bibx36" title="" class="ltx_ref">Mishra et al.(2019)Mishra, Shekhar, Singh, and
Chakraborty</a>]</cite>, Text-VQA has achieved rapid progress.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2208.01813/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="176" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Left:</span> statistics of the numbers of annotated QA pairs (blue) and scene text words (orange) for each image in the TextVQA training set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite>. Clearly, the scene text words are not fully leveraged in the annotations. <span id="S1.F1.4.2" class="ltx_text ltx_font_bold">Right:</span> an example of a training image with more than 5 scene text words, which is typical. Best viewed in color.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the amount of Text-VQA annotations available is still limited due to the sparse labeling of QA pairs in recent datasets. Consider for example the TextVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite> whose statistics are illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It shows that only one or at most two QA pairs are annotated in the training images. Meanwhile, we also compute the number of text words presented in each image<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>We compute the average of different OCR tokens acquired by the Microsoft-OCR system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> for the entire training set.</span></span></span> and observe that most of the images contain at least 5 text words. This observation indicates that scene text is not fully utilized in the annotations, and hence not fully leveraged by recent methods. A natural question would be – can we fully take advantage of text words in images without incurring extra annotation costs?</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Data Augmentation for VQA ‣ 2 Related Work ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we propose to tackle the problem by learning to generate large-scale and diverse text-related QA pairs from existing Text-VQA datasets, using the generated QA pairs to expand the training set and ultimately improving Text-VQA models.
Towards this end, we introduce TAG, a text-aware QA generation model, that generates novel text-related QA pairs at scale. It takes text words (the answer) as one of the inputs and aims at generating a question corresponding to this answer by leveraging the rich visual and scene textual cues. TAG is trained using the originally annotated QA pairs and adapts to generate new QA pairs containing scene text words in images that are not utilized in original annotations. No extra human annotation is required in our framework, so the size and diversity of the training data could be easily and largely increased. Since our generation process is disentangled with the training of Text-VQA models, our generated QA pairs can be used by most of the recent methods.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In summary, we introduce a simple yet efficient text-aware generation approach, which automatically and efficiently generates new QA pairs to improve the performance of the current Text-VQA methods. The main contributions of our work are three-fold:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We identify and analyze possible deficiencies of current Text-VQA datasets– sparse annotations of QA pairs - and propose to better utilize unused scene text information within each image to improve the model performance.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To the best of our knowledge, TAG is the first method that explores scene text-related QA pairs generation for improving Text-VQA tasks without additional labeled data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We consistently demonstrate the effectiveness of our method with two recent Text-VQA models on two Text-VQA datasets. The experimental results suggest that the existing Text-VQA algorithms can benefit from training with the high-quality and diverse QA pairs generated by our method.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-related VQA</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To study and evaluate the Text-VQA task, several scene text-based datasets are introduced, including VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Gurari et al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
Bigham</a>]</cite>, OCR-VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Mishra et al.(2019)Mishra, Shekhar, Singh, and
Chakraborty</a>]</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite>, and ST-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>]</cite>. With the help of these datasets, numerous approaches have been proposed in recent years which increasingly improve Text-VQA performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Jiang et al.(2018)Jiang, Natarajan, Chen, Rohrbach, Batra, and
Parikh</a>, <a href="#bib.bibx3" title="" class="ltx_ref">Anderson et al.(2018a)Anderson, He, Buehler, Teney,
Johnson, Gould, and Zhang</a>, <a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>, <a href="#bib.bibx32" title="" class="ltx_ref">Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Gao et al.(2020)Gao, Li, Wang, Shan, and Chen</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>, <a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>, <a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Zhang and Yang(2021)</a>, <a href="#bib.bibx51" title="" class="ltx_ref">Zeng et al.(2021)Zeng, Zhang, Zhou, and Yang</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Biten et al.(2022)Biten, Litman, Xie, Appalaraju, and
Manmatha</a>]</cite>.
LoRRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite> is an early work that extends the original VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Jiang et al.(2018)Jiang, Natarajan, Chen, Rohrbach, Batra, and
Parikh</a>, <a href="#bib.bibx3" title="" class="ltx_ref">Anderson et al.(2018a)Anderson, He, Buehler, Teney,
Johnson, Gould, and Zhang</a>]</cite> with an extra OCR attention branch to select the answer from either a fixed word vocabulary or OCR tokens. Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Devlin et al.(2018)Devlin, Chang, Lee, and Toutanova</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Chen et al.(2020)Chen, Li, Yu, El Kholy, Ahmed, Gan, Cheng, and
Liu</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Dosovitskiy et al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
et al.</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Carion et al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
Zagoruyko</a>, <a href="#bib.bibx33" title="" class="ltx_ref">Liu et al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
Guo</a>, <a href="#bib.bibx53" title="" class="ltx_ref">Zhao et al.(2021)Zhao, Jiang, Jia, Torr, and Koltun</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Guan et al.(2022)Guan, Wang, Lan, Chandra, Wu, Davis, and
Manocha</a>, <a href="#bib.bibx54" title="" class="ltx_ref">Zhou et al.(2020)Zhou, Palangi, Zhang, Hu, Corso, and
Gao</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Baevski et al.(2020)Baevski, Zhou, Mohamed, and
Auli</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Wang(2022)</a>, <a href="#bib.bibx47" title="" class="ltx_ref">Wang et al.(2022)Wang, Chen, Wu, Luo, Zhou, Zhao, Xie, Liu, Jiang, and
Yuan</a>]</cite> show the benefits of transformer for different vision, language and speech tasks. M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite> develops a transformer-based architecture to fuse different input modalities and iteratively predicts answers through a multi-step answer decoder. Inspired by M4C, more transformer-based models have been proposed with varied structure modifications. Among them, CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan</a>]</cite> constructs a graph network to model the interactions between text and visual objects. LaAP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>]</cite> predicts a bounding box to explain the generated answer. SSBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>]</cite> proposes to split the OCR token features into separate visual and linguistic attention branches. SMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>]</cite> reasons over structural text-object graphs and produces answers in a generative way. LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite> introduces a question-visual grounding pre-training task to connect question text and image regions. SA-M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>]</cite> builds a spatial graph to explicitly model relative spatial relations between visual objects and OCR tokens. TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> presents three text-aware pre-training tasks to align representations among scene text, text words, and visual objects.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">However, most of the existing works focus on designing sophisticated architectures that leverage the annotated text in an image and overlook the rich text information that is underused by the annotated QA activities. We fully explore the embedded scene text in images and explicitly generate novel QA pairs that can be used to boost the performance of downstream Text-VQA models.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Augmentation for VQA</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Data augmentation has been demonstrated to be an effective approach to improve the performance of the VQA task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Kafle et al.(2017)Kafle, Yousefhussien, and Kanan</a>, <a href="#bib.bibx39" title="" class="ltx_ref">Shah et al.(2019)Shah, Chen, Rohrbach, and Parikh</a>, <a href="#bib.bibx37" title="" class="ltx_ref">Ray et al.(2019)Ray, Sikka, Divakaran, Lee, and
Burachas</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Agarwal et al.(2020)Agarwal, Shetty, and Fritz</a>, <a href="#bib.bibx42" title="" class="ltx_ref">Tang et al.(2020)Tang, Ma, Zhang, Wu, and Yang</a>, <a href="#bib.bibx49" title="" class="ltx_ref">Wang et al.(2021b)Wang, Miao, and Specia</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Kant et al.(2021)Kant, Moudgil, Batra, Parikh, and
Agrawal</a>]</cite>. Kafle et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Kafle et al.(2017)Kafle, Yousefhussien, and Kanan</a>]</cite> propose to generate new questions using the existing semantic segmentation annotations and templates. Shah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Shah et al.(2019)Shah, Chen, Rohrbach, and Parikh</a>]</cite> introduce a cycle-consistent scheme generating question rephrasings to make VQA models more robust to linguistic variations. Ray et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Ray et al.(2019)Ray, Sikka, Divakaran, Lee, and
Burachas</a>]</cite> propose a consistency-improving data augmentation module to make VQA models answer consistently. Agarwal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Agarwal et al.(2020)Agarwal, Shetty, and Fritz</a>]</cite> explore data augmentation to improve the VQA model’s robustness to semantic visual variations. Tang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Tang et al.(2020)Tang, Ma, Zhang, Wu, and Yang</a>]</cite> use data augmentation to inject proper inductive biases into the VQA model. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Wang et al.(2021b)Wang, Miao, and Specia</a>]</cite> introduce a generative model for cross-modal data augmentation on VQA. Kant et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Kant et al.(2021)Kant, Moudgil, Batra, Parikh, and
Agrawal</a>]</cite> adopt the contrastive loss to make the VQA model robust to linguistic variations in generated questions.
However, these approaches are designed for the traditional VQA systems that do not emphasize the importance of scene text in their QA tasks. Our method is tailored for the problem of Text-VQA. It takes advantage of the underexploited scene text in images and enlarges the training samples by generating novel text-related QA pairs without the extra labeling cost.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2208.01813/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="85" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.2.1" class="ltx_text ltx_font_bold">The proposed Text-VQA framework.</span> It consists of two parts: a text-aware visual question-answer generation module (TAG), followed by a downstream Text-VQA model. TAG is based on a multi-modal transformer architecture, which takes a single image and text words (the answer) as input, and outputs a newly-generated question corresponding to the input answer. The generated QA pairs from TAG together with the originally labeled data are subsequently used to train Text-VQA models, leading to better Text-VQA performance.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The proposed framework is illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Data Augmentation for VQA ‣ 2 Related Work ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which consists of a transformer-based text-aware visual QA generation module named TAG, followed by a downstream Text-VQA model. Our core module, TAG, carries out text-aware data augmentation tailored for the Text-VQA task and generates novel QA pairs by leveraging underused scene text in an image. After the TAG module generates a large amount of new QA pairs, we directly augment the training data by combining the generated set and the originally labeled set. The augmented set is used by the downstream Text-VQA models to boost the model performance.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The workflow of our method is as follows. Given an image, an OCR system and an object detector are used to detect scene text and visual objects, respectively. As illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our TAG takes the scene text words of interest (the answer words), the visual objects and all the detected OCR tokens in the image as inputs and generates a question explicitly corresponding to the answer. Specifically, the answer words, visual objects, and all the OCR tokens are first represented by high-dimensional features (Section <a href="#S3.SS1" title="3.1 Multi-modality Feature Embeddings ‣ 3 Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Then, the multi-modality information is fully aggregated through a transformer architecture with the attention mechanism (Section <a href="#S3.SS2" title="3.2 Multi-modality Fusion ‣ 3 Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Finally, the enriched features are used to predict a question to the answer through iterative decoding in an auto-regressive manner (Section <a href="#S3.SS3" title="3.3 Text-aware Visual Question Prediction ‣ 3 Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). More details can be found in the supplementary.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2208.01813/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="122" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.4.1" class="ltx_text ltx_font_bold">The illustration of our proposed TAG.</span> High-dimensional feature representations are first extracted for three modalities, including extended answer words, visual objects, and scene text. Then, a multi-modal transformer is used to model the interactions of different modalities. Finally, a decoding module is used to predict the question corresponding to the answer through iterative decoding with an auto-regressive mechanism. <span id="S3.F3.5.2" class="ltx_text ltx_font_bold">Left:</span> Originally labeled QA pairs are used for training. <span id="S3.F3.6.3" class="ltx_text ltx_font_bold">Right:</span> During inference, detected OCR words are used as a novel answer to generate a question. Best viewed in color.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-modality Feature Embeddings</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We describe the feature embedding strategy of our work. The answer words, detected visual objects, and all the detected OCR tokens are embedded as high-dimensional features and then projected into a common d-dimensional embedding space.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.8" class="ltx_p"><span id="S3.SS1.p2.8.1" class="ltx_text ltx_font_bold">Embedding of extended answer words.</span>
We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> to use an extended representation to embed answer words. Given an answer input <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="{w}^{ans}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msup id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">w</mi><mrow id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.1.3.1" xref="S3.SS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.1.3.1a" xref="S3.SS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.1.m1.1.1.3.4" xref="S3.SS1.p2.1.m1.1.1.3.4.cmml">s</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑤</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><times id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.1"></times><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">𝑎</ci><ci id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3">𝑛</ci><ci id="S3.SS1.p2.1.m1.1.1.3.4.cmml" xref="S3.SS1.p2.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">{w}^{ans}</annotation></semantics></math>, we extend the words with labels of objects <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="{w}^{obj}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msup id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">w</mi><mrow id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.3.1a" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.1.3.4" xref="S3.SS1.p2.2.m2.1.1.3.4.cmml">j</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑤</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><times id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.1"></times><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">𝑜</ci><ci id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3">𝑏</ci><ci id="S3.SS1.p2.2.m2.1.1.3.4.cmml" xref="S3.SS1.p2.2.m2.1.1.3.4">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">{w}^{obj}</annotation></semantics></math> (detected from the object detector) and scene text OCR words <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="{w}^{ocr}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msup id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">w</mi><mrow id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.3.1" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.3.1a" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.3.m3.1.1.3.4" xref="S3.SS1.p2.3.m3.1.1.3.4.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑤</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><times id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.1"></times><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">𝑜</ci><ci id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3">𝑐</ci><ci id="S3.SS1.p2.3.m3.1.1.3.4.cmml" xref="S3.SS1.p2.3.m3.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">{w}^{ocr}</annotation></semantics></math> (generated from the OCR system) as a set of <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">K</annotation></semantics></math> text words.
A trainable BERT-style model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Devlin et al.(2018)Devlin, Chang, Lee, and Toutanova</a>]</cite> is adopted to extract the embedding of those text words, <math id="S3.SS1.p2.5.m5.4" class="ltx_Math" alttext="\bm{F}^{ans}=\{\bm{f}^{ans}_{1},\ \bm{f}^{ans}_{2},\ ...,\bm{f}^{ans}_{K}\}" display="inline"><semantics id="S3.SS1.p2.5.m5.4a"><mrow id="S3.SS1.p2.5.m5.4.4" xref="S3.SS1.p2.5.m5.4.4.cmml"><msup id="S3.SS1.p2.5.m5.4.4.5" xref="S3.SS1.p2.5.m5.4.4.5.cmml"><mi id="S3.SS1.p2.5.m5.4.4.5.2" xref="S3.SS1.p2.5.m5.4.4.5.2.cmml">𝑭</mi><mrow id="S3.SS1.p2.5.m5.4.4.5.3" xref="S3.SS1.p2.5.m5.4.4.5.3.cmml"><mi id="S3.SS1.p2.5.m5.4.4.5.3.2" xref="S3.SS1.p2.5.m5.4.4.5.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.4.4.5.3.1" xref="S3.SS1.p2.5.m5.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.4.4.5.3.3" xref="S3.SS1.p2.5.m5.4.4.5.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.4.4.5.3.1a" xref="S3.SS1.p2.5.m5.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.4.4.5.3.4" xref="S3.SS1.p2.5.m5.4.4.5.3.4.cmml">s</mi></mrow></msup><mo id="S3.SS1.p2.5.m5.4.4.4" xref="S3.SS1.p2.5.m5.4.4.4.cmml">=</mo><mrow id="S3.SS1.p2.5.m5.4.4.3.3" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p2.5.m5.4.4.3.3.4" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml">{</mo><msubsup id="S3.SS1.p2.5.m5.2.2.1.1.1" xref="S3.SS1.p2.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.5.m5.2.2.1.1.1.2.2" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.2.cmml">𝒇</mi><mn id="S3.SS1.p2.5.m5.2.2.1.1.1.3" xref="S3.SS1.p2.5.m5.2.2.1.1.1.3.cmml">1</mn><mrow id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.cmml"><mi id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.2" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.3" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1a" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.4" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.4.cmml">s</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p2.5.m5.4.4.3.3.5" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.5.m5.3.3.2.2.2" xref="S3.SS1.p2.5.m5.3.3.2.2.2.cmml"><mi id="S3.SS1.p2.5.m5.3.3.2.2.2.2.2" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.2.cmml">𝒇</mi><mn id="S3.SS1.p2.5.m5.3.3.2.2.2.3" xref="S3.SS1.p2.5.m5.3.3.2.2.2.3.cmml">2</mn><mrow id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.cmml"><mi id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.2" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.3" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1a" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.4" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.4.cmml">s</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p2.5.m5.4.4.3.3.6" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">…</mi><mo id="S3.SS1.p2.5.m5.4.4.3.3.7" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.5.m5.4.4.3.3.3" xref="S3.SS1.p2.5.m5.4.4.3.3.3.cmml"><mi id="S3.SS1.p2.5.m5.4.4.3.3.3.2.2" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.2.cmml">𝒇</mi><mi id="S3.SS1.p2.5.m5.4.4.3.3.3.3" xref="S3.SS1.p2.5.m5.4.4.3.3.3.3.cmml">K</mi><mrow id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.cmml"><mi id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.2" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.3" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1a" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.4" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.4.cmml">s</mi></mrow></msubsup><mo stretchy="false" id="S3.SS1.p2.5.m5.4.4.3.3.8" xref="S3.SS1.p2.5.m5.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.4b"><apply id="S3.SS1.p2.5.m5.4.4.cmml" xref="S3.SS1.p2.5.m5.4.4"><eq id="S3.SS1.p2.5.m5.4.4.4.cmml" xref="S3.SS1.p2.5.m5.4.4.4"></eq><apply id="S3.SS1.p2.5.m5.4.4.5.cmml" xref="S3.SS1.p2.5.m5.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.4.4.5.1.cmml" xref="S3.SS1.p2.5.m5.4.4.5">superscript</csymbol><ci id="S3.SS1.p2.5.m5.4.4.5.2.cmml" xref="S3.SS1.p2.5.m5.4.4.5.2">𝑭</ci><apply id="S3.SS1.p2.5.m5.4.4.5.3.cmml" xref="S3.SS1.p2.5.m5.4.4.5.3"><times id="S3.SS1.p2.5.m5.4.4.5.3.1.cmml" xref="S3.SS1.p2.5.m5.4.4.5.3.1"></times><ci id="S3.SS1.p2.5.m5.4.4.5.3.2.cmml" xref="S3.SS1.p2.5.m5.4.4.5.3.2">𝑎</ci><ci id="S3.SS1.p2.5.m5.4.4.5.3.3.cmml" xref="S3.SS1.p2.5.m5.4.4.5.3.3">𝑛</ci><ci id="S3.SS1.p2.5.m5.4.4.5.3.4.cmml" xref="S3.SS1.p2.5.m5.4.4.5.3.4">𝑠</ci></apply></apply><set id="S3.SS1.p2.5.m5.4.4.3.4.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3"><apply id="S3.SS1.p2.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.1.1.1.2.1.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.5.m5.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.2">𝒇</ci><apply id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3"><times id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.1"></times><ci id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.2.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.2">𝑎</ci><ci id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.3.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.3">𝑛</ci><ci id="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.4.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.2.3.4">𝑠</ci></apply></apply><cn type="integer" id="S3.SS1.p2.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS1.p2.5.m5.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p2.5.m5.3.3.2.2.2.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2">subscript</csymbol><apply id="S3.SS1.p2.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.3.3.2.2.2.2.1.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2">superscript</csymbol><ci id="S3.SS1.p2.5.m5.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.2">𝒇</ci><apply id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3"><times id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.1"></times><ci id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.2.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.2">𝑎</ci><ci id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.3.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.3">𝑛</ci><ci id="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.4.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.2.3.4">𝑠</ci></apply></apply><cn type="integer" id="S3.SS1.p2.5.m5.3.3.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">…</ci><apply id="S3.SS1.p2.5.m5.4.4.3.3.3.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.4.4.3.3.3.1.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3">subscript</csymbol><apply id="S3.SS1.p2.5.m5.4.4.3.3.3.2.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.4.4.3.3.3.2.1.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3">superscript</csymbol><ci id="S3.SS1.p2.5.m5.4.4.3.3.3.2.2.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.2">𝒇</ci><apply id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3"><times id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.1"></times><ci id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.2.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.2">𝑎</ci><ci id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.3.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.3">𝑛</ci><ci id="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.4.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.2.3.4">𝑠</ci></apply></apply><ci id="S3.SS1.p2.5.m5.4.4.3.3.3.3.cmml" xref="S3.SS1.p2.5.m5.4.4.3.3.3.3">𝐾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.4c">\bm{F}^{ans}=\{\bm{f}^{ans}_{1},\ \bm{f}^{ans}_{2},\ ...,\bm{f}^{ans}_{K}\}</annotation></semantics></math>, where <math id="S3.SS1.p2.6.m6.4" class="ltx_Math" alttext="k=\{1,2,...,K\}" display="inline"><semantics id="S3.SS1.p2.6.m6.4a"><mrow id="S3.SS1.p2.6.m6.4.5" xref="S3.SS1.p2.6.m6.4.5.cmml"><mi id="S3.SS1.p2.6.m6.4.5.2" xref="S3.SS1.p2.6.m6.4.5.2.cmml">k</mi><mo id="S3.SS1.p2.6.m6.4.5.1" xref="S3.SS1.p2.6.m6.4.5.1.cmml">=</mo><mrow id="S3.SS1.p2.6.m6.4.5.3.2" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.6.m6.4.5.3.2.1" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml">{</mo><mn id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">1</mn><mo id="S3.SS1.p2.6.m6.4.5.3.2.2" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p2.6.m6.2.2" xref="S3.SS1.p2.6.m6.2.2.cmml">2</mn><mo id="S3.SS1.p2.6.m6.4.5.3.2.3" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.6.m6.3.3" xref="S3.SS1.p2.6.m6.3.3.cmml">…</mi><mo id="S3.SS1.p2.6.m6.4.5.3.2.4" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml">,</mo><mi id="S3.SS1.p2.6.m6.4.4" xref="S3.SS1.p2.6.m6.4.4.cmml">K</mi><mo stretchy="false" id="S3.SS1.p2.6.m6.4.5.3.2.5" xref="S3.SS1.p2.6.m6.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.4b"><apply id="S3.SS1.p2.6.m6.4.5.cmml" xref="S3.SS1.p2.6.m6.4.5"><eq id="S3.SS1.p2.6.m6.4.5.1.cmml" xref="S3.SS1.p2.6.m6.4.5.1"></eq><ci id="S3.SS1.p2.6.m6.4.5.2.cmml" xref="S3.SS1.p2.6.m6.4.5.2">𝑘</ci><set id="S3.SS1.p2.6.m6.4.5.3.1.cmml" xref="S3.SS1.p2.6.m6.4.5.3.2"><cn type="integer" id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">1</cn><cn type="integer" id="S3.SS1.p2.6.m6.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2">2</cn><ci id="S3.SS1.p2.6.m6.3.3.cmml" xref="S3.SS1.p2.6.m6.3.3">…</ci><ci id="S3.SS1.p2.6.m6.4.4.cmml" xref="S3.SS1.p2.6.m6.4.4">𝐾</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.4c">k=\{1,2,...,K\}</annotation></semantics></math>, and <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\bm{f}^{ans}_{k}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msubsup id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.2.2.cmml">𝒇</mi><mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">k</mi><mrow id="S3.SS1.p2.7.m7.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.2.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.3.2" xref="S3.SS1.p2.7.m7.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.1.1.2.3.1" xref="S3.SS1.p2.7.m7.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.7.m7.1.1.2.3.3" xref="S3.SS1.p2.7.m7.1.1.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.1.1.2.3.1a" xref="S3.SS1.p2.7.m7.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.7.m7.1.1.2.3.4" xref="S3.SS1.p2.7.m7.1.1.2.3.4.cmml">s</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><apply id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.2">𝒇</ci><apply id="S3.SS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3"><times id="S3.SS1.p2.7.m7.1.1.2.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3.1"></times><ci id="S3.SS1.p2.7.m7.1.1.2.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3.2">𝑎</ci><ci id="S3.SS1.p2.7.m7.1.1.2.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3.3">𝑛</ci><ci id="S3.SS1.p2.7.m7.1.1.2.3.4.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3.4">𝑠</ci></apply></apply><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\bm{f}^{ans}_{k}</annotation></semantics></math> is the d-dimensional feature vectors for <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="k_{th}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">k</mi><mrow id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.1.1.3.1" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑘</ci><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><times id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.1"></times><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">𝑡</ci><ci id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">k_{th}</annotation></semantics></math> text word. The embeddings of the set of words are used jointly as the feature of the answer.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.5" class="ltx_p"><span id="S3.SS1.p3.5.1" class="ltx_text ltx_font_bold">Embedding of detected objects</span>.
Following M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite>, we run a pre-trained 2D object detector, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ren et al.(2015)Ren, He, Girshick, and Sun</a>]</cite> to localize <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">M</annotation></semantics></math> visual objects for each image. Two visual object features, including appearance and location features are extracted and then combined together to encode each detected object, <math id="S3.SS1.p3.2.m2.4" class="ltx_Math" alttext="\bm{F}^{obj}=\{\bm{f}^{obj}_{1},\ \bm{f}^{obj}_{2},\ ...,\bm{f}^{obj}_{m}\}" display="inline"><semantics id="S3.SS1.p3.2.m2.4a"><mrow id="S3.SS1.p3.2.m2.4.4" xref="S3.SS1.p3.2.m2.4.4.cmml"><msup id="S3.SS1.p3.2.m2.4.4.5" xref="S3.SS1.p3.2.m2.4.4.5.cmml"><mi id="S3.SS1.p3.2.m2.4.4.5.2" xref="S3.SS1.p3.2.m2.4.4.5.2.cmml">𝑭</mi><mrow id="S3.SS1.p3.2.m2.4.4.5.3" xref="S3.SS1.p3.2.m2.4.4.5.3.cmml"><mi id="S3.SS1.p3.2.m2.4.4.5.3.2" xref="S3.SS1.p3.2.m2.4.4.5.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.4.4.5.3.1" xref="S3.SS1.p3.2.m2.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.4.4.5.3.3" xref="S3.SS1.p3.2.m2.4.4.5.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.4.4.5.3.1a" xref="S3.SS1.p3.2.m2.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.4.4.5.3.4" xref="S3.SS1.p3.2.m2.4.4.5.3.4.cmml">j</mi></mrow></msup><mo id="S3.SS1.p3.2.m2.4.4.4" xref="S3.SS1.p3.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.4.4.3.3" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.4.4.3.3.4" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml">{</mo><msubsup id="S3.SS1.p3.2.m2.2.2.1.1.1" xref="S3.SS1.p3.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.2.2.1.1.1.2.2" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.2.cmml">𝒇</mi><mn id="S3.SS1.p3.2.m2.2.2.1.1.1.3" xref="S3.SS1.p3.2.m2.2.2.1.1.1.3.cmml">1</mn><mrow id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.cmml"><mi id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.2" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.3" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1a" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.4" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.4.cmml">j</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p3.2.m2.4.4.3.3.5" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p3.2.m2.3.3.2.2.2" xref="S3.SS1.p3.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.3.3.2.2.2.2.2" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.2.cmml">𝒇</mi><mn id="S3.SS1.p3.2.m2.3.3.2.2.2.3" xref="S3.SS1.p3.2.m2.3.3.2.2.2.3.cmml">2</mn><mrow id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.cmml"><mi id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.2" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.3" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1a" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.4" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.4.cmml">j</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p3.2.m2.4.4.3.3.6" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p3.2.m2.4.4.3.3.7" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p3.2.m2.4.4.3.3.3" xref="S3.SS1.p3.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS1.p3.2.m2.4.4.3.3.3.2.2" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.2.cmml">𝒇</mi><mi id="S3.SS1.p3.2.m2.4.4.3.3.3.3" xref="S3.SS1.p3.2.m2.4.4.3.3.3.3.cmml">m</mi><mrow id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.cmml"><mi id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.2" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.3" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1a" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.4" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.4.cmml">j</mi></mrow></msubsup><mo stretchy="false" id="S3.SS1.p3.2.m2.4.4.3.3.8" xref="S3.SS1.p3.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.4b"><apply id="S3.SS1.p3.2.m2.4.4.cmml" xref="S3.SS1.p3.2.m2.4.4"><eq id="S3.SS1.p3.2.m2.4.4.4.cmml" xref="S3.SS1.p3.2.m2.4.4.4"></eq><apply id="S3.SS1.p3.2.m2.4.4.5.cmml" xref="S3.SS1.p3.2.m2.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.4.4.5.1.cmml" xref="S3.SS1.p3.2.m2.4.4.5">superscript</csymbol><ci id="S3.SS1.p3.2.m2.4.4.5.2.cmml" xref="S3.SS1.p3.2.m2.4.4.5.2">𝑭</ci><apply id="S3.SS1.p3.2.m2.4.4.5.3.cmml" xref="S3.SS1.p3.2.m2.4.4.5.3"><times id="S3.SS1.p3.2.m2.4.4.5.3.1.cmml" xref="S3.SS1.p3.2.m2.4.4.5.3.1"></times><ci id="S3.SS1.p3.2.m2.4.4.5.3.2.cmml" xref="S3.SS1.p3.2.m2.4.4.5.3.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.4.4.5.3.3.cmml" xref="S3.SS1.p3.2.m2.4.4.5.3.3">𝑏</ci><ci id="S3.SS1.p3.2.m2.4.4.5.3.4.cmml" xref="S3.SS1.p3.2.m2.4.4.5.3.4">𝑗</ci></apply></apply><set id="S3.SS1.p3.2.m2.4.4.3.4.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3"><apply id="S3.SS1.p3.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1">subscript</csymbol><apply id="S3.SS1.p3.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.2.2.1.1.1.2.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.2">𝒇</ci><apply id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3"><times id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.1"></times><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.2.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.3.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.3">𝑏</ci><ci id="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.4.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.2.3.4">𝑗</ci></apply></apply><cn type="integer" id="S3.SS1.p3.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p3.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2">subscript</csymbol><apply id="S3.SS1.p3.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.3.3.2.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2">superscript</csymbol><ci id="S3.SS1.p3.2.m2.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.2">𝒇</ci><apply id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3"><times id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.1"></times><ci id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.2.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.3.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.3">𝑏</ci><ci id="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.4.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.2.3.4">𝑗</ci></apply></apply><cn type="integer" id="S3.SS1.p3.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">…</ci><apply id="S3.SS1.p3.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3">subscript</csymbol><apply id="S3.SS1.p3.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.4.4.3.3.3.2.1.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3">superscript</csymbol><ci id="S3.SS1.p3.2.m2.4.4.3.3.3.2.2.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.2">𝒇</ci><apply id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3"><times id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.1"></times><ci id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.2.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.2">𝑜</ci><ci id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.3.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.3">𝑏</ci><ci id="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.4.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.2.3.4">𝑗</ci></apply></apply><ci id="S3.SS1.p3.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS1.p3.2.m2.4.4.3.3.3.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.4c">\bm{F}^{obj}=\{\bm{f}^{obj}_{1},\ \bm{f}^{obj}_{2},\ ...,\bm{f}^{obj}_{m}\}</annotation></semantics></math>, where <math id="S3.SS1.p3.3.m3.4" class="ltx_Math" alttext="m=\{1,2,...,M\}" display="inline"><semantics id="S3.SS1.p3.3.m3.4a"><mrow id="S3.SS1.p3.3.m3.4.5" xref="S3.SS1.p3.3.m3.4.5.cmml"><mi id="S3.SS1.p3.3.m3.4.5.2" xref="S3.SS1.p3.3.m3.4.5.2.cmml">m</mi><mo id="S3.SS1.p3.3.m3.4.5.1" xref="S3.SS1.p3.3.m3.4.5.1.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.4.5.3.2" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.3.m3.4.5.3.2.1" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml">{</mo><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p3.3.m3.4.5.3.2.2" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml">2</mn><mo id="S3.SS1.p3.3.m3.4.5.3.2.3" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p3.3.m3.3.3" xref="S3.SS1.p3.3.m3.3.3.cmml">…</mi><mo id="S3.SS1.p3.3.m3.4.5.3.2.4" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml">,</mo><mi id="S3.SS1.p3.3.m3.4.4" xref="S3.SS1.p3.3.m3.4.4.cmml">M</mi><mo stretchy="false" id="S3.SS1.p3.3.m3.4.5.3.2.5" xref="S3.SS1.p3.3.m3.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.4b"><apply id="S3.SS1.p3.3.m3.4.5.cmml" xref="S3.SS1.p3.3.m3.4.5"><eq id="S3.SS1.p3.3.m3.4.5.1.cmml" xref="S3.SS1.p3.3.m3.4.5.1"></eq><ci id="S3.SS1.p3.3.m3.4.5.2.cmml" xref="S3.SS1.p3.3.m3.4.5.2">𝑚</ci><set id="S3.SS1.p3.3.m3.4.5.3.1.cmml" xref="S3.SS1.p3.3.m3.4.5.3.2"><cn type="integer" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">1</cn><cn type="integer" id="S3.SS1.p3.3.m3.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2">2</cn><ci id="S3.SS1.p3.3.m3.3.3.cmml" xref="S3.SS1.p3.3.m3.3.3">…</ci><ci id="S3.SS1.p3.3.m3.4.4.cmml" xref="S3.SS1.p3.3.m3.4.4">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.4c">m=\{1,2,...,M\}</annotation></semantics></math> and <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\bm{f}^{obj}_{m}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msubsup id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2.2" xref="S3.SS1.p3.4.m4.1.1.2.2.cmml">𝒇</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">m</mi><mrow id="S3.SS1.p3.4.m4.1.1.2.3" xref="S3.SS1.p3.4.m4.1.1.2.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2.3.2" xref="S3.SS1.p3.4.m4.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.1.2.3.1" xref="S3.SS1.p3.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.4.m4.1.1.2.3.3" xref="S3.SS1.p3.4.m4.1.1.2.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.1.2.3.1a" xref="S3.SS1.p3.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p3.4.m4.1.1.2.3.4" xref="S3.SS1.p3.4.m4.1.1.2.3.4.cmml">j</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><apply id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2.2">𝒇</ci><apply id="S3.SS1.p3.4.m4.1.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3"><times id="S3.SS1.p3.4.m4.1.1.2.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3.1"></times><ci id="S3.SS1.p3.4.m4.1.1.2.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3.2">𝑜</ci><ci id="S3.SS1.p3.4.m4.1.1.2.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3.3">𝑏</ci><ci id="S3.SS1.p3.4.m4.1.1.2.3.4.cmml" xref="S3.SS1.p3.4.m4.1.1.2.3.4">𝑗</ci></apply></apply><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\bm{f}^{obj}_{m}</annotation></semantics></math> is the projected d-dimensional feature vectors for <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="m_{th}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">m</mi><mrow id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml"><mi id="S3.SS1.p3.5.m5.1.1.3.2" xref="S3.SS1.p3.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.5.m5.1.1.3.1" xref="S3.SS1.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p3.5.m5.1.1.3.3" xref="S3.SS1.p3.5.m5.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">𝑚</ci><apply id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3"><times id="S3.SS1.p3.5.m5.1.1.3.1.cmml" xref="S3.SS1.p3.5.m5.1.1.3.1"></times><ci id="S3.SS1.p3.5.m5.1.1.3.2.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2">𝑡</ci><ci id="S3.SS1.p3.5.m5.1.1.3.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">m_{th}</annotation></semantics></math> object. Specifically, the feature vector output of the object detector (from the fc7 layer) is used to encode the appearance feature and the relative bounding box coordinates are employed as the location feature.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.5" class="ltx_p"><span id="S3.SS1.p4.5.1" class="ltx_text ltx_font_bold">Embedding of OCR tokens</span>. For the <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">N</annotation></semantics></math> OCR tokens extracted by an OCR system, we construct the embedding for each token containing both its visual and text feature. The visual feature extraction follows the strategy of the above visual object embedding. Additionally, FastText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Bojanowski et al.(2017)Bojanowski, Grave, Joulin, and
Mikolov</a>]</cite> and PHOC features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Almazán et al.(2014)Almazán, Gordo, Fornés, and
Valveny</a>]</cite> are extracted for each OCR token to represent its textual cues. A rich OCR representation is thus obtained, <math id="S3.SS1.p4.2.m2.4" class="ltx_Math" alttext="\bm{F}^{ocr}=\{\bm{f}^{ocr}_{1},\ \bm{f}^{ocr}_{2},\ ...,\bm{f}^{ocr}_{n}\}" display="inline"><semantics id="S3.SS1.p4.2.m2.4a"><mrow id="S3.SS1.p4.2.m2.4.4" xref="S3.SS1.p4.2.m2.4.4.cmml"><msup id="S3.SS1.p4.2.m2.4.4.5" xref="S3.SS1.p4.2.m2.4.4.5.cmml"><mi id="S3.SS1.p4.2.m2.4.4.5.2" xref="S3.SS1.p4.2.m2.4.4.5.2.cmml">𝑭</mi><mrow id="S3.SS1.p4.2.m2.4.4.5.3" xref="S3.SS1.p4.2.m2.4.4.5.3.cmml"><mi id="S3.SS1.p4.2.m2.4.4.5.3.2" xref="S3.SS1.p4.2.m2.4.4.5.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.4.4.5.3.1" xref="S3.SS1.p4.2.m2.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.4.4.5.3.3" xref="S3.SS1.p4.2.m2.4.4.5.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.4.4.5.3.1a" xref="S3.SS1.p4.2.m2.4.4.5.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.4.4.5.3.4" xref="S3.SS1.p4.2.m2.4.4.5.3.4.cmml">r</mi></mrow></msup><mo id="S3.SS1.p4.2.m2.4.4.4" xref="S3.SS1.p4.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS1.p4.2.m2.4.4.3.3" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS1.p4.2.m2.4.4.3.3.4" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml">{</mo><msubsup id="S3.SS1.p4.2.m2.2.2.1.1.1" xref="S3.SS1.p4.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.2.2" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.2.cmml">𝒇</mi><mn id="S3.SS1.p4.2.m2.2.2.1.1.1.3" xref="S3.SS1.p4.2.m2.2.2.1.1.1.3.cmml">1</mn><mrow id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.cmml"><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.2" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.3" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1a" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.4" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.4.cmml">r</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p4.2.m2.4.4.3.3.5" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p4.2.m2.3.3.2.2.2" xref="S3.SS1.p4.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.p4.2.m2.3.3.2.2.2.2.2" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.2.cmml">𝒇</mi><mn id="S3.SS1.p4.2.m2.3.3.2.2.2.3" xref="S3.SS1.p4.2.m2.3.3.2.2.2.3.cmml">2</mn><mrow id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.cmml"><mi id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.2" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.3" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1a" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.4" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.4.cmml">r</mi></mrow></msubsup><mo rspace="0.667em" id="S3.SS1.p4.2.m2.4.4.3.3.6" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p4.2.m2.4.4.3.3.7" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S3.SS1.p4.2.m2.4.4.3.3.3" xref="S3.SS1.p4.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS1.p4.2.m2.4.4.3.3.3.2.2" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.2.cmml">𝒇</mi><mi id="S3.SS1.p4.2.m2.4.4.3.3.3.3" xref="S3.SS1.p4.2.m2.4.4.3.3.3.3.cmml">n</mi><mrow id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.cmml"><mi id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.2" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.3" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1a" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.4" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.4.cmml">r</mi></mrow></msubsup><mo stretchy="false" id="S3.SS1.p4.2.m2.4.4.3.3.8" xref="S3.SS1.p4.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.4b"><apply id="S3.SS1.p4.2.m2.4.4.cmml" xref="S3.SS1.p4.2.m2.4.4"><eq id="S3.SS1.p4.2.m2.4.4.4.cmml" xref="S3.SS1.p4.2.m2.4.4.4"></eq><apply id="S3.SS1.p4.2.m2.4.4.5.cmml" xref="S3.SS1.p4.2.m2.4.4.5"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.4.4.5.1.cmml" xref="S3.SS1.p4.2.m2.4.4.5">superscript</csymbol><ci id="S3.SS1.p4.2.m2.4.4.5.2.cmml" xref="S3.SS1.p4.2.m2.4.4.5.2">𝑭</ci><apply id="S3.SS1.p4.2.m2.4.4.5.3.cmml" xref="S3.SS1.p4.2.m2.4.4.5.3"><times id="S3.SS1.p4.2.m2.4.4.5.3.1.cmml" xref="S3.SS1.p4.2.m2.4.4.5.3.1"></times><ci id="S3.SS1.p4.2.m2.4.4.5.3.2.cmml" xref="S3.SS1.p4.2.m2.4.4.5.3.2">𝑜</ci><ci id="S3.SS1.p4.2.m2.4.4.5.3.3.cmml" xref="S3.SS1.p4.2.m2.4.4.5.3.3">𝑐</ci><ci id="S3.SS1.p4.2.m2.4.4.5.3.4.cmml" xref="S3.SS1.p4.2.m2.4.4.5.3.4">𝑟</ci></apply></apply><set id="S3.SS1.p4.2.m2.4.4.3.4.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3"><apply id="S3.SS1.p4.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1">subscript</csymbol><apply id="S3.SS1.p4.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.2.2.1.1.1.2.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.2">𝒇</ci><apply id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3"><times id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.1"></times><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.2.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.2">𝑜</ci><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.3.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.3">𝑐</ci><ci id="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.4.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.2.3.4">𝑟</ci></apply></apply><cn type="integer" id="S3.SS1.p4.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p4.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.p4.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2">subscript</csymbol><apply id="S3.SS1.p4.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.3.3.2.2.2.2.1.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2">superscript</csymbol><ci id="S3.SS1.p4.2.m2.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.2">𝒇</ci><apply id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3"><times id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.1"></times><ci id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.2.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.2">𝑜</ci><ci id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.3.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.3">𝑐</ci><ci id="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.4.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.2.3.4">𝑟</ci></apply></apply><cn type="integer" id="S3.SS1.p4.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS1.p4.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">…</ci><apply id="S3.SS1.p4.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3">subscript</csymbol><apply id="S3.SS1.p4.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.4.4.3.3.3.2.1.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3">superscript</csymbol><ci id="S3.SS1.p4.2.m2.4.4.3.3.3.2.2.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.2">𝒇</ci><apply id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3"><times id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.1"></times><ci id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.2.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.2">𝑜</ci><ci id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.3.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.3">𝑐</ci><ci id="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.4.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.2.3.4">𝑟</ci></apply></apply><ci id="S3.SS1.p4.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS1.p4.2.m2.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.4c">\bm{F}^{ocr}=\{\bm{f}^{ocr}_{1},\ \bm{f}^{ocr}_{2},\ ...,\bm{f}^{ocr}_{n}\}</annotation></semantics></math>, where <math id="S3.SS1.p4.3.m3.4" class="ltx_Math" alttext="n=\{1,2,...,N\}" display="inline"><semantics id="S3.SS1.p4.3.m3.4a"><mrow id="S3.SS1.p4.3.m3.4.5" xref="S3.SS1.p4.3.m3.4.5.cmml"><mi id="S3.SS1.p4.3.m3.4.5.2" xref="S3.SS1.p4.3.m3.4.5.2.cmml">n</mi><mo id="S3.SS1.p4.3.m3.4.5.1" xref="S3.SS1.p4.3.m3.4.5.1.cmml">=</mo><mrow id="S3.SS1.p4.3.m3.4.5.3.2" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m3.4.5.3.2.1" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml">{</mo><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p4.3.m3.4.5.3.2.2" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml">,</mo><mn id="S3.SS1.p4.3.m3.2.2" xref="S3.SS1.p4.3.m3.2.2.cmml">2</mn><mo id="S3.SS1.p4.3.m3.4.5.3.2.3" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p4.3.m3.3.3" xref="S3.SS1.p4.3.m3.3.3.cmml">…</mi><mo id="S3.SS1.p4.3.m3.4.5.3.2.4" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml">,</mo><mi id="S3.SS1.p4.3.m3.4.4" xref="S3.SS1.p4.3.m3.4.4.cmml">N</mi><mo stretchy="false" id="S3.SS1.p4.3.m3.4.5.3.2.5" xref="S3.SS1.p4.3.m3.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.4b"><apply id="S3.SS1.p4.3.m3.4.5.cmml" xref="S3.SS1.p4.3.m3.4.5"><eq id="S3.SS1.p4.3.m3.4.5.1.cmml" xref="S3.SS1.p4.3.m3.4.5.1"></eq><ci id="S3.SS1.p4.3.m3.4.5.2.cmml" xref="S3.SS1.p4.3.m3.4.5.2">𝑛</ci><set id="S3.SS1.p4.3.m3.4.5.3.1.cmml" xref="S3.SS1.p4.3.m3.4.5.3.2"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">1</cn><cn type="integer" id="S3.SS1.p4.3.m3.2.2.cmml" xref="S3.SS1.p4.3.m3.2.2">2</cn><ci id="S3.SS1.p4.3.m3.3.3.cmml" xref="S3.SS1.p4.3.m3.3.3">…</ci><ci id="S3.SS1.p4.3.m3.4.4.cmml" xref="S3.SS1.p4.3.m3.4.4">𝑁</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.4c">n=\{1,2,...,N\}</annotation></semantics></math> and <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\bm{f}^{ocr}_{n}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msubsup id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2.2" xref="S3.SS1.p4.4.m4.1.1.2.2.cmml">𝒇</mi><mi id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">n</mi><mrow id="S3.SS1.p4.4.m4.1.1.2.3" xref="S3.SS1.p4.4.m4.1.1.2.3.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2.3.2" xref="S3.SS1.p4.4.m4.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m4.1.1.2.3.1" xref="S3.SS1.p4.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.4.m4.1.1.2.3.3" xref="S3.SS1.p4.4.m4.1.1.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m4.1.1.2.3.1a" xref="S3.SS1.p4.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p4.4.m4.1.1.2.3.4" xref="S3.SS1.p4.4.m4.1.1.2.3.4.cmml">r</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><apply id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.2.1.cmml" xref="S3.SS1.p4.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2.2">𝒇</ci><apply id="S3.SS1.p4.4.m4.1.1.2.3.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3"><times id="S3.SS1.p4.4.m4.1.1.2.3.1.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3.1"></times><ci id="S3.SS1.p4.4.m4.1.1.2.3.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3.2">𝑜</ci><ci id="S3.SS1.p4.4.m4.1.1.2.3.3.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3.3">𝑐</ci><ci id="S3.SS1.p4.4.m4.1.1.2.3.4.cmml" xref="S3.SS1.p4.4.m4.1.1.2.3.4">𝑟</ci></apply></apply><ci id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\bm{f}^{ocr}_{n}</annotation></semantics></math> is the projected d-dimensional feature vectors for <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="n_{th}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">n</mi><mrow id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml"><mi id="S3.SS1.p4.5.m5.1.1.3.2" xref="S3.SS1.p4.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.5.m5.1.1.3.1" xref="S3.SS1.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p4.5.m5.1.1.3.3" xref="S3.SS1.p4.5.m5.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝑛</ci><apply id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3"><times id="S3.SS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.p4.5.m5.1.1.3.1"></times><ci id="S3.SS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.p4.5.m5.1.1.3.2">𝑡</ci><ci id="S3.SS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">n_{th}</annotation></semantics></math> OCR token.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-modality Fusion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">Once the feature embedding representation from individual modality, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{F}^{ans}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">s</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑭</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">𝑎</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝑛</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\bm{F}^{ans}</annotation></semantics></math>, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\bm{F}^{obj}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1a" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.3.4.cmml">j</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑭</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝑜</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝑏</ci><ci id="S3.SS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.4">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\bm{F}^{obj}</annotation></semantics></math> and <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\bm{F}^{ocr}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msup id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1a" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.4" xref="S3.SS2.p1.3.m3.1.1.3.4.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑭</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><times id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">𝑜</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝑐</ci><ci id="S3.SS2.p1.3.m3.1.1.3.4.cmml" xref="S3.SS2.p1.3.m3.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\bm{F}^{ocr}</annotation></semantics></math> are generated, they are able to dynamically attend to each other from a stack of <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">L</annotation></semantics></math> transformer layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
Kaiser, and Polosukhin</a>]</cite> as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The input sequence to the multi-modal transformer is <math id="S3.SS2.p1.5.m5.3" class="ltx_Math" alttext="\bm{F}=\{\bm{F}^{ans},\bm{F}^{obj},\bm{F}^{ocr}\}" display="inline"><semantics id="S3.SS2.p1.5.m5.3a"><mrow id="S3.SS2.p1.5.m5.3.3" xref="S3.SS2.p1.5.m5.3.3.cmml"><mi id="S3.SS2.p1.5.m5.3.3.5" xref="S3.SS2.p1.5.m5.3.3.5.cmml">𝑭</mi><mo id="S3.SS2.p1.5.m5.3.3.4" xref="S3.SS2.p1.5.m5.3.3.4.cmml">=</mo><mrow id="S3.SS2.p1.5.m5.3.3.3.3" xref="S3.SS2.p1.5.m5.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.3.3.3.3.4" xref="S3.SS2.p1.5.m5.3.3.3.4.cmml">{</mo><msup id="S3.SS2.p1.5.m5.1.1.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.2" xref="S3.SS2.p1.5.m5.1.1.1.1.1.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.5.m5.1.1.1.1.1.3" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.1.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.1.1.1.1.3.1a" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.1.1.1.1.1.3.4" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.4.cmml">s</mi></mrow></msup><mo id="S3.SS2.p1.5.m5.3.3.3.3.5" xref="S3.SS2.p1.5.m5.3.3.3.4.cmml">,</mo><msup id="S3.SS2.p1.5.m5.2.2.2.2.2" xref="S3.SS2.p1.5.m5.2.2.2.2.2.cmml"><mi id="S3.SS2.p1.5.m5.2.2.2.2.2.2" xref="S3.SS2.p1.5.m5.2.2.2.2.2.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.5.m5.2.2.2.2.2.3" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.cmml"><mi id="S3.SS2.p1.5.m5.2.2.2.2.2.3.2" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.2.2.2.2.2.3.1" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.2.2.2.2.2.3.3" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.2.2.2.2.2.3.1a" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.2.2.2.2.2.3.4" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.4.cmml">j</mi></mrow></msup><mo id="S3.SS2.p1.5.m5.3.3.3.3.6" xref="S3.SS2.p1.5.m5.3.3.3.4.cmml">,</mo><msup id="S3.SS2.p1.5.m5.3.3.3.3.3" xref="S3.SS2.p1.5.m5.3.3.3.3.3.cmml"><mi id="S3.SS2.p1.5.m5.3.3.3.3.3.2" xref="S3.SS2.p1.5.m5.3.3.3.3.3.2.cmml">𝑭</mi><mrow id="S3.SS2.p1.5.m5.3.3.3.3.3.3" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.cmml"><mi id="S3.SS2.p1.5.m5.3.3.3.3.3.3.2" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.3.3.3.3.3.3.1" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.3.3.3.3.3.3.3" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.3.3.3.3.3.3.1a" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.1.cmml">​</mo><mi id="S3.SS2.p1.5.m5.3.3.3.3.3.3.4" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.4.cmml">r</mi></mrow></msup><mo stretchy="false" id="S3.SS2.p1.5.m5.3.3.3.3.7" xref="S3.SS2.p1.5.m5.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.3b"><apply id="S3.SS2.p1.5.m5.3.3.cmml" xref="S3.SS2.p1.5.m5.3.3"><eq id="S3.SS2.p1.5.m5.3.3.4.cmml" xref="S3.SS2.p1.5.m5.3.3.4"></eq><ci id="S3.SS2.p1.5.m5.3.3.5.cmml" xref="S3.SS2.p1.5.m5.3.3.5">𝑭</ci><set id="S3.SS2.p1.5.m5.3.3.3.4.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3"><apply id="S3.SS2.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.2">𝑭</ci><apply id="S3.SS2.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3"><times id="S3.SS2.p1.5.m5.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.2">𝑎</ci><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.3">𝑛</ci><ci id="S3.SS2.p1.5.m5.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1.1.3.4">𝑠</ci></apply></apply><apply id="S3.SS2.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2">superscript</csymbol><ci id="S3.SS2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.2">𝑭</ci><apply id="S3.SS2.p1.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3"><times id="S3.SS2.p1.5.m5.2.2.2.2.2.3.1.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.1"></times><ci id="S3.SS2.p1.5.m5.2.2.2.2.2.3.2.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.2">𝑜</ci><ci id="S3.SS2.p1.5.m5.2.2.2.2.2.3.3.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.3">𝑏</ci><ci id="S3.SS2.p1.5.m5.2.2.2.2.2.3.4.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2.2.3.4">𝑗</ci></apply></apply><apply id="S3.SS2.p1.5.m5.3.3.3.3.3.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.3.3.3.3.3.1.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3">superscript</csymbol><ci id="S3.SS2.p1.5.m5.3.3.3.3.3.2.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.2">𝑭</ci><apply id="S3.SS2.p1.5.m5.3.3.3.3.3.3.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3"><times id="S3.SS2.p1.5.m5.3.3.3.3.3.3.1.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.1"></times><ci id="S3.SS2.p1.5.m5.3.3.3.3.3.3.2.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.2">𝑜</ci><ci id="S3.SS2.p1.5.m5.3.3.3.3.3.3.3.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.3">𝑐</ci><ci id="S3.SS2.p1.5.m5.3.3.3.3.3.3.4.cmml" xref="S3.SS2.p1.5.m5.3.3.3.3.3.3.4">𝑟</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.3c">\bm{F}=\{\bm{F}^{ans},\bm{F}^{obj},\bm{F}^{ocr}\}</annotation></semantics></math>. The multi-modal transformer leverages feature embeddings from different modalities and accordingly models interaction among them through the multi-head attention mechanism. From the output of the multi-modal transformer, we extract a sequence of d-dimensional feature vectors for each modality, which is an enriched feature from a joint semantic embedding space.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Text-aware Visual Question Prediction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">With the enriched embedding from the multi-modal transformer, the multi-step decoding module predicts a question to the input answer and iteratively generates the question word by word. At each iterative decoding step, we feed in an embedding of previously predicted words, and then the next output word could be either selected from the fixed frequent word vocabulary or from the extracted OCR tokens. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>, two special tokens <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="&lt;begin&gt;" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1a" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.4" xref="S3.SS3.p1.1.m1.1.1.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1b" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.5" xref="S3.SS3.p1.1.m1.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1.1.1c" xref="S3.SS3.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.1.m1.1.1.1.1.6" xref="S3.SS3.p1.1.m1.1.1.1.1.6.cmml">n</mi></mrow><mo fence="true" lspace="0em" id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.2">expectation</csymbol><apply id="S3.SS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2">𝑏</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.3">𝑒</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.4">𝑔</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.5">𝑖</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.6.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">&lt;begin&gt;</annotation></semantics></math> and <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="&lt;end&gt;" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS3.p1.2.m2.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS3.p1.2.m2.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.1.1.1.1a" xref="S3.SS3.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.2.m2.1.1.1.1.4" xref="S3.SS3.p1.2.m2.1.1.1.1.4.cmml">d</mi></mrow><mo fence="true" lspace="0em" id="S3.SS3.p1.2.m2.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.2.m2.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.2">expectation</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1"><times id="S3.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.2">𝑒</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3">𝑛</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.4.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.4">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">&lt;end&gt;</annotation></semantics></math> are appended to the word vocabulary, where <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="&lt;begin&gt;" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS3.p1.3.m3.1.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.1.1.2" xref="S3.SS3.p1.3.m3.1.1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.1.1.3" xref="S3.SS3.p1.3.m3.1.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1.1.1a" xref="S3.SS3.p1.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.1.1.4" xref="S3.SS3.p1.3.m3.1.1.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1.1.1b" xref="S3.SS3.p1.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.1.1.5" xref="S3.SS3.p1.3.m3.1.1.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1.1.1c" xref="S3.SS3.p1.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.1.1.6" xref="S3.SS3.p1.3.m3.1.1.1.1.6.cmml">n</mi></mrow><mo fence="true" lspace="0em" id="S3.SS3.p1.3.m3.1.1.1.3" xref="S3.SS3.p1.3.m3.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.2">expectation</csymbol><apply id="S3.SS3.p1.3.m3.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.1"></times><ci id="S3.SS3.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.2">𝑏</ci><ci id="S3.SS3.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.3">𝑒</ci><ci id="S3.SS3.p1.3.m3.1.1.1.1.4.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.4">𝑔</ci><ci id="S3.SS3.p1.3.m3.1.1.1.1.5.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.5">𝑖</ci><ci id="S3.SS3.p1.3.m3.1.1.1.1.6.cmml" xref="S3.SS3.p1.3.m3.1.1.1.1.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">&lt;begin&gt;</annotation></semantics></math> is used as the input to the first decoding step and <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="&lt;end&gt;" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.SS3.p1.4.m4.1.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.1.cmml">&lt;</mo><mrow id="S3.SS3.p1.4.m4.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.1.1.2" xref="S3.SS3.p1.4.m4.1.1.1.1.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.4.m4.1.1.1.1.3" xref="S3.SS3.p1.4.m4.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.1.1.1a" xref="S3.SS3.p1.4.m4.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p1.4.m4.1.1.1.1.4" xref="S3.SS3.p1.4.m4.1.1.1.1.4.cmml">d</mi></mrow><mo fence="true" lspace="0em" id="S3.SS3.p1.4.m4.1.1.1.3" xref="S3.SS3.p1.4.m4.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.2">expectation</csymbol><apply id="S3.SS3.p1.4.m4.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1"><times id="S3.SS3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.1"></times><ci id="S3.SS3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.2">𝑒</ci><ci id="S3.SS3.p1.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.3">𝑛</ci><ci id="S3.SS3.p1.4.m4.1.1.1.1.4.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.4">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">&lt;end&gt;</annotation></semantics></math> indicates the end of the decoding process. Alternatively, the decoding process ends when the maximum number of steps <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">T</annotation></semantics></math> is reached.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">During training, our TAG is supervised with the binary cross-entropy loss applied using the originally annotated QA pairs and adapts to generate novel QA pairs during generation.
During the QA pairs generation process, we pass an input answer, each of which is selected from the extracted OCR tokens, into the TAG module and generate the corresponding question accordingly. In this way, the generated QA pairs cover a diverse set of scene text which was not directly exploited in the original annotation set. For answer selection, we perform a simple yet efficient strategy that is feeding the OCR token with the largest bounding box as the answer candidate to the proposed TAG. The intuition behind this design is that the scene text with the largest bounding box region is likely to encode semantically meaningful information for scene text-based understanding and reasoning. Also, scene text with a larger font size has a higher chance to be detected correctly without recognition error in general. As we illustrate in our experiments, our simple design facilitates a better understanding of the visual content and provides promising Text-VQA performance. Note that, more high-quality QA pairs could be continuously augmented with a more sophisticated answer-candidate selection strategy. We leave this direction as future work.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate TAG both qualitatively and quantitatively on the TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite> and the ST-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>]</cite> datasets. We first present a brief overview of the datasets and implementation details. Then, we empirically validate the effectiveness of our proposed method by comparing it with the existing Text-VQA approaches. Our framework clearly outperforms previous work by a significant margin on both datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">TextVQA dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite> is a widely used benchmark for the Text-VQA task. It consists of 28,408 images sourced from the Open Images dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Kuznetsova et al.(2020)Kuznetsova, Rom, Alldrin, Uijlings, Krasin,
Pont-Tuset, Kamali, Popov, Malloci, Kolesnikov, et al.</a>]</cite>, with human-annotated questions that require reasoning over text in the images. We follow the standard split on the training, validation and test sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>. For each question, the answer prediction is evaluated based on the soft-voting accuracy of 10 human-annotated answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Goyal et al.(2017)Goyal, Khot, Summers-Stay, Batra, and
Parikh</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">ST-VQA dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>]</cite> is another popular dataset for the Text-VQA task. It contains 23,038 images from multiple sources including ICDAR 2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Karatzas et al.(2013)Karatzas, Shafait, Uchida, Iwamura, i Bigorda,
Mestre, Mas, Mota, Almazan, and De Las Heras</a>]</cite>, ICDAR 2015 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Karatzas et al.(2015)Karatzas, Gomez-Bigorda, Nicolaou, Ghosh,
Bagdanov, Iwamura, Matas, Neumann, Chandrasekhar, Lu,
et al.</a>]</cite>, ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Deng et al.(2009)Deng, Dong, Socher, Li, Li, and
Fei-Fei</a>]</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Gurari et al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
Bigham</a>]</cite>, IIIT STR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Mishra et al.(2013)Mishra, Alahari, and Jawahar</a>]</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Krishna et al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
Kalantidis, Li, Shamma, et al.</a>]</cite>, and COCO-Text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Veit et al.(2016)Veit, Matera, Neumann, Matas, and
Belongie</a>]</cite>.
The standard evaluation protocol on the ST-VQA dataset consists of accuracy and Average Normalized Levenshtein Similarity (ANLS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We use PyTorch to implement our TAG<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_tag ltx_tag_note">†</span>Our implementation is built upon the codebase: https://github.com/microsoft/TAP.</span></span></span> that is used to augment the initially labeled data. The augmented dataset is used to improve two recent Text-VQA models, M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite> and TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>. M4C<sup id="S4.SS2.p1.1.1" class="ltx_sup">†</sup> is a variant version <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> of M4C, where the detected object labels and scene text tokens are also included in the text encoder, which further improves the performance.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.9.10.1" class="ltx_tr">
<th id="S4.T1.9.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="S4.T1.9.10.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OCR system</th>
<th id="S4.T1.9.10.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Extra Data</th>
<th id="S4.T1.9.10.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Val Acc.</th>
<th id="S4.T1.9.10.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Test Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan</a>]</cite>
</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Rosetta-en</td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><times id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">40.39</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">40.96</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LaAP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>]</cite>
</td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Rosetta-en</td>
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mo id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><times id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.68</td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.54</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>]</cite>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SBD-Trans OCR</td>
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mo id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><times id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">43.74</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.29</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SSBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>]</cite>
</td>
<td id="S4.T1.4.4.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SBD-Trans OCR</td>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mo id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><times id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">43.95</td>
<td id="S4.T1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.72</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite>
</td>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mo id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><times id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.79</td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.65</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">M4C<sup id="S4.T1.6.6.2.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite>
</td>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.6.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.6.6.1.m1.1a"><mo id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><times id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.50</td>
<td id="S4.T1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.75</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.7.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.7.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">M4C<sup id="S4.T1.7.7.2.1.1" class="ltx_sup">†</sup> + TAG</span></td>
<td id="S4.T1.7.7.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">Microsoft-OCR</span></td>
<td id="S4.T1.7.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" style="background-color:#E6E6E6;" alttext="\times" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><times id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.7.7.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">45.68</span></td>
<td id="S4.T1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.7.7.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">45.96</span></td>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<td id="S4.T1.8.8.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>
</td>
<td id="S4.T1.8.8.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.8.8.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.8.8.1.m1.1a"><mo id="S4.T1.8.8.1.m1.1.1" xref="S4.T1.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.m1.1b"><times id="S4.T1.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.91</td>
<td id="S4.T1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.71</td>
</tr>
<tr id="S4.T1.9.9" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.9.9.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.9.2.1" class="ltx_text" style="background-color:#E6E6E6;">TAP + TAG</span></td>
<td id="S4.T1.9.9.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.9.3.1" class="ltx_text" style="background-color:#E6E6E6;">Microsoft-OCR</span></td>
<td id="S4.T1.9.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T1.9.9.1.m1.1" class="ltx_Math" style="background-color:#E6E6E6;" alttext="\times" display="inline"><semantics id="S4.T1.9.9.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><times id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.9.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">52.54</span></td>
<td id="S4.T1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.9.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">52.57</span></td>
</tr>
<tr id="S4.T1.9.11.1" class="ltx_tr">
<td id="S4.T1.9.11.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">LaAP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>]</cite>
</td>
<td id="S4.T1.9.11.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Rosetta-en</td>
<td id="S4.T1.9.11.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.11.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">41.02</td>
<td id="S4.T1.9.11.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">41.41</td>
</tr>
<tr id="S4.T1.9.12.2" class="ltx_tr">
<td id="S4.T1.9.12.2.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SA-M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>]</cite>
</td>
<td id="S4.T1.9.12.2.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Google-OCR</td>
<td id="S4.T1.9.12.2.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.12.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.40</td>
<td id="S4.T1.9.12.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.60</td>
</tr>
<tr id="S4.T1.9.13.3" class="ltx_tr">
<td id="S4.T1.9.13.3.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>]</cite>
</td>
<td id="S4.T1.9.13.3.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SBD-Trans OCR</td>
<td id="S4.T1.9.13.3.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.13.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.58</td>
<td id="S4.T1.9.13.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.51</td>
</tr>
<tr id="S4.T1.9.14.4" class="ltx_tr">
<td id="S4.T1.9.14.4.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SSBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>]</cite>
</td>
<td id="S4.T1.9.14.4.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SBD-Trans OCR</td>
<td id="S4.T1.9.14.4.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.14.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.53</td>
<td id="S4.T1.9.14.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.66</td>
</tr>
<tr id="S4.T1.9.15.5" class="ltx_tr">
<td id="S4.T1.9.15.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite>
</td>
<td id="S4.T1.9.15.5.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.9.15.5.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.15.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.53</td>
<td id="S4.T1.9.15.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.08</td>
</tr>
<tr id="S4.T1.9.16.6" class="ltx_tr">
<td id="S4.T1.9.16.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">M4C<sup id="S4.T1.9.16.6.1.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite>
</td>
<td id="S4.T1.9.16.6.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.9.16.6.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.16.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.22</td>
<td id="S4.T1.9.16.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S4.T1.9.17.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.9.17.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.17.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">M4C<sup id="S4.T1.9.17.7.1.1.1" class="ltx_sup">†</sup> + TAG</span></td>
<td id="S4.T1.9.17.7.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.17.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">Microsoft-OCR</span></td>
<td id="S4.T1.9.17.7.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.17.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">ST-VQA</span></td>
<td id="S4.T1.9.17.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.17.7.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">46.33</span></td>
<td id="S4.T1.9.17.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.17.7.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">46.38</span></td>
</tr>
<tr id="S4.T1.9.18.8" class="ltx_tr">
<td id="S4.T1.9.18.8.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>
</td>
<td id="S4.T1.9.18.8.2" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Microsoft-OCR</td>
<td id="S4.T1.9.18.8.3" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ST-VQA</td>
<td id="S4.T1.9.18.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.57</td>
<td id="S4.T1.9.18.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.71</td>
</tr>
<tr id="S4.T1.9.19.9" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.9.19.9.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.19.9.1.1" class="ltx_text" style="background-color:#E6E6E6;">TAP + TAG</span></td>
<td id="S4.T1.9.19.9.2" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.19.9.2.1" class="ltx_text" style="background-color:#E6E6E6;">Microsoft-OCR</span></td>
<td id="S4.T1.9.19.9.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.19.9.3.1" class="ltx_text" style="background-color:#E6E6E6;">ST-VQA</span></td>
<td id="S4.T1.9.19.9.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.19.9.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">53.63</span></td>
<td id="S4.T1.9.19.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T1.9.19.9.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">53.69</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.12.1" class="ltx_text ltx_font_bold">TAG’s outperformance on the TextVQA dataset when trained on original and augmented dataset under two settings.</span> Note that M4C<sup id="S4.T1.13.2" class="ltx_sup">†</sup> is the improved version from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2208.01813/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="117" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We visualize the examples of the generated QA pairs (bottom in orange) by TAG module compared with the original annotated QA pairs (top in blue) on the TextVQA training set. "Q" and "A" refer to question and answer, respectively. Best viewed in color.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.10" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.10.11.1" class="ltx_tr">
<th id="S4.T2.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Method</th>
<th id="S4.T2.10.11.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Extra Data</th>
<td id="S4.T2.10.11.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Val Acc.</td>
<td id="S4.T2.10.11.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Val ANLS</td>
<td id="S4.T2.10.11.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Test ANLS</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">CRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan</a>]</cite>
</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><times id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.483</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">LaAP-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Han et al.(2020)Han, Huang, and Han</a>]</cite>
</th>
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><times id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.74</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.497</td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.485</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<th id="S4.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">SMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu</a>]</cite>
</th>
<th id="S4.T2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><times id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.486</td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<th id="S4.T2.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">SA-M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal</a>]</cite>
</th>
<th id="S4.T2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mo id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><times id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.23</td>
<td id="S4.T2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.512</td>
<td id="S4.T2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.504</td>
</tr>
<tr id="S4.T2.5.5" class="ltx_tr">
<th id="S4.T2.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">SSBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>]</cite>
</th>
<th id="S4.T2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.5.5.1.m1.1a"><mo id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><times id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.509</td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<th id="S4.T2.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite>
</th>
<th id="S4.T2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.6.6.1.m1.1a"><mo id="S4.T2.6.6.1.m1.1.1" xref="S4.T2.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.m1.1b"><times id="S4.T2.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">44.10</td>
<td id="S4.T2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.535</td>
<td id="S4.T2.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.522</td>
</tr>
<tr id="S4.T2.7.7" class="ltx_tr">
<th id="S4.T2.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">M4C<sup id="S4.T2.7.7.2.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite>
</th>
<th id="S4.T2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.7.7.1.m1.1a"><mo id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><times id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.28</td>
<td id="S4.T2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.517</td>
<td id="S4.T2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.517</td>
</tr>
<tr id="S4.T2.8.8" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.8.8.2.1" class="ltx_text" style="background-color:#E6E6E6;">M4C<sup id="S4.T2.8.8.2.1.1" class="ltx_sup">†</sup> + TAG</span></th>
<th id="S4.T2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.8.8.1.m1.1" class="ltx_Math" style="background-color:#E6E6E6;" alttext="\times" display="inline"><semantics id="S4.T2.8.8.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T2.8.8.1.m1.1.1" xref="S4.T2.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.1.m1.1b"><times id="S4.T2.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.8.8.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">44.52</span></td>
<td id="S4.T2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.8.8.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.540</span></td>
<td id="S4.T2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.8.8.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.529</span></td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<th id="S4.T2.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>
</th>
<th id="S4.T2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.9.9.1.m1.1a"><mo id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><times id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">45.29</td>
<td id="S4.T2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.551</td>
<td id="S4.T2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.543</td>
</tr>
<tr id="S4.T2.10.10" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.10.2.1" class="ltx_text" style="background-color:#E6E6E6;">TAP + TAG</span></th>
<th id="S4.T2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S4.T2.10.10.1.m1.1" class="ltx_Math" style="background-color:#E6E6E6;" alttext="\times" display="inline"><semantics id="S4.T2.10.10.1.m1.1a"><mo mathbackground="#E6E6E6" id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><times id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">\times</annotation></semantics></math></th>
<td id="S4.T2.10.10.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.10.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">50.18</span></td>
<td id="S4.T2.10.10.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.10.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.595</span></td>
<td id="S4.T2.10.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.10.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.586</span></td>
</tr>
<tr id="S4.T2.10.12.2" class="ltx_tr">
<th id="S4.T2.10.12.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SSBaseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Zhu et al.(2021)Zhu, Gao, Wang, and Wu</a>]</cite>
</th>
<th id="S4.T2.10.12.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">TextVQA</th>
<td id="S4.T2.10.12.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.10.12.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S4.T2.10.12.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.550</td>
</tr>
<tr id="S4.T2.10.13.3" class="ltx_tr">
<th id="S4.T2.10.13.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite>
</th>
<th id="S4.T2.10.13.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">TextVQA</th>
<td id="S4.T2.10.13.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">48.63</td>
<td id="S4.T2.10.13.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.581</td>
<td id="S4.T2.10.13.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.579</td>
</tr>
<tr id="S4.T2.10.14.4" class="ltx_tr">
<th id="S4.T2.10.14.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">M4C<sup id="S4.T2.10.14.4.1.1" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite>
</th>
<th id="S4.T2.10.14.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">TextVQA</th>
<td id="S4.T2.10.14.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">46.60</td>
<td id="S4.T2.10.14.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.560</td>
<td id="S4.T2.10.14.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.552</td>
</tr>
<tr id="S4.T2.10.15.5" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.10.15.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.15.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">M4C<sup id="S4.T2.10.15.5.1.1.1" class="ltx_sup">†</sup> + TAG</span></th>
<th id="S4.T2.10.15.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.15.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">TextVQA</span></th>
<td id="S4.T2.10.15.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.15.5.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">48.69</span></td>
<td id="S4.T2.10.15.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.15.5.4.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.579</span></td>
<td id="S4.T2.10.15.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.15.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.571</span></td>
</tr>
<tr id="S4.T2.10.16.6" class="ltx_tr">
<th id="S4.T2.10.16.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">TAP<sup id="S4.T2.10.16.6.1.1" class="ltx_sup">†</sup><sup id="S4.T2.10.16.6.1.2" class="ltx_sup">†</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>
</th>
<th id="S4.T2.10.16.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;">TextVQA, TextCaps, OCR-CC</th>
<td id="S4.T2.10.16.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50.83</td>
<td id="S4.T2.10.16.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.598</td>
<td id="S4.T2.10.16.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.597</td>
</tr>
<tr id="S4.T2.10.17.7" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S4.T2.10.17.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.17.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">TAP + TAG</span></th>
<th id="S4.T2.10.17.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.17.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">TextVQA</span></th>
<td id="S4.T2.10.17.7.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.17.7.3.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">53.53</span></td>
<td id="S4.T2.10.17.7.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.17.7.4.1" class="ltx_text" style="background-color:#E6E6E6;"> <span id="S4.T2.10.17.7.4.1.1" class="ltx_text ltx_font_bold">0.620</span></span></td>
<td id="S4.T2.10.17.7.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T2.10.17.7.5.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">0.602</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.14.1" class="ltx_text ltx_font_bold">Our framework outperforms prior work on the ST-VQA dataset.</span> Note that M4C<sup id="S4.T2.15.2" class="ltx_sup">†</sup> is the improved version from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>. Specifically, our model with TextVQA outperforms the SOTA approach TAP<sup id="S4.T2.16.3" class="ltx_sup">††</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> that is pre-trained with extra large-scale data from external TextCaps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx40" title="" class="ltx_ref">Sidorov et al.(2020)Sidorov, Hu, Rohrbach, and
Singh</a>]</cite> and OCR-CC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> datasets.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.6" class="ltx_p"><span id="S4.SS2.p2.6.1" class="ltx_text ltx_font_bold">TAG.</span>
We project the multi-modality feature embedding to be <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">d</annotation></semantics></math> = 768 channels. We extract the embedding of extended answer words using the same trainable structure as <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="BERT_{BASE}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1a" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.4" xref="S4.SS2.p2.2.m2.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1b" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS2.p2.2.m2.1.1.5" xref="S4.SS2.p2.2.m2.1.1.5.cmml"><mi id="S4.SS2.p2.2.m2.1.1.5.2" xref="S4.SS2.p2.2.m2.1.1.5.2.cmml">T</mi><mrow id="S4.SS2.p2.2.m2.1.1.5.3" xref="S4.SS2.p2.2.m2.1.1.5.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.5.3.2" xref="S4.SS2.p2.2.m2.1.1.5.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.5.3.1" xref="S4.SS2.p2.2.m2.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.5.3.3" xref="S4.SS2.p2.2.m2.1.1.5.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.5.3.1a" xref="S4.SS2.p2.2.m2.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.5.3.4" xref="S4.SS2.p2.2.m2.1.1.5.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.5.3.1b" xref="S4.SS2.p2.2.m2.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.5.3.5" xref="S4.SS2.p2.2.m2.1.1.5.3.5.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝐵</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">𝐸</ci><ci id="S4.SS2.p2.2.m2.1.1.4.cmml" xref="S4.SS2.p2.2.m2.1.1.4">𝑅</ci><apply id="S4.SS2.p2.2.m2.1.1.5.cmml" xref="S4.SS2.p2.2.m2.1.1.5"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.5.1.cmml" xref="S4.SS2.p2.2.m2.1.1.5">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.5.2.cmml" xref="S4.SS2.p2.2.m2.1.1.5.2">𝑇</ci><apply id="S4.SS2.p2.2.m2.1.1.5.3.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3"><times id="S4.SS2.p2.2.m2.1.1.5.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3.1"></times><ci id="S4.SS2.p2.2.m2.1.1.5.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3.2">𝐵</ci><ci id="S4.SS2.p2.2.m2.1.1.5.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3.3">𝐴</ci><ci id="S4.SS2.p2.2.m2.1.1.5.3.4.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3.4">𝑆</ci><ci id="S4.SS2.p2.2.m2.1.1.5.3.5.cmml" xref="S4.SS2.p2.2.m2.1.1.5.3.5">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">BERT_{BASE}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Devlin et al.(2018)Devlin, Chang, Lee, and Toutanova</a>]</cite>. Specifically, we initialize the weights of the model from the first three layers of <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="BERT_{BASE}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1a" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.4" xref="S4.SS2.p2.3.m3.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1b" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><msub id="S4.SS2.p2.3.m3.1.1.5" xref="S4.SS2.p2.3.m3.1.1.5.cmml"><mi id="S4.SS2.p2.3.m3.1.1.5.2" xref="S4.SS2.p2.3.m3.1.1.5.2.cmml">T</mi><mrow id="S4.SS2.p2.3.m3.1.1.5.3" xref="S4.SS2.p2.3.m3.1.1.5.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.5.3.2" xref="S4.SS2.p2.3.m3.1.1.5.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.5.3.1" xref="S4.SS2.p2.3.m3.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.5.3.3" xref="S4.SS2.p2.3.m3.1.1.5.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.5.3.1a" xref="S4.SS2.p2.3.m3.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.5.3.4" xref="S4.SS2.p2.3.m3.1.1.5.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.5.3.1b" xref="S4.SS2.p2.3.m3.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.5.3.5" xref="S4.SS2.p2.3.m3.1.1.5.3.5.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝐵</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝐸</ci><ci id="S4.SS2.p2.3.m3.1.1.4.cmml" xref="S4.SS2.p2.3.m3.1.1.4">𝑅</ci><apply id="S4.SS2.p2.3.m3.1.1.5.cmml" xref="S4.SS2.p2.3.m3.1.1.5"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.5.1.cmml" xref="S4.SS2.p2.3.m3.1.1.5">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.5.2.cmml" xref="S4.SS2.p2.3.m3.1.1.5.2">𝑇</ci><apply id="S4.SS2.p2.3.m3.1.1.5.3.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3"><times id="S4.SS2.p2.3.m3.1.1.5.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3.1"></times><ci id="S4.SS2.p2.3.m3.1.1.5.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3.2">𝐵</ci><ci id="S4.SS2.p2.3.m3.1.1.5.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3.3">𝐴</ci><ci id="S4.SS2.p2.3.m3.1.1.5.3.4.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3.4">𝑆</ci><ci id="S4.SS2.p2.3.m3.1.1.5.3.5.cmml" xref="S4.SS2.p2.3.m3.1.1.5.3.5">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">BERT_{BASE}</annotation></semantics></math> and eliminate the separate text transformer. In terms of the object embedding, a Faster R-CNN object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Ren et al.(2015)Ren, He, Girshick, and Sun</a>]</cite> pre-trained on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Krishna et al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
Kalantidis, Li, Shamma, et al.</a>]</cite> is adopted to extract <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="M=100" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">M</mi><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><eq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></eq><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">𝑀</ci><cn type="integer" id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">M=100</annotation></semantics></math> top-scoring objects on each image and represents each object with its appearance and location features. The Microsoft-OCR system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> is used to extract OCR tokens per image with each token represented with its appearance, location, FastText <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Bojanowski et al.(2017)Bojanowski, Grave, Joulin, and
Mikolov</a>]</cite> and PHOC features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Almazán et al.(2014)Almazán, Gordo, Fornés, and
Valveny</a>]</cite>. The multi-modality fusion module is a four-layer transformer with 12 attention heads, which has the same hyper-parameters as <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="BERT_{BASE}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1a" xref="S4.SS2.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.4" xref="S4.SS2.p2.5.m5.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.1b" xref="S4.SS2.p2.5.m5.1.1.1.cmml">​</mo><msub id="S4.SS2.p2.5.m5.1.1.5" xref="S4.SS2.p2.5.m5.1.1.5.cmml"><mi id="S4.SS2.p2.5.m5.1.1.5.2" xref="S4.SS2.p2.5.m5.1.1.5.2.cmml">T</mi><mrow id="S4.SS2.p2.5.m5.1.1.5.3" xref="S4.SS2.p2.5.m5.1.1.5.3.cmml"><mi id="S4.SS2.p2.5.m5.1.1.5.3.2" xref="S4.SS2.p2.5.m5.1.1.5.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.5.3.1" xref="S4.SS2.p2.5.m5.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.5.3.3" xref="S4.SS2.p2.5.m5.1.1.5.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.5.3.1a" xref="S4.SS2.p2.5.m5.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.5.3.4" xref="S4.SS2.p2.5.m5.1.1.5.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.5.3.1b" xref="S4.SS2.p2.5.m5.1.1.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.5.m5.1.1.5.3.5" xref="S4.SS2.p2.5.m5.1.1.5.3.5.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><times id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1"></times><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">𝐵</ci><ci id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3">𝐸</ci><ci id="S4.SS2.p2.5.m5.1.1.4.cmml" xref="S4.SS2.p2.5.m5.1.1.4">𝑅</ci><apply id="S4.SS2.p2.5.m5.1.1.5.cmml" xref="S4.SS2.p2.5.m5.1.1.5"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.5.1.cmml" xref="S4.SS2.p2.5.m5.1.1.5">subscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.5.2.cmml" xref="S4.SS2.p2.5.m5.1.1.5.2">𝑇</ci><apply id="S4.SS2.p2.5.m5.1.1.5.3.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3"><times id="S4.SS2.p2.5.m5.1.1.5.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3.1"></times><ci id="S4.SS2.p2.5.m5.1.1.5.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3.2">𝐵</ci><ci id="S4.SS2.p2.5.m5.1.1.5.3.3.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3.3">𝐴</ci><ci id="S4.SS2.p2.5.m5.1.1.5.3.4.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3.4">𝑆</ci><ci id="S4.SS2.p2.5.m5.1.1.5.3.5.cmml" xref="S4.SS2.p2.5.m5.1.1.5.3.5">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">BERT_{BASE}</annotation></semantics></math>. We use <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="T=30" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">T</mi><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><eq id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></eq><ci id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">𝑇</ci><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">T=30</annotation></semantics></math> decoding steps to predict the output question word by word in an auto-regressive manner.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Training parameters.</span>
Experiments are conducted on 4 Nvidia P6000 GPUs. We train TAG for 24K iterations with a batch size of 128. We adopt the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Kingma and Ba(2014)</a>]</cite> with a learning rate of 1e-4 and a staircase learning rate schedule, where we multiply the learning rate by 0.1 at 14K and at 19K iterations. We keep the original parameter settings of downstream Text-VQA models except that we increase their maximum iteration in proportion to the increased size of the augmented data to accommodate the enlarged number of training samples.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">TextVQA dataset.</span>
To perform a fair comparison with prior work, we conduct experiments in both the constrained setting (top part of Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and the unconstrained setting (bottom part of Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) on the TextVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx41" title="" class="ltx_ref">Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach</a>]</cite>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">‡</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‡</sup><span class="ltx_tag ltx_tag_note">‡</span>The constrained setting means training without extra data and the unconstrained one indicates otherwise.</span></span></span> The number of our augmented training QA examples for Text-VQA is 69.2K compared with 34.6K for the original one. In the constrained setting (top), our TAG improves the corresponding M4C and TAP baselines by 1.18% and 2.63% on the validation set, respectively. We note that although LOGOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé</a>]</cite> in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> uses an extra grounding dataset with 1.1 million images for pre-training and yet our method performs better. In the unconstrained setting (bottom), TAG further boosts M4C and TAP baselines by 1.11% and 3.06% on the validation set, respectively. On the TextVQA test set, TAG also obtains significant performance gains over existing methods. This validates the effectiveness of TAG.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We also visualize the generated QA pairs of our TAG in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. It shows that our TAG generates meaningful QA pairs that are novel compared to the originally annotated ones.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">ST-VQA dataset.</span>
We also compare our approach with the state-of-the-art (SOTA) methods under both the constrained setting and the unconstrained setting on the ST-VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas</a>]</cite>. We compute the accuracy and ANLS score as the evaluation metrics. The number of the newly built training QA examples for the ST-VQA task after augmentation is 46.8K compared with 23.4K for the original one. Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> suggests that TAG achieves SOTA performance and significantly outperforms the baselines. In particular, TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> achieves 50.83%, and 0.598 in terms of the accuracy and ANLS score on the validation set with additional TextVQA and 1.4 million large-scale pre-training data, while TAG improves these results by a significant 2.70% and 0.022 with only additional TextVQA data. In addition, we submit the prediction results of test set on the ST-VQA test server. The results show that TAG with TAP achieves the SOTA performance with ANLS score of 0.602 on the test set. Without bells and whistles, our approach greatly outperforms the baselines, M4C <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach</a>]</cite> and TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Studies</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We conduct extensive ablation studies to demonstrate the effectiveness of TAG using TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> under the constrained setting on the TextVQA validation set.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Ans.</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Obj.</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OCR.</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Val Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_nopad_l ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_nopad_l ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">48.76</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_nopad_l" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">48.95</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_nopad_l" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.13</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;">✓</td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S4.T3.1.5.4.4.1" class="ltx_text ltx_font_bold">52.54</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study of TAG with TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> under constrained setting on TextVQA validation set. "Ans.", "Obj." and "OCR." refer to embedding of answer words, detected objects and OCR tokens, respectively.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Answer Selection</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t">Val Acc.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T4.1.2.1.1.1" class="ltx_text ltx_font_italic">random</span></td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">49.26</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center">
<span id="S4.T4.1.3.2.1.1" class="ltx_text ltx_font_italic">largest</span></td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center">52.54</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<td id="S4.T4.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S4.T4.1.4.3.1.1" class="ltx_text ltx_font_italic">top three</span></td>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center">52.73</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<td id="S4.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b">
<span id="S4.T4.1.5.4.1.1" class="ltx_text ltx_font_italic">top five</span></td>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b">52.19</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation study of TAG with TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite> under constrained setting on TextVQA validation set. <span id="S4.T4.4.1" class="ltx_text ltx_font_italic">Random</span> means a random OCR token is selected as the answer input to TAG, while <span id="S4.T4.5.2" class="ltx_text ltx_font_italic">top three</span> means the top three largest OCR tokens are selected.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Contribution of each modality in TAG.</span>
To understand the contribution of different input modalities to the success of TAG, Table. <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the performance of our framework when a certain modality is removed. It suggests that when both the visual objects and OCR tokens modalities are removed, the performance of our TAG decreases by 3.78%. On the other side, when removing the visual objects modality and OCR tokens modality separately, the performance drops by 3.59% and 3.41%, respectively.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.2" class="ltx_p"><span id="S4.SS4.p3.2.1" class="ltx_text ltx_font_bold">Impact of the answer selection strategy.</span> To better explore the performance of our TAG, and understand how different answer selection strategies would affect the model performance, we design several experiments over the choice of input answer selection strategy. Our method adopts the <em id="S4.SS4.p3.2.2" class="ltx_emph ltx_font_italic">largest</em> OCR word as the answer candidate for TAG. We compare this strategy with other possibilities in Table. <a href="#S4.T4" title="Table 4 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The table shows that, if we use a <em id="S4.SS4.p3.2.3" class="ltx_emph ltx_font_italic">random</em> OCR token as the input answer, the performance drops by 3.28%. On the other hand, if we increase the number of answer candidates by including the top-3 largest OCR tokens to augment the labeled data by 3<math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><times id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\times</annotation></semantics></math>, the performance boosts additional 0.19% as compared to the <em id="S4.SS4.p3.2.4" class="ltx_emph ltx_font_italic">largest</em> strategy while it introduces 3<math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mo id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><times id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">\times</annotation></semantics></math> training time. To achieve a better balance between training efficiency and accuracy, we consider the OCR token with the <em id="S4.SS4.p3.2.5" class="ltx_emph ltx_font_italic">largest</em> bounding box as our final setting for the input answer to TAG. As we have mentioned previously, more high-quality QA pairs could be continuously augmented with a more sophisticated answer-candidate selection strategy. We leave this direction for future work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose a novel architecture TAG, a text-aware visual question-answer (QA) generation method to deal with the sparse annotation of existing Text-VQA datasets. Our approach leverages the rich yet underexplored visual and scene text information and directly enlarges the existing training set by generating high-quality and rich QA pairs without extra labeling cost. Without bells and whistles, experimental results show that our generated QA pairs boost the performance of recent Text-VQA models by a large margin on both TextVQA and ST-VQA datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Agarwal et al.(2020)Agarwal, Shetty, and Fritz]</span>
<span class="ltx_bibblock">
Vedika Agarwal, Rakshith Shetty, and Mario Fritz.

</span>
<span class="ltx_bibblock">Towards causal vqa: Revealing and reducing spurious correlations by
invariant and covariant semantic editing.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 9690–9698, 2020.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Almazán et al.(2014)Almazán, Gordo, Fornés, and
Valveny]</span>
<span class="ltx_bibblock">
Jon Almazán, Albert Gordo, Alicia Fornés, and Ernest Valveny.

</span>
<span class="ltx_bibblock">Word spotting and recognition with embedded attributes.

</span>
<span class="ltx_bibblock"><em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 36(12):2552–2566, 2014.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Anderson et al.(2018a)Anderson, He, Buehler, Teney,
Johnson, Gould, and Zhang]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086, 2018a.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Anderson et al.(2018b)Anderson, Wu, Teney, Bruce,
Johnson, Sünderhauf, Reid, Gould, and Van Den Hengel]</span>
<span class="ltx_bibblock">
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko
Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel.

</span>
<span class="ltx_bibblock">Vision-and-language navigation: Interpreting visually-grounded
navigation instructions in real environments.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 3674–3683, 2018b.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Antol et al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
Parikh]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Baevski et al.(2020)Baevski, Zhou, Mohamed, and
Auli]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations.

</span>
<span class="ltx_bibblock"><em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:12449–12460, 2020.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bigham et al.(2010)Bigham, Jayant, Ji, Little, Miller, Miller, Miller,
Tatarowicz, White, White, et al.]</span>
<span class="ltx_bibblock">
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller,
Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual
White, et al.

</span>
<span class="ltx_bibblock">Vizwiz: nearly real-time answers to visual questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23nd annual ACM symposium on User
interface software and technology</em>, pages 333–342, 2010.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Biten et al.(2019)Biten, Tito, Mafla, Gomez, Rusinol, Valveny,
Jawahar, and Karatzas]</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol,
Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</em>, pages 4291–4301, 2019.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Biten et al.(2022)Biten, Litman, Xie, Appalaraju, and
Manmatha]</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and R. Manmatha.

</span>
<span class="ltx_bibblock">Latr: Layout-aware transformer for scene-text vqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 16548–16558, June 2022.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bojanowski et al.(2017)Bojanowski, Grave, Joulin, and
Mikolov]</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.

</span>
<span class="ltx_bibblock">Enriching word vectors with subword information.

</span>
<span class="ltx_bibblock"><em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Transactions of the association for computational linguistics</em>,
5:135–146, 2017.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Carion et al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
Zagoruyko]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 213–229.
Springer, 2020.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chen et al.(2020)Chen, Li, Yu, El Kholy, Ahmed, Gan, Cheng, and
Liu]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 104–120.
Springer, 2020.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Deng et al.(2009)Deng, Dong, Socher, Li, Li, and
Fei-Fei]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</em>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al.(2018)Devlin, Chang, Lee, and Toutanova]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Dosovitskiy et al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
et al.]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gao et al.(2021)Gao, Zhu, Wang, Li, Liu, Van den Hengel, and
Wu]</span>
<span class="ltx_bibblock">
Chenyu Gao, Qi Zhu, Peng Wang, Hui Li, Yuliang Liu, Anton Van den Hengel, and
Qi Wu.

</span>
<span class="ltx_bibblock">Structured multimodal attentions for textvqa.

</span>
<span class="ltx_bibblock"><em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 2021.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gao et al.(2020)Gao, Li, Wang, Shan, and Chen]</span>
<span class="ltx_bibblock">
Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, and Xilin Chen.

</span>
<span class="ltx_bibblock">Multi-modal graph neural network for joint reasoning on vision and
scene text.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 12746–12756, 2020.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Goyal et al.(2017)Goyal, Khot, Summers-Stay, Batra, and
Parikh]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Guan et al.(2022)Guan, Wang, Lan, Chandra, Wu, Davis, and
Manocha]</span>
<span class="ltx_bibblock">
Tianrui Guan, Jun Wang, Shiyi Lan, Rohan Chandra, Zuxuan Wu, Larry Davis, and
Dinesh Manocha.

</span>
<span class="ltx_bibblock">M3detr: Multi-representation, multi-scale, mutual-relation 3d object
detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</em>, pages 772–782, 2022.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gurari et al.(2018)Gurari, Li, Stangl, Guo, Lin, Grauman, Luo, and
Bigham]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Han et al.(2020)Han, Huang, and Han]</span>
<span class="ltx_bibblock">
Wei Han, Hantao Huang, and Tao Han.

</span>
<span class="ltx_bibblock">Finding the evidence: Localization-aware answer prediction for text
visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.02582</em>, 2020.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hu et al.(2020)Hu, Singh, Darrell, and Rohrbach]</span>
<span class="ltx_bibblock">
Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Iterative answer prediction with pointer-augmented multimodal
transformers for textvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 9992–10002, 2020.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jiang et al.(2018)Jiang, Natarajan, Chen, Rohrbach, Batra, and
Parikh]</span>
<span class="ltx_bibblock">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh.

</span>
<span class="ltx_bibblock">Pythia v0. 1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09956</em>, 2018.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kafle et al.(2017)Kafle, Yousefhussien, and Kanan]</span>
<span class="ltx_bibblock">
Kushal Kafle, Mohammed A Yousefhussien, and Christopher Kanan.

</span>
<span class="ltx_bibblock">Data augmentation for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">INLG</em>, pages 198–202, 2017.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kant et al.(2020)Kant, Batra, Anderson, Schwing, Parikh, Lu, and
Agrawal]</span>
<span class="ltx_bibblock">
Yash Kant, Dhruv Batra, Peter Anderson, Alexander Schwing, Devi Parikh, Jiasen
Lu, and Harsh Agrawal.

</span>
<span class="ltx_bibblock">Spatially aware multimodal transformers for textvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 715–732.
Springer, 2020.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kant et al.(2021)Kant, Moudgil, Batra, Parikh, and
Agrawal]</span>
<span class="ltx_bibblock">
Yash Kant, Abhinav Moudgil, Dhruv Batra, Devi Parikh, and Harsh Agrawal.

</span>
<span class="ltx_bibblock">Contrast and classify: Training robust vqa models.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 1604–1613, 2021.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Karatzas et al.(2013)Karatzas, Shafait, Uchida, Iwamura, i Bigorda,
Mestre, Mas, Mota, Almazan, and De Las Heras]</span>
<span class="ltx_bibblock">
Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura,
Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota,
Jon Almazan Almazan, and Lluis Pere De Las Heras.

</span>
<span class="ltx_bibblock">Icdar 2013 robust reading competition.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">2013 12th international conference on document analysis and
recognition</em>, pages 1484–1493. IEEE, 2013.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Karatzas et al.(2015)Karatzas, Gomez-Bigorda, Nicolaou, Ghosh,
Bagdanov, Iwamura, Matas, Neumann, Chandrasekhar, Lu,
et al.]</span>
<span class="ltx_bibblock">
Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh,
Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann,
Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al.

</span>
<span class="ltx_bibblock">Icdar 2015 competition on robust reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">2015 13th international conference on document analysis and
recognition (ICDAR)</em>, pages 1156–1160. IEEE, 2015.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kingma and Ba(2014)]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Krishna et al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
Kalantidis, Li, Shamma, et al.]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32–73, 2017.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kuznetsova et al.(2020)Kuznetsova, Rom, Alldrin, Uijlings, Krasin,
Pont-Tuset, Kamali, Popov, Malloci, Kolesnikov, et al.]</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, et al.

</span>
<span class="ltx_bibblock">The open images dataset v4.

</span>
<span class="ltx_bibblock"><em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 128(7):1956–1981, 2020.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Liu et al.(2020)Liu, Xu, Wu, Du, Jia, and Tan]</span>
<span class="ltx_bibblock">
Fen Liu, Guanghui Xu, Qi Wu, Qing Du, Wei Jia, and Mingkui Tan.

</span>
<span class="ltx_bibblock">Cascade reasoning network for text-based visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</em>, pages 4060–4069, 2020.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Liu et al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
Guo]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 10012–10022, 2021.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lu et al.(2021)Lu, Fan, Wang, Oh, and Rosé]</span>
<span class="ltx_bibblock">
Xiaopeng Lu, Zhen Fan, Yansen Wang, Jean Oh, and Carolyn P Rosé.

</span>
<span class="ltx_bibblock">Localize, group, and select: Boosting text-vqa by scene text
modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 2631–2639, 2021.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mishra et al.(2013)Mishra, Alahari, and Jawahar]</span>
<span class="ltx_bibblock">
Anand Mishra, Karteek Alahari, and CV Jawahar.

</span>
<span class="ltx_bibblock">Image retrieval using textual cues.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 3040–3047, 2013.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Mishra et al.(2019)Mishra, Shekhar, Singh, and
Chakraborty]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">ICDAR</em>, 2019.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ray et al.(2019)Ray, Sikka, Divakaran, Lee, and
Burachas]</span>
<span class="ltx_bibblock">
Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas.

</span>
<span class="ltx_bibblock">Sunny and dark outside?! improving answer consistency in vqa through
entailed question generation.

</span>
<span class="ltx_bibblock"><em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.04696</em>, 2019.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ren et al.(2015)Ren, He, Girshick, and Sun]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Shah et al.(2019)Shah, Chen, Rohrbach, and Parikh]</span>
<span class="ltx_bibblock">
Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh.

</span>
<span class="ltx_bibblock">Cycle-consistency for robust visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6649–6658, 2019.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sidorov et al.(2020)Sidorov, Hu, Rohrbach, and
Singh]</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 742–758.
Springer, 2020.

</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Singh et al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh,
and Rohrbach]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 8317–8326, 2019.

</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tang et al.(2020)Tang, Ma, Zhang, Wu, and Yang]</span>
<span class="ltx_bibblock">
Ruixue Tang, Chao Ma, Wei Emma Zhang, Qi Wu, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Semantic equivalent adversarial data augmentation for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx42.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 437–453.
Springer, 2020.

</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
Kaiser, and Polosukhin]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bibx43.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Veit et al.(2016)Veit, Matera, Neumann, Matas, and
Belongie]</span>
<span class="ltx_bibblock">
Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie.

</span>
<span class="ltx_bibblock">Coco-text: Dataset and benchmark for text detection and recognition
in natural images.

</span>
<span class="ltx_bibblock"><em id="bib.bibx44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1601.07140</em>, 2016.

</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Vinyals et al.(2015)Vinyals, Fortunato, and
Jaitly]</span>
<span class="ltx_bibblock">
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.

</span>
<span class="ltx_bibblock">Pointer networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx45.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang(2022)]</span>
<span class="ltx_bibblock">
Jun Wang.

</span>
<span class="ltx_bibblock">ESSumm: Extractive Speech Summarization from Untranscribed Meeting.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx46.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, pages 3243–3247, 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.21437/Interspeech.2022-945" title="" class="ltx_ref">10.21437/Interspeech.2022-945</a>.

</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al.(2022)Wang, Chen, Wu, Luo, Zhou, Zhao, Xie, Liu, Jiang, and
Yuan]</span>
<span class="ltx_bibblock">
Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao,
Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan.

</span>
<span class="ltx_bibblock">Omnivl: One foundation model for image-language and video-language
tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07526</em>, 2022.

</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al.(2021a)Wang, Xiao, Lu, Jin, and
He]</span>
<span class="ltx_bibblock">
Qingqing Wang, Liqiang Xiao, Yue Lu, Yaohui Jin, and Hao He.

</span>
<span class="ltx_bibblock">Towards reasoning ability in scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</em>, pages 2281–2289, 2021a.

</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al.(2021b)Wang, Miao, and Specia]</span>
<span class="ltx_bibblock">
Zixu Wang, Yishu Miao, and Lucia Specia.

</span>
<span class="ltx_bibblock">Cross-modal generative augmentation for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bibx49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.04780</em>, 2021b.

</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo]</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang,
Cha Zhang, Lei Zhang, and Jiebo Luo.

</span>
<span class="ltx_bibblock">Tap: Text-aware pre-training for text-vqa and text-caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 8751–8761, 2021.

</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zeng et al.(2021)Zeng, Zhang, Zhou, and Yang]</span>
<span class="ltx_bibblock">
Gangyan Zeng, Yuan Zhang, Yu Zhou, and Xiaomeng Yang.

</span>
<span class="ltx_bibblock">Beyond ocr+ vqa: involving ocr into the flow for robust and accurate
textvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</em>, pages 376–385, 2021.

</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang and Yang(2021)]</span>
<span class="ltx_bibblock">
Xuanyu Zhang and Qing Yang.

</span>
<span class="ltx_bibblock">Position-augmented transformers with entity-aligned mesh for textvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</em>, pages 2519–2528, 2021.

</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhao et al.(2021)Zhao, Jiang, Jia, Torr, and Koltun]</span>
<span class="ltx_bibblock">
Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Point transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 16259–16268, 2021.

</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhou et al.(2020)Zhou, Palangi, Zhang, Hu, Corso, and
Gao]</span>
<span class="ltx_bibblock">
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng
Gao.

</span>
<span class="ltx_bibblock">Unified vision-language pre-training for image captioning and vqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 13041–13049, 2020.

</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhu et al.(2021)Zhu, Gao, Wang, and Wu]</span>
<span class="ltx_bibblock">
Qi Zhu, Chenyu Gao, Peng Wang, and Qi Wu.

</span>
<span class="ltx_bibblock">Simple is not easy: A simple strong baseline for textvqa and
textcaps.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 35, pages 3608–3615, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details of Our Approach</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold">Text-aware Visual Question Prediction.</span>
We leverage the powerful capability of the attention mechanism in transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
Kaiser, and Polosukhin</a>]</cite> to capture the interactions among the extended answer words, visual objects, and OCR tokens. Our decoding module is based on a dynamic pointer network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">Vinyals et al.(2015)Vinyals, Fortunato, and
Jaitly</a>]</cite>, which allows both copying words via pointing, and generating words from a fixed vocabulary obtained from the training set.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">§</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">§</sup><span class="ltx_tag ltx_tag_note">§</span>Our decoding module is implemented following the implementation of the decoding module in TAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Yang et al.(2021)Yang, Lu, Wang, Yin, Florencio, Wang, Zhang, Zhang,
and Luo</a>]</cite>. We keep their default hyper-parameters except otherwise noted.</span></span></span></p>
</div>
<figure id="A1.T1" class="ltx_table">
<table id="A1.T1.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T1.7.8.1" class="ltx_tr">
<td id="A1.T1.7.8.1.1" class="ltx_td ltx_align_left ltx_border_t">Hyper-parameters</td>
<td id="A1.T1.7.8.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">Value</td>
</tr>
<tr id="A1.T1.1.1" class="ltx_tr">
<td id="A1.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">Max length of answer words <math id="A1.T1.1.1.1.m1.1" class="ltx_Math" alttext="{w}^{ans}" display="inline"><semantics id="A1.T1.1.1.1.m1.1a"><msup id="A1.T1.1.1.1.m1.1.1" xref="A1.T1.1.1.1.m1.1.1.cmml"><mi id="A1.T1.1.1.1.m1.1.1.2" xref="A1.T1.1.1.1.m1.1.1.2.cmml">w</mi><mrow id="A1.T1.1.1.1.m1.1.1.3" xref="A1.T1.1.1.1.m1.1.1.3.cmml"><mi id="A1.T1.1.1.1.m1.1.1.3.2" xref="A1.T1.1.1.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A1.T1.1.1.1.m1.1.1.3.1" xref="A1.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.T1.1.1.1.m1.1.1.3.3" xref="A1.T1.1.1.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="A1.T1.1.1.1.m1.1.1.3.1a" xref="A1.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="A1.T1.1.1.1.m1.1.1.3.4" xref="A1.T1.1.1.1.m1.1.1.3.4.cmml">s</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.T1.1.1.1.m1.1b"><apply id="A1.T1.1.1.1.m1.1.1.cmml" xref="A1.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T1.1.1.1.m1.1.1.1.cmml" xref="A1.T1.1.1.1.m1.1.1">superscript</csymbol><ci id="A1.T1.1.1.1.m1.1.1.2.cmml" xref="A1.T1.1.1.1.m1.1.1.2">𝑤</ci><apply id="A1.T1.1.1.1.m1.1.1.3.cmml" xref="A1.T1.1.1.1.m1.1.1.3"><times id="A1.T1.1.1.1.m1.1.1.3.1.cmml" xref="A1.T1.1.1.1.m1.1.1.3.1"></times><ci id="A1.T1.1.1.1.m1.1.1.3.2.cmml" xref="A1.T1.1.1.1.m1.1.1.3.2">𝑎</ci><ci id="A1.T1.1.1.1.m1.1.1.3.3.cmml" xref="A1.T1.1.1.1.m1.1.1.3.3">𝑛</ci><ci id="A1.T1.1.1.1.m1.1.1.3.4.cmml" xref="A1.T1.1.1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.1.1.1.m1.1c">{w}^{ans}</annotation></semantics></math></td>
<td id="A1.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">20</td>
</tr>
<tr id="A1.T1.3.3" class="ltx_tr">
<td id="A1.T1.3.3.2" class="ltx_td ltx_align_left">Max length <math id="A1.T1.2.2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="A1.T1.2.2.1.m1.1a"><mi id="A1.T1.2.2.1.m1.1.1" xref="A1.T1.2.2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A1.T1.2.2.1.m1.1b"><ci id="A1.T1.2.2.1.m1.1.1.cmml" xref="A1.T1.2.2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.2.2.1.m1.1c">M</annotation></semantics></math> of visual objects <math id="A1.T1.3.3.2.m2.1" class="ltx_Math" alttext="{w}^{obj}" display="inline"><semantics id="A1.T1.3.3.2.m2.1a"><msup id="A1.T1.3.3.2.m2.1.1" xref="A1.T1.3.3.2.m2.1.1.cmml"><mi id="A1.T1.3.3.2.m2.1.1.2" xref="A1.T1.3.3.2.m2.1.1.2.cmml">w</mi><mrow id="A1.T1.3.3.2.m2.1.1.3" xref="A1.T1.3.3.2.m2.1.1.3.cmml"><mi id="A1.T1.3.3.2.m2.1.1.3.2" xref="A1.T1.3.3.2.m2.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="A1.T1.3.3.2.m2.1.1.3.1" xref="A1.T1.3.3.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.T1.3.3.2.m2.1.1.3.3" xref="A1.T1.3.3.2.m2.1.1.3.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="A1.T1.3.3.2.m2.1.1.3.1a" xref="A1.T1.3.3.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.T1.3.3.2.m2.1.1.3.4" xref="A1.T1.3.3.2.m2.1.1.3.4.cmml">j</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.T1.3.3.2.m2.1b"><apply id="A1.T1.3.3.2.m2.1.1.cmml" xref="A1.T1.3.3.2.m2.1.1"><csymbol cd="ambiguous" id="A1.T1.3.3.2.m2.1.1.1.cmml" xref="A1.T1.3.3.2.m2.1.1">superscript</csymbol><ci id="A1.T1.3.3.2.m2.1.1.2.cmml" xref="A1.T1.3.3.2.m2.1.1.2">𝑤</ci><apply id="A1.T1.3.3.2.m2.1.1.3.cmml" xref="A1.T1.3.3.2.m2.1.1.3"><times id="A1.T1.3.3.2.m2.1.1.3.1.cmml" xref="A1.T1.3.3.2.m2.1.1.3.1"></times><ci id="A1.T1.3.3.2.m2.1.1.3.2.cmml" xref="A1.T1.3.3.2.m2.1.1.3.2">𝑜</ci><ci id="A1.T1.3.3.2.m2.1.1.3.3.cmml" xref="A1.T1.3.3.2.m2.1.1.3.3">𝑏</ci><ci id="A1.T1.3.3.2.m2.1.1.3.4.cmml" xref="A1.T1.3.3.2.m2.1.1.3.4">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.3.3.2.m2.1c">{w}^{obj}</annotation></semantics></math></td>
<td id="A1.T1.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center">100</td>
</tr>
<tr id="A1.T1.5.5" class="ltx_tr">
<td id="A1.T1.5.5.2" class="ltx_td ltx_align_left">Max length <math id="A1.T1.4.4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A1.T1.4.4.1.m1.1a"><mi id="A1.T1.4.4.1.m1.1.1" xref="A1.T1.4.4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A1.T1.4.4.1.m1.1b"><ci id="A1.T1.4.4.1.m1.1.1.cmml" xref="A1.T1.4.4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.4.4.1.m1.1c">N</annotation></semantics></math> of scene texts <math id="A1.T1.5.5.2.m2.1" class="ltx_Math" alttext="{w}^{ocr}" display="inline"><semantics id="A1.T1.5.5.2.m2.1a"><msup id="A1.T1.5.5.2.m2.1.1" xref="A1.T1.5.5.2.m2.1.1.cmml"><mi id="A1.T1.5.5.2.m2.1.1.2" xref="A1.T1.5.5.2.m2.1.1.2.cmml">w</mi><mrow id="A1.T1.5.5.2.m2.1.1.3" xref="A1.T1.5.5.2.m2.1.1.3.cmml"><mi id="A1.T1.5.5.2.m2.1.1.3.2" xref="A1.T1.5.5.2.m2.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="A1.T1.5.5.2.m2.1.1.3.1" xref="A1.T1.5.5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.T1.5.5.2.m2.1.1.3.3" xref="A1.T1.5.5.2.m2.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A1.T1.5.5.2.m2.1.1.3.1a" xref="A1.T1.5.5.2.m2.1.1.3.1.cmml">​</mo><mi id="A1.T1.5.5.2.m2.1.1.3.4" xref="A1.T1.5.5.2.m2.1.1.3.4.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.T1.5.5.2.m2.1b"><apply id="A1.T1.5.5.2.m2.1.1.cmml" xref="A1.T1.5.5.2.m2.1.1"><csymbol cd="ambiguous" id="A1.T1.5.5.2.m2.1.1.1.cmml" xref="A1.T1.5.5.2.m2.1.1">superscript</csymbol><ci id="A1.T1.5.5.2.m2.1.1.2.cmml" xref="A1.T1.5.5.2.m2.1.1.2">𝑤</ci><apply id="A1.T1.5.5.2.m2.1.1.3.cmml" xref="A1.T1.5.5.2.m2.1.1.3"><times id="A1.T1.5.5.2.m2.1.1.3.1.cmml" xref="A1.T1.5.5.2.m2.1.1.3.1"></times><ci id="A1.T1.5.5.2.m2.1.1.3.2.cmml" xref="A1.T1.5.5.2.m2.1.1.3.2">𝑜</ci><ci id="A1.T1.5.5.2.m2.1.1.3.3.cmml" xref="A1.T1.5.5.2.m2.1.1.3.3">𝑐</ci><ci id="A1.T1.5.5.2.m2.1.1.3.4.cmml" xref="A1.T1.5.5.2.m2.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.5.5.2.m2.1c">{w}^{ocr}</annotation></semantics></math></td>
<td id="A1.T1.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center">100</td>
</tr>
<tr id="A1.T1.6.6" class="ltx_tr">
<td id="A1.T1.6.6.1" class="ltx_td ltx_align_left">Max length <math id="A1.T1.6.6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A1.T1.6.6.1.m1.1a"><mi id="A1.T1.6.6.1.m1.1.1" xref="A1.T1.6.6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A1.T1.6.6.1.m1.1b"><ci id="A1.T1.6.6.1.m1.1.1.cmml" xref="A1.T1.6.6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.6.6.1.m1.1c">K</annotation></semantics></math> of extended answer words</td>
<td id="A1.T1.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center">220</td>
</tr>
<tr id="A1.T1.7.7" class="ltx_tr">
<td id="A1.T1.7.7.1" class="ltx_td ltx_align_left">max length <math id="A1.T1.7.7.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="A1.T1.7.7.1.m1.1a"><mi id="A1.T1.7.7.1.m1.1.1" xref="A1.T1.7.7.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A1.T1.7.7.1.m1.1b"><ci id="A1.T1.7.7.1.m1.1.1.cmml" xref="A1.T1.7.7.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T1.7.7.1.m1.1c">T</annotation></semantics></math> of decoding step</td>
<td id="A1.T1.7.7.2" class="ltx_td ltx_nopad_l ltx_align_center">30</td>
</tr>
<tr id="A1.T1.7.9.2" class="ltx_tr">
<td id="A1.T1.7.9.2.1" class="ltx_td ltx_align_left">optimizer</td>
<td id="A1.T1.7.9.2.2" class="ltx_td ltx_nopad_l ltx_align_center">Adam</td>
</tr>
<tr id="A1.T1.7.10.3" class="ltx_tr">
<td id="A1.T1.7.10.3.1" class="ltx_td ltx_align_left">batch size</td>
<td id="A1.T1.7.10.3.2" class="ltx_td ltx_nopad_l ltx_align_center">128</td>
</tr>
<tr id="A1.T1.7.11.4" class="ltx_tr">
<td id="A1.T1.7.11.4.1" class="ltx_td ltx_align_left">max iterations</td>
<td id="A1.T1.7.11.4.2" class="ltx_td ltx_nopad_l ltx_align_center">24K</td>
</tr>
<tr id="A1.T1.7.12.5" class="ltx_tr">
<td id="A1.T1.7.12.5.1" class="ltx_td ltx_align_left">base learning rate</td>
<td id="A1.T1.7.12.5.2" class="ltx_td ltx_nopad_l ltx_align_center">1e-4</td>
</tr>
<tr id="A1.T1.7.13.6" class="ltx_tr">
<td id="A1.T1.7.13.6.1" class="ltx_td ltx_align_left">learning rate steps</td>
<td id="A1.T1.7.13.6.2" class="ltx_td ltx_nopad_l ltx_align_center">14K, 19K</td>
</tr>
<tr id="A1.T1.7.14.7" class="ltx_tr">
<td id="A1.T1.7.14.7.1" class="ltx_td ltx_align_left ltx_border_b">learning rate decay</td>
<td id="A1.T1.7.14.7.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b">0.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table A1: </span>Hyper-parameters of TAG.</figcaption>
</figure>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Hyper-parameters.</span>
Table. <a href="#A1.T1" title="Table A1 ‣ Appendix A Additional Details of Our Approach ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A1</span></a> overviews the hyper-parameter settings of TAG. We use the original parameter settings of downstream Text-VQA models except that we increase the maximum number of iteration in proportion to the increased size of the augmented data to accommodate the enlarged number of training samples.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Qualitatively Visualization of TAG</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We present additional visualization results of generated QA pairs by TAG on the TextVQA training set in Figure. <a href="#A2.F1" title="Figure A1 ‣ Appendix B Additional Qualitatively Visualization of TAG ‣ TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A1</span></a>.</p>
</div>
<figure id="A2.F1" class="ltx_figure"><img src="/html/2208.01813/assets/x5.png" id="A2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="140" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure A1: </span>We visualize the examples of the generated QA pairs (bottom in orange) by the TAG module compared with the original annotated QA pairs (top in blue) on the TextVQA training set. "Q" and "A" refer to question and answer, respectively. Best viewed in color.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.01812" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.01813" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.01813">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.01813" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.01814" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 15:44:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
