<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13779] Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data</title><meta property="og:description" content="We present a simple and efficient method to leverage emerging text-to-image generative models in creating large-scale synthetic supervision for the task of damage assessment from aerial images. While significant recentâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13779">

<!--Generated on Wed Jun  5 19:21:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Robust Disaster Assessment from Aerial Imagery Using 
<br class="ltx_break">Text-to-Image Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.1" class="ltx_text ltx_font_bold">Tarun Kalluri<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span> â€ƒ<span id="id2.2.2" class="ltx_text ltx_font_bold">Jihyeon Lee<sup id="id2.2.2.1" class="ltx_sup"><span id="id2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> â€ƒ<span id="id3.3.3" class="ltx_text ltx_font_bold">Kihyuk Sohn<sup id="id3.3.3.1" class="ltx_sup"><span id="id3.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> â€ƒ<span id="id4.4.4" class="ltx_text ltx_font_bold">Sahil Singla<sup id="id4.4.4.1" class="ltx_sup"><span id="id4.4.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> â€ƒ
<br class="ltx_break"><span id="id5.5.5" class="ltx_text ltx_font_bold">Manmohan Chandraker<sup id="id5.5.5.1" class="ltx_sup"><span id="id5.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span> â€ƒ<span id="id6.6.6" class="ltx_text ltx_font_bold">Joseph Xu<sup id="id6.6.6.1" class="ltx_sup"><span id="id6.6.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> â€ƒ<span id="id7.7.7" class="ltx_text ltx_font_bold">Jeremiah Liu<sup id="id7.7.7.1" class="ltx_sup"><span id="id7.7.7.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span> 
<br class="ltx_break"><sup id="id10.10.id1" class="ltx_sup">1</sup>UC San Diegoâ€ƒ<sup id="id11.11.id2" class="ltx_sup">2</sup>Google Research
<br class="ltx_break">
</span><span class="ltx_author_notes">Work done during TKâ€™s internship at Google.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">We present a simple and efficient method to leverage emerging text-to-image generative models in creating large-scale synthetic supervision for the task of damage assessment from aerial images. While significant recent advances have resulted in improved techniques for damage assessment using aerial or satellite imagery, they still suffer from poor robustness to domains where manual labeled data is unavailable, directly impacting post-disaster humanitarian assistance in such under-resourced geographies. Our contribution towards improving domain robustness in this scenario is two-fold. Firstly, we leverage the text-guided mask-based image editing capabilities of generative models and build an efficient and easily scalable pipeline to generate thousands of post-disaster images from low-resource domains. Secondly, we propose a simple two-stage training approach to train robust models while using manual supervision from different source domains along with the generated synthetic target domain data. We validate the strength of our proposed framework under cross-geography domain transfer setting from xBD and SKAI images in both single-source and multi-source settings, achieving significant improvements over a source-only baseline in each case.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In this work, we address the issue of poor robustness caused by traditional training methods for the task of disaster assessment by generating synthetic data using guided text-to-image generationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.
To accelerate rescue, recovery and aid routing through scalable and automated disaster assessment from images, recent methods propose efficient training paradigms using paired labeled data from before and after the disasterÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>. While being instrumental in significantly improving the accuracy in damage assessment, these methods greatly rely on manual supervision for efficient performance and perform poorly when deployed in novel domains - such as new disaster types or unseen geographies.
While unsupervised adaptation methods exist to overcome the overhead of manual annotationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, they still require unlabeled images captured from both before and after the disaster for learning domain agnostic features.
While readily available satellite imagery provides generalized aerial coverage for most geographic locations for pre-disaster images, the retrieval of post-disaster imagery remains a time-consuming process, hindering rapid damage assessment during critical response windows, with non-trivial domain shifts preventing cross-geographical deployment.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.13779/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="243" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Summary of our proposed pipeline.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> A disaster assessment model trained using labeled data from a different domain suffers from poor accuracy due to significant distribution shifts with the target. We offer a novel way of addressing this limitation, by leveraging the recent advances in mask-based text-to-image modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> to generate thousands of synthetic labeled data from the target domain where only pre-disaster images are accessible. We incorporate this synthetic data along with source labeled data in a two-stage training framework to achieve significant gains on challening transfer settings from xBDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and SKAIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> datasets.</span></span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the other hand, there has been a notable progress in the field of generating synthetic data using guided text-to-image modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, which overcome the cumbersome manual annotation process and enable controllable data-generation at scale to train robust and data-efficient models. While a majority of these works focus on tasks like image recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, extending these methods to suit the current setting of aerial disaster assessment is non-trivial, as it requires generating precisely synchronized pre- and post-disaster imagery of the affected area.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We present a novel framework that leverages image-guided text-conditioned generative models to synthesize large datasets of post-disaster imagery conditioned on pre-disaster imagery, which is trained efficiently using a two-stage approach by incorporating unlabeled target domain data alongside source labels.
By exploiting the mask-based image-editing abilities of transformer-based text-to-image models, we edit the pre-disaster image using a suitable text-prompt to create a corresponding synthetic post-disaster image, generating a new, synthetically labeled dataset specific to the target domain. To mitigate performance degradation caused by domain shift between generated data and real-world images, we adopt a two-stage training procedure. We first train a siamese vision-transformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> using labeled data from the source domains, and subsequently fine-tune the last layer on the synthetic data from the target domain following prior workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>.
We show the effectiveness of our framework in training robust models through experiments on several challenging transfer settings from xBD and SKAI datasets, significantly outperforming a source-only training baseline in each case.
As shown in <a href="#S1.F1" title="In 1 Introduction â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>, while training directly using the source domain only achieves only 25% accuracy on the target test-data, our synthetic-data augmented training achieves 54.4%, with a non-trivial improvement of 29% on the challenging xBD dataset. In summary, our contributions are as follows.</p>
</div>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We offer a cost-effective way to generate training data for disaster assessment in areas lacking real-world aerial imagery, leveraging the image-editing abilities of large-scale text-to-image models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Following prior work in robustness studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, we devise a simple and effective two-stage training strategy to use the synthetically generated data in training along with labeled data from different source domains to achieve complementary benefits.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We validate the effectiveness of our proposed framework on two benchmark datasets xBDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and SKAIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> images, obtaining significant improvements over a standard source-only baseline in both single-source (+9.8%, +25.2%) and multi-source (+5.33%, +29.13%) domain transfer settings.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Disaster Assessment using Satellite Images</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The task of image-based disaster assessment involves predicting the presence or extent of damage in a particular location by comparing pre and post disaster aerial or satellite imagery.
Fueled by the availability of paired pre- and post-disaster images captured from remote-sensing satellitesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, several methods have been proposed to identify the damageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, as well as to precisely localize the damage within the imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>. However, these approaches rely on labeled data for efficient performance, preventing their use in novel domains without incurring additional collection and annotation overheadsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. While domain adaptation methods exist to bridge this gapÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, they still need to access the post-disaster imagery which is difficult to acquire in a short window following a disaster. While prior works attempt generation of images using GANsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, they lack the ability to generate controllable synthetic data at scale.
Our work addresses these limitations by leveraging the advances in conditional text-to-image capabilities to generate large-scale synthetic supervision from low-resource target domains.
We also note that while there has been significant advances in unsupervised domain adaptation for image classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> and segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, they are typically not applicable to expert tasks like disaster assessment through aerial imagery, preventing their direct use or comparison for our problem.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Creating Synthetic Data from Generative Models</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Recent progress in the field of generative modeling has enabled the creation of diverse and realistic images conditioned on a variety of inputs such as textÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, layoutsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite> or semantic mapsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>. In particular, text-to-image synthesis enables creation of diverse visual content based on natural language promptsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. Recent works explored the use of leveraging the power of these models in generating synthetic supervision for various tasks including object recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, outlier detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> and long-tailed robustnessÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>.
Building upon this line of work, our work tackles image generation of post-disaster imagery in low-resource domains through localized editing of the corresponding pre-disaster images guided by suitable text prompts, showing an efficient way to improve domain robustness.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Setting</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.13779/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.17.8.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.14.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of the proposed synthetic data generation pipeline.<span id="S3.F2.14.7.7" class="ltx_text ltx_font_medium"> We first pass the pre-disaster image <math id="S3.F2.8.1.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.F2.8.1.1.m1.1b"><mi id="S3.F2.8.1.1.m1.1.1" xref="S3.F2.8.1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.1.1.m1.1c"><ci id="S3.F2.8.1.1.m1.1.1.cmml" xref="S3.F2.8.1.1.m1.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.1.1.m1.1d">U</annotation></semantics></math> from the target domain through a pre-trained VQGAN encoder followed by a tokenizer to compute the latent token, which is then masked using a binary mask. We use the MUSE model along with a suitable text prompt <math id="S3.F2.9.2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.F2.9.2.2.m2.1b"><mi id="S3.F2.9.2.2.m2.1.1" xref="S3.F2.9.2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F2.9.2.2.m2.1c"><ci id="S3.F2.9.2.2.m2.1.1.cmml" xref="S3.F2.9.2.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.2.2.m2.1d">T</annotation></semantics></math> to predict the output tokens from the masked tokens <math id="S3.F2.10.3.3.m3.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.F2.10.3.3.m3.1b"><mi id="S3.F2.10.3.3.m3.1.1" xref="S3.F2.10.3.3.m3.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.3.3.m3.1c"><ci id="S3.F2.10.3.3.m3.1.1.cmml" xref="S3.F2.10.3.3.m3.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.3.3.m3.1d">u</annotation></semantics></math>, which are then de-tokenized to generate the post-disaster image <math id="S3.F2.11.4.4.m4.1" class="ltx_Math" alttext="\hat{V}_{t}" display="inline"><semantics id="S3.F2.11.4.4.m4.1b"><msub id="S3.F2.11.4.4.m4.1.1" xref="S3.F2.11.4.4.m4.1.1.cmml"><mover accent="true" id="S3.F2.11.4.4.m4.1.1.2" xref="S3.F2.11.4.4.m4.1.1.2.cmml"><mi id="S3.F2.11.4.4.m4.1.1.2.2" xref="S3.F2.11.4.4.m4.1.1.2.2.cmml">V</mi><mo id="S3.F2.11.4.4.m4.1.1.2.1" xref="S3.F2.11.4.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.F2.11.4.4.m4.1.1.3" xref="S3.F2.11.4.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.11.4.4.m4.1c"><apply id="S3.F2.11.4.4.m4.1.1.cmml" xref="S3.F2.11.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F2.11.4.4.m4.1.1.1.cmml" xref="S3.F2.11.4.4.m4.1.1">subscript</csymbol><apply id="S3.F2.11.4.4.m4.1.1.2.cmml" xref="S3.F2.11.4.4.m4.1.1.2"><ci id="S3.F2.11.4.4.m4.1.1.2.1.cmml" xref="S3.F2.11.4.4.m4.1.1.2.1">^</ci><ci id="S3.F2.11.4.4.m4.1.1.2.2.cmml" xref="S3.F2.11.4.4.m4.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.F2.11.4.4.m4.1.1.3.cmml" xref="S3.F2.11.4.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.4.4.m4.1d">\hat{V}_{t}</annotation></semantics></math>. Our augmented dataset <math id="S3.F2.12.5.5.m5.1" class="ltx_Math" alttext="\hat{D}_{t}" display="inline"><semantics id="S3.F2.12.5.5.m5.1b"><msub id="S3.F2.12.5.5.m5.1.1" xref="S3.F2.12.5.5.m5.1.1.cmml"><mover accent="true" id="S3.F2.12.5.5.m5.1.1.2" xref="S3.F2.12.5.5.m5.1.1.2.cmml"><mi id="S3.F2.12.5.5.m5.1.1.2.2" xref="S3.F2.12.5.5.m5.1.1.2.2.cmml">D</mi><mo id="S3.F2.12.5.5.m5.1.1.2.1" xref="S3.F2.12.5.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="S3.F2.12.5.5.m5.1.1.3" xref="S3.F2.12.5.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.12.5.5.m5.1c"><apply id="S3.F2.12.5.5.m5.1.1.cmml" xref="S3.F2.12.5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.F2.12.5.5.m5.1.1.1.cmml" xref="S3.F2.12.5.5.m5.1.1">subscript</csymbol><apply id="S3.F2.12.5.5.m5.1.1.2.cmml" xref="S3.F2.12.5.5.m5.1.1.2"><ci id="S3.F2.12.5.5.m5.1.1.2.1.cmml" xref="S3.F2.12.5.5.m5.1.1.2.1">^</ci><ci id="S3.F2.12.5.5.m5.1.1.2.2.cmml" xref="S3.F2.12.5.5.m5.1.1.2.2">ğ·</ci></apply><ci id="S3.F2.12.5.5.m5.1.1.3.cmml" xref="S3.F2.12.5.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.5.5.m5.1d">\hat{D}_{t}</annotation></semantics></math> now contains the input image <math id="S3.F2.13.6.6.m6.1" class="ltx_Math" alttext="U_{t}" display="inline"><semantics id="S3.F2.13.6.6.m6.1b"><msub id="S3.F2.13.6.6.m6.1.1" xref="S3.F2.13.6.6.m6.1.1.cmml"><mi id="S3.F2.13.6.6.m6.1.1.2" xref="S3.F2.13.6.6.m6.1.1.2.cmml">U</mi><mi id="S3.F2.13.6.6.m6.1.1.3" xref="S3.F2.13.6.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.13.6.6.m6.1c"><apply id="S3.F2.13.6.6.m6.1.1.cmml" xref="S3.F2.13.6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.F2.13.6.6.m6.1.1.1.cmml" xref="S3.F2.13.6.6.m6.1.1">subscript</csymbol><ci id="S3.F2.13.6.6.m6.1.1.2.cmml" xref="S3.F2.13.6.6.m6.1.1.2">ğ‘ˆ</ci><ci id="S3.F2.13.6.6.m6.1.1.3.cmml" xref="S3.F2.13.6.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.6.6.m6.1d">U_{t}</annotation></semantics></math>, generated image <math id="S3.F2.14.7.7.m7.1" class="ltx_Math" alttext="\hat{V}_{t}" display="inline"><semantics id="S3.F2.14.7.7.m7.1b"><msub id="S3.F2.14.7.7.m7.1.1" xref="S3.F2.14.7.7.m7.1.1.cmml"><mover accent="true" id="S3.F2.14.7.7.m7.1.1.2" xref="S3.F2.14.7.7.m7.1.1.2.cmml"><mi id="S3.F2.14.7.7.m7.1.1.2.2" xref="S3.F2.14.7.7.m7.1.1.2.2.cmml">V</mi><mo id="S3.F2.14.7.7.m7.1.1.2.1" xref="S3.F2.14.7.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.F2.14.7.7.m7.1.1.3" xref="S3.F2.14.7.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.14.7.7.m7.1c"><apply id="S3.F2.14.7.7.m7.1.1.cmml" xref="S3.F2.14.7.7.m7.1.1"><csymbol cd="ambiguous" id="S3.F2.14.7.7.m7.1.1.1.cmml" xref="S3.F2.14.7.7.m7.1.1">subscript</csymbol><apply id="S3.F2.14.7.7.m7.1.1.2.cmml" xref="S3.F2.14.7.7.m7.1.1.2"><ci id="S3.F2.14.7.7.m7.1.1.2.1.cmml" xref="S3.F2.14.7.7.m7.1.1.2.1">^</ci><ci id="S3.F2.14.7.7.m7.1.1.2.2.cmml" xref="S3.F2.14.7.7.m7.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.F2.14.7.7.m7.1.1.3.cmml" xref="S3.F2.14.7.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.7.7.m7.1d">\hat{V}_{t}</annotation></semantics></math> and the binary label corresponding to the text prompt (indicating damage or no-damage).</span></span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">We now describe our problem setting of investigating domain robustness in image-based disaster assessment tasks. We denote our labeled source dataset as <math id="S3.SS1.p1.1.m1.3" class="ltx_Math" alttext="\mathcal{D}_{s}=\{U_{s}^{i},V_{s}^{i},y^{i}\}_{i=1}^{N_{s}}" display="inline"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml"><msub id="S3.SS1.p1.1.m1.3.3.5" xref="S3.SS1.p1.1.m1.3.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.3.3.5.2" xref="S3.SS1.p1.1.m1.3.3.5.2.cmml">ğ’Ÿ</mi><mi id="S3.SS1.p1.1.m1.3.3.5.3" xref="S3.SS1.p1.1.m1.3.3.5.3.cmml">s</mi></msub><mo id="S3.SS1.p1.1.m1.3.3.4" xref="S3.SS1.p1.1.m1.3.3.4.cmml">=</mo><msubsup id="S3.SS1.p1.1.m1.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.cmml"><mrow id="S3.SS1.p1.1.m1.3.3.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.3.3.3.3.3.3.4" xref="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml">{</mo><msubsup id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml">U</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml">s</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.SS1.p1.1.m1.3.3.3.3.3.3.5" xref="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.2.cmml">V</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.3.cmml">s</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.SS1.p1.1.m1.3.3.3.3.3.3.6" xref="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml">,</mo><msup id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.2" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.2.cmml">y</mi><mi id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.3" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.3.cmml">i</mi></msup><mo stretchy="false" id="S3.SS1.p1.1.m1.3.3.3.3.3.3.7" xref="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml">}</mo></mrow><mrow id="S3.SS1.p1.1.m1.3.3.3.3.5" xref="S3.SS1.p1.1.m1.3.3.3.3.5.cmml"><mi id="S3.SS1.p1.1.m1.3.3.3.3.5.2" xref="S3.SS1.p1.1.m1.3.3.3.3.5.2.cmml">i</mi><mo id="S3.SS1.p1.1.m1.3.3.3.3.5.1" xref="S3.SS1.p1.1.m1.3.3.3.3.5.1.cmml">=</mo><mn id="S3.SS1.p1.1.m1.3.3.3.3.5.3" xref="S3.SS1.p1.1.m1.3.3.3.3.5.3.cmml">1</mn></mrow><msub id="S3.SS1.p1.1.m1.3.3.3.5" xref="S3.SS1.p1.1.m1.3.3.3.5.cmml"><mi id="S3.SS1.p1.1.m1.3.3.3.5.2" xref="S3.SS1.p1.1.m1.3.3.3.5.2.cmml">N</mi><mi id="S3.SS1.p1.1.m1.3.3.3.5.3" xref="S3.SS1.p1.1.m1.3.3.3.5.3.cmml">s</mi></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><apply id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3"><eq id="S3.SS1.p1.1.m1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.4"></eq><apply id="S3.SS1.p1.1.m1.3.3.5.cmml" xref="S3.SS1.p1.1.m1.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.5.1.cmml" xref="S3.SS1.p1.1.m1.3.3.5">subscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.5.2.cmml" xref="S3.SS1.p1.1.m1.3.3.5.2">ğ’Ÿ</ci><ci id="S3.SS1.p1.1.m1.3.3.5.3.cmml" xref="S3.SS1.p1.1.m1.3.3.5.3">ğ‘ </ci></apply><apply id="S3.SS1.p1.1.m1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.3">superscript</csymbol><apply id="S3.SS1.p1.1.m1.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.3">subscript</csymbol><set id="S3.SS1.p1.1.m1.3.3.3.3.3.4.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3"><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2">ğ‘ˆ</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3">ğ‘ </ci></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.2">ğ‘‰</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.2.3">ğ‘ </ci></apply><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.2">ğ‘¦</ci><ci id="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.3.3.3.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p1.1.m1.3.3.3.3.5.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.5"><eq id="S3.SS1.p1.1.m1.3.3.3.3.5.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.5.1"></eq><ci id="S3.SS1.p1.1.m1.3.3.3.3.5.2.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.5.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p1.1.m1.3.3.3.3.5.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.3.5.3">1</cn></apply></apply><apply id="S3.SS1.p1.1.m1.3.3.3.5.cmml" xref="S3.SS1.p1.1.m1.3.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.3.5.1.cmml" xref="S3.SS1.p1.1.m1.3.3.3.5">subscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.3.5.2.cmml" xref="S3.SS1.p1.1.m1.3.3.3.5.2">ğ‘</ci><ci id="S3.SS1.p1.1.m1.3.3.3.5.3.cmml" xref="S3.SS1.p1.1.m1.3.3.3.5.3">ğ‘ </ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">\mathcal{D}_{s}=\{U_{s}^{i},V_{s}^{i},y^{i}\}_{i=1}^{N_{s}}</annotation></semantics></math>, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="U^{i}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">U</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘ˆ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">U^{i}</annotation></semantics></math> is the <span id="S3.SS1.p1.7.1" class="ltx_text ltx_font_italic">before</span> image (image captured before the disaster, also called pre-image), <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="V^{i}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ‘‰</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">V^{i}</annotation></semantics></math> is the <span id="S3.SS1.p1.7.2" class="ltx_text ltx_font_italic">after</span> image (image captured after the disaster, also called the post-image) along with a binary label <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="y^{i}\in\{0,1\}" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><msup id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml"><mi id="S3.SS1.p1.4.m4.2.3.2.2" xref="S3.SS1.p1.4.m4.2.3.2.2.cmml">y</mi><mi id="S3.SS1.p1.4.m4.2.3.2.3" xref="S3.SS1.p1.4.m4.2.3.2.3.cmml">i</mi></msup><mo id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.1" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">{</mo><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">0</mn><mo id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.3" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><in id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></in><apply id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.3.2.1.cmml" xref="S3.SS1.p1.4.m4.2.3.2">superscript</csymbol><ci id="S3.SS1.p1.4.m4.2.3.2.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2.2">ğ‘¦</ci><ci id="S3.SS1.p1.4.m4.2.3.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3.2.3">ğ‘–</ci></apply><set id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">0</cn><cn type="integer" id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">y^{i}\in\{0,1\}</annotation></semantics></math> indicating whether or not there is damage between the images due to a disaster. <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="N_{s}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">N_{s}</annotation></semantics></math> denotes the number of source images. In general, the pre and post images from the source images are paired, where the image collection is synchronized to capture the same location before and after the disasters, ensuring pixel-level correspondence between the images.
Furthermore, the target domain is denoted by <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{D}_{t}=\{U_{t}^{i}\}_{i=1}^{N_{t}}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><msub id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml">ğ’Ÿ</mi><mi id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">t</mi></msub><mo id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml"><mrow id="S3.SS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.2.cmml">U</mi><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.3.cmml">t</mi><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.6.m6.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p1.6.m6.1.1.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.6.m6.1.1.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS1.p1.6.m6.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.1.3.2.cmml">N</mi><mi id="S3.SS1.p1.6.m6.1.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.1.3.3.cmml">t</mi></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><eq id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"></eq><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">ğ’Ÿ</ci><ci id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">ğ‘¡</ci></apply><apply id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1">subscript</csymbol><set id="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1"><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.2">ğ‘ˆ</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.SS1.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3"><eq id="S3.SS1.p1.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS1.p1.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.p1.6.m6.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.3.2">ğ‘</ci><ci id="S3.SS1.p1.6.m6.1.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.3.3">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathcal{D}_{t}=\{U_{t}^{i}\}_{i=1}^{N_{t}}</annotation></semantics></math>, where <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="N_{t}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">N_{t}</annotation></semantics></math> denotes the number of (unlabeled and unpaired) target images.
Unique to our work, we assume a <span id="S3.SS1.p1.7.3" class="ltx_text ltx_font_italic">zero-shot</span> target setting, where we only have access to the pre-images from a new domain (which could be a new geographical location or new disaster type), and neither the post images nor the damage labels are available.
This setting is more realistic as capturing paired images immediately after a disaster and labeling them for damage incurs expensive time and manual annotation overhead in most cases, while pre-disaster images are naturally available through satellite footage.
Therefore, our main goal is to utilize labeled source images along with unlabeled pre-disaster target images to improve the performance on the target domain at test-time.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">While unsupervised domain adaptation (UDA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> has been a standard approach to address such domain shifts for disaster assessment, UDA methods still require access to paired pre and post-disaster data during training from both source and target domains. In contrast, we only require images captured before the disaster, and automatically generate synthetic post-disaster images to facilitate robust training.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Background: Text-to-Image Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In our work, we adopt the MUSEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> model for text-to-image generation, and we provide a brief overview of that framework here.
MUSE is a non-autoregressive model for text-conditioned synthesis capable of generating high-resolution images with fast inference speeds. In contrast to diffusion models requiring sequential decoding over several time-stepsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, MUSE adopts a purely transformer-based generation approach outlined in MaskGITÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> with improved inference speed enabled by parallel decoding. In particular, MUSE uses a pre-trained T5-XXL text encoderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> to first encode the text-prompt into a 4096-dimensional language embedding, while the input images are passed through a semantic tokenizer such as VQGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> consisting of an encoder and a quantization layer to map the image into a sequence of discrete tokens from a learned codebook, along with a decoder which maps the predicted or generated tokens back into the images.
To enable high resolution image synthesis, MUSE adopts the use of two VQGAN models with different downsampling ratio and spatial resolution of the tokens, along with two transformer modules called <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">base</span> and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">super-resolution</span> modules that generate low resolution and high resolution latent tokens respectively. During training, these tokens are trained using cross-entropy loss with a reference image, while during inference, the high resolution latent tokens are passed through the VQGAN decoder to generate images conditioned on the input text prompts. We refer the reader to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> for further details about the training and inference of MUSE generative models.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The use of MUSE in our pipeline as opposed to diffusion-based or auto-regressive based generative models is motivated by two favorable properties. Firstly, MUSE allows efficient mask-based editing capabilities based on its tokenized encoding and decoding-based architecture. Furthermore, MUSE is capable of high-resolution image synthesis with fast inference time and flexible latent token sizes, facilitating the generation of thousands of high quality synthetic post-disaster images conditioned on pre images at scale.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Generating Synthetic Data</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.12" class="ltx_p">We provide an overview of our proposed generation pipeline in <a href="#S3.F2" title="In 3.1 Problem Setting â€£ 3 Method â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
We first take the pre-disaster image from the target domain <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="U_{t}\in\mathbb{R}^{H\times W\times 3}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><msub id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">U</mi><mi id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p1.1.m1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.3.2" xref="S3.SS3.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.1.m1.1.1.3.3.1" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.1.m1.1.1.3.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.1.m1.1.1.3.3.1a" xref="S3.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS3.p1.1.m1.1.1.3.3.4" xref="S3.SS3.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><in id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></in><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">ğ‘ˆ</ci><ci id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">ğ‘¡</ci></apply><apply id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3"><times id="S3.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.2">ğ»</ci><ci id="S3.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.3">ğ‘Š</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">U_{t}\in\mathbb{R}^{H\times W\times 3}</annotation></semantics></math>, where <math id="S3.SS3.p1.2.m2.2" class="ltx_Math" alttext="H,W" display="inline"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.3.2" xref="S3.SS3.p1.2.m2.2.3.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">H</mi><mo id="S3.SS3.p1.2.m2.2.3.2.1" xref="S3.SS3.p1.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><list id="S3.SS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.p1.2.m2.2.3.2"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ»</ci><ci id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2">ğ‘Š</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">H,W</annotation></semantics></math> are the height and width of the image, and generate a corresponding binary mask <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{M}\in\mathbb{R}^{H\times W}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">â„³</mi><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.3.2" xref="S3.SS3.p1.3.m3.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.3.3.1" xref="S3.SS3.p1.3.m3.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.3.m3.1.1.3.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.3.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><in id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></in><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">â„³</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">â„</ci><apply id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3"><times id="S3.SS3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.2">ğ»</ci><ci id="S3.SS3.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3">ğ‘Š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathcal{M}\in\mathbb{R}^{H\times W}</annotation></semantics></math> with a center patch of size <math id="S3.SS3.p1.4.m4.2" class="ltx_Math" alttext="H^{\prime},W^{\prime}" display="inline"><semantics id="S3.SS3.p1.4.m4.2a"><mrow id="S3.SS3.p1.4.m4.2.2.2" xref="S3.SS3.p1.4.m4.2.2.3.cmml"><msup id="S3.SS3.p1.4.m4.1.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.1.1.2" xref="S3.SS3.p1.4.m4.1.1.1.1.2.cmml">H</mi><mo id="S3.SS3.p1.4.m4.1.1.1.1.3" xref="S3.SS3.p1.4.m4.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.SS3.p1.4.m4.2.2.2.3" xref="S3.SS3.p1.4.m4.2.2.3.cmml">,</mo><msup id="S3.SS3.p1.4.m4.2.2.2.2" xref="S3.SS3.p1.4.m4.2.2.2.2.cmml"><mi id="S3.SS3.p1.4.m4.2.2.2.2.2" xref="S3.SS3.p1.4.m4.2.2.2.2.2.cmml">W</mi><mo id="S3.SS3.p1.4.m4.2.2.2.2.3" xref="S3.SS3.p1.4.m4.2.2.2.2.3.cmml">â€²</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.2b"><list id="S3.SS3.p1.4.m4.2.2.3.cmml" xref="S3.SS3.p1.4.m4.2.2.2"><apply id="S3.SS3.p1.4.m4.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.2">ğ»</ci><ci id="S3.SS3.p1.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.1.1.3">â€²</ci></apply><apply id="S3.SS3.p1.4.m4.2.2.2.2.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.2.2.2.2.1.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2">superscript</csymbol><ci id="S3.SS3.p1.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2.2">ğ‘Š</ci><ci id="S3.SS3.p1.4.m4.2.2.2.2.3.cmml" xref="S3.SS3.p1.4.m4.2.2.2.2.3">â€²</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.2c">H^{\prime},W^{\prime}</annotation></semantics></math> as ones and rest as zeros, with <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="H^{\prime}&lt;H" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mrow id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><msup id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2.2" xref="S3.SS3.p1.5.m5.1.1.2.2.cmml">H</mi><mo id="S3.SS3.p1.5.m5.1.1.2.3" xref="S3.SS3.p1.5.m5.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS3.p1.5.m5.1.1.1" xref="S3.SS3.p1.5.m5.1.1.1.cmml">&lt;</mo><mi id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><lt id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1.1"></lt><apply id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.2.1.cmml" xref="S3.SS3.p1.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2.2">ğ»</ci><ci id="S3.SS3.p1.5.m5.1.1.2.3.cmml" xref="S3.SS3.p1.5.m5.1.1.2.3">â€²</ci></apply><ci id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">H^{\prime}&lt;H</annotation></semantics></math> and <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="W^{\prime}&lt;W" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mrow id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><msup id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2.2" xref="S3.SS3.p1.6.m6.1.1.2.2.cmml">W</mi><mo id="S3.SS3.p1.6.m6.1.1.2.3" xref="S3.SS3.p1.6.m6.1.1.2.3.cmml">â€²</mo></msup><mo id="S3.SS3.p1.6.m6.1.1.1" xref="S3.SS3.p1.6.m6.1.1.1.cmml">&lt;</mo><mi id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><lt id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1.1"></lt><apply id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.2.1.cmml" xref="S3.SS3.p1.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2.2">ğ‘Š</ci><ci id="S3.SS3.p1.6.m6.1.1.2.3.cmml" xref="S3.SS3.p1.6.m6.1.1.2.3">â€²</ci></apply><ci id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">W^{\prime}&lt;W</annotation></semantics></math>. Since the aerial images in the datasets we considered are typically centered around the subject, we align the center of the binary mask with the center of the image, helping us in directly editing the relevant subject in the image.
In practice, we randomly perturb the mask around the center by a factor <math id="S3.SS3.p1.7.m7.2" class="ltx_Math" alttext="(\delta_{x},\delta_{y})" display="inline"><semantics id="S3.SS3.p1.7.m7.2a"><mrow id="S3.SS3.p1.7.m7.2.2.2" xref="S3.SS3.p1.7.m7.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.7.m7.2.2.2.3" xref="S3.SS3.p1.7.m7.2.2.3.cmml">(</mo><msub id="S3.SS3.p1.7.m7.1.1.1.1" xref="S3.SS3.p1.7.m7.1.1.1.1.cmml"><mi id="S3.SS3.p1.7.m7.1.1.1.1.2" xref="S3.SS3.p1.7.m7.1.1.1.1.2.cmml">Î´</mi><mi id="S3.SS3.p1.7.m7.1.1.1.1.3" xref="S3.SS3.p1.7.m7.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS3.p1.7.m7.2.2.2.4" xref="S3.SS3.p1.7.m7.2.2.3.cmml">,</mo><msub id="S3.SS3.p1.7.m7.2.2.2.2" xref="S3.SS3.p1.7.m7.2.2.2.2.cmml"><mi id="S3.SS3.p1.7.m7.2.2.2.2.2" xref="S3.SS3.p1.7.m7.2.2.2.2.2.cmml">Î´</mi><mi id="S3.SS3.p1.7.m7.2.2.2.2.3" xref="S3.SS3.p1.7.m7.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS3.p1.7.m7.2.2.2.5" xref="S3.SS3.p1.7.m7.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.2b"><interval closure="open" id="S3.SS3.p1.7.m7.2.2.3.cmml" xref="S3.SS3.p1.7.m7.2.2.2"><apply id="S3.SS3.p1.7.m7.1.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.1.1.1.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m7.1.1.1.1.2.cmml" xref="S3.SS3.p1.7.m7.1.1.1.1.2">ğ›¿</ci><ci id="S3.SS3.p1.7.m7.1.1.1.1.3.cmml" xref="S3.SS3.p1.7.m7.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.SS3.p1.7.m7.2.2.2.2.cmml" xref="S3.SS3.p1.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m7.2.2.2.2.1.cmml" xref="S3.SS3.p1.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.7.m7.2.2.2.2.2.cmml" xref="S3.SS3.p1.7.m7.2.2.2.2.2">ğ›¿</ci><ci id="S3.SS3.p1.7.m7.2.2.2.2.3.cmml" xref="S3.SS3.p1.7.m7.2.2.2.2.3">ğ‘¦</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.2c">(\delta_{x},\delta_{y})</annotation></semantics></math> in both dimensions, where the perturbation factor <math id="S3.SS3.p1.8.m8.2" class="ltx_Math" alttext="\delta_{x}\sim\mathbb{U}[-W/16,W/16]" display="inline"><semantics id="S3.SS3.p1.8.m8.2a"><mrow id="S3.SS3.p1.8.m8.2.2" xref="S3.SS3.p1.8.m8.2.2.cmml"><msub id="S3.SS3.p1.8.m8.2.2.4" xref="S3.SS3.p1.8.m8.2.2.4.cmml"><mi id="S3.SS3.p1.8.m8.2.2.4.2" xref="S3.SS3.p1.8.m8.2.2.4.2.cmml">Î´</mi><mi id="S3.SS3.p1.8.m8.2.2.4.3" xref="S3.SS3.p1.8.m8.2.2.4.3.cmml">x</mi></msub><mo id="S3.SS3.p1.8.m8.2.2.3" xref="S3.SS3.p1.8.m8.2.2.3.cmml">âˆ¼</mo><mrow id="S3.SS3.p1.8.m8.2.2.2" xref="S3.SS3.p1.8.m8.2.2.2.cmml"><mi id="S3.SS3.p1.8.m8.2.2.2.4" xref="S3.SS3.p1.8.m8.2.2.2.4.cmml">ğ•Œ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.8.m8.2.2.2.3" xref="S3.SS3.p1.8.m8.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS3.p1.8.m8.2.2.2.2.2" xref="S3.SS3.p1.8.m8.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.8.m8.2.2.2.2.2.3" xref="S3.SS3.p1.8.m8.2.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p1.8.m8.1.1.1.1.1.1" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.8.m8.1.1.1.1.1.1a" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2.cmml">W</mi><mo id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.1" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.1.cmml">/</mo><mn id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3.cmml">16</mn></mrow></mrow><mo id="S3.SS3.p1.8.m8.2.2.2.2.2.4" xref="S3.SS3.p1.8.m8.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.8.m8.2.2.2.2.2.2" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.8.m8.2.2.2.2.2.2.2" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.2.cmml">W</mi><mo id="S3.SS3.p1.8.m8.2.2.2.2.2.2.1" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.1.cmml">/</mo><mn id="S3.SS3.p1.8.m8.2.2.2.2.2.2.3" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.3.cmml">16</mn></mrow><mo stretchy="false" id="S3.SS3.p1.8.m8.2.2.2.2.2.5" xref="S3.SS3.p1.8.m8.2.2.2.2.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.2b"><apply id="S3.SS3.p1.8.m8.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2"><csymbol cd="latexml" id="S3.SS3.p1.8.m8.2.2.3.cmml" xref="S3.SS3.p1.8.m8.2.2.3">similar-to</csymbol><apply id="S3.SS3.p1.8.m8.2.2.4.cmml" xref="S3.SS3.p1.8.m8.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m8.2.2.4.1.cmml" xref="S3.SS3.p1.8.m8.2.2.4">subscript</csymbol><ci id="S3.SS3.p1.8.m8.2.2.4.2.cmml" xref="S3.SS3.p1.8.m8.2.2.4.2">ğ›¿</ci><ci id="S3.SS3.p1.8.m8.2.2.4.3.cmml" xref="S3.SS3.p1.8.m8.2.2.4.3">ğ‘¥</ci></apply><apply id="S3.SS3.p1.8.m8.2.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2.2"><times id="S3.SS3.p1.8.m8.2.2.2.3.cmml" xref="S3.SS3.p1.8.m8.2.2.2.3"></times><ci id="S3.SS3.p1.8.m8.2.2.2.4.cmml" xref="S3.SS3.p1.8.m8.2.2.2.4">ğ•Œ</ci><interval closure="closed" id="S3.SS3.p1.8.m8.2.2.2.2.3.cmml" xref="S3.SS3.p1.8.m8.2.2.2.2.2"><apply id="S3.SS3.p1.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1"><minus id="S3.SS3.p1.8.m8.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1"></minus><apply id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2"><divide id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.1"></divide><ci id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.8.m8.1.1.1.1.1.1.2.3">16</cn></apply></apply><apply id="S3.SS3.p1.8.m8.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2"><divide id="S3.SS3.p1.8.m8.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.1"></divide><ci id="S3.SS3.p1.8.m8.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS3.p1.8.m8.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.8.m8.2.2.2.2.2.2.3">16</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.2c">\delta_{x}\sim\mathbb{U}[-W/16,W/16]</annotation></semantics></math> and <math id="S3.SS3.p1.9.m9.2" class="ltx_Math" alttext="\delta_{y}\sim\mathbb{U}[-H/16,H/16]" display="inline"><semantics id="S3.SS3.p1.9.m9.2a"><mrow id="S3.SS3.p1.9.m9.2.2" xref="S3.SS3.p1.9.m9.2.2.cmml"><msub id="S3.SS3.p1.9.m9.2.2.4" xref="S3.SS3.p1.9.m9.2.2.4.cmml"><mi id="S3.SS3.p1.9.m9.2.2.4.2" xref="S3.SS3.p1.9.m9.2.2.4.2.cmml">Î´</mi><mi id="S3.SS3.p1.9.m9.2.2.4.3" xref="S3.SS3.p1.9.m9.2.2.4.3.cmml">y</mi></msub><mo id="S3.SS3.p1.9.m9.2.2.3" xref="S3.SS3.p1.9.m9.2.2.3.cmml">âˆ¼</mo><mrow id="S3.SS3.p1.9.m9.2.2.2" xref="S3.SS3.p1.9.m9.2.2.2.cmml"><mi id="S3.SS3.p1.9.m9.2.2.2.4" xref="S3.SS3.p1.9.m9.2.2.2.4.cmml">ğ•Œ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.9.m9.2.2.2.3" xref="S3.SS3.p1.9.m9.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS3.p1.9.m9.2.2.2.2.2" xref="S3.SS3.p1.9.m9.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.9.m9.2.2.2.2.2.3" xref="S3.SS3.p1.9.m9.2.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p1.9.m9.1.1.1.1.1.1" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.9.m9.1.1.1.1.1.1a" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.2.cmml">H</mi><mo id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.1" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.1.cmml">/</mo><mn id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.3.cmml">16</mn></mrow></mrow><mo id="S3.SS3.p1.9.m9.2.2.2.2.2.4" xref="S3.SS3.p1.9.m9.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.9.m9.2.2.2.2.2.2" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.9.m9.2.2.2.2.2.2.2" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.2.cmml">H</mi><mo id="S3.SS3.p1.9.m9.2.2.2.2.2.2.1" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.1.cmml">/</mo><mn id="S3.SS3.p1.9.m9.2.2.2.2.2.2.3" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.3.cmml">16</mn></mrow><mo stretchy="false" id="S3.SS3.p1.9.m9.2.2.2.2.2.5" xref="S3.SS3.p1.9.m9.2.2.2.2.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.2b"><apply id="S3.SS3.p1.9.m9.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2"><csymbol cd="latexml" id="S3.SS3.p1.9.m9.2.2.3.cmml" xref="S3.SS3.p1.9.m9.2.2.3">similar-to</csymbol><apply id="S3.SS3.p1.9.m9.2.2.4.cmml" xref="S3.SS3.p1.9.m9.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.2.2.4.1.cmml" xref="S3.SS3.p1.9.m9.2.2.4">subscript</csymbol><ci id="S3.SS3.p1.9.m9.2.2.4.2.cmml" xref="S3.SS3.p1.9.m9.2.2.4.2">ğ›¿</ci><ci id="S3.SS3.p1.9.m9.2.2.4.3.cmml" xref="S3.SS3.p1.9.m9.2.2.4.3">ğ‘¦</ci></apply><apply id="S3.SS3.p1.9.m9.2.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2.2"><times id="S3.SS3.p1.9.m9.2.2.2.3.cmml" xref="S3.SS3.p1.9.m9.2.2.2.3"></times><ci id="S3.SS3.p1.9.m9.2.2.2.4.cmml" xref="S3.SS3.p1.9.m9.2.2.2.4">ğ•Œ</ci><interval closure="closed" id="S3.SS3.p1.9.m9.2.2.2.2.3.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2.2"><apply id="S3.SS3.p1.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1"><minus id="S3.SS3.p1.9.m9.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1"></minus><apply id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2"><divide id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.1"></divide><ci id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.2">ğ»</ci><cn type="integer" id="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.9.m9.1.1.1.1.1.1.2.3">16</cn></apply></apply><apply id="S3.SS3.p1.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2"><divide id="S3.SS3.p1.9.m9.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.1"></divide><ci id="S3.SS3.p1.9.m9.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.2">ğ»</ci><cn type="integer" id="S3.SS3.p1.9.m9.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.9.m9.2.2.2.2.2.2.3">16</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.2c">\delta_{y}\sim\mathbb{U}[-H/16,H/16]</annotation></semantics></math> is sampled from the uniform distribution. So our patch with all ones in the binary mask is centered around <math id="S3.SS3.p1.10.m10.2" class="ltx_Math" alttext="(H/2+\delta_{y},W/2+\delta_{x})" display="inline"><semantics id="S3.SS3.p1.10.m10.2a"><mrow id="S3.SS3.p1.10.m10.2.2.2" xref="S3.SS3.p1.10.m10.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m10.2.2.2.3" xref="S3.SS3.p1.10.m10.2.2.3.cmml">(</mo><mrow id="S3.SS3.p1.10.m10.1.1.1.1" xref="S3.SS3.p1.10.m10.1.1.1.1.cmml"><mrow id="S3.SS3.p1.10.m10.1.1.1.1.2" xref="S3.SS3.p1.10.m10.1.1.1.1.2.cmml"><mi id="S3.SS3.p1.10.m10.1.1.1.1.2.2" xref="S3.SS3.p1.10.m10.1.1.1.1.2.2.cmml">H</mi><mo id="S3.SS3.p1.10.m10.1.1.1.1.2.1" xref="S3.SS3.p1.10.m10.1.1.1.1.2.1.cmml">/</mo><mn id="S3.SS3.p1.10.m10.1.1.1.1.2.3" xref="S3.SS3.p1.10.m10.1.1.1.1.2.3.cmml">2</mn></mrow><mo id="S3.SS3.p1.10.m10.1.1.1.1.1" xref="S3.SS3.p1.10.m10.1.1.1.1.1.cmml">+</mo><msub id="S3.SS3.p1.10.m10.1.1.1.1.3" xref="S3.SS3.p1.10.m10.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.10.m10.1.1.1.1.3.2" xref="S3.SS3.p1.10.m10.1.1.1.1.3.2.cmml">Î´</mi><mi id="S3.SS3.p1.10.m10.1.1.1.1.3.3" xref="S3.SS3.p1.10.m10.1.1.1.1.3.3.cmml">y</mi></msub></mrow><mo id="S3.SS3.p1.10.m10.2.2.2.4" xref="S3.SS3.p1.10.m10.2.2.3.cmml">,</mo><mrow id="S3.SS3.p1.10.m10.2.2.2.2" xref="S3.SS3.p1.10.m10.2.2.2.2.cmml"><mrow id="S3.SS3.p1.10.m10.2.2.2.2.2" xref="S3.SS3.p1.10.m10.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.10.m10.2.2.2.2.2.2" xref="S3.SS3.p1.10.m10.2.2.2.2.2.2.cmml">W</mi><mo id="S3.SS3.p1.10.m10.2.2.2.2.2.1" xref="S3.SS3.p1.10.m10.2.2.2.2.2.1.cmml">/</mo><mn id="S3.SS3.p1.10.m10.2.2.2.2.2.3" xref="S3.SS3.p1.10.m10.2.2.2.2.2.3.cmml">2</mn></mrow><mo id="S3.SS3.p1.10.m10.2.2.2.2.1" xref="S3.SS3.p1.10.m10.2.2.2.2.1.cmml">+</mo><msub id="S3.SS3.p1.10.m10.2.2.2.2.3" xref="S3.SS3.p1.10.m10.2.2.2.2.3.cmml"><mi id="S3.SS3.p1.10.m10.2.2.2.2.3.2" xref="S3.SS3.p1.10.m10.2.2.2.2.3.2.cmml">Î´</mi><mi id="S3.SS3.p1.10.m10.2.2.2.2.3.3" xref="S3.SS3.p1.10.m10.2.2.2.2.3.3.cmml">x</mi></msub></mrow><mo stretchy="false" id="S3.SS3.p1.10.m10.2.2.2.5" xref="S3.SS3.p1.10.m10.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.2b"><interval closure="open" id="S3.SS3.p1.10.m10.2.2.3.cmml" xref="S3.SS3.p1.10.m10.2.2.2"><apply id="S3.SS3.p1.10.m10.1.1.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1"><plus id="S3.SS3.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.1"></plus><apply id="S3.SS3.p1.10.m10.1.1.1.1.2.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.2"><divide id="S3.SS3.p1.10.m10.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.2.1"></divide><ci id="S3.SS3.p1.10.m10.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.2.2">ğ»</ci><cn type="integer" id="S3.SS3.p1.10.m10.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.2.3">2</cn></apply><apply id="S3.SS3.p1.10.m10.1.1.1.1.3.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m10.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.10.m10.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.3.2">ğ›¿</ci><ci id="S3.SS3.p1.10.m10.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.10.m10.1.1.1.1.3.3">ğ‘¦</ci></apply></apply><apply id="S3.SS3.p1.10.m10.2.2.2.2.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2"><plus id="S3.SS3.p1.10.m10.2.2.2.2.1.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.1"></plus><apply id="S3.SS3.p1.10.m10.2.2.2.2.2.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.2"><divide id="S3.SS3.p1.10.m10.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.2.1"></divide><ci id="S3.SS3.p1.10.m10.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S3.SS3.p1.10.m10.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.2.3">2</cn></apply><apply id="S3.SS3.p1.10.m10.2.2.2.2.3.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m10.2.2.2.2.3.1.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.3">subscript</csymbol><ci id="S3.SS3.p1.10.m10.2.2.2.2.3.2.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.3.2">ğ›¿</ci><ci id="S3.SS3.p1.10.m10.2.2.2.2.3.3.cmml" xref="S3.SS3.p1.10.m10.2.2.2.2.3.3">ğ‘¥</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.2c">(H/2+\delta_{y},W/2+\delta_{x})</annotation></semantics></math> with respect to the input image <math id="S3.SS3.p1.11.m11.1" class="ltx_Math" alttext="U_{t}" display="inline"><semantics id="S3.SS3.p1.11.m11.1a"><msub id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml"><mi id="S3.SS3.p1.11.m11.1.1.2" xref="S3.SS3.p1.11.m11.1.1.2.cmml">U</mi><mi id="S3.SS3.p1.11.m11.1.1.3" xref="S3.SS3.p1.11.m11.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><apply id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS3.p1.11.m11.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1.2">ğ‘ˆ</ci><ci id="S3.SS3.p1.11.m11.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">U_{t}</annotation></semantics></math> and mask <math id="S3.SS3.p1.12.m12.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.SS3.p1.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.1b"><ci id="S3.SS3.p1.12.m12.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.1c">\mathcal{M}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.7" class="ltx_p">We then pass the target image <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="U_{t}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">U</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ‘ˆ</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">U_{t}</annotation></semantics></math> through the VQGAN encoder to compute the latent tokens for the image, and downsample the binary mask <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{M}</annotation></semantics></math> to match the spatial resolution of the latent tokens (which is <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="1/16^{\text{th}}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mn id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">1</mn><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">/</mo><msup id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><mn id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml">16</mn><mtext id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3a.cmml">th</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><divide id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></divide><cn type="integer" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">1</cn><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">16</cn><ci id="S3.SS3.p2.3.m3.1.1.3.3a.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3"><mtext mathsize="70%" id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3">th</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">1/16^{\text{th}}</annotation></semantics></math> and <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="1/8^{\text{th}}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mn id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">1</mn><mo id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">/</mo><msup id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml"><mn id="S3.SS3.p2.4.m4.1.1.3.2" xref="S3.SS3.p2.4.m4.1.1.3.2.cmml">8</mn><mtext id="S3.SS3.p2.4.m4.1.1.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3a.cmml">th</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><divide id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"></divide><cn type="integer" id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">1</cn><apply id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.2">8</cn><ci id="S3.SS3.p2.4.m4.1.1.3.3a.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3"><mtext mathsize="70%" id="S3.SS3.p2.4.m4.1.1.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3">th</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">1/8^{\text{th}}</annotation></semantics></math> of the original image size for the low-resolution and high-resolution pipelines respectively). Subsequently, we multiply the downsampled binary mask <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{M}^{\prime}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><msup id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">â„³</mi><mo id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">â„³</ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\mathcal{M}^{\prime}</annotation></semantics></math> with the latent tokens at each pixel location to get a masked token embedding <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="u_{t}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">u</mi><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">ğ‘¢</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">u_{t}</annotation></semantics></math>.
The masked-latent token embeddings along with the text-embedding of the input prompt are then passed through a series of cross-attention layers pre-trained in MUSE to predict the output tokens from the predicted latent tokens. The outputs are then passed through the decoder layer of the VQGAN resulting in the output image <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="\hat{V}_{t}" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><msub id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml"><mover accent="true" id="S3.SS3.p2.7.m7.1.1.2" xref="S3.SS3.p2.7.m7.1.1.2.cmml"><mi id="S3.SS3.p2.7.m7.1.1.2.2" xref="S3.SS3.p2.7.m7.1.1.2.2.cmml">V</mi><mo id="S3.SS3.p2.7.m7.1.1.2.1" xref="S3.SS3.p2.7.m7.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p2.7.m7.1.1.3" xref="S3.SS3.p2.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><apply id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">subscript</csymbol><apply id="S3.SS3.p2.7.m7.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2"><ci id="S3.SS3.p2.7.m7.1.1.2.1.cmml" xref="S3.SS3.p2.7.m7.1.1.2.1">^</ci><ci id="S3.SS3.p2.7.m7.1.1.2.2.cmml" xref="S3.SS3.p2.7.m7.1.1.2.2">ğ‘‰</ci></apply><ci id="S3.SS3.p2.7.m7.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">\hat{V}_{t}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We generate four output images for each pre-image and prompt pair, and pick the best one using the ranking obtained by CLIP similarity scoreÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> between the input prompt and the generated image. We repeat this process for every image in the target dataset, creating a synthetic dataset <math id="S3.SS3.p3.1.m1.3" class="ltx_Math" alttext="\hat{\mathcal{D}}_{t}=\{U_{t}^{i},\hat{V}_{t}^{i},\hat{y}^{i}\}_{i=1}^{N_{t}}" display="inline"><semantics id="S3.SS3.p3.1.m1.3a"><mrow id="S3.SS3.p3.1.m1.3.3" xref="S3.SS3.p3.1.m1.3.3.cmml"><msub id="S3.SS3.p3.1.m1.3.3.5" xref="S3.SS3.p3.1.m1.3.3.5.cmml"><mover accent="true" id="S3.SS3.p3.1.m1.3.3.5.2" xref="S3.SS3.p3.1.m1.3.3.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.1.m1.3.3.5.2.2" xref="S3.SS3.p3.1.m1.3.3.5.2.2.cmml">ğ’Ÿ</mi><mo id="S3.SS3.p3.1.m1.3.3.5.2.1" xref="S3.SS3.p3.1.m1.3.3.5.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.1.m1.3.3.5.3" xref="S3.SS3.p3.1.m1.3.3.5.3.cmml">t</mi></msub><mo id="S3.SS3.p3.1.m1.3.3.4" xref="S3.SS3.p3.1.m1.3.3.4.cmml">=</mo><msubsup id="S3.SS3.p3.1.m1.3.3.3" xref="S3.SS3.p3.1.m1.3.3.3.cmml"><mrow id="S3.SS3.p3.1.m1.3.3.3.3.3.3" xref="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS3.p3.1.m1.3.3.3.3.3.3.4" xref="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml">{</mo><msubsup id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.2" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.2.cmml">U</mi><mi id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.3" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.3.cmml">t</mi><mi id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo id="S3.SS3.p3.1.m1.3.3.3.3.3.3.5" xref="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.2.cmml">V</mi><mo id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.1" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.3.cmml">t</mi><mi id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.SS3.p3.1.m1.3.3.3.3.3.3.6" xref="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml">,</mo><msup id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.cmml"><mover accent="true" id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.cmml"><mi id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.2" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.2.cmml">y</mi><mo id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.1" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.1.cmml">^</mo></mover><mi id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.3" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.3.cmml">i</mi></msup><mo stretchy="false" id="S3.SS3.p3.1.m1.3.3.3.3.3.3.7" xref="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml">}</mo></mrow><mrow id="S3.SS3.p3.1.m1.3.3.3.3.5" xref="S3.SS3.p3.1.m1.3.3.3.3.5.cmml"><mi id="S3.SS3.p3.1.m1.3.3.3.3.5.2" xref="S3.SS3.p3.1.m1.3.3.3.3.5.2.cmml">i</mi><mo id="S3.SS3.p3.1.m1.3.3.3.3.5.1" xref="S3.SS3.p3.1.m1.3.3.3.3.5.1.cmml">=</mo><mn id="S3.SS3.p3.1.m1.3.3.3.3.5.3" xref="S3.SS3.p3.1.m1.3.3.3.3.5.3.cmml">1</mn></mrow><msub id="S3.SS3.p3.1.m1.3.3.3.5" xref="S3.SS3.p3.1.m1.3.3.3.5.cmml"><mi id="S3.SS3.p3.1.m1.3.3.3.5.2" xref="S3.SS3.p3.1.m1.3.3.3.5.2.cmml">N</mi><mi id="S3.SS3.p3.1.m1.3.3.3.5.3" xref="S3.SS3.p3.1.m1.3.3.3.5.3.cmml">t</mi></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.3b"><apply id="S3.SS3.p3.1.m1.3.3.cmml" xref="S3.SS3.p3.1.m1.3.3"><eq id="S3.SS3.p3.1.m1.3.3.4.cmml" xref="S3.SS3.p3.1.m1.3.3.4"></eq><apply id="S3.SS3.p3.1.m1.3.3.5.cmml" xref="S3.SS3.p3.1.m1.3.3.5"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.5.1.cmml" xref="S3.SS3.p3.1.m1.3.3.5">subscript</csymbol><apply id="S3.SS3.p3.1.m1.3.3.5.2.cmml" xref="S3.SS3.p3.1.m1.3.3.5.2"><ci id="S3.SS3.p3.1.m1.3.3.5.2.1.cmml" xref="S3.SS3.p3.1.m1.3.3.5.2.1">^</ci><ci id="S3.SS3.p3.1.m1.3.3.5.2.2.cmml" xref="S3.SS3.p3.1.m1.3.3.5.2.2">ğ’Ÿ</ci></apply><ci id="S3.SS3.p3.1.m1.3.3.5.3.cmml" xref="S3.SS3.p3.1.m1.3.3.5.3">ğ‘¡</ci></apply><apply id="S3.SS3.p3.1.m1.3.3.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.3.4.cmml" xref="S3.SS3.p3.1.m1.3.3.3">superscript</csymbol><apply id="S3.SS3.p3.1.m1.3.3.3.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.3.3.4.cmml" xref="S3.SS3.p3.1.m1.3.3.3">subscript</csymbol><set id="S3.SS3.p3.1.m1.3.3.3.3.3.4.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3"><apply id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.2">ğ‘ˆ</ci><ci id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><ci id="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2"><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.1">^</ci><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.2.2">ğ‘‰</ci></apply><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.2.3">ğ‘¡</ci></apply><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.1.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3">superscript</csymbol><apply id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2"><ci id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.1.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.1">^</ci><ci id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.2.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.2.2">ğ‘¦</ci></apply><ci id="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.3.3.3.3">ğ‘–</ci></apply></set><apply id="S3.SS3.p3.1.m1.3.3.3.3.5.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.5"><eq id="S3.SS3.p3.1.m1.3.3.3.3.5.1.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.5.1"></eq><ci id="S3.SS3.p3.1.m1.3.3.3.3.5.2.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.5.2">ğ‘–</ci><cn type="integer" id="S3.SS3.p3.1.m1.3.3.3.3.5.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3.3.5.3">1</cn></apply></apply><apply id="S3.SS3.p3.1.m1.3.3.3.5.cmml" xref="S3.SS3.p3.1.m1.3.3.3.5"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.3.3.3.5.1.cmml" xref="S3.SS3.p3.1.m1.3.3.3.5">subscript</csymbol><ci id="S3.SS3.p3.1.m1.3.3.3.5.2.cmml" xref="S3.SS3.p3.1.m1.3.3.3.5.2">ğ‘</ci><ci id="S3.SS3.p3.1.m1.3.3.3.5.3.cmml" xref="S3.SS3.p3.1.m1.3.3.3.5.3">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.3c">\hat{\mathcal{D}}_{t}=\{U_{t}^{i},\hat{V}_{t}^{i},\hat{y}^{i}\}_{i=1}^{N_{t}}</annotation></semantics></math> of pre and post disaster images from the target domain.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Prompt Pool for Generation</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">A major advantage of generating synthetic images is avoiding the need for manual annotation for unlabeled domains, as the labels can be directly derived from the corresponding prompts.
In our setting with binary labels indicating damaged or not damaged buildings or locations, we choose the prompts to reflect these criterion. For example, to create synthetic images for scenes damaged by <span id="S3.SS3.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">hurricane</span> disaster, we use prompts such as <span id="S3.SS3.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">An aerial view of a house damaged due to a hurricane</span> or <span id="S3.SS3.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">A satellite image of a building that was destroyed by a hurricane</span> and assign the label 1. Alternatively, for generating images which have no damage, we create a prompt pool indicating images which are undamaged (for example, <span id="S3.SS3.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">A satellite image of a building</span>) and assign the generated images with a label of 0. We list the pool of prompts adopted in our work in <a href="#S3.F3" title="In Prompt Pool for Generation â€£ 3.3 Generating Synthetic Data â€£ 3 Method â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="In Prompt Pool for Generation â€£ 3.3 Generating Synthetic Data â€£ 3 Method â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS3.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS0.Px1.p2.pic1" class="ltx_picture" height="212.72" overflow="visible" version="1.1" width="600"><g transform="translate(0,212.72) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FF4040" fill-opacity="1.0"><path d="M 0 5.91 L 0 206.81 C 0 210.08 2.64 212.72 5.91 212.72 L 594.09 212.72 C 597.36 212.72 600 210.08 600 206.81 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFF7F7" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 191.45 L 598.03 191.45 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.88 197.36)"><foreignObject width="582.23" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S3.SS3.SSS0.Px1.p2.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:420.8pt;">
<span id="S3.SS3.SSS0.Px1.p2.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p2.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">SKAI Dataset</span></span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.88 8.88)"><foreignObject width="582.23" height="175.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:420.8pt;">
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Damaged Set</span></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.2" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.2.1" class="ltx_emph ltx_font_italic">An aerial view of a house damaged due to a hurricane.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.3" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.3.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a building destroyed by a hurricane.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.4" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.4.1" class="ltx_emph ltx_font_italic">A top-down view of a house damaged by a hurricane.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.5" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.5.1" class="ltx_emph ltx_font_italic">A satellite image of a building destroyed by a hurricane.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.6" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.6.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a building damaged by a hurricane.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.7" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.7.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Undamaged Set</span></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.8" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.8.1" class="ltx_emph ltx_font_italic">A satellite image of a house covered by trees.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.9" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.9.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a house surrounded by trees.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.10" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.10.1" class="ltx_emph ltx_font_italic">A top-down view of a house under tree shade.</em></span>
<span id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.11" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p2.pic1.2.2.2.1.1.11.1" class="ltx_emph ltx_font_italic">An aerial view of an intact house under tree shade.</em></span>
</span></foreignObject></g></g></svg>
</div>
<div id="S3.SS3.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<div id="S3.SS3.SSS0.Px1.p3.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:223.6pt;">
<figure id="S3.F3" class="ltx_figure">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Prompt Pool for SKAI</span></figcaption>
</figure>
</div>
</div>
<div id="S3.SS3.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS0.Px1.p4.pic1" class="ltx_picture" height="315.04" overflow="visible" version="1.1" width="600"><g transform="translate(0,315.04) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#4040FF" fill-opacity="1.0"><path d="M 0 5.91 L 0 309.13 C 0 312.39 2.64 315.04 5.91 315.04 L 594.09 315.04 C 597.36 315.04 600 312.39 600 309.13 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F7F7FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 293.77 L 598.03 293.77 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.88 299.68)"><foreignObject width="582.23" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S3.SS3.SSS0.Px1.p4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:420.8pt;">
<span id="S3.SS3.SSS0.Px1.p4.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p4.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">xBD Dataset</span></span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.88 8.88)"><foreignObject width="582.23" height="277.97" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:420.8pt;">
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.1" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Moore Tornado</span></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.2" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.2.1" class="ltx_emph ltx_font_italic">An aerial view of a house damaged due to a tornado.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.3" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.3.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a building destroyed by a tornado.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.4" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.4.1" class="ltx_emph ltx_font_italic">A top-down view of a house damaged by a tornado.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.5" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.5.1" class="ltx_emph ltx_font_italic">A satellite image of a building destroyed by a tornado.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.6" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.6.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a building damaged by a tornado.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.7" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.7.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Nepal Floods</span></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.8" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.8.1" class="ltx_emph ltx_font_italic">An aerial view of houses surrounded by a flood.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.9" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.9.1" class="ltx_emph ltx_font_italic">A top-down view of houses damaged by floods.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.10" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.10.1" class="ltx_emph ltx_font_italic">A top-down view of a house damaged by floods inundated in water.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.11" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.11.1" class="ltx_emph ltx_font_italic">A satellite image of a building destroyed by a flood surrounded by water.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.12" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.12.1" class="ltx_emph ltx_font_italic">A satellite image of houses that was destroyed by a flood surrounded by water and trees.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.13" class="ltx_p"><span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.13.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">Portugal Wildfire</span></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.14" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.14.1" class="ltx_emph ltx_font_italic">An aerial view of forest land after it is torched by a wildfire.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.15" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.15.1" class="ltx_emph ltx_font_italic">An aerial view of buildings after a wildfire.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.16" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.16.1" class="ltx_emph ltx_font_italic">An aerial image of forest land scorched by a wildfire.</em></span>
<span id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.17" class="ltx_p"><em id="S3.SS3.SSS0.Px1.p4.pic1.2.2.2.1.1.17.1" class="ltx_emph ltx_font_italic">A birdâ€™s-eye view of a forest region with completely scorched trees.</em></span>
</span></foreignObject></g></g></svg>
</div>
<div id="S3.SS3.SSS0.Px1.p5" class="ltx_para ltx_noindent">
<div id="S3.SS3.SSS0.Px1.p5.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:223.6pt;">
<figure id="S3.F4" class="ltx_figure">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Prompt Pool for xBD</span></figcaption>
</figure>
</div>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training using Synthetic Data</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.11" class="ltx_p">Following prior work in disaster assessment taskÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, we adopt a siamese network with shared transformer backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> for training. Specifically, we pass both pre and post images <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">U</annotation></semantics></math> and <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">V</annotation></semantics></math> using parameter-shared transformer backbones <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{E}_{u}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">â„°</mi><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">â„°</ci><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\mathcal{E}_{u}</annotation></semantics></math> and <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{E}_{v}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">â„°</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">â„°</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\mathcal{E}_{v}</annotation></semantics></math>, resulting in feature embedding <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="f_{u}=\mathcal{E}_{u}(U)" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mrow id="S3.SS4.p1.5.m5.1.2" xref="S3.SS4.p1.5.m5.1.2.cmml"><msub id="S3.SS4.p1.5.m5.1.2.2" xref="S3.SS4.p1.5.m5.1.2.2.cmml"><mi id="S3.SS4.p1.5.m5.1.2.2.2" xref="S3.SS4.p1.5.m5.1.2.2.2.cmml">f</mi><mi id="S3.SS4.p1.5.m5.1.2.2.3" xref="S3.SS4.p1.5.m5.1.2.2.3.cmml">u</mi></msub><mo id="S3.SS4.p1.5.m5.1.2.1" xref="S3.SS4.p1.5.m5.1.2.1.cmml">=</mo><mrow id="S3.SS4.p1.5.m5.1.2.3" xref="S3.SS4.p1.5.m5.1.2.3.cmml"><msub id="S3.SS4.p1.5.m5.1.2.3.2" xref="S3.SS4.p1.5.m5.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.2.3.2.2" xref="S3.SS4.p1.5.m5.1.2.3.2.2.cmml">â„°</mi><mi id="S3.SS4.p1.5.m5.1.2.3.2.3" xref="S3.SS4.p1.5.m5.1.2.3.2.3.cmml">u</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p1.5.m5.1.2.3.1" xref="S3.SS4.p1.5.m5.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS4.p1.5.m5.1.2.3.3.2" xref="S3.SS4.p1.5.m5.1.2.3.cmml"><mo stretchy="false" id="S3.SS4.p1.5.m5.1.2.3.3.2.1" xref="S3.SS4.p1.5.m5.1.2.3.cmml">(</mo><mi id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">U</mi><mo stretchy="false" id="S3.SS4.p1.5.m5.1.2.3.3.2.2" xref="S3.SS4.p1.5.m5.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><apply id="S3.SS4.p1.5.m5.1.2.cmml" xref="S3.SS4.p1.5.m5.1.2"><eq id="S3.SS4.p1.5.m5.1.2.1.cmml" xref="S3.SS4.p1.5.m5.1.2.1"></eq><apply id="S3.SS4.p1.5.m5.1.2.2.cmml" xref="S3.SS4.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.2.2.1.cmml" xref="S3.SS4.p1.5.m5.1.2.2">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.2.2.2.cmml" xref="S3.SS4.p1.5.m5.1.2.2.2">ğ‘“</ci><ci id="S3.SS4.p1.5.m5.1.2.2.3.cmml" xref="S3.SS4.p1.5.m5.1.2.2.3">ğ‘¢</ci></apply><apply id="S3.SS4.p1.5.m5.1.2.3.cmml" xref="S3.SS4.p1.5.m5.1.2.3"><times id="S3.SS4.p1.5.m5.1.2.3.1.cmml" xref="S3.SS4.p1.5.m5.1.2.3.1"></times><apply id="S3.SS4.p1.5.m5.1.2.3.2.cmml" xref="S3.SS4.p1.5.m5.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m5.1.2.3.2.1.cmml" xref="S3.SS4.p1.5.m5.1.2.3.2">subscript</csymbol><ci id="S3.SS4.p1.5.m5.1.2.3.2.2.cmml" xref="S3.SS4.p1.5.m5.1.2.3.2.2">â„°</ci><ci id="S3.SS4.p1.5.m5.1.2.3.2.3.cmml" xref="S3.SS4.p1.5.m5.1.2.3.2.3">ğ‘¢</ci></apply><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">ğ‘ˆ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">f_{u}=\mathcal{E}_{u}(U)</annotation></semantics></math> and <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="f_{v}=\mathcal{E}_{v}(V)" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><mrow id="S3.SS4.p1.6.m6.1.2" xref="S3.SS4.p1.6.m6.1.2.cmml"><msub id="S3.SS4.p1.6.m6.1.2.2" xref="S3.SS4.p1.6.m6.1.2.2.cmml"><mi id="S3.SS4.p1.6.m6.1.2.2.2" xref="S3.SS4.p1.6.m6.1.2.2.2.cmml">f</mi><mi id="S3.SS4.p1.6.m6.1.2.2.3" xref="S3.SS4.p1.6.m6.1.2.2.3.cmml">v</mi></msub><mo id="S3.SS4.p1.6.m6.1.2.1" xref="S3.SS4.p1.6.m6.1.2.1.cmml">=</mo><mrow id="S3.SS4.p1.6.m6.1.2.3" xref="S3.SS4.p1.6.m6.1.2.3.cmml"><msub id="S3.SS4.p1.6.m6.1.2.3.2" xref="S3.SS4.p1.6.m6.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.6.m6.1.2.3.2.2" xref="S3.SS4.p1.6.m6.1.2.3.2.2.cmml">â„°</mi><mi id="S3.SS4.p1.6.m6.1.2.3.2.3" xref="S3.SS4.p1.6.m6.1.2.3.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p1.6.m6.1.2.3.1" xref="S3.SS4.p1.6.m6.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS4.p1.6.m6.1.2.3.3.2" xref="S3.SS4.p1.6.m6.1.2.3.cmml"><mo stretchy="false" id="S3.SS4.p1.6.m6.1.2.3.3.2.1" xref="S3.SS4.p1.6.m6.1.2.3.cmml">(</mo><mi id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml">V</mi><mo stretchy="false" id="S3.SS4.p1.6.m6.1.2.3.3.2.2" xref="S3.SS4.p1.6.m6.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><apply id="S3.SS4.p1.6.m6.1.2.cmml" xref="S3.SS4.p1.6.m6.1.2"><eq id="S3.SS4.p1.6.m6.1.2.1.cmml" xref="S3.SS4.p1.6.m6.1.2.1"></eq><apply id="S3.SS4.p1.6.m6.1.2.2.cmml" xref="S3.SS4.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.2.2.1.cmml" xref="S3.SS4.p1.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.2.2.2.cmml" xref="S3.SS4.p1.6.m6.1.2.2.2">ğ‘“</ci><ci id="S3.SS4.p1.6.m6.1.2.2.3.cmml" xref="S3.SS4.p1.6.m6.1.2.2.3">ğ‘£</ci></apply><apply id="S3.SS4.p1.6.m6.1.2.3.cmml" xref="S3.SS4.p1.6.m6.1.2.3"><times id="S3.SS4.p1.6.m6.1.2.3.1.cmml" xref="S3.SS4.p1.6.m6.1.2.3.1"></times><apply id="S3.SS4.p1.6.m6.1.2.3.2.cmml" xref="S3.SS4.p1.6.m6.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m6.1.2.3.2.1.cmml" xref="S3.SS4.p1.6.m6.1.2.3.2">subscript</csymbol><ci id="S3.SS4.p1.6.m6.1.2.3.2.2.cmml" xref="S3.SS4.p1.6.m6.1.2.3.2.2">â„°</ci><ci id="S3.SS4.p1.6.m6.1.2.3.2.3.cmml" xref="S3.SS4.p1.6.m6.1.2.3.2.3">ğ‘£</ci></apply><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">f_{v}=\mathcal{E}_{v}(V)</annotation></semantics></math> respectively, each dimension <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><mi id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">d</annotation></semantics></math>. We then fuse these embeddings by concatenating them to form <math id="S3.SS4.p1.8.m8.1" class="ltx_Math" alttext="f\in\mathbb{R}^{2d}" display="inline"><semantics id="S3.SS4.p1.8.m8.1a"><mrow id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml"><mi id="S3.SS4.p1.8.m8.1.1.2" xref="S3.SS4.p1.8.m8.1.1.2.cmml">f</mi><mo id="S3.SS4.p1.8.m8.1.1.1" xref="S3.SS4.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS4.p1.8.m8.1.1.3" xref="S3.SS4.p1.8.m8.1.1.3.cmml"><mi id="S3.SS4.p1.8.m8.1.1.3.2" xref="S3.SS4.p1.8.m8.1.1.3.2.cmml">â„</mi><mrow id="S3.SS4.p1.8.m8.1.1.3.3" xref="S3.SS4.p1.8.m8.1.1.3.3.cmml"><mn id="S3.SS4.p1.8.m8.1.1.3.3.2" xref="S3.SS4.p1.8.m8.1.1.3.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p1.8.m8.1.1.3.3.1" xref="S3.SS4.p1.8.m8.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.8.m8.1.1.3.3.3" xref="S3.SS4.p1.8.m8.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><apply id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"><in id="S3.SS4.p1.8.m8.1.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1.1"></in><ci id="S3.SS4.p1.8.m8.1.1.2.cmml" xref="S3.SS4.p1.8.m8.1.1.2">ğ‘“</ci><apply id="S3.SS4.p1.8.m8.1.1.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.8.m8.1.1.3.1.cmml" xref="S3.SS4.p1.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.8.m8.1.1.3.2.cmml" xref="S3.SS4.p1.8.m8.1.1.3.2">â„</ci><apply id="S3.SS4.p1.8.m8.1.1.3.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3"><times id="S3.SS4.p1.8.m8.1.1.3.3.1.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3.1"></times><cn type="integer" id="S3.SS4.p1.8.m8.1.1.3.3.2.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3.2">2</cn><ci id="S3.SS4.p1.8.m8.1.1.3.3.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">f\in\mathbb{R}^{2d}</annotation></semantics></math>, where <math id="S3.SS4.p1.9.m9.2" class="ltx_Math" alttext="f=\text{concat}(f_{s},f_{d})" display="inline"><semantics id="S3.SS4.p1.9.m9.2a"><mrow id="S3.SS4.p1.9.m9.2.2" xref="S3.SS4.p1.9.m9.2.2.cmml"><mi id="S3.SS4.p1.9.m9.2.2.4" xref="S3.SS4.p1.9.m9.2.2.4.cmml">f</mi><mo id="S3.SS4.p1.9.m9.2.2.3" xref="S3.SS4.p1.9.m9.2.2.3.cmml">=</mo><mrow id="S3.SS4.p1.9.m9.2.2.2" xref="S3.SS4.p1.9.m9.2.2.2.cmml"><mtext id="S3.SS4.p1.9.m9.2.2.2.4" xref="S3.SS4.p1.9.m9.2.2.2.4a.cmml">concat</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p1.9.m9.2.2.2.3" xref="S3.SS4.p1.9.m9.2.2.2.3.cmml">â€‹</mo><mrow id="S3.SS4.p1.9.m9.2.2.2.2.2" xref="S3.SS4.p1.9.m9.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS4.p1.9.m9.2.2.2.2.2.3" xref="S3.SS4.p1.9.m9.2.2.2.2.3.cmml">(</mo><msub id="S3.SS4.p1.9.m9.1.1.1.1.1.1" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.9.m9.1.1.1.1.1.1.2" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS4.p1.9.m9.1.1.1.1.1.1.3" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.SS4.p1.9.m9.2.2.2.2.2.4" xref="S3.SS4.p1.9.m9.2.2.2.2.3.cmml">,</mo><msub id="S3.SS4.p1.9.m9.2.2.2.2.2.2" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2.cmml"><mi id="S3.SS4.p1.9.m9.2.2.2.2.2.2.2" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS4.p1.9.m9.2.2.2.2.2.2.3" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2.3.cmml">d</mi></msub><mo stretchy="false" id="S3.SS4.p1.9.m9.2.2.2.2.2.5" xref="S3.SS4.p1.9.m9.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m9.2b"><apply id="S3.SS4.p1.9.m9.2.2.cmml" xref="S3.SS4.p1.9.m9.2.2"><eq id="S3.SS4.p1.9.m9.2.2.3.cmml" xref="S3.SS4.p1.9.m9.2.2.3"></eq><ci id="S3.SS4.p1.9.m9.2.2.4.cmml" xref="S3.SS4.p1.9.m9.2.2.4">ğ‘“</ci><apply id="S3.SS4.p1.9.m9.2.2.2.cmml" xref="S3.SS4.p1.9.m9.2.2.2"><times id="S3.SS4.p1.9.m9.2.2.2.3.cmml" xref="S3.SS4.p1.9.m9.2.2.2.3"></times><ci id="S3.SS4.p1.9.m9.2.2.2.4a.cmml" xref="S3.SS4.p1.9.m9.2.2.2.4"><mtext id="S3.SS4.p1.9.m9.2.2.2.4.cmml" xref="S3.SS4.p1.9.m9.2.2.2.4">concat</mtext></ci><interval closure="open" id="S3.SS4.p1.9.m9.2.2.2.2.3.cmml" xref="S3.SS4.p1.9.m9.2.2.2.2.2"><apply id="S3.SS4.p1.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.9.m9.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.9.m9.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1.2">ğ‘“</ci><ci id="S3.SS4.p1.9.m9.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.9.m9.1.1.1.1.1.1.3">ğ‘ </ci></apply><apply id="S3.SS4.p1.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.9.m9.2.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.9.m9.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2.2">ğ‘“</ci><ci id="S3.SS4.p1.9.m9.2.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.9.m9.2.2.2.2.2.2.3">ğ‘‘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m9.2c">f=\text{concat}(f_{s},f_{d})</annotation></semantics></math>. Finally, we add a 2-layer MLP network <math id="S3.SS4.p1.10.m10.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS4.p1.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.10.m10.1.1" xref="S3.SS4.p1.10.m10.1.1.cmml">â„‹</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.10.m10.1b"><ci id="S3.SS4.p1.10.m10.1.1.cmml" xref="S3.SS4.p1.10.m10.1.1">â„‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.10.m10.1c">\mathcal{H}</annotation></semantics></math> with a hidden dimension of <math id="S3.SS4.p1.11.m11.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS4.p1.11.m11.1a"><mi id="S3.SS4.p1.11.m11.1.1" xref="S3.SS4.p1.11.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.11.m11.1b"><ci id="S3.SS4.p1.11.m11.1.1.cmml" xref="S3.SS4.p1.11.m11.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.11.m11.1c">d</annotation></semantics></math> to predict a single output value indicating the probability of damaged building between the pre and post images. The whole network is then trained with a binary cross entropy loss using the binary ground truth labels.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.6" class="ltx_p">However, directly training predictive models using only synthetic data might result in poor accuracy due to the domain gap between synthetic and real imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. Therefore, we devise a two-stage training strategy to leverage the in-domain synthetic data, along with out-of-domain real data to effectively improve the target performance. In particular, we first train our network end-to-end including the encoders <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{u}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">â„°</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">â„°</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\mathcal{E}_{u}</annotation></semantics></math> and <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{E}_{v}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">â„°</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">â„°</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\mathcal{E}_{v}</annotation></semantics></math> as well as the MLP layers <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">â„‹</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">â„‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\mathcal{H}</annotation></semantics></math> using the source domain data <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{s}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ğ’Ÿ</ci><ci id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\mathcal{D}_{s}</annotation></semantics></math>.
Subsequently, we follow prior work in robust learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> to fine-tune only the final layers of the MLP network <math id="S3.SS4.p2.5.m5.1" class="ltx_math_unparsed" alttext="\mathcal{H}(.)" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mrow id="S3.SS4.p2.5.m5.1b"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.5.m5.1.1">â„‹</mi><mrow id="S3.SS4.p2.5.m5.1.2"><mo stretchy="false" id="S3.SS4.p2.5.m5.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S3.SS4.p2.5.m5.1.2.2">.</mo><mo stretchy="false" id="S3.SS4.p2.5.m5.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\mathcal{H}(.)</annotation></semantics></math> using the synthetic data supervision from the target, while keeping the encoders fixed during the fine-tuning stage. We observed that only re-training the last layer prevents over-fitting the network to the synthetic data compared to complete end-to-end fine-tuning (<a href="#S4.T4" title="In 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>), so we adopt this two-stage mechanism in our framework.
During inference, we apply a sigmoid layer on top of the predicted output and threshold this probability to predict damaged buildings in the post-image if the predicted probability is <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="&gt;0.5" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><mrow id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi id="S3.SS4.p2.6.m6.1.1.2" xref="S3.SS4.p2.6.m6.1.1.2.cmml"></mi><mo id="S3.SS4.p2.6.m6.1.1.1" xref="S3.SS4.p2.6.m6.1.1.1.cmml">&gt;</mo><mn id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><gt id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1.1"></gt><csymbol cd="latexml" id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2">absent</csymbol><cn type="float" id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">&gt;0.5</annotation></semantics></math>, and predict no damage otherwise.</p>
</div>
<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning the MUSE model</h4>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">In the framework illustrated so far, we only use a frozen pre-trained MUSE model, where we fix the generative model itself and only use it for inference given input images and corresponding text prompts. However, such off-the-shelf models trained on billions of web-scale image-text data might contain images from a wide variety of domains, and might not be fully suited for use in specific domains like aerial or satellite imagery. Therefore, we also investigate the potential benefits offered by fine-tuning the pre-trained generative model for the specific task of aerial image classification.
In particular, we collect the pre-disaster images from <math id="S3.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{t}" display="inline"><semantics id="S3.SS4.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.2">ğ’Ÿ</ci><ci id="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px1.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px1.p1.1.m1.1c">\mathcal{D}_{t}</annotation></semantics></math> and create prompts for each image from the <span id="S3.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">undamaged</span> pool to create a dataset of image-text pairs from the target domain. We then adopt adapter-fine tuningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> to fine-tune the pre-trained model using these image-text pairs, which we found to be more resource-efficient than end-to-end fine-tuning. This fine-tuned model is expected to capture more domain specific properties unique to aerial and satellite imagery, and we compare this procedure with generation using the frozen model in <a href="#S4.SS3" title="4.3 Results â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:477.0pt;height:60.7pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-119.2pt,15.0pt) scale(0.666671003125236,0.666671003125236) ;">
<table id="S4.T1.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S4.T1.4.4.4.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Ian <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.7" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Michael<math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Laura<math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.9" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Maria<math id="S4.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.10" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T1.4.4.4.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.4.5.1" class="ltx_tr">
<td id="S4.T1.4.4.5.1.1" class="ltx_td"></td>
<th id="S4.T1.4.4.5.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Michael</th>
<th id="S4.T1.4.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Laura</th>
<th id="S4.T1.4.4.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Maria</th>
<th id="S4.T1.4.4.5.1.6" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.4.5.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ian</th>
<th id="S4.T1.4.4.5.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">Laura</th>
<th id="S4.T1.4.4.5.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Maria</th>
<th id="S4.T1.4.4.5.1.10" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.4.5.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ian</th>
<th id="S4.T1.4.4.5.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column">Michael</th>
<th id="S4.T1.4.4.5.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column">Maria</th>
<th id="S4.T1.4.4.5.1.14" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T1.4.4.5.1.15" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ian</th>
<th id="S4.T1.4.4.5.1.16" class="ltx_td ltx_align_center ltx_th ltx_th_column">Michael</th>
<th id="S4.T1.4.4.5.1.17" class="ltx_td ltx_align_center ltx_th ltx_th_column">Laura</th>
<th id="S4.T1.4.4.5.1.18" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S4.T1.4.4.5.1.19" class="ltx_td"></td>
</tr>
<tr id="S4.T1.4.4.6.2" class="ltx_tr">
<td id="S4.T1.4.4.6.2.1" class="ltx_td ltx_align_left ltx_border_t">Source Only</td>
<td id="S4.T1.4.4.6.2.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_t">41.6</td>
<td id="S4.T1.4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_t">19.3</td>
<td id="S4.T1.4.4.6.2.5" class="ltx_td ltx_align_center ltx_border_t">27.3</td>
<td id="S4.T1.4.4.6.2.6" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.4.4.6.2.7" class="ltx_td ltx_align_center ltx_border_t">38.0</td>
<td id="S4.T1.4.4.6.2.8" class="ltx_td ltx_align_center ltx_border_t">32.0</td>
<td id="S4.T1.4.4.6.2.9" class="ltx_td ltx_align_center ltx_border_t">29.7</td>
<td id="S4.T1.4.4.6.2.10" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.4.4.6.2.11" class="ltx_td ltx_align_center ltx_border_t">38.3</td>
<td id="S4.T1.4.4.6.2.12" class="ltx_td ltx_align_center ltx_border_t">46.9</td>
<td id="S4.T1.4.4.6.2.13" class="ltx_td ltx_align_center ltx_border_t">26.3</td>
<td id="S4.T1.4.4.6.2.14" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.4.4.6.2.15" class="ltx_td ltx_align_center ltx_border_t">30.0</td>
<td id="S4.T1.4.4.6.2.16" class="ltx_td ltx_align_center ltx_border_t">39.6</td>
<td id="S4.T1.4.4.6.2.17" class="ltx_td ltx_align_center ltx_border_t">21.9</td>
<td id="S4.T1.4.4.6.2.18" class="ltx_td ltx_border_t"></td>
<td id="S4.T1.4.4.6.2.19" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
</tr>
<tr id="S4.T1.4.4.7.3" class="ltx_tr">
<td id="S4.T1.4.4.7.3.1" class="ltx_td ltx_align_left">Ours w/ ZeroShot MUSE</td>
<td id="S4.T1.4.4.7.3.2" class="ltx_td"></td>
<td id="S4.T1.4.4.7.3.3" class="ltx_td ltx_align_center">49.2</td>
<td id="S4.T1.4.4.7.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.4.1" class="ltx_text ltx_font_bold">36.8</span></td>
<td id="S4.T1.4.4.7.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.5.1" class="ltx_text ltx_font_bold">31.9</span></td>
<td id="S4.T1.4.4.7.3.6" class="ltx_td"></td>
<td id="S4.T1.4.4.7.3.7" class="ltx_td ltx_align_center">47.4</td>
<td id="S4.T1.4.4.7.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.8.1" class="ltx_text ltx_font_bold">42.5</span></td>
<td id="S4.T1.4.4.7.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.9.1" class="ltx_text ltx_font_bold">32.0</span></td>
<td id="S4.T1.4.4.7.3.10" class="ltx_td"></td>
<td id="S4.T1.4.4.7.3.11" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.11.1" class="ltx_text ltx_font_bold">50.0</span></td>
<td id="S4.T1.4.4.7.3.12" class="ltx_td ltx_align_center">54.7</td>
<td id="S4.T1.4.4.7.3.13" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.13.1" class="ltx_text ltx_font_bold">30.6</span></td>
<td id="S4.T1.4.4.7.3.14" class="ltx_td"></td>
<td id="S4.T1.4.4.7.3.15" class="ltx_td ltx_align_center">42.6</td>
<td id="S4.T1.4.4.7.3.16" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.16.1" class="ltx_text ltx_font_bold">54.5</span></td>
<td id="S4.T1.4.4.7.3.17" class="ltx_td ltx_align_center"><span id="S4.T1.4.4.7.3.17.1" class="ltx_text ltx_font_bold">36.6</span></td>
<td id="S4.T1.4.4.7.3.18" class="ltx_td"></td>
<td id="S4.T1.4.4.7.3.19" class="ltx_td ltx_align_center">
<span id="S4.T1.4.4.7.3.19.1" class="ltx_text ltx_font_bold">42.4</span> (<span id="S4.T1.4.4.7.3.19.2" class="ltx_text" style="color:#800080;">+9.8%</span>)</td>
</tr>
<tr id="S4.T1.4.4.8.4" class="ltx_tr">
<td id="S4.T1.4.4.8.4.1" class="ltx_td ltx_align_left ltx_border_bb">Ours w/ fine-tuned MUSE</td>
<td id="S4.T1.4.4.8.4.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.4.4.8.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.4.4.8.4.3.1" class="ltx_text ltx_font_bold">49.6</span></td>
<td id="S4.T1.4.4.8.4.4" class="ltx_td ltx_align_center ltx_border_bb">29.9</td>
<td id="S4.T1.4.4.8.4.5" class="ltx_td ltx_align_center ltx_border_bb">25.8</td>
<td id="S4.T1.4.4.8.4.6" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.4.4.8.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.4.4.8.4.7.1" class="ltx_text ltx_font_bold">50.9</span></td>
<td id="S4.T1.4.4.8.4.8" class="ltx_td ltx_align_center ltx_border_bb">31.5</td>
<td id="S4.T1.4.4.8.4.9" class="ltx_td ltx_align_center ltx_border_bb">28.8</td>
<td id="S4.T1.4.4.8.4.10" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.4.4.8.4.11" class="ltx_td ltx_align_center ltx_border_bb">49.2</td>
<td id="S4.T1.4.4.8.4.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.4.4.8.4.12.1" class="ltx_text ltx_font_bold">55.6</span></td>
<td id="S4.T1.4.4.8.4.13" class="ltx_td ltx_align_center ltx_border_bb">26.4</td>
<td id="S4.T1.4.4.8.4.14" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.4.4.8.4.15" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.4.4.8.4.15.1" class="ltx_text ltx_font_bold">44.1</span></td>
<td id="S4.T1.4.4.8.4.16" class="ltx_td ltx_align_center ltx_border_bb">53.6</td>
<td id="S4.T1.4.4.8.4.17" class="ltx_td ltx_align_center ltx_border_bb">27.6</td>
<td id="S4.T1.4.4.8.4.18" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.4.4.8.4.19" class="ltx_td ltx_align_center ltx_border_bb">39.4 (<span id="S4.T1.4.4.8.4.19.1" class="ltx_text" style="color:#800080;">+6.8%</span>)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.7.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Single-source Domain Adaptation Results on SKAI dataset<span id="S4.T1.8.2.1" class="ltx_text ltx_font_medium"> AUPRC values for different transfer settings from the SKAI dataset. We compare the results obtained by training using only real data from the source domain and combining it with synthetic generated data from the target domains on all the transfer settings. Evidently, our approach outperforms the source-only baseline setting new state-of-the-art.</span></span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:447.2pt;height:54.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.3pt,18.2pt) scale(0.596443244964519,0.596443244964519) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S4.T2.3.3.3.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Moore-Tornado<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Nepal-Flooding<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.7" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Portugal-Wildfire<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.3.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<td id="S4.T2.3.3.4.1.1" class="ltx_td"></td>
<th id="S4.T2.3.3.4.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Nepal-Flooding</th>
<th id="S4.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Portugal-Wildfire</th>
<th id="S4.T2.3.3.4.1.5" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T2.3.3.4.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moore-Tornado</th>
<th id="S4.T2.3.3.4.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Portugal-Wildfire</th>
<th id="S4.T2.3.3.4.1.8" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T2.3.3.4.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Moore-Tornado</th>
<th id="S4.T2.3.3.4.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">Nepal-Flooding</th>
<th id="S4.T2.3.3.4.1.11" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S4.T2.3.3.4.1.12" class="ltx_td"></td>
</tr>
<tr id="S4.T2.3.3.5.2" class="ltx_tr">
<td id="S4.T2.3.3.5.2.1" class="ltx_td ltx_align_left ltx_border_t">Source Only</td>
<td id="S4.T2.3.3.5.2.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t">23.8</td>
<td id="S4.T2.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_t">23.2</td>
<td id="S4.T2.3.3.5.2.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_t">14.5</td>
<td id="S4.T2.3.3.5.2.7" class="ltx_td ltx_align_center ltx_border_t">18.5</td>
<td id="S4.T2.3.3.5.2.8" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.3.3.5.2.9" class="ltx_td ltx_align_center ltx_border_t">45.3</td>
<td id="S4.T2.3.3.5.2.10" class="ltx_td ltx_align_center ltx_border_t">24.7</td>
<td id="S4.T2.3.3.5.2.11" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.3.3.5.2.12" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
</tr>
<tr id="S4.T2.3.3.6.3" class="ltx_tr">
<td id="S4.T2.3.3.6.3.1" class="ltx_td ltx_align_left">Ours w/ ZeroShot MUSE</td>
<td id="S4.T2.3.3.6.3.2" class="ltx_td"></td>
<td id="S4.T2.3.3.6.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.6.3.3.1" class="ltx_text ltx_font_bold">49.5</span></td>
<td id="S4.T2.3.3.6.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.6.3.4.1" class="ltx_text ltx_font_bold">24.1</span></td>
<td id="S4.T2.3.3.6.3.5" class="ltx_td"></td>
<td id="S4.T2.3.3.6.3.6" class="ltx_td ltx_align_center">75.1</td>
<td id="S4.T2.3.3.6.3.7" class="ltx_td ltx_align_center">25.1</td>
<td id="S4.T2.3.3.6.3.8" class="ltx_td"></td>
<td id="S4.T2.3.3.6.3.9" class="ltx_td ltx_align_center">76.0</td>
<td id="S4.T2.3.3.6.3.10" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.6.3.10.1" class="ltx_text ltx_font_bold">51.6</span></td>
<td id="S4.T2.3.3.6.3.11" class="ltx_td"></td>
<td id="S4.T2.3.3.6.3.12" class="ltx_td ltx_align_center">50.2 (<span id="S4.T2.3.3.6.3.12.1" class="ltx_text" style="color:#800080;">+25.2%</span>)</td>
</tr>
<tr id="S4.T2.3.3.7.4" class="ltx_tr">
<td id="S4.T2.3.3.7.4.1" class="ltx_td ltx_align_left ltx_border_bb">Ours w/ fine-tuned MUSE</td>
<td id="S4.T2.3.3.7.4.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_bb">43.9</td>
<td id="S4.T2.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.7.4.4.1" class="ltx_text ltx_font_bold">24.1</span></td>
<td id="S4.T2.3.3.7.4.5" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.7.4.6.1" class="ltx_text ltx_font_bold">82.3</span></td>
<td id="S4.T2.3.3.7.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.7.4.7.1" class="ltx_text ltx_font_bold">25.3</span></td>
<td id="S4.T2.3.3.7.4.8" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.3.3.7.4.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.7.4.9.1" class="ltx_text ltx_font_bold">83.1</span></td>
<td id="S4.T2.3.3.7.4.10" class="ltx_td ltx_align_center ltx_border_bb">47.5</td>
<td id="S4.T2.3.3.7.4.11" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.3.3.7.4.12" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.3.3.7.4.12.1" class="ltx_text ltx_font_bold">51.1</span> (<span id="S4.T2.3.3.7.4.12.2" class="ltx_text" style="color:#800080;">+26.1%</span>)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Single-source Domain Adaptation Results on xBD dataset.<span id="S4.T2.7.2.1" class="ltx_text ltx_font_medium"> AUPRC values for different transfer settings from the xBD datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. On each of the transfer setting, augmenting training using synthetic data from the target domain significantly outperforms the source-only baseline, with an improvement of 25.2% using a zeroshot generative model, and 26.1% with further fine-tuning the generative backbone on aerial image-text pairs.</span></span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We next demonstrate the effectiveness of the proposed approach on several challenging transfer settings. We first introduce our choice of datasets in <a href="#S4.SS1" title="4.1 Datasets â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.1</span></a>, specify the training details in <a href="#S4.SS2" title="4.2 Training and Evaluation Details â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.2</span></a> followed by the results in <a href="#S4.SS3" title="4.3 Results â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.3</span></a> and several ablations into our modeling and training choices in <a href="#S4.SS4" title="4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:60.2pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.0pt,15.2pt) scale(0.661664997465769,0.661664997465769) ;">
<table id="S4.T3.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.7.7.8.1" class="ltx_tr">
<th id="S4.T3.7.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dataset</th>
<th id="S4.T3.7.7.8.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T3.7.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5">SKAI-Dataset</th>
<th id="S4.T3.7.7.8.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T3.7.7.8.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">xBD-Dataset</th>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<th id="S4.T3.7.7.7.8" class="ltx_td ltx_align_left ltx_th ltx_th_column">Method</th>
<th id="S4.T3.7.7.7.9" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math>Ian</th>
<th id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math>Michael</th>
<th id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math>Maria</th>
<th id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T3.4.4.4.4.m1.1.1" xref="S4.T3.4.4.4.4.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math>Laura</th>
<th id="S4.T3.7.7.7.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Avg.</th>
<th id="S4.T3.7.7.7.11" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.5.m1.1.1" xref="S4.T3.5.5.5.5.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">\rightarrow</annotation></semantics></math>Moore-Tornado</th>
<th id="S4.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T3.6.6.6.6.m1.1.1" xref="S4.T3.6.6.6.6.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.6.m1.1b"><ci id="S4.T3.6.6.6.6.m1.1.1.cmml" xref="S4.T3.6.6.6.6.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.6.m1.1c">\rightarrow</annotation></semantics></math>Nepal-Flooding</th>
<th id="S4.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T3.7.7.7.7.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.7.7.7.7.m1.1a"><mo stretchy="false" id="S4.T3.7.7.7.7.m1.1.1" xref="S4.T3.7.7.7.7.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.7.m1.1b"><ci id="S4.T3.7.7.7.7.m1.1.1.cmml" xref="S4.T3.7.7.7.7.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.7.m1.1c">\rightarrow</annotation></semantics></math>Portugal-Wildfire</th>
<th id="S4.T3.7.7.7.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.7.7.9.1" class="ltx_tr">
<td id="S4.T3.7.7.9.1.1" class="ltx_td ltx_align_left ltx_border_t">Source Only</td>
<td id="S4.T3.7.7.9.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.7.7.9.1.3" class="ltx_td ltx_align_center ltx_border_t">44.21</td>
<td id="S4.T3.7.7.9.1.4" class="ltx_td ltx_align_center ltx_border_t">48.62</td>
<td id="S4.T3.7.7.9.1.5" class="ltx_td ltx_align_center ltx_border_t">29.54</td>
<td id="S4.T3.7.7.9.1.6" class="ltx_td ltx_align_center ltx_border_t">36.83</td>
<td id="S4.T3.7.7.9.1.7" class="ltx_td ltx_align_center ltx_border_t">39.78</td>
<td id="S4.T3.7.7.9.1.8" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.7.7.9.1.9" class="ltx_td ltx_align_center ltx_border_t">18.96</td>
<td id="S4.T3.7.7.9.1.10" class="ltx_td ltx_align_center ltx_border_t">27.16</td>
<td id="S4.T3.7.7.9.1.11" class="ltx_td ltx_align_center ltx_border_t">29.69</td>
<td id="S4.T3.7.7.9.1.12" class="ltx_td ltx_align_center ltx_border_t">25.27</td>
</tr>
<tr id="S4.T3.7.7.10.2" class="ltx_tr">
<td id="S4.T3.7.7.10.2.1" class="ltx_td ltx_align_left">Ours w/ ZeroShot MUSE</td>
<td id="S4.T3.7.7.10.2.2" class="ltx_td"></td>
<td id="S4.T3.7.7.10.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.10.2.3.1" class="ltx_text ltx_font_bold">54.79</span></td>
<td id="S4.T3.7.7.10.2.4" class="ltx_td ltx_align_center">52.24</td>
<td id="S4.T3.7.7.10.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.10.2.5.1" class="ltx_text ltx_font_bold">34.05</span></td>
<td id="S4.T3.7.7.10.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.10.2.6.1" class="ltx_text ltx_font_bold">39.38</span></td>
<td id="S4.T3.7.7.10.2.7" class="ltx_td ltx_align_center">
<span id="S4.T3.7.7.10.2.7.1" class="ltx_text ltx_font_bold">45.11</span> (<span id="S4.T3.7.7.10.2.7.2" class="ltx_text" style="color:#800080;">+5.33%</span>)</td>
<td id="S4.T3.7.7.10.2.8" class="ltx_td"></td>
<td id="S4.T3.7.7.10.2.9" class="ltx_td ltx_align_center">78.70</td>
<td id="S4.T3.7.7.10.2.10" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.10.2.10.1" class="ltx_text ltx_font_bold">52.40</span></td>
<td id="S4.T3.7.7.10.2.11" class="ltx_td ltx_align_center"><span id="S4.T3.7.7.10.2.11.1" class="ltx_text ltx_font_bold">32.10</span></td>
<td id="S4.T3.7.7.10.2.12" class="ltx_td ltx_align_center">
<span id="S4.T3.7.7.10.2.12.1" class="ltx_text ltx_font_bold">54.40</span>(<span id="S4.T3.7.7.10.2.12.2" class="ltx_text" style="color:#800080;">+29.13%</span>)</td>
</tr>
<tr id="S4.T3.7.7.11.3" class="ltx_tr">
<td id="S4.T3.7.7.11.3.1" class="ltx_td ltx_align_left ltx_border_bb">Ours w/ fine-tuned MUSE</td>
<td id="S4.T3.7.7.11.3.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.7.7.11.3.3" class="ltx_td ltx_align_center ltx_border_bb">49.12</td>
<td id="S4.T3.7.7.11.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.4.1" class="ltx_text ltx_font_bold">53.83</span></td>
<td id="S4.T3.7.7.11.3.5" class="ltx_td ltx_align_center ltx_border_bb">30.92</td>
<td id="S4.T3.7.7.11.3.6" class="ltx_td ltx_align_center ltx_border_bb">39.29</td>
<td id="S4.T3.7.7.11.3.7" class="ltx_td ltx_align_center ltx_border_bb">43.29 (<span id="S4.T3.7.7.11.3.7.1" class="ltx_text" style="color:#800080;">+3.51%</span>)</td>
<td id="S4.T3.7.7.11.3.8" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.7.7.11.3.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.9.1" class="ltx_text ltx_font_bold">83.18</span></td>
<td id="S4.T3.7.7.11.3.10" class="ltx_td ltx_align_center ltx_border_bb">50.16</td>
<td id="S4.T3.7.7.11.3.11" class="ltx_td ltx_align_center ltx_border_bb">30.72</td>
<td id="S4.T3.7.7.11.3.12" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T3.7.7.11.3.12.1" class="ltx_text ltx_font_bold">54.69</span>(<span id="S4.T3.7.7.11.3.12.2" class="ltx_text" style="color:#800080;">+29.42%</span>)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.10.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.11.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Multi-source Domain Adaptation Results on SKAI and xBD datasets.<span id="S4.T3.11.2.1" class="ltx_text ltx_font_medium"> AUPRC values for different transfer settings, where we show the result of training using synthetic generated data from the respective target domain along with manual supervision from all the three remaining domains. Our approach outperforms the source-only baseline highlighting the effectiveness of training with generated synthetic data in bridging domain gaps.</span></span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">xBD Dataset</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">xBD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> is a large-scale dataset designed for automatic disaster assessment using aerial and satellite imagery. The dataset covers synchronized pre- and post- event satellite imagery of both damaged and undamaged scenes from more than 19 events across the world, covering a variety of disaster types across varying severity levels. Since our focus in this paper is to improve robustness of aerial disaster assessment algorithms across disparate geographies, we choose 3 domains from xBD, namely <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">nepal-flooding</span>, <span id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">portugal-wildfires</span> and <span id="S4.SS1.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">moore-tornado</span> to demostrate our results, which have 36456, 18884 and 18491 images respectively. These domains encompass data from three distinct geographical subregions, each affected by entirely different types of disasters making it a challenging problem to improve cross-domain robustness.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SKAI Satellite Imagery</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">In order to verify the effectiveness of our method in improving the performance across subtle domain variations, we adopt the SKAI datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> consisting of pre and post hurricane images captured from different regions in the United States. The images in SKAI includes data collected from Ian, Maria, Michael and Laura hurricanes with 2733, 3709, 3991 and 3991 images respectively, which we use as the different domains for our cross-domain robustness setting. Note that both these datasets consist of heavy class imbalance, with more than 80% of the image-pairs capturing non-damaged buildings, adding an additional layer of complexity in bridging the domain shifts. For both the datasets, we show results using single-source and multi-source adaptation settings, in which we use supervised data from single source domain or all the domains except the target respectively.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training and Evaluation Details</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We use an Imagenet pretrained ViT-B/16 transformer backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> as the encoder in our setting, and remove the last classification layer replacing it with the MLP head for binary classification. We then train the network using the two-stage approach discussed in <a href="#S3.SS4" title="3.4 Training using Synthetic Data â€£ 3 Method â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a>, first using the supervised source domain images using Adam optimizer with a learning rate of 2e-6 for the pre-trained backbone and 2e-5 for the randomly initialized MLP layer, followed by re-training only the last MLP layer using synthesized target domain images using the same hyperparameters as above. We use a batch size of 64 in both cases and perform training for 5000 iterations. We use the validation images from the target domain to perform early stopping, which we observed to be very crucial to obtain good performance in our setting.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Following prior work in disaster assessment tasks from satellite imageryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, we adopt the AUPRC metric for evaluation which measures the area under the precision-recall curve across various thresholds, and is shown to be relatively more robust for cases like ours where there is severe class imbalance against positive examples. In terms of baselines, we compare with a source-only baseline which only trains a predictive model on the source domain and evaluates on the target test-set. Since this does not use any target data, it serves as a fundamental baseline to illustrate the benefits obtained by our method. Note that prior UDA methods require both pre and post disaster images to learn domain agnostic featuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, preventing a direct comparison for our setting where only pre-disaster images from the target dataset are available.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Single-source Zeroshot Adaptation</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.3" class="ltx_p">We show the results for single-source UDA for domains from the SKAI dataset in <a href="#S4.T1" title="In 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> and xBD dataset in <a href="#S4.T2" title="In 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. As shown, our method of augmenting out of distribution training using synthetic images generated from MUSE model achieves better accuracy than the source-only baseline, with <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">âˆ¼</mo><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">10</mn><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">\sim 10\%</annotation></semantics></math> and <math id="S4.SS3.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\sim 25\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml">âˆ¼</mo><mrow id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mn id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">25</mn><mo id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2">absent</csymbol><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.2">25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.2.m2.1c">\sim 25\%</annotation></semantics></math> improvements on the SKAI and the xBD datasets on average. Our improvements are consistent across all the transfer settings, with up to <math id="S4.SS3.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\sim 70\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml"></mi><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml">âˆ¼</mo><mrow id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mn id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">70</mn><mo id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2">absent</csymbol><apply id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.2">70</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.3.m3.1c">\sim 70\%</annotation></semantics></math> improvement on the more challenging cross-disaster cross-geography setting from xBD dataset, highlighting the effectiveness of leveraging generative foundational models to create synthetic data for low-resource domains even in expert tasks like disaster assessment.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p2.1" class="ltx_p">Furthermore, we also compare the AUPRC results observed through fine-tuning the generative model on aerial imagery and satellite images, using the procedure outlined in <a href="#S3.SS4" title="3.4 Training using Synthetic Data â€£ 3 Method â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a>. We observe that the model trained with data generated from fine-tuned model outperforms both the source-only baseline as well as the zeroshot settings on 4 out of 6 settings in xBD dataset with <math id="S4.SS3.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\sim 1\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p2.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml">âˆ¼</mo><mrow id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml"><mn id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.2" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.2.cmml">1</mn><mo id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.1" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p2.1.m1.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p2.1.m1.1c">\sim 1\%</annotation></semantics></math> improvement on the average accuracy, indicating the potential in fine-tuning MUSE model on domain-specific images. On SKAI data however, we observe zeroshot model is better on the average AUPRC.
A potential reason for this could be that the generative ability of the text-to-image generative model is reduced after fine-tuning on domain-specific images, impacting accuracy in few of the transfer settings, highlighting room for further improvement through more carefully designed fine-tuning strategies.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-source Zeroshot Adaptation</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.2" class="ltx_p">The comparison for both SKAI and xBD datasets on multi-source adaptation setting is shown in <a href="#S4.T3" title="In 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>. Firstly, the accuracy achieved by multi-source models on all target domains is higher than single-source setting, which is expected since multi-source models have access to relatively more supervised data. Furthermore, the results from <a href="#S4.T3" title="In 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> clearly show the effectiveness of our approach even for such multi-source evaluation setting, where our method using zeroshot text-to-image generation yields <math id="S4.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="5.33\%" display="inline"><semantics id="S4.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">5.33</mn><mo id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2">5.33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.1.m1.1c">5.33\%</annotation></semantics></math> improvement over baseline on SKAI dataset and <math id="S4.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="29.13\%" display="inline"><semantics id="S4.SS3.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml">29.13</mn><mo id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.2.m2.1.1.2">29.13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.2.m2.1c">29.13\%</annotation></semantics></math> improvement over baseline on the xBD dataset. Our benefits are consistent for both the datasets across all the transfer tasks, further supporting our hypothesis that text-to-image models can serve as strong data generators for low-resource domains. As seen for the case of single-source setting, we observe the gains yielded by data generation using zeroshot text-to-image models to be competitive when compared to fine-tuned models on both the datasets.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablations</h3>

<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:88.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.8pt,9.9pt) scale(0.816430762026999,0.816430762026999) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SKAI</th>
<th id="S4.T4.2.1.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">xBD</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.2.1" class="ltx_tr">
<td id="S4.T4.2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">(<span id="S4.T4.2.1.2.1.1.1" class="ltx_text ltx_font_typewriter">R0</span>) Source Only</td>
<td id="S4.T4.2.1.2.1.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">39.78</td>
<td id="S4.T4.2.1.2.1.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T4.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">25.27</td>
</tr>
<tr id="S4.T4.2.1.3.2" class="ltx_tr">
<td id="S4.T4.2.1.3.2.1" class="ltx_td ltx_align_left">(<span id="S4.T4.2.1.3.2.1.1" class="ltx_text ltx_font_typewriter">R1</span>) Only Synthetic Data</td>
<td id="S4.T4.2.1.3.2.2" class="ltx_td"></td>
<td id="S4.T4.2.1.3.2.3" class="ltx_td ltx_align_center">40.60</td>
<td id="S4.T4.2.1.3.2.4" class="ltx_td"></td>
<td id="S4.T4.2.1.3.2.5" class="ltx_td ltx_align_center">47.76</td>
</tr>
<tr id="S4.T4.2.1.4.3" class="ltx_tr">
<td id="S4.T4.2.1.4.3.1" class="ltx_td ltx_align_left">(<span id="S4.T4.2.1.4.3.1.1" class="ltx_text ltx_font_typewriter">R2</span>) Joint Training on Real + Synthetic</td>
<td id="S4.T4.2.1.4.3.2" class="ltx_td"></td>
<td id="S4.T4.2.1.4.3.3" class="ltx_td ltx_align_center">43.11</td>
<td id="S4.T4.2.1.4.3.4" class="ltx_td"></td>
<td id="S4.T4.2.1.4.3.5" class="ltx_td ltx_align_center">49.66</td>
</tr>
<tr id="S4.T4.2.1.5.4" class="ltx_tr">
<td id="S4.T4.2.1.5.4.1" class="ltx_td ltx_align_left">(<span id="S4.T4.2.1.5.4.1.1" class="ltx_text ltx_font_typewriter">R3</span>) End-to-end Finetuning</td>
<td id="S4.T4.2.1.5.4.2" class="ltx_td"></td>
<td id="S4.T4.2.1.5.4.3" class="ltx_td ltx_align_center">44.10</td>
<td id="S4.T4.2.1.5.4.4" class="ltx_td"></td>
<td id="S4.T4.2.1.5.4.5" class="ltx_td ltx_align_center">53.44</td>
</tr>
<tr id="S4.T4.2.1.6.5" class="ltx_tr">
<td id="S4.T4.2.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">(<span id="S4.T4.2.1.6.5.1.1" class="ltx_text ltx_font_typewriter">R4</span>) Last-Layer Finetune on SynData</td>
<td id="S4.T4.2.1.6.5.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T4.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.2.1.6.5.3.1" class="ltx_text ltx_font_bold">45.11</span></td>
<td id="S4.T4.2.1.6.5.4" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T4.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.2.1.6.5.5.1" class="ltx_text ltx_font_bold">54.40</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.8.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Effect of training choices<span id="S4.T4.9.2.1" class="ltx_text ltx_font_medium"> We show the effect of various training choices in our framework, where last-layer re-training using only synthetic data (<span id="S4.T4.9.2.1.1" class="ltx_text ltx_font_typewriter">R4</span>) outperforms training using only synthetic data without the source labels (<span id="S4.T4.9.2.1.2" class="ltx_text ltx_font_typewriter">R1</span>), jointly training on both real and synthetic data (<span id="S4.T4.9.2.1.3" class="ltx_text ltx_font_typewriter">R2</span>) as well as end-to-end finetuning using synthetic data (<span id="S4.T4.9.2.1.4" class="ltx_text ltx_font_typewriter">R3</span>).</span></span></figcaption>
</figure>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablations and Insights</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">We show the effect of various design choices in our framework in <a href="#S4.T4" title="In 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. Firstly, we observe that training only using synthetic data without source domain data leads to poor results, potentially highlighting the limitations of synthetic data alone in training (<span id="S4.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">R1</span> vs <span id="S4.SS4.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">R4</span>). This facet of synthetic data has also been noted in prior worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, indicating that manual supervision is still necessary to observe gains with synthetic supervision. Furthermore, we also show that joint-training using real source and synthetic target datasets is inferior to our proposed approach of first pre-training on real source data followed by last-layer retraining on generated target domain data (<span id="S4.SS4.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_typewriter">R2</span> vs <span id="S4.SS4.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_typewriter">R4</span>), supporting our two-stage training framework. Finally, we also observe that end-to-end fine-tuning using synthetic data under-performs the approach of finetuning the last layer only (<span id="S4.SS4.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_typewriter">R3</span> vs <span id="S4.SS4.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_typewriter">R4</span>).</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:199.5pt;"><img src="/html/2405.13779/assets/x3.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="528" height="364" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.1.2.2" class="ltx_text" style="font-size:90%;">Ian Hurricane</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:199.5pt;"><img src="/html/2405.13779/assets/x4.png" id="S4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="528" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.2.2.2" class="ltx_text" style="font-size:90%;">Moore Tornado</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.5.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Effect of the amount of generated synthetic data.<span id="S4.F5.6.2.1" class="ltx_text ltx_font_medium"> We show the positive influence of the volume of generated synthetic data (as a % of the target domain images) on the multi-source transfer setting, where adding more target data invariably helps to improve the final target accuracy for both SKAI <a href="#S4.F5" title="Figure 5 â€£ Ablations and Insights â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and xBD <a href="#S4.F5" title="Figure 5 â€£ Ablations and Insights â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> datasets, with potential for further enhancement with more generated data.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of Volume of Synthetic Data </h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">We show the effect of the amount of generated synthetic data on the target AUPRC in <a href="#S4.F5" title="In Ablations and Insights â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. We observe that adding more synthetic data invariably helps the final target accuracy for both the datasets studied. More importantly, we observe no saturation even when using all target data to generate images indicating further room for improvement of target performance through low-cost synthetically generated data.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:496.9pt;"><img src="/html/2405.13779/assets/x5.png" id="S4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="475" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F6.1.2.2" class="ltx_text" style="font-size:90%;">Images from Ian domain.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:496.9pt;"><img src="/html/2405.13779/assets/x6.png" id="S4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="475" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F6.2.2.2" class="ltx_text" style="font-size:90%;">Images from the Michael domain.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:496.9pt;"><img src="/html/2405.13779/assets/x7.png" id="S4.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="475" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F6.3.2.2" class="ltx_text" style="font-size:90%;">Images from Moore-Tornado.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:496.9pt;"><img src="/html/2405.13779/assets/x8.png" id="S4.F6.4.g1" class="ltx_graphics ltx_img_landscape" width="475" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F6.4.2.2" class="ltx_text" style="font-size:90%;">Images from Nepal-Flooding.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:496.9pt;"><img src="/html/2405.13779/assets/x9.png" id="S4.F6.5.g1" class="ltx_graphics ltx_img_landscape" width="475" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.1.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S4.F6.5.2.2" class="ltx_text" style="font-size:90%;">Images from Portugal-Wildfire.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.8.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of text-to-image results.<span id="S4.F6.9.2.1" class="ltx_text ltx_font_medium"> We show several examples from our generated images, along with the pre-disaster image, correponding conditioning mask (overlapped on the pre-image) as well as the text-prompt used to generate the post-image from <a href="#S4.F6" title="Figure 6 â€£ Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> Ian-Hurricane, <a href="#S4.F6" title="Figure 6 â€£ Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> Michael-Hurricane, <a href="#S4.F6" title="Figure 6 â€£ Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> Moore-Tornado, <a href="#S4.F6" title="Figure 6 â€£ Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> Nepal-Floods and <a href="#S4.F6" title="Figure 6 â€£ Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> Portugal-Wildfire. </span></span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visualizing Generated Images</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">We show several illustrations of samples generated through our method in <a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> on both SKAI (<a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>) and xBD (<a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S4.F6" title="In Effect of Volume of Synthetic Data â€£ 4.4 Ablations â€£ 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>) datasets, where we include the pre-disaster image as well as the mask and the text-prompt used for conditional image editing through our generative model. In most cases, we observe that the text-to-image model incorporates the textual guidance and performs localized editing on the input image to generate a synthetic post-disaster image with great effectiveness. The model shows excellent capability in seamlessly handling the various types of disasters through our text-guidance, which helps to create realistic images in low-resource domains leading to significant empirical gains (<a href="#S4.T3" title="In 4 Experiments â€£ Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we explore the potential of leveraging emerging text-to-image models in generating synthetic supervision to improve robustness across low-resource domains for disaster assessment tasks. We design an efficient and scalable data-generation pipeline by leveraging the localized image editing capabilities of transformer-based generative modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>. Using this framework, we generate several thousand synthetic post-disaster images conditioned on pre-disaster images and text guidance, followed by a simple two-stage training mechanism that yields non-trivial benefits over a source-only baseline in both single source and multi-source domain adaptation setting.
In terms of limitations, we noted a significant sensitivity of the training process to the quality and coherence of the generated synthetic data, which is directly affected by the presence of low-quality generated images. A potential future work can therefore be to additionally incorporate better filtering strategies into our framework to remove poor quality images and improve training.
Nevertheless, our work serves as one of the first to explore the potential of text-to-image synthetic data for expert tasks like satellite disaster assessment, which holds massive potential for continued improvement with the development of better image generation models.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Azizi etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and DavidÂ J Fleet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Synthetic data from diffusion models improves imagenet classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.08466</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bai etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Yanbing Bai, Junjie Hu, Jinhua Su, Xing Liu, Haoyu Liu, Xianwen He, Shengwang Meng, Erick Mas, and Shunichi Koshimura.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Pyramid pooling module-based semi-siamese network: A benchmark model for assessing building damage from xbd satellite imagery datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 12(24):4055, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">Bandara and Patel [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
W.Â G.Â C. Bandara and VishalÂ M. Patel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">A transformer-based siamese network for change detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Geoscience and Remote Sensing Symposium</em><span id="bib.bib3.9.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Benson and Ecker [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
Vitus Benson and Alexander Ecker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">Assessing out-of-domain generalization for robust building damage detection, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Bin etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Junchi Bin, Ran Zhang, Rui Wang, Yue Cao, Yufeng Zheng, ErikÂ P. Blasch, and Zheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">An efficient and uncertainty-aware decision support system for disaster response using aerial imagery.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sensors (Basel, Switzerland)</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 22, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Bouchard etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Isabelle Bouchard, Marie-Ãˆve Rancourt, Daniel Aloise, and Freddie Kalaitzis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">On transfer learning for building damage assessment from satellite imagery in emergency contexts.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 14(11), 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Chang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and WilliamÂ T. Freeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Maskgit: Masked generative image transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Chang etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, WilliamÂ T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Muse: Text-to-image generation via masked generative transformers, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Da etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Yifan Da, Zhiyuan Ji, and Yongsheng Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Building damage assessment based on siamese hierarchical transformer framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Mathematics</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Dhariwal and Nichol [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
Prafulla Dhariwal and Alexander Nichol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">Diffusion models beat gans on image synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib10.9.2" class="ltx_text" style="font-size:90%;">, 34:8780â€“8794, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Du etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Xuefeng Du, Yiyou Sun, Jerry Zhu, and Yixuan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Dream the impossible: Outlier imagination with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Esser etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Patrick Esser, Robin Rombach, and BjÃ¶rn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Taming transformers for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, pages 12873â€“12883. Computer Vision Foundation / IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Ge etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Yunhao Ge, Jiashu Xu, BrianÂ Nlong Zhao, Neel Joshi, Laurent Itti, and Vibhav Vineet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Beyond generation: Harnessing text to image models for object detection and segmentation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.05956</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">Gupta and Shah [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
Rohit Gupta and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">Rescuenet: Joint building segmentation and damage assessment from satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 25th International Conference on Pattern Recognition (ICPR)</em><span id="bib.bib15.10.3" class="ltx_text" style="font-size:90%;">, pages 4405â€“4411. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Gupta etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, and Matthew Gaston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">xbd: A dataset for assessing building damage from satellite imagery, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Hessel etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Jack Hessel, Ari Holtzman, Maxwell Forbes, RonanÂ Le Bras, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Clipscore: A reference-free evaluation metric for image captioning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.08718</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Houlsby etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin DeÂ Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Parameter-efficient transfer learning for nlp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 2790â€“2799. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Kalluri etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Tarun Kalluri, Astuti Sharma, and Manmohan Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Memsac: Memory augmented sample consistency for large scale domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XXX</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 550â€“568. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Kawar etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Imagic: Text-based real image editing with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, pages 6007â€“6017, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Kirichenko etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Polina Kirichenko, Pavel Izmailov, and AndrewÂ Gordon Wilson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Last layer re-training is sufficient for robustness to spurious correlations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.02937</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Lai etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Xin Lai, Zhuotao Tian, Xiaogang Xu, Ying-Cong Chen, Shu Liu, Hengshuang Zhao, Liwei Wang, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Decouplenet: Decoupled network for domain adaptive semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Lee etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Jihyeon Lee, JosephÂ Z. Xu, Kihyuk Sohn, Wenhan Lu, David Berthelot, Izzeddin Gur, Pranav Khaitan, Ke-Wei, Huang, Kyriacos Koupparis, and Bernhard Kowatsch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Assessing post-disaster damage from satellite imagery using semi-supervised learning techniques, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Li etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Yundong Li, Chen Lin, Hongguang Li, Wei Hu, Han Dong, and Yi Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation with self-attention for post-disaster building damage detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 415:27â€“39, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Lin etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Explore the power of synthetic data on few-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, pages 638â€“647, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Lu etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Wen Lu, Lu Wei, and Minh Nguyen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Bitemporal attention transformer for building change detection and building damage assessment.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 17:4917â€“4935, 2024.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Lu etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Xiaopeng Lu, Lynnette Ng, Jared Fernandez, and Hao Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Cigli: Conditional image generation from language &amp; image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 3134â€“3138, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Nasrallah etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Hasan Nasrallah, Mustafa Shukor, and AliÂ J. Ghandour.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Sci-net: scale-invariant model for buildings segmentation from aerial imagery.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Signal, Image and Video Processing</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 17:2999 â€“ 3007, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Nguyen etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Quang Nguyen, Truong Vu, Anh Tran, and Khoi Nguyen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Potnis etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
AbhishekÂ V Potnis, RajatÂ C Shinde, SuryaÂ S Durbha, and KuldeepÂ R Kurte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Multi-class segmentation of urban floods from multispectral imagery using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IGARSS 2019-2019 IEEE international geoscience and remote sensing symposium</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 9741â€“9744. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Rahnemoonfar etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, and RobinÂ Roberson Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Floodnet: A high resolution aerial imagery dataset for post flood scene understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 9:89644â€“89654, 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Ramesh etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Zero-shot text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, pages 8821â€“8831. Pmlr, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Roberts etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, PeterÂ J Liu, Sharan Narang, Wei Li, and Yanqi Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Google, Tech. Rep.</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Rombach etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, pages 10684â€“10695, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Ruiz etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, pages 22500â€“22510, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Saharia etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, EmilyÂ L Denton, Kamyar Ghasemipour, Raphael GontijoÂ Lopes, Burcu KaragolÂ Ayan, Tim Salimans, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 35:36479â€“36494, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Saito etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Maximum classifier discrepancy for unsupervised domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">SarÄ±yÄ±ldÄ±z etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
MertÂ BÃ¼lent SarÄ±yÄ±ldÄ±z, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Fake it till you make it: Learning transferable representations from synthetic imagenet clones.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages 8011â€“8021, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Shao etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Jie Shao, Ke Zhu, Hanxiao Zhang, and Jianxin Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Diffult: How to make diffusion model useful for long-tail recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.05170</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Shin etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Joonghyuk Shin, Minguk Kang, and Jaesik Park.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Fill-up: Balancing long-tailed data with generative models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.07200</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Sirko etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Wojciech Sirko, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser SalahÂ Eddine Bouchareb, Yann Dauphin, Daniel Keysers, Maxim Neumann, Moustapha Cisse, and John Quinn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Continental-scale building detection from high resolution satellite imagery.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.12283</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Steiner etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">How to train your vit? data, augmentation, and regularization in vision transformers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.10270</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Tian etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Learning vision from models rivals learning vision from data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2312.17742</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Tian etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Stablerep: Synthetic images from text-to-image models make strong visual representation learners.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Tsai etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Learning to adapt structured output space for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 7472â€“7481, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Valentijn etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Tinka Valentijn, Jacopo Margutti, Marc vanÂ den Homberg, and Jorma Laaksonen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Multi-hazard and spatial transferability of a cnn for automated building damage assessment.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 12(17), 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Wang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Semantic image synthesis via diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2207.00050</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Wang etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Xiang Wang, Yundong Li, Chen Lin, Yi Liu, and Shuo Geng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Building damage detection based on multi-source adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Applied Remote Sensing</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 15(3):036503, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.4.4.1" class="ltx_text" style="font-size:90%;">Weber and KanÃ© [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text" style="font-size:90%;">
Ethan Weber and Hassan KanÃ©.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">Building disaster damage assessment in satellite imagery with multi-temporal fusion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.05525</em><span id="bib.bib49.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Weber etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Ethan Weber, DimÂ P. Papadopoulos, Ã€gata Lapedriza, Ferda Ofli, Muhammad Imran, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Incidents1m: A large-scale dataset of images with natural disasters, damage, and incidents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, 45:4768â€“4781, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Wu etÂ al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Chuyi Wu, Feng Zhang, Junshi Xia, Yichen Xu, Guoqing Li, Jibo Xie, Zhenhong Du, and Renyi Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Building damage detection using u-net with attention mechanism from pre-and post-disaster remote sensing datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 13(5):905, 2021a.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Wu etÂ al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Chuyi Wu, Feng Zhang, Junshi Xia, Yichen Xu, Guoqing Li, Jibo Xie, Zhenhong Du, and Ren yi Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote. Sens.</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 13:905, 2021b.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Xia etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Junshi Xia, Naoto Yokoya, Bruno Adriano, and Clifford Broni-Bediako.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Openearthmap: A benchmark dataset for global high-resolution land cover mapping.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, pages 6243â€“6253, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Xie etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and MikeÂ Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, pages 7452â€“7461, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Xu etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
JosephÂ Z. Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, and Valeriya Zaytseva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Building damage detection in satellite imagery using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, abs/1910.06444, 2019.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Xu etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, and Rong Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Cdtrans: Cross-domain transformer for unsupervised domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib56.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.06165</em><span id="bib.bib56.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Yang etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Lihe Yang, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, and Hengshuang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Freemask: Synthetic images with dense annotations make stronger segmentation models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Yu etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Yuanzhong Xu, JingÂ Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, BurcuÂ Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Scaling autoregressive models for content-rich text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2206.10789</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Yu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, and YongÂ Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Diversify, donâ€™t fine-tune: Scaling up visual recognition training with synthetic images.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib59.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.02253</em><span id="bib.bib59.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Zhang etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and Xin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Controllable text-to-image generation with gpt-4.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib60.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2305.18583</em><span id="bib.bib60.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.4.4.1" class="ltx_text" style="font-size:90%;">Zhao and Zhang [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.6.1" class="ltx_text" style="font-size:90%;">
Fei Zhao and Chengcui Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">Building damage evaluation from satellite imagery using deep learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib61.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 IEEE 21st International Conference on Information Reuse and Integration for Data Science (IRI)</em><span id="bib.bib61.9.2" class="ltx_text" style="font-size:90%;">, pages 82â€“89, 2020.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Zheng etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">Layoutdiffusion: Controllable diffusion model for layout-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib62.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition</em><span id="bib.bib62.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Zhou etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Yongchao Zhou, Hshmat Sahak, and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Training on thin air: Improve image classification with generated data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib63.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2305.15316</em><span id="bib.bib63.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Zhu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
Jinjing Zhu, Haotian Bai, and Lin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Patch-mix transformer for unsupervised domain adaptation: A game perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib64.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib64.11.3" class="ltx_text" style="font-size:90%;">, pages 3561â€“3571, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13778" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13779" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13779">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13779" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13780" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 19:21:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
