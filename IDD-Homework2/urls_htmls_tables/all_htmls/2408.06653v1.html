<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Hierarchical Structured Neural Network for Retrieval</title>
<!--Generated on Tue Aug 13 05:48:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Deep Retrieval,  Clustering,  Recommender Systems" lang="en" name="keywords"/>
<base href="/html/2408.06653v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S1" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2.SS0.SSS0.Px1" title="In 2. Related Work ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Clustering Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2.SS0.SSS0.Px2" title="In 2. Related Work ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Embedding Based Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2.SS0.SSS0.Px3" title="In 2. Related Work ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Generative Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2.SS0.SSS0.Px4" title="In 2. Related Work ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Joint Optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Motivation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.SS1" title="In 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.SS1.SSS0.Px1" title="In 3.1. Model Design ‣ 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Features and Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.SS1.SSS0.Px2" title="In 3.1. Model Design ‣ 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.SS2" title="In 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Disjoint Optimization of Clustering and Retrieval Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.SS3" title="In 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>System Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1" title="In 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Hierarchical Structured Neural Network (HSNN)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS1" title="In 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS1.Px1" title="In 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Ad Modules</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS1.Px2" title="In 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Cluster Modules</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS1.Px3" title="In 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Loss Functions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS2" title="In 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS1.SSS3" title="In 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Serving</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2" title="In 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Learning To Cluster (LTC)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS1" title="In 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Curriculum Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS1.Px1" title="In 4.2.1. Curriculum Learning ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Softmax Temperature</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS1.Px2" title="In 4.2.1. Curriculum Learning ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Scheduling Strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS2" title="In 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Cluster Distribution</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS2.Px1" title="In 4.2.2. Cluster Distribution ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">FLOPs Regularizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS2.Px2" title="In 4.2.2. Cluster Distribution ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Random Replacement</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.SS2.SSS3" title="In 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Introducing Hierarchy</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS0.SSS0.Px1" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Calibration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS0.SSS0.Px2" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Normalized Entropy (NE)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS0.SSS0.Px3" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Recall@K</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS1" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Interaction Arch and features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS2" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Co-Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.SS3" title="In 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>LTC Algorithm Ablation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.SS1" title="In 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Online Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.SS1.SSS0.Px1" title="In 6.1. Online Results ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title">Relevance &amp; Efficiency Metric</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.SS2" title="In 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Deployment Lessons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.SS2.SSS1" title="In 6.2. Deployment Lessons ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Cluster Collapse</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.SS2.SSS2" title="In 6.2. Deployment Lessons ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Staleness of Cluster Centroids</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S7" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Next Steps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S8" title="In Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Hierarchical Structured Neural Network for Retrieval</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaushik Rangadurai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:krangadu@meta.com">krangadu@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siyang Yuan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:syyuan@meta.com">syyuan@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minhui Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mhhuang@meta.com">mhhuang@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiqun Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yiqliu@meta.com">yiqliu@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Golnaz Ghasemiesfeh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:golnazghasemi@meta.com">golnazghasemi@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunchen Pu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:pyc40@meta.com">pyc40@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id22.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id23.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id24.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinfeng Xie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xinfeng@meta.com">xinfeng@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Bellevue</span><span class="ltx_text ltx_affiliation_state" id="id27.3.id3">WA</span><span class="ltx_text ltx_affiliation_country" id="id28.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingfeng He
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xingfenghe@meta.com">xingfenghe@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id29.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id30.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id31.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id32.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fangzhou Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fxu@meta.com">fxu@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id33.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id34.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id35.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id36.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew Cui
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:andycui97@meta.com">andycui97@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id37.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id38.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id39.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id40.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vidhoon Viswanathan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:vidhoon@meta.com">vidhoon@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id41.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id42.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id43.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id44.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yan Dong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:dongyan@meta.com">dongyan@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id45.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id46.2.id2">Bellevue</span><span class="ltx_text ltx_affiliation_state" id="id47.3.id3">WA</span><span class="ltx_text ltx_affiliation_country" id="id48.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liang Xiong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lxiong@meta.com">lxiong@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id49.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id50.2.id2">Menlo Park</span><span class="ltx_text ltx_affiliation_state" id="id51.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id52.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lin Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ylin1@meta.com">ylin1@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id53.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id54.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id55.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id56.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liang Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liangwang@meta.com">liangwang@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id57.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id58.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id59.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id60.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiyan Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chocjy@meta.com">chocjy@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id61.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id62.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id63.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id64.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chonglin Sun
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:clsun@meta.com">clsun@meta.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id65.1.id1">Meta Platforms Inc.</span><span class="ltx_text ltx_affiliation_city" id="id66.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id67.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id68.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id69.id1">Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage in (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to learn embeddings for both users and items (ads). It then employs an Approximate Nearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for a specific user. Despite the recent rise to popularity in the industry, they have a couple of limitations. Firstly, Two Tower model architecture uses a single dot product interaction which despite their efficiency fail to capture the data distribution in practice. Secondly, the centroid representation and cluster assignment, which are components of ANN, occur after the training process has been completed. As a result, they do not take into account the optimization criteria used for retrieval model. In this paper, we present <span class="ltx_text ltx_font_italic" id="id69.id1.1">Hierarchical Structured Neural Network</span> (HSNN), a deployed jointly optimized hierarchical clustering and neural network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost. We achieve <span class="ltx_text ltx_font_bold" id="id69.id1.2">6.5%</span> improvement in offline evaluation and also demonstrate <span class="ltx_text ltx_font_bold" id="id69.id1.3">1.22%</span> online gains through A/B experiments. HSNN has been successfully deployed into the Ads Recommendation system and is currently handling major portion of the traffic. The paper shares our experience in developing this system, dealing with challenges like freshness, volatility, cold start recommendations, cluster collapse and lessons deploying the model in a large scale retrieval production system.</p>
</div>
<div class="ltx_keywords">Deep Retrieval, Clustering, Recommender Systems
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct conference title from your rights confirmation emai; June 03–05, 2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine Learning</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine learning algorithms and recommender systems play a vital role in identifying the potential interest of an ad for a specific user, which ultimately enhances the efficiency of the Ad marketplace. In order to tackle a very large number of candidate ads per request, a common practice in the industry <cite class="ltx_cite ltx_citemacro_citep">(Gallagher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib11" title="">2019</a>; Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib6" title="">2016</a>)</cite> is to build a cascade of recommendation systems of increasing computational cost. In this paper we focus on the first stage - known as the retrieval stage that narrows down from millions of candidates to a few thousands.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The Retrieval stage is limited by strict infrastructure constraints, making model architecture like Siamese Networks <cite class="ltx_cite ltx_citemacro_citep">(Bromley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib3" title="">1993</a>)</cite> a common choice. Siamese networks, or also called as Dual Encoder or Two Tower, incorporate a late fusion technique. Each of the towers output a fixed-sized representation (embedding) and rely on dot-product interaction. Embedding Based Retrieval (EBR) <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib18" title="">2020</a>)</cite> using approximate nearest neighbors (ANN) search <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib20" title="">2017</a>)</cite> algorithms are commonly used to retrieve top relevant candidates.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite their popularity in the industry <cite class="ltx_cite ltx_citemacro_citep">(Engineering, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib10" title="">2021</a>; Doshi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib8" title="">2020</a>; Hervé Jegou, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib16" title="">2017</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib14" title="">2020</a>)</cite>, EBR has a few disadvantages. Firstly, EBR relies on the Siamese Network model architecture which restricts the interaction on both the model (dot-product interactions) and feature space (only user or ad features). Secondly, ANN is a post-training operation and is not aware of the retrieval model optimization criteria.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We propose <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Hierarchical Structured Neural Network</span> (HSNN) that can be used as a drop-in replacement to any Embedding Based Retrieval (EBR) system. In HSNN, ads are organized into a hierarchy of clusters that are jointly trained with neural network. Under this framework, it is possible to adopt advanced ML model architectures and interaction features which rely on both the user and ad that are more commonly found in the ranking system. To learn the hierarchical structure, we introduce a gradient-descent based clustering algorithm - Learning To Cluster (LTC) that co-trains both the hierarchical structure and the retrieval neural network together.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">There were several challenges in building Hierarchical Structured Neural Network:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Cardinality and Volatility</span>: We handle a large volume of ads, which have a short lifespan of only a few days or weeks. This poses significant challenges in maintaining up-to-date and relevant cluster indices and representations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">Embedding Distribution Shift</span>: The retrieval model is trained in an online continuous manner which enables it to stay up-to-date with new ads. However, with online training, the retrieval model keeps publishing new snapshots. This implies that even for the same inference input, the computed user and ad embeddings are changing over time. This presents additional challenges when constructing extensive embedding indices for ad retrieval.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">Cluster Distribution</span>: It is common practice that while developing clustering algorithms, most of the items belong to very few clusters. This problem is known as cluster collapse. In order to build a scalable retrieval system with sub-linear inference cost, it is imperative that the cluster distribution is more uniform.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The primary contributions of the paper include:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce a Hierarchical Structured Neural Network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We propose a jointly optimized cluster learning framework that can be integrated to any model architecture.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">We demonstrate the effectiveness of our proposed approach and showcase substantial performance improvements in both offline evaluation and online A/B experiments.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Organization</span>: The rest of the paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S2" title="2. Related Work ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a> talks about the related work in academia and industry, Section  <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3" title="3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a> talks about the current production setting and what motivated the new design. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4" title="4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a> introduces the retrieval system and <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">Hierarchical Structured Neural Network</span>. Section  <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5" title="5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> highlights the source of gain from various components in the model and Section <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6" title="6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrates the gains in online production setting.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Clustering Methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">At a high level, clustering algorithms can be generally classified into two main categories: hierarchical and partitional methods. Agglomerative clustering is a hierarchical clustering algorithm that starts with many small clusters and gradually merges them together. Regarding partitioning clustering techniques, the most widely recognized is K-means <cite class="ltx_cite ltx_citemacro_citep">(MacQueen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib24" title="">1967</a>)</cite>, which aims to minimize the sum of squared distances between data points and their closest cluster centers. Other line of work in this category include expectation maximization (EM) <cite class="ltx_cite ltx_citemacro_citep">(Dempster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib7" title="">2018</a>)</cite>, spectral clustering, and non-negative matrix factorization (NMF) <cite class="ltx_cite ltx_citemacro_citep">(Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib4" title="">2009</a>)</cite> based clustering.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Embedding Based Retrieval</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Embedding based retrieval (EBR) has been successfully applied in retrieval layers for search and recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib23" title="">2021</a>; Rangadurai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib30" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib18" title="">2020</a>)</cite> extends this idea to incorporate text, user, context and social graph information into a unified embedding to retrieve documents that are not only relevant to the query but also personalized to the user. On the system side, a significant number of companies have built or deployed approximate nearest neighbor algorithms that can find the top-k candidates given a query embedding in sub-linear time <cite class="ltx_cite ltx_citemacro_citep">(Engineering, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib10" title="">2021</a>; Hervé Jegou, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib16" title="">2017</a>; Doshi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib8" title="">2020</a>)</cite>. Efficient MIPS or ANN algorithms include tree-based algorithms <cite class="ltx_cite ltx_citemacro_citep">(Houle and Nett, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib17" title="">2015</a>; Muja and Lowe, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib28" title="">2014</a>)</cite>, locality sensitive hashing (LSH) <cite class="ltx_cite ltx_citemacro_citep">(Shrivastava and Li, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib31" title="">2014</a>; Spring and Shrivastava, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib32" title="">2017</a>)</cite>, product quantization (PQ) <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib13" title="">2014</a>; Jégou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib21" title="">2011</a>)</cite>, hierarchical navigable small world graphs (HNSW) <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib25" title="">2018</a>)</cite>, etc. Another line of research attempts to encode a vector in a discrete latent space. Vector Quantized-Variational AutoEncoder (VQ-VAE) <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib35" title="">2018</a>)</cite> proposes a simple yet powerful generative model to learn a discrete representation. HashRec <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib22" title="">2019</a>)</cite> and Merged-Averaged Classifiers via Hashing <cite class="ltx_cite ltx_citemacro_citep">(Medini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib27" title="">2019</a>)</cite> use multi-index hash functions to encode items in large recommendation systems. Hierarchical quantization methods like Residual Quantized Variational AutoEncoder (RQ-VAE) <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib38" title="">2021</a>)</cite> and Tree-VAE <cite class="ltx_cite ltx_citemacro_citep">(Manduchi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib26" title="">2023</a>)</cite> are also used to learn tree structure of a vector. However, all of these systems either assume a stable item vocabulary or don’t take embedding distribution shift into account.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Generative Retrieval</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Recently, generative retrieval has emerged
as a new paradigm for document retrieval <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib33" title="">2023</a>; Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib2" title="">2022</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib5" title="">2021</a>; Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib34" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib36" title="">2023</a>)</cite> and we see parallel to our line of work here with ads as documents and users as queries. The learnt codebook is similar to the hierarchical cluster ids that we propose here.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Joint Optimization</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">The line of work that most resembles our work are the joint optimized systems where the item (ad) hierarchy and the large-scale retrieval system are jointly optimized together. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib12" title="">Gao et al<span class="ltx_text">.</span></a></cite> <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib12" title="">2021</a>)</cite> propose Deep Retrieval, which encodes all items in a discrete latent space for end-to-end retrieval. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib40" title="">Zhu et al<span class="ltx_text">.</span></a></cite> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib40" title="">2018</a>)</cite> propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as deep neural networks. In this line of work, tree-based methods <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib39" title="">2019</a>; Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib41" title="">2020</a>; You et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib37" title="">2019</a>)</cite> are quite common. These methods map each item to a leaf node in a tree-based structure model and learn an objective function for the tree structure and the model parameters jointly.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Motivation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The motivation for HSNN stems from understanding the current production baseline:</p>
</div>
<div class="ltx_para" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The retrieval model uses a Two Tower architecture which limits the interaction of users and ads both in the model and features.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The clustering (as part of KNN) happens as a post-training operation and hence is not aware of the retrieval optimization criteria.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">The model is trained in a recurring fashion and hence a new embedding is available for all ads every few hours which forces new cluster indices being built. This leads to delays which affects the model quality.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Model Design</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Features and Model Architecture</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">Two Tower model is a powerful tool which is widely in the industry for the retrieval stage. The late fusion property of Two Tower model makes it possible to deploy each tower model separately. In our case, we build User-to-Ad two tower model, where one tower is the user tower (which takes in user features as input) and outputs a fixed-size vector representation of the user. The other tower is the ad tower (which takes in ad features as input) and outputs the same size vector representation for the Ad. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.F1" title="Figure 1 ‣ Features and Model Architecture ‣ 3.1. Model Design ‣ 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> shows the model architecture diagram. Note that the Two Tower architecture enforces the feature space to We can use user tower embedding to retrieve nearest neighbors in ad embedding space using Embedding-based Retrieval (EBR).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="165" id="S3.F1.g1" src="extracted/5788583/figures/ltc_ttsn.png" width="118"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Model Architecture for the current production - In a Siamese Network, we’ve user features as input to the user tower to produce user embedding and similarly for the ad side. Both of them are fed as input to the Merge Net (dot product interaction in this case with no trainable parameters).</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Training</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The retrieval model is trained in a online manner which allows the model to learn about the new Ads on the marketplace. This frequent snapshot publishing, combined with online inference, empowers the retrieval model to consistently deliver up-to-date user and ad embeddings.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Disjoint Optimization of Clustering and Retrieval Model</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The architecture diagram is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.F2" title="Figure 2 ‣ 3.3. System Architecture ‣ 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>. We identify 2 components -</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">A retrieval model like Siamese Networks that takes a ¡user, ad¿ pair and predicts if the user would engage on the given ad.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">An Embedding Based Retrieval System that takes the Ad representations learnt from the ranking model and creates an index through clustering. Given the user embedding (query), it then searches over this index and finds the top-k ads for a given user.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">While each of the components could be individually improved to build an overall improved system, we hypothesized that there exists gains in building a co-trained system where the individual components are optimized to work well together. As a result, we propose a jointly-optimized clustering and retrieval model that can produce better gains and also generalize to advanced model architectures with interaction features.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>System Architecture</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The following steps are performed to fetch the required cluster data and serve model at request time:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i1.p1.1.1">Data loading and Clustering</span>: The ads embedding needed for clustering from active ads are loaded. Standalone clustering algorithms such as Approximate Nearest Neighbor (ANN) with FAISS <cite class="ltx_cite ltx_citemacro_citep">(Douze et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib9" title="">2024</a>)</cite> on K-means vector codec are employed to build a similarity index using ad embeddings. This process is repeated at different cluster levels to generate hierarchical cluster data.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i2.p1.1.1">Indexing</span>: Generated cluster information is used to build the ads index containing mapping to clusters at different levels of hierarchy. This index is shipped for serving ads at request time.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I3.i3.p1.1.1">Serving</span>: User/Request info, cluster info and any interaction feature data are used to order clusters. Ads are processed based on cluster ordering to fetch top-k ads for given user requests.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Data loading &amp; Clustering steps are repeated as new model snapshots are available from training and as active ads pool changes to ensure cluster information is available at serving time. This incurs overhead in terms of compute resources and time taken.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S3.F2.g1" src="extracted/5788583/figures/ltc_kmeans_serving.jpg" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Serving Architecture of the current production system. In the Data loading &amp; Clustering stage, we fetch the ad embeddings by calling the model predictor and calculating the cluster assignment and centroid embedding. At the indexing stage, the assignment and centroid embedding is indexed. At query time, the user embedding and centroid embedding is used to fetch the top N ads which is then passed on to the ranking system.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Proposed Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we introduce HSNN, describe the LTC algorithm and also show how it could be applied to sophisticated model architectures.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Hierarchical Structured Neural Network (HSNN)</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Modeling</h4>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S4.F3.g1" src="extracted/5788583/figures/hsnn_1_clustering_module.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Compared to the Siamese Network in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S3.F1" title="Figure 1 ‣ Features and Model Architecture ‣ 3.1. Model Design ‣ 3. Motivation ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>, in addition to the user tower and ad tower, it uses interaction features that are fed as input to the interaction tower to produce interaction embedding. The MergeNet takes 3 embeddings as input (user, ad and interaction), passes them through another MLP to produce the logit.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The retrieval stage model architecture is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.F3" title="Figure 3 ‣ 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>. The ad modules have been marked in blue, while the clustering modules have been marked in pink.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Ad Modules</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p1.1">It consists of 3 decoupled towers - the user tower, the ad tower and the interaction tower. Each of the towers, takes in corresponding features (e.g. user features for user tower) as input and produces a fixed size representation as an intermediate output. The 3 intermediate outputs (user, ad and interaction) are then fed to a Merge Net. The MergeNet aims to captures higher order interactions and as the name suggests, it merges the inputs to produce the logit.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Cluster Modules</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px2.p1.1">In order to integrate the Learning To Cluster (LTC) algorithm (Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#alg1" title="Algorithm 1 ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>) into the retrieval model, we propose changes as following.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.Px2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i1.p1.1.1">Ad Cluster Tower</span> - The ad cluster embedding is generated based on the Ad embedding using the LTC algorithm, which we’ll describe in the next section.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.1">Interaction Cluster Tower</span> - The user embedding and ad cluster embedding are fed into the interaction tower, where they are processed through an MLP layer to generate the cluster interaction embedding.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Loss Functions</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px3.p1.1">There are 2 main supervision losses - User¡¿Ad supervision loss and User¡¿Cluster supervision loss (discussed in line 9 in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#alg1" title="Algorithm 1 ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>). Both of these are also calibrated to ensure that the model doesn’t under-predict or over-predict even at the cluster level. This is also one of the advantages of co-training the clustering module with the ad module. Besides the above supervision loss, we’ve added a couple of auxillary losses that help in the stability of the model - a. MSE loss between the Ad Tower and Ad Cluster tower helps in ensuring that the cluster embedding resembles the ad embedding and b. MSE loss between the Ad Interaction and Ad Cluster Interaction performs a similar role.The cluster modules aim to learn from its ad module counterpart.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Indexing</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The retrieval model is split into 5 parts for serving - the user tower, ad tower, interaction tower, cluster model and over-arch model.</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">The user tower takes in user features and returns the user embedding.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">The Ad tower takes in ad features and returns the ad embedding and cluster assignment (IDs) across all hierarchies. This part of the model is also used to fetch the cluster assignment for new ads and ensure there is no staleness in the system. As the LTC algorithm is lightweight, this doesn’t hurt the inference QPS.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">The interaction tower takes in interaction features (of both user and ad) and returns an embedding for the ¡user-ad¿ interaction.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">The cluster model returns the cluster embeddings for all hierarchies.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">The over-arch model takes input from user-tower, ad-tower, interaction-tower and returns predicted score.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">The cluster ids across all hierarchies are now indexed i.e. an inverted index is built with each of the hierarchy acting as a separate index term. The cluster ID terms also have the relevant model information like cluster embeddings and the interaction model with them.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S4.F4.g1" src="extracted/5788583/figures/ltc_architecture.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>System architecture of the proposed HSNN system. A key difference is that the cluster ids and embeddings are now available as part of the model publisher right away.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Serving</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Now that we’ve indexed the cluster assignment for each of the ads, we’re ready to serve them. We start by fetching the user embedding given the user ID. For each of the clusters, we use the cluster embedding for the cluster tower and a MLP + concatenation of the user embedding and the cluster embedding to get the interaction embedding. All these 3 embeddings are used to fetch the score for the cluster. As the number of clusters is much lesser than the number of Ads, this is relatively a small cost and can be performed in batch. Once we’ve the cluster scores, we then sort the clusters in the decreasing order of the scores. All the ads that belong to the top clusters are then pushed to a queue to be scored by the next stage ranker. This is then followed by all the ads in the next cluster and so on until either the time budget runs out or we’ve finished scoring all the ads.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Learning To Cluster (LTC)</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">LTC is a gradient-descent based clustering algorithm that takes the ad embedding as input and returns both the cluster assignment and centroid embedding. The LTC algorithm is described in detail in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#alg1" title="Algorithm 1 ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> and can be explained through the following steps.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Initialize cluster centroids of shape (num clusters <math alttext="\times" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mo id="S4.I3.i1.p1.1.m1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><times id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.1.m1.1d">×</annotation></semantics></math> embedding dim).</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">For every Ad in the batch, perform steps as follows.</p>
<ul class="ltx_itemize" id="S4.I3.i2.I1">
<li class="ltx_item" id="S4.I3.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I3.i2.I1.i1.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i1.p1.1">Compute the distance between an Ad and every cluster centroid.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I3.i2.I1.i2.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i2.p1.1">Take the softmax of the distance to get the ad ¡¿ cluster affinity score. We call this the soft assignment of an ad to a cluster.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S4.I3.i2.I1.i3.p1">
<p class="ltx_p" id="S4.I3.i2.I1.i3.p1.1">Take the weighted sum of the learned cluster embedding to get the cluster embedding for this Ad.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Learning To Cluster (LTC)</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l0">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l0.1.1.1" style="font-size:80%;">0:</span></span>  Entity Embedding module, number of clusters (K)

</div>
<div class="ltx_listingline" id="alg1.l0a">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l0a.1.1.1" style="font-size:80%;">0:</span></span>  Center Embeddings

</div>
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span>  Initialize <math alttext="K" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_K</annotation></semantics></math> center embeddings <math alttext="\{c_{k}\}_{k=1}^{K}" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><msubsup id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mrow id="alg1.l1.m2.1.1.1.1.1" xref="alg1.l1.m2.1.1.1.1.2.cmml"><mo id="alg1.l1.m2.1.1.1.1.1.2" stretchy="false" xref="alg1.l1.m2.1.1.1.1.2.cmml">{</mo><msub id="alg1.l1.m2.1.1.1.1.1.1" xref="alg1.l1.m2.1.1.1.1.1.1.cmml"><mi id="alg1.l1.m2.1.1.1.1.1.1.2" xref="alg1.l1.m2.1.1.1.1.1.1.2.cmml">c</mi><mi id="alg1.l1.m2.1.1.1.1.1.1.3" xref="alg1.l1.m2.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="alg1.l1.m2.1.1.1.1.1.3" stretchy="false" xref="alg1.l1.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="alg1.l1.m2.1.1.1.3" xref="alg1.l1.m2.1.1.1.3.cmml"><mi id="alg1.l1.m2.1.1.1.3.2" xref="alg1.l1.m2.1.1.1.3.2.cmml">k</mi><mo id="alg1.l1.m2.1.1.1.3.1" xref="alg1.l1.m2.1.1.1.3.1.cmml">=</mo><mn id="alg1.l1.m2.1.1.1.3.3" xref="alg1.l1.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1">superscript</csymbol><apply id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.2.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><set id="alg1.l1.m2.1.1.1.1.2.cmml" xref="alg1.l1.m2.1.1.1.1.1"><apply id="alg1.l1.m2.1.1.1.1.1.1.cmml" xref="alg1.l1.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.1.1.1.1.cmml" xref="alg1.l1.m2.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.1.1.1.1.2.cmml" xref="alg1.l1.m2.1.1.1.1.1.1.2">𝑐</ci><ci id="alg1.l1.m2.1.1.1.1.1.1.3.cmml" xref="alg1.l1.m2.1.1.1.1.1.1.3">𝑘</ci></apply></set><apply id="alg1.l1.m2.1.1.1.3.cmml" xref="alg1.l1.m2.1.1.1.3"><eq id="alg1.l1.m2.1.1.1.3.1.cmml" xref="alg1.l1.m2.1.1.1.3.1"></eq><ci id="alg1.l1.m2.1.1.1.3.2.cmml" xref="alg1.l1.m2.1.1.1.3.2">𝑘</ci><cn id="alg1.l1.m2.1.1.1.3.3.cmml" type="integer" xref="alg1.l1.m2.1.1.1.3.3">1</cn></apply></apply><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">\{c_{k}\}_{k=1}^{K}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">{ italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>  <span class="ltx_text ltx_font_bold" id="alg1.l2.2">while</span> we don’t want to stop training:) <span class="ltx_text ltx_font_bold" id="alg1.l2.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span>     Read a mini-batch <math alttext="it" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">i</mi><mo id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><times id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></times><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑖</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">it</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_i italic_t</annotation></semantics></math> from <math alttext="B" class="ltx_Math" display="inline" id="alg1.l3.m2.1"><semantics id="alg1.l3.m2.1a"><mi id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m2.1d">italic_B</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l4.2">for</span> each example in <math alttext="\hat{B}" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mover accent="true" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">B</mi><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><ci id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1">^</ci><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\hat{B}</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">over^ start_ARG italic_B end_ARG</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l4.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span>        Assume this example corresponds to user <math alttext="i" class="ltx_Math" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">italic_i</annotation></semantics></math> and entity (ad) to cluster <math alttext="j" class="ltx_Math" display="inline" id="alg1.l5.m2.1"><semantics id="alg1.l5.m2.1a"><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m2.1d">italic_j</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span>        Generate User embeddings <math alttext="u_{i}" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><msub id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">u</mi><mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1">subscript</csymbol><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑢</ci><ci id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">u_{i}</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and Ad embeddings <math alttext="v_{j}" class="ltx_Math" display="inline" id="alg1.l6.m2.1"><semantics id="alg1.l6.m2.1a"><msub id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><mi id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml">v</mi><mi id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><csymbol cd="ambiguous" id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1">subscript</csymbol><ci id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2">𝑣</ci><ci id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">v_{j}</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.1d">italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span>        Compute distance to cluster centers as <math alttext="a_{k}=\frac{e^{-\alpha\|v_{j}-c_{k}\|^{2}}}{\sum_{k^{\prime}}e^{-\|v_{j}-c_{k^%
{\prime}}\|^{2}}}" class="ltx_Math" display="inline" id="alg1.l7.m1.2"><semantics id="alg1.l7.m1.2a"><mrow id="alg1.l7.m1.2.3" xref="alg1.l7.m1.2.3.cmml"><msub id="alg1.l7.m1.2.3.2" xref="alg1.l7.m1.2.3.2.cmml"><mi id="alg1.l7.m1.2.3.2.2" xref="alg1.l7.m1.2.3.2.2.cmml">a</mi><mi id="alg1.l7.m1.2.3.2.3" xref="alg1.l7.m1.2.3.2.3.cmml">k</mi></msub><mo id="alg1.l7.m1.2.3.1" xref="alg1.l7.m1.2.3.1.cmml">=</mo><mfrac id="alg1.l7.m1.2.2" xref="alg1.l7.m1.2.2.cmml"><msup id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml"><mi id="alg1.l7.m1.1.1.1.3" xref="alg1.l7.m1.1.1.1.3.cmml">e</mi><mrow id="alg1.l7.m1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.cmml"><mo id="alg1.l7.m1.1.1.1.1.1a" xref="alg1.l7.m1.1.1.1.1.1.cmml">−</mo><mrow id="alg1.l7.m1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l7.m1.1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.1.3.cmml">α</mi><mo id="alg1.l7.m1.1.1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.1.2.cmml">⁢</mo><msup id="alg1.l7.m1.1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.1.cmml"><mrow id="alg1.l7.m1.1.1.1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.2.cmml"><mo id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">c</mi><mi id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="alg1.l7.m1.1.1.1.1.1.1.1.3" xref="alg1.l7.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></msup><mrow id="alg1.l7.m1.2.2.2" xref="alg1.l7.m1.2.2.2.cmml"><mstyle displaystyle="false" id="alg1.l7.m1.2.2.2.2" xref="alg1.l7.m1.2.2.2.2.cmml"><msub id="alg1.l7.m1.2.2.2.2a" xref="alg1.l7.m1.2.2.2.2.cmml"><mo id="alg1.l7.m1.2.2.2.2.2" xref="alg1.l7.m1.2.2.2.2.2.cmml">∑</mo><msup id="alg1.l7.m1.2.2.2.2.3" xref="alg1.l7.m1.2.2.2.2.3.cmml"><mi id="alg1.l7.m1.2.2.2.2.3.2" xref="alg1.l7.m1.2.2.2.2.3.2.cmml">k</mi><mo id="alg1.l7.m1.2.2.2.2.3.3" xref="alg1.l7.m1.2.2.2.2.3.3.cmml">′</mo></msup></msub></mstyle><msup id="alg1.l7.m1.2.2.2.3" xref="alg1.l7.m1.2.2.2.3.cmml"><mi id="alg1.l7.m1.2.2.2.3.2" xref="alg1.l7.m1.2.2.2.3.2.cmml">e</mi><mrow id="alg1.l7.m1.2.2.2.1.1" xref="alg1.l7.m1.2.2.2.1.1.cmml"><mo id="alg1.l7.m1.2.2.2.1.1a" xref="alg1.l7.m1.2.2.2.1.1.cmml">−</mo><msup id="alg1.l7.m1.2.2.2.1.1.1" xref="alg1.l7.m1.2.2.2.1.1.1.cmml"><mrow id="alg1.l7.m1.2.2.2.1.1.1.1.1" xref="alg1.l7.m1.2.2.2.1.1.1.1.2.cmml"><mo id="alg1.l7.m1.2.2.2.1.1.1.1.1.2" stretchy="false" xref="alg1.l7.m1.2.2.2.1.1.1.1.2.1.cmml">‖</mo><mrow id="alg1.l7.m1.2.2.2.1.1.1.1.1.1" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.cmml"><msub id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.cmml"><mi id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.2" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.2.cmml">v</mi><mi id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.3" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.1" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.1.cmml">−</mo><msub id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.cmml"><mi id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.2" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml">c</mi><msup id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml"><mi id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.2" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.3" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.3.cmml">′</mo></msup></msub></mrow><mo id="alg1.l7.m1.2.2.2.1.1.1.1.1.3" stretchy="false" xref="alg1.l7.m1.2.2.2.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="alg1.l7.m1.2.2.2.1.1.1.3" xref="alg1.l7.m1.2.2.2.1.1.1.3.cmml">2</mn></msup></mrow></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.2b"><apply id="alg1.l7.m1.2.3.cmml" xref="alg1.l7.m1.2.3"><eq id="alg1.l7.m1.2.3.1.cmml" xref="alg1.l7.m1.2.3.1"></eq><apply id="alg1.l7.m1.2.3.2.cmml" xref="alg1.l7.m1.2.3.2"><csymbol cd="ambiguous" id="alg1.l7.m1.2.3.2.1.cmml" xref="alg1.l7.m1.2.3.2">subscript</csymbol><ci id="alg1.l7.m1.2.3.2.2.cmml" xref="alg1.l7.m1.2.3.2.2">𝑎</ci><ci id="alg1.l7.m1.2.3.2.3.cmml" xref="alg1.l7.m1.2.3.2.3">𝑘</ci></apply><apply id="alg1.l7.m1.2.2.cmml" xref="alg1.l7.m1.2.2"><divide id="alg1.l7.m1.2.2.3.cmml" xref="alg1.l7.m1.2.2"></divide><apply id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1">superscript</csymbol><ci id="alg1.l7.m1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.3">𝑒</ci><apply id="alg1.l7.m1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1"><minus id="alg1.l7.m1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1"></minus><apply id="alg1.l7.m1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1"><times id="alg1.l7.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.2"></times><ci id="alg1.l7.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.3">𝛼</ci><apply id="alg1.l7.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="alg1.l7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="alg1.l7.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1"><minus id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝑣</ci><ci id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝑐</ci><ci id="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="alg1.l7.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply><cn id="alg1.l7.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="alg1.l7.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply><apply id="alg1.l7.m1.2.2.2.cmml" xref="alg1.l7.m1.2.2.2"><apply id="alg1.l7.m1.2.2.2.2.cmml" xref="alg1.l7.m1.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.2.1.cmml" xref="alg1.l7.m1.2.2.2.2">subscript</csymbol><sum id="alg1.l7.m1.2.2.2.2.2.cmml" xref="alg1.l7.m1.2.2.2.2.2"></sum><apply id="alg1.l7.m1.2.2.2.2.3.cmml" xref="alg1.l7.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.2.3.1.cmml" xref="alg1.l7.m1.2.2.2.2.3">superscript</csymbol><ci id="alg1.l7.m1.2.2.2.2.3.2.cmml" xref="alg1.l7.m1.2.2.2.2.3.2">𝑘</ci><ci id="alg1.l7.m1.2.2.2.2.3.3.cmml" xref="alg1.l7.m1.2.2.2.2.3.3">′</ci></apply></apply><apply id="alg1.l7.m1.2.2.2.3.cmml" xref="alg1.l7.m1.2.2.2.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.3.1.cmml" xref="alg1.l7.m1.2.2.2.3">superscript</csymbol><ci id="alg1.l7.m1.2.2.2.3.2.cmml" xref="alg1.l7.m1.2.2.2.3.2">𝑒</ci><apply id="alg1.l7.m1.2.2.2.1.1.cmml" xref="alg1.l7.m1.2.2.2.1.1"><minus id="alg1.l7.m1.2.2.2.1.1.2.cmml" xref="alg1.l7.m1.2.2.2.1.1"></minus><apply id="alg1.l7.m1.2.2.2.1.1.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.1.1.1.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1">superscript</csymbol><apply id="alg1.l7.m1.2.2.2.1.1.1.1.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1"><csymbol cd="latexml" id="alg1.l7.m1.2.2.2.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.2">norm</csymbol><apply id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1"><minus id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.1"></minus><apply id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.2">𝑣</ci><ci id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.3.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.2">𝑐</ci><apply id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3">superscript</csymbol><ci id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.2">𝑘</ci><ci id="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="alg1.l7.m1.2.2.2.1.1.1.1.1.1.3.3.3">′</ci></apply></apply></apply></apply><cn id="alg1.l7.m1.2.2.2.1.1.1.3.cmml" type="integer" xref="alg1.l7.m1.2.2.2.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.2c">a_{k}=\frac{e^{-\alpha\|v_{j}-c_{k}\|^{2}}}{\sum_{k^{\prime}}e^{-\|v_{j}-c_{k^%
{\prime}}\|^{2}}}</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.2d">italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_α ∥ italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT - ∥ italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span>        Compute the weighted center as <math alttext="\bar{c}=\sum_{k}a_{k}c_{k}" class="ltx_Math" display="inline" id="alg1.l8.m1.1"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mover accent="true" id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml"><mi id="alg1.l8.m1.1.1.2.2" xref="alg1.l8.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l8.m1.1.1.2.1" xref="alg1.l8.m1.1.1.2.1.cmml">¯</mo></mover><mo id="alg1.l8.m1.1.1.1" rspace="0.111em" xref="alg1.l8.m1.1.1.1.cmml">=</mo><mrow id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><msub id="alg1.l8.m1.1.1.3.1" xref="alg1.l8.m1.1.1.3.1.cmml"><mo id="alg1.l8.m1.1.1.3.1.2" xref="alg1.l8.m1.1.1.3.1.2.cmml">∑</mo><mi id="alg1.l8.m1.1.1.3.1.3" xref="alg1.l8.m1.1.1.3.1.3.cmml">k</mi></msub><mrow id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml"><msub id="alg1.l8.m1.1.1.3.2.2" xref="alg1.l8.m1.1.1.3.2.2.cmml"><mi id="alg1.l8.m1.1.1.3.2.2.2" xref="alg1.l8.m1.1.1.3.2.2.2.cmml">a</mi><mi id="alg1.l8.m1.1.1.3.2.2.3" xref="alg1.l8.m1.1.1.3.2.2.3.cmml">k</mi></msub><mo id="alg1.l8.m1.1.1.3.2.1" xref="alg1.l8.m1.1.1.3.2.1.cmml">⁢</mo><msub id="alg1.l8.m1.1.1.3.2.3" xref="alg1.l8.m1.1.1.3.2.3.cmml"><mi id="alg1.l8.m1.1.1.3.2.3.2" xref="alg1.l8.m1.1.1.3.2.3.2.cmml">c</mi><mi id="alg1.l8.m1.1.1.3.2.3.3" xref="alg1.l8.m1.1.1.3.2.3.3.cmml">k</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><eq id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1"></eq><apply id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2"><ci id="alg1.l8.m1.1.1.2.1.cmml" xref="alg1.l8.m1.1.1.2.1">¯</ci><ci id="alg1.l8.m1.1.1.2.2.cmml" xref="alg1.l8.m1.1.1.2.2">𝑐</ci></apply><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><apply id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.1.1.cmml" xref="alg1.l8.m1.1.1.3.1">subscript</csymbol><sum id="alg1.l8.m1.1.1.3.1.2.cmml" xref="alg1.l8.m1.1.1.3.1.2"></sum><ci id="alg1.l8.m1.1.1.3.1.3.cmml" xref="alg1.l8.m1.1.1.3.1.3">𝑘</ci></apply><apply id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2"><times id="alg1.l8.m1.1.1.3.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.1"></times><apply id="alg1.l8.m1.1.1.3.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.2.1.cmml" xref="alg1.l8.m1.1.1.3.2.2">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.2.2.cmml" xref="alg1.l8.m1.1.1.3.2.2.2">𝑎</ci><ci id="alg1.l8.m1.1.1.3.2.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.2.3">𝑘</ci></apply><apply id="alg1.l8.m1.1.1.3.2.3.cmml" xref="alg1.l8.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.l8.m1.1.1.3.2.3.1.cmml" xref="alg1.l8.m1.1.1.3.2.3">subscript</csymbol><ci id="alg1.l8.m1.1.1.3.2.3.2.cmml" xref="alg1.l8.m1.1.1.3.2.3.2">𝑐</ci><ci id="alg1.l8.m1.1.1.3.2.3.3.cmml" xref="alg1.l8.m1.1.1.3.2.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">\bar{c}=\sum_{k}a_{k}c_{k}</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.m1.1d">over¯ start_ARG italic_c end_ARG = ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span>        Compute 
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="l=logloss(y,\langle u_{i},v_{j}\rangle)+logloss(y,\langle u_{i},\bar{c}\rangle%
)+sparsityloss(a_{k})" class="ltx_Math" display="block" id="S4.Ex1.m1.6"><semantics id="S4.Ex1.m1.6a"><mrow id="S4.Ex1.m1.6.6" xref="S4.Ex1.m1.6.6.cmml"><mi id="S4.Ex1.m1.6.6.5" xref="S4.Ex1.m1.6.6.5.cmml">l</mi><mo id="S4.Ex1.m1.6.6.4" xref="S4.Ex1.m1.6.6.4.cmml">=</mo><mrow id="S4.Ex1.m1.6.6.3" xref="S4.Ex1.m1.6.6.3.cmml"><mrow id="S4.Ex1.m1.4.4.1.1" xref="S4.Ex1.m1.4.4.1.1.cmml"><mi id="S4.Ex1.m1.4.4.1.1.3" xref="S4.Ex1.m1.4.4.1.1.3.cmml">l</mi><mo id="S4.Ex1.m1.4.4.1.1.2" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.4" xref="S4.Ex1.m1.4.4.1.1.4.cmml">o</mi><mo id="S4.Ex1.m1.4.4.1.1.2a" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.5" xref="S4.Ex1.m1.4.4.1.1.5.cmml">g</mi><mo id="S4.Ex1.m1.4.4.1.1.2b" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.6" xref="S4.Ex1.m1.4.4.1.1.6.cmml">l</mi><mo id="S4.Ex1.m1.4.4.1.1.2c" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.7" xref="S4.Ex1.m1.4.4.1.1.7.cmml">o</mi><mo id="S4.Ex1.m1.4.4.1.1.2d" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.8" xref="S4.Ex1.m1.4.4.1.1.8.cmml">s</mi><mo id="S4.Ex1.m1.4.4.1.1.2e" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.4.4.1.1.9" xref="S4.Ex1.m1.4.4.1.1.9.cmml">s</mi><mo id="S4.Ex1.m1.4.4.1.1.2f" xref="S4.Ex1.m1.4.4.1.1.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.4.4.1.1.1.1" xref="S4.Ex1.m1.4.4.1.1.1.2.cmml"><mo id="S4.Ex1.m1.4.4.1.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.4.4.1.1.1.2.cmml">(</mo><mi id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">y</mi><mo id="S4.Ex1.m1.4.4.1.1.1.1.3" xref="S4.Ex1.m1.4.4.1.1.1.2.cmml">,</mo><mrow id="S4.Ex1.m1.4.4.1.1.1.1.1.2" xref="S4.Ex1.m1.4.4.1.1.1.1.1.3.cmml"><mo id="S4.Ex1.m1.4.4.1.1.1.1.1.2.3" stretchy="false" xref="S4.Ex1.m1.4.4.1.1.1.1.1.3.cmml">⟨</mo><msub id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.2.cmml">u</mi><mi id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex1.m1.4.4.1.1.1.1.1.2.4" xref="S4.Ex1.m1.4.4.1.1.1.1.1.3.cmml">,</mo><msub id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.cmml"><mi id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.2" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.2.cmml">v</mi><mi id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.3" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S4.Ex1.m1.4.4.1.1.1.1.1.2.5" stretchy="false" xref="S4.Ex1.m1.4.4.1.1.1.1.1.3.cmml">⟩</mo></mrow><mo id="S4.Ex1.m1.4.4.1.1.1.1.4" stretchy="false" xref="S4.Ex1.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.6.6.3.4" xref="S4.Ex1.m1.6.6.3.4.cmml">+</mo><mrow id="S4.Ex1.m1.5.5.2.2" xref="S4.Ex1.m1.5.5.2.2.cmml"><mi id="S4.Ex1.m1.5.5.2.2.3" xref="S4.Ex1.m1.5.5.2.2.3.cmml">l</mi><mo id="S4.Ex1.m1.5.5.2.2.2" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.4" xref="S4.Ex1.m1.5.5.2.2.4.cmml">o</mi><mo id="S4.Ex1.m1.5.5.2.2.2a" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.5" xref="S4.Ex1.m1.5.5.2.2.5.cmml">g</mi><mo id="S4.Ex1.m1.5.5.2.2.2b" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.6" xref="S4.Ex1.m1.5.5.2.2.6.cmml">l</mi><mo id="S4.Ex1.m1.5.5.2.2.2c" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.7" xref="S4.Ex1.m1.5.5.2.2.7.cmml">o</mi><mo id="S4.Ex1.m1.5.5.2.2.2d" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.8" xref="S4.Ex1.m1.5.5.2.2.8.cmml">s</mi><mo id="S4.Ex1.m1.5.5.2.2.2e" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mi id="S4.Ex1.m1.5.5.2.2.9" xref="S4.Ex1.m1.5.5.2.2.9.cmml">s</mi><mo id="S4.Ex1.m1.5.5.2.2.2f" xref="S4.Ex1.m1.5.5.2.2.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.5.5.2.2.1.1" xref="S4.Ex1.m1.5.5.2.2.1.2.cmml"><mo id="S4.Ex1.m1.5.5.2.2.1.1.2" stretchy="false" xref="S4.Ex1.m1.5.5.2.2.1.2.cmml">(</mo><mi id="S4.Ex1.m1.3.3" xref="S4.Ex1.m1.3.3.cmml">y</mi><mo id="S4.Ex1.m1.5.5.2.2.1.1.3" xref="S4.Ex1.m1.5.5.2.2.1.2.cmml">,</mo><mrow id="S4.Ex1.m1.5.5.2.2.1.1.1.1" xref="S4.Ex1.m1.5.5.2.2.1.1.1.2.cmml"><mo id="S4.Ex1.m1.5.5.2.2.1.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.5.5.2.2.1.1.1.2.cmml">⟨</mo><msub id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.2" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.2.cmml">u</mi><mi id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.3" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex1.m1.5.5.2.2.1.1.1.1.3" xref="S4.Ex1.m1.5.5.2.2.1.1.1.2.cmml">,</mo><mover accent="true" id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mi id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml">c</mi><mo id="S4.Ex1.m1.2.2.1" xref="S4.Ex1.m1.2.2.1.cmml">¯</mo></mover><mo id="S4.Ex1.m1.5.5.2.2.1.1.1.1.4" stretchy="false" xref="S4.Ex1.m1.5.5.2.2.1.1.1.2.cmml">⟩</mo></mrow><mo id="S4.Ex1.m1.5.5.2.2.1.1.4" stretchy="false" xref="S4.Ex1.m1.5.5.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.6.6.3.4a" xref="S4.Ex1.m1.6.6.3.4.cmml">+</mo><mrow id="S4.Ex1.m1.6.6.3.3" xref="S4.Ex1.m1.6.6.3.3.cmml"><mi id="S4.Ex1.m1.6.6.3.3.3" xref="S4.Ex1.m1.6.6.3.3.3.cmml">s</mi><mo id="S4.Ex1.m1.6.6.3.3.2" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.4" xref="S4.Ex1.m1.6.6.3.3.4.cmml">p</mi><mo id="S4.Ex1.m1.6.6.3.3.2a" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.5" xref="S4.Ex1.m1.6.6.3.3.5.cmml">a</mi><mo id="S4.Ex1.m1.6.6.3.3.2b" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.6" xref="S4.Ex1.m1.6.6.3.3.6.cmml">r</mi><mo id="S4.Ex1.m1.6.6.3.3.2c" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.7" xref="S4.Ex1.m1.6.6.3.3.7.cmml">s</mi><mo id="S4.Ex1.m1.6.6.3.3.2d" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.8" xref="S4.Ex1.m1.6.6.3.3.8.cmml">i</mi><mo id="S4.Ex1.m1.6.6.3.3.2e" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.9" xref="S4.Ex1.m1.6.6.3.3.9.cmml">t</mi><mo id="S4.Ex1.m1.6.6.3.3.2f" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.10" xref="S4.Ex1.m1.6.6.3.3.10.cmml">y</mi><mo id="S4.Ex1.m1.6.6.3.3.2g" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.11" xref="S4.Ex1.m1.6.6.3.3.11.cmml">l</mi><mo id="S4.Ex1.m1.6.6.3.3.2h" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.12" xref="S4.Ex1.m1.6.6.3.3.12.cmml">o</mi><mo id="S4.Ex1.m1.6.6.3.3.2i" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.13" xref="S4.Ex1.m1.6.6.3.3.13.cmml">s</mi><mo id="S4.Ex1.m1.6.6.3.3.2j" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mi id="S4.Ex1.m1.6.6.3.3.14" xref="S4.Ex1.m1.6.6.3.3.14.cmml">s</mi><mo id="S4.Ex1.m1.6.6.3.3.2k" xref="S4.Ex1.m1.6.6.3.3.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.6.6.3.3.1.1" xref="S4.Ex1.m1.6.6.3.3.1.1.1.cmml"><mo id="S4.Ex1.m1.6.6.3.3.1.1.2" stretchy="false" xref="S4.Ex1.m1.6.6.3.3.1.1.1.cmml">(</mo><msub id="S4.Ex1.m1.6.6.3.3.1.1.1" xref="S4.Ex1.m1.6.6.3.3.1.1.1.cmml"><mi id="S4.Ex1.m1.6.6.3.3.1.1.1.2" xref="S4.Ex1.m1.6.6.3.3.1.1.1.2.cmml">a</mi><mi id="S4.Ex1.m1.6.6.3.3.1.1.1.3" xref="S4.Ex1.m1.6.6.3.3.1.1.1.3.cmml">k</mi></msub><mo id="S4.Ex1.m1.6.6.3.3.1.1.3" stretchy="false" xref="S4.Ex1.m1.6.6.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.6b"><apply id="S4.Ex1.m1.6.6.cmml" xref="S4.Ex1.m1.6.6"><eq id="S4.Ex1.m1.6.6.4.cmml" xref="S4.Ex1.m1.6.6.4"></eq><ci id="S4.Ex1.m1.6.6.5.cmml" xref="S4.Ex1.m1.6.6.5">𝑙</ci><apply id="S4.Ex1.m1.6.6.3.cmml" xref="S4.Ex1.m1.6.6.3"><plus id="S4.Ex1.m1.6.6.3.4.cmml" xref="S4.Ex1.m1.6.6.3.4"></plus><apply id="S4.Ex1.m1.4.4.1.1.cmml" xref="S4.Ex1.m1.4.4.1.1"><times id="S4.Ex1.m1.4.4.1.1.2.cmml" xref="S4.Ex1.m1.4.4.1.1.2"></times><ci id="S4.Ex1.m1.4.4.1.1.3.cmml" xref="S4.Ex1.m1.4.4.1.1.3">𝑙</ci><ci id="S4.Ex1.m1.4.4.1.1.4.cmml" xref="S4.Ex1.m1.4.4.1.1.4">𝑜</ci><ci id="S4.Ex1.m1.4.4.1.1.5.cmml" xref="S4.Ex1.m1.4.4.1.1.5">𝑔</ci><ci id="S4.Ex1.m1.4.4.1.1.6.cmml" xref="S4.Ex1.m1.4.4.1.1.6">𝑙</ci><ci id="S4.Ex1.m1.4.4.1.1.7.cmml" xref="S4.Ex1.m1.4.4.1.1.7">𝑜</ci><ci id="S4.Ex1.m1.4.4.1.1.8.cmml" xref="S4.Ex1.m1.4.4.1.1.8">𝑠</ci><ci id="S4.Ex1.m1.4.4.1.1.9.cmml" xref="S4.Ex1.m1.4.4.1.1.9">𝑠</ci><interval closure="open" id="S4.Ex1.m1.4.4.1.1.1.2.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1"><ci id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1">𝑦</ci><list id="S4.Ex1.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2"><apply id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.2">𝑢</ci><ci id="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.2">𝑣</ci><ci id="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex1.m1.4.4.1.1.1.1.1.2.2.3">𝑗</ci></apply></list></interval></apply><apply id="S4.Ex1.m1.5.5.2.2.cmml" xref="S4.Ex1.m1.5.5.2.2"><times id="S4.Ex1.m1.5.5.2.2.2.cmml" xref="S4.Ex1.m1.5.5.2.2.2"></times><ci id="S4.Ex1.m1.5.5.2.2.3.cmml" xref="S4.Ex1.m1.5.5.2.2.3">𝑙</ci><ci id="S4.Ex1.m1.5.5.2.2.4.cmml" xref="S4.Ex1.m1.5.5.2.2.4">𝑜</ci><ci id="S4.Ex1.m1.5.5.2.2.5.cmml" xref="S4.Ex1.m1.5.5.2.2.5">𝑔</ci><ci id="S4.Ex1.m1.5.5.2.2.6.cmml" xref="S4.Ex1.m1.5.5.2.2.6">𝑙</ci><ci id="S4.Ex1.m1.5.5.2.2.7.cmml" xref="S4.Ex1.m1.5.5.2.2.7">𝑜</ci><ci id="S4.Ex1.m1.5.5.2.2.8.cmml" xref="S4.Ex1.m1.5.5.2.2.8">𝑠</ci><ci id="S4.Ex1.m1.5.5.2.2.9.cmml" xref="S4.Ex1.m1.5.5.2.2.9">𝑠</ci><interval closure="open" id="S4.Ex1.m1.5.5.2.2.1.2.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1"><ci id="S4.Ex1.m1.3.3.cmml" xref="S4.Ex1.m1.3.3">𝑦</ci><list id="S4.Ex1.m1.5.5.2.2.1.1.1.2.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1"><apply id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.2">𝑢</ci><ci id="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.5.5.2.2.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><ci id="S4.Ex1.m1.2.2.1.cmml" xref="S4.Ex1.m1.2.2.1">¯</ci><ci id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2">𝑐</ci></apply></list></interval></apply><apply id="S4.Ex1.m1.6.6.3.3.cmml" xref="S4.Ex1.m1.6.6.3.3"><times id="S4.Ex1.m1.6.6.3.3.2.cmml" xref="S4.Ex1.m1.6.6.3.3.2"></times><ci id="S4.Ex1.m1.6.6.3.3.3.cmml" xref="S4.Ex1.m1.6.6.3.3.3">𝑠</ci><ci id="S4.Ex1.m1.6.6.3.3.4.cmml" xref="S4.Ex1.m1.6.6.3.3.4">𝑝</ci><ci id="S4.Ex1.m1.6.6.3.3.5.cmml" xref="S4.Ex1.m1.6.6.3.3.5">𝑎</ci><ci id="S4.Ex1.m1.6.6.3.3.6.cmml" xref="S4.Ex1.m1.6.6.3.3.6">𝑟</ci><ci id="S4.Ex1.m1.6.6.3.3.7.cmml" xref="S4.Ex1.m1.6.6.3.3.7">𝑠</ci><ci id="S4.Ex1.m1.6.6.3.3.8.cmml" xref="S4.Ex1.m1.6.6.3.3.8">𝑖</ci><ci id="S4.Ex1.m1.6.6.3.3.9.cmml" xref="S4.Ex1.m1.6.6.3.3.9">𝑡</ci><ci id="S4.Ex1.m1.6.6.3.3.10.cmml" xref="S4.Ex1.m1.6.6.3.3.10">𝑦</ci><ci id="S4.Ex1.m1.6.6.3.3.11.cmml" xref="S4.Ex1.m1.6.6.3.3.11">𝑙</ci><ci id="S4.Ex1.m1.6.6.3.3.12.cmml" xref="S4.Ex1.m1.6.6.3.3.12">𝑜</ci><ci id="S4.Ex1.m1.6.6.3.3.13.cmml" xref="S4.Ex1.m1.6.6.3.3.13">𝑠</ci><ci id="S4.Ex1.m1.6.6.3.3.14.cmml" xref="S4.Ex1.m1.6.6.3.3.14">𝑠</ci><apply id="S4.Ex1.m1.6.6.3.3.1.1.1.cmml" xref="S4.Ex1.m1.6.6.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.6.6.3.3.1.1.1.1.cmml" xref="S4.Ex1.m1.6.6.3.3.1.1">subscript</csymbol><ci id="S4.Ex1.m1.6.6.3.3.1.1.1.2.cmml" xref="S4.Ex1.m1.6.6.3.3.1.1.1.2">𝑎</ci><ci id="S4.Ex1.m1.6.6.3.3.1.1.1.3.cmml" xref="S4.Ex1.m1.6.6.3.3.1.1.1.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.6c">l=logloss(y,\langle u_{i},v_{j}\rangle)+logloss(y,\langle u_{i},\bar{c}\rangle%
)+sparsityloss(a_{k})</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.6d">italic_l = italic_l italic_o italic_g italic_l italic_o italic_s italic_s ( italic_y , ⟨ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⟩ ) + italic_l italic_o italic_g italic_l italic_o italic_s italic_s ( italic_y , ⟨ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over¯ start_ARG italic_c end_ARG ⟩ ) + italic_s italic_p italic_a italic_r italic_s italic_i italic_t italic_y italic_l italic_o italic_s italic_s ( italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table> where <math alttext="y" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_y</annotation></semantics></math> is the label

</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span>        If there are any interaction features involving Ad <math alttext="i" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_i</annotation></semantics></math>, replace the interaction tower with a mix of user (<math alttext="u_{i}" class="ltx_Math" display="inline" id="alg1.l10.m2.1"><semantics id="alg1.l10.m2.1a"><msub id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml"><mi id="alg1.l10.m2.1.1.2" xref="alg1.l10.m2.1.1.2.cmml">u</mi><mi id="alg1.l10.m2.1.1.3" xref="alg1.l10.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><apply id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1"><csymbol cd="ambiguous" id="alg1.l10.m2.1.1.1.cmml" xref="alg1.l10.m2.1.1">subscript</csymbol><ci id="alg1.l10.m2.1.1.2.cmml" xref="alg1.l10.m2.1.1.2">𝑢</ci><ci id="alg1.l10.m2.1.1.3.cmml" xref="alg1.l10.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">u_{i}</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m2.1d">italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) and ad cluster embeddings.

</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l11.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l11.3">for</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span>  <span class="ltx_text ltx_font_bold" id="alg1.l12.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l12.3">while</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span>  Publish the center embeddings for inference

</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span>  For each Ad embedding, determine the its cluster assignment based on the closeness between it and each center embedding

</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We’ve made the 3 improvements to the LTC algorithm - curriculum learning, cluster collapse and introducing hierarchy.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Curriculum Learning</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data/task/problem to harder data/task/problem. The motivation arises from how we humans learn in an academic curriculum setting - we start with simple problems, master them and gradually increase the complexity. The motivation for curriculum learning arises from 2 factors -</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">The model learns a soft assignment between an ad and a cluster during training but uses a hard assignment during inference. The soft assignment learning provides the ability to backpropagate through samples and the hard assignment is critical for sub-linear time inference.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">Soft assignment is an easier task to learn than hard assignment and one can change from soft assignment to hard assignment by changing the softmax alpha (or temperature).</p>
</div>
</li>
</ul>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Softmax Temperature</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.F5" title="Figure 5 ‣ Softmax Temperature ‣ 4.2.1. Curriculum Learning ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a> shows the impact of the temperature parameter to softmax. When the temperature value is -1, the output value is evenly distributed but as the temperature value decreases to -1000, the distribution is skewed towards the smallest value and it is almost equivalent to a hard assignment. Note that the negative value of the temperature is due to the fact that the LTC algorithm uses the L2 distance metric (lower is closer). Note that this method shares the same spirit as Gumbel Softmax <cite class="ltx_cite ltx_citemacro_citep">(Jang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib19" title="">2017</a>)</cite> with varying alpha parameter in the softmax temperature used in sampling.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="56" id="S4.F5.g1" src="extracted/5788583/figures/ltc_cl.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The plot above shows the softmax value for various temperature values using the input array [1, 1.1, 1.2]. As we increase the temperature parameter, the distribution gets more skewed towards the closest cluster. Note that the temperature is negative as we’re using softmax on L2 distances (lower is better).</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Scheduling Strategy</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1">With the understanding of the importance of softmax temperature, we can now use a scheduling algorithm to change the temperature with respect to the number of iterations. We explored numerous strategies (linear, cosine and exponential) and observed the best performance with exponential scheduling strategy (chart below). This also makes intuitive sense as the model spends most of the training time converging on the soft assignment task and then slowly increasing the complexity of the task towards hard assignment.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S4.F6.g1" src="extracted/5788583/figures/ltc_cl_schedule.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The x-axis represents the training iteration and the y-axis represents the value of the softmax temperature. In this example, we scale the softmax temperature from -1 (at iteration 1) to -10 (at final iteration). We also show various values of the exponent parameter. The chart shows a positive value for plotting purposes.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Cluster Distribution</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Clustering methods often face the challenge of cluster collapse, characterized by the model utilizing only a limited subset of cluster IDs. A balanced cluster distribution is a key factor ensuring that we can scale large models from ad level to the cluster level. One of the key assumptions of the the cluster based retrieval paradigm is that by reducing the number of inferences by a factor of num clusters, we can scale our model complexity. In order to truly achieve this, we assume a balanced cluster distribution - every cluster roughly has an equal number of ads. To address this issue, we employ the following two strategies.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">FLOPs Regularizer</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">The motivation for this idea arises from <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib29" title="">Paria et al<span class="ltx_text">.</span></a></cite> <cite class="ltx_cite ltx_citemacro_citep">(Paria et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib29" title="">2020</a>)</cite>. The primary intuition is to use the soft assignment matrix (batch size <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS2.Px1.p1.1.m1.1"><semantics id="S4.SS2.SSS2.Px1.p1.1.m1.1a"><mo id="S4.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.1.m1.1b"><times id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.Px1.p1.1.m1.1d">×</annotation></semantics></math> num clusters) to ensure that the in-batch cluster distribution is even. We introduce a regularizer that penalizes the model if all ads in a batch are assigned to the same cluster or if the cluster distribution is skewed. We achieve this by introducing a sparsity loss that minimizes the sum of squares of the mean soft assignment. While this approach works reasonably well, one disadvantage is that it is sensitive to the batch size.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Random Replacement</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px2.p1.1">We reset unused cluster embeddings vectors to a random vector within a batch to improve the utilization <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib38" title="">Zeghidour et al<span class="ltx_text">.</span></a></cite> <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib38" title="">2021</a>)</cite>. We calculate unused cluster assignments in a streaming fashion during training using an exponential moving average (EMA) approach.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Introducing Hierarchy</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.4">In Embedding Based Retrieval (EBR) systems, it is common to build a hierarchy of clusters to deal with a large number of items for retrieval. Similarly, in HSNN, we propose to introduce hierarchy through Residual Quantization. In order to extend the model architecture, we introduce <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.1.m1.1"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mi id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><ci id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.1.m1.1d">italic_N</annotation></semantics></math> Ad Cluster Towers where <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.2.m2.1"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mi id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><ci id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.2.m2.1d">italic_N</annotation></semantics></math> is the depth of the hierarchy. The input to the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.3.m3.1"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mi id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><ci id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.3.m3.1d">italic_i</annotation></semantics></math>-th Ad Cluster Tower is the residue of Ad Embedding and the (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.4.m4.1"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mi id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><ci id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.4.m4.1d">italic_i</annotation></semantics></math>-1)th Ad Cluster Embedding. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.F7" title="Figure 7 ‣ 4.2.3. Introducing Hierarchy ‣ 4.2. Learning To Cluster (LTC) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">7</span></a> shows diagram of HSNN with 2 clustering modules.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="S4.F7.g1" src="extracted/5788583/figures/ltc_hsnn_2_cluster_modules.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>HSNN with 2 clustering modules. The first clustering module is similar to the one shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.F3" title="Figure 3 ‣ 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>. The input to the second clustering module is the residue of the first centroid embedding and the ad embedding.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Ablation Studies</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we prioritize the factors that affect the machine learning model’s performance and we use the accuracy of prediction as our primary offline evaluation metric. Specifically, we utilize calibrated Normalized Entropy (NE) and Recall@K as our key offline evaluation metrics.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Calibration</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Calibration is the ratio of the average estimated Click Through Rate (CTR)/Conversion Rate (CVR) and empirical CTR/CVR. We should consider checking calibration when evaluating and comparing different models using NE as a metric. For example, it is possible for 2 models to have the same ordering of Ads but have different NE metrics. All the NE numbers reported in this paper are calibrated.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Normalized Entropy (NE)</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Normalized Entropy <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#bib.bib15" title="">2014</a>)</cite> is equivalent to the average log loss per impression divided by what the average log loss per impression would be if a model predicted the average empirical CTR/CVR of the training data set. <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">The lower the value, the better the model’s predictions.</span> In order to measure the impact of cluster, we use the cluster prediction to compute NE and call this as <span class="ltx_text ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.2">Cluster NE</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Recall@K</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.3">Recall@K gives a measure of how many of the relevant ads are present in top K out of all the relevant ads, where <math alttext="K" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.1.m1.1d">italic_K</annotation></semantics></math> is the number of recommendations generated for a user. In our case, we choose <math alttext="K" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S5.SS0.SSS0.Px3.p1.2.m2.1a"><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.2.m2.1b"><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.2.m2.1d">italic_K</annotation></semantics></math> based on the number of ads ranked by the ranking (later stage) models and for business reasons, we cannot reveal the value of <math alttext="K" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S5.SS0.SSS0.Px3.p1.3.m3.1a"><mi id="S5.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.3.m3.1b"><ci id="S5.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px3.p1.3.m3.1d">italic_K</annotation></semantics></math> here.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Interaction Arch and features</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">In this ablation study, we aim to capture the benefit of using interaction features and exploring other ¡user-ad¿ interactions instead of a dot product interaction. To do this, we ablate the interaction features and interaction tower in Figue <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S4.F3" title="Figure 3 ‣ 4.1.1. Modeling ‣ 4.1. Hierarchical Structured Neural Network (HSNN) ‣ 4. Proposed Method ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a> to validate the importance of these interaction features. We can see the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.T1" title="Table 1 ‣ 5.1. Interaction Arch and features ‣ 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>. To understand the scale of importance, we created 2 model version - 1 with <math alttext="1x" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">1</mn><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><cn id="S5.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.2">1</cn><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">1x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">1 italic_x</annotation></semantics></math> number of interaction features and another with <math alttext="20x" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">20</mn><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><times id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></times><cn id="S5.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.p1.2.m2.1.1.2">20</cn><ci id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">20x</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">20 italic_x</annotation></semantics></math>. We see more NE loss when ablating a stronger model with 20x interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.1.2">Model Architecture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1">NE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.2.1.1">Siamese Networks</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2">0.0% (reference)</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.3.2.1">Siamese + Interaction Tower (1x) + MergeNet</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2">-0.3%</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.1.4.3.1">Siamese + Interaction Tower (20x) + MergeNet</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.1.4.3.2">-0.45%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>We see more NE gains when using more interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Co-Training</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this ablation study, we aim to capture the benefit of co-training the clustering with the retrieval model. In other words, by making the clustering aware of the retrieval optimization criteria, we believe that it could improve clustering. In the EM style algorithm, we alternate between learning the cluster centroid representation given the assignment and then updating the cluster assignment given the learnt cluster centroid representation. In the LTC algorithm, both the cluster representation and ad representation are jointly optimized in a gradient descent fashion using techniques like curriculum learning to deal with discrete clustering operation. We can see the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.T2" title="Table 2 ‣ 5.2. Co-Training ‣ 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.1.1.2">Clustering Technique</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1">Cluster NE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.1.1">K-Means Clustering</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2">0.0% (reference)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.3.2.1">K-Means Init + FineTuning</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.3.2.2">-0.6%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.4.3.1">K-Means Init + EM co-Trained</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.3.2">-6.2%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.1.5.4.1">LTC co-trained</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.5.4.2">-6.5%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>K-Means init + fine-tuning on the retrieval model supervision shows minor cluster NE gains demonstrating value in this direction. Both EM and LTC co-trained showed large cluster NE gains. The LTC algorithm is a gradient-descent based clustering algorithm and this aligns with the existing inference stack making it also easier to integrate into production.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>LTC Algorithm Ablation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">There are numerous factors that bring gains to the LTC algorithm - the curriculum learning makes the LTC algorithm trainable on the discrete clustering operation. The warmup strategy makes the learning from the ad tower more smooth and the flops regularizer and the codebook reset ensure that we don’t see a cluster collapse. As we can see in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S5.T3" title="Table 3 ‣ 5.3. LTC Algorithm Ablation ‣ 5. Ablation Studies ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>, the curriculum learning brings in most of the gains. While the FLOPS regularizer and codebook reset were orirginally added to deal with cluster collapse, it is interesting to see them bring in accuracy (NE) gains as well. We attribute this to the</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.1.2">Training Technique</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1">Cluster NE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.2.1.1">K-Means Clustering</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">0.0% (reference)</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.3.2.1">LTC w/o curriculum learning</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.2">6.2%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.4.3.1">LTC w/o FLOPs regularizer</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.3.2">0.8%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.5.4.1">LTC w/o Codebook reset</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.4.2">0.4%</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.6.5.1">LTC with 2 clustering modules</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.6.5.2">-0.2%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>An ablation study of various components of LTC algorithm. Curriculum learning with variable softmax temperature showed the maximum impact. NE gains from solving cluster collapse techiques like FLOPs regularizer and codebook reset demonstate the generalization benefits of clustering.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Experiments</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Online Results</h3>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Relevance &amp; Efficiency Metric</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">The online relevance metric is proportional to revenue from high quality clicks and conversions. The motivation for this stems from the fact that we want to optimize for long-term revenue and also have a good experience for all of our users. Due to security reasons, we can’t disclose the actual online metric computation.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p2.1">We’ve deployed HSNN using the LTC algorithm (both 1 level and 2 levels of clustering) and tested the improvements over Embedding Based Retrieval (EBR) using an A/B experiment. For all of our experiments, we used the K-Means vector codec in EBR - we didn’t see any improvements in using different codecs or indices. Upon deploying the LTC algorithm in production, we observe a 0.2% improvement over K-Means algorithm in our topline metric. We’ve shared the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.T5" title="Table 5 ‣ Relevance &amp; Efficiency Metric ‣ 6.1. Online Results ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a>. For an equivalent offline metrics comparison, we’ve consolidated the offline metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.T4" title="Table 4 ‣ Relevance &amp; Efficiency Metric ‣ 6.1. Online Results ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p3.1">We attribute the relevance gains to 3 reasons -</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p4">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">Co-optimizing the clustering module with the retrieval model produces the centroid embedding that is not only optimized for within-cluster variance of an ad but also optimized for the ¡user, centroid¿ interaction.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">Use of interaction features and sophisticated MergeNet brings in additional value over using just user and ad entity features with dot product interaction.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">Freshness of centroid embedding - In the new co-trained approach, we’ve both the cluster assignment and centroid embedding as part of the model inference (no need to run a separate K-Means algorithm). This helps save time and makes the centroid embedding more fresh.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S6.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T4.2.2.3">Technique</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T4.1.1.1">Cluster NE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T4.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.m1.1a"><mo id="S6.T4.1.1.1.m1.1.1" stretchy="false" xref="S6.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2">Recall@K (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T4.2.2.2.m1.1"><semantics id="S6.T4.2.2.2.m1.1a"><mo id="S6.T4.2.2.2.m1.1.1" stretchy="false" xref="S6.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.m1.1b"><ci id="S6.T4.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.m1.1d">↑</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.1.1">K-Means</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T4.2.3.1.2">0.0% (reference)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.2.3.1.3">100% (reference)</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.2.4.2.1">Co-Trained EM Algorithm</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.2.4.2.2">-4.2%</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.4.2.3">106.3%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.2.5.3.1">HSNN with 1 clustering module</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T4.2.5.3.2">-6.5%</td>
<td class="ltx_td ltx_align_center" id="S6.T4.2.5.3.3">110.5%</td>
</tr>
<tr class="ltx_tr" id="S6.T4.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.2.6.4.1">HSNN with 2 clustering modules</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T4.2.6.4.2">-6.9%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.2.6.4.3">111.8%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>We show that the LTC algorithm can show improvement over K-Means on both NE and Recall@K.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p5">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p5.1">We attribute the efficiency gains of HSNN with 2 clustering module due to scoring significantly lower number of clusters.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T5.2.2.3">Technique</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T5.1.1.1">Relevance (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T5.1.1.1.m1.1"><semantics id="S6.T5.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.m1.1.1" stretchy="false" xref="S6.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.2.2.2">Efficiency (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T5.2.2.2.m1.1"><semantics id="S6.T5.2.2.2.m1.1a"><mo id="S6.T5.2.2.2.m1.1.1" stretchy="false" xref="S6.T5.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.m1.1b"><ci id="S6.T5.2.2.2.m1.1.1.cmml" xref="S6.T5.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.m1.1d">↑</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T5.2.3.1.1">Embedding Based Retrieval (EBR)</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T5.2.3.1.2">0% (reference)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.2.3.1.3">0% (reference)</td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T5.2.4.2.1">HSNN with 1 clustering module</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T5.2.4.2.2">1.22%</td>
<td class="ltx_td ltx_align_center" id="S6.T5.2.4.2.3">0.10%</td>
</tr>
<tr class="ltx_tr" id="S6.T5.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T5.2.5.3.1">HSNN with 2 clustering modules</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T5.2.5.3.2">1.52%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.2.5.3.3">0.23%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>We show that HSNN brings improvement on both accuracy and efficiency over a well-established baseline of EBR. We also show that the gains improve as we increase the number of hierarchical layers.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Deployment Lessons</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Cluster Collapse</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.6">One of the motivations of <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.6.1">Hierarchical Structured Neural Network</span> with 1 clustering module is that it reduces the complexity of the candidates selection from O(<math alttext="K\times C" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.1.m1.1"><semantics id="S6.SS2.SSS1.p1.1.m1.1a"><mrow id="S6.SS2.SSS1.p1.1.m1.1.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S6.SS2.SSS1.p1.1.m1.1.1.2" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.cmml">K</mi><mo id="S6.SS2.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S6.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S6.SS2.SSS1.p1.1.m1.1.1.3" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.1.m1.1b"><apply id="S6.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1"><times id="S6.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.1"></times><ci id="S6.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2">𝐾</ci><ci id="S6.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.1.m1.1c">K\times C</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.1.m1.1d">italic_K × italic_C</annotation></semantics></math>) to O(<math alttext="K" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.2.m2.1"><semantics id="S6.SS2.SSS1.p1.2.m2.1a"><mi id="S6.SS2.SSS1.p1.2.m2.1.1" xref="S6.SS2.SSS1.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.2.m2.1b"><ci id="S6.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.2.m2.1d">italic_K</annotation></semantics></math>+<math alttext="T" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.3.m3.1"><semantics id="S6.SS2.SSS1.p1.3.m3.1a"><mi id="S6.SS2.SSS1.p1.3.m3.1.1" xref="S6.SS2.SSS1.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.3.m3.1b"><ci id="S6.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.3.m3.1d">italic_T</annotation></semantics></math>), where <math alttext="K" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.4.m4.1"><semantics id="S6.SS2.SSS1.p1.4.m4.1a"><mi id="S6.SS2.SSS1.p1.4.m4.1.1" xref="S6.SS2.SSS1.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.4.m4.1b"><ci id="S6.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.4.m4.1c">K</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.4.m4.1d">italic_K</annotation></semantics></math> is the number of clusters, <math alttext="C" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.5.m5.1"><semantics id="S6.SS2.SSS1.p1.5.m5.1a"><mi id="S6.SS2.SSS1.p1.5.m5.1.1" xref="S6.SS2.SSS1.p1.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.5.m5.1b"><ci id="S6.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S6.SS2.SSS1.p1.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.5.m5.1d">italic_C</annotation></semantics></math> is the number of Ads in each cluster and <math alttext="T" class="ltx_Math" display="inline" id="S6.SS2.SSS1.p1.6.m6.1"><semantics id="S6.SS2.SSS1.p1.6.m6.1a"><mi id="S6.SS2.SSS1.p1.6.m6.1.1" xref="S6.SS2.SSS1.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.6.m6.1b"><ci id="S6.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S6.SS2.SSS1.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.6.m6.1c">T</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS1.p1.6.m6.1d">italic_T</annotation></semantics></math> is the total number of Ads to return. A uniform cluster distribution is necessary in achieving this. Without techniques like FLOPs regularizer and random replace, we observed uneven cluster distribution at best and only a fraction of clusters utilized at worst. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.F8" title="Figure 8 ‣ 6.2.1. Cluster Collapse ‣ 6.2. Deployment Lessons ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">8</span></a> shows the end result of cluster distribution and a good improvement over K-Means algorithm.</p>
</div>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S6.F8.g1" src="extracted/5788583/figures/ltc_box.png" width="314"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>By incorporating FLOPs regularizer and random replace, we observe an even cluster distribution. We demonstrate this by sharing a box-plot of the number of ads belonging to a cluster before and after the cluster regularization techniques.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Staleness of Cluster Centroids</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">Stalesness comes into picture for 2 reasons:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S6.I2">
<li class="ltx_item" id="S6.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I2.i1.p1">
<p class="ltx_p" id="S6.I2.i1.p1.1">New ads are created and the system isn’t quick enough to capture and score them.</p>
</div>
</li>
<li class="ltx_item" id="S6.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I2.i2.p1">
<p class="ltx_p" id="S6.I2.i2.p1.1">Features of existing ads are updated and the system isn’t quick enough to capture and score them.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1">We measure the impact of staleness by measuring the delta between fresh centroid embeddings vs stale centroid embeddings. As we have shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.06653v1#S6.T6" title="Table 6 ‣ 6.2.2. Staleness of Cluster Centroids ‣ 6.2. Deployment Lessons ‣ 6. Experiments ‣ Hierarchical Structured Neural Network for Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a>, we find that NE when compared to a fresh model worsens over time. It is also worth noting that the cluster assignments are relatively stable compared the the centroid embeddings.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T6.1.1.2">Num Hours Stale</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1">Cluster NE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T6.1.1.1.m1.1"><semantics id="S6.T6.1.1.1.m1.1a"><mo id="S6.T6.1.1.1.m1.1.1" stretchy="false" xref="S6.T6.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.m1.1b"><ci id="S6.T6.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T6.1.1.1.m1.1d">↓</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T6.1.2.1.1">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.2.1.2">0.0% (reference)</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.3.2.1">1</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.3.2.2">0.1%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T6.1.4.3.1">2</th>
<td class="ltx_td ltx_align_center" id="S6.T6.1.4.3.2">0.3%</td>
</tr>
<tr class="ltx_tr" id="S6.T6.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T6.1.5.4.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.5.4.2">0.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>We show the impact of staleness to the centroid embeddings. This is especially important as the LTC algorithm ensures that the centroid embeddings are available at the same time as the ad embedding without having to run disjoint-training clustering algorithm like K-Means.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion and Next Steps</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper we presented a learnable hierarchical clustering module called <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">Hierarchical Structured Neural Network</span>. We show that it is possible to jointly learn and optimize hierarchical structure and neural network. We introduce interaction features to the retrieval layer and also make the clustering module aware of these features. And HSNN has been successfully deployed to a Ads recommendation system.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In the future iterations, we plan to explore more complex interactions between the hierarchies. We also introduce personalization by incorporating clustering to the user entity as well.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Acknowledgements</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The authors would like to thank Le Fang, Tushar Tiwari, Wei Lu, Jason Liu, Trevor Waite, Shu Yan, Alexander Petrov, Dheevatsa Mudigere, Benny Chen, GP Musumeci, Bo Long, Wenlin Chen, Santanu Kolay and others who contributed, supported and collaborated with us.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen tau Yih, Sebastian Riedel, and Fabio Petroni. 2022.

</span>
<span class="ltx_bibblock">Autoregressive Search Engines: Generating Substrings as Document Identifiers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.CL/2204.10628

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bromley et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (1993)</span>
<span class="ltx_bibblock">
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1993.

</span>
<span class="ltx_bibblock">Signature Verification Using a ”Siamese” Time Delay Neural Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 6th International Conference on Neural Information Processing Systems</em> (Denver, Colorado) <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(NIPS’93)</em>. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 737–744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Deng Cai, Xiaofei He, Xuanhui Wang, Hujun Bao, and Jiawei Han. 2009.

</span>
<span class="ltx_bibblock">Locality Preserving Nonnegative Matrix Factorization. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the 21st International Joint Conference on Artificial Intelligence</em> (Pasadena, California, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.2">(IJCAI’09)</em>. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1010–1015.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021.

</span>
<span class="ltx_bibblock">Autoregressive Entity Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.CL/2010.00904

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay Adams, and Emre Sargin. 2016.

</span>
<span class="ltx_bibblock">Deep Neural Networks for YouTube Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 10th ACM Conference on Recommender Systems</em> (Boston, Massachusetts, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(RecSys ’16)</em>. Association for Computing Machinery, New York, NY, USA, 191–198.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2959100.2959190" title="">https://doi.org/10.1145/2959100.2959190</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dempster et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
A. P. Dempster, N. M. Laird, and D. B. Rubin. 2018.

</span>
<span class="ltx_bibblock">Maximum Likelihood from Incomplete Data Via the EM Algorithm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Journal of the Royal Statistical Society: Series B (Methodological)</em> 39, 1 (12 2018), 1–22.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111/j.2517-6161.1977.tb01600.x" title="">https://doi.org/10.1111/j.2517-6161.1977.tb01600.x</a>
arXiv:https://academic.oup.com/jrsssb/article-pdf/39/1/1/49117094/jrsssb_39_1_1.pdf

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doshi et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ishita Doshi, Dhritiman Das, Ashish Bhutani, Rajeev Kumar, Rushi Bhatt, and Niranjan Balasubramanian. 2020.

</span>
<span class="ltx_bibblock">LANNS: A Web-Scale Approximate Nearest Neighbor Lookup System.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/2010.09426

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2010.09426" title="">https://arxiv.org/abs/2010.09426</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.

</span>
<span class="ltx_bibblock">The Faiss library.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/2401.08281

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.08281" title="">https://arxiv.org/abs/2401.08281</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engineering (2021)</span>
<span class="ltx_bibblock">
Pinterest Engineering. 2021.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Manas HNSW Realtime: Powering Realtime Embedding-Based Retrieval</em>.

</span>
<span class="ltx_bibblock">Pinterest.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/pinterest-engineering/manas-hnsw-realtime-powering-realtime-embedding-based-retrieval-dc71dfd6afdd" title="">https://medium.com/pinterest-engineering/manas-hnsw-realtime-powering-realtime-embedding-based-retrieval-dc71dfd6afdd</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gallagher et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, and J. Shane Culpepper. 2019.

</span>
<span class="ltx_bibblock">Joint Optimization of Cascade Ranking Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em> (Melbourne VIC, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(WSDM ’19)</em>. Association for Computing Machinery, New York, NY, USA, 15–23.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3289600.3290986" title="">https://doi.org/10.1145/3289600.3290986</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzhi Xiao, Ruofan Ding, Xingyan Bin, Hui Yang, and Xiaobing Liu. 2021.

</span>
<span class="ltx_bibblock">Deep Retrieval: Learning A Retrievable Structure for Large-Scale Recommendations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/2007.07203

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2014.

</span>
<span class="ltx_bibblock">Optimized Product Quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 36, 4 (2014), 744–755.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2013.240" title="">https://doi.org/10.1109/TPAMI.2013.240</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020.

</span>
<span class="ltx_bibblock">Accelerating Large-Scale Inference with Anisotropic Vector Quantization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/1908.10396

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1908.10396" title="">https://arxiv.org/abs/1908.10396</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich, Stuart Bowers, and Joaquin Quiñonero Candela. 2014.

</span>
<span class="ltx_bibblock">Practical Lessons from Predicting Clicks on Ads at Facebook. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</em> (New York, NY, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(ADKDD’14)</em>. Association for Computing Machinery, New York, NY, USA, 1–9.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2648584.2648589" title="">https://doi.org/10.1145/2648584.2648589</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hervé Jegou (2017)</span>
<span class="ltx_bibblock">
Jeff Johnson Hervé Jegou, Matthijs Douze. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Faiss: A library for efficient similarity search</em>.

</span>
<span class="ltx_bibblock">Facebook.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/" title="">https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houle and Nett (2015)</span>
<span class="ltx_bibblock">
Michael E. Houle and Michael Nett. 2015.

</span>
<span class="ltx_bibblock">Rank-Based Similarity Search: Reducing the Dimensional Dependence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 37, 1 (2015), 136–150.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2014.2343223" title="">https://doi.org/10.1109/TPAMI.2014.2343223</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020.

</span>
<span class="ltx_bibblock">Embedding-based Retrieval in Facebook Search.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3394486.3403305" title="">https://doi.org/10.1145/3394486.3403305</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole. 2017.

</span>
<span class="ltx_bibblock">Categorical Reparameterization with Gumbel-Softmax.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:stat.ML/1611.01144

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with GPUs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.CV/1702.08734

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jégou et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Herve Jégou, Matthijs Douze, and Cordelia Schmid. 2011.

</span>
<span class="ltx_bibblock">Product Quantization for Nearest Neighbor Search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 33, 1 (2011), 117–128.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2010.57" title="">https://doi.org/10.1109/TPAMI.2010.57</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang and McAuley (2019)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang and Julian McAuley. 2019.

</span>
<span class="ltx_bibblock">Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/1909.05475

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui, Xiaoyi Liu, and Fedor Borisyuk. 2021.

</span>
<span class="ltx_bibblock">Que2Search: Fast and Accurate Query and Document Understanding for Search at Facebook.

</span>
<span class="ltx_bibblock">, 9 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3447548.3467127" title="">https://doi.org/10.1145/3447548.3467127</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacQueen et al<span class="ltx_text" id="bib.bib24.3.3.1">.</span> (1967)</span>
<span class="ltx_bibblock">
James MacQueen et al<span class="ltx_text" id="bib.bib24.4.1">.</span> 1967.

</span>
<span class="ltx_bibblock">Some methods for classification and analysis of multivariate observations.

</span>
<span class="ltx_bibblock">, 281–297 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2018)</span>
<span class="ltx_bibblock">
Yu. A. Malkov and D. A. Yashunin. 2018.

</span>
<span class="ltx_bibblock">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.DS/1603.09320

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manduchi et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Laura Manduchi, Moritz Vandenhirtz, Alain Ryser, and Julia Vogt. 2023.

</span>
<span class="ltx_bibblock">Tree Variational Autoencoders.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/2306.08984

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Medini et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tharun Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava. 2019.

</span>
<span class="ltx_bibblock">Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/1910.13830

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muja and Lowe (2014)</span>
<span class="ltx_bibblock">
Marius Muja and David G. Lowe. 2014.

</span>
<span class="ltx_bibblock">Scalable Nearest Neighbor Algorithms for High Dimensional Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 36, 11 (2014), 2227–2240.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TPAMI.2014.2321376" title="">https://doi.org/10.1109/TPAMI.2014.2321376</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paria et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and Barnabás Póczos. 2020.

</span>
<span class="ltx_bibblock">Minimizing FLOPs to Learn Efficient Sparse Representations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/2004.05665

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rangadurai et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaushik Rangadurai, Yiqun Liu, Siddarth Malreddy, Xiaoyi Liu, Piyush Maheshwari, Vishwanath Sangale, and Fedor Borisyuk. 2022.

</span>
<span class="ltx_bibblock">NxtPost: User to Post Recommendations in Facebook Groups.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/2202.03645

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2202.03645" title="">https://arxiv.org/abs/2202.03645</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrivastava and Li (2014)</span>
<span class="ltx_bibblock">
Anshumali Shrivastava and Ping Li. 2014.

</span>
<span class="ltx_bibblock">Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:stat.ML/1405.5869

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spring and Shrivastava (2017)</span>
<span class="ltx_bibblock">
Ryan Spring and Anshumali Shrivastava. 2017.

</span>
<span class="ltx_bibblock">A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators for Partition Function Computation in Log-Linear Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:stat.ML/1703.05160

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin, Maarten de Rijke, and Zhaochun Ren. 2023.

</span>
<span class="ltx_bibblock">Learning to Tokenize for Generative Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/2304.04171

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock">Transformer Memory as a Differentiable Search Index.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.CL/2202.06991

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2018.

</span>
<span class="ltx_bibblock">Neural Discrete Representation Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.LG/1711.00937

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2023.

</span>
<span class="ltx_bibblock">A Neural Corpus Indexer for Document Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/2206.02743

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. 2019.

</span>
<span class="ltx_bibblock">AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.CL/1811.01727

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021.

</span>
<span class="ltx_bibblock">SoundStream: An End-to-End Neural Audio Codec.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.SD/2107.03312

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian Xu, and Kun Gai. 2019.

</span>
<span class="ltx_bibblock">Joint Optimization of Tree-based Index and Deep Model for Recommender Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:cs.IR/1902.07565

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Learning Tree-based Deep Model for Recommender Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3219819.3219826" title="">https://doi.org/10.1145/3219819.3219826</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, Han Li, Jian Xu, and Kun Gai. 2020.

</span>
<span class="ltx_bibblock">Learning Optimal Tree Models Under Beam Search.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:stat.ML/2006.15408

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug 13 05:48:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
