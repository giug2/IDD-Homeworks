<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.02331] Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering</title><meta property="og:description" content="Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and ob…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.02331">

<!--Generated on Tue Mar 12 08:17:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Mind Your Outliers! Investigating the Negative Impact of Outliers on 
<br class="ltx_break">Active Learning for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Siddharth Karamcheti     Ranjay Krishna     Li Fei-Fei     Christopher D. Manning 
<br class="ltx_break">Department of Computer Science, Stanford University 
<br class="ltx_break"> <span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{skaramcheti, ranjaykrishna, feifeili, manning}@cs.stanford.edu </span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text">Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as <span id="id2.id1.1.1" class="ltx_text ltx_font_italic">collective outliers</span> – groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2107.02331/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="438" height="678" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We systematically evaluate active learning on VQA datasets and isolate their inability to perform better than random sampling due to the presence of <span id="S1.F1.2.1" class="ltx_text ltx_font_italic">collective outliers</span>. Active learning methods prefer to acquire these outliers, which are hard and often impossible for models to learn. We show that Dataset Maps, like the one shown here, can heuristically identify these collective outliers as examples assigned low model confidence and prediction variability during training.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Today, language-equipped vision systems such as VizWiz, TapTapSee, BeMyEyes, and CamFind are actively being deployed across a broad spectrum of users.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Applications can be found at <a target="_blank" href="https://vizwiz.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vizwiz.org/</a>, <a target="_blank" href="https://taptapsee.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://taptapsee.com/</a>, <a target="_blank" href="https://www.bemyeyes.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.bemyeyes.com/</a>, and <a target="_blank" href="https://camfindapp.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://camfindapp.com/</a>
</span></span></span>
As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs <cite class="ltx_cite ltx_citemacro_citep">(Bigham et al., <a href="#bib.bib5" title="" class="ltx_ref">2010</a>; Tellex et al., <a href="#bib.bib66" title="" class="ltx_ref">2011</a>; Mei et al., <a href="#bib.bib50" title="" class="ltx_ref">2016</a>; Zhu et al., <a href="#bib.bib79" title="" class="ltx_ref">2017</a>; Anderson et al., <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>; Park et al., <a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite>. Visual Question Answering (VQA), the task of answering questions about visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>; Krishna et al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>; Gordon et al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>; Hudson and Manning, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>. Unfortunately, today’s VQA models are data hungry: Their performance scales monotonically with more training data <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib48" title="" class="ltx_ref">2016</a>; Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>, motivating the need for data acquisition mechanisms such as active learning, which maximize performance while minimizing expensive data labeling.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While active learning is often key to effective data acquisition when such labeled data is difficult to obtain <cite class="ltx_cite ltx_citemacro_citep">(Lewis and Catlett, <a href="#bib.bib42" title="" class="ltx_ref">1994</a>; Tong and Koller, <a href="#bib.bib69" title="" class="ltx_ref">2001</a>; Culotta and McCallum, <a href="#bib.bib13" title="" class="ltx_ref">2005</a>; Settles, <a href="#bib.bib60" title="" class="ltx_ref">2009</a>)</cite>, we find that 8 modern active learning methods <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Siddhant and Lipton, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>; Lowell et al., <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite> show little to no improvement in sample efficiency across 5 models on 4 VQA datasets – indeed, in some cases performing worse than randomly selecting data to label. This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as topic classification <cite class="ltx_cite ltx_citemacro_citep">(Siddhant and Lipton, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>; Lowell et al., <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>, object recognition <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>, digit classification <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, and named entity recognition <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>. Our negative results hold even when accounting for common active learning ailments: cold starts, correlated sampling, and uncalibrated uncertainty. We mitigate the cold start challenge of needing a representative initial dataset by varying the size of the seed set in our experiments. We account for sampling correlated data within a given batch by including Core-Set selection <cite class="ltx_cite ltx_citemacro_citep">(Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite> in the set of active learning methods we evaluate. Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data <cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al., <a href="#bib.bib28" title="" class="ltx_ref">2011</a>; Gal and Ghahramani, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>; Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">After concluding that negative results are consistent across all experimental conditions, we investigate active learning’s ineffectiveness on VQA as a data problem and identify the existence of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">collective outliers</span> <cite class="ltx_cite ltx_citemacro_citep">(Han and Kamber, <a href="#bib.bib25" title="" class="ltx_ref">2000</a>)</cite> as the source of the problem. Leveraging recent advances in model interpretability, we build <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">Dataset Maps</span> <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, which distinguish between collective outliers and useful data that improve validation set performance (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). While global outliers deviate from the rest of the data and are often a consequence of labeling error, collective outliers cluster together; they may not individually be identifiable as outliers but collectively deviate from other examples in the dataset. For instance, VQA-2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite> is riddled with collections of hard questions that require external knowledge to answer (e.g., “What is the symbol on the hood often associated with?”) or that ask the model to read text in the images (e.g., “What is the word on the wall?”). Similarly, GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> asks underspecified questions (e.g., “what is the person wearing?” which can have multiple correct answers). Collective outliers are not specific to VQA, but can similarly be found in many open-ended tasks, including visual navigation <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite> (e.g., “Go to the grandfather clock” requires identifying rare grandfather clocks), and open-domain question answering <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, amongst others.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Using Dataset Maps, we profile active learning methods and show that they prefer acquiring collective outliers that models are unable to learn, explaining their poor improvements in sample efficiency relative to random sampling. Building on this, we use these maps to perform ablations where we identify and remove outliers iteratively from the active learning pool, observing correlated improvements in sample efficiency. This allows us to conclude that collective outliers are, indeed, responsible for the ineffectiveness of active learning for VQA. We end with prescriptive suggestions for future work in building active learning methods robust to these types of outliers.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work tests the utility of multiple recent active learning methods on the open-ended understanding task of VQA. We draw on the dataset analysis literature to identify collective outliers as the bottleneck hindering active learning methods in this setting.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Active Learning.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Active learning strategies have been successfully applied to image recognition <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a href="#bib.bib33" title="" class="ltx_ref">2009</a>; Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite>, information extraction <cite class="ltx_cite ltx_citemacro_citep">(Scheffer et al., <a href="#bib.bib57" title="" class="ltx_ref">2001</a>; Finn and Kushmerick, <a href="#bib.bib17" title="" class="ltx_ref">2003</a>; Jones et al., <a href="#bib.bib32" title="" class="ltx_ref">2003</a>; Culotta and McCallum, <a href="#bib.bib13" title="" class="ltx_ref">2005</a>)</cite>, named entity recognition <cite class="ltx_cite ltx_citemacro_citep">(Hachey et al., <a href="#bib.bib24" title="" class="ltx_ref">2005</a>; Shen et al., <a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite>, semantic parsing <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>, and text categorization <cite class="ltx_cite ltx_citemacro_citep">(Lewis and Gale, <a href="#bib.bib43" title="" class="ltx_ref">1994</a>; Hoi et al., <a href="#bib.bib27" title="" class="ltx_ref">2006</a>)</cite>. However, these same methods struggle to outperform a random baseline when applied to the task of VQA <cite class="ltx_cite ltx_citemacro_citep">(Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>; Jedoui et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>.
To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty <cite class="ltx_cite ltx_citemacro_citep">(Abramson and Freund, <a href="#bib.bib1" title="" class="ltx_ref">2004</a>; Collins et al., <a href="#bib.bib12" title="" class="ltx_ref">2008</a>; Joshi et al., <a href="#bib.bib33" title="" class="ltx_ref">2009</a>)</cite>, Bayesian uncertainty <cite class="ltx_cite ltx_citemacro_citep">(Gal and Ghahramani, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>; Kendall and Gal, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>, disagreement <cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al., <a href="#bib.bib28" title="" class="ltx_ref">2011</a>; Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, and Core-Set selection <cite class="ltx_cite ltx_citemacro_citep">(Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Question Answering.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>; Malinowski et al., <a href="#bib.bib49" title="" class="ltx_ref">2015</a>; Ren et al., <a href="#bib.bib55" title="" class="ltx_ref">2015a</a>; Johnson et al., <a href="#bib.bib31" title="" class="ltx_ref">2017</a>; Goyal et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>; Krishna et al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>; Suhr et al., <a href="#bib.bib63" title="" class="ltx_ref">2019</a>; Hudson and Manning, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> and models <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib77" title="" class="ltx_ref">2015</a>; Fukui et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>; Lu et al., <a href="#bib.bib48" title="" class="ltx_ref">2016</a>; Yang et al., <a href="#bib.bib76" title="" class="ltx_ref">2016</a>; Zhu et al., <a href="#bib.bib78" title="" class="ltx_ref">2016</a>; Wu et al., <a href="#bib.bib74" title="" class="ltx_ref">2016</a>; Anderson et al., <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>; Tan and Bansal, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>; Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. To ensure that our negative results are not dataset or model-specific, we sample 4 datasets and 5 representative models, each utilizing unique visual and linguistic features and employing different inductive biases.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Interpreting and Analyzing Datasets.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties
to remove redundancies <cite class="ltx_cite ltx_citemacro_citep">(Gururangan et al., <a href="#bib.bib23" title="" class="ltx_ref">2018</a>; Li and Vasconcelos, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> or biases <cite class="ltx_cite ltx_citemacro_citep">(Torralba and Efros, <a href="#bib.bib70" title="" class="ltx_ref">2011</a>; Khosla et al., <a href="#bib.bib37" title="" class="ltx_ref">2012</a>; Bolukbasi et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite>, both of which negatively impact sample efficiency.
Prior work has used training dynamics to find examples which are frequently forgotten <cite class="ltx_cite ltx_citemacro_citep">(Krymolowski, <a href="#bib.bib39" title="" class="ltx_ref">2002</a>; Toneva et al., <a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite> versus those that are easy to learn <cite class="ltx_cite ltx_citemacro_citep">(Bras et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>. This work suggests using two model-specific measures – <span id="S2.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">confidence</span> and <span id="S2.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">prediction variance</span> – as indicators of a training example’s “learnability” <cite class="ltx_cite ltx_citemacro_citep">(Chang et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>. Dataset Maps <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, a recently introduced framework uses these two measures to profile datasets to find learnable examples. Unlike prior datasets analyzed by Dataset Maps that have a small number of global outliers as hard examples, we discover that VQA datasets contain copious amounts of collective outliers, which are difficult or even impossible for models to learn.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Active Learning Experimental Setup</h2>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Pool Size</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"># Answers</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">VQA-Sports</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">5,411 [5k]</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">20</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left">VQA-Food</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_right">4,082 [4k]</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_right">20</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left">VQA-2</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_right">411,272 [400k]</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_nopad_r ltx_align_right">3130</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_bb">GQA</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_right ltx_border_bb">943,000 [900k]</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb">1842</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>We evaluate active learning on 4 VQA datasets. We display the total available training examples, effective pool sizes we use [in brackets], and the total number of possible answers for each dataset.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.9" class="ltx_p">We adopt the standard pool-based active learning setup from prior work <cite class="ltx_cite ltx_citemacro_citep">(Lewis and Gale, <a href="#bib.bib43" title="" class="ltx_ref">1994</a>; Settles, <a href="#bib.bib60" title="" class="ltx_ref">2009</a>; Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>, consisting of a model <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathcal{M}</annotation></semantics></math>, initial seed set of labeled examples <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="(x_{i},y_{i})\in\mathcal{D}_{\text{seed}}" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.2" xref="S3.p1.2.m2.2.2.cmml"><mrow id="S3.p1.2.m2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.p1.2.m2.2.2.2.2.3" xref="S3.p1.2.m2.2.2.2.3.cmml">(</mo><msub id="S3.p1.2.m2.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.p1.2.m2.1.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.p1.2.m2.1.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p1.2.m2.2.2.2.2.4" xref="S3.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.p1.2.m2.2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.p1.2.m2.2.2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.p1.2.m2.2.2.2.2.2.3" xref="S3.p1.2.m2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p1.2.m2.2.2.2.2.5" xref="S3.p1.2.m2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.p1.2.m2.2.2.3" xref="S3.p1.2.m2.2.2.3.cmml">∈</mo><msub id="S3.p1.2.m2.2.2.4" xref="S3.p1.2.m2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.2.2.4.2" xref="S3.p1.2.m2.2.2.4.2.cmml">𝒟</mi><mtext id="S3.p1.2.m2.2.2.4.3" xref="S3.p1.2.m2.2.2.4.3a.cmml">seed</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><apply id="S3.p1.2.m2.2.2.cmml" xref="S3.p1.2.m2.2.2"><in id="S3.p1.2.m2.2.2.3.cmml" xref="S3.p1.2.m2.2.2.3"></in><interval closure="open" id="S3.p1.2.m2.2.2.2.3.cmml" xref="S3.p1.2.m2.2.2.2.2"><apply id="S3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.1.2">𝑥</ci><ci id="S3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.2.2.2.2">𝑦</ci><ci id="S3.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.p1.2.m2.2.2.2.2.2.3">𝑖</ci></apply></interval><apply id="S3.p1.2.m2.2.2.4.cmml" xref="S3.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.p1.2.m2.2.2.4.1.cmml" xref="S3.p1.2.m2.2.2.4">subscript</csymbol><ci id="S3.p1.2.m2.2.2.4.2.cmml" xref="S3.p1.2.m2.2.2.4.2">𝒟</ci><ci id="S3.p1.2.m2.2.2.4.3a.cmml" xref="S3.p1.2.m2.2.2.4.3"><mtext mathsize="70%" id="S3.p1.2.m2.2.2.4.3.cmml" xref="S3.p1.2.m2.2.2.4.3">seed</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">(x_{i},y_{i})\in\mathcal{D}_{\text{seed}}</annotation></semantics></math> used to initialize <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\mathcal{M}</annotation></semantics></math>, an unlabeled pool of data <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{\text{pool}}" display="inline"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">𝒟</mi><mtext id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3a.cmml">pool</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝒟</ci><ci id="S3.p1.4.m4.1.1.3a.cmml" xref="S3.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">pool</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\mathcal{D}_{\text{pool}}</annotation></semantics></math>, and an acquisition function <math id="S3.p1.5.m5.2" class="ltx_Math" alttext="\mathcal{A}(x,\mathcal{M})" display="inline"><semantics id="S3.p1.5.m5.2a"><mrow id="S3.p1.5.m5.2.3" xref="S3.p1.5.m5.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.2.3.2" xref="S3.p1.5.m5.2.3.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S3.p1.5.m5.2.3.1" xref="S3.p1.5.m5.2.3.1.cmml">​</mo><mrow id="S3.p1.5.m5.2.3.3.2" xref="S3.p1.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S3.p1.5.m5.2.3.3.2.1" xref="S3.p1.5.m5.2.3.3.1.cmml">(</mo><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">x</mi><mo id="S3.p1.5.m5.2.3.3.2.2" xref="S3.p1.5.m5.2.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.5.m5.2.2" xref="S3.p1.5.m5.2.2.cmml">ℳ</mi><mo stretchy="false" id="S3.p1.5.m5.2.3.3.2.3" xref="S3.p1.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.2b"><apply id="S3.p1.5.m5.2.3.cmml" xref="S3.p1.5.m5.2.3"><times id="S3.p1.5.m5.2.3.1.cmml" xref="S3.p1.5.m5.2.3.1"></times><ci id="S3.p1.5.m5.2.3.2.cmml" xref="S3.p1.5.m5.2.3.2">𝒜</ci><interval closure="open" id="S3.p1.5.m5.2.3.3.1.cmml" xref="S3.p1.5.m5.2.3.3.2"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">𝑥</ci><ci id="S3.p1.5.m5.2.2.cmml" xref="S3.p1.5.m5.2.2">ℳ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.2c">\mathcal{A}(x,\mathcal{M})</annotation></semantics></math>. We run active learning over a series of acquisition iterations <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">T</annotation></semantics></math> where at each iteration we acquire a batch of <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.p1.7.m7.1a"><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">B</annotation></semantics></math> new examples per: <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="\hat{x}\in\mathcal{D}_{\text{pool}}" display="inline"><semantics id="S3.p1.8.m8.1a"><mrow id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mover accent="true" id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml"><mi id="S3.p1.8.m8.1.1.2.2" xref="S3.p1.8.m8.1.1.2.2.cmml">x</mi><mo id="S3.p1.8.m8.1.1.2.1" xref="S3.p1.8.m8.1.1.2.1.cmml">^</mo></mover><mo id="S3.p1.8.m8.1.1.1" xref="S3.p1.8.m8.1.1.1.cmml">∈</mo><msub id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.8.m8.1.1.3.2" xref="S3.p1.8.m8.1.1.3.2.cmml">𝒟</mi><mtext id="S3.p1.8.m8.1.1.3.3" xref="S3.p1.8.m8.1.1.3.3a.cmml">pool</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><in id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1.1"></in><apply id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2"><ci id="S3.p1.8.m8.1.1.2.1.cmml" xref="S3.p1.8.m8.1.1.2.1">^</ci><ci id="S3.p1.8.m8.1.1.2.2.cmml" xref="S3.p1.8.m8.1.1.2.2">𝑥</ci></apply><apply id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.3.1.cmml" xref="S3.p1.8.m8.1.1.3">subscript</csymbol><ci id="S3.p1.8.m8.1.1.3.2.cmml" xref="S3.p1.8.m8.1.1.3.2">𝒟</ci><ci id="S3.p1.8.m8.1.1.3.3a.cmml" xref="S3.p1.8.m8.1.1.3.3"><mtext mathsize="70%" id="S3.p1.8.m8.1.1.3.3.cmml" xref="S3.p1.8.m8.1.1.3.3">pool</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\hat{x}\in\mathcal{D}_{\text{pool}}</annotation></semantics></math> to label per <math id="S3.p1.9.m9.2" class="ltx_Math" alttext="\hat{x}=\operatorname*{arg\,max}_{x\in\mathcal{D}_{\text{pool}}}\mathcal{A}(x,\mathcal{M})" display="inline"><semantics id="S3.p1.9.m9.2a"><mrow id="S3.p1.9.m9.2.3" xref="S3.p1.9.m9.2.3.cmml"><mover accent="true" id="S3.p1.9.m9.2.3.2" xref="S3.p1.9.m9.2.3.2.cmml"><mi id="S3.p1.9.m9.2.3.2.2" xref="S3.p1.9.m9.2.3.2.2.cmml">x</mi><mo id="S3.p1.9.m9.2.3.2.1" xref="S3.p1.9.m9.2.3.2.1.cmml">^</mo></mover><mo id="S3.p1.9.m9.2.3.1" xref="S3.p1.9.m9.2.3.1.cmml">=</mo><mrow id="S3.p1.9.m9.2.3.3" xref="S3.p1.9.m9.2.3.3.cmml"><mrow id="S3.p1.9.m9.2.3.3.2" xref="S3.p1.9.m9.2.3.3.2.cmml"><msub id="S3.p1.9.m9.2.3.3.2.1" xref="S3.p1.9.m9.2.3.3.2.1.cmml"><mrow id="S3.p1.9.m9.2.3.3.2.1.2" xref="S3.p1.9.m9.2.3.3.2.1.2.cmml"><mi id="S3.p1.9.m9.2.3.3.2.1.2.2" xref="S3.p1.9.m9.2.3.3.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.p1.9.m9.2.3.3.2.1.2.1" xref="S3.p1.9.m9.2.3.3.2.1.2.1.cmml">​</mo><mi id="S3.p1.9.m9.2.3.3.2.1.2.3" xref="S3.p1.9.m9.2.3.3.2.1.2.3.cmml">max</mi></mrow><mrow id="S3.p1.9.m9.2.3.3.2.1.3" xref="S3.p1.9.m9.2.3.3.2.1.3.cmml"><mi id="S3.p1.9.m9.2.3.3.2.1.3.2" xref="S3.p1.9.m9.2.3.3.2.1.3.2.cmml">x</mi><mo id="S3.p1.9.m9.2.3.3.2.1.3.1" xref="S3.p1.9.m9.2.3.3.2.1.3.1.cmml">∈</mo><msub id="S3.p1.9.m9.2.3.3.2.1.3.3" xref="S3.p1.9.m9.2.3.3.2.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.9.m9.2.3.3.2.1.3.3.2" xref="S3.p1.9.m9.2.3.3.2.1.3.3.2.cmml">𝒟</mi><mtext id="S3.p1.9.m9.2.3.3.2.1.3.3.3" xref="S3.p1.9.m9.2.3.3.2.1.3.3.3a.cmml">pool</mtext></msub></mrow></msub><mo id="S3.p1.9.m9.2.3.3.2a" xref="S3.p1.9.m9.2.3.3.2.cmml">⁡</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.9.m9.2.3.3.2.2" xref="S3.p1.9.m9.2.3.3.2.2.cmml">𝒜</mi></mrow><mo lspace="0em" rspace="0em" id="S3.p1.9.m9.2.3.3.1" xref="S3.p1.9.m9.2.3.3.1.cmml">​</mo><mrow id="S3.p1.9.m9.2.3.3.3.2" xref="S3.p1.9.m9.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.p1.9.m9.2.3.3.3.2.1" xref="S3.p1.9.m9.2.3.3.3.1.cmml">(</mo><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">x</mi><mo id="S3.p1.9.m9.2.3.3.3.2.2" xref="S3.p1.9.m9.2.3.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.p1.9.m9.2.2" xref="S3.p1.9.m9.2.2.cmml">ℳ</mi><mo stretchy="false" id="S3.p1.9.m9.2.3.3.3.2.3" xref="S3.p1.9.m9.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.2b"><apply id="S3.p1.9.m9.2.3.cmml" xref="S3.p1.9.m9.2.3"><eq id="S3.p1.9.m9.2.3.1.cmml" xref="S3.p1.9.m9.2.3.1"></eq><apply id="S3.p1.9.m9.2.3.2.cmml" xref="S3.p1.9.m9.2.3.2"><ci id="S3.p1.9.m9.2.3.2.1.cmml" xref="S3.p1.9.m9.2.3.2.1">^</ci><ci id="S3.p1.9.m9.2.3.2.2.cmml" xref="S3.p1.9.m9.2.3.2.2">𝑥</ci></apply><apply id="S3.p1.9.m9.2.3.3.cmml" xref="S3.p1.9.m9.2.3.3"><times id="S3.p1.9.m9.2.3.3.1.cmml" xref="S3.p1.9.m9.2.3.3.1"></times><apply id="S3.p1.9.m9.2.3.3.2.cmml" xref="S3.p1.9.m9.2.3.3.2"><apply id="S3.p1.9.m9.2.3.3.2.1.cmml" xref="S3.p1.9.m9.2.3.3.2.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.2.3.3.2.1.1.cmml" xref="S3.p1.9.m9.2.3.3.2.1">subscript</csymbol><apply id="S3.p1.9.m9.2.3.3.2.1.2.cmml" xref="S3.p1.9.m9.2.3.3.2.1.2"><times id="S3.p1.9.m9.2.3.3.2.1.2.1.cmml" xref="S3.p1.9.m9.2.3.3.2.1.2.1"></times><ci id="S3.p1.9.m9.2.3.3.2.1.2.2.cmml" xref="S3.p1.9.m9.2.3.3.2.1.2.2">arg</ci><ci id="S3.p1.9.m9.2.3.3.2.1.2.3.cmml" xref="S3.p1.9.m9.2.3.3.2.1.2.3">max</ci></apply><apply id="S3.p1.9.m9.2.3.3.2.1.3.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3"><in id="S3.p1.9.m9.2.3.3.2.1.3.1.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.1"></in><ci id="S3.p1.9.m9.2.3.3.2.1.3.2.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.2">𝑥</ci><apply id="S3.p1.9.m9.2.3.3.2.1.3.3.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.p1.9.m9.2.3.3.2.1.3.3.1.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.3">subscript</csymbol><ci id="S3.p1.9.m9.2.3.3.2.1.3.3.2.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.3.2">𝒟</ci><ci id="S3.p1.9.m9.2.3.3.2.1.3.3.3a.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.3.3"><mtext mathsize="50%" id="S3.p1.9.m9.2.3.3.2.1.3.3.3.cmml" xref="S3.p1.9.m9.2.3.3.2.1.3.3.3">pool</mtext></ci></apply></apply></apply><ci id="S3.p1.9.m9.2.3.3.2.2.cmml" xref="S3.p1.9.m9.2.3.3.2.2">𝒜</ci></apply><interval closure="open" id="S3.p1.9.m9.2.3.3.3.1.cmml" xref="S3.p1.9.m9.2.3.3.3.2"><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">𝑥</ci><ci id="S3.p1.9.m9.2.2.cmml" xref="S3.p1.9.m9.2.2">ℳ</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.2c">\hat{x}=\operatorname*{arg\,max}_{x\in\mathcal{D}_{\text{pool}}}\mathcal{A}(x,\mathcal{M})</annotation></semantics></math>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">Acquiring an example often refers to using an oracle or human expert to annotate a new example with a correct label. We follow prior work to simulate an oracle using existing datasets, forming <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{seed}}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">𝒟</mi><mtext id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3a.cmml">seed</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">𝒟</ci><ci id="S3.p2.1.m1.1.1.3a.cmml" xref="S3.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">seed</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathcal{D}_{\text{seed}}</annotation></semantics></math> from a fixed percentage of the full dataset, and using the remainder as <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{\text{pool}}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">𝒟</mi><mtext id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3a.cmml">pool</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝒟</ci><ci id="S3.p2.2.m2.1.1.3a.cmml" xref="S3.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">pool</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathcal{D}_{\text{pool}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>; Siddhant and Lipton, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite>. We re-train <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\mathcal{M}</annotation></semantics></math> after each acquisition iteration.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Prior work has noted the impact of seed set size on active learning performance <cite class="ltx_cite ltx_citemacro_citep">(Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>; Misra et al., <a href="#bib.bib51" title="" class="ltx_ref">2018</a>; Jedoui et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>. We run multiple active learning evaluations with varying seed set sizes (ranging from 5% to 50% of the full pool size). We keep the size of each acquisition batch <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">B</annotation></semantics></math> to a constant 10% of the overall pool size.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Visual Question Answering (VQA) requires reasoning over two modalities: images and text. Most models use feature “backbones” (e.g., features from object recognition models pretrained on ImageNet, and pretrained word vectors for text). For image features we use grid-based features from ResNet-101 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>, or object-based features from Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib56" title="" class="ltx_ref">2015b</a>)</cite> fine-tuned on Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>)</cite>. We evaluate with a representative sample of existing VQA models, including the following:<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
Key implementation details can be found in the appendix. In the interest of full reproducibility and further work in active learning and VQA, we release our code and results here: <a target="_blank" href="https://github.com/siddk/vqa-outliers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/siddk/vqa-outliers</a>.
</span></span></span></p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LogReg</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">is a logistic regression model that uses either ResNet-101 or Faster R-CNN image features with mean-pooled GloVe question embeddings <cite class="ltx_cite ltx_citemacro_citep">(Pennington et al., <a href="#bib.bib54" title="" class="ltx_ref">2014</a>)</cite>. Although these models are not as performant as the subsequent models, logistic regression has been effective on VQA <cite class="ltx_cite ltx_citemacro_citep">(Suhr et al., <a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite>, and is pervasive in the active learning literature <cite class="ltx_cite ltx_citemacro_citep">(Schein and Ungar, <a href="#bib.bib58" title="" class="ltx_ref">2007</a>; Yang and Loog, <a href="#bib.bib75" title="" class="ltx_ref">2018</a>; Mussmann and Liang, <a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LSTM-CNN</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">is a standard model introduced with VQA-1 <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. We use more performant ResNet-101 features instead of the original VGGNet features as our visual backbone.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BUTD</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">(Bottom-Up Top-Down Attention) uses object-based features in tandem with attention over objects <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>)</cite>. <span id="S3.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">BUTD</span> won the 2017 VQA Challenge <cite class="ltx_cite ltx_citemacro_citep">(Teney et al., <a href="#bib.bib67" title="" class="ltx_ref">2018</a>)</cite>, and has been a consistent baseline for recent work in VQA.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LXMERT</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">is a large multi-modal transformer model that uses <span id="S3.SS1.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">BUTD</span>’s object features and contextualized BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> language features <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>. LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib45" title="" class="ltx_ref">2014</a>; Krishna et al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>; Goyal et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>; Suhr et al., <a href="#bib.bib63" title="" class="ltx_ref">2019</a>; Hudson and Manning, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>, initializing a cross-modal representation space conducive to fine-tuning.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>
Results for LXMERT in <cite class="ltx_cite ltx_citemacro_citet">Tan and Bansal (<a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite> are reported <span id="footnote3.1" class="ltx_text ltx_font_italic">after</span> pretraining on training and validation examples from the VQA datasets we use. While this is fair if the goal is optimizing for test performance, this exposure to training and validation examples leaks important information; to remedy this, we obtained a model checkpoint from the LXMERT authors trained <span id="footnote3.2" class="ltx_text ltx_font_italic">without</span> VQA data. This is also why our LXMERT results are lower than the numbers reported in the original paper – however, the general boost provided by cross-modal pretraining holds.
</span></span></span></p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x3.png" id="S3.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x4.png" id="S3.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Results for varied active learning methods on VQA-Sports, a simplified VQA dataset. Strategies perform on par with or worse than the random baseline, when using <math id="S3.F2.5.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S3.F2.5.m1.1b"><mrow id="S3.F2.5.m1.1.1" xref="S3.F2.5.m1.1.1.cmml"><mn id="S3.F2.5.m1.1.1.2" xref="S3.F2.5.m1.1.1.2.cmml">10</mn><mo id="S3.F2.5.m1.1.1.1" xref="S3.F2.5.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.5.m1.1c"><apply id="S3.F2.5.m1.1.1.cmml" xref="S3.F2.5.m1.1.1"><csymbol cd="latexml" id="S3.F2.5.m1.1.1.1.cmml" xref="S3.F2.5.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.F2.5.m1.1.1.2.cmml" xref="S3.F2.5.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m1.1d">10\%</annotation></semantics></math> of the full dataset as the seed set.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x5.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x6.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x7.png" id="S3.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Results for the full VQA-2 dataset, also using 10% of the full dataset as a seed set. Similar to the plot above, all active learning methods perform similar to a random baseline.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x8.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x9.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x10.png" id="S3.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Results on VQA-2 using 50% of the dataset as a seed set. While methods are <span id="S3.F4.5.1" class="ltx_text ltx_font_italic">relatively</span> better when using a larger seed set—confirming results from <cite class="ltx_cite ltx_citemacro_citep">(Lin and Parikh, <a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>—no methods outperform random.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x11.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x12.png" id="S3.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F5.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x13.png" id="S3.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results on <span id="S3.F5.5.1" class="ltx_text ltx_font_smallcaps">GQA</span> using 10% of the dataset for the seed set. Even with different question structures, the above trends hold, with strategies performing worse than or equivalent to random.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Acquisition Functions</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Several active learning methods have been developed to account for different aspects of the machine learning training pipeline: while some acquire examples with high aleotoric uncertainty <cite class="ltx_cite ltx_citemacro_citep">(Settles, <a href="#bib.bib60" title="" class="ltx_ref">2009</a>)</cite> (having to do with the natural uncertainty in the data) or epistemic uncertainty <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> (having to do with the uncertainty in the modeling/learning process), others attempt to acquire examples that reflect the distribution of data in the pool <cite class="ltx_cite ltx_citemacro_citep">(Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite>. We sample a diverse set of these methods:</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Random Sampling</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">serves as our baseline passive approach for acquiring examples.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Least Confidence</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">acquires examples with lowest model prediction probability <cite class="ltx_cite ltx_citemacro_citep">(Settles, <a href="#bib.bib60" title="" class="ltx_ref">2009</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Entropy</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">acquires examples with the highest entropy in the model’s output <cite class="ltx_cite ltx_citemacro_citep">(Settles, <a href="#bib.bib60" title="" class="ltx_ref">2009</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MC-Dropout Entropy</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">(Monte-Carlo Dropout with Entropy acquisition) acquires examples with high entropy in the model’s output averaged over multiple passes through a neural network with different dropout masks <cite class="ltx_cite ltx_citemacro_citep">(Gal and Ghahramani, <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>. This process is a consequence of a theoretical casting of dropout as approximate Bayesian inference in deep Gaussian processes.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BALD</h4>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">(Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy <cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al., <a href="#bib.bib28" title="" class="ltx_ref">2011</a>; Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Siddhant and Lipton, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite> – capturing “disagreement” across different dropout masks.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Core-Set Selection</h4>

<div id="S3.SS2.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px6.p1.1" class="ltx_p">samples examples that capture the diversity of the data pool <cite class="ltx_cite ltx_citemacro_citep">(Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>; Coleman et al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. It acquires examples to minimize the distance between an example in the unlabeled pool to its closest labeled example. Since Core-Set selection operates over a representation space (and not an output distribution, like prior strategies) and VQA models operate over two modalities, we employ three Core-Set variants: <span id="S3.SS2.SSS0.Px6.p1.1.1" class="ltx_text ltx_font_bold">Core-Set (Language)</span> and <span id="S3.SS2.SSS0.Px6.p1.1.2" class="ltx_text ltx_font_bold">Core-Set (Vision)</span> operate over their respective representation spaces while <span id="S3.SS2.SSS0.Px6.p1.1.3" class="ltx_text ltx_font_bold">Core-Set (Fused)</span> operates over the “fused” vision and language representation space.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x14.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="134" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x15.png" id="S3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="134" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x16.png" id="S3.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="134" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>We visualize the difference in acquisition preferences between random and active learning acquisitions (least confidence and BALD) across multiple iterations. Active learning methods prefer to sample impossible examples which models are unable to learn, hurting sample efficiency relative to the random baseline.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate the 8 active learning strategies across the 5 models described in the previous section. Figures <a href="#S3.F2" title="Figure 2 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>–<a href="#S3.F5" title="Figure 5 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show a representative sample of active learning results across datasets. Due to space constraints, we only visualize 4 active learning strategies – Least-Confidence, BALD, CoreSet-Fused, and the Random Baseline – using 3 models (LSTM-CNN, BUTD, LXMERT).<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For LXMERT, running Core-Set selection is prohibitive, so we omit these results; please see Appendix B for more details.</span></span></span> Results and trends are consistent across the different acquisition functions, models and seed set sizes (see the appendix for results with other models, acquisition functions, and seed set sizes). We now go on to provide descriptions of the datasets we evaluate against, and the corresponding results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Simplified VQA Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">One complexity of VQA is the size of the output space and the number of examples present <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib2" title="" class="ltx_ref">2015</a>; Goyal et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>; VQA-2 has 400k training examples, and in excess of 3k possible answers (see Table <a href="#S3.T1" title="Table 1 ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). However, prior work in active learning focuses on smaller datasets like the 10-class MNIST dataset <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, binary classification <cite class="ltx_cite ltx_citemacro_citep">(Siddhant and Lipton, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite>, or small-cardinality (<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><leq id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\leq</annotation></semantics></math> 20 classes) text categorization <cite class="ltx_cite ltx_citemacro_citep">(Lowell et al., <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>. To ensure our results and conclusions are not due to the size of the output space, we build two meaningful, but narrow-domain VQA datasets from subsets of VQA-2. These simplified datasets reduce the complexity of the underlying learning problem and provide a fair comparison to existing active learning literature.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA-Sports.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We generate VQA-Sports by compiling a list of 20 popular sports (e.g., soccer, football, tennis, etc.) in VQA-2, and restricting the set of questions to those with answers in this list. We picked the sports categories by ranking the GloVe vector similarity between the word “sports” to answers in VQA-2, and selected the 20 most commonly occurring answers.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA-Food.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We generate the VQA-Food dataset similarly, compiling a list of the 20 commonly occurring food categories by GloVe vector similarity to the word “food.”</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents results for VQA-Sports, with an initial seed set restricted to 10% of the total pool (500 examples). The appendix reports similar results on VQA-Food. For LSTM-CNN, <span id="S4.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">Least-Confidence</span> appears to be slightly more sample efficient, while all other strategies perform on par with or worse than random. For BUTD, all methods are on par with random; for LXMERT, they perform worse than random. Generally on VQA-Sports, active learning performance varies, but fails to outperform random acquisition.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>VQA-2</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">VQA-2 is the canonical dataset for evaluating VQA models <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>. In keeping with prior work <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>; Tan and Bansal, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>, we filter the training set to only include answers that appear at least 9 times, resulting in 3130 unique answers. Unlike traditional VQA-2 evaluation, which treats the task as a <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">multi-label</span> binary classification problem, we follow prior active learning work on VQA <cite class="ltx_cite ltx_citemacro_cite">Lin and Parikh (<a href="#bib.bib46" title="" class="ltx_ref">2017</a>)</cite>, which formulates it as a <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">multi-class</span> classification problem, enabling the use of acquisition functions such as uncertainty sampling and BALD.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">Figures <a href="#S3.F3" title="Figure 3 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show results on VQA-2 with different seed set sizes – 10% (40k examples) and 50% (200k examples). Active learning performs relatively better with larger seed sets but still underperforms random. Surprisingly, when initialized with 50% of the pool as the seed set, the gain in validation accuracy after acquiring the entire pool of examples (400k examples total) is only 2%. This is an indication that the lack of sample efficiency might be a result of the underlying data, a problem we explore in the next section.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2107.02331/assets/x17.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example groups of collective outliers in the VQA-2 and GQA datasets.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>GQA</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">GQA was introduced as a means for evaluating compositional reasoning <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>. Unlike VQA’s natural human-written questions, GQA contains synthetic questions of the form “what is inside the bottle the glasses are to the right of?”. We use the standard GQA training set of 943k questions, 900k of which we use for the active learning pool.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ LXMERT ‣ 3.1 Models ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows results on GQA using a seed set of 10% of the full pool (90k examples). Despite its notable differences in question structure to VQA-2, active learning still performs on par with or slightly worse than random.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis via Dataset Maps</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The previous section shows that active learning fails to improve over random acquisition on VQA across models and datasets. A simple question remains – <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">why</span>? One hypothesis is that sample inefficiency stems from the data itself: there is only a 2% gain in validation accuracy when training on half versus the whole dataset. Working from this, we characterize the underlying datasets using Dataset Maps <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite> and discover that active learning methods prefer sampling “hard-to-learn” examples, leading to poor performance.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Mapping VQA Datasets.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">A Dataset Map <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite> is a model-specific graph for profiling the learnability of individual training examples. Dataset Maps present holistic pictures of classification datasets relative to the training dynamics of a given model; as a model trains for multiple epochs and sees the same examples repeatedly, the mapping process logs statistics about the confidence assigned to individual predictions. Maps then visualize these statistics against two axes: the y-axis plots the average model confidence assigned to the correct answer over training epochs, while the x-axis plots the spread, or variability, of these values. This introduces a 2D representation of a dataset (viewed through its relationship with individual model) where examples are placed on the map by coarse statistics describing their “learnability“. We show the Dataset Map for BUTD trained on VQA-2 in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For our work, we build this map post-hoc, training on the entire pool as a means for analyzing what active learning is doing – treating it as a diagnostic tool for identifying the root cause why active learning seems to fail for VQA.</p>
</div>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">In an ideal setting, the majority of examples in the training set should lie in the upper half of the graph – i.e., the mean confidence assigned to the correct answer should be relatively high. Examples towards the upper-left side represent the “easy-to-learn” examples, as the variability in the confidence assigned by the model over time is fairly low.</p>
</div>
<div id="S5.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p3.1" class="ltx_p">A curious feature of VQA-2 and other VQA datasets is the presence of the 25-30% of examples in the bottom-left of the map (shown in red in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) – examples that have low confidence and variability. In other words, models are unable to learn a large proportion of training examples. While prior work attributes examples in this quadrant to “labeling errors” <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta et al., <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, labeling errors in VQA are sparse, and cannot account for the density of such examples in these maps.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x18.png" id="S5.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>10% of Dataset Removed</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x19.png" id="S5.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>25% of Dataset Removed</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x20.png" id="S5.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="483" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>50% of Dataset Removed</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Using Dataset Maps, we remove hard-to-learn examples, which we identify as collective outliers. With the outliers removed, active learning methods demonstrate up to 2–3x sample efficiency versus random sampling.</figcaption>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Interpreting Acquisitions.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.4" class="ltx_p">We profile the acquisitions made by each active learning method, contextualizing the acquired examples via their placement on the associated Dataset Map. We segregate training examples into four buckets using the map’s y-axis: easy (<math id="S5.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\geq 0.75" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">≥</mo><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1"><geq id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1"></geq><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2">absent</csymbol><cn type="float" id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">\geq 0.75</annotation></semantics></math>), medium (<math id="S5.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\geq 0.50" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml"></mi><mo id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">≥</mo><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">0.50</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1"><geq id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1"></geq><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2">absent</csymbol><cn type="float" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3">0.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">\geq 0.50</annotation></semantics></math>), hard (<math id="S5.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\geq 0.25" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S5.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml"></mi><mo id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">≥</mo><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1"><geq id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1"></geq><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2">absent</csymbol><cn type="float" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m3.1c">\geq 0.25</annotation></semantics></math>), and impossible (<math id="S5.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\geq 0.00" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S5.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml"></mi><mo id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">≥</mo><mn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">0.00</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1"><geq id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1"></geq><csymbol cd="latexml" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2">absent</csymbol><cn type="float" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3">0.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.4.m4.1c">\geq 0.00</annotation></semantics></math>). Ideally, active learning should be robust to “hard-to-learn” examples, focusing instead on learnable, high uncertainty examples towards the upper-right portion of the Dataset Map. Instead, we find that active learning methods acquire a large proportion of impossible examples early on and concentrate on the easier examples only after the impossible examples dwindle (see Figure <a href="#S3.F6" title="Figure 6 ‣ Core-Set Selection ‣ 3.2 Acquisition Functions ‣ 3 Active Learning Experimental Setup ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). In contrast, the random baseline acquires examples proportional to each bucket’s density in the underlying map; acquiring easier examples earlier and performing on par with or better than all others.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Collective Outliers</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This leaves two questions: 1) can we characterize these “hard” examples, and 2) are these examples responsible for the ineffectiveness of active learning on VQA? We first identify hard-to-learn examples as collective outliers and explain why active learning methods prefer to acquire them. Next, we perform ablation experiments, removing these outliers from the active learning pool iteratively, and demonstrate a corresponding boost in sample efficiency relative to random acquisition.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Hard Examples are Collective Outliers.</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">Collective outliers are groups of examples that deviate from the rest of the examples but cluster together <cite class="ltx_cite ltx_citemacro_citep">(Han and Kamber, <a href="#bib.bib25" title="" class="ltx_ref">2000</a>)</cite> – they often present as fundamental subproblems of a broader task. For instance (Figure <a href="#S4.F7" title="Figure 7 ‣ Results. ‣ 4.2 VQA-2 ‣ 4 Experimental Results ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), in VQA-2, we identify clusters of hard-to-learn examples that require optical character recognition (OCR) for reasoning about text (e.g., “What is the first word on the black car?”); another cluster requires external knowledge to answer (“What is the symbol on the hood often associated with?”). In GQA, we identify different clusters of collective outliers; one cluster stems from innate underspecification (e.g., “what is on the shelf?” with multiple objects present on the shelf); another cluster requires multiple reasoning hops difficult for current models (e.g., “What is the vehicle that is driving down the road the box is on the side of?”).</p>
</div>
<div id="S6.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p2.1" class="ltx_p">We sample 100 random “hard-to-learn” examples from both VQA-2 and GQA and find that 100% of the examples belong to one of the two aforementioned collectives. Since hard-to-learn examples constitute 25–30% of the data pool, active learning methods cannot avoid them. Uncertainty-based methods (e.g., Least-Confidence, Entropy, Monte-Carlo Dropout) identify them as valid acquisition targets because models lack the capacity to correctly answer these examples, assigning low confidence and high uncertainty. Disagreement-based methods (e.g., BALD) are similar; model confidence is generally low but high variance (lower middle/lower right of the Dataset Maps). Finally, diversity methods (e.g., Core-Set selection) identify these examples as different enough from the existing pool to warrant acquisition, but fail to learn meaningful representations, fueling a vicious cycle wherein they continue to pick these examples.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablating Outliers.</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">To verify that collective outliers are responsible for the degradation of active learning performance, we re-run our experiments using active learning pools with varying numbers of outliers removed. To remove these outliers, we sort and remove all examples in the data pool using the product of their model confidence and prediction variability (x and y-axis values of the Dataset Maps). We systematically remove examples with a low product value and observe how active learning performance changes (see Figure <a href="#S5.F8" title="Figure 8 ‣ Mapping VQA Datasets. ‣ 5 Analysis via Dataset Maps ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<div id="S6.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p2.1" class="ltx_p">We observe a 2–3x improvement in sample efficiency when removing 50% of the entire data pool, consisting mainly of collective outliers (Figure <a href="#S5.F8" title="Figure 8 ‣ Mapping VQA Datasets. ‣ 5 Analysis via Dataset Maps ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>c). This improvement decreases if we only remove 25% of the full pool (Figure <a href="#S5.F8" title="Figure 8 ‣ Mapping VQA Datasets. ‣ 5 Analysis via Dataset Maps ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>b), and further degrades if we remove only 10% (Figure <a href="#S5.F8" title="Figure 8 ‣ Mapping VQA Datasets. ‣ 5 Analysis via Dataset Maps ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>a). This ablation demonstrates that active learning methods are more sample efficient than the random baseline when collective outliers are absent from the unlabelled pool.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper asks a simple question – why does the modern neural active learning toolkit fail when applied to complex, open ended tasks? While we focus on VQA, collective outliers are abundant in tasks such as natural language inference <cite class="ltx_cite ltx_citemacro_citep">(Bowman et al., <a href="#bib.bib7" title="" class="ltx_ref">2015</a>; Williams et al., <a href="#bib.bib71" title="" class="ltx_ref">2018</a>)</cite> and open-domain question answering <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, amongst others. More insidious is their nature; collective outliers can take multiple forms, requiring external domain knowledge or “commonsense” reasoning, containing underspecification, or requiring capabilities beyond the scope of a given model (e.g., requiring OCR ability). While we perform ablations in this work removing collective outliers, demonstrating that active learning fails as collective outliers take up larger portions of the dataset, this is only an analytical tool; these outliers are, and will continue to be, pervasive in open-ended datasets – and as such, we will need to develop better tools for learning (and performing active learning) in their presence.</p>
</div>
<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Selective Classification.</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">One potential direction for future work is to develop systems that abstain when they encounter collective outliers. Historical artificial intelligence systems, such as SHRDLU <cite class="ltx_cite ltx_citemacro_citep">(Winograd, <a href="#bib.bib72" title="" class="ltx_ref">1972</a>)</cite> and QUALM <cite class="ltx_cite ltx_citemacro_citep">(Lehnert, <a href="#bib.bib41" title="" class="ltx_ref">1977</a>)</cite>, were designed to flag input sequences that they were not designed to parse. Ideas from those methods can and should be resurrected using modern techniques; for example, recent work suggests that a simple classifier can be trained to identify out-of-domain data inputs, provided a seed out-of-domain dataset <cite class="ltx_cite ltx_citemacro_citep">(Kamath et al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>. Active learning methods can be augmented with a similar classifier, which re-calibrates active learning uncertainty scores with this classifier’s predictions. Other work learns to identify novel utterances by learning to intelligently set thresholds in representation space <cite class="ltx_cite ltx_citemacro_citep">(Karamcheti et al., <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>, a powerful idea especially if combined with other representation-centric active learning methods like Core-Set Sampling <cite class="ltx_cite ltx_citemacro_citep">(Sener and Savarese, <a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Active Learning with Global Reasoning.</h4>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">Another direction for future work to explore is to leverage Dataset Maps to perform more global, holistic reasoning over datasets, to intelligently identify promising examples – in a sense, baking part of the analysis done in this work directly into the active learning algorithms. A possible instantiation of this idea would be in training a discriminator to differentiate between “learnable” examples (upper half of each Dataset Map) from the “unlearnable”, collective outliers with low confidence and low variability. Between each active learning acquisition iteration, one can generate an updated Dataset Map, thereby reflecting what models are learning as they obtain new labeled examples.</p>
</div>
<div id="S7.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p2.1" class="ltx_p">Machine learning systems deployed in real-world settings will inevitably encounter open-world datasets, ones that contain a mixture of learnable and unlearnable inputs. Our work provides a framework to study when models encounter such inputs. Overall, we hope that our experiments serve as a catalyst for future work on evaluating active learning methods with inputs drawn from open-world datasets.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Reproducibility</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">All code for data preprocessing, model implementation, and active learning algorithms is made available at <a target="_blank" href="https://github.com/siddk/vqa-outliers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/siddk/vqa-outliers</a>. Additionally, this repository also contains the full set of results and dataset maps as well.</p>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">The authors are fully committed to maintaining this repository, in terms of both functionality and ease of use, and will actively monitor both email and Github Issues should there be problems.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We thank Kaylee Burns, Eric Mitchell, Stephen Mussman, Dorsa Sadigh, and our anonymous ACL reviewers for their useful feedback on earlier versions of this paper. We are also grateful to Hao Tan for providing us with the LXMERT checkpoint trained without access to VQA datasets, as well as for general LXMERT fine-tuning pointers.</p>
</div>
<div id="Sx2.p2" class="ltx_para ltx_noindent">
<p id="Sx2.p2.1" class="ltx_p">Siddharth Karamcheti is graciously supported by the Open Philanthropy Project AI Fellowship. Christopher D. Manning is a CIFAR Fellow.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramson and Freund (2004)</span>
<span class="ltx_bibblock">
Yotam Abramson and Yoav Freund. 2004.

</span>
<span class="ltx_bibblock">Active learning for visual object recognition.

</span>
<span class="ltx_bibblock">Technical report, University of California, San Diego.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2015)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence
Zitnick, Devi Parikh, and Dhruv Batra. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123:4–31.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018a)</span>
<span class="ltx_bibblock">
Peter Anderson, X. He, C. Buehler, Damien Teney, Mark Johnson, Stephen Gould,
and Lei Zhang. 2018a.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
6077–6086.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018b)</span>
<span class="ltx_bibblock">
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko
Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel.
2018b.

</span>
<span class="ltx_bibblock">Vision-and-language navigation: Interpreting visually-grounded
navigation instructions in real environments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham et al. (2010)</span>
<span class="ltx_bibblock">
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller,
Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual
White, and Tom Yeh. 2010.

</span>
<span class="ltx_bibblock">VizWiz: nearly real-time answers to visual questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">User Interface Software and Technology (UIST)</em>, pages
333–342.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al. (2016)</span>
<span class="ltx_bibblock">
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T
Kalai. 2016.

</span>
<span class="ltx_bibblock">Man is to computer programmer as woman is to homemaker? Debiasing
word embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 4349–4357.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et al. (2015)</span>
<span class="ltx_bibblock">
Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.
2015.

</span>
<span class="ltx_bibblock">A large annotated corpus for learning natural language inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bras et al. (2020)</span>
<span class="ltx_bibblock">
Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew
Peters, Ashish Sabharwal, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Adversarial filters of dataset biases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>, pages
1078–1088.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2017)</span>
<span class="ltx_bibblock">
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. 2017.

</span>
<span class="ltx_bibblock">Active bias: Training more accurate neural networks by emphasizing
high variance samples.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 1002–1012.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, pages
104–120.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coleman et al. (2020)</span>
<span class="ltx_bibblock">
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. 2020.

</span>
<span class="ltx_bibblock">Selection via proxy: Efficient data selection for deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collins et al. (2008)</span>
<span class="ltx_bibblock">
Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei. 2008.

</span>
<span class="ltx_bibblock">Towards scalable dataset construction: An active learning approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, pages
86–98.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Culotta and McCallum (2005)</span>
<span class="ltx_bibblock">
Aron Culotta and Andrew McCallum. 2005.

</span>
<span class="ltx_bibblock">Reducing labeling effort for structured prediction tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Association for the Advancement of Artificial Intelligence
(AAAI)</em>, pages 746–751.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2018)</span>
<span class="ltx_bibblock">
Yue Deng, KaWai Chen, Yilin Shen, and Hongxia Jin. 2018.

</span>
<span class="ltx_bibblock">Adversarial active learning for sequences labeling and generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Artificial Intelligence
(IJCAI)</em>, pages 4012–4018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>, pages
4171–4186.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2018)</span>
<span class="ltx_bibblock">
Li Dong, Chris Quirk, and Mirella Lapata. 2018.

</span>
<span class="ltx_bibblock">Confidence modeling for neural semantic parsing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finn and Kushmerick (2003)</span>
<span class="ltx_bibblock">
Aidan Finn and Nicolas Kushmerick. 2003.

</span>
<span class="ltx_bibblock">Active learning selection strategies for information extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Workshop on Adaptive Text
Extraction and Mining (ATEM-03)</em>, pages 18–25.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al. (2016)</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach. 2016.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal and Ghahramani (2016)</span>
<span class="ltx_bibblock">
Yarin Gal and Zoubin Ghahramani. 2016.

</span>
<span class="ltx_bibblock">Dropout as a Bayesian approximation: Representing model uncertainty
in deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et al. (2017)</span>
<span class="ltx_bibblock">
Yarin Gal, R. Islam, and Zoubin Ghahramani. 2017.

</span>
<span class="ltx_bibblock">Deep Bayesian active learning with image data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et al. (2018)</span>
<span class="ltx_bibblock">
Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter
Fox, and Ali Farhadi. 2018.

</span>
<span class="ltx_bibblock">IQA: Visual question answering in interactive environments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2018)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman,
and Noah A Smith. 2018.

</span>
<span class="ltx_bibblock">Annotation artifacts in natural language inference data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>, pages
107–112.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hachey et al. (2005)</span>
<span class="ltx_bibblock">
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.

</span>
<span class="ltx_bibblock">Investigating the effects of selective sampling on the annotation
task.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Computational Natural Language Learning (CoNLL)</em>, pages
144–151.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han and Kamber (2000)</span>
<span class="ltx_bibblock">
Jiawei Han and Micheline Kamber. 2000.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Data Mining: Concepts and Techniques</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoi et al. (2006)</span>
<span class="ltx_bibblock">
Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. 2006.

</span>
<span class="ltx_bibblock">Batch mode active learning and its application to medical image
classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine
learning</em>, pages 417–424.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2011)</span>
<span class="ltx_bibblock">
Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel.
2011.

</span>
<span class="ltx_bibblock">Bayesian active learning for classification and preference
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1112.5745</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock">GQA: A new dataset for real-world visual reasoning and
compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jedoui et al. (2019)</span>
<span class="ltx_bibblock">
Khaled Jedoui, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. 2019.

</span>
<span class="ltx_bibblock">Deep Bayesian active learning for multiple correct outputs.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.01119</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et al. (2003)</span>
<span class="ltx_bibblock">
Rosie Jones, Rayid Ghani, Tom Mitchell, and Ellen Riloff. 2003.

</span>
<span class="ltx_bibblock">Active learning for information extraction with multiple view feature
sets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Knowledge Discovery and Data
Mining (KDD)</em>, pages 26–34.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2009)</span>
<span class="ltx_bibblock">
Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. 2009.

</span>
<span class="ltx_bibblock">Multi-class active learning for image classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
2372–2379.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2020)</span>
<span class="ltx_bibblock">
Amita Kamath, Robin Jia, and Percy Liang. 2020.

</span>
<span class="ltx_bibblock">Selective question answering under domain shift.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karamcheti et al. (2020)</span>
<span class="ltx_bibblock">
Siddharth Karamcheti, Dorsa Sadigh, and Percy Liang. 2020.

</span>
<span class="ltx_bibblock">Learning adaptive language interfaces through decomposition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">EMNLP Workshop for Interactive and Executable Semantic
Parsing (IntEx-SemPar)</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kendall and Gal (2017)</span>
<span class="ltx_bibblock">
Alex Kendall and Yarin Gal. 2017.

</span>
<span class="ltx_bibblock">What uncertainties do we need in Bayesian deep learning for
computer vision?

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 5574–5584.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. (2012)</span>
<span class="ltx_bibblock">
Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio
Torralba. 2012.

</span>
<span class="ltx_bibblock">Undoing the damage of dataset bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, pages
158–171.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidi, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Fei-Fei Li. 2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123:32–73.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krymolowski (2002)</span>
<span class="ltx_bibblock">
Yuval Krymolowski. 2002.

</span>
<span class="ltx_bibblock">Distinguishing easy and hard instances.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Computational Linguistics
(COLING)</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang,
Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural questions: A benchmark for question answering research.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lehnert (1977)</span>
<span class="ltx_bibblock">
Wendy Lehnert. 1977.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">The Process of Question Answering</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Yale University.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis and Catlett (1994)</span>
<span class="ltx_bibblock">
David D Lewis and Jason Catlett. 1994.

</span>
<span class="ltx_bibblock">Heterogeneous uncertainty sampling for supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>, pages
148–156.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis and Gale (1994)</span>
<span class="ltx_bibblock">
David D Lewis and William A Gale. 1994.

</span>
<span class="ltx_bibblock">A sequential algorithm for training text classifiers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">ACM Special Interest Group on Information Retreival
(SIGIR)</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Vasconcelos (2019)</span>
<span class="ltx_bibblock">
Yi Li and Nuno Vasconcelos. 2019.

</span>
<span class="ltx_bibblock">Repair: Removing representation bias by dataset resampling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
9572–9581.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, pages
740–755.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Parikh (2017)</span>
<span class="ltx_bibblock">
Xiao Lin and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">Active learning for visual question answering: An empirical study.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.01732</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lowell et al. (2019)</span>
<span class="ltx_bibblock">
David Lowell, Zachary C. Lipton, and Byron C. Wallace. 2019.

</span>
<span class="ltx_bibblock">Practical obstacles to deploying active learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. (2015)</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision (ICCV)</em>, pages
1–9.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei et al. (2016)</span>
<span class="ltx_bibblock">
Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 2016.

</span>
<span class="ltx_bibblock">Listen, attend, and walk: Neural mapping of navigational instructions
to action sequences.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Association for the Advancement of Artificial Intelligence
(AAAI)</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Misra et al. (2018)</span>
<span class="ltx_bibblock">
Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, and
Laurens Van Der Maaten. 2018.

</span>
<span class="ltx_bibblock">Learning by asking questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
11–20.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mussmann and Liang (2018)</span>
<span class="ltx_bibblock">
Stephen Mussmann and Percy Liang. 2018.

</span>
<span class="ltx_bibblock">On the relationship between data efficiency and error in active
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2019)</span>
<span class="ltx_bibblock">
Junwon Park, Ranjay Krishna, Pranav Khadpe, Li Fei-Fei, and Michael Bernstein.
2019.

</span>
<span class="ltx_bibblock">AI-based request augmentation to increase crowdsourcing
participation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Association for the Advancement of Artificial Intelligence
(AAAI)</em>, volume 7, pages 115–124.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014.

</span>
<span class="ltx_bibblock">GloVe: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>,
pages 1532–1543.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015a)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
(NeurIPS)</em>, pages 2953–2961.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015b)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015b.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence
(PAMI)</em>, 39:1137–1149.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheffer et al. (2001)</span>
<span class="ltx_bibblock">
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001.

</span>
<span class="ltx_bibblock">Active hidden Markov models for information extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">International Symposium on Intelligent Data Analysis</em>, pages
309–318.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schein and Ungar (2007)</span>
<span class="ltx_bibblock">
A. Schein and Lyle H. Ungar. 2007.

</span>
<span class="ltx_bibblock">Active learning for logistic regression: An evaluation.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>, 68:235–265.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sener and Savarese (2018)</span>
<span class="ltx_bibblock">
Ozan Sener and Silvio Savarese. 2018.

</span>
<span class="ltx_bibblock">Active learning for convolutional neural networks: A core-set
approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Settles (2009)</span>
<span class="ltx_bibblock">
Burr Settles. 2009.

</span>
<span class="ltx_bibblock">Active learning literature survey.

</span>
<span class="ltx_bibblock">Technical report, University of Wisconsin, Madison.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2017)</span>
<span class="ltx_bibblock">
Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod, and Animashree
Anandkumar. 2017.

</span>
<span class="ltx_bibblock">Deep active learning for named entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Workshop on Representation
Learning for NLP (Repl4NLP)</em>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddhant and Lipton (2018)</span>
<span class="ltx_bibblock">
Aditya Siddhant and Zachary C Lipton. 2018.

</span>
<span class="ltx_bibblock">Deep Bayesian active learning for natural language processing:
Results of a large-scale empirical study.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. (2019)</span>
<span class="ltx_bibblock">
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
2019.

</span>
<span class="ltx_bibblock">A corpus for reasoning about natural language grounded in
photographs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swayamdipta et al. (2020)</span>
<span class="ltx_bibblock">
Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh
Hajishirzi, Noah A. Smith, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Dataset cartography: Mapping and diagnosing datasets with training
dynamics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock">LXMERT: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tellex et al. (2011)</span>
<span class="ltx_bibblock">
Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal
Banerjee, Seth J Teller, and Nicholas Roy. 2011.

</span>
<span class="ltx_bibblock">Understanding natural language commands for robotic navigation and
mobile manipulation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Association for the Advancement of Artificial Intelligence
(AAAI)</em>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2018)</span>
<span class="ltx_bibblock">
Damien Teney, Peter Anderson, Xiaodong He, and Anton V. D. Hengel. 2018.

</span>
<span class="ltx_bibblock">Tips and tricks for visual question answering: Learnings from the
2017 challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
4223–4232.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva et al. (2019)</span>
<span class="ltx_bibblock">
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler,
Yoshua Bengio, and Geoffrey J Gordon. 2019.

</span>
<span class="ltx_bibblock">An empirical study of example forgetting during deep neural network
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong and Koller (2001)</span>
<span class="ltx_bibblock">
Simon Tong and Daphne Koller. 2001.

</span>
<span class="ltx_bibblock">Support vector machine active learning with applications to text
classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, 2(0):45–66.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torralba and Efros (2011)</span>
<span class="ltx_bibblock">
Antonio Torralba and Alexei A Efros. 2011.

</span>
<span class="ltx_bibblock">Unbiased look at dataset bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
1521–1528.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al. (2018)</span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock">A broad-coverage challenge corpus for sentence understanding through
inference.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>, pages
1112–1122.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winograd (1972)</span>
<span class="ltx_bibblock">
Terry Winograd. 1972.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Understanding Natural Language</em>.

</span>
<span class="ltx_bibblock">Academic Press.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and
Jamie Brew. 2019.

</span>
<span class="ltx_bibblock">HuggingFace’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016)</span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. 2016.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
4622–4630.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Loog (2018)</span>
<span class="ltx_bibblock">
Yazhou Yang and Marco Loog. 2018.

</span>
<span class="ltx_bibblock">A benchmark and comparison of active learning for logistic
regression.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, 83.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2015)</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.
2015.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.02167</em>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2016)</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016.

</span>
<span class="ltx_bibblock">Visual7W: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition (CVPR)</em>, pages
4995–5004.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2017)</span>
<span class="ltx_bibblock">
Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta,
Li Fei-Fei, and Ali Farhadi. 2017.

</span>
<span class="ltx_bibblock">Target-driven visual navigation in indoor scenes using deep
reinforcement learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">International Conference on Robotics and Automation (ICRA)</em>,
pages 3357–3364.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Overview</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Due to the broad scope of our experiments and analysis, we were unable to fit all our results in the main body of the paper. Furthermore, given the limited length provided by the appendix, we provide only salient implementation details and other representative results here; however, we make all code, models, data, results, active learning implementations available at this link: <a target="_blank" href="https://github.com/siddk/vqa-outliers" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/siddk/vqa-outliers</a>.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.5" class="ltx_p">Generally, any combination of <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="A1.p2.1.m1.1a"><mo stretchy="false" id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><ci id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">\{</annotation></semantics></math><span id="A1.p2.5.2" class="ltx_text ltx_font_italic">active learning strategy</span> <math id="A1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.p2.2.m2.1a"><mo id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><times id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">\times</annotation></semantics></math> <span id="A1.p2.5.3" class="ltx_text ltx_font_italic">model</span> <math id="A1.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.p2.3.m3.1a"><mo id="A1.p2.3.m3.1.1" xref="A1.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.p2.3.m3.1b"><times id="A1.p2.3.m3.1.1.cmml" xref="A1.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.3.m3.1c">\times</annotation></semantics></math> <span id="A1.p2.5.4" class="ltx_text ltx_font_italic">seed set size</span> <math id="A1.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.p2.4.m4.1a"><mo id="A1.p2.4.m4.1.1" xref="A1.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.p2.4.m4.1b"><times id="A1.p2.4.m4.1.1.cmml" xref="A1.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.4.m4.1c">\times</annotation></semantics></math> <span id="A1.p2.5.1" class="ltx_text ltx_font_italic">analysis/acquisition plot<math id="A1.p2.5.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="A1.p2.5.1.m1.1a"><mo stretchy="false" id="A1.p2.5.1.m1.1.1" xref="A1.p2.5.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="A1.p2.5.1.m1.1b"><ci id="A1.p2.5.1.m1.1.1.cmml" xref="A1.p2.5.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.5.1.m1.1c">\}</annotation></semantics></math></span> is present in this paper, and is available in the public code repository.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Models &amp; Training</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Where applicable, we implement our models based on publicly available PyTorch implementations. For the LSTM-CNN model, we base our implementation off of this repository: <a target="_blank" href="https://github.com/Shivanshu-Gupta/Visual-Question-Answering" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Shivanshu-Gupta/Visual-Question-Answering</a>, while for the Bottom-Up Top-Down Attention Model, we use this repository: <a target="_blank" href="https://github.com/hengyuan-hu/bottom-up-attention-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hengyuan-hu/bottom-up-attention-vqa</a>, keeping default hyperparameters the same.</p>
</div>
<section id="A2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Logistic Regression.</h4>

<div id="A2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px1.p1.1" class="ltx_p">When implementing Logistic Regression, we base our PyTorch implementation on the broadly used Scikit-Learn (<a target="_blank" href="https://scikit-learn.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://scikit-learn.org</a>) implementation, using the default parameters (including L2 weight decay). We optimize our models via stochastic gradient descent.</p>
</div>
</section>
<section id="A2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LXMERT.</h4>

<div id="A2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px2.p1.1" class="ltx_p">As mentioned in Section 3, the default LXMERT checkpoint and fine-tuning code made publicly available in <cite class="ltx_cite ltx_citemacro_citet">Tan and Bansal (<a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite> (associated code repository: <a target="_blank" href="https://github.com/airsplay/lxmert" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/airsplay/lxmert</a>) is pretrained on data from VQA-2 and GQA, leaking information that could substantially affect our active learning results. To mitigate this, we contacted the authors, who kindly provided us with a checkpoint of the model without VQA pretraining.</p>
</div>
<div id="A2.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="A2.SS1.SSS0.Px2.p2.1" class="ltx_p">However, in addition to this model obtaining different results from those reported in the original work, the provided pretrained checkpoint behaves slightly differently during fine-tuning, requiring different hyperparameters than provided in the original repository. We perform a coarse grid search over hyperparameters, using the LXMERT implementation provided by HuggingFace Transformers <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib73" title="" class="ltx_ref">2019</a>)</cite>, and find that using an AdamW optimizer rather than the BERT-Adam Optimizer used in the original work <span id="A2.SS1.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">without any special learning rate scheduling</span> results in the best fine-tuning performance.</p>
</div>
</section>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Acquisition Functions</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">We use standard implementations of the 8 active learning strategies described, borrowing from prior implementations <cite class="ltx_cite ltx_citemacro_citep">(Mussmann and Liang, <a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite> and existing code repositories (<a target="_blank" href="https://github.com/google/active-learning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/active-learning</a>). We provide additional details below.</p>
</div>
<section id="A2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Monte-Carlo Dropout.</h4>

<div id="A2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS2.SSS0.Px1.p1.1" class="ltx_p">For our implementations of the deep Bayesian active learning methods (Monte-Carlo Dropout w/ Entropy, BALD), we follow <cite class="ltx_cite ltx_citemacro_citet">Gal and Ghahramani (<a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite> and estimate a Dropout distribution via test-time dropout, running multiple forward passes through our neural networks, with different, randomly sampled Dropout masks. We use a value of <math id="A2.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="A2.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="A2.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">k</mi><mo id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1"><eq id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="A2.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A2.SS2.SSS0.Px1.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.SSS0.Px1.p1.1.m1.1c">k=10</annotation></semantics></math> forward passes to form our Dropout distribution.</p>
</div>
</section>
<section id="A2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Amortized Core-Set Selection.</h4>

<div id="A2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS2.SSS0.Px2.p1.1" class="ltx_p">In the original Core-Set selection active learning work introduced by <cite class="ltx_cite ltx_citemacro_citet">Sener and Savarese (<a href="#bib.bib59" title="" class="ltx_ref">2018</a>)</cite>, it is shown that Core-Set selection for active learning can be reduced to a version of the <span id="A2.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">k-centers problem</span>, which can be solved approximately (2-OPT) with a greedy algorithm. However, running this algorithm on high-dimensional representations, across large pools can be prohibitive; Core-Set selection is <span id="A2.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">batch-aware</span>, requiring recomputing distances from each “cluster-center” (points in the set of acquired examples) to all points in the active learning pool <span id="A2.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">after each acquisition in a batch</span>. While we can run this out completely for smaller datasets (and indeed, this is what we do for our small datasets VQA-Sports and VQA-Food), a single acquisition iteration for a large dataset for the full VQA-2 dataset takes approximately 20 GPU-hours on the resources we have available, or up to 9 days for a single Core-Set selection run. For GQA, performing exact Core-Set selection takes at least twice as long.</p>
</div>
<div id="A2.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="A2.SS2.SSS0.Px2.p2.1" class="ltx_p">To still capture the spirit of Core-Set diversity-based selection in our evaluation, we instead introduce an <span id="A2.SS2.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">amortized implementation of Core-Set selection</span>, which is comprised of two steps. We first downsample the high-dimensional representations (of either the fused language and text, or either unimodal representations) via Principal Component Analysis (PCA) to make the distance computation faster by an order of magnitude. Then, rather than updating distances from examples in our acquired set to points in our pool <span id="A2.SS2.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">after each acquisition <math id="A2.SS2.SSS0.Px2.p2.1.1.m1.1" class="ltx_Math" alttext="\hat{x}" display="inline"><semantics id="A2.SS2.SSS0.Px2.p2.1.1.m1.1a"><mover accent="true" id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.cmml"><mi id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2.cmml">x</mi><mo id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.1" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS2.SSS0.Px2.p2.1.1.m1.1b"><apply id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.cmml" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1"><ci id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.1.cmml" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.1">^</ci><ci id="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2.cmml" xref="A2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.SSS0.Px2.p2.1.1.m1.1c">\hat{x}</annotation></semantics></math></span>, we delay updates, instead only refreshing the distance computation every 2000 acquisitions (roughly 5% of an acquisition batch for VQA-2). This allows us to report results for Core-Set selection with the three different proposed representations (Fused, Language-Only, Vision-Only) for VQA-2; unfortunately, for GQA and LXMERT (due to the high cost of training), even running this amortized version of Core-Set selection is prohibitive, so we report a subset of results, and omit the rest.</p>
</div>
</section>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Active Learning Results</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We include further results from our study of active learning applied to VQA, including results on VQA-Food (not included in the main body), active learning results for the two logistic regression models – Log-Reg (ResNet-101) and Log-Reg (Faster R-CNN), as well as with the 4 acquisition strategies not included in the main body of the paper – Entropy, Monte-Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision).</p>
</div>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>VQA-Food</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">Figure <a href="#A4.F9" title="Figure 9 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows results on VQA-Food with the LSTM-CNN, BUTD, and LXMERT models, with a seed set comprised of 10% of the total pool. The results are mostly similar to those reported in the paper; strategies track or underperform random sampling, with the exception of Least-Confidence for the LSTM-CNN model – however, this is the sole exception, and the LSTM-CNN has the highest training variance of all the models we try.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Logistic Regression (ResNet-101)</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">Figure <a href="#A4.F10" title="Figure 10 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows active learning results for the LogReg (ResNet-101) model on VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%). Results are similar to those reported in the paper, with active learning failing to outperform random acqusition.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Logistic Regression (Faster R-CNN)</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">Figure <a href="#A4.F11" title="Figure 11 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> presents the same set of experiments as the prior section, except with the LogReg (Faster R-CNN) model. While the object-based Faster R-CNN representation enables much higher performance than the ResNet-101 representation, active learning results are consistent with those reported in the paper.</p>
</div>
</section>
<section id="A3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Other Acquisition Strategies</h3>

<div id="A3.SS4.p1" class="ltx_para">
<p id="A3.SS4.p1.1" class="ltx_p">Figure <a href="#A4.F12" title="Figure 12 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> presents results for the four other active learning strategies we implement – Entropy, Monte Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision) – for the BUTD model. Results are across VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%) – despite the unique features of each strategy, the trends remain consistent with those in the paper.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Dataset Maps &amp; Acquisitions</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">To provide further context around active learning acquisitions across datasets, Figures <a href="#A4.F13" title="Figure 13 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>–<a href="#A4.F16" title="Figure 16 ‣ Appendix D Dataset Maps &amp; Acquisitions ‣ Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> present Dataset Maps and acquisitions for the BUTD Model across VQA-Sports, VQA-Food, and GQA respectively. Interesting to note is that while VQA-Sports and VQA-Food are generally easier, with fewer “hard-to-learn” examples, active learning still has a bias for picking those examples. For GQA, our earlier analysis is confirmed; active learning is picking the collective outliers populating the bottom half of the Dataset Map.</p>
</div>
<figure id="A4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F9.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x21.png" id="A4.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F9.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x22.png" id="A4.F9.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F9.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x23.png" id="A4.F9.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Results for the representative active learning methods on VQA-Food, a simplified VQA dataset similar to VQA-Food, across LSTM-CNN, BUTD, and LXMERT.</figcaption>
</figure>
<figure id="A4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F10.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x24.png" id="A4.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F10.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x25.png" id="A4.F10.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F10.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x26.png" id="A4.F10.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Active learning results using the Logistic Regression (ResNet-101) model on VQA-Sports (10% seed set), and VQA-2 (10% and 50% seed set). Most strategies either track or underperform random acquisition.</figcaption>
</figure>
<figure id="A4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F11.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x27.png" id="A4.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F11.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x28.png" id="A4.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F11.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x29.png" id="A4.F11.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Active learning results using the Logistic Regression (Faster R-CNN) model on VQA-Sports (10% seed set), and VQA-2 (10% and 50% seed set). While the Faster R-CNN representation leads to better validation accuracies, active learning performance remains consistent.</figcaption>
</figure>
<figure id="A4.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F12.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x30.png" id="A4.F12.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F12.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x31.png" id="A4.F12.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F12.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x32.png" id="A4.F12.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Results with the BUTD on VQA-Sports, VQA-2 and GQA using the alternative 4 acquisition strategies not included in the main body of the paper. Unsurprisingly, results are consistent with those reported in the paper.</figcaption>
</figure>
<figure id="A4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x33.png" id="A4.F13.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="150" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x34.png" id="A4.F13.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="150" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F13.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/supplemental-figures/butd-maps/butd-gqa-all.png" id="A4.F13.3.g1" class="ltx_graphics ltx_img_landscape" width="628" height="194" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Dataset Maps for the Bottom-Up Top-Down Attention model on VQA-Sports, VQA-Food, and GQA respectively. Note that VQA-Sports and VQA-Food have fewer “hard-to-learn” examples.</figcaption>
</figure>
<figure id="A4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F14.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x35.png" id="A4.F14.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F14.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x36.png" id="A4.F14.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F14.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x37.png" id="A4.F14.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Acquisitions with the BUTD Model on VQA-Sports. The dataset has fewer “hard-to-learn” examples, but active learning strategies pick the medium–hard examples, which still negatively impact performance.</figcaption>
</figure>
<figure id="A4.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F15.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x38.png" id="A4.F15.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F15.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x39.png" id="A4.F15.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F15.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x40.png" id="A4.F15.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Acquisitions with the BUTD Model on VQA-Food. Despite the sparsity of hard examples, active learning strategies still tend towards them. BALD is high-variance, selecting examples all over the map.</figcaption>
</figure>
<figure id="A4.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F16.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x41.png" id="A4.F16.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F16.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x42.png" id="A4.F16.2.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A4.F16.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2107.02331/assets/x43.png" id="A4.F16.3.g1" class="ltx_graphics ltx_img_landscape" width="484" height="149" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Acquisitions with the BUTD Model on the full GQA dataset. Given that the map for GQA is similar to the map for VQA-2, it is not surprising that the active learning acquisitions follow a similar trend, preferring to select “hard-to-learn” examples.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.02330" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.02331" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.02331">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.02331" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.02332" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 12 08:17:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
